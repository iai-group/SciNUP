{"id": "0704.0589", "contents": "Title: Analysis of the real estate market in Las Vegas: Bubble, seasonal\n  patterns, and prediction of the CSW indexes Abstract: We analyze 27 house price indexes of Las Vegas from Jun. 1983 to Mar. 2005,\ncorresponding to 27 different zip codes. These analyses confirm the existence\nof a real-estate bubble, defined as a price acceleration faster than\nexponential, which is found however to be confined to a rather limited time\ninterval in the recent past from approximately 2003 to mid-2004 and has\nprogressively transformed into a more normal growth rate comparable to\npre-bubble levels in 2005. There has been no bubble till 2002 except for a\nmedium-sized surge in 1990. In addition, we have identified a strong yearly\nperiodicity which provides a good potential for fine-tuned prediction from\nmonth to month. A monthly monitoring using a model that we have developed could\nconfirm, by testing the intra-year structure, if indeed the market has returned\nto ``normal'' or if more turbulence is expected ahead. We predict the evolution\nof the indexes one year ahead, which is validated with new data up to Sep.\n2006. The present analysis demonstrates the existence of very significant\nvariations at the local scale, in the sense that the bubble in Las Vegas seems\nto have preceded the more global USA bubble and has ended approximately two\nyears earlier (mid 2004 for Las Vegas compared with mid-2006 for the whole of\nthe USA). \n\n"}
{"id": "0705.0029", "contents": "Title: EGT through Quantum Mechanics & from Statistical Physics to Economics Abstract: By analyzing the relationships between a socioeconomical system modeled\nthrough evolutionary game theory and a physical system modeled through quantum\nmechanics we show how although both systems are described through two theories\napparently different both are analogous and thus exactly equivalents. The\nextensions of quantum mechanics to statistical physics and information theory\nlet us use some of their definitions for the best understanding of the behavior\nof economics and biology. The quantum analogue of the replicator dynamics is\nthe von Neumann equation. A system in where all its members are in Nash\nequilibrium is equivalent to a system in a maximum entropy state. Nature is a\ngame in where its players compete for a common welfare and the equilibrium of\nthe system that they are members. They act as a whole besides individuals like\nthey obey a rule in where they prefer to work for the welfare of the collective\nbesides the individual welfare. \n\n"}
{"id": "0705.1939", "contents": "Title: Towards Informative Statistical Flow Inversion Abstract: A problem which has recently attracted research attention is that of\nestimating the distribution of flow sizes in internet traffic. On high traffic\nlinks it is sometimes impossible to record every packet. Researchers have\napproached the problem of estimating flow lengths from sampled packet data in\ntwo separate ways. Firstly, different sampling methodologies can be tried to\nmore accurately measure the desired system parameters. One such method is the\nsample-and-hold method where, if a packet is sampled, all subsequent packets in\nthat flow are sampled. Secondly, statistical methods can be used to ``invert''\nthe sampled data and produce an estimate of flow lengths from a sample.\n  In this paper we propose, implement and test two variants on the\nsample-and-hold method. In addition we show how the sample-and-hold method can\nbe inverted to get an estimation of the genuine distribution of flow sizes.\nExperiments are carried out on real network traces to compare standard packet\nsampling with three variants of sample-and-hold. The methods are compared for\ntheir ability to reconstruct the genuine distribution of flow sizes in the\ntraffic. \n\n"}
{"id": "0705.3430", "contents": "Title: The Macro Model of the Inequality Process and The Surging Relative\n  Frequency of Large Wage Incomes Abstract: This paper presents a model of the dynamics of the wage income distribution. \n\n"}
{"id": "0706.1028", "contents": "Title: Hiking the hypercube: producers and consumers Abstract: We study the dynamics of co-evolution of producers and customers described by\nbit-strings representing individual traits. Individual ''size-like'' properties\nare controlled by binary encounters which outcome depends upon a recognition\nprocess. Depending upon the parameter set-up, mutual selection of producers and\ncustomers results in different types of attractors, either an exclusive niches\nregime or a competition regime. \n\n"}
{"id": "0707.0808", "contents": "Title: The Cyborg Astrobiologist: Porting from a wearable computer to the\n  Astrobiology Phone-cam Abstract: We have used a simple camera phone to significantly improve an `exploration\nsystem' for astrobiology and geology. This camera phone will make it much\neasier to develop and test computer-vision algorithms for future planetary\nexploration. We envision that the `Astrobiology Phone-cam' exploration system\ncan be fruitfully used in other problem domains as well. \n\n"}
{"id": "0707.1897", "contents": "Title: Maximum Entropy, the Collective Welfare Principle and the Globalization\n  Process Abstract: Although both systems analyzed are described through two theories apparently\ndifferent (quantum mechanics and game theory) it is shown that both are\nanalogous and thus exactly equivalents. The quantum analogue of the replicator\ndynamics is the von Neumann equation. Quantum mechanics could be used to\nexplain more correctly biological and economical processes. It could even\nencloses theories like games and evolutionary dynamics. We can take some\nconcepts and definitions from quantum mechanics and physics for the best\nunderstanding of the behavior of economics and biology. Also, we could maybe\nunderstand nature like a game in where its players compete for a common welfare\nand the equilibrium of the system that they are members. All the members of our\nsystem will play a game in which its maximum payoff is the equilibrium of the\nsystem. They act as a whole besides individuals like they obey a rule in where\nthey prefer to work for the welfare of the collective besides the individual\nwelfare. A system where its members are in Nash Equilibrium (or ESS) is exactly\nequivalent to a system in a maximum entropy state. A system is stable only if\nit maximizes the welfare of the collective above the welfare of the individual.\nIf it is maximized the welfare of the individual above the welfare of the\ncollective the system gets unstable an eventually collapses. The results of\nthis work shows that the \"globalization\" process has a behavior exactly\nequivalent to a system that is tending to a maximum entropy state and predicts\nthe apparition of big common markets and strong common currencies that will\nfind its \"equilibrium\" by decreasing its number until they get a state\ncharacterized by only one common currency and only one common market around the\nworld. \n\n"}
{"id": "0707.4347", "contents": "Title: The International Trade Network Abstract: Bilateral trade relationships in the international level between pairs of\ncountries in the world give rise to the notion of the International Trade\nNetwork (ITN). This network has attracted the attention of network researchers\nas it serves as an excellent example of the weighted networks, the link weight\nbeing defined as a measure of the volume of trade between two countries. In\nthis paper we analyzed the international trade data for 53 years and studied in\ndetail the variations of different network related quantities associated with\nthe ITN. Our observation is that the ITN has also a scale invariant structure\nlike many other real-world networks. \n\n"}
{"id": "0708.0522", "contents": "Title: Quasi-stationary distributions as centrality measures of reducible\n  graphs Abstract: Random walk can be used as a centrality measure of a directed graph. However,\nif the graph is reducible the random walk will be absorbed in some subset of\nnodes and will never visit the rest of the graph. In Google PageRank the\nproblem was solved by introduction of uniform random jumps with some\nprobability. Up to the present, there is no clear criterion for the choice this\nparameter. We propose to use parameter-free centrality measure which is based\non the notion of quasi-stationary distribution. Specifically we suggest four\nquasi-stationary based centrality measures, analyze them and conclude that they\nproduce approximately the same ranking. The new centrality measures can be\napplied in spam detection to detect ``link farms'' and in image search to find\nphoto albums. \n\n"}
{"id": "0708.1579", "contents": "Title: Homogeneous temporal activity patterns in a large online communication\n  space Abstract: The many-to-many social communication activity on the popular technology-news\nwebsite Slashdot has been studied. We have concentrated on the dynamics of\nmessage production without considering semantic relations and have found\nregular temporal patterns in the reaction time of the community to a news-post\nas well as in single user behavior. The statistics of these activities follow\nlog-normal distributions. Daily and weekly oscillatory cycles, which cause\nslight variations of this simple behavior, are identified. A superposition of\ntwo log-normal distributions can account for these variations. The findings are\nremarkable since the distribution of the number of comments per users, which is\nalso analyzed, indicates a great amount of heterogeneity in the community. The\nreader may find surprising that only a few parameters allow a detailed\ndescription, or even prediction, of social many-to-many information exchange in\nthis kind of popular public spaces. \n\n"}
{"id": "0709.2016", "contents": "Title: Distribution of PageRank Mass Among Principle Components of the Web Abstract: We study the PageRank mass of principal components in a bow-tie Web Graph, as\na function of the damping factor c. Using a singular perturbation approach, we\nshow that the PageRank share of IN and SCC components remains high even for\nvery large values of the damping factor, in spite of the fact that it drops to\nzero when c goes to one. However, a detailed study of the OUT component reveals\nthe presence ``dead-ends'' (small groups of pages linking only to each other)\nthat receive an unfairly high ranking when c is close to one. We argue that\nthis problem can be mitigated by choosing c as small as 1/2. \n\n"}
{"id": "0709.2694", "contents": "Title: Innovation Success and Structural Change: An Abstract Agent Based Study Abstract: A model is developed to study the effectiveness of innovation and its impact\non structure creation and structure change on agent-based societies. The\nabstract model that is developed is easily adapted to any particular field. In\nany interacting environment, the agents receive something from the environment\n(the other agents) in exchange for their effort and pay the environment a\ncertain amount of value for the fulfilling of their needs or for the very price\nof existence in that environment. This is coded by two bit strings and the\ndynamics of the exchange is based on the matching of these strings to those of\nthe other agents. Innovation is related to the adaptation by the agents of\ntheir bit strings to improve some utility function. \n\n"}
{"id": "0709.3094", "contents": "Title: Transition from small to large world in growing networks Abstract: We examine the global organization of growing networks in which a new vertex\nis attached to already existing ones with a probability depending on their age.\nWe find that the network is infinite- or finite-dimensional depending on\nwhether the attachment probability decays slower or faster than $(age)^{-1}$.\nThe network becomes one-dimensional when the attachment probability decays\nfaster than $(age)^{-2}$. We describe structural characteristics of these\nphases and transitions between them. \n\n"}
{"id": "0709.4096", "contents": "Title: Quantum Auctions: Facts and Myths Abstract: Quantum game theory, whatever opinions may be held due to its abstract\nphysical formalism, have already found various applications even outside the\northodox physics domain. In this paper we introduce the concept of a quantum\nauction, its advantages and drawbacks. Then we describe the models that have\nalready been put forward. A general model involves Wigner formalism and\ninfinite dimensional Hilbert spaces - we envisage that the implementation might\nnot be an easy task. But a restricted model advocated by the Hewlett-Packard\ngroup seems to be much easier to implement. Simulations involving humans have\nalready been performed. We will focus on problems related to combinatorial\nauctions and technical assumptions that are made. Quantum approach offers at\nleast two important developments. Powerful quantum algorithms for finding\nsolutions would extend the range of possible applications. Quantum strategies,\nbeing qubits, can be teleported but are immune from cloning - therefore extreme\nprivacy of agent's activity could in principle be guaranteed. Then we point out\nsome key problem that have to be solved before commercial use would be\npossible. With present technology, optical networks, single photon sources and\ndetectors seems to be sufficient for experimental realization in the near\nfuture. We conclude by describing potential customers, estimating the potential\nmarket size and possible timing. \n\n"}
{"id": "0710.0459", "contents": "Title: Statistical properties of agent-based market area model Abstract: One dimensional stylized model taking into account spatial activity of firms\nwith uniformly distributed customers is proposed. The spatial selling area of\neach firm is defined by a short interval cut out from selling space (large\ninterval). In this representation, the firm size is directly associated with\nthe size of its selling interval.\n  The recursive synchronous dynamics of economic evolution is discussed where\nthe growth rate is proportional to the firm size incremented by the term\nincluding the overlap of the selling area with areas of competing firms. Other\nwords, the overlap of selling areas inherently generate a negative feedback\noriginated from the pattern of demand. Numerical simulations focused on the\nobtaining of the firm size distributions uncovered that the range of free\nparameters where the Pareto's law holds corresponds to the range for which the\npair correlation between the nearest neighbor firms attains its minimum. \n\n"}
{"id": "0710.1014", "contents": "Title: Wealth distribution in a System with Wealth-limited Interactions Abstract: We model a closed economic system with interactions that generates the\nfeatures of empirical wealth distribution across all wealth brackets, namely a\nGibbsian trend in the lower and middle wealth range and a Pareto trend in the\nhigher range, by simply limiting the an agents' interaction to only agents with\nnearly the same wealth. To do this, we introduce a parameter BETA that limits\nthe range on the wealth of a partner with which an agent is allowed to\ninteract. We show that this wealth-limited interaction is enough to distribute\nwealth in a purely power law trend. If the interaction is not wealth limited,\nthe wealth distribution is expectedly Gibbsian. The value of BETA where the\ntransition from a purely Gibbsian law to a purely power law distribution\nhappens depends on whether the choice of interaction partner is mutual nor not.\nFor a non-mutual choice, where the richer agent gets to decide, the transition\nhappens at BETA=1.0. For a mutual choice, the transition is at BETA= 0.60. In\norder to generate a mixed Gibbs-Pareto distribution, we apply another\nwealth-based rule that depends on the parameter w_limit. An agent whose wealth\nis below w_limit can choose any partner to interact with, while an agent whose\nwealth is above w_limit is subject to the wealth-limited range in his choice of\npartner. A Gibbs-Pareto distribution appears if both these wealth-based rules\nare applied. \n\n"}
{"id": "0710.1307", "contents": "Title: Common Markets, Strong Currencies & the Collective Welfare Abstract: The so called \"globalization\" process (i.e. the inexorable integration of\nmarkets, currencies, nation-states, technologies and the intensification of\nconsciousness of the world as a whole) has a behavior exactly equivalent to a\nsystem that is tending to a maximum entropy state. This globalization process\nobeys a collective welfare principle in where the maximum payoff is given by\nthe equilibrium of the system and its stability by the maximization of the\nwelfare of the collective besides the individual welfare. This let us predict\nthe apparition of big common markets and strong common currencies. They will\nreach the \"equilibrium\" by decreasing its number until they reach a state\ncharacterized by only one common currency and only one big common community\naround the world. \n\n"}
{"id": "0710.2736", "contents": "Title: L2 norm performance index of synchronization and optimal control\n  synthesis of complex networks Abstract: In this paper, the synchronizability problem of dynamical networks is\naddressed, where better synchronizability means that the network synchronizes\nfaster with lower-overshoot. The L2 norm of the error vector e is taken as a\nperformance index to measure this kind of synchronizability. For the\nequilibrium synchronization case, it is shown that there is a close\nrelationship between the L2 norm of the error vector e and the H2 norm of the\ntransfer function G of the linearized network about the equilibrium point.\nConsequently, the effect of the network coupling topology on the H2 norm of the\ntransfer function G is analyzed. Finally, an optimal controller is designed,\naccording to the so-called LQR problem in modern control theory, which can\ndrive the whole network to its equilibrium point and meanwhile minimize the L2\nnorm of the output of the linearized network. \n\n"}
{"id": "0711.4710", "contents": "Title: Effects of network topology on wealth distributions Abstract: We focus on the problem of how wealth is distributed among the units of a\nnetworked economic system. We first review the empirical results documenting\nthat in many economies the wealth distribution is described by a combination of\nlog--normal and power--law behaviours. We then focus on the Bouchaud--M\\'ezard\nmodel of wealth exchange, describing an economy of interacting agents connected\nthrough an exchange network. We report analytical and numerical results showing\nthat the system self--organises towards a stationary state whose associated\nwealth distribution depends crucially on the underlying interaction network. In\nparticular we show that if the network displays a homogeneous density of links,\nthe wealth distribution displays either the log--normal or the power--law form.\nThis means that the first--order topological properties alone (such as the\nscale--free property) are not enough to explain the emergence of the\nempirically observed \\emph{mixed} form of the wealth distribution. In order to\nreproduce this nontrivial pattern, the network has to be heterogeneously\ndivided into regions with variable density of links. We show new results\ndetailing how this effect is related to the higher--order correlation\nproperties of the underlying network. In particular, we analyse assortativity\nby degree and the pairwise wealth correlations, and discuss the effects that\nthese properties have on each other. \n\n"}
{"id": "0712.2684", "contents": "Title: An Economic Model of Coupled Exponential Maps Abstract: In this work, an ensemble of economic interacting agents is considered. The\nagents are arranged in a linear array where only local couplings are allowed.\nThe deterministic dynamics of each agent is given by a map. This map is\nexpressed by two factors. The first one is a linear term that models the\nexpansion of the agent's economy and that is controlled by the {\\it growth\ncapacity parameter}. The second one is an inhibition exponential term that is\nregulated by the {\\it local environmental pressure}. Depending on the parameter\nsetting, the system can display Pareto or Boltzmann-Gibbs behavior in the\nasymptotic dynamical regime. The regions of parameter space where the system\nexhibits one of these two statistical behaviors are delimited. Other properties\nof the system, such as the mean wealth, the standard deviation and the Gini\ncoefficient, are also calculated. \n\n"}
{"id": "0802.3267", "contents": "Title: The Forgiving Tree: A Self-Healing Distributed Data Structure Abstract: We consider the problem of self-healing in peer-to-peer networks that are\nunder repeated attack by an omniscient adversary. We assume that the following\nprocess continues for up to n rounds where n is the total number of nodes\ninitially in the network: the adversary deletes an arbitrary node from the\nnetwork, then the network responds by quickly adding a small number of new\nedges.\n  We present a distributed data structure that ensures two key properties.\nFirst, the diameter of the network is never more than $O(\\log \\Delta)$ times\nits original diameter, where $\\Delta$ is the maximum degree of the network\ninitially. We note that for many peer-to-peer systems, $\\Delta$ is\npolylogarithmic, so the diameter increase would be a O(log log n)\nmultiplicative factor. Second, the degree of any node never increases by more\nthan 3 over its original degree. Our data structure is fully distributed, has\nO(1) latency per round and requires each node to send and receive O(1) messages\nper round. The data structure requires an initial setup phase that has latency\nequal to the diameter of the original network, and requires, with high\nprobability, each node v to send O(log n) messages along every edge incident to\nv. Our approach is orthogonal and complementary to traditional topology-based\napproaches to defending against attack. \n\n"}
{"id": "0802.4410", "contents": "Title: Gamma-distribution and wealth inequality Abstract: We discuss the equivalence between kinetic wealth-exchange models, in which\nagents exchange wealth during trades, and mechanical models of particles,\nexchanging energy during collisions. The universality of the underlying\ndynamics is shown both through a variational approach based on the minimization\nof the Boltzmann entropy and a complementary microscopic analysis of the\ncollision dynamics of molecules in a gas. In various relevant cases the\nequilibrium distribution is the same for all these models, namely a\ngamma-distribution with suitably defined temperature and number of dimensions.\nThis in turn allows one to quantify the inequalities observed in the wealth\ndistributions and suggests that their origin should be traced back to very\ngeneral underlying mechanisms: for instance, it follows that the smaller the\nfraction of the relevant quantity (e.g. wealth or energy) that agents can\nexchange during an interaction, the closer the corresponding equilibrium\ndistribution is to a fair distribution. \n\n"}
{"id": "0804.3658", "contents": "Title: The Problem of Modelling of Economic Dynamics in Differential Form Abstract: Traditional models of macroeconomic dynamics are fundamentally incorrect. The\nreason lies in a misunderstanding of peculiarities of the analysis of\ninfinitesimal quantities. However, even those types of solutions that are\nenvisaged by the above-mentioned models are nonrepresentative in the sense of\nthe reflection of realities. It became obvious that the techniques of the\ntheory of linear differential equations were insufficient here. Accordingly,\nthe scientists' attention switched to the theory of nonlinear differential\nequations. At the same time, balance and, accordingly, the model with matrix\nproperties are objectively inherent in the economic system. For the reduction\nof this model to a differential form, there exist rather elementary means that\nproved to be unclaimed. Macroeconomic rhetoric - the power of the accelerator,\na lag on the part of demand, etc. - accompanied by the use of a lot of abstract\ncoefficients prevailed. However, there is no organic interrelation between\nmatrix and nonlinear differential equations. On the contrary, it can be said\nthat linear theory of integral equations originated in matrix analysis. The\nFredholm linear integral equation of the second kind with a parameter-dependent\nkernel proves to be rather representative with regard to the class of possible\nsolutions. It seems that it can be used for the description of any zigzags of\nthe economy. The price one has to pay for this is the nontriviality of existing\ntheory. \n\n"}
{"id": "0804.3784", "contents": "Title: On the metric distortion of nearest-neighbour graphs on random point\n  sets Abstract: We study the graph constructed on a Poisson point process in $d$ dimensions\nby connecting each point to the $k$ points nearest to it. This graph a.s. has\nan infinite cluster if $k > k_c(d)$ where $k_c(d)$, known as the critical\nvalue, depends only on the dimension $d$. This paper presents an improved upper\nbound of 188 on the value of $k_c(2)$. We also show that if $k \\geq 188$ the\ninfinite cluster of $\\NN(2,k)$ has an infinite subset of points with the\nproperty that the distance along the edges of the graphs between these points\nis at most a constant multiplicative factor larger than their Euclidean\ndistance. Finally we discuss in detail the relevance of our results to the\nstudy of multi-hop wireless sensor networks. \n\n"}
{"id": "0806.1170", "contents": "Title: The 2006-2008 Oil Bubble and Beyond Abstract: We present an analysis of oil prices in US$ and in other major currencies\nthat diagnoses unsustainable faster-than-exponential behavior. This supports\nthe hypothesis that the recent oil price run-up has been amplified by\nspeculative behavior of the type found during a bubble-like expansion. We also\nattempt to unravel the information hidden in the oil supply-demand data\nreported by two leading agencies, the US Energy Information Administration\n(EIA) and the International Energy Agency (IEA). We suggest that the found\nincreasing discrepancy between the EIA and IEA figures provides a measure of\nthe estimation errors. Rather than a clear transition to a supply restricted\nregime, we interpret the discrepancy between the IEA and EIA as a signature of\nuncertainty, and there is no better fuel than uncertainty to promote\nspeculation! \n\n"}
{"id": "0806.1845", "contents": "Title: An efficient approach of controlling traffic congestion in scale-free\n  networks Abstract: We propose and study a model of traffic in communication networks. The\nunderlying network has a structure that is tunable between a scale-free growing\nnetwork with preferential attachments and a random growing network. To model\nrealistic situations where different nodes in a network may have different\ncapabilities, the message or packet creation and delivering rates at a node are\nassumed to depend on the degree of the node. Noting that congestions are more\nlikely to take place at the nodes with high degrees in networks with scale-free\ncharacter, an efficient approach of selectively enhancing the\nmessage-processing capability of a small fraction (e.g. 3%) of the nodes is\nshown to perform just as good as enhancing the capability of all nodes. The\ninterplay between the creation rate and the delivering rate in determining\nnon-congested or congested traffic in a network is studied more numerically and\nanalytically. \n\n"}
{"id": "0807.1823", "contents": "Title: Cooperation Evolution in Random Multiplicative Environments Abstract: Most real life systems have a random component: the multitude of endogenous\nand exogenous factors influencing them result in stochastic fluctuations of the\nparameters determining their dynamics. These empirical systems are in many\ncases subject to noise of multiplicative nature. The special properties of\nmultiplicative noise as opposed to additive noise have been noticed for a long\nwhile. Even though apparently and formally the difference between free additive\nvs. multiplicative random walks consists in just a move from normal to\nlog-normal distributions, in practice the implications are much more far\nreaching. While in an additive context the emergence and survival of\ncooperation requires special conditions (especially some level of reward,\npunishment, reciprocity), we find that in the multiplicative random context the\nemergence of cooperation is much more natural and effective. We study the\nvarious implications of this observation and its applications in various\ncontexts. \n\n"}
{"id": "0808.1655", "contents": "Title: Shelf space strategy in long-tail markets Abstract: The Internet is known to have had a powerful impact on on-line retailer\nstrategies in markets characterised by long-tail distribution of sales. Such\nretailers can exploit the long tail of the market, since they are effectively\nwithout physical limit on the number of choices on offer. Here we examine two\nextensions of this phenomenon. First, we introduce turnover into the long-tail\ndistribution of sales. Although over any given period such as a week or a\nmonth, the distribution is right-skewed and often power law distributed, over\ntime there is considerable turnover in the rankings of sales of individual\nproducts. Second, we establish some initial results on the implications for\nshelf-space strategy of physical retailers in such markets. \n\n"}
{"id": "0808.3937", "contents": "Title: Understanding Fairness and its Impact on Quality of Service in IEEE\n  802.11 Abstract: The Distributed Coordination Function (DCF) aims at fair and efficient medium\naccess in IEEE 802.11. In face of its success, it is remarkable that there is\nlittle consensus on the actual degree of fairness achieved, particularly\nbearing its impact on quality of service in mind. In this paper we provide an\naccurate model for the fairness of the DCF. Given M greedy stations we assume\nfairness if a tagged station contributes a share of 1/M to the overall number\nof packets transmitted. We derive the probability distribution of fairness\ndeviations and support our analytical results by an extensive set of\nmeasurements. We find a closed-form expression for the improvement of long-term\nover short-term fairness. Regarding the random countdown values we quantify the\nsignificance of their distribution whereas we discover that fairness is largely\ninsensitive to the distribution parameters. Based on our findings we view the\nDCF as emulating an ideal fair queuing system to quantify the deviations from a\nfair rate allocation. We deduce a stochastic service curve model for the DCF to\npredict packet delays in IEEE 802.11. We show how a station can estimate its\nfair bandwidth share from passive measurements of its traffic arrivals and\ndepartures. \n\n"}
{"id": "0808.4104", "contents": "Title: Flow-level Characteristics of Spam and Ham Abstract: Despite a large amount of effort devoted in the past years trying to limit\nunsolicited mail, spam is still a major global concern. Content-analysis\ntechniques and blacklists, the most popular methods used to identify and block\nspam, are beginning to lose their edge in the battle. We argue here that one\nnot only needs to look into the network-related characteristics of spam\ntraffic, as has been recently suggested, but also to look deeper into the\nnetwork core, in order to counter the increasing sophistication of spam-ing\nmethods. Yet, at the same time, local knowledge available at a given server can\noften be irreplaceable in identifying specific spammers. To this end, in this\npaper we show how the local intelligence of mail servers can be gathered and\ncorrelated pas- sively at the ISP-level providing valuable network-wide\ninformation. Specifically, we use first a large network flow trace from a\nmedium size, national ISP, to demonstrate that the pre-filtering decisions of\nindividual mail servers can be tracked and combined at the flow level. Then, we\nargue that such aggregated knowledge not only can allow ISPs to develop and\nevaluate powerful new methods for fighting spam, but also to monitor remotely\nwhat their own servers are doing. \n\n"}
{"id": "0809.1534", "contents": "Title: Outflow Dynamics in Modeling Oligopoly Markets: The Case of the Mobile\n  Telecommunications Market in Poland Abstract: In this paper we introduce two models of opinion dynamics in oligopoly\nmarkets and apply them to a situation, where a new entrant challenges two\nincumbents of the same size. The models differ in the way the two forces\ninfluencing consumer choice -- (local) social interactions and (global)\nadvertising -- interact. We study the general behavior of the models using the\nMean Field Approach and Monte Carlo simulations and calibrate the models to\ndata from the Polish telecommunications market. For one of the models\ncriticality is observed -- below a certain critical level of advertising the\nmarket approaches a lock-in situation, where one market leader dominates the\nmarket and all other brands disappear. Interestingly, for both models the best\nfits to real data are obtained for conformity level $p \\in (0.3,0.4)$. This\nagrees very well with the conformity level found by Solomon Asch in his famous\nsocial experiment. \n\n"}
{"id": "0810.3869", "contents": "Title: Power Control in Two-Tier Femtocell Networks Abstract: In a two tier cellular network -- comprised of a central macrocell underlaid\nwith shorter range femtocell hotspots -- cross-tier interference limits overall\ncapacity with universal frequency reuse. To quantify near-far effects with\nuniversal frequency reuse, this paper derives a fundamental relation providing\nthe largest feasible cellular Signal-to-Interference-Plus-Noise Ratio (SINR),\ngiven any set of feasible femtocell SINRs. We provide a link budget analysis\nwhich enables simple and accurate performance insights in a two-tier network. A\ndistributed utility-based SINR adaptation at femtocells is proposed in order to\nalleviate cross-tier interference at the macrocell from cochannel femtocells.\nThe Foschini-Miljanic (FM) algorithm is a special case of the adaptation. Each\nfemtocell maximizes their individual utility consisting of a SINR based reward\nless an incurred cost (interference to the macrocell). Numerical results show\ngreater than 30% improvement in mean femtocell SINRs relative to FM. In the\nevent that cross-tier interference prevents a cellular user from obtaining its\nSINR target, an algorithm is proposed that reduces transmission powers of the\nstrongest femtocell interferers. The algorithm ensures that a cellular user\nachieves its SINR target even with 100 femtocells/cell-site, and requires a\nworst case SINR reduction of only 16% at femtocells. These results motivate\ndesign of power control schemes requiring minimal network overhead in two-tier\nnetworks with shared spectrum. \n\n"}
{"id": "0811.1064", "contents": "Title: Transition from Pareto to Boltzmann-Gibbs behavior in a deterministic\n  economic model Abstract: The one-dimensional deterministic economic model recently studied by\nGonzalez-Estevez et al. [Physica A 387, 4367 (2008)] is considered on a\ntwo-dimensional square lattice with periodic boundary conditions. In this\nmodel, the evolution of each agent is described by a map coupled with its\nnearest neighbors. The map has two factors: a linear term that accounts for the\nagent's own tendency to grow and an exponential term that saturates this growth\nthrough the control effect of the environment. The regions in the parameter\nspace where the system displays Pareto and Boltzmann-Gibbs statistics are\ncalculated for the cases of von Neumann and of Moore's neighborhoods. It is\nfound that, even when the parameters in the system are kept fixed, a transition\nfrom Pareto to Boltzmann-Gibbs behavior can occur when the number of neighbors\nof each agent increases. \n\n"}
{"id": "0812.2664", "contents": "Title: Evidence for the Gompertz Curve in the Income Distribution of Brazil\n  1978-2005 Abstract: This work presents an empirical study of the evolution of the personal income\ndistribution in Brazil. Yearly samples available from 1978 to 2005 were studied\nand evidence was found that the complementary cumulative distribution of\npersonal income for 99% of the economically less favorable population is well\nrepresented by a Gompertz curve of the form $G(x)=\\exp [\\exp (A-Bx)]$, where\n$x$ is the normalized individual income. The complementary cumulative\ndistribution of the remaining 1% richest part of the population is well\nrepresented by a Pareto power law distribution $P(x)= \\beta x^{-\\alpha}$. This\nresult means that similarly to other countries, Brazil's income distribution is\ncharacterized by a well defined two class system. The parameters $A$, $B$,\n$\\alpha$, $\\beta$ were determined by a mixture of boundary conditions,\nnormalization and fitting methods for every year in the time span of this\nstudy. Since the Gompertz curve is characteristic of growth models, its\npresence here suggests that these patterns in income distribution could be a\nconsequence of the growth dynamics of the underlying economic system. In\naddition, we found out that the percentage share of both the Gompertzian and\nParetian components relative to the total income shows an approximate cycling\npattern with periods of about 4 years and whose maximum and minimum peaks in\neach component alternate at about every 2 years. This finding suggests that the\ngrowth dynamics of Brazil's economic system might possibly follow a\nGoodwin-type class model dynamics based on the application of the\nLotka-Volterra equation to economic growth and cycle. \n\n"}
{"id": "0901.1500", "contents": "Title: Superstatistics of Labour Productivity in Manufacturing and\n  Nonmanufacturing Sectors Abstract: Labour productivity distribution (dispersion) is studied both theoretically\nand empirically. Superstatistics is presented as a natural theoretical\nframework for productivity. The demand index $\\kappa$ is proposed within this\nframework as a new business index. Japanese productivity data covering\nsmall-to-medium to large firms from 1996 to 2006 is analyzed and the power-law\nfor both firms and workers is established. The demand index $\\kappa$ is\nevaluated in the manufacturing sector. A new discovery is reported for the\nnonmanufacturing (service) sector, which calls for expansion of the\nsuperstatistics framework to negative temperature range. \n\n"}
{"id": "0901.1794", "contents": "Title: Agent-Based Model Approach to Complex Phenomena in Real Economy Abstract: An agent-based model for firms' dynamics is developed. The model consists of\nfirm agents with identical characteristic parameters and a bank agent. Dynamics\nof those agents is described by their balance sheets. Each firm tries to\nmaximize its expected profit with possible risks in market. Infinite growth of\na firm directed by the \"profit maximization\" principle is suppressed by a\nconcept of \"going concern\". Possibility of bankruptcy of firms is also\nintroduced by incorporating a retardation effect of information on firms'\ndecision. The firms, mutually interacting through the monopolistic bank, become\nheterogeneous in the course of temporal evolution. Statistical properties of\nfirms' dynamics obtained by simulations based on the model are discussed in\nlight of observations in the real economy. \n\n"}
{"id": "0901.2333", "contents": "Title: Q-CSMA: Queue-Length Based CSMA/CA Algorithms for Achieving Maximum\n  Throughput and Low Delay in Wireless Networks Abstract: Recently, it has been shown that CSMA-type random access algorithms can\nachieve the maximum possible throughput in ad hoc wireless networks. However,\nthese algorithms assume an idealized continuous-time CSMA protocol where\ncollisions can never occur. In addition, simulation results indicate that the\ndelay performance of these algorithms can be quite bad. On the other hand,\nalthough some simple heuristics (such as distributed approximations of greedy\nmaximal scheduling) can yield much better delay performance for a large set of\narrival rates, they may only achieve a fraction of the capacity region in\ngeneral. In this paper, we propose a discrete-time version of the CSMA\nalgorithm. Central to our results is a discrete-time distributed randomized\nalgorithm which is based on a generalization of the so-called Glauber dynamics\nfrom statistical physics, where multiple links are allowed to update their\nstates in a single time slot. The algorithm generates collision-free\ntransmission schedules while explicitly taking collisions into account during\nthe control phase of the protocol, thus relaxing the perfect CSMA assumption.\nMore importantly, the algorithm allows us to incorporate mechanisms which lead\nto very good delay performance while retaining the throughput-optimality\nproperty. It also resolves the hidden and exposed terminal problems associated\nwith wireless networks. \n\n"}
{"id": "0901.2377", "contents": "Title: Structure and temporal change of the credit network between banks and\n  large firms in Japan Abstract: We present a new approach to understanding credit relationships between\ncommercial banks and quoted firms, and with this approach, examine the temporal\nchange in the structure of the Japanese credit network from 1980 to 2005. At\neach year, the credit network is regarded as a weighted bipartite graph where\nedges correspond to the relationships and weights refer to the amounts of\nloans. Reduction in the supply of credit affects firms as debtor, and failure\nof a firm influences banks as creditor. To quantify the dependency and\ninfluence between banks and firms, we propose a set of scores of banks and\nfirms, which can be calculated by solving an eigenvalue problem determined by\nthe weight of the credit network. We found that a few largest eigenvalues and\ncorresponding eigenvectors are significant by using a null hypothesis of random\nbipartite graphs, and that the scores can quantitatively describe the stability\nor fragility of the credit network during the 25 years. \n\n"}
{"id": "0901.2381", "contents": "Title: Visualizing a large-scale structure of production network by N-body\n  simulation Abstract: Our recent study of a nation-wide production network uncovered a community\nstructure, namely how firms are connected by supplier-customer links into\ntightly-knit groups with high density in intra-groups and with lower\nconnectivity in inter-groups. Here we propose a method to visualize the\ncommunity structure by a graph layout based on a physical analogy. The layout\ncan be calculated in a practical computation-time and is possible to be\naccelerated by a special-purpose device of GRAPE (gravity pipeline) developed\nfor astrophysical N-body simulation. We show that the method successfully\nidentifies the communities in a hierarchical way by applying it to the\nmanufacturing sector comprising tenth million nodes and a half million edges.\nIn addition, we discuss several limitations of this method, and propose a\npossible way to avoid all those problems. \n\n"}
{"id": "0901.3003", "contents": "Title: Timed tuplix calculus and the Wesseling and van den Bergh equation Abstract: We develop an algebraic framework for the description and analysis of\nfinancial behaviours, that is, behaviours that consist of transferring certain\namounts of money at planned times. To a large extent, analysis of financial\nproducts amounts to analysis of such behaviours. We formalize the cumulative\ninterest compliant conservation requirement for financial products proposed by\nWesseling and van den Bergh by an equation in the framework developed and\ndefine a notion of financial product behaviour using this formalization. We\nalso present some properties of financial product behaviours. The development\nof the framework has been influenced by previous work on the process algebra\nACP. \n\n"}
{"id": "0902.4274", "contents": "Title: Time and symmetry in models of economic markets Abstract: These notes discuss several topics in neoclassical economics and\nalternatives, with an aim of reviewing fundamental issues in modeling economic\nmarkets. I start with a brief, non-rigorous summary of the basic Arrow-Debreu\nmodel of general equilibrium, as well as its extensions to include time and\ncontingency. I then argue that symmetries due to similarly endowed individuals\nand similar products are generically broken by the constraints of scarcity,\nleading to the existence of multiple equilibria.\n  This is followed by an evaluation of the strengths and weaknesses of the\nmodel generally. Several of the weaknesses are concerned with the treatments of\ntime and contingency. To address these we discuss a class of agent based\nmodels.\n  Another set of issues has to do with the fundamental meaning of prices and\nthe related question of what the observables of a non-equilibrium, dynamic\nmodel of an economic market should be. We argue that these issues are addressed\nby formulating economics in the language of a gauge theory, as proposed\noriginally by Malaney and Weinstein. We review some of their work and provide a\nsketch of how gauge invariance can be incorporated into the formulation of\nagent based models. \n\n"}
{"id": "0903.0282", "contents": "Title: A dynamic nonlinear model for saturation in industrial growth Abstract: A general nonlinear logistic equation has been proposed to model long-time\nsaturation in industrial growth. An integral solution of this equation has been\nderived for any arbitrary degree of nonlinearity. A time scale for the onset of\nnonlinear saturation in industrial growth can be estimated from an\nequipartition condition between nonlinearity and purely exponential growth.\nPrecise predictions can be made about the limiting values of the annual revenue\nand the human resource content that an industrial organisation may attain.\nThese variables have also been modelled to set up an autonomous first-order\ndynamical system, whose equilibrium condition forms a stable node (an attractor\nstate) in a related phase portrait. The theoretical model has received close\nsupport from all relevant data pertaining to the well-known global company,\nIBM. \n\n"}
{"id": "0903.0694", "contents": "Title: Digital Ecosystems in the Clouds: Towards Community Cloud Computing Abstract: Cloud Computing is rising fast, with its data centres growing at an\nunprecedented rate. However, this has come with concerns of privacy, efficiency\nat the expense of resilience, and environmental sustainability, because of the\ndependence on Cloud vendors such as Google, Amazon, and Microsoft. Community\nCloud Computing makes use of the principles of Digital Ecosystems to provide a\nparadigm for Clouds in the community, offering an alternative architecture for\nthe use cases of Cloud Computing. It is more technically challenging to deal\nwith issues of distributed computing, such as latency, differential resource\nmanagement, and additional security requirements. However, these are not\ninsurmountable challenges, and with the need to retain control over our digital\nlives and the potential environmental consequences, it is a challenge we must\npursue. \n\n"}
{"id": "0903.2352", "contents": "Title: A Mean Field Approach for Optimization in Particles Systems and\n  Applications Abstract: This paper investigates the limit behavior of Markov Decision Processes\n(MDPs) made of independent particles evolving in a common environment, when the\nnumber of particles goes to infinity. In the finite horizon case or with a\ndiscounted cost and an infinite horizon, we show that when the number of\nparticles becomes large, the optimal cost of the system converges almost surely\nto the optimal cost of a discrete deterministic system (the ``optimal mean\nfield''). Convergence also holds for optimal policies. We further provide\ninsights on the speed of convergence by proving several central limits theorems\nfor the cost and the state of the Markov decision process with explicit\nformulas for the variance of the limit Gaussian laws. Then, our framework is\napplied to a brokering problem in grid computing. The optimal policy for the\nlimit deterministic system is computed explicitly. Several simulations with\ngrowing numbers of processors are reported. They compare the performance of the\noptimal policy of the limit system used in the finite case with classical\npolicies (such as Join the Shortest Queue) by measuring its asymptotic gain as\nwell as the threshold above which it starts outperforming classical policies. \n\n"}
{"id": "0903.4100", "contents": "Title: Decentralized Management of Bi-modal Network Resources in a Distributed\n  Stream Processing Platform Abstract: This paper presents resource management techniques for allocating\ncommunication and computational resources in a distributed stream processing\nplatform. The platform is designed to exploit the synergy of two classes of\nnetwork connections -- dedicated and opportunistic. Previous studies we\nconducted have demonstrated the benefits of such bi-modal resource organization\nthat combines small pools of dedicated computers with a very large pool of\nopportunistic computing capacities of idle computers to serve high throughput\ncomputing applications. This paper extends the idea of bi-modal resource\norganization into the management of communication resources. Since distributed\nstream processing applications demand large volume of data transmission between\nprocessing sites at a consistent rate, adequate control over the network\nresources is important to assure a steady flow of processing. The system model\nused in this paper is a platform where stream processing servers at distributed\nsites are interconnected with a combination of dedicated and opportunistic\ncommunication links. Two pertinent resource allocation problems are analyzed in\ndetails and solved using decentralized algorithms. One is the mapping of the\nstream processing tasks on the processing and the communication resources. The\nother is the adaptive re-allocation of the opportunistic communication links\ndue to the variations in their capacities. Overall optimization goal is higher\ntask throughput and better utilization of the expensive dedicated links. The\nevaluation demonstrates that the algorithms are able to exploit the synergy of\nbi-modal communication links towards achieving the optimization goals. \n\n"}
{"id": "0904.0805", "contents": "Title: The (unfortunate) complexity of the economy Abstract: This article is a follow-up of a short essay that appeared in Nature 455,\n1181 (2008) [arXiv:0810.5306]. It has become increasingly clear that the\nerratic dynamics of markets is mostly endogenous and not due to the rational\nprocessing of exogenous news. I elaborate on the idea that spin-glass type of\nproblems, where the combination of competition and heterogeneities generically\nleads to long epochs of statis interrupted by crises and hyper-sensitivity to\nsmall changes of the environment, could be metaphors for the complexity of\neconomic systems. I argue that the most valuable contribution of physics to\neconomics might end up being of methodological nature, and that simple models\nfrom physics and agent based numerical simulations, although highly stylized,\nare more realistic than the traditional models of economics that assume\nrational agents with infinite foresight and infinite computing abilities. \n\n"}
{"id": "0904.2389", "contents": "Title: Extracting the multiscale backbone of complex weighted networks Abstract: A large number of complex systems find a natural abstraction in the form of\nweighted networks whose nodes represent the elements of the system and the\nweighted edges identify the presence of an interaction and its relative\nstrength. In recent years, the study of an increasing number of large scale\nnetworks has highlighted the statistical heterogeneity of their interaction\npattern, with degree and weight distributions which vary over many orders of\nmagnitude. These features, along with the large number of elements and links,\nmake the extraction of the truly relevant connections forming the network's\nbackbone a very challenging problem. More specifically, coarse-graining\napproaches and filtering techniques are at struggle with the multiscale nature\nof large scale systems. Here we define a filtering method that offers a\npractical procedure to extract the relevant connection backbone in complex\nmultiscale networks, preserving the edges that represent statistical\nsignificant deviations with respect to a null model for the local assignment of\nweights to edges. An important aspect of the method is that it does not\nbelittle small-scale interactions and operates at all scales defined by the\nweight distribution. We apply our method to real world network instances and\ncompare the obtained results with alternative backbone extraction techniques. \n\n"}
{"id": "0904.2716", "contents": "Title: Fast dynamics in Internet topology: preliminary observations and\n  explanations Abstract: By focusing on what can be observed by running traceroute-like measurements\nat a high frequency from a single monitor to a fixed destination set, we show\nthat the observed view of the topology is constantly evolving at a pace much\nhigher than expected. Repeated measurements discover new IP addresses at a\nconstant rate, for long period of times (up to several months). In order to\nprovide explanations, we study this phenomenon both at the IP, and at the\nAutonomous System levels. We show that this renewal of IP addresses is\npartially caused by a BGP routing dynamics, altering paths between existing\nASes. Furthermore, we conjecture that an intra AS routing dynamics is another\ncause of this phenomenon. \n\n"}
{"id": "0904.3004", "contents": "Title: Macroeconomic Phase Transitions Detected from the Dow Jones Industrial\n  Average Time Series Abstract: In this paper, we perform statistical segmentation and clustering analysis of\nthe Dow Jones Industrial Average time series between January 1997 and August\n2008. Modeling the index movements and log-index movements as stationary\nGaussian processes, we find a total of 116 and 119 statistically stationary\nsegments respectively. These can then be grouped into between five to seven\nclusters, each representing a different macroeconomic phase. The macroeconomic\nphases are distinguished primarily by their volatilities. We find the US\neconomy, as measured by the DJI, spends most of its time in a low-volatility\nphase and a high-volatility phase. The former can be roughly associated with\neconomic expansion, while the latter contains the economic contraction phase in\nthe standard economic cycle. Both phases are interrupted by a\nmoderate-volatility market, but extremely-high-volatility market crashes are\nfound mostly within the high-volatility phase. From the temporal distribution\nof various phases, we see a high-volatility phase from mid-1998 to mid-2003,\nand another starting mid-2007 (the current global financial crisis).\nTransitions from the low-volatility phase to the high-volatility phase are\npreceded by a series of precursor shocks, whereas the transition from the\nhigh-volatility phase to the low-volatility phase is preceded by a series of\ninverted shocks. The time scale for both types of transitions is about a year.\nWe also identify the July 1997 Asian Financial Crisis to be the trigger for the\nmid-1998 transition, and an unnamed May 2006 market event related to\ncorrections in the Chinese markets to be the trigger for the mid-2007\ntransition. \n\n"}
{"id": "0904.3222", "contents": "Title: Efficient Measurement of Complex Networks Using Link Queries Abstract: Complex networks are at the core of an intense research activity. However, in\nmost cases, intricate and costly measurement procedures are needed to explore\ntheir structure. In some cases, these measurements rely on link queries: given\ntwo nodes, it is possible to test the existence of a link between them. These\ntests may be costly, and thus minimizing their number while maximizing the\nnumber of discovered links is a key issue. This paper studies this problem: we\nobserve that properties classically observed on real-world complex networks\ngive hints for their efficient measurement; we derive simple principles and\nseveral measurement strategies based on this, and experimentally evaluate their\nefficiency on real-world cases. In order to do so, we introduce methods to\nevaluate the efficiency of strategies. We also explore the bias that different\nmeasurement strategies may induce. \n\n"}
{"id": "0905.0220", "contents": "Title: Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the\n  Financial and Economic Crisis Abstract: The financial crisis of 2008, which started with an initially well-defined\nepicenter focused on mortgage backed securities (MBS), has been cascading into\na global economic recession, whose increasing severity and uncertain duration\nhas led and is continuing to lead to massive losses and damage for billions of\npeople. Heavy central bank interventions and government spending programs have\nbeen launched worldwide and especially in the USA and Europe, with the hope to\nunfreeze credit and boltster consumption. Here, we present evidence and\narticulate a general framework that allows one to diagnose the fundamental\ncause of the unfolding financial and economic crisis: the accumulation of\nseveral bubbles and their interplay and mutual reinforcement has led to an\nillusion of a \"perpetual money machine\" allowing financial institutions to\nextract wealth from an unsustainable artificial process. Taking stock of this\ndiagnostic, we conclude that many of the interventions to address the so-called\nliquidity crisis and to encourage more consumption are ill-advised and even\ndangerous, given that precautionary reserves were not accumulated in the \"good\ntimes\" but that huge liabilities were. The most \"interesting\" present times\nconstitute unique opportunities but also great challenges, for which we offer a\nfew recommendations. \n\n"}
{"id": "0906.1899", "contents": "Title: Money Distributions in Chaotic Economies Abstract: This paper considers the ideal gas-like model of trading markets, where each\nindividual is identified as a gas molecule that interacts with others trading\nin elastic or money-conservative collisions. Traditionally this model\nintroduces different rules of random selection and exchange between pair\nagents. Real economic transactions are complex but obviously non-random.\nConsequently, unlike this traditional model, this work implements chaotic\nelements in the evolution of an economic system. In particular, we use a\nchaotic signal that breaks the natural pairing symmetry\n$(i,j)\\Leftrightarrow(j,i)$ of a random gas-like model. As a result of that, it\nis found that a chaotic market like this can reproduce the referenced wealth\ndistributions observed in real economies (the Gamma, Exponential and Pareto\ndistributions). \n\n"}
{"id": "0907.4468", "contents": "Title: What type of distribution for packet delay in a global network should be\n  used in the control theory? Abstract: In this paper correspondence between experimental data for packet delay and\ntwo theoretical types of distribution is investigated. Calculations have shown\nthat the exponential distribution describes the data on network delay better,\nthan truncated normal distribution. Precision experimental data to within\nmicroseconds are gathered by means of the RIPE Test Box. In addition to exact\nmeasurements the data gathered by means of the utility {\\em ping} has been\nparsed that has not changed the main result. As a result, the equation for an\nexponential distribution, in the best way describing process of packet delay in\na TCP/IP based network is written. The search algorithm for key parameters as\nfor normal, and an exponential distribution is resulted. \n\n"}
{"id": "0907.5402", "contents": "Title: Optimal Scheduling for Fair Resource Allocation in Ad Hoc Networks with\n  Elastic and Inelastic Traffic Abstract: This paper studies the problem of congestion control and scheduling in ad hoc\nwireless networks that have to support a mixture of best-effort and real-time\ntraffic. Optimization and stochastic network theory have been successful in\ndesigning architectures for fair resource allocation to meet long-term\nthroughput demands. However, to the best of our knowledge, strict packet delay\ndeadlines were not considered in this framework previously. In this paper, we\npropose a model for incorporating the quality of service (QoS) requirements of\npackets with deadlines in the optimization framework. The solution to the\nproblem results in a joint congestion control and scheduling algorithm which\nfairly allocates resources to meet the fairness objectives of both elastic and\ninelastic flows, and per-packet delay requirements of inelastic flows. \n\n"}
{"id": "0908.1273", "contents": "Title: A General Class of Throughput Optimal Routing Policies in Multi-hop\n  Wireless Networks Abstract: This paper considers the problem of throughput optimal routing/scheduling in\na multi-hop constrained queueing network with random connectivity whose special\ncase includes opportunistic multi-hop wireless networks and input-queued switch\nfabrics. The main challenge in the design of throughput optimal routing\npolicies is closely related to identifying appropriate and universal Lyapunov\nfunctions with negative expected drift. The few well-known throughput optimal\npolicies in the literature are constructed using simple quadratic or\nexponential Lyapunov functions of the queue backlogs and as such they seek to\nbalance the queue backlogs across network independent of the topology. By\nconsidering a class of continuous, differentiable, and piece-wise quadratic\nLyapunov functions, this paper provides a large class of throughput optimal\nrouting policies. The proposed class of Lyapunov functions allow for the\nrouting policy to control the traffic along short paths for a large portion of\nstate-space while ensuring a negative expected drift. This structure enables\nthe design of a large class of routing policies. In particular, and in addition\nto recovering the throughput optimality of the well known backpressure routing\npolicy, an opportunistic routing policy with congestion diversity is proved to\nbe throughput optimal. \n\n"}
{"id": "0909.3356", "contents": "Title: Capacity of Large-scale CSMA Wireless Networks Abstract: In the literature, asymptotic studies of multi-hop wireless network capacity\noften consider only centralized and deterministic TDMA (time-division\nmulti-access) coordination schemes. There have been fewer studies of the\nasymptotic capacity of large-scale wireless networks based on CSMA\n(carrier-sensing multi-access), which schedules transmissions in a distributed\nand random manner. With the rapid and widespread adoption of CSMA technology, a\ncritical question is that whether CSMA networks can be as scalable as TDMA\nnetworks. To answer this question and explore the capacity of CSMA networks, we\nfirst formulate the models of CSMA protocols to take into account the unique\nCSMA characteristics not captured by existing interference models in the\nliterature. These CSMA models determine the feasible states, and consequently\nthe capacity of CSMA networks. We then study the throughput efficiency of CSMA\nscheduling as compared to TDMA. Finally, we tune the CSMA parameters so as to\nmaximize the throughput to the optimal order. As a result, we show that CSMA\ncan achieve throughput as $\\Omega(\\frac{1}{\\sqrt{n}})$, the same order as\noptimal centralized TDMA, on uniform random networks. Our CSMA scheme makes use\nof an efficient backbone-peripheral routing scheme and a careful design of dual\ncarrier-sensing and dual channel scheme. We also address the implementation\nissues of our CSMA scheme. \n\n"}
{"id": "0909.3481", "contents": "Title: Planet-scale Human Mobility Measurement Abstract: Research into, and design and construction of mobile systems and algorithms\nrequires access to large-scale mobility data. Unfortunately, the wireless and\nmobile research community lacks such data. For instance, the largest available\nhuman contact traces contain only 100 nodes with very sparse connectivity,\nlimited by experimental logistics. In this paper we pose a challenge to the\ncommunity: how can we collect mobility data from billions of human\nparticipants? We re-assert the importance of large-scale datasets in\ncommunication network design, and claim that this could impact fundamental\nstudies in other academic disciplines. In effect, we argue that planet-scale\nmobility measurements can help to save the world. For example, through\nunderstanding large-scale human mobility, we can track and model and contain\nthe spread of epidemics of various kinds. \n\n"}
{"id": "0909.3984", "contents": "Title: Weighted Trade Network in a Model of Preferential Bipartite Transactions Abstract: Using a model of wealth distribution where traders are characterized by\nquenched random saving propensities and trade among themselves by bipartite\ntransactions, we mimic the enhanced rates of trading of the rich by introducing\nthe preferential selection rule using a pair of continuously tunable\nparameters. The bipartite trading defines a growing trade network of traders\nlinked by their mutual trade relationships. With the preferential selection\nrule this network appears to be highly heterogeneous characterized by the\nscale-free nodal degree and the link weight distributions and presents\nsignatures of non-trivial strength-degree correlations. With detailed numerical\nsimulations and using finite-size scaling analysis we present evidence that the\nassociated critical exponents are continuous functions of the tuning\nparameters. However the wealth distribution has been observed to follow the\nwell-known Pareto law robustly for all positive values of the tuning\nparameters. \n\n"}
{"id": "0910.0064", "contents": "Title: Eroding market stability by proliferation of financial instruments Abstract: We contrast Arbitrage Pricing Theory (APT), the theoretical basis for the\ndevelopment of financial instruments, with a dynamical picture of an\ninteracting market, in a simple setting. The proliferation of financial\ninstruments apparently provides more means for risk diversification, making the\nmarket more efficient and complete. In the simple market of interacting traders\ndiscussed here, the proliferation of financial instruments erodes systemic\nstability and it drives the market to a critical state characterized by large\nsusceptibility, strong fluctuations and enhanced correlations among risks. This\nsuggests that the hypothesis of APT may not be compatible with a stable market\ndynamics. In this perspective, market stability acquires the properties of a\ncommon good, which suggests that appropriate measures should be introduced in\nderivative markets, to preserve stability. \n\n"}
{"id": "0910.4704", "contents": "Title: Performance of Joint Spectrum Sensing and MAC Algorithms for\n  Multichannel Opportunistic Spectrum Access Ad Hoc Networks Abstract: We present an analytical framework to assess the link layer throughput of\nmultichannel Opportunistic Spectrum Access (OSA) ad hoc networks. Specifically,\nwe focus on analyzing various combinations of collaborative spectrum sensing\nand Medium Access Control (MAC) protocol abstractions. We decompose\ncollaborative spectrum sensing into layers, parametrize each layer, classify\nexisting solutions, and propose a new protocol called Truncated Time Division\nMultiple Access (TTDMA) that supports efficient distribution of sensing results\nin \"k out of N\" fusion rule. In case of multichannel MAC protocols we evaluate\ntwo main approaches of control channel design with (i) dedicated and (ii)\nhopping channel. We propose to augment these protocols with options of handling\nsecondary user (SU) connections preempted by primary user (PU) by (i)\nconnection buffering until PU departure and (ii) connection switching to a\nvacant PU channel. By comparing and optimizing different design combinations we\nshow that (i) it is generally better to buffer preempted SU connections than to\nswitch them to PU vacant channels and (ii) TTDMA is a promising design option\nfor collaborative spectrum sensing process when k does not change over time. \n\n"}
{"id": "0911.1972", "contents": "Title: People-Sensing Spatial Characteristics of RF Sensor Networks Abstract: An \"RF sensor\" network can monitor RSS values on links in the network and\nperform device-free localization, i.e., locating a person or object moving in\nthe area in which the network is deployed. This paper provides a statistical\nmodel for the RSS variance as a function of the person's position w.r.t. the\ntransmitter (TX) and receiver (RX). We show that the ensemble mean of the RSS\nvariance has an approximately linear relationship with the expected total\naffected power (ETAP). We then use analysis to derive approximate expressions\nfor the ETAP as a function of the person's position, for both scattering and\nreflection. Counterintuitively, we show that reflection, not scattering, causes\nthe RSS variance contours to be shaped like Cassini ovals. Experimental tests\nreported here and in past literature are shown to validate the analysis. \n\n"}
{"id": "0911.4763", "contents": "Title: Causal Links Between US Economic Sectors Abstract: In this paper, we perform a comparative segmentation and clustering analysis\nof the time series for the ten Dow Jones US economic sector indices between 14\nFebruary 2000 and 31 August 2008. From the temporal distributions of clustered\nsegments, we find that the US economy took one and a half years to recover from\nthe mid-1998-to-mid-2003 financial crisis, but only two months to completely\nenter the present financial crisis. We also find the oil & gas and basic\nmaterials sectors leading the recovery from the previous financial crisis,\nwhile the consumer goods and utilities sectors led the descent into the present\nfinancial crisis. On a macroscopic level, we find sectors going earlier into a\ncrisis emerge later from it, whereas sectors going later into the crisis emerge\nearlier. On the mesoscopic level, we find leading sectors experiencing stronger\nand longer volatility shocks, while trailing sectors experience weaker and\nshorter volatility shocks. In our shock-by-shock causal-link analysis, we also\nfind shorter delays between corresponding shocks in more closely related\neconomic sectors. In addition, our analysis reveals evidences for complex\nsectorial structures, as well as nonlinear amplification in the propagating\nvolatility shocks. From a perspective relevant to public policy, our study\nsuggests an endogeneous sectorial dynamics during the mid-2003 economic\nrecovery, in contrast to strong exogeneous driving by Federal Reserve interest\nrate cuts during the mid-2007 onset. Most interestingly, we find for the\nsequence of closely spaced interest rate cuts instituted in 2007/2008, the\nfirst few cuts effectively lowered market volatilities, while the next few cuts\ncounter-effectively increased market volatilities. Subsequent cuts evoked\nlittle response from the market. \n\n"}
{"id": "0911.5503", "contents": "Title: Finitely additive probabilities and the Fundamental Theorem of Asset\n  Pricing Abstract: This work aims at a deeper understanding of the mathematical implications of\nthe economically-sound condition of absence of arbitrages of the first kind in\na financial market. In the spirit of the Fundamental Theorem of Asset Pricing\n(FTAP), it is shown here that absence of arbitrages of the first kind in the\nmarket is equivalent to the existence of a finitely additive probability,\nweakly equivalent to the original and only locally countably additive, under\nwhich the discounted wealth processes become \"local martingales\". The\naforementioned result is then used to obtain an independent proof of the FTAP\nof Delbaen and Schachermayer. Finally, an elementary and short treatment of the\nprevious discussion is presented for the case of continuous-path semimartingale\nasset-price processes. \n\n"}
{"id": "0911.5667", "contents": "Title: End-to-End Algebraic Network Coding for Wireless TCP/IP Networks Abstract: The Transmission Control Protocol (TCP) was designed to provide reliable\ntransport services in wired networks. In such networks, packet losses mainly\noccur due to congestion. Hence, TCP was designed to apply congestion avoidance\ntechniques to cope with packet losses. Nowadays, TCP is also utilized in\nwireless networks where, besides congestion, numerous other reasons for packet\nlosses exist. This results in reduced throughput and increased transmission\nround-trip time when the state of the wireless channel is bad. We propose a new\nnetwork layer, that transparently sits below the transport layer and hides non\ncongestion-imposed packet losses from TCP. The network coding in this new layer\nis based on the well-known class of Maximum Distance Separable (MDS) codes. \n\n"}
{"id": "1001.1616", "contents": "Title: A Subjective and Probabilistic Approach to Derivatives Abstract: We propose a probabilistic framework for pricing derivatives, which\nacknowledges that information and beliefs are subjective. Market prices can be\ntranslated into implied probabilities. In particular, futures imply returns for\nthese implied probability distributions. We argue that volatility is not risk,\nbut uncertainty. Non-normal distributions combine the risk in the left tail\nwith the opportunities in the right tail -- unifying the \"risk premium\" with\nthe possible loss. Risk and reward must be part of the same picture and\nexpected returns must include possible losses due to risks. We reinterpret the\nBlack-Scholes pricing formulas as prices for maximum-entropy probability\ndistributions, illuminating their importance from a new angle. Using these\nideas we show how derivatives can be priced under \"uncertain uncertainty\" and\nhow this creates a skew for the implied volatilities. We argue that the current\nstandard approach based on stochastic modelling and risk-neutral pricing fails\nto account for subjectivity in markets and mistreats uncertainty as risk.\nFurthermore, it is founded on a questionable argument -- that uncertainty is\neliminated at all cost. \n\n"}
{"id": "1002.1070", "contents": "Title: The Lehman Brothers Effect and Bankruptcy Cascades Abstract: Inspired by the bankruptcy of Lehman Brothers and its consequences on the\nglobal financial system, we develop a simple model in which the Lehman default\nevent is quantified as having an almost immediate effect in worsening the\ncredit worthiness of all financial institutions in the economic network. In our\nstylized description, all properties of a given firm are captured by its\neffective credit rating, which follows a simple dynamics of co-evolution with\nthe credit ratings of the other firms in our economic network. The dynamics\nresembles the evolution of Potts spin-glass with external global field\ncorresponding to a panic effect in the economy. The existence of a global phase\ntransition, between paramagnetic and ferromagnetic phases, explains the large\nsusceptibility of the system to negative shocks. We show that bailing out the\nfirst few defaulting firms does not solve the problem, but does have the effect\nof alleviating considerably the global shock, as measured by the fraction of\nfirms that are not defaulting as a consequence. This beneficial effect is the\ncounterpart of the large vulnerability of the system of coupled firms, which\nare both the direct consequences of the collective self-organized endogenous\nbehaviors of the credit ratings of the firms in our economic network. \n\n"}
{"id": "1002.2385", "contents": "Title: Traffic Capacity of Large WDM Passive Optical Networks Abstract: As passive optical networks (PON) are increasingly deployed to provide high\nspeed Internet access, it is important to understand their fundamental traffic\ncapacity limits. The paper discusses performance models applicable to\nwavelength division multiplexing (WDM) EPONs and GPONs under the assumption\nthat users access the fibre via optical network units equipped with tunable\ntransmitters. The considered stochastic models are based on multiserver polling\nsystems for which explicit analytical results are not known. A large system\nasymptotic, mean-field approximation, is used to derive closed form solutions\nof these complex systems. Convergence of the mean field dynamics is proved in\nthe case of a simple network configuration. Simulation results show that, for a\nrealistic sized PON, the mean field approximation is accurate. \n\n"}
{"id": "1002.3689", "contents": "Title: Explicit equilibria in a kinetic model of gambling Abstract: We introduce and discuss a nonlinear kinetic equation of Boltzmann type which\ndescribes the evolution of wealth in a pure gambling process, where the entire\nsum of wealths of two agents is up for gambling, and randomly shared between\nthe agents. For this equation the analytical form of the steady states is found\nfor various realizations of the random fraction of the sum which is shared to\nthe agents. Among others, Gibbs distribution appears as steady state in case of\na uniformly distributed random fraction, while Gamma distribution appears for a\nrandom fraction which is Beta distributed. The case in which the gambling game\nis only conservative-in-the-mean is shown to lead to an explicit heavy tailed\ndistribution. \n\n"}
{"id": "1003.0793", "contents": "Title: Boolean delay equations on networks: An application to economic damage\n  propagation Abstract: We introduce economic models based on Boolean Delay Equations: this formalism\nmakes easier to take into account the complexity of the interactions between\nfirms and is particularly appropriate for studying the propagation of an\ninitial damage due to a catastrophe. Here we concentrate on simple cases, which\nallow to understand the effects of multiple concurrent production paths as well\nas the presence of stochasticity in the path time lengths or in the network\nstructure.\n  In absence of flexibility, the shortening of production of a single firm in\nan isolated network with multiple connections usually ends up by attaining a\nfinite fraction of the firms or the whole economy, whereas the interactions\nwith the outside allow a partial recovering of the activity, giving rise to\nperiodic solutions with waves of damage which propagate across the structure.\nThe damage propagation speed is strongly dependent upon the topology. The\nexistence of multiple concurrent production paths does not necessarily imply a\nslowing down of the propagation, which can be as fast as the shortest path. \n\n"}
{"id": "1003.5309", "contents": "Title: Gossip Algorithms for Distributed Signal Processing Abstract: Gossip algorithms are attractive for in-network processing in sensor networks\nbecause they do not require any specialized routing, there is no bottleneck or\nsingle point of failure, and they are robust to unreliable wireless network\nconditions. Recently, there has been a surge of activity in the computer\nscience, control, signal processing, and information theory communities,\ndeveloping faster and more robust gossip algorithms and deriving theoretical\nperformance guarantees. This article presents an overview of recent work in the\narea. We describe convergence rate results, which are related to the number of\ntransmitted messages and thus the amount of energy consumed in the network for\ngossiping. We discuss issues related to gossiping over wireless links,\nincluding the effects of quantization and noise, and we illustrate the use of\ngossip algorithms for canonical signal processing tasks including distributed\nestimation, source localization, and compression. \n\n"}
{"id": "1004.0844", "contents": "Title: Quantum Portfolios of Observables and the Risk Neutral Valuation Model Abstract: Quantum Portfolios of quantum algorithms encoded on qbits have recently been\nreported. In this paper a discussion of the continuous variables version of\nquantum portfolios is presented. A risk neutral valuation model for options\ndependent on the measured values of the observables, analogous to the\ntraditional Black-Scholes valuation model, is obtained from the underlying\nstochastic equations. The quantum algorithms are here encoded on simple\nharmonic oscillator (SHO) states, and a Fokker-Planck equation for the Glauber\nP-representation is obtained as a starting point for the analysis. A discussion\nof the observation of the polarization of a portfolio of qbits is also obtained\nand the resultant Fokker-Planck equation is used to obtain the risk neutral\nvaluation of the qbit polarization portfolio. \n\n"}
{"id": "1004.4438", "contents": "Title: A Survey on Network Codes for Distributed Storage Abstract: Distributed storage systems often introduce redundancy to increase\nreliability. When coding is used, the repair problem arises: if a node storing\nencoded information fails, in order to maintain the same level of reliability\nwe need to create encoded information at a new node. This amounts to a partial\nrecovery of the code, whereas conventional erasure coding focuses on the\ncomplete recovery of the information from a subset of encoded packets. The\nconsideration of the repair network traffic gives rise to new design\nchallenges. Recently, network coding techniques have been instrumental in\naddressing these challenges, establishing that maintenance bandwidth can be\nreduced by orders of magnitude compared to standard erasure codes. This paper\nprovides an overview of the research results on this topic. \n\n"}
{"id": "1005.0496", "contents": "Title: Stable-1/2 Bridges and Insurance Abstract: We develop a class of non-life reserving models using a stable-1/2 random\nbridge to simulate the accumulation of paid claims, allowing for an essentially\narbitrary choice of a priori distribution for the ultimate loss. Taking an\ninformation-based approach to the reserving problem, we derive the process of\nthe conditional distribution of the ultimate loss. The \"best-estimate ultimate\nloss process\" is given by the conditional expectation of the ultimate loss. We\nderive explicit expressions for the best-estimate ultimate loss process, and\nfor expected recoveries arising from aggregate excess-of-loss reinsurance\ntreaties. Use of a deterministic time change allows for the matching of any\ninitial (increasing) development pattern for the paid claims. We show that\nthese methods are well-suited to the modelling of claims where there is a\nnon-trivial probability of catastrophic loss. The generalized inverse-Gaussian\n(GIG) distribution is shown to be a natural choice for the a priori ultimate\nloss distribution. For particular GIG parameter choices, the best-estimate\nultimate loss process can be written as a rational function of the paid-claims\nprocess. We extend the model to include a second paid-claims process, and allow\nthe two processes to be dependent. The results obtained can be applied to the\nmodelling of multiple lines of business or multiple origin years. The\nmulti-dimensional model has the property that the dimensionality of\ncalculations remains low, regardless of the number of paid-claims processes. An\nalgorithm is provided for the simulation of the paid-claims processes. \n\n"}
{"id": "1005.4976", "contents": "Title: An empirical study of the tails of mutual fund size Abstract: The mutual fund industry manages about a quarter of the assets in the U.S.\nstock market and thus plays an important role in the U.S. economy. The question\nof how much control is concentrated in the hands of the largest players is best\nquantitatively discussed in terms of the tail behavior of the mutual fund size\ndistribution. We study the distribution empirically and show that the tail is\nmuch better described by a log-normal than a power law, indicating less\nconcentration than, for example, personal income. The results are highly\nstatistically significant and are consistent across fifteen years. This\ncontradicts a recent theory concerning the origin of the power law tails of the\ntrading volume distribution. Based on the analysis in a companion paper, the\nlog-normality is to be expected, and indicates that the distribution of mutual\nfunds remains perpetually out of equilibrium. \n\n"}
{"id": "1006.1956", "contents": "Title: A Semi-distributed Reputation Based Intrusion Detection System for\n  Mobile Adhoc Networks Abstract: A Mobile Adhoc Network (MANET) is a cooperative engagement of a collection of\nmobile nodes without any centralized access point or infrastructure to\ncoordinate among the peers. The underlying concept of coordination among nodes\nin a cooperative MANET has induced in them a vulnerability to attacks due to\nissues like lack of fixed infrastructure, dynamically changing network\ntopology, cooperative algorithms, lack of centralized monitoring and management\npoint, and lack of a clear line of defense. We propose a semi-distributed\napproach towards Reputation Based Intrusion Detection System (IDS) that\ncombines with the DSR routing protocol for strengthening the defense of a\nMANET. Our system inherits the features of reputation from human behavior,\nhence making the IDS socially inspired. It has a semi-distributed architecture\nas the critical observation results of the system are neither spread globally\nnor restricted locally. The system assigns maximum weightage to self\nobservation by nodes for updating any reputation values, thus avoiding the need\nof a trust relationship between nodes. Our system is also unique in the sense\nthat it features the concepts of Redemption and Fading with a robust Path\nManager and Monitor system. Simulation studies show that DSR fortified with our\nsystem outperforms normal DSR in terms of the packet delivery ratio and routing\noverhead even when up to half of nodes in the network behave as malicious.\nVarious parameters introduced such as timing window size, reputation update\nvalues, congestion parameter and other thresholds have been optimized over\nseveral simulation test runs of the system. By combining the semi-distributed\narchitecture and other design essentials like path manager, monitor module,\nredemption and fading concepts; Our system proves to be robust enough to\ncounter most common attacks in MANETs. \n\n"}
{"id": "1006.3708", "contents": "Title: Econophysics studies in Estonia Abstract: A short review of the econophysics research done in Estonia, devoted to the\n15th anniversary of the term \"econophysics\". \n\n"}
{"id": "1006.4228", "contents": "Title: Sustainable Throughput of Wireless LANs with Multi-Packet Reception\n  Capability under Bounded Delay-Moment Requirements Abstract: With the rapid proliferation of broadband wireless services, it is of\nparamount importance to understand how fast data can be sent through a wireless\nlocal area network (WLAN). Thanks to a large body of research following the\nseminal work of Bianchi, WLAN throughput under saturated traffic condition has\nbeen well understood. By contrast, prior investigations on throughput\nperformance under unsaturated traffic condition was largely based on\nphenomenological observations, which lead to a common misconception that WLAN\ncan support a traffic load as high as saturation throughput, if not higher,\nunder non-saturation condition. In this paper, we show through rigorous\nanalysis that this misconception may result in unacceptable quality of service:\nmean packet delay and delay jitter may approach infinity even when the traffic\nload is far below the saturation throughput. Hence, saturation throughput is\nnot a sound measure of WLAN capacity under non-saturation condition. To bridge\nthe gap, we define safe-bounded-mean-delay (SBMD) throughput and\nsafe-bounded-delay-jitter (SBDJ) throughput that reflect the actual network\ncapacity users can enjoy when they require finite mean delay and delay jitter,\nrespectively.\n  Our earlier work proved that in a WLAN with multi-packet reception (MPR)\ncapability, saturation throughput scales super-linearly with the MPR capability\nof the network. This paper extends the investigation to the non-saturation case\nand shows that super-linear scaling also holds for SBMD and SBDJ throughputs.\nOur results here complete the demonstration of MPR as a powerful\ncapacity-enhancement technique for WLAN under both saturation and\nnon-saturation conditions. \n\n"}
{"id": "1006.4595", "contents": "Title: Wealth Distributions in Asset Exchange Models Abstract: How do individuals accumulate wealth as they interact economically? We\noutline the consequences of a simple microscopic model in which repeated\npairwise exchanges of assets between individuals build the wealth distribution\nof a population. This distribution is determined for generic exchange rules ---\ntransactions that involve a fixed amount or a fixed fraction of individual\nwealth, as well as random or greedy exchanges. In greedy multiplicative\nexchange, a continuously evolving power law wealth distribution arises, a\nfeature that qualitatively mimics empirical observations. \n\n"}
{"id": "1006.5587", "contents": "Title: Econophysics on Real Economy -The First Decade of the Kyoto Econophysics\n  Group- Abstract: Research activities of Kyoto Econophysics Group is reviewed. Strong emphasis\nhas been placed on real economy. While the initial stage of research was a\nfirst high-definition data analysis on personal income, it soon progressed to\nfirm dynamics, growth rate distribution and establishment of Pareto's law and\nGibrat's law. It then led to analysis and simulation of firm dynamics on\neconomic network. Currently it covers a wide rage of dynamics of firms and\nfinancial institutions on complex network, using Japanese large-scale network\ndata, some of which are not available in other countries. Activities of this\ngroup for publicising and promoting understanding of econophysics is also\nreviewed. \n\n"}
{"id": "1007.5032", "contents": "Title: Approximation Algorithms for Secondary Spectrum Auctions Abstract: We study combinatorial auctions for the secondary spectrum market. In this\nmarket, short-term licenses shall be given to wireless nodes for communication\nin their local neighborhood. In contrast to the primary market, channels can be\nassigned to multiple bidders, provided that the corresponding devices are well\nseparated such that the interference is sufficiently low. Interference\nconflicts are described in terms of a conflict graph in which the nodes\nrepresent the bidders and the edges represent conflicts such that the feasible\nallocations for a channel correspond to the independent sets in the conflict\ngraph.\n  In this paper, we suggest a novel LP formulation for combinatorial auctions\nwith conflict graph using a non-standard graph parameter, the so-called\ninductive independence number. Taking into account this parameter enables us to\nbypass the well-known lower bound of \\Omega(n^{1-\\epsilon}) on the\napproximability of independent set in general graphs with n nodes (bidders). We\nachieve significantly better approximation results by showing that interference\nconstraints for wireless networks yield conflict graphs with bounded inductive\nindependence number.\n  Our framework covers various established models of wireless communication,\ne.g., the protocol or the physical model. For the protocol model, we achieve an\nO(\\sqrt{k})-approximation, where k is the number of available channels. For the\nmore realistic physical model, we achieve an O(\\sqrt{k} \\log^2 n) approximation\nbased on edge-weighted conflict graphs. Combining our approach with the the\nLP-based framework of Lavi and Swamy, we obtain incentive compatible mechanisms\nfor general bidders with arbitrary valuations on bundles of channels specified\nin terms of demand oracles. \n\n"}
{"id": "1007.5080", "contents": "Title: Analysis Framework for Opportunistic Spectrum OFDMA and its Application\n  to the IEEE 802.22 Standard Abstract: We present an analytical model that enables throughput evaluation of\nOpportunistic Spectrum Orthogonal Frequency Division Multiple Access (OS-OFDMA)\nnetworks. The core feature of the model, based on a discrete time Markov chain,\nis the consideration of different channel and subchannel allocation strategies\nunder different Primary and Secondary user types, traffic and priority levels.\nThe analytical model also assesses the impact of different spectrum sensing\nstrategies on the throughput of OS-OFDMA network. The analysis applies to the\nIEEE 802.22 standard, to evaluate the impact of two-stage spectrum sensing\nstrategy and varying temporal activity of wireless microphones on the IEEE\n802.22 throughput. Our study suggests that OS-OFDMA with subchannel notching\nand channel bonding could provide almost ten times higher throughput compared\nwith the design without those options, when the activity and density of\nwireless microphones is very high. Furthermore, we confirm that OS-OFDMA\nimplementation without subchannel notching, used in the IEEE 802.22, is able to\nsupport real-time and non-real-time quality of service classes, provided that\nwireless microphones temporal activity is moderate (with approximately one\nwireless microphone per 3,000 inhabitants with light urban population density\nand short duty cycles). Finally, two-stage spectrum sensing option improves\nOS-OFDMA throughput, provided that the length of spectrum sensing at every\nstage is optimized using our model. \n\n"}
{"id": "1008.0053", "contents": "Title: Achieving the Scaling Law of SNR-Monitoring in Dynamic Wireless Networks Abstract: The characteristics of wireless communication channels may vary with time due\nto fading, environmental changes and movement of mobile wireless devices.\nTracking and estimating channel gains of wireless channels is therefore a\nfundamentally important element of many wireless communication systems. In\nparticular, the receivers in many wireless networks need to estimate the\nchannel gains by means of a training sequence. This paper studies the scaling\nlaw (on the network size) of the overhead for channel gain monitoring in\nwireless network. We first investigate the scenario in which a receiver needs\nto track the channel gains with respect to multiple transmitters. To be\nconcrete, suppose that there are n transmitters, and that in the current round\nof channel-gain estimation, no more than k channels suffer significant\nvariations since the last round. We proves that \"\\Theta(k\\log((n+1)/k)) time\nslots\" is the minimum number of time slots needed to catch up with the k varied\nchannels. At the same time, we propose a novel channel-gain monitoring scheme\nnamed ADMOT to achieve the overhead lower-bound. ADMOT leverages recent\nadvances in compressive sensing in signal processing and interference\nprocessing in wireless communication, to enable the receiver to estimate all n\nchannels in a reliable and computationally efficient manner within\nO(k\\log((n+1)/k)) time slots. To our best knowledge, all previous\nchannel-tracking schemes require \\Theta(n) time slots regardless of k. Note\nthat based on above results for single receiver scenario, the scaling law of\ngeneral setting is achieved in which there are multiple transmitters, relay\nnodes and receivers. \n\n"}
{"id": "1008.0060", "contents": "Title: Belief Propagation Methods for Intercell Interference Coordination Abstract: We consider a broad class of interference coordination and resource\nallocation problems for wireless links where the goal is to maximize the sum of\nfunctions of individual link rates. Such problems arise in the context of, for\nexample, fractional frequency reuse (FFR) for macro-cellular networks and\ndynamic interference management in femtocells. The resulting optimization\nproblems are typically hard to solve optimally even using centralized\nalgorithms but are an essential computational step in implementing rate-fair\nand queue stabilizing scheduling policies in wireless networks. We consider a\nbelief propagation framework to solve such problems approximately. In\nparticular, we construct approximations to the belief propagation iterations to\nobtain computationally simple and distributed algorithms with low communication\noverhead. Notably, our methods are very general and apply to, for example, the\noptimization of transmit powers, transmit beamforming vectors, and sub-band\nallocation to maximize the above objective. Numerical results for femtocell\ndeployments demonstrate that such algorithms compute a very good operating\npoint in typically just a couple of iterations. \n\n"}
{"id": "1008.0227", "contents": "Title: Fast Mixing of Parallel Glauber Dynamics and Low-Delay CSMA Scheduling Abstract: Glauber dynamics is a powerful tool to generate randomized, approximate\nsolutions to combinatorially difficult problems. It has been used to analyze\nand design distributed CSMA (Carrier Sense Multiple Access) scheduling\nalgorithms for multi-hop wireless networks. In this paper we derive bounds on\nthe mixing time of a generalization of Glauber dynamics where multiple links\nare allowed to update their states in parallel and the fugacity of each link\ncan be different. The results can be used to prove that the average queue\nlength (and hence, the delay) under the parallel Glauber dynamics based CSMA\ngrows polynomially in the number of links for wireless networks with\nbounded-degree interference graphs when the arrival rate lies in a fraction of\nthe capacity region. We also show that in specific network topologies, the\nlow-delay capacity region can be further improved. \n\n"}
{"id": "1008.0919", "contents": "Title: Compressive Sensing over Graphs Abstract: In this paper, motivated by network inference and tomography applications, we\nstudy the problem of compressive sensing for sparse signal vectors over graphs.\nIn particular, we are interested in recovering sparse vectors representing the\nproperties of the edges from a graph. Unlike existing compressive sensing\nresults, the collective additive measurements we are allowed to take must\nfollow connected paths over the underlying graph. For a sufficiently connected\ngraph with $n$ nodes, it is shown that, using $O(k \\log(n))$ path measurements,\nwe are able to recover any $k$-sparse link vector (with no more than $k$\nnonzero elements), even though the measurements have to follow the graph path\nconstraints. We further show that the computationally efficient $\\ell_1$\nminimization can provide theoretical guarantees for inferring such $k$-sparse\nvectors with $O(k \\log(n))$ path measurements from the graph. \n\n"}
{"id": "1008.2249", "contents": "Title: A Novel Association Policy for Web Browsing in a Multirate WLAN Abstract: We obtain an association policy for STAs in an IEEE 802.11 WLAN by taking\ninto account explicitly two aspects of practical importance: (a) TCP-controlled\nshort file downloads interspersed with read times (motivated by web browsing),\nand (b) different STAs associated with an AP at possibly different rates\n(depending on distance from the AP). Our approach is based on two steps. First,\nwe consider an analytical model to obtain the aggregate AP throughput for long\nTCP-controlled file downloads when STAs are associated at k different rates r1,\nr2, : : :, rk; this extends earlier work in the literature. Second, we present\na 2-node closed queueing network model to approximate the expected\naverage-sized file download time for a user who shares the AP with other users\nassociated at a multiplicity of rates. These analytical results motivate the\nproposed association policy, called the Estimated Delay based Association (EDA)\npolicy: Associate with the AP at which the expected file download time is the\nleast. Simulations indicate that for a web-browsing type traffic scenario, EDA\noutperforms other policies that have been proposed earlier; the extent of\nimprovement ranges from 12.8% to 46.4% for a 9-AP network. To the best of our\nknowledge, this is the first work that proposes an association policy tailored\nspecifically for web browsing. Apart from this, our analytical results could be\nof independent interest \n\n"}
{"id": "1008.2565", "contents": "Title: Multigraph Sampling of Online Social Networks Abstract: State-of-the-art techniques for probability sampling of users of online\nsocial networks (OSNs) are based on random walks on a single social relation\n(typically friendship). While powerful, these methods rely on the social graph\nbeing fully connected. Furthermore, the mixing time of the sampling process\nstrongly depends on the characteristics of this graph. In this paper, we\nobserve that there often exist other relations between OSN users, such as\nmembership in the same group or participation in the same event. We propose to\nexploit the graphs these relations induce, by performing a random walk on their\nunion multigraph. We design a computationally efficient way to perform\nmultigraph sampling by randomly selecting the graph on which to walk at each\niteration. We demonstrate the benefits of our approach through (i) simulation\nin synthetic graphs, and (ii) measurements of Last.fm - an Internet website for\nmusic with social networking features. More specifically, we show that\nmultigraph sampling can obtain a representative sample and faster convergence,\neven when the individual graphs fail, i.e., are disconnected or highly\nclustered. \n\n"}
{"id": "1008.3705", "contents": "Title: Techniques for Enhanced Physical-Layer Security Abstract: Information-theoretic security--widely accepted as the strictest notion of\nsecurity--relies on channel coding techniques that exploit the inherent\nrandomness of propagation channels to strengthen the security of communications\nsystems. Within this paradigm, we explore strategies to improve secure\nconnectivity in a wireless network. We first consider the intrinsically secure\ncommunications graph (iS-graph), a convenient representation of the links that\ncan be established with information-theoretic security on a large-scale\nnetwork. We then propose and characterize two techniques--sectorized\ntransmission and eavesdropper neutralization--which are shown to dramatically\nenhance the connectivity of the iS-graph. \n\n"}
{"id": "1008.4161", "contents": "Title: Percolation and Connectivity in the Intrinsically Secure Communications\n  Graph Abstract: The ability to exchange secret information is critical to many commercial,\ngovernmental, and military networks. The intrinsically secure communications\ngraph (iS-graph) is a random graph which describes the connections that can be\nsecurely established over a large-scale network, by exploiting the physical\nproperties of the wireless medium. This paper aims to characterize the global\nproperties of the iS-graph in terms of: (i) percolation on the infinite plane,\nand (ii) full connectivity on a finite region. First, for the Poisson iS-graph\ndefined on the infinite plane, the existence of a phase transition is proven,\nwhereby an unbounded component of connected nodes suddenly arises as the\ndensity of legitimate nodes is increased. This shows that long-range secure\ncommunication is still possible in the presence of eavesdroppers. Second, full\nconnectivity on a finite region of the Poisson iS-graph is considered. The\nexact asymptotic behavior of full connectivity in the limit of a large density\nof legitimate nodes is characterized. Then, simple, explicit expressions are\nderived in order to closely approximate the probability of full connectivity\nfor a finite density of legitimate nodes. The results help clarify how the\npresence of eavesdroppers can compromise long-range secure communication. \n\n"}
{"id": "1009.0619", "contents": "Title: Field Reconstruction in Sensor Networks with Coverage Holes and Packet\n  Losses Abstract: Environmental monitoring is often performed through a wireless sensor\nnetwork, whose nodes are randomly deployed over the geographical region of\ninterest. Sensors sample a physical phenomenon (the so-called field) and send\ntheir measurements to a {\\em sink} node, which is in charge of reconstructing\nthe field from such irregular samples. In this work, we focus on scenarios of\npractical interest where the sensor deployment is unfeasible in certain areas\nof the geographical region, e.g., due to terrain asperities, and the delivery\nof sensor measurements to the sink may fail due to fading or to transmission\ncollisions among sensors simultaneously accessing the wireless medium. Under\nthese conditions, we carry out an asymptotic analysis and evaluate the quality\nof the estimation of a d-dimensional field when the sink uses linear filtering\nas a reconstruction technique. Specifically, given the matrix representing the\nsampling system, V, we derive both the moments and an expression of the\nlimiting spectral distribution of VV*, as the size of V goes to infinity and\nits aspect ratio has a finite limit bounded away from zero. By using such\nasymptotic results, we approximate the mean square error on the estimated field\nthrough the eta-transform of VV*, and derive the sensor network performance\nunder the conditions described above. \n\n"}
{"id": "1009.0932", "contents": "Title: On the Multi-Dimensional Controller and Stopper Games Abstract: We consider a zero-sum stochastic differential controller-and-stopper game in\nwhich the state process is a controlled diffusion evolving in a\nmulti-dimensional Euclidean space. In this game, the controller affects both\nthe drift and the volatility terms of the state process. Under appropriate\nconditions, we show that the game has a value and the value function is the\nunique viscosity solution to an obstacle problem for a Hamilton-Jacobi-Bellman\nequation. \n\n"}
{"id": "1009.3479", "contents": "Title: Incomplete Continuous-time Securities Markets with Stochastic Income\n  Volatility Abstract: In an incomplete continuous-time securities market with uncertainty generated\nby Brownian motions, we derive closed-form solutions for the equilibrium\ninterest rate and market price of risk processes. The economy has a finite\nnumber of heterogeneous exponential utility investors, who receive partially\nunspanned income and can trade continuously on a finite time-interval in a\nmoney market account and a single risky security. Besides establishing the\nexistence of an equilibrium, our main result shows that if the investors'\nunspanned income has stochastic countercyclical volatility, the resulting\nequilibrium can display both lower interest rates and higher risk premia\ncompared to the Pareto efficient equilibrium in an otherwise identical complete\nmarket. \n\n"}
{"id": "1009.3522", "contents": "Title: Open, Closed, and Shared Access Femtocells in the Downlink Abstract: A fundamental choice in femtocell deployments is the set of users which are\nallowed to access each femtocell. Closed access restricts the set to\nspecifically registered users, while open access allows any mobile subscriber\nto use any femtocell. Which one is preferable depends strongly on the distance\nbetween the macrocell base station (MBS) and femtocell. The main results of the\npaper are lemmas which provide expressions for the SINR distribution for\nvarious zones within a cell as a function of this MBS-femto distance. The\naverage sum throughput (or any other SINR-based metric) of home users and\ncellular users under open and closed access can be readily determined from\nthese expressions. We show that unlike in the uplink, the interests of home and\ncellular users are in conflict, with home users preferring closed access and\ncellular users preferring open access. The conflict is most pronounced for\nfemtocells near the cell edge, when there are many cellular users and fewer\nfemtocells. To mitigate this conflict, we propose a middle way which we term\nshared access in which femtocells allocate an adjustable number of time-slots\nbetween home and cellular users such that a specified minimum rate for each can\nbe achieved. The optimal such sharing fraction is derived. Analysis shows that\nshared access achieves at least the overall throughput of open access while\nalso satisfying rate requirements, while closed access fails for cellular users\nand open access fails for the home user. \n\n"}
{"id": "1009.4798", "contents": "Title: Role of feedback and broadcasting in the naming game Abstract: The naming game (NG) describes the agreement dynamics of a population of\nagents that interact locally in a pairwise fashion, and in recent years\nstatistical physics tools and techniques have greatly contributed to shed light\non its rich phenomenology. Here we investigate in details the role played by\nthe way in which the two agents update their states after an interaction. We\nshow that slightly modifying the NG rules in terms of which agent performs the\nupdate in given circumstances (i.e. after a success) can either alter\ndramatically the overall dynamics or leave it qualitatively unchanged. We\nunderstand analytically the first case by casting the model in the broader\nframework of a generalized NG. As for the second case, on the other hand, we\nnote that the modified rule reproducing the main features of the usual NG\ncorresponds in fact to a simplification of it consisting in the elimination of\nfeedback between the agents. This allows us to introduce and study a very\nnatural broadcasting scheme on networks that can be potentially relevant for\ndifferent applications, such as the design and implementation of autonomous\nsensor networks, as pointed out in the recent literature. \n\n"}
{"id": "1010.0476", "contents": "Title: Interference Alignment as a Rank Constrained Rank Minimization Abstract: We show that the maximization of the sum degrees-of-freedom for the static\nflat-fading multiple-input multiple-output (MIMO) interference channel is\nequivalent to a rank constrained rank minimization problem (RCRM), when the\nsignal spaces span all available dimensions. The rank minimization corresponds\nto maximizing interference alignment (IA) so that interference spans the lowest\ndimensional subspace possible. The rank constraints account for the useful\nsignal spaces spanning all available spatial dimensions. That way, we\nreformulate all IA requirements to requirements involving ranks. Then, we\npresent a convex relaxation of the RCRM problem inspired by recent results in\ncompressed sensing and low-rank matrix completion theory that rely on\napproximating rank with the nuclear norm. We show that the convex envelope of\nthe sum of ranks of the interference matrices is the normalized sum of their\ncorresponding nuclear norms and introduce tractable constraints that are\nasymptotically equivalent to the rank constraints for the initial problem. We\nalso show that our heuristic relaxation can be tuned for the multi-cell\ninterference channel. Furthermore, we experimentally show that in many cases\nthe proposed algorithm attains perfect interference alignment and in some cases\noutperforms previous approaches for finding precoding and zero-forcing matrices\nfor interference alignment. \n\n"}
{"id": "1010.0485", "contents": "Title: Distributed Storage Codes Meet Multiple-Access Wiretap Channels Abstract: We consider {\\it i)} the overhead minimization of maximum-distance separable\n(MDS) storage codes for the repair of a single failed node and {\\it ii)} the\ntotal secure degrees-of-freedom (S-DoF) maximization in a multiple-access\ncompound wiretap channel. We show that the two problems are connected.\nSpecifically, the overhead minimization for a single node failure of an {\\it\noptimal} MDS code, i.e. one that can achieve the information theoretic overhead\nminimum, is equivalent to maximizing the S-DoF in a multiple-access compound\nwiretap channel. Additionally, we show that maximizing the S-DoF in a\nmultiple-access compound wiretap channel is equivalent to minimizing the\noverhead of an MDS code for the repair of a departed node. An optimal MDS code\nmaps to a full S-DoF channel and a full S-DoF channel maps to an MDS code with\nminimum repair overhead for one failed node. We also state a general framework\nfor code-to-channel and channel-to-code mappings and performance bounds between\nthe two settings. The underlying theme for all connections presented is\ninterference alignment (IA). The connections between the two problems become\napparent when we restate IA as an optimization problem. Specifically, we\nformulate the overhead minimization and the S-DoF maximization as rank\nconstrained, sum-rank and max-rank minimization problems respectively. The\nderived connections allow us to map repair strategies of recently discovered\nrepair codes to beamforming matrices and characterize the maximum S-DoF for the\nsingle antenna multiple-access compound wiretap channel. \n\n"}
{"id": "1010.4249", "contents": "Title: Wireless Capacity with Oblivious Power in General Metrics Abstract: The capacity of a wireless network is the maximum possible amount of\nsimultaneous communication, taking interference into account.\n  Formally, we treat the following problem. Given is a set of links, each a\nsender-receiver pair located in a metric space, and an assignment of power to\nthe senders. We seek a maximum subset of links that are feasible in the SINR\nmodel: namely, the signal received on each link should be larger than the sum\nof the interferences from the other links. We give a constant-factor\napproximation that holds for any length-monotone, sub-linear power assignment\nand any distance metric.\n  We use this to give essentially tight characterizations of capacity\nmaximization under power control using oblivious power assignments.\nSpecifically, we show that the mean power assignment is optimal for capacity\nmaximization of bi-directional links, and give a tight $\\theta(\\log\nn)$-approximation of scheduling bi-directional links with power control using\noblivious power. For uni-directional links we give a nearly optimal $O(\\log n +\n\\log \\log \\Delta)$-approximation to the power control problem using mean power,\nwhere $\\Delta$ is the ratio of longest and shortest links. Combined, these\nresults clarify significantly the centralized complexity of wireless\ncommunication problems. \n\n"}
{"id": "1010.4858", "contents": "Title: S-MATE: Secure Coding-based Multipath Adaptive Traffic Engineering Abstract: There have been several approaches to provisioning traffic between core\nnetwork nodes in Internet Service Provider (ISP) networks. Such approaches aim\nto minimize network delay, increase network capacity, and enhance network\nsecurity services. MATE (Multipath Adaptive Traffic Engineering) protocol has\nbeen proposed for multipath adaptive traffic engineering between an ingress\nnode (source) and an egress node (destination). Its novel idea is to avoid\nnetwork congestion and attacks that might exist in edge and node disjoint paths\nbetween two core network nodes.\n  This paper builds an adaptive, robust, and reliable traffic engineering\nscheme for better performance of communication network operations. This will\nalso provision quality of service (QoS) and protection of traffic engineering\nto maximize network efficiency. Specifically, we present a new approach, S-MATE\n(secure MATE) is developed to protect the network traffic between two core\nnodes (routers or switches) in a cloud network. S-MATE secures against a single\nlink attack/failure by adding redundancy in one of the operational paths\nbetween the sender and receiver. The proposed scheme can be built to secure\ncore networks such as optical and IP networks. \n\n"}
{"id": "1010.4920", "contents": "Title: Jointly Optimal Channel Pairing and Power Allocation for Multichannel\n  Multihop Relaying Abstract: We study the problem of channel pairing and power allocation in a\nmultichannel multihop relay network to enhance the end-to-end data rate. Both\namplify-and-forward (AF) and decode-and-forward (DF) relaying strategies are\nconsidered. Given fixed power allocation to the channels, we show that channel\npairing over multiple hops can be decomposed into independent pairing problems\nat each relay, and a sorted-SNR channel pairing strategy is sum-rate optimal,\nwhere each relay pairs its incoming and outgoing channels by their SNR order.\nFor the joint optimization of channel pairing and power allocation under both\ntotal and individual power constraints, we show that the problem can be\ndecoupled into two subproblems solved separately. This separation principle is\nestablished by observing the equivalence between sorting SNRs and sorting\nchannel gains in the jointly optimal solution. It significantly reduces the\ncomputational complexity in finding the jointly optimal solution. It follows\nthat the channel pairing problem in joint optimization can be again decomposed\ninto independent pairing problems at each relay based on sorted channel gains.\nThe solution for optimizing power allocation for DF relaying is also provided,\nas well as an asymptotically optimal solution for AF relaying. Numerical\nresults are provided to demonstrate substantial performance gain of the jointly\noptimal solution over some suboptimal alternatives. It is also observed that\nmore gain is obtained from optimal channel pairing than optimal power\nallocation through judiciously exploiting the variation among multiple\nchannels. Impact of the variation of channel gain, the number of channels, and\nthe number of hops on the performance gain is also studied through numerical\nexamples. \n\n"}
{"id": "1010.5648", "contents": "Title: The additive property of the inconsistency degree in intertemporal\n  decision making through the generalization of psychophysical laws Abstract: Intertemporal decision making involves choices among options whose effects\noccur at different moments. These choices are influenced not only by the effect\nof rewards value perception at different moments, but also by the time\nperception effect. One of the main difficulties that affect standard\nexperiments involving intertemporal choices is the simultaneity of both effects\non time discounting. In this paper, we unify the psycophysical laws and\ndiscount value functions using the one-parameter exponential and logaritmic\nfunctions from nonextensive statistical mechanics. Also, we propose to measure\nthe degree of inconsistency. This quantity allow us to discriminate both\neffects of time and value perception on discounting process and, by\nintegration, obtain other main quantities like impulsivity and discount\nfunctions. \n\n"}
{"id": "1011.1892", "contents": "Title: Pushing BitTorrent Locality to the Limit Abstract: Peer-to-peer (P2P) locality has recently raised a lot of interest in the\ncommunity. Indeed, whereas P2P content distribution enables financial savings\nfor the content providers, it dramatically increases the traffic on inter-ISP\nlinks. To solve this issue, the idea to keep a fraction of the P2P traffic\nlocal to each ISP was introduced a few years ago. Since then, P2P solutions\nexploiting locality have been introduced. However, several fundamental issues\non locality still need to be explored. In particular, how far can we push\nlocality, and what is, at the scale of the Internet, the reduction of traffic\nthat can be achieved with locality? In this paper, we perform extensive\nexperiments on a controlled environment with up to 10,000 BitTorrent clients to\nevaluate the impact of high locality on inter-ISP links traffic and peers\ndownload completion time. We introduce two simple mechanisms that make high\nlocality possible in challenging scenarios and we show that we save up to\nseveral orders of magnitude inter-ISP traffic compared to traditional locality\nwithout adversely impacting peers download completion time. In addition, we\ncrawled 214,443 torrents representing 6,113,224 unique peers spread among 9,605\nASes. We show that whereas the torrents we crawled generated 11.6 petabytes of\ninter-ISP traffic, our locality policy implemented for all torrents could have\nreduced the global inter-ISP traffic by up to 40%. \n\n"}
{"id": "1011.2835", "contents": "Title: Approximately Optimal Wireless Broadcasting Abstract: We study a wireless broadcast network, where a single source reliably\ncommunicates independent messages to multiple destinations, with the aid of\nrelays and cooperation between destinations. The wireless nature of the medium\nis captured by the broadcast nature of transmissions as well as the\nsuperposition of all transmit signals plus independent Gaussian noise at the\nreceived signal at any radio. We propose a scheme that can achieve rate tuples\nwithin a constant gap away from the cut-set bound, where the constant is\nindependent of channel coefficients and power constraints.\n  The proposed scheme operates in two steps. The inner code, in which the\nrelays perform a quantize-and-encode operation, is constructed by lifting a\nscheme designed for a corresponding discrete superposition network. The outer\ncode is a Marton code for the non-Gaussian vector broadcast channel induced by\nthe relaying scheme, and is constructed by adopting a ``receiver-centric''\nviewpoint. \n\n"}
{"id": "1012.0349", "contents": "Title: Limit Order Books Abstract: Limit order books (LOBs) match buyers and sellers in more than half of the\nworld's financial markets. This survey highlights the insights that have\nemerged from the wealth of empirical and theoretical studies of LOBs. We\nexamine the findings reported by statistical analyses of historical LOB data\nand discuss how several LOB models provide insight into certain aspects of the\nmechanism. We also illustrate that many such models poorly resemble real LOBs\nand that several well-established empirical facts have yet to be reproduced\nsatisfactorily. Finally, we identify several key unresolved questions about\nLOBs. \n\n"}
{"id": "1012.4446", "contents": "Title: Fundamental and Real-World Challenges in Economics Abstract: In the same way as the Hilbert Program was a response to the foundational\ncrisis of mathematics, this article tries to formulate a research program for\nthe socio-economic sciences. The aim of this contribution is to stimulate\nresearch in order to close serious knowledge gaps in mainstream economics that\nthe recent financial and economic crisis has revealed. By identifying weak\npoints of conventional approaches in economics, we identify the scientific\nproblems which need to be addressed. We expect that solving these questions\nwill bring scientists in a position to give better decision support and policy\nadvice. We also indicate, what kinds of insights can be contributed by\nscientists from other research fields such as physics, biology, computer and\nsocial science. In order to make a quick progress and gain a systemic\nunderstanding of the whole interconnected socio-economic-environmental system,\nusing the data, information and computer systems available today and in the\nnear future, we suggest a multi-disciplinary collaboration as most promising\nresearch approach. \n\n"}
{"id": "1012.4815", "contents": "Title: Analytical Modeling of Saturation Throughput in Power Save Mode of an\n  IEEE 802.11 Infrastructure WLAN Abstract: We consider a single station (STA) in the Power Save Mode (PSM) of an IEEE\n802.11 infrastructure WLAN. This STA is assumed to be carrying uplink and\ndownlink traffic via the access point (AP). We assume that the transmission\nqueues of the AP and the STA are saturated, i.e., the AP and the STA always\nhave at least one packet to send. For this scenario, it is observed that uplink\nand downlink throughputs achieved are different. The reason behind the\ndifference is the long term attempt rates of the STA and the AP due to the PSM\nprotocol. In this paper we first obtain the the long term attempt rates of the\nSTA and the AP and using these, we obtain the saturation throughputs of the AP\nand the STA. We provide a validation of analytical results using the NS-2\nsimulator. \n\n"}
{"id": "1012.4909", "contents": "Title: Connectivity statistics of store-and-forward inter-vehicle communication Abstract: Inter-vehicle communication (IVC) enables vehicles to exchange messages\nwithin a limited broadcast range and thus self-organize into dynamical\nvehicular ad hoc networks. For the foreseeable future, however, a direct\nconnectivity between equipped vehicles in one direction is rarely possible. We\ntherefore investigate an alternative mode in which messages are stored by relay\nvehicles traveling in the opposite direction, and forwarded to vehicles in the\noriginal direction at a later time. The wireless communication consists of two\n`transversal' message hops across driving directions. Since direct connectivity\nfor transversal hops and a successful message transmission to vehicles in the\ndestination region is only a matter of time, the quality of this IVC strategy\ncan be described in terms of the distribution function for the total\ntransmission time. Assuming a Poissonian distance distribution between equipped\nvehicles, we derive analytical probability distributions for message\ntransmission times and related propagation speeds for a deterministic and a\nstochastic model of the maximum range of direct communication. By means of\nintegrated microscopic simulations of communication and bi-directional traffic\nflows, we validated the theoretical expectation for multi-lane roadways. We\nfound little deviation of the analytical result for multi-lane scenarios, but\nsignificant deviations for a single-lane. This can be explained by vehicle\nplatooning. We demonstrate the efficiency of the transverse hopping mechanism\nfor a congestion-warning application in a microscopic traffic simulation\nscenario. Messages are created on an event-driven basis by equipped vehicles\nentering and leaving a traffic jam. This application is operative for\npenetration levels as low as 1%. \n\n"}
{"id": "1012.5896", "contents": "Title: Punctuated Equilibrium and Power Law in Economic Dynamics Abstract: An interesting toy model has recently been proposed on Schumpeterian economic\ndynamics by Thurner {\\it et al.} following the idea of economist Joseph\nSchumpeter. Punctuated equilibrium dynamics is shown to emerge from this model\nand some detail analyses of the time series indicate SOC kind of behaviours.\nThe focus in the present work is to toss the idea whether the dynamics can\nreally be like a self organized critical (SOC) type. This study indicates that\nit is necessary to incorporate the concepts of 'fitness' and 'selection' in\nsuch a model in the line of the biological evolutionary model by Bak and\nSneppen in order to obtain power law and thus SOC behaviour. \n\n"}
{"id": "1101.3617", "contents": "Title: An almost linear stochastic map related to the particle system models of\n  social sciences Abstract: We propose a stochastic map model of economic dynamics. In the last decade,\nan array of observations in economics has been investigated in the econophysics\nliterature, a major example being the universal features of inequality in terms\nof income and wealth. Another area of inquiry is the formation of opinion in a\nsociety. The proposed model attempts to produce positively skewed distributions\nand the power law distributions as has been observed in the real data of income\nand wealth. Also, it shows a non-trivial phase transition in the opinion of a\nsociety (opinion formation). A number of physical models also generates similar\nresults. In particular, the kinetic exchange models have been especially\nsuccessful in this regard. Therefore, we compare the results obtained from\nthese two approaches and discuss a number of new features and drawbacks of this\nmodel. \n\n"}
{"id": "1101.4548", "contents": "Title: Leverage efficiency Abstract: Peters (2011a) defined an optimal leverage which maximizes the time-average\ngrowth rate of an investment held at constant leverage. It was hypothesized\nthat this optimal leverage is attracted to 1, such that, e.g., leveraging an\ninvestment in the market portfolio cannot yield long-term outperformance. This\nplaces a strong constraint on the stochastic properties of prices of traded\nassets, which we call \"leverage efficiency.\" Market conditions that deviate\nfrom leverage efficiency are unstable and may create leverage-driven bubbles.\nHere we expand on the hypothesis and its implications. These include a theory\nof noise that explains how systemic stability rules out smooth price changes at\nany pricing frequency; a resolution of the so-called equity premium puzzle; a\nprotocol for central bank interest rate setting to avoid leverage-driven price\ninstabilities; and a method for detecting fraudulent investment schemes by\nexploiting differences between the stochastic properties of their prices and\nthose of legitimately-traded assets. To submit the hypothesis to a rigorous\ntest we choose price data from different assets: the S&P500 index, Bitcoin,\nBerkshire Hathaway Inc., and Bernard L. Madoff Investment Securities LLC.\nAnalysis of these data supports the hypothesis. \n\n"}
{"id": "1101.6016", "contents": "Title: Let Cognitive Radios Imitate: Imitation-based Spectrum Access for\n  Cognitive Radio Networks Abstract: In this paper, we tackle the problem of opportunistic spectrum access in\nlarge-scale cognitive radio networks, where the unlicensed Secondary Users (SU)\naccess the frequency channels partially occupied by the licensed Primary Users\n(PU). Each channel is characterized by an availability probability unknown to\nthe SUs. We apply evolutionary game theory to model the spectrum access problem\nand develop distributed spectrum access policies based on imitation, a behavior\nrule widely applied in human societies consisting of imitating successful\nbehavior. We first develop two imitation-based spectrum access policies based\non the basic Proportional Imitation (PI) rule and the more advanced Double\nImitation (DI) rule given that a SU can imitate any other SUs. We then adapt\nthe proposed policies to a more practical scenario where a SU can only imitate\nthe other SUs operating on the same channel. A systematic theoretical analysis\nis presented for both scenarios on the induced imitation dynamics and the\nconvergence properties of the proposed policies to an imitation-stable\nequilibrium, which is also the $\\epsilon$-optimum of the system. Simple,\nnatural and incentive-compatible, the proposed imitation-based spectrum access\npolicies can be implemented distributedly based on solely local interactions\nand thus is especially suited in decentralized adaptive learning environments\nas cognitive radio networks. \n\n"}
{"id": "1102.2785", "contents": "Title: Minimizing interference in ad-hoc networks with bounded communication\n  radius Abstract: We consider a topology control problem in which we are given a set of $n$\nsensors in the plane and we would like to assign a communication radius to each\nof them. The radii assignment must generate a strongly connected network and\nhave low receiver-based interference (i.e., we minimize the largest in-degree\nof the network). We give an algorithm that generates a network with $O(\\log\n\\Delta)$ interference, where $\\Delta$ is the interference of a uniform-radius\nad-hoc network. We then adapt the construction to the case in which no sensor\ncan have a communication radius larger than $R_{\\min}$, the minimum value\nneeded to obtain connectivity. We also show that $\\log \\Delta$ interference is\nneeded for some instances, making our algorithms asymptotically optimal. \n\n"}
{"id": "1102.4599", "contents": "Title: Towards Unbiased BFS Sampling Abstract: Breadth First Search (BFS) is a widely used approach for sampling large\nunknown Internet topologies. Its main advantage over random walks and other\nexploration techniques is that a BFS sample is a plausible graph on its own,\nand therefore we can study its topological characteristics. However, it has\nbeen empirically observed that incomplete BFS is biased toward high-degree\nnodes, which may strongly affect the measurements. In this paper, we first\nanalytically quantify the degree bias of BFS sampling. In particular, we\ncalculate the node degree distribution expected to be observed by BFS as a\nfunction of the fraction f of covered nodes, in a random graph RG(pk) with an\narbitrary degree distribution pk. We also show that, for RG(pk), all commonly\nused graph traversal techniques (BFS, DFS, Forest Fire, Snowball Sampling, RDS)\nsuffer from exactly the same bias. Next, based on our theoretical analysis, we\npropose a practical BFS-bias correction procedure. It takes as input a\ncollected BFS sample together with its fraction f. Even though RG(pk) does not\ncapture many graph properties common in real-life graphs (such as\nassortativity), our RG(pk)-based correction technique performs well on a broad\nrange of Internet topologies and on two large BFS samples of Facebook and Orkut\nnetworks. Finally, we consider and evaluate a family of alternative correction\nprocedures, and demonstrate that, although they are unbiased for an arbitrary\ntopology, their large variance makes them far less effective than the\nRG(pk)-based technique. \n\n"}
{"id": "1102.5138", "contents": "Title: Low-Complexity Near-Optimal Codes for Gaussian Relay Networks Abstract: We consider the problem of information flow over Gaussian relay networks.\nSimilar to the recent work by Avestimehr \\emph{et al.} [1], we propose network\ncodes that achieve up to a constant gap from the capacity of such networks.\nHowever, our proposed codes are also computationally tractable. Our main\ntechnique is to use the codes of Avestimehr \\emph{et al.} as inner codes in a\nconcatenated coding scheme. \n\n"}
{"id": "1103.1992", "contents": "Title: Shocks in financial markets, price expectation, and damped harmonic\n  oscillators Abstract: Using a modified damped harmonic oscillator model equivalent to a model of\nmarket dynamics with price expectations, we analyze the reaction of financial\nmarkets to shocks. In order to do this, we gather data from indices of a\nvariety of financial markets for the 1987 Black Monday, the Russian crisis of\n1998, the crash after September 11th (2001), and the recent downturn of markets\ndue to the subprime mortgage crisis in the USA (2008). Analyzing those data we\nwere able to establish the amount by which each market felt the shocks, a\ndampening factor which expresses the capacity of a market of absorving a shock,\nand also a frequency related with volatility after the shock. The results gauge\nthe efficiency of different markets in recovering from such shocks, and measure\nsome level of dependence between them. We also show, using the correlation\nmatrices between the indices used, that financial markets are now much more\nconnected than they were two decades ago. \n\n"}
{"id": "1103.2662", "contents": "Title: Cost Analysis of Redundancy Schemes for Distributed Storage Systems Abstract: Distributed storage infrastructures require the use of data redundancy to\nachieve high data reliability. Unfortunately, the use of redundancy introduces\nstorage and communication overheads, which can either reduce the overall\nstorage capacity of the system or increase its costs. To mitigate the storage\nand communication overhead, different redundancy schemes have been proposed.\nHowever, due to the great variety of underlaying storage infrastructures and\nthe different application needs, optimizing these redundancy schemes for each\nstorage infrastructure is cumbersome. The lack of rules to determine the\noptimal level of redundancy for each storage configuration leads developers in\nindustry to often choose simpler redundancy schemes, which are usually not the\noptimal ones. In this paper we analyze the cost of different redundancy schemes\nand derive a set of rules to determine which redundancy scheme minimizes the\nstorage and the communication costs for a given system configuration.\nAdditionally, we use simulation to show that theoretically-optimal schemes may\nnot be viable in a realistic setting where nodes can go off-line and repairs\nmay be delayed. In these cases, we identify which are the trade-offs between\nthe storage and communication overheads of the redundancy scheme and its data\nreliability. \n\n"}
{"id": "1103.4016", "contents": "Title: Delay Constrained Throughput Analysis of a Correlated MIMO Wireless\n  Channel Abstract: The maximum traffic arrival rate at the network for a given delay guarantee\n(delay constrained throughput) has been well studied for wired channels.\nHowever, few results are available for wireless channels, especially when\nmultiple antennas are employed at the transmitter and receiver. In this work,\nwe analyze the network delay constrained throughput of a multiple input\nmultiple output (MIMO) wireless channel with time-varying spatial correlation.\nThe MIMO channel is modeled via its virtual representation, where the\nindividual spatial paths between the antenna pairs are Gilbert-Elliot channels.\nThe whole system is then described by a K-State Markov chain, where K depends\nupon the degree of freedom (DOF) of the channel. We prove that the DOF based\nmodeling is indeed accurate. Furthermore, we study the impact of the delay\nrequirements at the network layer, violation probability and the number of\nantennas on the throughput under different fading speeds and signal strength. \n\n"}
{"id": "1104.0327", "contents": "Title: Asymptotically Tight Steady-State Queue Length Bounds Implied By Drift\n  Conditions Abstract: The Foster-Lyapunov theorem and its variants serve as the primary tools for\nstudying the stability of queueing systems. In addition, it is well known that\nsetting the drift of the Lyapunov function equal to zero in steady-state\nprovides bounds on the expected queue lengths. However, such bounds are often\nvery loose due to the fact that they fail to capture resource pooling effects.\nThe main contribution of this paper is to show that the approach of \"setting\nthe drift of a Lyapunov function equal to zero\" can be used to obtain bounds on\nthe steady-state queue lengths which are tight in the heavy-traffic limit. The\nkey is to establish an appropriate notion of state-space collapse in terms of\nsteady-state moments of weighted queue length differences, and use this\nstate-space collapse result when setting the Lyapunov drift equal to zero. As\nan application of the methodology, we prove the steady-state equivalent of the\nheavy-traffic optimality result of Stolyar for wireless networks operating\nunder the MaxWeight scheduling policy. \n\n"}
{"id": "1104.4249", "contents": "Title: Robustness and Contagion in the International Financial Network Abstract: The recent financial crisis of 2008 and the 2011 indebtedness of Greece\nhighlight the importance of understanding the structure of the global financial\nnetwork. In this paper we set out to analyze and characterize this network, as\ncaptured by the IMF Coordinated Portfolio Investment Survey (CPIS), in two\nways. First, through an adaptation of the \"error and attack\" methodology [1],\nwe show that the network is of the \"robust-yet-fragile\" type, a topology found\nin a wide variety of evolved networks. We compare these results against four\ncommon null-models, generated only from first-order statistics of the empirical\ndata. In addition, we suggest a fifth, log-normal model, which generates\nnetworks that seem to match the empirical one more closely. Still, this model\ndoes not account for several higher order network statistics, which reenforces\nthe added value of the higher-order analysis. Second, using loss-given-default\ndynamics [2], we model financial interdependence and potential cascading of\nfinancial distress through the network. Preliminary simulations indicate that\ndefault by a single relatively small country like Greece can be absorbed by the\nnetwork, but that default in combination with defaults of other PIGS countries\n(Portugal, Ireland, and Spain) could lead to a massive extinction cascade in\nthe global economy. \n\n"}
{"id": "1104.5170", "contents": "Title: A Novel Power Allocation Scheme for Two-User GMAC with Finite Input\n  Constellations Abstract: Constellation Constrained (CC) capacity regions of two-user Gaussian Multiple\nAccess Channels (GMAC) have been recently reported, wherein an appropriate\nangle of rotation between the constellations of the two users is shown to\nenlarge the CC capacity region. We refer to such a scheme as the Constellation\nRotation (CR) scheme. In this paper, we propose a novel scheme called the\nConstellation Power Allocation (CPA) scheme, wherein the instantaneous transmit\npower of the two users are varied by maintaining their average power\nconstraints. We show that the CPA scheme offers CC sum capacities equal (at low\nSNR values) or close (at high SNR values) to those offered by the CR scheme\nwith reduced decoding complexity for QAM constellations. We study the\nrobustness of the CPA scheme for random phase offsets in the channel and\nunequal average power constraints for the two users. With random phase offsets\nin the channel, we show that the CC sum capacity offered by the CPA scheme is\nmore than the CR scheme at high SNR values. With unequal average power\nconstraints, we show that the CPA scheme provides maximum gain when the power\nlevels are close, and the advantage diminishes with the increase in the power\ndifference. \n\n"}
{"id": "1105.2243", "contents": "Title: More about Base Station Location Games Abstract: This paper addresses the problem of locating base stations in a certain area\nwhich is highly populated by mobile stations; each mobile station is assumed to\nselect the closest base station. Base stations are modeled by players who\nchoose their best location for maximizing their uplink throughput. The approach\nof this paper is to make some simplifying assumptions in order to get\ninterpretable analytical results and insights to the problem under study.\nSpecifically, a relatively complete Nash equilibrium (NE) analysis is conducted\n(existence, uniqueness, determination, and efficiency). Then, assuming that the\nbase station location can be adjusted dynamically, the best-response dynamics\nand reinforcement learning algorithm are applied, discussed, and illustrated\nthrough numerical results. \n\n"}
{"id": "1106.0027", "contents": "Title: On the geometry of wireless network multicast in 2-D Abstract: We provide a geometric solution to the problem of optimal relay positioning\nto maximize the multicast rate for low-SNR networks. The networks we consider,\nconsist of a single source, multiple receivers and the only intermediate and\nlocatable node as the relay. We construct network the hypergraph of the system\nnodes from the underlying information theoretic model of low-SNR regime that\noperates using superposition coding and FDMA in conjunction (which we call the\n\"achievable hypergraph model\"). We make the following contributions. 1) We show\nthat the problem of optimal relay positioning maximizing the multicast rate can\nbe completely decoupled from the flow optimization by noticing and exploiting\ngeometric properties of multicast flow. 2) All the flow maximizing the\nmulticast rate is sent over at most two paths, in succession. The relay\nposition is dependent only on one path (out of the two), irrespective of the\nnumber of receiver nodes in the system. Subsequently, we propose simple and\nefficient geometric algorithms to compute the optimal relay position. 3)\nFinally, we show that in our model at the optimal relay position, the\ndifference between the maximized multicast rate and the cut-set bound is\nminimum. We solve the problem for all (Ps,Pr) pairs of source and relay\ntransmit powers and the path loss exponent \\alpha greater than 2. \n\n"}
{"id": "1106.0735", "contents": "Title: Cross-Layer Scheduling for Cooperative Multi-Hop Cognitive Radio\n  Networks Abstract: The paper aims to design cross-layer optimal scheduling algorithms for\ncooperative multi-hop Cognitive Radio Networks (CRNs), where secondary users\n(SUs) assist primary user (PU)'s multi-hop transmissions and in return gain\nauthorization to access a share of the spectrum. We build two models for two\ndifferent types of PUs, corresponding to elastic and inelastic service classes.\nFor CRNs with elastic service, the PU maximizes its throughput while assigning\na time-share of the channel to SUs proportional to SUs' assistance. For the\ninelastic case, the PU is guaranteed a minimum utility. The proposed algorithm\nfor elastic PU model can achieve arbitrarily close to the optimal PU\nthroughput, while the proposed algorithm for inelastic PU model can achieve\narbitrarily close to the optimal SU utility. Both algorithms provide\ndeterministic upper-bounds for PU queue backlogs. In addition, we show a\ntradeoff between throughput/utility and PU's average end-to-end delay\nupper-bounds for both algorithms. Furthermore, the algorithms work in both\nbacklogged as well as arbitrary arrival rate systems. \n\n"}
{"id": "1106.0840", "contents": "Title: Random Deployment of Data Collectors for Serving Randomly-Located\n  Sensors Abstract: Recently, wireless communication industries have begun to extend their\nservices to machine-type communication devices as well as to user equipments.\nSuch machine-type communication devices as meters and sensors need intermittent\nuplink resources to report measured or sensed data to their serving data\ncollector. It is however hard to dedicate limited uplink resources to each of\nthem. Thus, efficient service of a tremendous number of devices with low\nactivities may consider simple random access as a solution. The data collectors\nreceiving the measured data from many sensors simultaneously can successfully\ndecode only signals with signal-to-interference-plus-noise-ratio (SINR) above a\ncertain value. The main design issues for this environment become how many data\ncollectors are needed, how much power sensor nodes transmit with, and how\nwireless channels affect the performance. This paper provides answers to those\nquestions through a stochastic analysis based on a spatial point process and on\nsimulations. \n\n"}
{"id": "1106.1577", "contents": "Title: Market efficiency, anticipation and the formation of bubbles-crashes Abstract: A dynamical model is introduced for the formation of a bullish or bearish\ntrends driving an asset price in a given market. Initially, each agent decides\nto buy or sell according to its personal opinion, which results from the\ncombination of its own private information, the public information and its own\nanalysis. It then adjusts such opinion through the market as it observes\nsequentially the behavior of a group of random selection of other agents. Its\nchoice is then determined by a local majority rule including itself. Whenever\nthe selected group is at a tie, i.e., it is undecided on what to do, the choice\nis determined by the local group belief with respect to the anticipated trend\nat that time. These local adjustments create a dynamic that leads the market\nprice formation. In case of balanced anticipations the market is found to be\nefficient in being successful to make the \"right price\" to emerge from the\nsequential aggregation of all the local individual informations which all\ntogether contain the fundamental value. However, when a leading optimistic\nbelief prevails, the same efficient market mechanisms are found to produce a\nbullish dynamic even though most agents have bearish private informations. The\nmarket yields then a wider and wider discrepancy between the fundamental value\nand the market value, which in turn creates a speculative bubble. Nevertheless,\nthere exists a limit in the growing of the bubble where private opinions take\nover again and at once invert the trend, originating a sudden bearish trend.\nMoreover, in the case of a drastic shift in the collective expectations, a huge\ndrop in price levels may also occur extremely fast and puts the market out of\ncontrol, it is a market crash. \n\n"}
{"id": "1106.2325", "contents": "Title: SVM and Dimensionality Reduction in Cognitive Radio with Experimental\n  Validation Abstract: There is a trend of applying machine learning algorithms to cognitive radio.\nOne fundamental open problem is to determine how and where these algorithms are\nuseful in a cognitive radio network. In radar and sensing signal processing,\nthe control of degrees of freedom (DOF)---or dimensionality---is the first\nstep, called pre-processing. In this paper, the combination of dimensionality\nreduction with SVM is proposed apart from only applying SVM for classification\nin cognitive radio. Measured Wi-Fi signals with high signal to noise ratio\n(SNR) are employed to the experiments. The DOF of Wi-Fi signals is extracted by\ndimensionality reduction techniques. Experimental results show that with\ndimensionality reduction, the performance of classification is much better with\nfewer features than that of without dimensionality reduction. The error rates\nof classification with only one feature of the proposed algorithm can match the\nerror rates of 13 features of the original data. The proposed method will be\nfurther tested in our cognitive radio network testbed. \n\n"}
{"id": "1106.4288", "contents": "Title: Continuum Limits of Markov Chains with Application to Network Modeling Abstract: In this paper we investigate the continuum limits of a class of Markov\nchains. The investigation of such limits is motivated by the desire to model\nvery large networks. We show that under some conditions, a sequence of Markov\nchains converges in some sense to the solution of a partial differential\nequation. Based on such convergence we approximate Markov chains modeling\nnetworks with a large number of components by partial differential equations.\nWhile traditional Monte Carlo simulation for very large networks is practically\ninfeasible, partial differential equations can be solved with reasonable\ncomputational overhead using well-established mathematical tools. \n\n"}
{"id": "1108.0443", "contents": "Title: Sparse Recovery with Graph Constraints: Fundamental Limits and\n  Measurement Construction Abstract: This paper addresses the problem of sparse recovery with graph constraints in\nthe sense that we can take additive measurements over nodes only if they induce\na connected subgraph. We provide explicit measurement constructions for several\nspecial graphs. A general measurement construction algorithm is also proposed\nand evaluated. For any given graph $G$ with $n$ nodes, we derive order optimal\nupper bounds of the minimum number of measurements needed to recover any\n$k$-sparse vector over $G$ ($M^G_{k,n}$). Our study suggests that $M^G_{k,n}$\nmay serve as a graph connectivity metric. \n\n"}
{"id": "1108.1951", "contents": "Title: How much multifractality is included in monofractal signals? Abstract: We investigate the presence of residual multifractal background for\nmonofractal signals which appears due to the finite length of the signals and\n(or) due to the long memory the signals reveal. This phenomenon is investigated\nnumerically within the multifractal detrended fluctuation analysis (MF-DFA) for\nartificially generated time series. Next, the analytical formulas enabling to\ndescribe the multifractal content in such signals are provided. Final results\nare shown in the frequently used generalized Hurst exponent h(q) multifractal\nscenario and are presented as a function of time series length L and the\nautocorrelation exponent value {\\gamma}. The multifractal spectrum ({\\alpha}, f\n({\\alpha})) approach is also discussed. The obtained results may be significant\nin any practical application of multifractality, including financial data\nanalysis, because the \"true\" multifractal effect should be clearly separated\nfrom the so called \"multifractal noise\". Examples from finance in this context\nare given. The provided formulas may help to decide whether we do deal with the\nsignal of real multifractal origin or not. \n\n"}
{"id": "1108.4226", "contents": "Title: Research on Wireless Multi-hop Networks: Current State and Challenges Abstract: Wireless multi-hop networks, in various forms and under various names, are\nbeing increasingly used in military and civilian applications. Studying\nconnectivity and capacity of these networks is an important problem. The\nscaling behavior of connectivity and capacity when the network becomes\nsufficiently large is of particular interest. In this position paper, we\nbriefly overview recent development and discuss research challenges and\nopportunities in the area, with a focus on the network connectivity. \n\n"}
{"id": "1108.5893", "contents": "Title: Edge-preserving self-healing: keeping network backbones densely\n  connected Abstract: Healing algorithms play a crucial part in distributed P2P networks where\nfailures occur continuously and frequently. Several self-healing algorithms\nhave been suggested recently [IPDPS'08, PODC'08, PODC'09, PODC'11] in a line of\nwork that has yielded gradual improvements in the properties ensured on the\ngraph. This work motivates a strong general phenomenon of edge-preserving\nhealing that aims at obtaining self-healing algorithms with the constraint that\nall original edges in the graph (not deleted by the adversary), be retained in\nevery intermediate graph.\n  The previous algorithms, in their nascent form, are not explicitly edge\npreserving. In this paper, we show they can be suitably modified (We introduce\nXheal+, an edge-preserving version of Xheal[PODC'11]). Towards this end, we\npresent a general self-healing model that unifies the previous models. The main\ncontribution of this paper is not in the technical complexity, rather in the\nsimplicity with which the edge-preserving property can be ensured and the\nmessage that this is a crucial property with several benefits. In particular,\nwe highlight this by showing that, almost as an immediate corollary, subgraph\ndensities are preserved or increased. Maintaining density is a notion motivated\nby the fact that in certain distributed networks, certain nodes may require and\ninitially have a larger number of inter-connections. It is vital that a healing\nalgorithm, even amidst failures, respect these requirements. Our suggested\nmodifications yield such subgraph density preservation as a by product. In\naddition, edge preservation helps maintain any subgraph induced property that\nis monotonic. Also, algorithms that are edge-preserving require minimal\nalteration of edges which can be an expensive cost in healing - something that\nhas not been modeled in any of the past work. \n\n"}
{"id": "1109.0264", "contents": "Title: Simple Regenerating Codes: Network Coding for Cloud Storage Abstract: Network codes designed specifically for distributed storage systems have the\npotential to provide dramatically higher storage efficiency for the same\navailability. One main challenge in the design of such codes is the exact\nrepair problem: if a node storing encoded information fails, in order to\nmaintain the same level of reliability we need to create encoded information at\na new node. One of the main open problems in this emerging area has been the\ndesign of simple coding schemes that allow exact and low cost repair of failed\nnodes and have high data rates. In particular, all prior known explicit\nconstructions have data rates bounded by 1/2.\n  In this paper we introduce the first family of distributed storage codes that\nhave simple look-up repair and can achieve arbitrarily high rates. Our\nconstructions are very simple to implement and perform exact repair by simple\nXORing of packets. We experimentally evaluate the proposed codes in a realistic\ncloud storage simulator and show significant benefits in both performance and\nreliability compared to replication and standard Reed-Solomon codes. \n\n"}
{"id": "1109.2076", "contents": "Title: Escalation, timing and severity of insurgent and terrorist events:\n  Toward a unified theory of future threats Abstract: I present a unified discussion of several recently published results\nconcerning the escalation, timing and severity of violent events in human\nconflicts and global terrorism, and set them in the wider context of real-world\nand cyber-based collective violence and illicit activity. I point out how the\nborders distinguishing between such activities are becoming increasingly\nblurred in practice -- from insurgency, terrorism, criminal gangs and\ncyberwars, through to the 2011 Arab Spring uprisings and London riots. I review\nthe robust empirical patterns that have been found, and summarize a minimal\nmechanistic model which can explain these patterns. I also explain why this\nmechanistic approach, which is inspired by non-equilibrium statistical physics,\nfits naturally within the framework of recent ideas within the social science\nliterature concerning analytical sociology. In passing, I flag the fundamental\nflaws in each of the recent critiques which have surfaced concerning the\nrobustness of these results and the realism of the underlying model mechanisms. \n\n"}
{"id": "1110.1569", "contents": "Title: Robust Estimators for Variance-Based Device-Free Localization and\n  Tracking Abstract: Human motion in the vicinity of a wireless link causes variations in the link\nreceived signal strength (RSS). Device-free localization (DFL) systems, such as\nvariance-based radio tomographic imaging (VRTI), use these RSS variations in a\nstatic wireless network to detect, locate and track people in the area of the\nnetwork, even through walls. However, intrinsic motion, such as branches moving\nin the wind and rotating or vibrating machinery, also causes RSS variations\nwhich degrade the performance of a DFL system. In this paper, we propose and\nevaluate two estimators to reduce the impact of the variations caused by\nintrinsic motion. One estimator uses subspace decomposition, and the other\nestimator uses a least squares formulation. Experimental results show that both\nestimators reduce localization root mean squared error by about 40% compared to\nVRTI. In addition, the Kalman filter tracking results from both estimators have\n97% of errors less than 1.3 m, more than 60% improvement compared to tracking\nresults from VRTI. \n\n"}
{"id": "1110.2075", "contents": "Title: Conservative self-organized extremal model for wealth distribution Abstract: We present a detailed numerical analysis of the modified version of a\nconservative self-organized extremal model introduced by Pianegonda et. al. for\nthe distribution of wealth of the people in a society. Here the trading process\nhas been modified by the stochastic bipartite trading rule. More specifically\nin a trade one of the agents is necessarily the one with the globally minimal\nvalue of wealth, the other one being selected randomly from the neighbors of\nthe first agent. The pair of agents then randomly re-shuffle their entire\namount of wealth without saving. This model has most of the characteristics\nsimilar to the self-organized critical Bak-Sneppen model of evolutionary\ndynamics. Numerical estimates of a number of critical exponents indicate this\nmodel is likely to belong to a new universality class different from the well\nknown models in the literature. In addition the persistence time, which is the\ntime interval between two successive updates of wealth of an agent has been\nobserved to have a non-trivial power law distribution. An opposite version of\nthe model has also been studied where the agent with maximal wealth is selected\ninstead of the one with minimal wealth, which however, exhibits similar\nbehavior as the Minimal Wealth model. \n\n"}
{"id": "1110.3248", "contents": "Title: Integral representation of martingales motivated by the problem of\n  endogenous completeness in financial economics Abstract: Let $\\mathbb{Q}$ and $\\mathbb{P}$ be equivalent probability measures and let\n$\\psi$ be a $J$-dimensional vector of random variables such that\n$\\frac{d\\mathbb{Q}}{d\\mathbb{P}}$ and $\\psi$ are defined in terms of a weak\nsolution $X$ to a $d$-dimensional stochastic differential equation. Motivated\nby the problem of \\emph{endogenous completeness} in financial economics we\npresent conditions which guarantee that every local martingale under\n$\\mathbb{Q}$ is a stochastic integral with respect to the $J$-dimensional\nmartingale $S_t \\set \\mathbb{E}^{\\mathbb{Q}}[\\psi|\\mathcal{F}_t]$. While the\ndrift $b=b(t,x)$ and the volatility $\\sigma = \\sigma(t,x)$ coefficients for $X$\nneed to have only minimal regularity properties with respect to $x$, they are\nassumed to be analytic functions with respect to $t$. We provide a\ncounter-example showing that this $t$-analyticity assumption for $\\sigma$\ncannot be removed. \n\n"}
{"id": "1111.2251", "contents": "Title: On the Optimal Transmission Scheme to Maximize Local Capacity in\n  Wireless Networks Abstract: We study the optimal transmission scheme that maximizes the local capacity in\ntwo-dimensional (2D) wireless networks. Local capacity is defined as the\naverage information rate received by a node randomly located in the network.\nUsing analysis based on analytical and numerical methods, we show that maximum\nlocal capacity can be obtained if simultaneous emitters are positioned in a\ngrid pattern based on equilateral triangles. We also compare this maximum local\ncapacity with the local capacity of slotted ALOHA scheme and our results show\nthat slotted ALOHA can achieve at least half of the maximum local capacity in\nwireless networks. \n\n"}
{"id": "1111.5289", "contents": "Title: Heisenberg uncertainty principle and economic analogues of basic\n  physical quantities Abstract: From positions, attained by modern theoretical physics in understanding of\nthe universe bases, the methodological and philosophical analysis of\nfundamental physical concepts and their formal and informal connections with\nthe real economic measurings is carried out. Procedures for heterogeneous\neconomic time determination, normalized economic coordinates and economic mass\nare offered, based on the analysis of time series, the concept of economic\nPlank's constant has been proposed. The theory has been approved on the real\neconomic dynamic's time series, including stock indices, Forex and spot prices,\nthe achieved results are open for discussion. \n\n"}
{"id": "1111.6689", "contents": "Title: Bounding Interference in Wireless Ad Hoc Networks with Nodes in Random\n  Position Abstract: The interference at a wireless node s can be modelled by the number of\nwireless nodes whose transmission ranges cover s. Given a set of positions for\nwireless nodes, the interference minimization problem is to assign a\ntransmission radius (equivalently, a power level) to each node such that the\nresulting communication graph is connected, while minimizing the maximum\ninterference. We consider the model introduced by von Rickenback et al. (2005),\nin which each transmission range is represented by a ball and edges in the\ncommunication graph are symmetric. The problem is NP-complete in two dimensions\n(Buchin 2008) and no polynomial-time approximation algorithm is known.\nFurthermore, even in one dimension (the highway model), the problem's\ncomplexity is unknown and the maximum interference of a set of n wireless nodes\ncan be as high as Theta(sqrt(n)) (von Rickenback et al. 2005). In this paper we\nshow how to solve the problem efficiently in settings typical for wireless ad\nhoc networks. In particular, we show that if node positions are represented by\na set P of n points selected uniformly and independently at random over a\nd-dimensional rectangular region, for any fixed d, then the topology given by\nthe closure of the Euclidean minimum spanning tree of P has maximum\ninterference O(log n) with high probability. We extend this bound to a general\nclass of communication graphs over a broad set of probability distributions.\nNext we present a local algorithm that constructs a graph from this class; this\nis the first local algorithm to provide an upper bound on the expected maximum\ninterference. Finally, we discuss an empirical evaluation of our algorithm with\na suite of simulation results. \n\n"}
{"id": "1112.2059", "contents": "Title: Randomised Mixture Models for Pricing Kernels Abstract: Numerous kinds of uncertainties may affect an economy, e.g. economic,\npolitical, and environmental ones. We model the aggregate impact by the\nuncertainties on an economy and its associated financial market by randomised\nmixtures of L\\'evy processes. We assume that market participants observe the\nrandomised mixtures only through best estimates based on noisy market\ninformation. The concept of incomplete information introduces an element of\nstochastic filtering theory in constructing what we term \"filtered Esscher\nmartingales\". We make use of this family of martingales to develop pricing\nkernel models. Examples of bond price models are examined, and we show that the\nchoice of the random mixture has a significant effect on the model dynamics and\nthe types of movements observed in the associated yield curves. Parameter\nsensitivity is analysed and option price processes are derived. We extend the\nclass of pricing kernel models by considering a weighted heat kernel approach,\nand develop models driven by mixtures of Markov processes. \n\n"}
{"id": "1112.2168", "contents": "Title: Firm dynamics in a closed, conserved economy: A model of size\n  distribution of employment and related statistics Abstract: We address the issue of the distribution of firm size. To this end we propose\na model of firms in a closed, conserved economy populated with\nzero-intelligence agents who continuously move from one firm to another. We\nthen analyze the size distribution and related statistics obtained from the\nmodel. Our ultimate goal is to reproduce the well known statistical features\nobtained from the panel study of the firms i.e., the power law in size (in\nterms of income and/or employment), the Laplace distribution in the growth\nrates and the slowly declining standard deviation of the growth rates\nconditional on the firm size. First, we show that the model generalizes the\nusual kinetic exchange models with binary interaction to interactions between\nan arbitrary number of agents. When the number of interacting agents is in the\norder of the system itself, it is possible to decouple the model. We provide\nsome exact results on the distributions. Our model easily reproduces the power\nlaw. The fluctuations in the growth rate falls with increasing size following a\npower law (with an exponent 1 whereas the data suggests that the exponent is\naround 1/6). However, the distribution of the difference of the firm-size in\nthis model has Laplace distribution whereas the real data suggests that the\ndifference of the log sizes has the same distribution. \n\n"}
{"id": "1112.2895", "contents": "Title: Null Models of Economic Networks: The Case of the World Trade Web Abstract: In all empirical-network studies, the observed properties of economic\nnetworks are informative only if compared with a well-defined null model that\ncan quantitatively predict the behavior of such properties in constrained\ngraphs. However, predictions of the available null-model methods can be derived\nanalytically only under assumptions (e.g., sparseness of the network) that are\nunrealistic for most economic networks like the World Trade Web (WTW). In this\npaper we study the evolution of the WTW using a recently-proposed family of\nnull network models. The method allows to analytically obtain the expected\nvalue of any network statistic across the ensemble of networks that preserve on\naverage some local properties, and are otherwise fully random. We compare\nexpected and observed properties of the WTW in the period 1950-2000, when\neither the expected number of trade partners or total country trade is kept\nfixed and equal to observed quantities. We show that, in the binary WTW,\nnode-degree sequences are sufficient to explain higher-order network properties\nsuch as disassortativity and clustering-degree correlation, especially in the\nlast part of the sample. Conversely, in the weighted WTW, the observed sequence\nof total country imports and exports are not sufficient to predict higher-order\npatterns of the WTW. We discuss some important implications of these findings\nfor international-trade models. \n\n"}
{"id": "1112.3095", "contents": "Title: Evidence of market manipulation in the financial crisis Abstract: We provide direct evidence of market manipulation at the beginning of the\nfinancial crisis in November 2007. The type of manipulation, a \"bear raid,\"\nwould have been prevented by a regulation that was repealed by the Securities\nand Exchange Commission in July 2007. The regulation, the uptick rule, was\ndesigned to prevent manipulation and promote stability and was in force from\n1938 as a key part of the government response to the 1929 market crash and its\naftermath. On November 1, 2007, Citigroup experienced an unusual increase in\ntrading volume and decrease in price. Our analysis of financial industry data\nshows that this decline coincided with an anomalous increase in borrowed\nshares, the selling of which would be a large fraction of the total trading\nvolume. The selling of borrowed shares cannot be explained by news events as\nthere is no corresponding increase in selling by share owners. A similar number\nof shares were returned on a single day six days later. The magnitude and\ncoincidence of borrowing and returning of shares is evidence of a concerted\neffort to drive down Citigroup's stock price and achieve a profit, i.e., a bear\nraid. Interpretations and analyses of financial markets should consider the\npossibility that the intentional actions of individual actors or coordinated\ngroups can impact market behavior. Markets are not sufficiently transparent to\nreveal even major market manipulation events. Our results point to the need for\nregulations that prevent intentional actions that cause markets to deviate from\nequilibrium and contribute to crashes. Enforcement actions cannot reverse\nsevere damage to the economic system. The current \"alternative\" uptick rule\nwhich is only in effect for stocks dropping by over 10% in a single day is\ninsufficient. Prevention may be achieved through improved availability of\nmarket data and the original uptick rule or other transaction limitations. \n\n"}
{"id": "1112.5340", "contents": "Title: Fundamental theorems of asset pricing for piecewise semimartingales of\n  stochastic dimension Abstract: The purpose of this paper is two-fold. First is to extend the notions of an\nn-dimensional semimartingale and its stochastic integral to a piecewise\nsemimartingale of stochastic dimension. The properties of the former carry over\nlargely intact to the latter, avoiding some of the pitfalls of\ninfinite-dimensional stochastic integration. Second is to extend two\nfundamental theorems of asset pricing (FTAPs): the equivalence of no free lunch\nwith vanishing risk to the existence of an equivalent sigma-martingale measure\nfor the price process, and the equivalence of no arbitrage of the first kind to\nthe existence of an equivalent local martingale deflator for the set of\nnonnegative wealth processes. \n\n"}
{"id": "1201.0662", "contents": "Title: Transmission capacity of wireless networks Abstract: Transmission capacity (TC) is a performance metric for wireless networks that\nmeasures the spatial intensity of successful transmissions per unit area,\nsubject to a constraint on the permissible outage probability (where outage\noccurs when the SINR at a receiver is below a threshold). This volume gives a\nunified treatment of the TC framework that has been developed by the authors\nand their collaborators over the past decade. The mathematical framework\nunderlying the analysis (reviewed in Ch. 2) is stochastic geometry: Poisson\npoint processes model the locations of interferers, and (stable) shot noise\nprocesses represent the aggregate interference seen at a receiver. Ch. 3\npresents TC results (exact, asymptotic, and bounds) on a simple model in order\nto illustrate a key strength of the framework: analytical tractability yields\nexplicit performance dependence upon key model parameters. Ch. 4 presents\nenhancements to this basic model --- channel fading, variable link distances,\nand multi-hop. Ch. 5 presents four network design case studies well-suited to\nTC: i) spectrum management, ii) interference cancellation, iii) signal\nthreshold transmission scheduling, and iv) power control. Ch. 6 studies the TC\nwhen nodes have multiple antennas, which provides a contrast vs. classical\nresults that ignore interference. \n\n"}
{"id": "1201.0769", "contents": "Title: Robust utility maximization in non-dominated models with 2BSDEs Abstract: The problem of robust utility maximization in an incomplete market with\nvolatility uncertainty is considered, in the sense that the volatility of the\nmarket is only assumed to lie between two given bounds. The set of all possible\nmodels (probability measures) considered here is non-dominated. We propose\nstudying this problem in the framework of second-order backward stochastic\ndifferential equations (2BSDEs for short) with quadratic growth generators. We\nshow for exponential, power and logarithmic utilities that the value function\nof the problem can be written as the initial value of a particular 2BSDE and\nprove existence of an optimal strategy. Finally several examples which shed\nmore light on the problem and its links with the classical utility maximization\none are provided. In particular, we show that in some cases, the upper bound of\nthe volatility interval plays a central role, exactly as in the option pricing\nproblem with uncertain volatility models of [2]. \n\n"}
{"id": "1201.2937", "contents": "Title: Opportunistic Adaptive Relaying in Cognitive Radio Networks Abstract: Combining cognitive radio technology with user cooperation could be\nadvantageous to both primary and secondary transmissions. In this paper, we\npropose a first relaying scheme for cognitive radio networks (called \"Adaptive\nrelaying scheme 1\"), where one relay node can assist the primary or the\nsecondary transmission with the objective of improving the outage probability\nof the secondary transmission with respect to a primary outage probability\nthreshold. Upper bound expressions of the secondary outage probability using\nthe proposed scheme are derived over Rayleigh fading channels. Numerical and\nsimulation results show that the secondary outage probability using the\nproposed scheme is lower than that of other relaying schemes. Then, we extend\nthe proposed scheme to the case where the relay node has the ability to decode\nboth the primary and secondary signals and also can assist simultaneously both\ntransmissions. Simulations show the performance improvement that can be\nobtained due to this extension in terms of secondary outage probability. \n\n"}
{"id": "1201.3584", "contents": "Title: Ecological analysis of world trade Abstract: Ecological systems have a high level of complexity combined with stability\nand rich biodiversity. Recently, the analysis of their properties and evolution\nhas been pushed forward on a basis of concept of mutualistic networks that\nprovides a detailed understanding of their features being linked to a high\nnestedness of these networks. It was shown that the nestedness architecture of\nmutualistic networks of plants and their pollinators minimizes competition and\nincreases biodiversity. Here, using the United Nations COMTRADE database for\nyears 1962 - 2009, we show that a similar ecological analysis gives a valuable\ndescription of the world trade. In fact the countries and trade products are\nanalogous to plants and pollinators, and the whole trade network is\ncharacterized by a low nestedness temperature which is typical for the\necological networks. This approach provides new mutualistic features of the\nworld trade highlighting new significance of countries and trade products for\nthe world trade. \n\n"}
{"id": "1201.4013", "contents": "Title: Connectivity of Confined Dense Networks: Boundary Effects and Scaling\n  Laws Abstract: In this paper, we study the probability that a dense network confined within\na given geometry is fully connected. We employ a cluster expansion approach\noften used in statistical physics to analyze the effects that the boundaries of\nthe geometry have on connectivity. To maximize practicality and applicability,\nwe adopt four important point-to-point link models based on outage probability\nin our analysis: single-input single-output (SISO), single-input\nmultiple-output (SIMO), multiple-input single-output (MISO), and multiple-input\nmultiple-output (MIMO). Furthermore, we derive diversity and power scaling laws\nthat dictate how boundary effects can be mitigated (to leading order) in\nconfined dense networks for each of these models. Finally, in order to\ndemonstrate the versatility of our theory, we analyze boundary effects for\ndense networks comprising MIMO point-to-point links confined within a right\nprism, a polyhedron that accurately models many geometries that can be found in\npractice. We provide numerical results for this example, which verify our\nanalytical results. \n\n"}
{"id": "1201.4197", "contents": "Title: A Survey of Smart Data Pricing: Past Proposals, Current Plans, and\n  Future Trends Abstract: Traditionally, network operators have used simple flat-rate broadband data\nplans for both wired and wireless network access. But today, with the\npopularity of mobile devices and exponential growth of apps, videos, and\nclouds, service providers are gradually moving towards more sophisticated\npricing schemes. This decade will therefore likely witness a major change in\nthe ways in which network resources are managed, and the role of economics in\nallocating these resources. This survey reviews some of the well-known past\nbroadband pricing proposals (both static and dynamic), including their current\nrealizations in various consumer data plans around the world, and discusses\nseveral research problems and open questions. By exploring the benefits and\nchallenges of pricing data, this paper attempts to facilitate both the\nindustrial and the academic communities' efforts in understanding the existing\nliterature, recognizing new trends, and shaping an appropriate and timely\nresearch agenda. \n\n"}
{"id": "1201.4586", "contents": "Title: To lag or not to lag? How to compare indices of stock markets that\n  operate at different times Abstract: Financial markets worldwide do not have the same working hours. As a\nconsequence, the study of correlation or causality between financial market\nindices becomes dependent on wether we should consider in computations of\ncorrelation matrices all indices in the same day or lagged indices. The answer\nthis article proposes is that we should consider both. In this work, we use 79\nindices of a diversity of stock markets across the world in order to study\ntheir correlation structure, and discover that representing in the same network\noriginal and lagged indices, we obtain a better understanding of how indices\nthat operate at different hours relate to each other. \n\n"}
{"id": "1201.5608", "contents": "Title: Combinatorial Channel Signature Modulation for Wireless ad-hoc Networks Abstract: In this paper we introduce a novel modulation and multiplexing method which\nfacilitates highly efficient and simultaneous communication between multiple\nterminals in wireless ad-hoc networks. We term this method Combinatorial\nChannel Signature Modulation (CCSM). The CCSM method is particularly efficient\nin situations where communicating nodes operate in highly time dispersive\nenvironments. This is all achieved with a minimal MAC layer overhead, since all\nusers are allowed to transmit and receive at the same time/frequency (full\nsimultaneous duplex). The CCSM method has its roots in sparse modelling and the\nreceiver is based on compressive sampling techniques. Towards this end, we\ndevelop a new low complexity algorithm termed Group Subspace Pursuit. Our\nanalysis suggests that CCSM at least doubles the throughput when compared to\nthe state-of-the art. \n\n"}
{"id": "1202.3018", "contents": "Title: Using TV Receiver Information to Increase Cognitive White Space Spectrum Abstract: In this paper we investigate the usage of cognitive radio devices within the\nservice area of TV broadcast stations. Until now the main approach for a\ncognitive radio to operate in the TV bands has been to register TV broadcast\nstations locations and thus protecting the broadcast stations service area.\nThrough information about TV receivers location, we show that a cognitive radio\nshould be able to operate within this service area without causing harmful\ninterference to the TV receivers as defined by Ofcom and FCC. We provide\nsimulations based on real statistics from Norway that show that especially in\nrural areas TV receiver registration can provide a substantial gain in terms of\nexploitable frequencies for a cognitive radio. \n\n"}
{"id": "1202.3993", "contents": "Title: Internet Topology over Time Abstract: There are few studies that look closely at how the topology of the Internet\nevolves over time; most focus on snapshots taken at a particular point in time.\nIn this paper, we investigate the evolution of the topology of the Autonomous\nSystems graph of the Internet, examining how eight commonly-used topological\nmeasures change from January 2002 to January 2010. We find that the\ndistributions of most of the measures remain unchanged, except for average path\nlength and clustering coefficient. The average path length has slowly and\nsteadily increased since 2005 and the average clustering coefficient has\nsteadily declined. We hypothesize that these changes are due to changes in\npeering policies as the Internet evolves. We also investigate a surprising\nfeature, namely that the maximum degree has changed little, an aspect that\ncannot be captured without modeling link deletion. Our results suggest that\nevaluating models of the Internet graph by comparing steady-state generated\ntopologies to snapshots of the real data is reasonable for many measures.\nHowever, accurately matching time-variant properties is more difficult, as we\ndemonstrate by evaluating ten well-known models against the 2010 data. \n\n"}
{"id": "1202.4385", "contents": "Title: An Overview of Local Capacity in Wireless Networks Abstract: This article introduces a metric for performance evaluation of medium access\nschemes in wireless ad hoc networks known as local capacity. Although deriving\nthe end-to-end capacity of wireless ad hoc networks is a difficult problem, the\nlocal capacity framework allows us to quantify the average information rate\nreceived by a receiver node randomly located in the network. In this article,\nthe basic network model and analytical tools are first discussed and applied to\na simple network to derive the local capacity of various medium access schemes.\nOur goal is to identify the most optimal scheme and also to see how does it\ncompare with more practical medium access schemes. We analyzed grid pattern\nschemes where simultaneous transmitters are positioned in a regular grid\npattern, ALOHA schemes where simultaneous transmitters are dispatched according\nto a uniform Poisson distribution and exclusion schemes where simultaneous\ntransmitters are dispatched according to an exclusion rule such as node\ncoloring and carrier sense schemes. Our analysis shows that local capacity is\noptimal when simultaneous transmitters are positioned in a grid pattern based\non equilateral triangles and our results show that this optimal local capacity\nis at most double the local capacity of ALOHA based scheme. Our results also\nshow that node coloring and carrier sense schemes approach the optimal local\ncapacity by an almost negligible difference. At the end, we also discuss the\nshortcomings in our model as well as future research directions. \n\n"}
{"id": "1202.5926", "contents": "Title: Second-order Price Dynamics: Approach to Equilibrium with Perpetual\n  Arbitrage Abstract: The notion that economies should normally be in equilibrium is by now\nwell-established; equally well-established is that economies are almost never\nprecisely in equilibrium. Using a very general formulation, we show that under\ndynamics that are second-order in time a price system can remain away from\nequilibrium with permanent and repeating opportunities for arbitrage, even when\na damping term drives the system towards equilibrium. We also argue that\nsecond-order dynamic equations emerge naturally when there are heterogeneous\neconomic actors, some behaving as active and knowledgeable arbitrageurs, and\nothers using heuristics. The essential mechanism is that active arbitrageurs\nare able to repeatedly benefit from the suboptimal heuristics that govern most\neconomic behavior. \n\n"}
{"id": "1202.5945", "contents": "Title: A Note on Interference in Random Point Sets Abstract: The (maximum receiver-centric) interference of a geometric graph (von\nRickenbach etal (2005)) is studied. It is shown that, with high probability,\nthe following results hold for a set, V, of n points independently and\nuniformly distributed in the unit d-cube, for constant dimension d: (1) there\nexists a connected graph with vertex set V that has interference O((log\nn)^{1/3}); (2) no connected graph with vertex set V has interference o((log\nn)^{1/4}); and (3) the minimum spanning tree of $V$ has interference\nTheta((\\log n)^{1/2}). \n\n"}
{"id": "1202.6632", "contents": "Title: Coherent Price Systems and Uncertainty-Neutral Valuation Abstract: We consider fundamental questions of arbitrage pricing arising when the\nuncertainty model is given by a set of possible mutually singular probability\nmeasures. With a single probability model, essential equivalence between the\nabsence of arbitrage and the existence of an equivalent martingale measure is a\nfolk theorem, see Harrison and Kreps (1979). We establish a microeconomic\nfoundation of sublinear price systems and present an extension result. In this\ncontext we introduce a prior dependent notion of marketed spaces and viable\nprice systems. We associate this extension with a canonically altered concept\nof equivalent symmetric martingale measure sets, in a dynamic trading framework\nunder absence of prior depending arbitrage. We prove the existence of such sets\nwhen volatility uncertainty is modeled by a stochastic differential equation,\ndriven by Peng's G-Brownian motions. \n\n"}
{"id": "1203.0536", "contents": "Title: Algorithms for Wireless Capacity Abstract: In this paper we address two basic questions in wireless communication:\nFirst, how long does it take to schedule an arbitrary set of communication\nrequests? Second, given a set of communication requests, how many of them can\nbe scheduled concurrently? Our results are derived in an interference model\nwith geometric path loss and consist of efficient algorithms that find a\nconstant approximation for the second problem and a logarithmic approximation\nfor the first problem. In addition, we analyze some important properties of the\ninterference model and show that it is robust to various factors that can\ninfluence the signal attenuation. More specifically, we prove that as long as\nsuch influences on the signal attenuation are constant, they affect the\ncapacity only by a constant factor. \n\n"}
{"id": "1203.1226", "contents": "Title: Dynamic Packet Scheduling in Wireless Networks Abstract: We consider protocols that serve communication requests arising over time in\na wireless network that is subject to interference. Unlike previous approaches,\nwe take the geometry of the network and power control into account, both\nallowing to increase the network's performance significantly. We introduce a\nstochastic and an adversarial model to bound the packet injection. Although\ntaken as the primary motivation, this approach is not only suitable for models\nbased on the signal-to-interference-plus-noise ratio (SINR). It also covers\nvirtually all other common interference models, for example the multiple-access\nchannel, the radio-network model, the protocol model, and distance-2 matching.\nPacket-routing networks allowing each edge or each node to transmit or receive\none packet at a time can be modeled as well.\n  Starting from algorithms for the respective scheduling problem with static\ntransmission requests, we build distributed stable protocols. This is more\ninvolved than in previous, similar approaches because the algorithms we\nconsider do not necessarily scale linearly when scaling the input instance. We\ncan guarantee a throughput that is as large as the one of the original static\nalgorithm. In particular, for SINR models the competitive ratios of the\nprotocol in comparison to optimal ones in the respective model are between\nconstant and O(log^2 m) for a network of size m. \n\n"}
{"id": "1203.1570", "contents": "Title: In-network Sparsity-regularized Rank Minimization: Algorithms and\n  Applications Abstract: Given a limited number of entries from the superposition of a low-rank matrix\nplus the product of a known fat compression matrix times a sparse matrix,\nrecovery of the low-rank and sparse components is a fundamental task subsuming\ncompressed sensing, matrix completion, and principal components pursuit. This\npaper develops algorithms for distributed sparsity-regularized rank\nminimization over networks, when the nuclear- and $\\ell_1$-norm are used as\nsurrogates to the rank and nonzero entry counts of the sought matrices,\nrespectively. While nuclear-norm minimization has well-documented merits when\ncentralized processing is viable, non-separability of the singular-value sum\nchallenges its distributed minimization. To overcome this limitation, an\nalternative characterization of the nuclear norm is adopted which leads to a\nseparable, yet non-convex cost minimized via the alternating-direction method\nof multipliers. The novel distributed iterations entail reduced-complexity\nper-node tasks, and affordable message passing among single-hop neighbors.\nInterestingly, upon convergence the distributed (non-convex) estimator provably\nattains the global optimum of its centralized counterpart, regardless of\ninitialization. Several application domains are outlined to highlight the\ngenerality and impact of the proposed framework. These include unveiling\ntraffic anomalies in backbone networks, predicting networkwide path latencies,\nand mapping the RF ambiance using wireless cognitive radios. Simulations with\nsynthetic and real network data corroborate the convergence of the novel\ndistributed algorithm, and its centralized performance guarantees. \n\n"}
{"id": "1203.5298", "contents": "Title: Dynamical fluctuations in a simple housing market model Abstract: We consider a simple stochastic model of a urban rental housing market, in\nwhich the interaction of tenants and landlords induces rent fluctuations. We\nsimulate the model numerically and measure the equilibrium rent distribution,\nwhich is found to be close to a lognormal law. We also study the influence of\nthe density of agents (or equivalently, the vacancy rate) on the rent\ndistribution. A simplified version of the model, amenable to analytical\ntreatment, is studied and leads to a lognormal distribution of rents. The\npredicted equilibrium value agrees quantitatively with numerical simulations,\nwhile a qualitative agreement is obtained for the standard deviation. The\nconnection with non-equilibrium statistical physics models like ratchets is\nalso emphasized. \n\n"}
{"id": "1204.3156", "contents": "Title: Price and Quantity Trajectories: Second-order Dynamics Abstract: In two previous papers the author developed a second-order price adjustment\n(t\\^atonnement) process. This paper extends the approach to include both\nquantity and price adjustments. We demonstrate three results: a analogue to\nphysical energy, called \"activity\" arises naturally in the model, and is not\nconserved in general; price and quantity trajectories must either end at a\nlocal minimum of a scalar potential or circulate endlessly; and disturbances\ninto a subspace of substitutable commodities decay over time. From this we\nargue, although we do not prove, that the model features global stability,\ncombined with local instability, a characteristic of many real markets.\nFollowing these observations and a brief survey of empirical results for\nprice-setting and consumption behavior in markets for \"real\" goods (as opposed\nto financial markets), we conjecture that Stigler and Becker's well-known\ntheory of consumer preference opens the possibility of substantial degeneracy\nin commodity space, and therefore that price and quantity trajectories could\nlie on a relatively low-dimensional subspace within the full commodity space. \n\n"}
{"id": "1204.4840", "contents": "Title: Energy-Delay Tradeoff and Dynamic Sleep Switching for Bluetooth-Like\n  Body-Area Sensor Networks Abstract: Wireless technology enables novel approaches to healthcare, in particular the\nremote monitoring of vital signs and other parameters indicative of people's\nhealth. This paper considers a system scenario relevant to such applications,\nwhere a smart-phone acts as a data-collecting hub, gathering data from a number\nof wireless-capable body sensors, and relaying them to a healthcare provider\nhost through standard existing cellular networks. Delay of critical data and\nsensors' energy efficiency are both relevant and conflicting issues. Therefore,\nit is important to operate the wireless body-area sensor network at some\ndesired point close to the optimal energy-delay tradeoff curve. This tradeoff\ncurve is a function of the employed physical-layer protocol: in particular, it\ndepends on the multiple-access scheme and on the coding and modulation schemes\navailable. In this work, we consider a protocol closely inspired by the\nwidely-used Bluetooth standard. First, we consider the calculation of the\nminimum energy function, i.e., the minimum sum energy per symbol that\nguarantees the stability of all transmission queues in the network. Then, we\napply the general theory developed by Neely to develop a dynamic scheduling\npolicy that approaches the optimal energy-delay tradeoff for the network at\nhand. Finally, we examine the queue dynamics and propose a novel policy that\nadaptively switches between connected and disconnected (sleeping) modes. We\ndemonstrate that the proposed policy can achieve significant gains in the\nrealistic case where the control \"NULL\" packets necessary to maintain the\nconnection alive, have a non-zero energy cost, and the data arrival statistics\ncorresponding to the sensed physical process are bursty. \n\n"}
{"id": "1204.5028", "contents": "Title: Regenerating Codes: A System Perspective Abstract: The explosion of the amount of data stored in cloud systems calls for more\nefficient paradigms for redundancy. While replication is widely used to ensure\ndata availability, erasure correcting codes provide a much better trade-off\nbetween storage and availability. Regenerating codes are good candidates for\nthey also offer low repair costs in term of network bandwidth. While they have\nbeen proven optimal, they are difficult to understand and parameterize. In this\npaper we provide an analysis of regenerating codes for practitioners to grasp\nthe various trade-offs. More specifically we make two contributions: (i) we\nstudy the impact of the parameters by conducting an analysis at the level of\nthe system, rather than at the level of a single device; (ii) we compare the\ncomputational costs of various implementations of codes and highlight the most\nefficient ones. Our goal is to provide system designers with concrete\ninformation to help them choose the best parameters and design for regenerating\ncodes. \n\n"}
{"id": "1204.5443", "contents": "Title: FIFO Queueing Policies for Packets with Heterogeneous Processing Abstract: We consider the problem of managing a bounded size First-In-First-Out (FIFO)\nqueue buffer, where each incoming unit-sized packet requires several rounds of\nprocessing before it can be transmitted out. Our objective is to maximize the\ntotal number of successfully transmitted packets. We consider both push-out\n(when the policy is permitted to drop already admitted packets) and\nnon-push-out cases. In particular, we provide analytical guarantees for the\nthroughput performance of our algorithms. We further conduct a comprehensive\nsimulation study which experimentally validates the predicted theoretical\nbehaviour. \n\n"}
{"id": "1205.3519", "contents": "Title: Restructuring the Italian NHS: a case study of the regional hospital\n  network Abstract: One of the main issues affecting the Italian NHS is the healthcare deficit:\naccording to current agreements between the Italian State and its Regions,\npublic funding of regional NHS is now limited to the amount of regional deficit\nand is subject to previous assessment of strict adherence to constraint on\nregional healthcare balance sheet. Many Regions with previously uncontrolled\nhealthcare deficit have now to plan their \"Piano di Rientro\" (PdR) and submit\nit for the approval of the Italian Ministry of Economy and Finances. Those\nRegions that will fail to comply to deficit constraints will suffer cuts on\ntheir public NHS financing. A smart Health Planning can make sure health\nspending is managed appropriately. Indeed a restructuring of the Italian\nhealthcare system has recently been enforced in order to cope for the clumsy\nregional healthcare balance sheets. Half of total Italian healthcare\nexpenditure is accounted by hospital services which therefore configure as one\nof the main restructuring targets. This paper provides a general framework for\nplanning a re-engineering of a hospital network. This framework is made of\neconomic, legal and healthcare constraints. We apply the general framework to\nthe particular case of Puglia region and explore a set of re-engineered\nsolutions which to different extent could help solve the difficult dilemma:\ncutting costs without worsening the delivery of public healthcare services. \n\n"}
{"id": "1205.3846", "contents": "Title: A Method for the Characterisation of Observer Effects and its\n  Application to OML Abstract: In all measurement campaigns, one needs to assert that the instrumentation\ntools do not significantly impact the system being monitored. This is critical\nto future claims based on the collected data and is sometimes overseen in\nexperimental studies. We propose a method to evaluate the potential \"observer\neffect\" of an instrumentation system, and apply it to the OMF Measurement\nLibrary (OML). OML allows the instrumentation of almost any software to collect\nany type of measurements. As it is increasingly being used in networking\nresearch, it is important to characterise possible biases it may introduce in\nthe collected metrics. Thus, we study its effect on multiple types of reports\nfrom various applications commonly used in wireless research. To this end, we\ndesigned experiments comparing OML-instrumented software with their original\nflavours. Our analyses of the results from these experiments show that, with an\nappropriate reporting setup, OML has no significant impact on the instrumented\napplications, and may even improve some of their performances in specifics\ncases. We discuss our methodology and the implication of using OML, and provide\nguidelines on instrumenting off-the-shelf software. \n\n"}
{"id": "1205.4589", "contents": "Title: Structural Hamiltonian of the international trade network Abstract: It is common wisdom that no nation is an isolated economic island. All\nnations participate in the global economy and are linked together through trade\nand finance. Here we analyze international trade network (ITN), being the\nnetwork of import-export relationships between countries. We show that in each\nyear over the analyzed period of 50 years (since 1950) the network is a typical\nrepresentative of the ensemble of maximally random networks. Structural\nHamiltonians characterizing binary and weighted versions of ITN are formulated\nand discussed. In particular, given binary representation of ITN (i.e. binary\nnetwork of trade channels) we show that the network of partnership in trade is\nwell described by the configuration model. We also show that in the weighted\nversion of ITN, bilateral trade volumes (i.e. directed connections which\nrepresent trade/money flows between countries) are only characterized by the\nproduct of the trading countries' GDPs, like in the famous gravity model of\ntrade. \n\n"}
{"id": "1205.4778", "contents": "Title: Backscatter from the Data Plane --- Threats to Stability and Security in\n  Information-Centric Networking Abstract: Information-centric networking proposals attract much attention in the\nongoing search for a future communication paradigm of the Internet. Replacing\nthe host-to-host connectivity by a data-oriented publish/subscribe service\neases content distribution and authentication by concept, while eliminating\nthreats from unwanted traffic at an end host as are common in today's Internet.\nHowever, current approaches to content routing heavily rely on data-driven\nprotocol events and thereby introduce a strong coupling of the control to the\ndata plane in the underlying routing infrastructure. In this paper, threats to\nthe stability and security of the content distribution system are analyzed in\ntheory and practical experiments. We derive relations between state resources\nand the performance of routers and demonstrate how this coupling can be misused\nin practice. We discuss new attack vectors present in its current state of\ndevelopment, as well as possibilities and limitations to mitigate them. \n\n"}
{"id": "1205.4856", "contents": "Title: Bounds on Minimum Number of Anchors for Iterative Localization and its\n  Connections to Bootstrap Percolation Abstract: Iterated localization is considered where each node of a network needs to get\nlocalized (find its location on 2-D plane), when initially only a subset of\nnodes have their location information. The iterated localization process\nproceeds as follows. Starting with a subset of nodes that have their location\ninformation, possibly using global positioning system (GPS) devices, any other\nnode gets localized if it has three or more localized nodes in its radio range.\nThe newly localized nodes are included in the subset of nodes that have their\nlocation information for the next iteration. This process is allowed to\ncontinue, until no new node can be localized. The problem is to find the\nminimum size of the initially localized subset to start with so that the whole\nnetwork is localized with high probability. There are intimate connections\nbetween iterated localization and bootstrap percolation, that is well studied\nin statistical physics. Using results known in bootstrap percolation, we find a\nsufficient condition on the size of the initially localized subset that\nguarantees the localization of all nodes in the network with high probability. \n\n"}
{"id": "1205.5662", "contents": "Title: Google+ or Google-?: Dissecting the Evolution of the New OSN in its\n  First Year Abstract: In the era when Facebook and Twitter dominate the market for social media,\nGoogle has introduced Google+ (G+) and reported a significant growth in its\nsize while others called it a ghost town. This begs the question that \"whether\nG+ can really attract a significant number of connected and active users\ndespite the dominance of Facebook and Twitter?\".\n  This paper tackles the above question by presenting a detailed\ncharacterization of G+ based on large scale measurements. We identify the main\ncomponents of G+ structure, characterize the key features of their users and\ntheir evolution over time. We then conduct detailed analysis on the evolution\nof connectivity and activity among users in the largest connected component\n(LCC) of G+ structure, and compare their characteristics with other major OSNs.\nWe show that despite the dramatic growth in the size of G+, the relative size\nof LCC has been decreasing and its connectivity has become less clustered.\nWhile the aggregate user activity has gradually increased, only a very small\nfraction of users exhibit any type of activity. To our knowledge, our study\noffers the most comprehensive characterization of G+ based on the largest\ncollected data sets. \n\n"}
{"id": "1206.1007", "contents": "Title: On the scaling ranges of detrended fluctuation analysis for long-memory\n  correlated short series of data Abstract: We examine the scaling regime for the detrended fluctuation analysis (DFA) -\nthe most popular method used to detect the presence of long memory in data and\nthe fractal structure of time series. First, the scaling range for DFA is\nstudied for uncorrelated data as a function of length $L$ of time series and\nregression line coefficient $R^2$ at various confidence levels. Next, an\nanalysis of artificial short series with long memory is performed. In both\ncases the scaling range $\\lambda$ is found to change linearly -- both with $L$\nand $R^2$. We show how this dependence can be generalized to a simple unified\nmodel describing the relation $\\lambda=\\lambda(L, R^2, H)$ where $H$ ($1/2\\leq\nH \\leq 1$) stands for the Hurst exponent of long range autocorrelated data. Our\nfindings should be useful in all applications of DFA technique, particularly\nfor instantaneous (local) DFA where enormous number of short time series has to\nbe examined at once, without possibility for preliminary check of the scaling\nrange of each series separately. \n\n"}
{"id": "1206.2153", "contents": "Title: The fine-structure of volatility feedback I: multi-scale\n  self-reflexivity Abstract: We attempt to unveil the fine structure of volatility feedback effects in the\ncontext of general quadratic autoregressive (QARCH) models, which assume that\ntoday's volatility can be expressed as a general quadratic form of the past\ndaily returns. The standard ARCH or GARCH framework is recovered when the\nquadratic kernel is diagonal. The calibration of these models on US stock\nreturns reveals several unexpected features. The off-diagonal (non ARCH)\ncoefficients of the quadratic kernel are found to be highly significant both\nIn-Sample and Out-of-Sample, but all these coefficients turn out to be one\norder of magnitude smaller than the diagonal elements. This confirms that daily\nreturns play a special role in the volatility feedback mechanism, as postulated\nby ARCH models. The feedback kernel exhibits a surprisingly complex structure,\nincompatible with models proposed so far in the literature. Its spectral\nproperties suggest the existence of volatility-neutral patterns of past\nreturns. The diagonal part of the quadratic kernel is found to decay as a\npower-law of the lag, in line with the long-memory of volatility. Finally,\nQARCH models suggest some violations of Time Reversal Symmetry in financial\ntime series, which are indeed observed empirically, although of much smaller\namplitude than predicted. We speculate that a faithful volatility model should\ninclude both ARCH feedback effects and a stochastic component. \n\n"}
{"id": "1206.2448", "contents": "Title: Pareto-optimal Nash equilibrium in capacity allocation game for\n  self-managed networks Abstract: In this paper we introduce a capacity allocation game which models the\nproblem of maximizing network utility from the perspective of distributed\nnoncooperative agents. Motivated by the idea of self-managed networks, in the\ndeveloped framework decision-making entities are associated with individual\ntransmission links, deciding on the way they split capacity among concurrent\nflows. An efficient decentralized algorithm is given for computing strongly\nPareto-optimal strategies, constituting a pure Nash equilibrium. Subsequently,\nwe discuss the properties of the introduced game related to the Price of\nAnarchy and Price of Stability. The paper is concluded with an experimental\nstudy. \n\n"}
{"id": "1206.5655", "contents": "Title: Path Selection for Quantum Repeater Networks Abstract: Quantum networks will support long-distance quantum key distribution (QKD)\nand distributed quantum computation, and are an active area of both\nexperimental and theoretical research. Here, we present an analysis of\ntopologically complex networks of quantum repeaters composed of heterogeneous\nlinks. Quantum networks have fundamental behavioral differences from classical\nnetworks; the delicacy of quantum states makes a practical path selection\nalgorithm imperative, but classical notions of resource utilization are not\ndirectly applicable, rendering known path selection mechanisms inadequate. To\nadapt Dijkstra's algorithm for quantum repeater networks that generate\nentangled Bell pairs, we quantify the key differences and define a link cost\nmetric, seconds per Bell pair of a particular fidelity, where a single Bell\npair is the resource consumed to perform one quantum teleportation. Simulations\nthat include both the physical interactions and the extensive classical\nmessaging confirm that Dijkstra's algorithm works well in a quantum context.\nSimulating about three hundred heterogeneous paths, comparing our path cost and\nthe total work along the path gives a coefficient of determination of 0.88 or\nbetter. \n\n"}
{"id": "1206.6302", "contents": "Title: Maximum Secondary Stable Throughput of a Cooperative Secondary\n  Transmitter-Receiver Pair: Protocol Design and Stability Analysis Abstract: In this paper, we investigate the impact of cooperation between a secondary\ntransmitter-receiver pair and a primary transmitter (PT) on the maximum stable\nthroughput of the primary-secondary network. Each transmitter, primary or\nsecondary, has a buffer for storing its own traffic. In addition to its own\nbuffer, the secondary transmitter (ST) has a buffer for storing a fraction of\nthe undelivered primary packets due to channel impairments. Moreover, the\nsecondary destination has a relaying queue for storing a fraction of the\nundelivered primary packets. In the proposed cooperative system, the ST and the\nsecondary destination increase the spectrum availability for the secondary\npackets by relaying the unsuccessfully transmitted packets of the PT. We\nconsider two multiple access strategies to be used by the ST and the secondary\ndestination to utilize the silence sessions of the PT. Numerical results\ndemonstrate the gains of the proposed cooperative system over the\nnon-cooperation case. \n\n"}
{"id": "1206.7111", "contents": "Title: Data Minimisation in Communication Protocols: A Formal Analysis\n  Framework and Application to Identity Management Abstract: With the growing amount of personal information exchanged over the Internet,\nprivacy is becoming more and more a concern for users. One of the key\nprinciples in protecting privacy is data minimisation. This principle requires\nthat only the minimum amount of information necessary to accomplish a certain\ngoal is collected and processed. \"Privacy-enhancing\" communication protocols\nhave been proposed to guarantee data minimisation in a wide range of\napplications. However, currently there is no satisfactory way to assess and\ncompare the privacy they offer in a precise way: existing analyses are either\ntoo informal and high-level, or specific for one particular system. In this\nwork, we propose a general formal framework to analyse and compare\ncommunication protocols with respect to privacy by data minimisation. Privacy\nrequirements are formalised independent of a particular protocol in terms of\nthe knowledge of (coalitions of) actors in a three-layer model of personal\ninformation. These requirements are then verified automatically for particular\nprotocols by computing this knowledge from a description of their\ncommunication. We validate our framework in an identity management (IdM) case\nstudy. As IdM systems are used more and more to satisfy the increasing need for\nreliable on-line identification and authentication, privacy is becoming an\nincreasingly critical issue. We use our framework to analyse and compare four\nidentity management systems. Finally, we discuss the completeness and\n(re)usability of the proposed framework. \n\n"}
{"id": "1207.0233", "contents": "Title: From characteristic functions to implied volatility expansions Abstract: For any strictly positive martingale $S = \\exp(X)$ for which $X$ has a\ncharacteristic function, we provide an expansion for the implied volatility.\nThis expansion is explicit in the sense that it involves no integrals, but only\npolynomials in the log strike. We illustrate the versatility of our expansion\nby computing the approximate implied volatility smile in three well-known\nmartingale models: one finite activity exponential L\\'evy model (Merton), one\ninfinite activity exponential L\\'evy model (Variance Gamma), and one stochastic\nvolatility model (Heston). Finally, we illustrate how our expansion can be used\nto perform a model-free calibration of the empirically observed implied\nvolatility surface. \n\n"}
{"id": "1207.0873", "contents": "Title: Hybrid performance modelling of opportunistic networks Abstract: We demonstrate the modelling of opportunistic networks using the process\nalgebra stochastic HYPE. Network traffic is modelled as continuous flows,\ncontact between nodes in the network is modelled stochastically, and\ninstantaneous decisions are modelled as discrete events. Our model describes a\nnetwork of stationary video sensors with a mobile ferry which collects data\nfrom the sensors and delivers it to the base station. We consider different\nmobility models and different buffer sizes for the ferries. This case study\nillustrates the flexibility and expressive power of stochastic HYPE. We also\ndiscuss the software that enables us to describe stochastic HYPE models and\nsimulate them. \n\n"}
{"id": "1207.1630", "contents": "Title: The Smile of certain L\\'evy-type Models Abstract: We consider a class of assets whose risk-neutral pricing dynamics are\ndescribed by an exponential L\\'evy-type process subject to default. The class\nof processes we consider features locally-dependent drift, diffusion and\ndefault-intensity as well as a locally-dependent L\\'evy measure. Using\ntechniques from regular perturbation theory and Fourier analysis, we derive a\nseries expansion for the price of a European-style option. We also provide\nprecise conditions under which this series expansion converges to the exact\nprice. Additionally, for a certain subclass of assets in our modeling\nframework, we derive an expansion for the implied volatility induced by our\noption pricing formula. The implied volatility expansion is exact within its\nradius of convergence. As an example of our framework, we propose a class of\nCEV-like L\\'evy-type models. Within this class, approximate option prices can\nbe computed by a single Fourier integral and approximate implied volatilities\nare explicit (i.e., no integration is required). Furthermore, the class of\nCEV-like L\\'evy-type models is shown to provide a tight fit to the implied\nvolatility surface of S{&}P500 index options. \n\n"}
{"id": "1207.2010", "contents": "Title: Existence of Financial Equilibria in Continuous Time with Potentially\n  Complete Markets Abstract: We prove that in smooth Markovian continuous-time economies with potentially\ncomplete asset markets, Radner equilibria with endogenously complete markets\nexist. \n\n"}
{"id": "1207.2825", "contents": "Title: Guard Zones and the Near-Far Problem in DS-CDMA Ad Hoc Networks Abstract: The central issue in direct-sequence code-division multiple-access (DS-CDMA)\nad hoc networks is the prevention of a near-far problem. This paper considers\ntwo types of guard zones that may be used to control the near-far problem: a\nfundamental exclusion zone and an additional CSMA guard zone that may be\nestablished by the carrier-sense multiple-access (CSMA) protocol. In the\nexclusion zone, no mobiles are physically present, modeling the minimum\nphysical separation among mobiles that is always present in actual networks.\nPotentially interfering mobiles beyond a transmitting mobile's exclusion zone,\nbut within its CSMA guard zone, are deactivated by the protocol. This paper\nprovides an analysis of DS-CSMA networks with either or both types of guard\nzones. A network of finite extent with a finite number of mobiles is modeled as\na uniform clustering process. The analysis uses a closed-form expression for\nthe outage probability in the presence of Nakagami fading, conditioned on the\nnetwork geometry. By using the analysis developed in this paper, the tradeoffs\nbetween exclusion zones and CSMA guard zones are explored for DS-CDMA and\nunspread networks. \n\n"}
{"id": "1207.4028", "contents": "Title: Signal processing with Levy information Abstract: Levy processes, which have stationary independent increments, are ideal for\nmodelling the various types of noise that can arise in communication channels.\nIf a Levy process admits exponential moments, then there exists a parametric\nfamily of measure changes called Esscher transformations. If the parameter is\nreplaced with an independent random variable, the true value of which\nrepresents a \"message\", then under the transformed measure the original Levy\nprocess takes on the character of an \"information process\". In this paper we\ndevelop a theory of such Levy information processes. The underlying Levy\nprocess, which we call the fiducial process, represents the \"noise type\". Each\nsuch noise type is capable of carrying a message of a certain specification. A\nnumber of examples are worked out in detail, including information processes of\nthe Brownian, Poisson, gamma, variance gamma, negative binomial, inverse\nGaussian, and normal inverse Gaussian type. Although in general there is no\nadditive decomposition of information into signal and noise, one is led\nnevertheless for each noise type to a well-defined scheme for signal detection\nand enhancement relevant to a variety of practical situations. \n\n"}
{"id": "1207.5708", "contents": "Title: Improved Interference in Wireless Sensor Networks Abstract: Given a set ${\\cal V}$ of $n$ sensor node distributed on a 2-dimensional\nplane and a source node $s \\in {\\cal V}$, the {\\it interference problem} deals\nwith assigning transmission range to each $v \\in {\\cal V}$ such that the\nmembers in ${\\cal V}$ maintain connectivity predicate ${\\cal P}$, and the\nmaximum/total interference is minimum. We propose algorithm for both {\\it\nminimizing maximum interference} and {\\it minimizing total interference} of the\nnetworks. For minimizing maximum interference we present optimum solution with\nrunning time $O(({\\cal P}_n + n^2) \\log n)$ for connectivity predicate ${\\cal\nP}$ like strong connectivity, broadcast ($s$ is the source), $k$-edge(vertex)\nconnectivity, spanner, where $O({\\cal P}_n)$ is the time complexity for\nchecking the connectivity predicate ${\\cal P}$. The running time of the\nprevious best known solution was $O({\\cal P}_n \\times n^2)$ [Bil$\\grave{o}$ and\nProietti, 2008].\n  For the minimizing total interference we propose optimum algorithm for the\nconnectivity predicate broadcast. The running time of the propose algorithm is\nO(n). For the same problem, the previous best known result was $2(1 + \\ln\n(n-1))$-factor approximation algorithm [Bil$\\grave{o}$ and Proietti, 2008]. We\nalso propose a heuristic for minimizing total interference in the case of\nstrongly connected predicate and compare our result with the best result\navailable in the literature. Experimental results demonstrate that our\nheuristic outperform existing result. \n\n"}
{"id": "1208.1149", "contents": "Title: Uncertainty-dependent data collection in vehicular sensor networks Abstract: Vehicular sensor networks (VSNs) are built on top of vehicular ad-hoc\nnetworks (VANETs) by equipping vehicles with sensing devices. These new\ntechnologies create a huge opportunity to extend the sensing capabilities of\nthe existing road traffic control systems and improve their performance.\nEfficient utilisation of wireless communication channel is one of the basic\nissues in the vehicular networks development. This paper presents and evaluates\ndata collection algorithms that use uncertainty estimates to reduce data\ntransmission in a VSN-based road traffic control system. \n\n"}
{"id": "1208.3212", "contents": "Title: Modeling Network Coded TCP: Analysis of Throughput and Energy Cost Abstract: We analyze the performance of TCP and TCP with network coding (TCP/NC) in\nlossy networks. We build upon the framework introduced by Padhye et al. and\ncharacterize the throughput behavior of classical TCP and TCP/NC as a function\nof erasure probability, round-trip time, maximum window size, and duration of\nthe connection. Our analytical results show that network coding masks random\nerasures from TCP, thus preventing TCP's performance degradation in lossy\nnetworks. It is further seen that TCP/NC has significant throughput gains over\nTCP.\n  In addition, we show that TCP/NC may lead to cost reduction for wireless\nnetwork providers while maintaining a certain quality of service to their\nusers. We measure the cost in terms of number of base stations, which is highly\ncorrelated to the energy, capital, and operational costs of a network provider.\nWe show that increasing the available bandwidth may not necessarily lead to\nincrease in throughput, particularly in lossy networks in which TCP does not\nperform well. We show that using protocols such as TCP/NC, which are more\nresilient to erasures, may lead to a throughput commensurate the bandwidth\ndedicated to each user. \n\n"}
{"id": "1208.3789", "contents": "Title: On Global Stability of Financial Networks Abstract: The recent financial crisis have generated renewed interests in fragilities\nof global financial networks among economists and regulatory authorities. In\nparticular, a potential vulnerability of the financial networks is the\n\"financial contagion\" process in which insolvencies of individual entities\npropagate through the \"web of dependencies\" to affect the entire system. In\nthis paper, we formalize an extension of a financial network model originally\nproposed by Nier et al. for scenarios such as the OTC derivatives market,\ndefine a suitable global stability measure for this model, and perform a\ncomprehensive empirical evaluation of this stability measure over more than\n700,000 combinations of networks types and parameter combinations. Based on our\nevaluations, we discover many interesting implications of our evaluations of\nthis stability measure, and derive topological properties and parameters\ncombinations that may be used to flag the network as a possible fragile\nnetwork. An interactive software FIN-STAB for computing the stability is\navailable from the website www2.cs.uic.edu/~dasgupta/financial-simulator-files \n\n"}
{"id": "1208.3994", "contents": "Title: Coordination in Network Security Games: a Monotone Comparative Statics\n  Approach Abstract: Malicious softwares or malwares for short have become a major security\nthreat. While originating in criminal behavior, their impact are also\ninfluenced by the decisions of legitimate end users. Getting agents in the\nInternet, and in networks in general, to invest in and deploy security features\nand protocols is a challenge, in particular because of economic reasons arising\nfrom the presence of network externalities.\n  In this paper, we focus on the question of incentive alignment for agents of\na large network towards a better security. We start with an economic model for\na single agent, that determines the optimal amount to invest in protection. The\nmodel takes into account the vulnerability of the agent to a security breach\nand the potential loss if a security breach occurs. We derive conditions on the\nquality of the protection to ensure that the optimal amount spent on security\nis an increasing function of the agent's vulnerability and potential loss. We\nalso show that for a large class of risks, only a small fraction of the\nexpected loss should be invested.\n  Building on these results, we study a network of interconnected agents\nsubject to epidemic risks. We derive conditions to ensure that the incentives\nof all agents are aligned towards a better security. When agents are strategic,\nwe show that security investments are always socially inefficient due to the\nnetwork externalities. Moreover alignment of incentives typically implies a\ncoordination problem, leading to an equilibrium with a very high price of\nanarchy. \n\n"}
{"id": "1208.4043", "contents": "Title: Dynamic Anomalography: Tracking Network Anomalies via Sparsity and Low\n  Rank Abstract: In the backbone of large-scale networks, origin-to-destination (OD) traffic\nflows experience abrupt unusual changes known as traffic volume anomalies,\nwhich can result in congestion and limit the extent to which end-user quality\nof service requirements are met. As a means of maintaining seamless end-user\nexperience in dynamic environments, as well as for ensuring network security,\nthis paper deals with a crucial network monitoring task termed dynamic\nanomalography. Given link traffic measurements (noisy superpositions of\nunobserved OD flows) periodically acquired by backbone routers, the goal is to\nconstruct an estimated map of anomalies in real time, and thus summarize the\nnetwork `health state' along both the flow and time dimensions. Leveraging the\nlow intrinsic-dimensionality of OD flows and the sparse nature of anomalies, a\nnovel online estimator is proposed based on an exponentially-weighted\nleast-squares criterion regularized with the sparsity-promoting $\\ell_1$-norm\nof the anomalies, and the nuclear norm of the nominal traffic matrix. After\nrecasting the non-separable nuclear norm into a form amenable to online\noptimization, a real-time algorithm for dynamic anomalography is developed and\nits convergence established under simplifying technical assumptions. For\noperational conditions where computational complexity reductions are at a\npremium, a lightweight stochastic gradient algorithm based on Nesterov's\nacceleration technique is developed as well. Comprehensive numerical tests with\nboth synthetic and real network data corroborate the effectiveness of the\nproposed online algorithms and their tracking capabilities, and demonstrate\nthat they outperform state-of-the-art approaches developed to diagnose traffic\nanomalies. \n\n"}
{"id": "1208.4409", "contents": "Title: Yard-Sale exchange on networks: Wealth sharing and wealth appropriation Abstract: Yard-Sale (YS) is a stochastic multiplicative wealth-exchange model with two\nphases: a stable one where wealth is shared, and an unstable one where wealth\ncondenses onto one agent. YS is here studied numerically on 1d rings, 2d square\nlattices, and random graphs with variable average coordination, comparing its\nproperties with those in mean field (MF). Equilibrium properties in the stable\nphase are almost unaffected by the introduction of a network. Measurement of\ndecorrelation times in the stable phase allow us to determine the critical\ninterface with very good precision, and it turns out to be the same, for all\nnetworks analyzed, as the one that can be analytically derived in MF. In the\nunstable phase, on the other hand, dynamical as well as asymptotic properties\nare strongly network-dependent. Wealth no longer condenses on a single agent,\nas in MF, but onto an extensive set of agents, the properties of which depend\non the network. Connections with previous studies of coalescence of immobile\nreactants are discussed, and their analytic predictions are successfully\ncompared with our numerical results. \n\n"}
{"id": "1208.5738", "contents": "Title: Efficient Construction of Dominating Set in Wireless Networks Abstract: Considering a communication topology of a wireless network modeled by a graph\nwhere an edge exists between two nodes if they are within each other's\ncommunication range. A subset $U$ of nodes is a dominating set if each node is\neither in $U$ or adjacent to some node in $U$. Assume each node has a disparate\ncommunication range and is associated with a positive weight, we present a\nrandomized algorithm to find a min-weight dominating set. Considering any\norientation of the graph where an arc $\\overrightarrow{uv}$ exists if the node\n$v$ lies in $u$'s communication range. A subset $U$ of nodes is a strongly\ndominating set if every node except $U$ has both in-neighbor(s) and\nout-neighbor(s) in $U$. We present a polynomial-time algorithm to find a\nstrongly dominating set of size at most $(2+\\epsilon)$ times of the optimum. We\nalso investigate another related problem called $K$-Coverage. Given are a set\n${\\cal D}$ of disks with positive weight and a set ${\\cal P}$ of nodes. Assume\nall input nodes lie below a horizontal line $l$ and all input disks lie above\nthis line $l$ in the plane. The objective is to find a min-weight subset ${\\cal\nD}'\\subseteq {\\cal D}$ of disks such that each node is covered at least $K$\ndisks in ${\\cal D}'$. We propose a novel two-approximation algorithm for this\nproblem. \n\n"}
{"id": "1209.0453", "contents": "Title: Crises and collective socio-economic phenomena: simple models and\n  challenges Abstract: Financial and economic history is strewn with bubbles and crashes, booms and\nbusts, crises and upheavals of all sorts. Understanding the origin of these\nevents is arguably one of the most important problems in economic theory. In\nthis paper, we review recent efforts to include heterogeneities and\ninteractions in models of decision. We argue that the Random Field Ising model\n(RFIM) indeed provides a unifying framework to account for many collective\nsocio-economic phenomena that lead to sudden ruptures and crises. We discuss\ndifferent models that can capture potentially destabilising self-referential\nfeedback loops, induced either by herding, i.e. reference to peers, or\ntrending, i.e. reference to the past, and account for some of the phenomenology\nmissing in the standard models. We discuss some empirically testable\npredictions of these models, for example robust signatures of RFIM-like herding\neffects, or the logarithmic decay of spatial correlations of voting patterns.\nOne of the most striking result, inspired by statistical physics methods, is\nthat Adam Smith's invisible hand can badly fail at solving simple coordination\nproblems. We also insist on the issue of time-scales, that can be extremely\nlong in some cases, and prevent socially optimal equilibria to be reached. As a\ntheoretical challenge, the study of so-called \"detailed-balance\" violating\ndecision rules is needed to decide whether conclusions based on current models\n(that all assume detailed-balance) are indeed robust and generic. \n\n"}
{"id": "1209.0708", "contents": "Title: On the global economic potentials and marginal costs of non-renewable\n  resources and the price of energy commodities Abstract: A model is presented in this work for simulating endogenously the evolution\nof the marginal costs of production of energy carriers from non-renewable\nresources, their consumption, depletion pathways and timescales. Such marginal\ncosts can be used to simulate the long term average price formation of energy\ncommodities. Drawing on previous work where a global database of energy\nresource economic potentials was constructed, this work uses cost distributions\nof non-renewable resources in order to evaluate global flows of energy\ncommodities. A mathematical framework is given to calculate endogenous flows of\nenergy resources given an exogenous commodity price path. This framework can be\nused in reverse in order to calculate an exogenous marginal cost of production\nof energy carriers given an exogenous carrier demand. Using rigid price\ninelastic assumptions independent of the economy, these two approaches generate\nlimiting scenarios that depict extreme use of natural resources. This is useful\nto characterise the current state and possible uses of remaining non-renewable\nresources such as fossil fuels and natural uranium. The theory is however\ndesigned for use within economic or technology models that allow technology\nsubstitutions. In this work, it is implemented in the global power sector model\nFTT:Power. Policy implications are given. \n\n"}
{"id": "1209.1321", "contents": "Title: Entanglement between Demand and Supply in Markets with Bandwagon Goods Abstract: Whenever customers' choices (e.g. to buy or not a given good) depend on\nothers choices (cases coined 'positive externalities' or 'bandwagon effect' in\nthe economic literature), the demand may be multiply valued: for a same posted\nprice, there is either a small number of buyers, or a large one -- in which\ncase one says that the customers coordinate. This leads to a dilemma for the\nseller: should he sell at a high price, targeting a small number of buyers, or\nat low price targeting a large number of buyers? In this paper we show that the\ninteraction between demand and supply is even more complex than expected,\nleading to what we call the curse of coordination: the pricing strategy for the\nseller which aimed at maximizing his profit corresponds to posting a price\nwhich, not only assumes that the customers will coordinate, but also lies very\nnear the critical price value at which such high demand no more exists. This is\nobtained by the detailed mathematical analysis of a particular model formally\nrelated to the Random Field Ising Model and to a model introduced in social\nsciences by T C Schelling in the 70's. \n\n"}
{"id": "1209.5604", "contents": "Title: Tail Probabilities in Queueing Processes Abstract: In the study of large scale stochastic networks with resource management,\ndifferential equations and mean-field limits are two key techniques. Recent\nresearch shows that the expected fraction vector (that is, the tailed\nprobability vector) plays a key role in setting up mean-field differential\nequations. To further apply the technique of tailed probability vector to deal\nwith resource management of large scale stochastic networks, this paper\ndiscusses tailed probabilities in some basic queueing processes including QBD\nprocesses, Markov chains of GI/M/1 type and of M/G/1 type, and also provides\nsome effective and efficient algorithms for computing the tailed probabilities\nby means of the matrix-geometric solution, the matrix-iterative solution, the\nmatrix-product solution and the two types of RG-factorizations. Furthermore, we\nconsider four queueing examples: The M/M/1 retrial queue, the M(n)/M(n)/1\nqueue, the M/M/1 queue with server multiple vacations and the M/M/1 queue with\nrepairable server, where the M/M/1 retrial queue is given a detailed\ndiscussion, while the other three examples are analyzed simply. Note that the\nresults given in this paper will be very useful in the study of large scale\nstochastic networks with resource management, including the supermarket models\nand the work stealing models. \n\n"}
{"id": "1210.5031", "contents": "Title: Semi-Definite Programming Relaxation for Non-Line-of-Sight Localization Abstract: We consider the problem of estimating the locations of a set of points in a\nk-dimensional euclidean space given a subset of the pairwise distance\nmeasurements between the points. We focus on the case when some fraction of\nthese measurements can be arbitrarily corrupted by large additive noise. Given\nthat the problem is highly non-convex, we propose a simple semidefinite\nprogramming relaxation that can be efficiently solved using standard\nalgorithms. We define a notion of non-contractibility and show that the\nrelaxation gives the exact point locations when the underlying graph is\nnon-contractible. The performance of the algorithm is evaluated on an\nexperimental data set obtained from a network of 44 nodes in an indoor\nenvironment and is shown to be robust to non-line-of-sight errors. \n\n"}
{"id": "1210.5390", "contents": "Title: Ethics and Finance: the role of mathematics Abstract: This paper presents the contemporary Fundamental Theorem of Asset Pricing as\nbeing equivalent to approaches to pricing that emerged before 1700 in the\ncontext of Virtue Ethics. This is done by considering the history of science\nand mathematics in the thirteenth and seventeenth century. An explanation as to\nwhy these approaches to pricing were forgotten between 1700 and 2000 is given,\nalong with some of the implications on economics of viewing the Fundamental\nTheorem as a product of Virtue Ethics.\n  The Fundamental Theorem was developed in mathematics to establish a `theory'\nthat underpinned the Black-Scholes-Merton approach to pricing derivatives. In\ndoing this, the Fundamental Theorem unified a number of different approaches in\nfinancial economics, this strengthened the status of neo-classical economics\nbased on Consequentialist Ethics. We present an alternative to this narrative. \n\n"}
{"id": "1210.7468", "contents": "Title: Link Scheduling in Amplify-and-Forward Cooperative Wireless Networks Abstract: In this work we are concerned with the problem of link scheduling for\nthroughput maximization in wireless networks that employ a cooperative amplify\nand forward (AF) protocol. To address this problem first we define the\nsignal-to-interference plus noise ratio (SINR) expression for the complete\ncooperative AF-based transmission. Next, we formulate the problem of link\nscheduling as a mixed integer linear program (MILP) that uses as a constraint\nthe developed SINR expression. The proposed formulation is motivated by the\nobservation that the aggregate interference that affects a single cooperative\ntransmission can be decomposed into two separate SINR constraints. Results for\nthe optimal solution and a polynomial time approximation algorithm are also\npresented. \n\n"}
{"id": "1210.8433", "contents": "Title: Green Cellular Wireless Networks: Where to Begin? Abstract: Conventional cellular wireless networks were designed with the purpose of\nproviding high throughput for the user and high capacity for the service\nprovider, without any provisions of energy efficiency. As a result, these\nnetworks have an enormous Carbon footprint. In this note, we describe the\nsources of the inefficiencies in such networks. First we quantify how much\nCarbon footprint such networks generate. We also discuss how much more mobile\ntraffic is expected to increase so that this Carbon footprint will even\nincrease tremendously more. We then discuss specific sources of inefficiency\nand potential sources of improvement at the physical layer as well as higher\nlayers of the communication protocol hierarchy. In particular, considering that\nmost of the energy inefficiency in wireless cellular networks is at the base\nstations, we discuss multi-tier networks and point to the potential of\nexploiting mobility patterns in order to use base station energy judiciously. \n\n"}
{"id": "1211.4041", "contents": "Title: Modeling, Analysis and Design for Carrier Aggregation in Heterogeneous\n  Cellular Networks Abstract: Carrier aggregation (CA) and small cells are two distinct features of\nnext-generation cellular networks. Cellular networks with small cells take on a\nvery heterogeneous characteristic, and are often referred to as HetNets. In\nthis paper, we introduce a load-aware model for CA-enabled \\textit{multi}-band\nHetNets. Under this model, the impact of biasing can be more appropriately\ncharacterized; for example, it is observed that with large enough biasing, the\nspectral efficiency of small cells may increase while its counterpart in a\nfully-loaded model always decreases. Further, our analysis reveals that the\npeak data rate does not depend on the base station density and transmit powers;\nthis strongly motivates other approaches e.g. CA to increase the peak data\nrate. Last but not least, different band deployment configurations are studied\nand compared. We find that with large enough small cell density, spatial reuse\nwith small cells outperforms adding more spectrum for increasing user rate.\nMore generally, universal cochannel deployment typically yields the largest\nrate; and thus a capacity loss exists in orthogonal deployment. This\nperformance gap can be reduced by appropriately tuning the HetNet coverage\ndistribution (e.g. by optimizing biasing factors). \n\n"}
{"id": "1211.4598", "contents": "Title: How Non-Arbitrage, Viability and Num\\'eraire Portfolio are Related Abstract: This paper proposes two approaches that quantify the exact relationship among\nthe viability, the absence of arbitrage, and/or the existence of the\nnum\\'eraire portfolio under minimal assumptions and for general continuous-time\nmarket models. Precisely, our first and principal contribution proves the\nequivalence among the No-Unbounded-Profit-with-Bounded-Risk condition (NUPBR\nhereafter), the existence of the num\\'eraire portfolio, and the existence of\nthe optimal portfolio under an equivalent probability measure for any \"nice\"\nutility and positive initial capital. Herein, a 'nice\" utility is any smooth\nvon Neumann-Morgenstern utility satisfying Inada's conditions and the\nelasticity assumptions of Kramkov and Schachermayer. Furthermore, the\nequivalent probability measure ---under which the utility maximization problems\nhave solutions--- can be chosen as close to the real-world probability measure\nas we want (but might not be equal). Without changing the underlying\nprobability measure and under mild assumptions, our second contribution proves\nthat the NUPBR is equivalent to the \"{\\it local}\" existence of the optimal\nportfolio. This constitutes an alternative to the first contribution, if one\ninsists on working under the real-world probability. These two contributions\nlead naturally to new types of viability that we call weak and local\nviabilities. \n\n"}
{"id": "1211.6255", "contents": "Title: Keyhole and Reflection Effects in Network Connectivity Analysis Abstract: Recent research has demonstrated the importance of boundary effects on the\noverall connection probability of wireless networks, but has largely focused on\nconvex domains. We consider two generic scenarios of practical importance to\nwireless communications, in which one or more nodes are located outside the\nconvex space where the remaining nodes reside. Consequently, conventional\napproaches with the underlying assumption that only line-of-sight (LOS) or\ndirect connections between nodes are possible, fail to provide the correct\nanalysis for the connectivity. We present an analytical framework that\nexplicitly considers the effects of reflections from the system boundaries on\nthe full connection probability. This study provides a different strategy to\nray tracing tools for predicting the wireless propagation environment. A simple\ntwo-dimensional geometry is first considered, followed by a more practical\nthree-dimensional system. We investigate the effects of different system\nparameters on the connectivity of the network though analysis corroborated by\nnumerical simulations, and highlight the potential of our approach for more\ngeneral non-convex geometries.t system parameters on the connectivity of the\nnetwork through simulation and analysis. \n\n"}
{"id": "1211.6517", "contents": "Title: Momentum universe shrinkage effect in price momentum Abstract: We test the price momentum effect in the Korean stock markets under the\nmomentum universe shrinkage to subuniverses of the KOSPI 200. Performance of\nthe momentum strategy is not homogeneous with respect to change of the momentum\nuniverse. It is found that some submarkets generate the higher momentum returns\nthan other universes do but large-size companies such as the KOSPI 50\ncomponents hinder the performance of the momentum strategy. The observation is\nalso cross-checked with size portfolios and liquidity portfolios. Transactions\nby investor groups, in particular, the trading patterns by foreign investors\ncan be a source of the momentum universe shrinkage effect in the momentum\nreturns. \n\n"}
{"id": "1211.6950", "contents": "Title: Dynamic Network Cartography Abstract: Communication networks have evolved from specialized, research and tactical\ntransmission systems to large-scale and highly complex interconnections of\nintelligent devices, increasingly becoming more commercial, consumer-oriented,\nand heterogeneous. Propelled by emergent social networking services and\nhigh-definition streaming platforms, network traffic has grown explosively\nthanks to the advances in processing speed and storage capacity of\nstate-of-the-art communication technologies. As \"netizens\" demand a seamless\nnetworking experience that entails not only higher speeds, but also resilience\nand robustness to failures and malicious cyber-attacks, ample opportunities for\nsignal processing (SP) research arise. The vision is for ubiquitous smart\nnetwork devices to enable data-driven statistical learning algorithms for\ndistributed, robust, and online network operation and management, adaptable to\nthe dynamically-evolving network landscape with minimal need for human\nintervention. The present paper aims at delineating the analytical background\nand the relevance of SP tools to dynamic network monitoring, introducing the SP\nreadership to the concept of dynamic network cartography -- a framework to\nconstruct maps of the dynamic network state in an efficient and scalable manner\ntailored to large-scale heterogeneous networks. \n\n"}
{"id": "1211.6988", "contents": "Title: Simultaneous Distributed Sensor Self-Localization and Target Tracking\n  Using Belief Propagation and Likelihood Consensus Abstract: We introduce the framework of cooperative simultaneous localization and\ntracking (CoSLAT), which provides a consistent combination of cooperative\nself-localization (CSL) and distributed target tracking (DTT) in sensor\nnetworks without a fusion center. CoSLAT extends simultaneous localization and\ntracking (SLAT) in that it uses also intersensor measurements. Starting from a\nfactor graph formulation of the CoSLAT problem, we develop a particle-based,\ndistributed message passing algorithm for CoSLAT that combines nonparametric\nbelief propagation with the likelihood consensus scheme. The proposed CoSLAT\nalgorithm improves on state-of-the-art CSL and DTT algorithms by exchanging\nprobabilistic information between CSL and DTT. Simulation results demonstrate\nsubstantial improvements in both self-localization and tracking performance. \n\n"}
{"id": "1212.0075", "contents": "Title: Optimal Power Allocation for Outage Minimization in Fading Channels with\n  Energy Harvesting Constraints Abstract: This paper studies the optimal power allocation for outage minimization in\npoint-to-point fading channels with the energy-harvesting constraints and\nchannel distribution information (CDI) at the transmitter. Both the cases with\nnon-causal and causal energy state information (ESI) are considered, which\ncorrespond to the energy harvesting rates being known and unknown prior to the\ntransmissions, respectively. For the non-causal ESI case, the average outage\nprobability minimization problem over a finite horizon is shown to be\nnon-convex for a large class of practical fading channels. However, the\nglobally optimal \"offline\" power allocation is obtained by a forward search\nalgorithm with at most $N$ one-dimensional searches, and the optimal power\nprofile is shown to be non-decreasing over time and have an interesting\n\"save-then-transmit\" structure. In particular, for the special case of N=1, our\nresult revisits the classic outage capacity for fading channels with uniform\npower allocation. Moreover, for the case with causal ESI, we propose both the\noptimal and suboptimal \"online\" power allocation algorithms, by applying the\ntechnique of dynamic programming and exploring the structure of optimal offline\nsolutions, respectively. \n\n"}
{"id": "1212.0724", "contents": "Title: A Potential Game for Power and Frequency Allocation in Large-Scale\n  Wireless Networks Abstract: In this paper we analyze power and frequency allocation in wireless networks\nthrough potential games. Potential games are used frequently in the literature\nfor this purpose due to their desirable properties, such as convergence and\nstability. However, potential games usually assume massive message passing to\nobtain the necessary neighbor information at each user to achieve these\nproperties. In this paper we show an example of a game where we are able to\ncharacterize the necessary neighbor information in order to show that the game\nhas a potential function and the properties of potential games. We consider a\nnetwork consisting of local access points where the goal of each AP is to\nallocate power and frequency to achieve some SINR requirement. We use the\nphysical SINR model to validate a successful allocation, and show that given a\nsuitable payoff function the game emits a generalized ordinal potential\nfunction under the assumption of sufficient neighbor information. Through\nsimulations we evaluate the performance of the proposed game on a large scale\nin relation to the amount of information at each AP. \n\n"}
{"id": "1212.4915", "contents": "Title: Can P2P Technology Benefit Eyeball ISPs? A Cooperative Profit\n  Distribution Answer Abstract: Peer-to-Peer (P2P) technology has been regarded as a promising way to help\nContent Providers (CPs) cost-effectively distribute content. However, under the\ntraditional Internet pricing mechanism, the fact that most P2P traffic flows\namong peers can dramatically decrease the profit of ISPs, who may take actions\nagainst P2P and impede the progress of P2P technology. In this paper, we\ndevelop a mathematical framework to analyze such economic issues. Inspired by\nthe idea from cooperative game theory, we propose a cooperative\nprofit-distribution model based on Nash Bargaining Solution (NBS), in which\neyeball ISPs and Peer-assisted CPs (PCPs) form two coalitions respectively and\nthen compute a fair Pareto point to determine profit distribution. Moreover, we\ndesign a fair and feasible mechanism for profit distribution within each\ncoalition. We show that such a cooperative method not only guarantees the fair\nprofit distribution among network participators, but also helps to improve the\neconomic efficiency of the overall network system. To our knowledge, this is\nthe first work that systematically studies solutions for P2P caused unbalanced\nprofit distribution and gives a feasible cooperative method to increase and\nfairly share profit. \n\n"}
{"id": "1301.0907", "contents": "Title: On a dynamic adaptation of the Distribution Builder approach to\n  investment decisions Abstract: Sharpe et al. proposed the idea of having an expected utility maximizer\nchoose a probability distribution for future wealth as an input to her\ninvestment problem instead of a utility function. They developed a computer\nprogram, called The Distribution Builder, as one way to elicit such a\ndistribution. In a single-period model, they then showed how this desired\ndistribution for terminal wealth can be used to infer the investor's risk\npreferences. We adapt their idea, namely that a risk-averse investor can choose\na desired distribution for future wealth as an alternative input attribute for\ninvestment decisions, to continuous time. In a variety of scenarios, we show\nhow the investor's desired distribution combines with her initial wealth and\nmarket-related input to determine the feasibility of her distribution, her\nimplied risk preferences, and her optimal policies throughout her investment\nhorizon. We then provide several examples. \n\n"}
{"id": "1301.1294", "contents": "Title: FAST CLOUD: Pushing the Envelope on Delay Performance of Cloud Storage\n  with Coding Abstract: Our paper presents solutions that can significantly improve the delay\nperformance of putting and retrieving data in and out of cloud storage. We\nfirst focus on measuring the delay performance of a very popular cloud storage\nservice Amazon S3. We establish that there is significant randomness in service\ntimes for reading and writing small and medium size objects when assigned\ndistinct keys. We further demonstrate that using erasure coding, parallel\nconnections to storage cloud and limited chunking (i.e., dividing the object\ninto a few smaller objects) together pushes the envelope on service time\ndistributions significantly (e.g., 76%, 80%, and 85% reductions in mean, 90th,\nand 99th percentiles for 2 Mbyte files) at the expense of additional storage\n(e.g., 1.75x). However, chunking and erasure coding increase the load and hence\nthe queuing delays while reducing the supportable rate region in number of\nrequests per second per node. Thus, in the second part of our paper we focus on\nanalyzing the delay performance when chunking, FEC, and parallel connections\nare used together. Based on this analysis, we develop load adaptive algorithms\nthat can pick the best code rate on a per request basis by using off-line\ncomputed queue backlog thresholds. The solutions work with homogeneous services\nwith fixed object sizes, chunk sizes, operation type (e.g., read or write) as\nwell as heterogeneous services with mixture of object sizes, chunk sizes, and\noperation types. We also present a simple greedy solution that\nopportunistically uses idle connections and picks the erasure coding rate\naccordingly on the fly. Both backlog and greedy solutions support the full rate\nregion and provide best mean delay performance when compared to the best fixed\ncoding rate policy. Our evaluations show that backlog based solutions achieve\nbetter delay performance at higher percentile values than the greedy solution. \n\n"}
{"id": "1301.1746", "contents": "Title: Generalized Secure Transmission Protocol for Flexible Load-Balance\n  Control with Cooperative Relays in Two-Hop Wireless Networks Abstract: This work considers secure transmission protocol for flexible load-balance\ncontrol in two-hop relay wireless networks without the information of both\neavesdropper channels and locations. The available secure transmission\nprotocols via relay cooperation in physical layer secrecy framework cannot\nprovide a flexible load-balance control, which may significantly limit their\napplication scopes. This paper extends the conventional works and proposes a\ngeneral transmission protocol with considering load-balance control, in which\nthe relay is randomly selected from the first $k$ preferable assistant relays\nlocated in the circle area with the radius $r$ and the center at the middle\nbetween source and destination (2HR-($r,k$) for short). This protocol covers\nthe available works as special cases, like ones with the optimal relay\nselection ($r=\\infty$, $k=1$) and with the random relay selection ($r=\\infty$,\n$k = n$ i.e. the number of system nodes) in the case of equal path-loss, ones\nwith relay selected from relay selection region ($r \\in (0, \\infty), k = 1$) in\nthe case of distance-dependent path-loss. The theoretic analysis is further\nprovided to determine the maximum number of eavesdroppers one network can\ntolerate to ensure a desired performance in terms of the secrecy outage\nprobability and transmission outage probability. The analysis results also show\nthe proposed protocol can balance load distributed among the relays by a proper\nsetting of $r$ and $k$ under the premise of specified secure and reliable\nrequirements. \n\n"}
{"id": "1301.2076", "contents": "Title: Modeling of income distribution in the European Union with the\n  Fokker-Planck equation Abstract: Herein, we applied statistical physics to study incomes of three (low-,\nmedium- and high-income) society classes instead of the two (low- and\nmedium-income)classes studied so far. In the frame of the threshold nonlinear\nLangevin dynamics and its threshold Fokker-Planck counterpart, we derived a\nunified formula for description of income of all society classes, by way of\nexample, of those of the European Union in year 2006 and 2008. Hence, the\nformula is more general than the well known that of Yakovenko et al. That is,\nour formula well describes not only two regions but simultaneously the third\nregion in the plot of the complementary cumulative distribution function vs. an\nannual household income. Furthermore, the known stylised facts concerning this\nincome are well described by our formula. Namely, the formula provides the\nBoltzmann-Gibbs income distribution function for the low-income society class\nand the weak Pareto law for the medium-income society class, as expected.\nImportantly, it predicts (to satisfactory approximation) the Zipf law for the\nhigh-income society class. Moreover, the region of medium-income society class\nis now distinctly reduced because the bottom of high-income society class is\ndistinctly lowered. This reduction made, in fact, the medium-income society\nclass an intermediate-income society class. \n\n"}
{"id": "1301.2848", "contents": "Title: Database-assisted Distributed Spectrum Sharing Abstract: According to FCC's ruling for white-space spectrum access, white-space\ndevices are required to query a database to determine the spectrum\navailability. In this paper, we study the database-assisted distributed\nwhite-space access point (AP) network design. We first model the cooperative\nand non-cooperative channel selection problems among the APs as the system-wide\nthroughput optimization and non-cooperative AP channel selection games,\nrespectively, and design distributed AP channel selection algorithms that\nachieve system optimal point and Nash equilibrium, respectively. We then\npropose a state-based game formulation for the distributed AP association\nproblem of the secondary users by taking the cost of mobility into account. We\nshow that the state-based distributed AP association game has the finite\nimprovement property, and design a distributed AP association algorithm that\ncan converge to a state-based Nash equilibrium. Numerical results show that the\nalgorithm is robust to the perturbation by secondary users' dynamical leaving\nand entering the system. \n\n"}
{"id": "1301.3230", "contents": "Title: A Framework for Quality of Service with a Multiple Access Strategy Abstract: We study a problem of scheduling real-time traffic with hard delay\nconstraints in an unreliable wireless channel. Packets arrive at a constant\nrate to the network and have to be delivered within a fixed number of slots in\na fading wireless channel. For an infrastructure mode of traffic with a\ncentralized scheduler, we are interested in the long time average throughput\nachievable for the real time traffic. In [1], the authors have stud- ied the\nfeasible throughput vectors by identifying the necessary and sufficient\nconditions using work load characterization. In our work, we provide a\ncharacterization of the feasible throughput vectors using the notion of the\nrate region. We then discuss an extension to the network model studied in [1]\nby allowing multiple access during contention and propose an enhancement to the\nrate region of the wireless network. We characterize the feasible throughput\nvectors with the multiple access technique and study throughput optimal and\nutility maximizing strategies for the network scenario. Using simulations, we\nevaluate the performance of the proposed strategy and discuss its advantages. \n\n"}
{"id": "1301.4909", "contents": "Title: Analyzing the Performance of LRU Caches under Non-Stationary Traffic\n  Patterns Abstract: This work presents, to the best of our knowledge of the literature, the first\nanalytic model to address the performance of an LRU (Least Recently Used)\nimplementing cache under non-stationary traffic conditions, i.e., when the\npopularity of content evolves with time. We validate the accuracy of the model\nusing Monte Carlo simulations. We show that the model is capable of accurately\nestimating the cache hit probability, when the popularity of content is\nnon-stationary.\n  We find that there exists a dependency between the performance of an LRU\nimplementing cache and i) the lifetime of content in a system, ii) the volume\nof requests associated with it, iii) the distribution of content request\nvolumes and iv) the shape of the popularity profile over time. \n\n"}
{"id": "1301.5877", "contents": "Title: Pricing Using a Homogeneously Saturated Equation Abstract: A homogeneously saturated equation for the time development of the price of a\nfinancial asset is presented and investigated for the pricing of European call\noptions using noise that is distributed as a Student's t-distribution. In the\nlimit that the saturation parameter of the equation equals zero, the standard\nmodel of geometric motion for the price of an asset is obtained. The\nhomogeneously saturated equation for the price of an asset is similar to a\nsimple equation for the output of a homogeneously broadened laser. The\nhomogeneously saturated equation tends to limit the range of returns and thus\nseems to be realistic.\n  Fits to linear returns obtained from the adjusted closing values for the S&P\n500 index were used to obtain best-fit parameters for Student's t-distributions\nand for normal distributions, and these fits were used to price options, and to\ncompare approaches to modelling prices.\n  This work has value in understanding the pricing of assets and of European\ncall options. \n\n"}
{"id": "1301.5938", "contents": "Title: Evolution of the Internet k-dense structure Abstract: As the Internet AS-level topology grows over time, some of its structural\nproperties remain unchanged. Such time- invariant properties are generally\ninteresting, because they tend to reflect some fundamental processes or\nconstraints behind Internet growth. As has been shown before, the\ntime-invariant structural properties of the Internet include some most basic\nones, such as the degree distribution or clustering. Here we add to this\ntime-invariant list a non-trivial property - k-dense decomposition. This\nproperty is derived from a recursive form of edge multiplicity, defined as the\nnumber of triangles that share a given edge. We show that after proper\nnormalization, the k- dense decomposition of the Internet has remained stable\nover the last decade, even though the Internet size has approximately doubled,\nand so has the k-density of its k-densest core. This core consists mostly of\ncontent providers peering at Internet eXchange Points, and it only loosely\noverlaps with the high-degree or high-rank AS core, consisting mostly of tier-1\ntransit providers. We thus show that high degrees and high k-densities reflect\ntwo different Internet-specific properties of ASes (transit versus content\nproviders). As a consequence, even though degrees and k-densities of nodes are\ncorrelated, the relative fluctuations are strong, and related to that, random\ngraphs with the same degree distribution or even degree correlations as in the\nInternet, do not reproduce its k-dense decomposition. Therefore an interesting\nopen question is what Internet topology models or generators can fully explain\nor at least reproduce the k-dense properties of the Internet. \n\n"}
{"id": "1302.2185", "contents": "Title: Passive Self-Interference Suppression for Full-Duplex Infrastructure\n  Nodes Abstract: Recent research results have demonstrated the feasibility of full-duplex\nwireless communication for short-range links. Although the focus of the\nprevious works has been active cancellation of the self-interference signal, a\nmajority of the overall self-interference suppression is often due to passive\nsuppression, i.e., isolation of the transmit and receive antennas. We present a\nmeasurement-based study of the capabilities and limitations of three key\nmechanisms for passive self-interference suppression: directional isolation,\nabsorptive shielding, and cross-polarization. The study demonstrates that more\nthan 70 dB of passive suppression can be achieved in certain environments, but\nalso establishes two results on the limitations of passive suppression: (1)\nenvironmental reflections limit the amount of passive suppression that can be\nachieved, and (2) passive suppression, in general, increases the frequency\nselectivity of the residual self-interference signal. These results suggest two\ndesign implications: (1) deployments of full-duplex infrastructure nodes should\nminimize near-antenna reflectors, and (2) active cancellation in concatenation\nwith passive suppression should employ higher-order filters or per-subcarrier\ncancellation. \n\n"}
{"id": "1302.2337", "contents": "Title: The Heston Riemannian distance function Abstract: The Heston model is a popular stock price model with stochastic volatility\nthat has found numerous applications in practice. In the present paper, we\nstudy the Riemannian distance function associated with the Heston model and\nobtain explicit formulas for this function using geometrical and analytical\nmethods. Geometrical approach is based on the study of the Heston geodesics,\nwhile the analytical approach exploits the links between the Heston distance\nfunction and the sub-Riemannian distance function in the Grushin plane. For the\nGrushin plane, we establish an explicit formula for the Legendre-Fenchel\ntransform of the limiting cumulant generating function and prove a partial\nlarge deviation principle that is true only inside a special set. \n\n"}
{"id": "1302.3250", "contents": "Title: Exploiting the Past to Reduce Delay in CSMA Scheduling: A High-order\n  Markov Chain Approach Abstract: Recently several CSMA algorithms based on the Glauber dynamics model have\nbeen proposed for multihop wireless scheduling, as viable solutions to achieve\nthe throughput optimality, yet are simple to implement. However, their delay\nperformances still remain unsatisfactory, mainly due to the nature of the\nunderlying Markov chains that imposes a fundamental constraint on how the link\nstate can evolve over time. In this paper, we propose a new approach toward\nbetter queueing and delay performance, based on our observation that the\nalgorithm needs not be Markovian, as long as it can be implemented in a\ndistributed manner, achieve the same throughput optimality, while offering far\nbetter delay performance for general network topologies. Our approach hinges\nupon utilizing past state information observed by local link and then\nconstructing a high-order Markov chain for the evolution of the feasible link\nschedules. We show in theory and simulation that our proposed algorithm, named\ndelayed CSMA, adds virtually no additional overhead onto the existing\nCSMA-based algorithms, achieves the throughput optimality under the usual\nchoice of link weight as a function of local queue length, and also provides\nmuch better delay performance by effectively `de-correlating' the link state\nprocess (thus removing link starvation) under any arbitrary network topology.\nFrom our extensive simulations we observe that the delay under our algorithm\ncan be often reduced by a factor of 20 over a wide range of scenarios, compared\nto the standard Glauber-dynamics-based CSMA algorithm. \n\n"}
{"id": "1302.4474", "contents": "Title: On the multiple unicast capacity of 3-source, 3-terminal directed\n  acyclic networks Abstract: We consider the multiple unicast problem with three source-terminal pairs\nover directed acyclic networks with unit-capacity edges. The three $s_i-t_i$\npairs wish to communicate at unit-rate via network coding. The connectivity\nbetween the $s_i - t_i$ pairs is quantified by means of a connectivity level\nvector, $[k_1 k_2 k_3]$ such that there exist $k_i$ edge-disjoint paths between\n$s_i$ and $t_i$. In this work we attempt to classify networks based on the\nconnectivity level. It can be observed that unit-rate transmission can be\nsupported by routing if $k_i \\geq 3$, for all $i = 1, \\dots, 3$. In this work,\nwe consider, connectivity level vectors such that $\\min_{i = 1, \\dots, 3} k_i <\n3$. We present either a constructive linear network coding scheme or an\ninstance of a network that cannot support the desired unit-rate requirement,\nfor all such connectivity level vectors except the vector $[1~2~4]$ (and its\npermutations). The benefits of our schemes extend to networks with higher and\npotentially different edge capacities. Specifically, our experimental results\nindicate that for networks where the different source-terminal paths have a\nsignificant overlap, our constructive unit-rate schemes can be packed along\nwith routing to provide higher throughput as compared to a pure routing\napproach. \n\n"}
{"id": "1303.2952", "contents": "Title: Traffic Congestion in Expanders, $(p,\\delta)$--Hyperbolic Spaces and\n  Product of Trees Abstract: In this paper we define the notion of $(p,\\delta)$--Gromov hyperbolic space\nwhere we relax Gromov's {\\it slimness} condition to allow that not all but a\npositive fraction of all triangles are $\\delta$--slim. Furthermore, we study\nmaximum vertex congestion under geodesic routing and show that it scales as\n$\\Omega(p^2n^2/D_n^2)$ where $D_n$ is the diameter of the graph. We also\nconstruct a constant degree family of expanders with congestion $\\Theta(n^2)$\nin contrast with random regular graphs that have congestion $O(n\\log^{3}(n))$.\nFinally, we study traffic congestion on graphs defined as product of trees. \n\n"}
{"id": "1303.6518", "contents": "Title: SRP-MS: A New Routing Protocol for Delay Tolerant Wireless Sensor\n  Networks Abstract: Sink Mobility is becoming popular due to excellent load balancing between\nnodes and ultimately resulting in prolonged network lifetime and throughput. A\nmajor challenge is to provide reliable and energy-efficient operations are to\nbe taken into consideration for differentmobility patterns of sink. Aim of this\npaper is lifetime maximization of Delay TolerantWireless Sensor Networks (WSNs)\nthrough the manipulation of Mobile Sink (MS) on different trajectories. We\npropose Square Routing Protocol with MS (SRP-MS) based on existing SEP (Stable\nElection Protocol) by making it Cluster Less (CL) and introducing sink\nmobility. \n\n"}
{"id": "1304.3516", "contents": "Title: Existence of an endogenously complete equilibrium driven by a diffusion Abstract: The existence of complete Radner equilibria is established in an economy\nwhich parameters are driven by a diffusion process. Our results complement\nthose in the literature. In particular, we work under essentially minimal\nregularity conditions and treat time-inhomogeneous case. \n\n"}
{"id": "1304.4311", "contents": "Title: A Model for Scaling in Firms' Size and Growth Rate Distribution Abstract: We introduce a simple agent-based model which allows us to analyze three\nstylized facts: a fat-tailed size distribution of companies, a `tent-shaped'\ngrowth rate distribution, the scaling relation of the growth rate variance with\nfirm size, and the causality between them. This is achieved under the simple\nhypothesis that firms compete for a scarce quantity (either aggregate demand or\nworkforce) which is allocated probabilistically. The model allows us to relate\nsize and growth rate distributions. We compare the results of our model to\nsimulations with other scaling relationships, and to similar models and relate\nit to existing theory. Effects arising from binning data are discussed. \n\n"}
{"id": "1305.2275", "contents": "Title: Spreading Information in Mobile Wireless Networks Abstract: Device-to-device (D2D) communication enables us to spread information in the\nlocal area without infrastructure support. In this paper, we focus on\ninformation spreading in mobile wireless networks where all nodes move around.\nThe source nodes deliver a given information packet to mobile users using D2D\ncommunication as an underlay to the cellular uplink. By stochastic geometry, we\nderive the average number of nodes that have successfully received a given\ninformation packet as a function of the transmission power and the number of\ntransmissions. Based on these results, we formulate a redundancy minimization\nproblem under the maximum transmission power and delay constraints. By solving\nthe problem, we provide an optimal rule for the transmission power of the\nsource node. \n\n"}
{"id": "1305.3356", "contents": "Title: Analytical Evaluation of Coverage-Oriented Femtocell Network Deployment Abstract: This paper proposes a coverage-oriented femtocell network deployment scheme,\nin which the femtocell base stations (BSs) can decide whether to be active or\ninactive depending on their distances from the macrocell BSs. Specifically, as\nthe areas close to the macrocell BSs already have satisfactory cellular\ncoverage, the femtocell BSs located inside such areas are kept to be inactive.\nThus, all the active femtocells are located in the poor macrocell coverage\nareas. Based on a stochastic geometric framework, the coverage probability can\nbe analyzed with tractable results. Surprisingly, the results show that the\nproposed scheme, although with a lower defacto femtocell density, can achieve\nbetter coverage performance than that keeping all femtocells in the entire\nnetwork to be active. The analytical results further identify the achievable\noptimal performance of the new scheme, which provides mobile operators a\nguideline for femtocell deployment and operation. \n\n"}
{"id": "1305.3586", "contents": "Title: Utility Optimal Scheduling and Admission Control for Adaptive Video\n  Streaming in Small Cell Networks Abstract: We consider the jointly optimal design of a transmission scheduling and\nadmission control policy for adaptive video streaming over small cell networks.\nWe formulate the problem as a dynamic network utility maximization and observe\nthat it naturally decomposes into two subproblems: admission control and\ntransmission scheduling. The resulting algorithms are simple and suitable for\ndistributed implementation. The admission control decisions involve each user\nchoosing the quality of the video chunk asked for download, based on the\nnetwork congestion in its neighborhood. This form of admission control is\ncompatible with the current video streaming technology based on the DASH\nprotocol over TCP connections. Through simulations, we evaluate the performance\nof the proposed algorithm under realistic assumptions for a small-cell network. \n\n"}
{"id": "1305.5530", "contents": "Title: Optimal Scheduling for Energy Harvesting Transmitters with Hybrid Energy\n  Storage Abstract: We consider data transmission with an energy harvesting transmitter which has\na hybrid energy storage unit composed of a perfectly efficient super-capacitor\n(SC) and an inefficient battery. The SC has finite space for energy storage\nwhile the battery has unlimited space. The transmitter can choose to store the\nharvested energy in the SC or in the battery. The energy is drained from the SC\nand the battery simultaneously. In this setting, we consider the offline\nthroughput maximization problem by a deadline over a point-to-point channel. In\ncontrast to previous works, the hybrid energy storage model with finite and\nunlimited storage capacities imposes a generalized set of constraints on the\ntransmission policy. As such, we show that the solution generalizes that for a\nsingle battery and is obtained by applying directional water-filling algorithm\nmultiple times. \n\n"}
{"id": "1305.7114", "contents": "Title: Temporal Locality in Today's Content Caching: Why it Matters and How to\n  Model it Abstract: The dimensioning of caching systems represents a difficult task in the design\nof infrastructures for content distribution in the current Internet. This paper\naddresses the problem of defining a realistic arrival process for the content\nrequests generated by users, due its critical importance for both analytical\nand simulative evaluations of the performance of caching systems. First, with\nthe aid of YouTube traces collected inside operational residential networks, we\nidentify the characteristics of real traffic that need to be considered or can\nbe safely neglected in order to accurately predict the performance of a cache.\nSecond, we propose a new parsimonious traffic model, named the Shot Noise Model\n(SNM), that enables users to natively capture the dynamics of content\npopularity, whilst still being sufficiently simple to be employed effectively\nfor both analytical and scalable simulative studies of caching systems.\nFinally, our results show that the SNM presents a much better solution to\naccount for the temporal locality observed in real traffic compared to existing\napproaches. \n\n"}
{"id": "1306.0183", "contents": "Title: Cell-Level Modeling of IEEE 802.11 WLANs Abstract: We develop a scalable \\textit{cell-level} analytical model for multi-cell\ninfrastructure IEEE 802.11 WLANs under a so-called Pairwise Binary Dependence\n(PBD) condition. The PBD condition is a geometric property under which the\nrelative locations of the nodes inside a cell do not matter and the network is\nfree of \\textit{hidden nodes}. For the cases of saturated nodes and\nTCP-controlled long-file downloads, we provide accurate predictions of cell\nthroughputs. Similar to Bonald et al (Sigmetrics, 2008), we model a multi-cell\nWLAN under short-file downloads as \"a network of processor-sharing queues with\nstate-dependent service rates.\" Whereas the state-dependent service rates\nproposed by Bonald et al are based only on the \\textit{number} of contending\nneighbors, we employ state-dependent service rates that incorporate the the\nimpact of the overall \\textit{topology} of the network. We propose an\n\\textit{effective service rate approximation} technique and obtain good\napproximations for the \\textit{mean flow transfer delay} in each cell. For\nTCP-controlled downloads where the APs transmit a large fraction of time, we\nshow that the throughputs predicted under the PBD condition are very good\napproximations in two important scenarios where hidden nodes are indeed present\nand the PBD condition does not strictly hold. \n\n"}
{"id": "1306.0215", "contents": "Title: Cross-border Portfolio Investment Networks and Indicators for Financial\n  Crises Abstract: Cross-border equity and long-term debt securities portfolio investment\nnetworks are analysed from 2002 to 2012, covering the 2008 global financial\ncrisis. They serve as network-proxies for measuring the robustness of the\nglobal financial system and the interdependence of financial markets,\nrespectively. Two early-warning indicators for financial crises are identified:\nFirst, the algebraic connectivity of the equity securities network, as a\nmeasure for structural robustness, drops close to zero already in 2005, while\nthere is an over-representation of high-degree off-shore financial centres\namong the countries most-related to this observation, suggesting an\ninvestigation of such nodes with respect to the structural stability of the\nglobal financial system. Second, using a phenomenological model, the edge\ndensity of the debt securities network is found to describe, and even forecast,\nthe proliferation of several over-the-counter-traded financial derivatives,\nmost prominently credit default swaps, enabling one to detect potentially\ndangerous levels of market interdependence and systemic risk. \n\n"}
{"id": "1306.0772", "contents": "Title: Equivalence and comparison of heterogeneous cellular networks Abstract: We consider a general heterogeneous network in which, besides general\npropagation effects (shadowing and/or fading), individual base stations can\nhave different emitting powers and be subject to different parameters of\nHata-like path-loss models (path-loss exponent and constant) due to, for\nexample, varying antenna heights. We assume also that the stations may have\nvarying parameters of, for example, the link layer performance (SINR threshold,\netc). By studying the propagation processes of signals received by the typical\nuser from all antennas marked by the corresponding antenna parameters, we show\nthat seemingly different heterogeneous networks based on Poisson point\nprocesses can be equivalent from the point of view a typical user. These\nneworks can be replaced with a model where all the previously varying\npropagation parameters (including path-loss exponents) are set to constants\nwhile the only trade-off being the introduction of an isotropic base station\ndensity. This allows one to perform analytic comparisons of different network\nmodels via their isotropic representations. In the case of a constant path-loss\nexponent, the isotropic representation simplifies to a homogeneous modification\nof the constant intensity of the original network, thus generalizing a previous\nresult showing that the propagation processes only depend on one moment of the\nemitted power and propagation effects. We give examples and applications to\nmotivate these results and highlight an interesting observation regarding\nrandom path-loss exponents. \n\n"}
{"id": "1306.4975", "contents": "Title: A Stochastic Feedback Model for Volatility Abstract: Financial time series exhibit a number of interesting properties that are\ndifficult to explain with simple models. These properties include fat-tails in\nthe distribution of price fluctuations (or returns) that are slowly removed at\nlonger timescales, strong autocorrelations in absolute returns but zero\nautocorrelation in returns themselves, and multifractal scaling. Although the\nunderlying cause of these features is unknown, there is growing evidence they\noriginate in the behavior of volatility, i.e., in the behavior of the magnitude\nof price fluctuations. In this paper, we posit a feedback mechanism for\nvolatility that closely reproduces the non-trivial properties of empirical\nprices. The model is parsimonious, contains only two parameters that are easily\nestimated, fits empirical data better than standard models, and can be grounded\nin a straightforward framework where volatility fluctuations are driven by the\nestimation error of an exogenous Poisson rate. \n\n"}
{"id": "1306.5082", "contents": "Title: Non-Equivalent Beliefs and Subjective Equilibrium Bubbles Abstract: This paper develops a dynamic equilibrium model where agents exhibit a strong\nform of belief heterogeneity: they disagree about zero probability events. It\nis shown that, somewhat surprisingly, equilibrium exists in this setting, and\nthat the disagreement about nullsets naturally leads to equilibrium asset\npricing bubbles. The bubbles are subjective in the sense that they are\nperceived by some but not necessarily all agents. In contrast to existing\nmodels, bubbles arise with no restrictions on trade beyond a standard solvency\nconstraint. \n\n"}
{"id": "1306.6122", "contents": "Title: Downlink Rate Distribution in Heterogeneous Cellular Networks under\n  Generalized Cell Selection Abstract: Considering both small-scale fading and long-term shadowing, we characterize\nthe downlink rate distribution at a typical user equipment (UE) in a\nheterogeneous cellular network (HetNet), where shadowing, following any general\ndistribution, impacts cell selection while fading does not. Prior work either\nignores the impact of channel randomness on cell selection or lumps all the\nsources of randomness into a single variable, with cell selection based on the\ninstantaneous signal strength, which is unrealistic. As an application of the\nresults, we study the impact of shadowing on load balancing in terms of the\noptimal per-tier selection bias needed for rate maximization. \n\n"}
{"id": "1307.0084", "contents": "Title: Catch a Breath: Non-invasive Respiration Rate Monitoring via Wireless\n  Communication Abstract: Radio signals are sensitive to changes in the environment, which for example\nis reflected on the received signal strength (RSS) measurements of low-cost\nwireless devices. This information has been used effectively in the past years\ne.g. in device-free localization and tracking. Recent literature has also shown\nthat the fading information of the wireless channel can be exploited to\nestimate the breathing rate of a person in a non-invasive manner; a research\ntopic we address in this paper. To the best of our knowledge, we demonstrate\nfor the first time that the respiration rate of a person can be accurately\nestimated using only a single IEEE 802.15.4 compliant TX-RX pair. We exploit\nchannel diversity, low-jitter periodic communication, and oversampling to\nenhance the breathing estimates, and make use of a decimation filter to\ndecrease the computational requirements of breathing estimation. In addition,\nwe develop a hidden Markov model (HMM) to identify the time instances when\nbreathing estimation is not possible, i.e., during times when other motion than\nbreathing occurs. We experimentally validate the accuracy of the system and the\nresults suggest that the respiration rate can be estimated with a mean error of\n0.03 breaths per minute, the lowest breathing rate error reported to date using\nIEEE 802.15.4 compliant transceivers. We also demonstrate that the breathing of\ntwo people can be monitored simultaneously, a result not reported in earlier\nliterature. \n\n"}
{"id": "1307.0785", "contents": "Title: Explicit Description of HARA Forward Utilities and Their Optimal\n  Portfolios Abstract: This paper deals with forward performances of HARA type. Precisely, for a\nmarket model in which stock price processes are modeled by a locally bounded\n$d$-dimensional semimartingale, we elaborate a complete and explicit\ncharacterization for this type of forward utilities. Furthermore, the optimal\nportfolios for each of these forward utilities are explicitly described. Our\napproach is based on the minimal Hellinger martingale densities that are\nobtained from the important statistical concept of Hellinger process. These\nmartingale densities were introduced recently, and appeared herein tailor-made\nfor these forward utilities. After outlining our parametrization method for the\nHARA forward, we provide illustrations on discrete-time market models. Finally,\nwe conclude our paper by pointing out a number of related open questions. \n\n"}
{"id": "1307.2690", "contents": "Title: BGP Security in Partial Deployment: Is the Juice Worth the Squeeze? Abstract: As the rollout of secure route origin authentication with the RPKI slowly\ngains traction among network operators, there is a push to standardize secure\npath validation for BGP (i.e., S*BGP: S-BGP, soBGP, BGPSEC, etc.). Origin\nauthentication already does much to improve routing security. Moreover, the\ntransition to S*BGP is expected to be long and slow, with S*BGP coexisting in\n\"partial deployment\" alongside BGP for a long time. We therefore use\ntheoretical and experimental approach to study the security benefits provided\nby partially-deployed S*BGP, vis-a-vis those already provided by origin\nauthentication. Because routing policies have a profound impact on routing\nsecurity, we use a survey of 100 network operators to find the policies that\nare likely to be most popular during partial S*BGP deployment. We find that\nS*BGP provides only meagre benefits over origin authentication when these\npopular policies are used. We also study the security benefits of other routing\npolicies, provide prescriptive guidelines for partially-deployed S*BGP, and\nshow how interactions between S*BGP and BGP can introduce new vulnerabilities\ninto the routing system. \n\n"}
{"id": "1307.3835", "contents": "Title: Joint Optimization of Radio Resources and Code Partitioning in Mobile\n  Edge Computing Abstract: The aim of this paper is to propose a computation offloading strategy for\nmobile edge computing. We exploit the concept of call graph, which models a\ngeneric computer program as a set of procedures related to each other through a\nweighted directed graph. Our goal is to derive the optimal partition of the\ncall graph establishing which procedures are to be executed locally or\nremotely. The main novelty of our work is that the optimal partition is\nobtained jointly with the selection of radio parameters, e.g., transmit power\nand constellation size, in order to minimize the energy consumption at the\nmobile handset, under a latency constraint taking into account transmit time\nand execution time. We consider both single and multi-channel transmission\nstrategies and we prove that a globally optimal solution can be achieved in\nboth cases. Finally, we propose a suboptimal strategy aimed at solving a\nrelaxed version of the original problem in order to tradeoff complexity and\nperformance of the proposed framework. Finally, several numerical results\nillustrate under what conditions in terms of call graph topology, communication\nstrategy, and computation parameters, the proposed offloading strategy provides\nlarge performance gains. \n\n"}
{"id": "1307.4744", "contents": "Title: On the Coexistence of a Primary User with an Energy Harvesting Secondary\n  User: A Case of Cognitive Cooperation Abstract: In this paper, we consider a cognitive scenario where an energy harvesting\nsecondary user (SU) shares the spectrum with a primary user (PU). The secondary\nsource helps the primary source in delivering its undelivered packets during\nperiods of silence of the primary source. The primary source has a queue for\nstoring its data packets, whereas the secondary source has two data queues; a\nqueue for storing its own packets and the other for storing the fraction of the\nundelivered primary packets accepted for relaying. The secondary source is\nassumed to be a battery-based node which harvests energy packets from the\nenvironment. In addition to its data queues, the SU has an energy queue to\nstore the harvested energy packets. The secondary energy packets are used for\nprimary packets decoding and data packets transmission. More specifically, if\nthe secondary energy queue is empty, the secondary source can neither help the\nprimary source nor transmit a packet from the data queues. The energy queue is\nmodeled as a discrete time queue with Markov arrival and service processes. Due\nto the interaction of the queues, we provide inner and outer bounds on the\nstability region of the proposed system. We investigate the impact of the\nenergy arrival rate on the stability region. Numerical results show the\nsignificant gain of cooperation. \n\n"}
{"id": "1307.5268", "contents": "Title: South African Riots: Repercussion of the Global Food Crisis and US\n  Drought Abstract: High and volatile global food prices have led to food riots and played a\ncritical role in triggering the Arab Spring revolutions in recent years. The\nsevere drought in the US in the summer of 2012 led to a new increase in food\nprices. Through the fall, they remained at a threshold above which the riots\nand revolutions had predominantly occurred. Global prices at this level create\nconditions where an exacerbating local circumstance can trigger unrest. Global\ncorn (maize) prices reached new highs, and countries that depend mostly on\nmaize are more likely to experience high local food prices and associated\npressures toward social unrest. Here we analyze the conditions in South Africa,\nwhich is a heavily maize-dependent country. Coinciding with increased consumer\nfood indices this summer, massive labor strikes in mining and agriculture have\nled to the greatest single incident of social violence since the fall of\napartheid in 1994. Worker demands for dramatic pay increases reflect that their\nwages have not kept up with drastic increases in the prices of necessities,\nespecially food. Without attention to the global food price situation, more\nincidents of food-based social instability are likely to arise. Other countries\nthat have manifested food-related protests and riots in 2012 include Haiti and\nArgentina. Moreover, these cases of unrest are just the most visible symptom of\nwidespread suffering of poor populations worldwide due to elevated food prices.\nPolicy decisions that would directly impact food prices are decreasing the\nconversion of maize to ethanol in the US, and reimposing regulations on\ncommodity futures markets to prevent excessive speculation, which we have shown\ncauses bubbles and crashes in these markets. Absent such policy actions,\ngovernments and companies should track and mitigate the impact of high and\nvolatile food prices on citizens and employees. \n\n"}
{"id": "1307.6373", "contents": "Title: Effect of Spatial Interference Correlation on the Performance of Maximum\n  Ratio Combining Abstract: While the performance of maximum ratio combining (MRC) is well understood for\na single isolated link, the same is not true in the presence of interference,\nwhich is typically correlated across antennas due to the common locations of\ninterferers. For tractability, prior work focuses on the two extreme cases\nwhere the interference power across antennas is either assumed to be fully\ncorrelated or fully uncorrelated. In this paper, we address this shortcoming\nand characterize the performance of MRC in the presence of spatially-correlated\ninterference across antennas. Modeling the interference field as a Poisson\npoint process, we derive the exact distribution of the signal-to-interference\nratio (SIR) for the case of two receive antennas, and upper and lower bounds\nfor the general case. Using these results, we study the diversity behavior of\nMRC and characterize the critical density of simultaneous transmissions for a\ngiven outage constraint. The exact SIR distribution is also useful in\nbenchmarking simpler correlation models. We show that the full-correlation\nassumption is considerably pessimistic (up to 30% higher outage probability for\ntypical values) and the no-correlation assumption is significantly optimistic\ncompared to the true performance. \n\n"}
{"id": "1307.7057", "contents": "Title: A Survey of Home Energy Management Systems in Future Smart Grid\n  Communications Abstract: In this paper we present a systematic review of various home energy\nmanagement (HEM) schemes. Employment of home energy management programs will\nmake the electricity consumption smarter and more efficient. Advantages of HEM\ninclude, increased savings for consumers as well as utilities, reduced peak to\naverage ratio (PAR) and peak demand. Where there are numerous applications of\nsmart grid technologies, home energy management is probably the most important\none to be addressed. Utilities across the globe have taken various steps for\nefficient consumption of electricity. New pricing schemes like, Real Time\nPricing (RTP), Time of Use (ToU), Inclining Block Rates (IBR), Critical Peak\nPricing (CPP) etc, have been proposed for smart grid. Distributed Energy\nResources (DER) (local generation) and/or home appliances coordination along\nwith different tariff schemes lead towards efficient consumption of\nelectricity. This work also discusses a HEM systems general architecture and\nvarious challenges in implementation of this architecture in smart grid. \n\n"}
{"id": "1307.7309", "contents": "Title: Optimal Rate Sampling in 802.11 Systems Abstract: In 802.11 systems, Rate Adaptation (RA) is a fundamental mechanism allowing\ntransmitters to adapt the coding and modulation scheme as well as the MIMO\ntransmission mode to the radio channel conditions, and in turn, to learn and\ntrack the (mode, rate) pair providing the highest throughput. So far, the\ndesign of RA mechanisms has been mainly driven by heuristics. In contrast, in\nthis paper, we rigorously formulate such design as an online stochastic\noptimisation problem. We solve this problem and present ORS (Optimal Rate\nSampling), a family of (mode, rate) pair adaptation algorithms that provably\nlearn as fast as it is possible the best pair for transmission. We study the\nperformance of ORS algorithms in both stationary radio environments where the\nsuccessful packet transmission probabilities at the various (mode, rate) pairs\ndo not vary over time, and in non-stationary environments where these\nprobabilities evolve. We show that under ORS algorithms, the throughput loss\ndue to the need to explore sub-optimal (mode, rate) pairs does not depend on\nthe number of available pairs, which is a crucial advantage as evolving 802.11\nstandards offer an increasingly large number of (mode, rate) pairs. We\nillustrate the efficiency of ORS algorithms (compared to the state-of-the-art\nalgorithms) using simulations and traces extracted from 802.11 test-beds. \n\n"}
{"id": "1307.8083", "contents": "Title: TOFEC: Achieving Optimal Throughput-Delay Trade-off of Cloud Storage\n  Using Erasure Codes Abstract: Our paper presents solutions using erasure coding, parallel connections to\nstorage cloud and limited chunking (i.e., dividing the object into a few\nsmaller segments) together to significantly improve the delay performance of\nuploading and downloading data in and out of cloud storage.\n  TOFEC is a strategy that helps front-end proxy adapt to level of workload by\ntreating scalable cloud storage (e.g. Amazon S3) as a shared resource requiring\nadmission control. Under light workloads, TOFEC creates more smaller chunks and\nuses more parallel connections per file, minimizing service delay. Under heavy\nworkloads, TOFEC automatically reduces the level of chunking (fewer chunks with\nincreased size) and uses fewer parallel connections to reduce overhead,\nresulting in higher throughput and preventing queueing delay. Our trace-driven\nsimulation results show that TOFEC's adaptation mechanism converges to an\nappropriate code that provides the optimal delay-throughput trade-off without\nreducing system capacity. Compared to a non-adaptive strategy optimized for\nthroughput, TOFEC delivers 2.5x lower latency under light workloads; compared\nto a non-adaptive strategy optimized for latency, TOFEC can scale to support\nover 3x as many requests. \n\n"}
{"id": "1307.8208", "contents": "Title: Formal Probabilistic Analysis of a Wireless Sensor Network for Forest\n  Fire Detection Abstract: Wireless Sensor Networks (WSNs) have been widely explored for forest fire\ndetection, which is considered a fatal threat throughout the world. Energy\nconservation of sensor nodes is one of the biggest challenges in this context\nand random scheduling is frequently applied to overcome that. The performance\nanalysis of these random scheduling approaches is traditionally done by\npaper-and-pencil proof methods or simulation. These traditional techniques\ncannot ascertain 100% accuracy, and thus are not suitable for analyzing a\nsafety-critical application like forest fire detection using WSNs. In this\npaper, we propose to overcome this limitation by applying formal probabilistic\nanalysis using theorem proving to verify scheduling performance of a real-world\nWSN for forest fire detection using a k-set randomized algorithm as an energy\nsaving mechanism. In particular, we formally verify the expected values of\ncoverage intensity, the upper bound on the total number of disjoint subsets,\nfor a given coverage intensity, and the lower bound on the total number of\nnodes. \n\n"}
{"id": "1308.0041", "contents": "Title: A Tractable Model for Non-Coherent Joint-Transmission Base Station\n  Cooperation Abstract: This paper presents a tractable model for analyzing non-coherent joint\ntransmission base station (BS) cooperation, taking into account the irregular\nBS deployment typically encountered in practice. Besides cellular-network\nspecific aspects such as BS density, channel fading, average path loss and\ninterference, the model also captures relevant cooperation mechanisms including\nuser-centric BS clustering and channel-dependent cooperation activation. The\nlocations of all BSs are modeled by a Poisson point process. Using tools from\nstochastic geometry, the signal-to-interference-plus-noise ratio\n($\\mathtt{SINR}$) distribution with cooperation is precisely characterized in a\ngenerality-preserving form. The result is then applied to practical design\nproblems of recent interest. We find that increasing the network-wide BS\ndensity improves the $\\mathtt{SINR}$, while the gains increase with the path\nloss exponent. For pilot-based channel estimation, the average spectral\nefficiency saturates at cluster sizes of around $7$ BSs for typical values,\nirrespective of backhaul quality. Finally, it is shown that intra-cluster\nfrequency reuse is favorable in moderately loaded cells with generous\ncooperation activation, while intra-cluster coordinated scheduling may be\nbetter in lightly loaded cells with conservative cooperation activation. \n\n"}
{"id": "1308.0178", "contents": "Title: Coded Caching with Nonuniform Demands Abstract: We consider a network consisting of a file server connected through a shared\nlink to a number of users, each equipped with a cache. Knowing the popularity\ndistribution of the files, the goal is to optimally populate the caches such as\nto minimize the expected load of the shared link. For a single cache, it is\nwell known that storing the most popular files is optimal in this setting.\nHowever, we show here that this is no longer the case for multiple caches.\nIndeed, caching only the most popular files can be highly suboptimal. Instead,\na fundamentally different approach is needed, in which the cache contents are\nused as side information for coded communication over the shared link. We\npropose such a coded caching scheme and prove that it is close to optimal. \n\n"}
{"id": "1308.1767", "contents": "Title: WARP: A ICN architecture for social data Abstract: Social network companies maintain complete visibility and ownership of the\ndata they store. However users should be able to maintain full control over\ntheir content. For this purpose, we propose WARP, an architecture based upon\nInformation-Centric Networking (ICN) designs, which expands the scope of the\nICN architecture beyond media distribution, to provide data control in social\nnetworks. The benefit of our solution lies in the lightweight nature of the\nprotocol and in its layered design. With WARP, data distribution and access\npolicies are enforced on the user side. Data can still be replicated in an ICN\nfashion but we introduce control channels, named \\textit{thread updates}, which\nensures that the access to the data is always updated to the latest control\npolicy. WARP decentralizes the social network but still offers APIs so that\nsocial network providers can build products and business models on top of WARP.\nSocial applications run directly on the user's device and store their data on\nthe user's \\textit{butler} that takes care of encryption and distribution.\nMoreover, users can still rely on third parties to have high-availability\nwithout renouncing their privacy. \n\n"}
{"id": "1308.3548", "contents": "Title: Distributed Ranging and Localization for Wireless Networks via\n  Compressed Sensing Abstract: Location-based services in a wireless network require nodes to know their\nlocations accurately. Conventional solutions rely on contention-based medium\naccess, where only one node can successfully transmit at any time in any\nneighborhood. In this paper, a novel, complete, distributed ranging and\nlocalization solution is proposed, which let all nodes in the network broadcast\ntheir location estimates and measure distances to all neighbors simultaneously.\nAn on-off signaling is designed to overcome the physical half-duplex\nconstraint. In each iteration, all nodes transmit simultaneously, each\nbroadcasting codewords describing the current location estimate. From the\nsuperposed signals from all neighbors, each node decodes their neighbors'\nlocations and also estimates their distances using the signal strengths. The\nnode then broadcasts its improved location estimates in the subsequent\niteration. Simulations demonstrate accurate localization throughout a large\nnetwork over a few thousand symbol intervals, suggesting much higher efficiency\nthan conventional schemes based on ALOHA or CSMA. \n\n"}
{"id": "1308.3892", "contents": "Title: Do the rich get richer? An empirical analysis of the BitCoin transaction\n  network Abstract: The possibility to analyze everyday monetary transactions is limited by the\nscarcity of available data, as this kind of information is usually considered\nhighly sensitive. Present econophysics models are usually employed on presumed\nrandom networks of interacting agents, and only macroscopic properties (e.g.\nthe resulting wealth distribution) are compared to real-world data. In this\npaper, we analyze BitCoin, which is a novel digital currency system, where the\ncomplete list of transactions is publicly available. Using this dataset, we\nreconstruct the network of transactions, and extract the time and amount of\neach payment. We analyze the structure of the transaction network by measuring\nnetwork characteristics over time, such as the degree distribution, degree\ncorrelations and clustering. We find that linear preferential attachment drives\nthe growth of the network. We also study the dynamics taking place on the\ntransaction network, i.e. the flow of money. We measure temporal patterns and\nthe wealth accumulation. Investigating the microscopic statistics of money\nmovement, we find that sublinear preferential attachment governs the evolution\nof the wealth distribution. We report a scaling relation between the degree and\nwealth associated to individual nodes. \n\n"}
{"id": "1309.0569", "contents": "Title: Product-form solutions for integrated services packet networks and cloud\n  computing systems Abstract: We iteratively derive the product-form solutions of stationary distributions\nof priority multiclass queueing networks with multi-sever stations. The\nnetworks are Markovian with exponential interarrival and service time\ndistributions. These solutions can be used to conduct performance analysis or\nas comparison criteria for approximation and simulation studies of large scale\nnetworks with multi-processor shared-memory switches and cloud computing\nsystems with parallel-server stations. Numerical comparisons with existing\nBrownian approximating model are provided to indicate the effectiveness of our\nalgorithm. \n\n"}
{"id": "1309.1110", "contents": "Title: When Backpressure Meets Predictive Scheduling Abstract: Motivated by the increasing popularity of learning and predicting human user\nbehavior in communication and computing systems, in this paper, we investigate\nthe fundamental benefit of predictive scheduling, i.e., predicting and\npre-serving arrivals, in controlled queueing systems. Based on a lookahead\nwindow prediction model, we first establish a novel equivalence between the\npredictive queueing system with a \\emph{fully-efficient} scheduling scheme and\nan equivalent queueing system without prediction. This connection allows us to\nanalytically demonstrate that predictive scheduling necessarily improves system\ndelay performance and can drive it to zero with increasing prediction power. We\nthen propose the \\textsf{Predictive Backpressure (PBP)} algorithm for achieving\noptimal utility performance in such predictive systems. \\textsf{PBP}\nefficiently incorporates prediction into stochastic system control and avoids\nthe great complication due to the exponential state space growth in the\nprediction window size. We show that \\textsf{PBP} can achieve a utility\nperformance that is within $O(\\epsilon)$ of the optimal, for any $\\epsilon>0$,\nwhile guaranteeing that the system delay distribution is a\n\\emph{shifted-to-the-left} version of that under the original Backpressure\nalgorithm. Hence, the average packet delay under \\textsf{PBP} is strictly\nbetter than that under Backpressure, and vanishes with increasing prediction\nwindow size. This implies that the resulting utility-delay tradeoff with\npredictive scheduling beats the known optimal $[O(\\epsilon),\nO(\\log(1/\\epsilon))]$ tradeoff for systems without prediction. \n\n"}
{"id": "1309.4978", "contents": "Title: An Analytical Model of Packet Collisions in IEEE 802.15.4 Wireless\n  Networks Abstract: Numerous studies showed that concurrent transmissions can boost wireless\nnetwork performance despite collisions. While these works provide empirical\nevidence that concurrent transmissions may be received reliably, existing\nsignal capture models only partially explain the root causes of this\nphenomenon. We present a comprehensive mathematical model that reveals the\nreasons and provides insights on the key parameters affecting the performance\nof MSK-modulated transmissions. A major contribution is a closed-form\nderivation of the receiver bit decision variable for arbitrary numbers of\ncolliding signals and constellations of power ratios, timing offsets, and\ncarrier phase offsets. We systematically explore the root causes for successful\npacket delivery under concurrent transmissions across the whole parameter space\nof the model. We confirm the capture threshold behavior observed in previous\nstudies but also reveal new insights relevant for the design of optimal\nprotocols: We identify capture zones depending not only on the signal power\nratio but also on time and phase offsets. \n\n"}
{"id": "1309.5491", "contents": "Title: Anticipatory Buffer Control and Quality Selection for Wireless Video\n  Streaming Abstract: Video streaming is in high demand by mobile users, as recent studies\nindicate. In cellular networks, however, the unreliable wireless channel leads\nto two major problems. Poor channel states degrade video quality and interrupt\nthe playback when a user cannot sufficiently fill its local playout buffer:\nbuffer underruns occur. In contrast to that, good channel conditions cause\ncommon greedy buffering schemes to pile up very long buffers. Such\nover-buffering wastes expensive wireless channel capacity.\n  To keep buffering in balance, we employ a novel approach. Assuming that we\ncan predict data rates, we plan the quality and download time of the video\nsegments ahead. This anticipatory scheduling avoids buffer underruns by\ndownloading a large number of segments before a channel outage occurs, without\nwasting wireless capacity by excessive buffering. We formalize this approach as\nan optimization problem and derive practical heuristics for segmented video\nstreaming protocols (e.g., HLS or MPEG DASH). Simulation results and testbed\nmeasurements show that our solution essentially eliminates playback\ninterruptions without significantly decreasing video quality. \n\n"}
{"id": "1309.5686", "contents": "Title: On the tradeoff of average delay and average power for fading\n  point-to-point links with monotone policies Abstract: We consider a fading point-to-point link with packets arriving randomly at\nrate $\\lambda$ per slot to the transmitter queue. We assume that the\ntransmitter can control the number of packets served in a slot by varying the\ntransmit power for the slot. We restrict to transmitter scheduling policies\nthat are monotone and stationary, i.e., the number of packets served is a\nnon-decreasing function of the queue length at the beginning of the slot for\nevery slot fade state. For such policies, we obtain asymptotic lower bounds for\nthe minimum average delay of the packets, when average transmitter power is a\nsmall positive quantity $V$ more than the minimum average power required for\ntransmitter queue stability. We show that the minimum average delay grows\neither to a finite value or as $\\Omega\\brap{\\log(1/V)}$ or $\\Omega\\brap{1/V}$\nwhen $V \\downarrow 0$, for certain sets of values of $\\lambda$. These sets are\ndetermined by the distribution of fading gain, the maximum number of packets\nwhich can be transmitted in a slot, and the transmit power function of the\nfading gain and the number of packets transmitted that is assumed. We identify\na case where the above behaviour of the tradeoff differs from that obtained\nfrom a previously considered approximate model, in which the random queue\nlength process is assumed to evolve on the non-negative real line, and the\ntransmit power function is strictly convex. We also consider a fading\npoint-to-point link, where the transmitter, in addition to controlling the\nnumber of packets served, can also control the number of packets admitted in\nevery slot. Our approach, which uses bounds on the stationary probability\ndistribution of the queue length, also leads to an intuitive explanation of the\nasymptotic behaviour of average delay in the regime where $V \\downarrow 0$. \n\n"}
{"id": "1309.7066", "contents": "Title: High Throughput Data Center Topology Design Abstract: With high throughput networks acquiring a crucial role in supporting\ndata-intensive applications, a variety of data center network topologies have\nbeen proposed to achieve high capacity at low cost. While this literature\nexplores a large number of design points, even in the limited case of a network\nof identical switches, no proposal has been able to claim any notion of\noptimality. The case of heterogeneous networks, incorporating multiple\nline-speeds and port-counts as data centers grow over time, introduces even\ngreater complexity.\n  In this paper, we present the first non-trivial upper-bound on network\nthroughput under uniform traffic patterns for any topology with identical\nswitches. We then show that random graphs achieve throughput surprisingly close\nto this bound, within a few percent at the scale of a few thousand servers.\nApart from demonstrating that homogeneous topology design may be reaching its\nlimits, this result also motivates our use of random graphs as building blocks\nto explore the design of heterogeneous networks. Given a heterogeneous pool of\nnetwork switches, through experiments and analysis, we explore how the\ndistribution of servers across switches and the interconnection of switches\naffect network throughput. We apply these insights to a real-world\nheterogeneous data center topology, VL2, demonstrating as much as 43% higher\nthroughput with the same equipment. \n\n"}
{"id": "1309.7527", "contents": "Title: Structured Spectrum Allocation and User Association in Heterogeneous\n  Cellular Networks Abstract: We study joint spectrum allocation and user association in heterogeneous\ncellular networks with multiple tiers of base stations. A stochastic geometric\napproach is applied as the basis to derive the average downlink user data rate\nin a closed-form expression. Then, the expression is employed as the objective\nfunction in jointly optimizing spectrum allocation and user association, which\nis of non-convex programming in nature. A computationally efficient Structured\nSpectrum Allocation and User Association (SSAUA) approach is proposed, solving\nthe optimization problem optimally when the density of users is low, and\nnear-optimally with a guaranteed performance bound when the density of users is\nhigh. A Surcharge Pricing Scheme (SPS) is also presented, such that the\ndesigned association bias values can be achieved in Nash equilibrium.\nSimulations and numerical studies are conducted to validate the accuracy and\nefficiency of the proposed SSAUA approach and SPS. \n\n"}
{"id": "1310.1142", "contents": "Title: Non-Arbitrage up to Random Horizon for Semimartingale Models Abstract: This paper addresses the question of how an arbitrage-free semimartingale\nmodel is affected when stopped at a random horizon. We focus on\nNo-Unbounded-Profit-with-Bounded-Risk (called NUPBR hereafter) concept, which\nis also known in the literature as the first kind of non-arbitrage. For this\nnon-arbitrage notion, we obtain two principal results. The first result lies in\ndescribing the pairs of market model and random time for which the resulting\nstopped model fulfills NUPBR condition. The second main result characterises\nthe random time models that preserve the NUPBR property after stopping for any\nmarket model. These results are elaborated in a very general market model, and\nwe also pay attention to some particular and practical models. The analysis\nthat drives these results is based on new stochastic developments in\nsemimartingale theory with progressive enlargement. Furthermore, we construct\nexplicit martingale densities (deflators) for some classes of local martingales\nwhen stopped at random time. \n\n"}
{"id": "1310.4403", "contents": "Title: Complexity, economic science and possible economic benefits of climate\n  change mitigation policy Abstract: Conventional economic analysis of stringent climate change mitigation policy\ngenerally concludes various levels of economic slowdown as a result of\nsubstantial spending on low carbon technology. Equilibrium economics however\ncould not explain or predict the current economic crisis, which is of financial\nnature. Meanwhile the economic impacts of climate policy find their source\nthrough investments for the diffusion of environmental innovations, in parts a\nfinancial problem. Here, we expose how results of economic analysis of climate\nchange mitigation policy depend entirely on assumptions and theory concerning\nthe finance of the diffusion of innovations, and that in many cases, results\nare simply re-iterations of model assumptions. We show that, while equilibrium\neconomics always predict economic slowdown, methods using non-equilibrium\napproaches suggest the opposite could occur. We show that the solution to\nunderstanding the economic impacts of reducing greenhouse gas emissions lies\nwith research on the dynamics of the financial sector interacting with\ninnovation and technology developments, economic history providing powerful\ninsights through important analogies with previous historical waves of\ninnovation. \n\n"}
{"id": "1310.4761", "contents": "Title: Towards Energy Neutrality in Energy Harvesting Wireless Sensor Networks:\n  A Case for Distributed Compressive Sensing? Abstract: This paper advocates the use of the emerging distributed compressive sensing\n(DCS) paradigm in order to deploy energy harvesting (EH) wireless sensor\nnetworks (WSN) with practical network lifetime and data gathering rates that\nare substantially higher than the state-of-the-art. In particular, we argue\nthat there are two fundamental mechanisms in an EH WSN: i) the energy diversity\nassociated with the EH process that entails that the harvested energy can vary\nfrom sensor node to sensor node, and ii) the sensing diversity associated with\nthe DCS process that entails that the energy consumption can also vary across\nthe sensor nodes without compromising data recovery. We also argue that such\nmechanisms offer the means to match closely the energy demand to the energy\nsupply in order to unlock the possibility for energy-neutral WSNs that leverage\nEH capability. A number of analytic and simulation results are presented in\norder to illustrate the potential of the approach. \n\n"}
{"id": "1310.4777", "contents": "Title: Content-Specific Broadcast Cellular Networks based on User Demand\n  Prediction: A Revenue Perspective Abstract: The Long Term Evolution (LTE) broadcast is a promising solution to cope with\nexponentially increasing user traffic by broadcasting common user requests over\nthe same frequency channels. In this paper, we propose a novel network\nframework provisioning broadcast and unicast services simultaneously. For each\nserving file to users, a cellular base station determines either to broadcast\nor unicast the file based on user demand prediction examining the file's\ncontent specific characteristics such as: file size, delay tolerance, price\nsensitivity. In a network operator's revenue maximization perspective while not\ninflicting any user payoff degradation, we jointly optimize resource\nallocation, pricing, and file scheduling. In accordance with the state of the\nart LTE specifications, the proposed network demonstrates up to 32% increase in\nrevenue for a single cell and more than a 7-fold increase for a 7 cell\ncoordinated LTE broadcast network, compared to the conventional unicast\ncellular networks. \n\n"}
{"id": "1310.4907", "contents": "Title: Message and time efficient multi-broadcast schemes Abstract: We consider message and time efficient broadcasting and multi-broadcasting in\nwireless ad-hoc networks, where a subset of nodes, each with a unique rumor,\nwish to broadcast their rumors to all destinations while minimizing the total\nnumber of transmissions and total time until all rumors arrive to their\ndestination. Under centralized settings, we introduce a novel approximation\nalgorithm that provides almost optimal results with respect to the number of\ntransmissions and total time, separately. Later on, we show how to efficiently\nimplement this algorithm under distributed settings, where the nodes have only\nlocal information about their surroundings. In addition, we show multiple\napproximation techniques based on the network collision detection capabilities\nand explain how to calibrate the algorithms' parameters to produce optimal\nresults for time and messages. \n\n"}
{"id": "1310.6635", "contents": "Title: Network Coded TCP (CTCP) Performance over Satellite Networks Abstract: We show preliminary results for the performance of Network Coded TCP (CTCP)\nover large latency networks. While CTCP performs very well in networks with\nrelatively short RTT, the slow-start mechanism currently employed does not\nadequately fill the available bandwidth when the RTT is large. Regardless, we\nshow that CTCP still outperforms current TCP variants (i.e., Cubic TCP and\nHybla TCP) for high packet loss rates (e.g., >2.5%). We then explore the\npossibility of a modified congestion control mechanism based off of H-TCP that\nopens the congestion window quickly to overcome the challenges of large latency\nnetworks. Preliminary results are provided that show the combination of network\ncoding with an appropriate congestion control algorithm can provide gains on\nthe order of 20 times that of existing TCP variants. Finally, we provide a\ndiscussion of the future work needed to increase CTCP's performance in these\nnetworks. \n\n"}
{"id": "1310.8135", "contents": "Title: Large-Scale Sensor Network Localization via Rigid Subnetwork\n  Registration Abstract: In this paper, we describe an algorithm for sensor network localization (SNL)\nthat proceeds by dividing the whole network into smaller subnetworks, then\nlocalizes them in parallel using some fast and accurate algorithm, and finally\nregisters the localized subnetworks in a global coordinate system. We\ndemonstrate that this divide-and-conquer algorithm can be used to leverage\nexisting high-precision SNL algorithms to large-scale networks, which could\notherwise only be applied to small-to-medium sized networks. The main\ncontribution of this paper concerns the final registration phase. In\nparticular, we consider a least-squares formulation of the registration problem\n(both with and without anchor constraints) and demonstrate how this otherwise\nnon-convex problem can be relaxed into a tractable convex program. We provide\nsome preliminary simulation results for large-scale SNL demonstrating that the\nproposed registration algorithm (together with an accurate localization scheme)\noffers a good tradeoff between run time and accuracy. \n\n"}
{"id": "1311.4601", "contents": "Title: Achievable Rate Regions for Network Coding Abstract: Determining the achievable rate region for networks using routing, linear\ncoding, or non-linear coding is thought to be a difficult task in general, and\nfew are known. We describe the achievable rate regions for four interesting\nnetworks (completely for three and partially for the fourth). In addition to\nthe known matrix-computation method for proving outer bounds for linear coding,\nwe present a new method which yields actual characteristic-dependent linear\nrank inequalities from which the desired bounds follow immediately. \n\n"}
{"id": "1311.4798", "contents": "Title: The multiplex structure of interbank networks Abstract: The interbank market has a natural multiplex network representation. We\nemploy a unique database of supervisory reports of Italian banks to the Banca\nd'Italia that includes all bilateral exposures broken down by maturity and by\nthe secured and unsecured nature of the contract. We find that layers have\ndifferent topological properties and persistence over time. The presence of a\nlink in a layer is not a good predictor of the presence of the same link in\nother layers. Maximum entropy models reveal different unexpected substructures,\nsuch as network motifs, in different layers. Using the total interbank network\nor focusing on a specific layer as representative of the other layers provides\na poor representation of interlinkages in the interbank market and could lead\nto biased estimation of systemic risk. \n\n"}
{"id": "1311.6187", "contents": "Title: Pathwise stochastic integrals for model free finance Abstract: We present two different approaches to stochastic integration in frictionless\nmodel free financial mathematics. The first one is in the spirit of It\\^o's\nintegral and based on a certain topology which is induced by the outer measure\ncorresponding to the minimal superhedging price. The second one is based on the\ncontrolled rough path integral. We prove that every \"typical price path\" has a\nnaturally associated It\\^o rough path, and justify the application of the\ncontrolled rough path integral in finance by showing that it is the limit of\nnon-anticipating Riemann sums, a new result in itself. Compared to the first\napproach, rough paths have the disadvantage of severely restricting the space\nof integrands, but the advantage of being a Banach space theory. Both\napproaches are based entirely on financial arguments and do not require any\nprobabilistic structure. \n\n"}
{"id": "1312.0323", "contents": "Title: Towards a microeconomic theory of the finance-driven business cycle Abstract: I sketch a program for a microeconomic theory of the main component of the\nbusiness cycle as a recurring disequilibrium, driven by incompleteness of the\nfinancial market and by information asymmetries between borrowers and lenders.\nThis proposal seeks to incorporate five distinct but connected processes that\nhave been discussed at varying lengths in the literature: the leverage cycle,\nfinancial panic, debt deflation, debt overhang, and deleveraging of households.\nIn the wake of the 2007-08 financial crisis, policy responses by central banks\nhave addressed only financial panic and debt deflation. Debt overhang and the\nslowness of household deleveraging account for the Keynesian \"excessive saving\"\nseen in recessions, which raises questions about the suitability of the\nstandard Keynesian remedies. \n\n"}
{"id": "1312.0825", "contents": "Title: FRANTIC: A Fast Reference-based Algorithm for Network Tomography via\n  Compressive Sensing Abstract: We study the problem of link and node delay estimation in undirected networks\nwhen at most k out of n links or nodes in the network are congested. Our\napproach relies on end-to-end measurements of path delays across pre-specified\npaths in the network. We present a class of algorithms that we call FRANTIC.\nThe FRANTIC algorithms are motivated by compressive sensing; however, unlike\ntraditional compressive sensing, the measurement design here is constrained by\nthe network topology and the matrix entries are constrained to be positive\nintegers. A key component of our design is a new compressive sensing algorithm\nSHO-FA-INT that is related to the prior SHO-FA algorithm for compressive\nsensing, but unlike SHO-FA, the matrix entries here are drawn from the set of\nintegers {0, 1, ..., M}. We show that O(k log n /log M) measurements suffice\nboth for SHO-FA-INT and FRANTIC. Further, we show that the computational\ncomplexity of decoding is also O(k log n/log M) for each of these algorithms.\nFinally, we look at efficient constructions of the measurement operations\nthrough Steiner Trees. \n\n"}
{"id": "1312.2004", "contents": "Title: Optimal Trading Strategies as Measures of Market Disequilibrium Abstract: For classification of the high frequency trading quantities, waiting times,\nprice increments within and between sessions are referred to as the a-, b-, and\nc-increments. Statistics of the a-b-c-increments are computed for the Time &\nSales records posted by the Chicago Mercantile Exchange Group for the futures\ntraded on Globex. The Weibull, Kumaraswamy, Riemann and Hurwitz Zeta,\nparabolic, Zipf-Mandelbrot distributions are tested for the a- and\nb-increments. A discrete version of the Fisher-Tippett distribution is\nsuggested for approximating the extreme b-increments. Kolmogorov and Uspenskii\nclassification of stochastic, typical, and chaotic random sequences is reviewed\nwith regard to the futures price limits. Non-parametric L1 and log-likelihood\ntests are applied to check dependencies between the a- and b-increments. The\nmaximum profit strategies and optimal trading elements are suggested as\nmeasures of frequency and magnitude of the market offers and disequilibrium.\nEmpirical cumulative distribution functions of optimal profits are reported. A\nfew classical papers are reviewed with more details in order to trace the\norigin and foundation of modern finance. \n\n"}
{"id": "1312.2177", "contents": "Title: Machine Learning Techniques for Intrusion Detection Abstract: An Intrusion Detection System (IDS) is a software that monitors a single or a\nnetwork of computers for malicious activities (attacks) that are aimed at\nstealing or censoring information or corrupting network protocols. Most\ntechniques used in today's IDS are not able to deal with the dynamic and\ncomplex nature of cyber attacks on computer networks. Hence, efficient adaptive\nmethods like various techniques of machine learning can result in higher\ndetection rates, lower false alarm rates and reasonable computation and\ncommunication costs. In this paper, we study several such schemes and compare\ntheir performance. We divide the schemes into methods based on classical\nartificial intelligence (AI) and methods based on computational intelligence\n(CI). We explain how various characteristics of CI techniques can be used to\nbuild efficient IDS. \n\n"}
{"id": "1312.2466", "contents": "Title: An Analog Baseband Approach for Designing Full-Duplex Radios Abstract: Recent wireless testbed implementations have proven that full-duplex\ncommunication is in fact possible and can outperform half-duplex systems. Many\nof these implementations modify existing half-duplex systems to operate in\nfull-duplex. To realize the full potential of full-duplex, radios need to be\ndesigned with self-interference in mind. In our work, we use an experimental\nsetup with a patch antenna prototype to characterize the self-interference\nchannel between two radios. In doing so, we form an analytical model to design\nanalog baseband cancellation techniques. We show that our cancellation scheme\ncan provide up to 10 dB improved signal strength, 2.5 bps/Hz increase in rate,\nand a 10,000 improvement in BER as compared to the RF only cancellation\nprovided by the patch antenna. \n\n"}
{"id": "1401.0124", "contents": "Title: Mean field approximation for biased diffusion on Japanese inter-firm\n  trading network Abstract: By analysing the financial data of firms across Japan, a nonlinear power law\nwith an exponent of 1.3 was observed between the number of business partners\n(i.e. the degree of the inter-firm trading network) and sales. In a previous\nstudy using numerical simulations, we found that this scaling can be explained\nby both the money-transport model, where a firm (i.e. customer) distributes\nmoney to its out-edges (suppliers) in proportion to the in-degree of\ndestinations, and by the correlations among the Japanese inter-firm trading\nnetwork. However, in this previous study, we could not specifically identify\nwhat types of structure properties (or correlations) of the network determine\nthe 1.3 exponent. In the present study, we more clearly elucidate the\nrelationship between this nonlinear scaling and the network structure by\napplying mean-field approximation of the diffusion in a complex network to this\nmoney-transport model. Using theoretical analysis, we obtained the mean-field\nsolution of the model and found that, in the case of the Japanese firms, the\nscaling exponent of 1.3 can be determined from the power law of the average\ndegree of the nearest neighbours of the network with an exponent of -0.7. \n\n"}
{"id": "1401.1191", "contents": "Title: DASS: Distributed Adaptive Sparse Sensing Abstract: Wireless sensor networks are often designed to perform two tasks: sensing a\nphysical field and transmitting the data to end-users. A crucial aspect of the\ndesign of a WSN is the minimization of the overall energy consumption. Previous\nresearchers aim at optimizing the energy spent for the communication, while\nmostly ignoring the energy cost due to sensing. Recently, it has been shown\nthat considering the sensing energy cost can be beneficial for further\nimproving the overall energy efficiency. More precisely, sparse sensing\ntechniques were proposed to reduce the amount of collected samples and recover\nthe missing data by using data statistics. While the majority of these\ntechniques use fixed or random sampling patterns, we propose to adaptively\nlearn the signal model from the measurements and use the model to schedule when\nand where to sample the physical field. The proposed method requires minimal\non-board computation, no inter-node communications and still achieves appealing\nreconstruction performance. With experiments on real-world datasets, we\ndemonstrate significant improvements over both traditional sensing schemes and\nthe state-of-the-art sparse sensing schemes, particularly when the measured\ndata is characterized by a strong intra-sensor (temporal) or inter-sensors\n(spatial) correlation. \n\n"}
{"id": "1401.2560", "contents": "Title: Millimeter Wave Cellular Wireless Networks: Potentials and Challenges Abstract: Millimeter wave (mmW) frequencies between 30 and 300 GHz are a new frontier\nfor cellular communication that offers the promise of orders of magnitude\ngreater bandwidths combined with further gains via beamforming and spatial\nmultiplexing from multi-element antenna arrays. This paper surveys measurements\nand capacity studies to assess this technology with a focus on small cell\ndeployments in urban environments. The conclusions are extremely encouraging;\nmeasurements in New York City at 28 and 73 GHz demonstrate that, even in an\nurban canyon environment, significant non-line-of-sight (NLOS) outdoor,\nstreet-level coverage is possible up to approximately 200 m from a potential\nlow power micro- or picocell base station. In addition, based on statistical\nchannel models from these measurements, it is shown that mmW systems can offer\nmore than an order of magnitude increase in capacity over current\nstate-of-the-art 4G cellular networks at current cell densities. Cellular\nsystems, however, will need to be significantly redesigned to fully achieve\nthese gains. Specifically, the requirement of highly directional and adaptive\ntransmissions, directional isolation between links and significant\npossibilities of outage have strong implications on multiple access, channel\nstructure, synchronization and receiver design. To address these challenges,\nthe paper discusses how various technologies including adaptive beamforming,\nmultihop relaying, heterogeneous network architectures and carrier aggregation\ncan be leveraged in the mmW context. \n\n"}
{"id": "1401.2853", "contents": "Title: Technical Report: Sleep-Route - Routing through Sleeping Sensors Abstract: In this article, we propose an energy-efficient data gathering scheme for\nwireless sensor network called Sleep-Route, which splits the sensor nodes into\ntwo sets - active and dormant (low-power sleep). Only the active set of sensor\nnodes participate in data collection. The sensing values of the dormant sensor\nnodes are predicted with the help of an active sensor node. Virtual Sensing\nFramework (VSF) provides the mechanism to predict the sensing values by\nexploiting the data correlation among the sensor nodes. If the number of active\nsensor nodes can be minimized, a lot of energy can be saved. The active nodes'\nselection must fulfill the following constraints - (i) the set of active nodes\nare sufficient to predict the sensing values of the dormant nodes, (ii) each\nactive sensor nodes can report their data to the sink node (directly or through\nsome other active node(s)). The goal is to select a minimal number of active\nsensor nodes so that energy savings can be maximized.\n  The optimal set of active node selection raise a combinatorial optimization\nproblem, which we refer as Sleep-Route problem. We show that Sleep-Route\nproblem is NP-hard. Then, we formulate an integer linear program (ILP) to solve\nthe problem optimally. To solve the problem in polynomial time, we also propose\na heuristic algorithm that performs near optimally. \n\n"}
{"id": "1401.3174", "contents": "Title: Comments on \"Optimal Utilization of a Cognitive Shared Channel with a\n  Rechargeable Primary Source Node\" Abstract: In a recent paper [1], the authors investigated the maximum stable throughput\nregion of a network composed of a rechargeable primary user and a secondary\nuser plugged to a reliable power supply. The authors studied the cases of an\ninfinite and a finite energy queue at the primary transmitter. However, the\nresults of the finite case are incorrect. We show that under the proposed\nenergy queue model (a decoupled ${\\rm M/D/1}$ queueing system with Bernoulli\narrivals and the consumption of one energy packet per time slot), the energy\nqueue capacity does not affect the stability region of the network. \n\n"}
{"id": "1401.3677", "contents": "Title: The Ginibre Point Process as a Model for Wireless Networks with\n  Repulsion Abstract: The spatial structure of transmitters in wireless networks plays a key role\nin evaluating the mutual interference and hence the performance. Although the\nPoisson point process (PPP) has been widely used to model the spatial\nconfiguration of wireless networks, it is not suitable for networks with\nrepulsion. The Ginibre point process (GPP) is one of the main examples of\ndeterminantal point processes that can be used to model random phenomena where\nrepulsion is observed. Considering the accuracy, tractability and\npracticability tradeoffs, we introduce and promote the $\\beta$-GPP, an\nintermediate class between the PPP and the GPP, as a model for wireless\nnetworks when the nodes exhibit repulsion. To show that the model leads to\nanalytically tractable results in several cases of interest, we derive the mean\nand variance of the interference using two different approaches: the Palm\nmeasure approach and the reduced second moment approach, and then provide\napproximations of the interference distribution by three known probability\ndensity functions. Besides, to show that the model is relevant for cellular\nsystems, we derive the coverage probability of the typical user and also find\nthat the fitted $\\beta$-GPP can closely model the deployment of actual base\nstations in terms of the coverage probability and other statistics. \n\n"}
{"id": "1401.4005", "contents": "Title: Studying the SINR process of the typical user in Poisson networks by\n  using its factorial moment measures Abstract: Based on a stationary Poisson point process, a wireless network model with\nrandom propagation effects (shadowing and/or fading) is considered in order to\nexamine the process formed by the signal-to-interference-plus-noise ratio\n(SINR) values experienced by a typical user with respect to all base stations\nin the down-link channel. This SINR process is completely characterized by\nderiving its factorial moment measures, which involve numerically tractable,\nexplicit integral expressions. This novel framework naturally leads to\nexpressions for the k-coverage probability, including the case of random SINR\nthreshold values considered in multi-tier network models. While the k-coverage\nprobabilities correspond to the marginal distributions of the order statistics\nof the SINR process, a more general relation is presented connecting the\nfactorial moment measures of the SINR process to the joint densities of these\norder statistics. This gives a way for calculating exact values of the coverage\nprobabilities arising in a general scenario of signal combination and\ninterference cancellation between base stations. The presented framework\nconsisting of mathematical representations of SINR characteristics with respect\nto the factorial moment measures holds for the whole domain of SINR and is\namenable to considerable model extension. \n\n"}
{"id": "1401.5099", "contents": "Title: A Stable Fountain Code Mechanism for Peer-to-Peer Content Distribution Abstract: Most peer-to-peer content distribution systems require the peers to privilege\nthe welfare of the overall system over greedily maximizing their own utility.\nWhen downloading a file broken up into multiple pieces, peers are often asked\nto pass on some possible download opportunities of common pieces in order to\nfavor rare pieces. This is to avoid the missing piece syndrome, which throttles\nthe download rate of the peer-to-peer system to that of downloading the file\nstraight from the server. In other situations, peers are asked to stay in the\nsystem even though they have collected all the file's pieces and have an\nincentive to leave right away.\n  We propose a mechanism which allows peers to act greedily and yet stabilizes\nthe peer-to-peer content sharing system. Our mechanism combines a fountain code\nat the server to generate innovative new pieces, and a prioritization for the\nserver to deliver pieces only to new peers. While by itself, neither the\nfountain code nor the prioritization of new peers alone stabilizes the system,\nwe demonstrate that their combination does, through both analytical and\nnumerical evaluation. \n\n"}
{"id": "1401.6145", "contents": "Title: On Stochastic Geometry Modeling of Cellular Uplink Transmission with\n  Truncated Channel Inversion Power Control Abstract: Using stochastic geometry, we develop a tractable uplink modeling paradigm\nfor outage probability and spectral efficiency in both single and multi-tier\ncellular wireless networks. The analysis accounts for per user equipment (UE)\npower control as well as the maximum power limitations for UEs. More\nspecifically, for interference mitigation and robust uplink communication, each\nUE is required to control its transmit power such that the average received\nsignal power at its serving base station (BS) is equal to a certain threshold\n$\\rho_o$. Due to the limited transmit power, the UEs employ a truncated channel\ninversion power control policy with a cutoff threshold of $\\rho_o$. We show\nthat there exists a transfer point in the uplink system performance that\ndepends on the tuple: BS intensity ($\\lambda$), maximum transmit power of UEs\n($P_u$), and $\\rho_o$. That is, when $P_u$ is a tight operational constraint\nwith respect to [w.r.t.] $\\lambda$ and $\\rho_o$, the uplink outage probability\nand spectral efficiency highly depend on the values of $\\lambda$ and $\\rho_o$.\nIn this case, there exists an optimal cutoff threshold $\\rho^*_o$, which\ndepends on the system parameters, that minimizes the outage probability. On the\nother hand, when $P_u$ is not a binding operational constraint w.r.t. $\\lambda$\nand $\\rho_o$, the uplink outage probability and spectral efficiency become\nindependent of $\\lambda$ and $\\rho_o$. We obtain approximate yet accurate\nsimple expressions for outage probability and spectral efficiency which reduce\nto closed-forms in some special cases. \n\n"}
{"id": "1402.1761", "contents": "Title: On Scalability of Wireless Networks: A Practical Primer for Large Scale\n  Cooperation Abstract: An intuitive overview of the scalability of a variety of types of wireless\nnetworks is presented. Simple heuris- tic arguments are demonstrated here for\nscaling laws presented in other works, as well as for conditions not previously\nconsidered in the literature. Unicast and multicast messages, topology,\nhierarchy, and effects of reliability protocols are discussed. We show how two\nkey factors, bottlenecks and erasures, can often domi- nate the network scaling\nbehavior. Scaling of through- put or delay with the number of transmitting\nnodes, the number of receiving nodes, and the file size is described. \n\n"}
{"id": "1402.2596", "contents": "Title: On Arbitrage and Duality under Model Uncertainty and Portfolio\n  Constraints Abstract: We consider the fundamental theorem of asset pricing (FTAP) and hedging\nprices of options under non-dominated model uncertainty and portfolio\nconstrains in discrete time. We first show that no arbitrage holds if and only\nif there exists some family of probability measures such that any admissible\nportfolio value process is a local super-martingale under these measures. We\nalso get the non-dominated optional decomposition with constraints. From this\ndecomposition, we get duality of the super-hedging prices of European options,\nas well as the sub- and super-hedging prices of American options. Finally, we\nget the FTAP and duality of super-hedging prices in a market where stocks are\ntraded dynamically and options are traded statically. \n\n"}
{"id": "1402.3225", "contents": "Title: Market-Based Power Allocation for a Differentially Priced FDMA System Abstract: In this paper, we study the problem of differential pricing and QoS\nassignment by a broadband data provider. In our model, the broadband data\nprovider decides on the power allocated to an end-user not only based on\nparameters of the transmission medium, but also based on the price the user is\nwilling to pay. In addition, end-users bid the price that they are willing to\npay to the BTS based on their channel condition, the throughput they require,\nand their belief about other users' parameters. We will characterize the\noptimum power allocation by the BTS which turns out to be a modification of the\nsolution to the well-known water-filling problem. We also characterize the\noptimum bidding strategy of end-users using the belief of each user about the\ncell condition. \n\n"}
{"id": "1402.4171", "contents": "Title: Reconstructing the world trade multiplex: the role of intensive and\n  extensive biases Abstract: In economic and financial networks, the strength of each node has always an\nimportant economic meaning, such as the size of supply and demand, import and\nexport, or financial exposure. Constructing null models of networks matching\nthe observed strengths of all nodes is crucial in order to either detect\ninteresting deviations of an empirical network from economically meaningful\nbenchmarks or reconstruct the most likely structure of an economic network when\nthe latter is unknown. However, several studies have proved that real economic\nnetworks and multiplexes are topologically very different from configurations\ninferred only from node strengths. Here we provide a detailed analysis of the\nWorld Trade Multiplex by comparing it to an enhanced null model that\nsimultaneously reproduces the strength and the degree of each node. We study\nseveral temporal snapshots and almost one hundred layers (commodity classes) of\nthe multiplex and find that the observed properties are systematically well\nreproduced by our model. Our formalism allows us to introduce the (static)\nconcept of extensive and intensive bias, defined as a measurable tendency of\nthe network to prefer either the formation of extra links or the reinforcement\nof link weights, with respect to a reference case where only strengths are\nenforced. Our findings complement the existing economic literature on (dynamic)\nintensive and extensive trade margins. More in general, they show that\nreal-world multiplexes can be strongly shaped by layer-specific local\nconstraints. \n\n"}
{"id": "1402.4576", "contents": "Title: On the Average Performance of Caching and Coded Multicasting with Random\n  Demands Abstract: For a network with one sender, $n$ receivers (users) and $m$ possible\nmessages (files), caching side information at the users allows to satisfy\narbitrary simultaneous demands by sending a common (multicast) coded message.\nIn the worst-case demand setting, explicit deterministic and random caching\nstrategies and explicit linear coding schemes have been shown to be order\noptimal. In this work, we consider the same scenario where the user demands are\nrandom i.i.d., according to a Zipf popularity distribution. In this case, we\npose the problem in terms of the minimum average number of equivalent message\ntransmissions. We present a novel decentralized random caching placement and a\ncoded delivery scheme which are shown to achieve order-optimal performance. As\na matter of fact, this is the first order-optimal result for the caching and\ncoded multicasting problem in the case of random demands. \n\n"}
{"id": "1402.5207", "contents": "Title: How to Scale Exponential Backoff Abstract: Randomized exponential backoff is a widely deployed technique for\ncoordinating access to a shared resource. A good backoff protocol should,\narguably, satisfy three natural properties: (i) it should provide constant\nthroughput, wasting as little time as possible; (ii) it should require few\nfailed access attempts, minimizing the amount of wasted effort; and (iii) it\nshould be robust, continuing to work efficiently even if some of the access\nattempts fail for spurious reasons. Unfortunately, exponential backoff has some\nwell-known limitations in two of these areas: it provides poor (sub-constant)\nthroughput (in the worst case), and is not robust (to resource acquisition\nfailures).\n  The goal of this paper is to \"fix\" exponential backoff by making it scalable,\nparticularly focusing on the case where processes arrive in an on-line,\nworst-case fashion. We present a relatively simple backoff\nprotocol~Re-Backoff~that has, at its heart, a version of exponential backoff.\nIt guarantees expected constant throughput with dynamic process arrivals and\nrequires only an expected polylogarithmic number of access attempts per\nprocess.\n  Re-Backoff is also robust to periods where the shared resource is unavailable\nfor a period of time. If it is unavailable for $D$ time slots, Re-Backoff\nprovides the following guarantees. When the number of packets is a finite $n$,\nthe average expected number of access attempts for successfully sending a\npacket is $O(\\log^2( n + D))$. In the infinite case, the average expected\nnumber of access attempts for successfully sending a packet is $O( \\log^2(\\eta)\n+ \\log^2(D) )$ where $\\eta$ is the maximum number of processes that are ever in\nthe system concurrently. \n\n"}
{"id": "1403.1086", "contents": "Title: Recovering from Derivatives Funding: A consistent approach to DVA, FVA\n  and Hedging Abstract: The inclusion of DVA in the fair-value of derivative transactions has now\nbecome standard accounting practice in most parts of the world. Furthermore,\nsome sophisticated banks are including an FVA (Funding Valuation Adjustment),\nbut since DVA can be interpreted as a funding benefit the oft-debated issue\nregarding a possible double-counting of funding benefits arises, with little\nconsensus as to its resolution. One possibility is to price the derivative by\nreplication, guaranteeing a consistent inclusion of costs and benefits.\nHowever, as has recently been noted, DVA is (at least partially) unhedgeable,\nhaving no exact market hedge. Furthermore, current frameworks shed little light\non the controversial question, raised by Hull (2012), of whether the effect a\nderivative has on the riskiness of an institution's debt should be taken into\naccount when calculating FVA.\n  In this paper we propose a solution to these two problems by identifying an\ninstrument, a fictitious CDS written on the hedging counterparty which is\nimplicitly contained in any given derivatives transaction. This allows us to\nshow that the hedger's unhedged jump-to-default risk has, despite not being\nactively managed, a well defined value associated to a funding benefit.\nCarrying out the replication including such a CDS, we obtain a price for the\nderivative consisting of its collateralized equivalent, a contingent CVA, a\ncontingent DVA, and an FVA, coupled to the price via the hedger's short-term\nbond-CDS basis.\n  The resulting funding cost is non-zero, but substantially smaller than what\nis obtained in alternative approaches due to the effect the derivative has on\nthe recovery of the hedger's liabilities. Also, price agreement is possible for\ntwo sophisticated counterparties entering a deal if their bond-CDS bases obey a\ncertain relationship, similar to what was first obtained by Morini and\nPrampolini (2010). \n\n"}
{"id": "1403.5007", "contents": "Title: On Throughput-Delay Optimal Access to Storage Clouds via Load Adaptive\n  Coding and Chunking Abstract: Recent literature including our past work provide analysis and solutions for\nusing (i) erasure coding, (ii) parallelism, or (iii) variable slicing/chunking\n(i.e., dividing an object of a specific size into a variable number of smaller\nchunks) in speeding the I/O performance of storage clouds. However, a\ncomprehensive approach that considers all three dimensions together to achieve\nthe best throughput-delay trade-off curve had been lacking. This paper presents\nthe first set of solutions that can pick the best combination of coding rate\nand object chunking/slicing options as the load dynamically changes. Our\nspecific contributions are as follows: (1) We establish via measurement that\ncombining variable coding rate and chunking is mostly feasible over a popular\npublic cloud. (2) We relate the delay optimal values for chunking level and\ncode rate to the queue backlogs via an approximate queueing analysis. (3) Based\non this analysis, we propose TOFEC that adapts the chunking level and coding\nrate against the queue backlogs. Our trace-driven simulation results show that\nTOFEC's adaptation mechanism converges to an appropriate code that provides the\noptimal throughput-delay trade-off without reducing system capacity. Compared\nto a non-adaptive strategy optimized for throughput, TOFEC delivers $2.5\\times$\nlower latency under light workloads; compared to a non-adaptive strategy\noptimized for latency, TOFEC can scale to support over $3\\times$ as many\nrequests. (4) We propose a simpler greedy solution that performs on a par with\nTOFEC in average delay performance, but exhibits significantly more performance\nvariations. \n\n"}
{"id": "1403.5479", "contents": "Title: Catalog Dynamics: Impact of Content Publishing and Perishing on the\n  Performance of a LRU Cache Abstract: The Internet heavily relies on Content Distribution Networks and transparent\ncaches to cope with the ever-increasing traffic demand of users. Content,\nhowever, is essentially versatile: once published at a given time, its\npopularity vanishes over time. All requests for a given document are then\nconcentrated between the publishing time and an effective perishing time.\n  In this paper, we propose a new model for the arrival of content requests,\nwhich takes into account the dynamical nature of the content catalog. Based on\ntwo large traffic traces collected on the Orange network, we use the\nsemi-experimental method and determine invariants of the content request\nprocess. This allows us to define a simple mathematical model for content\nrequests; by extending the so-called \"Che approximation\", we then compute the\nperformance of a LRU cache fed with such a request process, expressed by its\nhit ratio. We numerically validate the good accuracy of our model by comparison\nto trace-based simulation. \n\n"}
{"id": "1403.5828", "contents": "Title: A Survey on Network Tomography with Network Coding Abstract: The overhead of internal network monitoring motivates techniques of network\ntomography. Network coding (NC) presents a new opportunity for network\ntomography as NC introduces topology-dependent correlation that can be further\nexploited in topology estimation. Compared with traditional methods, network\ntomography with NC has many advantages such as the improvement of tomography\naccuracy and the reduction of complexity in choosing monitoring paths. In this\npaper we first introduce the problem of tomography with NC and then propose the\ntaxonomy criteria to classify various methods. We also present existing\nsolutions and future trend. We expect that our comprehensive review on network\ntomography with NC can serve as a good reference for researchers and\npractitioners working in the area. \n\n"}
{"id": "1403.7179", "contents": "Title: Modelling Returns and Volatilities During Financial Crises: a Time\n  Varying Coefficient Approach Abstract: We examine how the most prevalent stochastic properties of key financial time\nseries have been affected during the recent financial crises. In particular we\nfocus on changes associated with the remarkable economic events of the last two\ndecades in the mean and volatility dynamics, including the underlying\nvolatility persistence and volatility spillovers structure. Using daily data\nfrom several key stock market indices we find that stock market returns exhibit\ntime varying persistence in their corresponding conditional variances.\nFurthermore, the results of our bivariate GARCH models show the existence of\ntime varying correlations as well as time varying shock and volatility\nspillovers between the returns of FTSE and DAX, and those of NIKKEI and Hang\nSeng, which became more prominent during the recent financial crisis. Our\ntheoretical considerations on the time varying model which provides the\nplatform upon which we integrate our multifaceted empirical approaches are also\nof independent interest. In particular, we provide the general solution for low\norder time varying specifications, which is a long standing research topic.\nThis enables us to characterize these models by deriving, first, their\nmultistep ahead predictors, second, the first two time varying unconditional\nmoments, and third, their covariance structure. \n\n"}
{"id": "1403.7192", "contents": "Title: Delay Analysis of Multichannel Opportunistic Spectrum Access MAC\n  Protocols Abstract: We provide in this paper a comprehensive delay and queueing analysis for two\nbaseline medium access control (MAC) protocols for multi-user cognitive radio\n(CR) networks and investigate the impact of different network parameters, such\nas packet size, Aloha-type medium access probability and number of channels on\nthe system performance. In addition to an accurate Markov chain, which follows\nthe queue status of all users, several lower complexity queueing theory\napproximations are provided. Accuracy and performance of the proposed\nanalytical approximations are verified with extensive simulations. It is\nobserved that for CR networks using an Aloha-type access to the control\nchannel, a buffering MAC protocol, where in case of interruption the CR user\nwaits for the primary user to vacate the channel before resuming the\ntransmission, outperforms a switching MAC protocol, where the CR user vacates\nthe channel in case of appearance of primary users and then compete again to\ngain access to a new channel. The reason is that the delay bottleneck for both\nprotocols is the time required to successfully access the control channel,\nwhich occurs more frequently for the switching MAC protocol. We also propose a\nuser clustering approach, where users are divided into clusters with a separate\ncontrol channel per cluster, and observe that it can significantly improve the\nperformance by reducing the number of competing users per control channel. \n\n"}
{"id": "1403.7800", "contents": "Title: Evolution of wealth in a nonconservative economy driven by local Nash\n  equilibria Abstract: We develop a model for the evolution of wealth in a non-conservative economic\nenvironment, extending a theory developed earlier by the authors. The model\nconsiders a system of rational agents interacting in a game theoretical\nframework. This evolution drives the dynamic of the agents in both wealth and\neconomic configuration variables. The cost function is chosen to represent a\nrisk averse strategy of each agent. That is, the agent is more likely to\ninteract with the market, the more predictable the market, and therefore the\nsmaller its individual risk. This yields a kinetic equation for an effective\nsingle particle agent density with a Nash equilibrium serving as the local\nthermodynamic equilibrium. We consider a regime of scale separation where the\nlarge scale dynamics is given by a hydrodynamic closure with this local\nequilibrium. A class of generalized collision invariants (GCIs) is developed to\novercome the difficulty of the non-conservative property in the hydrodynamic\nclosure derivation of the large scale dynamics for the evolution of wealth\ndistribution. The result is a system of gas dynamics-type equations for the\ndensity and average wealth of the agents on large scales. We recover the\ninverse Gamma distribution, which has been previously considered in the\nliterature, as a local equilibrium for particular choices of the cost function. \n\n"}
{"id": "1404.1622", "contents": "Title: MU-MIMO MAC Protocols for Wireless Local Area Networks: A Survey Abstract: As wireless devices boom, and bandwidth-hungry applications (e.g., video and\ncloud uploading) get popular, today's Wireless Local Area Networks (WLANs)\nbecome not only crowded but also stressed at throughput. Multi-user\nMultiple-Input and Multiple-Output (MU-MIMO), an advanced form of MIMO, has\ngained attention due to its huge potential in improving the performance of\nWLANs.\n  This paper surveys random access based MAC protocols for MU-MIMO enabled\nWLANs. It first provides background information about the evolution and the\nfundamental MAC schemes of IEEE 802.11 Standards and Amendments, and then\nidentifies the key requirements of designing MU-MIMO MAC protocols for WLANs.\nAfter that, the most representative MU-MIMO MAC proposals in the literature are\noverviewed by benchmarking their MAC procedures and examining the key\ncomponents, such as the channel state information acquisition, de/pre-coding\nand scheduling schemes. Classifications and discussions on important findings\nof the surveyed MAC protocols are provided, based on which, the research\nchallenges for designing effective MU-MIMO MAC protocols, as well as the\nenvisaged MAC's role in the future heterogeneous networks, are highlighted. \n\n"}
{"id": "1404.2387", "contents": "Title: Fast Structuring of Radio Networks for Multi-Message Communications Abstract: We introduce collision free layerings as a powerful way to structure radio\nnetworks. These layerings can replace hard-to-compute BFS-trees in many\ncontexts while having an efficient randomized distributed construction. We\ndemonstrate their versatility by using them to provide near optimal distributed\nalgorithms for several multi-message communication primitives.\n  Designing efficient communication primitives for radio networks has a rich\nhistory that began 25 years ago when Bar-Yehuda et al. introduced fast\nrandomized algorithms for broadcasting and for constructing BFS-trees. Their\nBFS-tree construction time was $O(D \\log^2 n)$ rounds, where $D$ is the network\ndiameter and $n$ is the number of nodes. Since then, the complexity of a\nbroadcast has been resolved to be $T_{BC} = \\Theta(D \\log \\frac{n}{D} + \\log^2\nn)$ rounds. On the other hand, BFS-trees have been used as a crucial building\nblock for many communication primitives and their construction time remained a\nbottleneck for these primitives.\n  We introduce collision free layerings that can be used in place of BFS-trees\nand we give a randomized construction of these layerings that runs in nearly\nbroadcast time, that is, w.h.p. in $T_{Lay} = O(D \\log \\frac{n}{D} +\n\\log^{2+\\epsilon} n)$ rounds for any constant $\\epsilon>0$. We then use these\nlayerings to obtain: (1) A randomized algorithm for gathering $k$ messages\nrunning w.h.p. in $O(T_{Lay} + k)$ rounds. (2) A randomized $k$-message\nbroadcast algorithm running w.h.p. in $O(T_{Lay} + k \\log n)$ rounds. These\nalgorithms are optimal up to the small difference in the additive\npoly-logarithmic term between $T_{BC}$ and $T_{Lay}$. Moreover, they imply the\nfirst optimal $O(n \\log n)$ round randomized gossip algorithm. \n\n"}
{"id": "1404.2959", "contents": "Title: SocioAware Content Distribution using P2P solutions in Hybrid Networks Abstract: The growing online traffic that is bringing the infrastructure to its limits\ninduces an urgent demand for an efficient content delivery model. Capitalizing\nsocial networks and using advanced delivery networks potentially can help to\nsolve this problem. However, due to the complex nature of the involved networks\nsuch a model is difficult to assess. In this paper we use a simulative approach\nto analyze how the SatTorrent P2P protocol supported by social networks can\nimprove content delivery by means of reduced download duration and traffic. \n\n"}
{"id": "1404.4818", "contents": "Title: The fifteen year struggle of decentralizing privacy-enhancing technology Abstract: Ever since the introduction of the internet, it has been void of any privacy.\nThe majority of internet traffic currently is and always has been unencrypted.\nA number of anonymous communication overlay networks exist whose aim it is to\nprovide privacy to its users. However, due to the nature of the internet, there\nis major difficulty in getting these networks to become both decentralized and\nanonymous. We list reasons for having anonymous networks, discern the problems\nin achieving decentralization and sum up the biggest initiatives in the field\nand their current status. To do so, we use one exemplary network, the Tor\nnetwork. We explain how Tor works, what vulnerabilities this network currently\nhas, and possible attacks that could be used to violate privacy and anonymity.\nThe Tor network is used as a key comparison network in the main part of the\nreport: a tabular overview of the major anonymous networking technologies in\nuse today. \n\n"}
{"id": "1404.5381", "contents": "Title: The Futures Premium and Rice Market Efficiency in Prewar Japan Abstract: This paper studies the interrelation between spot and futures prices in the\ntwo major rice markets in prewar Japan from the perspective of market\nefficiency. Applying a non-Bayesian time-varying model approach to the\nfundamental equation for spot returns and the futures premium, we detect when\nefficiency reductions in the two major rice markets occurred. We also examine\nhow government interventions affected the rice markets in Japan, which\ncolonized Taiwan and Korea before World War II, and argue that the function of\nrice futures markets crucially depended on the differences in rice spot\nmarket's structure. The increased volume of imported rice of a different\nvariety from domestic rice first disrupted the rice futures. Then, government\nintervention in the rice futures markets failed to improve the disruption.\nChanges in colonial rice cropping successfully improved the disruption, and\ncolonial rice was promoted to unify the different varieties of inland and\ncolonial rice. \n\n"}
{"id": "1404.5709", "contents": "Title: In Order Packet Delivery in Instantly Decodable Network Coded Systems\n  over Wireless Broadcast Abstract: In this paper, we study in-order packet delivery in instantly decodable\nnetwork coded systems for wireless broadcast networks. We are interested in\napplications, in which the successful delivery of a packet depends on the\ncorrect reception of this packet and all its preceding packets. We formulate\nthe problem of minimizing the number of undelivered packets to all receivers\nover all transmissions until completion as a stochastic shortest path (SSP)\nproblem. Although finding the optimal packet selection policy using SSP is\ncomputationally complex, it allows us to systematically exploit the problem\nstructure and draw guidelines for efficient packet selection policies that can\nreduce the number of undelivered packets to all receivers over all\ntransmissions until completion. According to these guidelines, we design a\nsimple heuristic packet selection algorithm. Simulation results illustrate that\nour proposed algorithm provides quicker packet delivery to the receivers\ncompared to the existing algorithms in the literature. \n\n"}
{"id": "1404.6687", "contents": "Title: When Queueing Meets Coding: Optimal-Latency Data Retrieving Scheme in\n  Storage Clouds Abstract: In this paper, we study the problem of reducing the delay of downloading data\nfrom cloud storage systems by leveraging multiple parallel threads, assuming\nthat the data has been encoded and stored in the clouds using fixed rate\nforward error correction (FEC) codes with parameters (n, k). That is, each file\nis divided into k equal-sized chunks, which are then expanded into n chunks\nsuch that any k chunks out of the n are sufficient to successfully restore the\noriginal file. The model can be depicted as a multiple-server queue with\narrivals of data retrieving requests and a server corresponding to a thread.\nHowever, this is not a typical queueing model because a server can terminate\nits operation, depending on when other servers complete their service (due to\nthe redundancy that is spread across the threads). Hence, to the best of our\nknowledge, the analysis of this queueing model remains quite uncharted.\n  Recent traces from Amazon S3 show that the time to retrieve a fixed size\nchunk is random and can be approximated as a constant delay plus an i.i.d.\nexponentially distributed random variable. For the tractability of the\ntheoretical analysis, we assume that the chunk downloading time is i.i.d.\nexponentially distributed. Under this assumption, we show that any\nwork-conserving scheme is delay-optimal among all on-line scheduling schemes\nwhen k = 1. When k > 1, we find that a simple greedy scheme, which allocates\nall available threads to the head of line request, is delay optimal among all\non-line scheduling schemes. We also provide some numerical results that point\nto the limitations of the exponential assumption, and suggest further research\ndirections. \n\n"}
{"id": "1405.2000", "contents": "Title: Tier-Aware Resource Allocation in OFDMA Macrocell-Small Cell Networks Abstract: We present a joint sub-channel and power allocation framework for downlink\ntransmission an orthogonal frequency-division multiple access (OFDMA)-based\ncellular network composed of a macrocell overlaid by small cells. In this\nframework, the resource allocation (RA) problems for both the macrocell and\nsmall cells are formulated as optimization problems. Numerical results confirm\nthe performance gains of our proposed RA formulation for the macrocell over the\ntraditional resource allocation based on minimizing the transmission power.\nBesides, it is shown that the formulation based on convex relaxation yields a\nsimilar behavior to the MINLP formulation. Also, the distributed solution\nconverges to the same solution obtained by solving the corresponding convex\noptimization problem in a centralized fashion. \n\n"}
{"id": "1405.2957", "contents": "Title: What Will 5G Be? Abstract: What will 5G be? What it will not be is an incremental advance on 4G. The\nprevious four generations of cellular technology have each been a major\nparadigm shift that has broken backwards compatibility. And indeed, 5G will\nneed to be a paradigm shift that includes very high carrier frequencies with\nmassive bandwidths, extreme base station and device densities and unprecedented\nnumbers of antennas. But unlike the previous four generations, it will also be\nhighly integrative: tying any new 5G air interface and spectrum together with\nLTE and WiFi to provide universal high-rate coverage and a seamless user\nexperience. To support this, the core network will also have to reach\nunprecedented levels of flexibility and intelligence, spectrum regulation will\nneed to be rethought and improved, and energy and cost efficiencies will become\neven more critical considerations. This paper discusses all of these topics,\nidentifying key challenges for future research and preliminary 5G\nstandardization activities, while providing a comprehensive overview of the\ncurrent literature, and in particular of the papers appearing in this special\nissue. \n\n"}
{"id": "1405.4120", "contents": "Title: On Energy-efficiency in Wireless Networks: A Game-theoretic Approach to\n  Cooperation Inspired by Evolutionary Biology Abstract: We develop a game-theoretic framework to investigate the effect of\ncooperation on the energy efficiency in wireless networks. We address two\nexamples of network architectures, resembling ad-hoc network and network with\ncentral infrastructure node. Most present approaches address the issue of\nenergy efficiency in communication networks by using complex algorithms to\nenforce cooperation in the network, followed by extensive signal processing at\nthe network nodes. Instead, we address cooperative communication scenarios\nwhich are governed by simple, evolutionary-like, local rules, and do not\nrequire strategic complexity of the network nodes. The approach is motivated by\nrecent results in evolutionary biology which suggest that cooperation can\nemerge in Nature by evolution, i. e. can be favoured by natural selection, if\ncertain mechanism is at work. As result, we are able to show by experiments\nthat cooperative behavior can indeed emerge and persist in wireless networks,\neven if the behavior of the individual nodes is driven by selfish decision\nmaking. The results from this work indicate that uncomplicated local rules,\nfollowed by simple fitness evaluation, can promote cooperation and generate\nnetwork behavior which yields global energy efficiency in certain wireless\nnetworks. \n\n"}
{"id": "1405.5365", "contents": "Title: Common Problems in Delay-Based Congestion Control Algorithms: A Gallery\n  of Solutions Abstract: Although delay-based congestion control protocols such as FAST promise to\ndeliver better performance than traditional TCP Reno, they have not yet been\nwidely incorporated to the Internet. Several factors have contributed to their\nlack of deployment. Probably, the main contributing factor is that they are not\nable to compete fairly against loss-based congestion control protocols. In\nfact, the transmission rate in equilibrium of delay-based approaches is always\nless than their fair share when they share the network with traditional\nTCP-Reno derivatives, that employ packet losses as their congestion signal.\nThere are also other performance impairments caused by the sensitivity to\nerrors in the measurement of the congestion signal (queuing delay) that reduce\nthe efficiency and the intra-protocol fairness of the algorithms. In this paper\nwe report, analyze and discuss some recent proposals in the literature to\nimprove the dynamic behavior of delay-based congestion control algorithms, and\nFAST in particular. Coexistence of sources reacting differently to congestion,\nidentifying congestion appearance in the reverse path and the persistent\ncongestion problem are the issues specifically addressed. \n\n"}
{"id": "1405.5864", "contents": "Title: Caching Eliminates the Wireless Bottleneck in Video-Aware Wireless\n  Networks Abstract: Cellular data traffic almost doubles every year, greatly straining network\ncapacity. The main driver for this development is wireless video. Traditional\nmethods for capacity increase (like using more spectrum and increasing base\nstation density) are very costly, and do not exploit the unique features of\nvideo, in particular a high degree of {\\em asynchronous content reuse}. In this\npaper we give an overview of our work that proposed and detailed a new\ntransmission paradigm exploiting content reuse, and the fact that storage is\nthe fastest-increasing quantity in modern hardware. Our network structure uses\ncaching in helper stations (femto-caching) and/or devices, combined with highly\nspectrally efficient short-range communications to deliver video files. For\nfemto-caching, we develop optimum storage schemes and dynamic streaming\npolicies that optimize video quality. For caching on devices, combined with\ndevice-to-device communications, we show that communications within {\\em\nclusters} of mobile stations should be used; the cluster size can be adjusted\nto optimize the tradeoff between frequency reuse and the probability that a\ndevice finds a desired file cached by another device in the same cluster. We\nshow that in many situations the network throughput increases linearly with the\nnumber of users, and that D2D communications also is superior in providing a\nbetter tradeoff between throughput and outage than traditional base-station\ncentric systems. Simulation results with realistic numbers of users and channel\nconditions show that network throughput (possibly with outage constraints) can\nbe increased by two orders of magnitude compared to conventional schemes. \n\n"}
{"id": "1406.1058", "contents": "Title: Specifying and Placing Chains of Virtual Network Functions Abstract: Network appliances perform different functions on network flows and\nconstitute an important part of an operator's network. Normally, a set of\nchained network functions process network flows. Following the trend of\nvirtualization of networks, virtualization of the network functions has also\nbecome a topic of interest. We define a model for formalizing the chaining of\nnetwork functions using a context-free language. We process deployment requests\nand construct virtual network function graphs that can be mapped to the\nnetwork. We describe the mapping as a Mixed Integer Quadratically Constrained\nProgram (MIQCP) for finding the placement of the network functions and chaining\nthem together considering the limited network resources and requirements of the\nfunctions. We have performed a Pareto set analysis to investigate the possible\ntrade-offs between different optimization objectives. \n\n"}
{"id": "1406.2255", "contents": "Title: Energy-Efficient Cooperative Cognitive Relaying Schemes for Cognitive\n  Radio Networks Abstract: We investigate a cognitive radio network in which a primary user (PU) may\ncooperate with a cognitive radio user (i.e., a secondary user (SU)) for\ntransmissions of its data packets. The PU is assumed to be a buffered node\noperating in a time-slotted fashion where the time is partitioned into\nequal-length slots. We develop two schemes which involve cooperation between\nprimary and secondary users. To satisfy certain quality of service (QoS)\nrequirements, users share time slot duration and channel frequency bandwidth.\nMoreover, the SU may leverage the primary feedback message to further increase\nboth its data rate and satisfy the PU QoS requirements. The proposed\ncooperative schemes are designed such that the SU data rate is maximized under\nthe constraint that the PU average queueing delay is maintained less than the\naverage queueing delay in case of non-cooperative PU. In addition, the proposed\nschemes guarantee the stability of the PU queue and maintain the average energy\nemitted by the SU below a certain value. The proposed schemes also provide more\nrobust and potentially continuous service for SUs compared to the conventional\npractice in cognitive networks where SUs transmit in the spectrum holes and\nsilence sessions of the PUs. We include primary source burstiness, sensing\nerrors, and feedback decoding errors to the analysis of our proposed\ncooperative schemes. The optimization problems are solved offline and require a\nsimple 2-dimensional grid-based search over the optimization variables.\nNumerical results show the beneficial gains of the cooperative schemes in terms\nof SU data rate and PU throughput, average PU queueing delay, and average PU\nenergy savings. \n\n"}
{"id": "1406.2738", "contents": "Title: Wireless Backhaul Networks: Capacity Bound, Scalability Analysis and\n  Design Guidelines Abstract: This paper studies the scalability of a wireless backhaul network modeled as\na random extended network with multi-antenna base stations (BSs), where the\nnumber of antennas per BS is allowed to scale as a function of the network\nsize. The antenna scaling is justified by the current trend towards the use of\nhigher carrier frequencies, which allows to pack large number of antennas in\nsmall form factors. The main goal is to study the per-BS antenna requirement\nthat ensures scalability of this network, i.e., its ability to deliver\nnon-vanishing rate to each source-destination pair. We first derive an\ninformation theoretic upper bound on the capacity of this network under a\ngeneral propagation model, which provides a lower bound on the per-BS antenna\nrequirement. Then, we characterize the scalability requirements for two\ncompeting strategies of interest: (i) long hop: each source-destination pair\nminimizes the number of hops by sacrificing multiplexing gain while achieving\nfull beamforming (power) gain over each hop, and (ii) short hop: each\nsource-destination pair communicates through a series of short hops, each\nachieving full multiplexing gain. While long hop may seem more intuitive in the\ncontext of massive multiple-input multiple-output (MIMO) transmission, we show\nthat the short hop strategy is significantly more efficient in terms of per-BS\nantenna requirement for throughput scalability. As a part of the proof, we\nconstruct a scalable short hop strategy and show that it does not violate any\nfundamental limits on the spatial degrees of freedom (DoFs). \n\n"}
{"id": "1406.4448", "contents": "Title: Stability Region of a Slotted Aloha Network with K-Exponential Backoff Abstract: Stability region of random access wireless networks is known for only simple\nnetwork scenarios. The main problem in this respect is due to interaction among\nqueues. When transmission probabilities during successive transmissions change,\ne.g., when exponential backoff mechanism is exploited, the interactions in the\nnetwork are stimulated. In this paper, we derive the stability region of a\nbuffered slotted Aloha network with K-exponential backoff mechanism,\napproximately, when a finite number of nodes exist. To this end, we propose a\nnew approach in modeling the interaction among wireless nodes. In this\napproach, we model the network with inter-related quasi-birth-death (QBD)\nprocesses such that at each QBD corresponding to each node, a finite number of\nphases consider the status of the other nodes. Then, by exploiting the\navailable theorems on stability of QBDs, we find the stability region. We show\nthat exponential backoff mechanism is able to increase the area of the\nstability region of a simple slotted Aloha network with two nodes, more than\n40\\%. We also show that a slotted Aloha network with exponential backoff may\nperform very near to ideal scheduling. The accuracy of our modeling approach is\nverified by simulation in different conditions. \n\n"}
{"id": "1406.4917", "contents": "Title: Max-Weight Scheduling and Quality-Aware Streaming for Device-to-Device\n  Video Delivery Abstract: We propose and analyze centralized and distributed algorithms for\ndevice-to-device video scheduling and streaming. The proposed algorithms\naddress jointly the problems of device-to-device link scheduling and video\nquality adaptation in streaming. Our simulations show that the proposed\nalgorithms significantly outperform conventional separated approaches that\ntreat these two problems independently. \n\n"}
{"id": "1407.0787", "contents": "Title: Decision-theoretic approaches to non-knowledge in economics Abstract: We review two strands of conceptual approaches to the formal representation\nof a decision maker's non-knowledge at the initial stage of a static\none-person, one-shot decision problem in economic theory. One focuses on\nrepresentations of non-knowledge in terms of probability measures over sets of\nmutually exclusive and exhaustive consequence-relevant states of Nature, the\nother deals with unawareness of potentially important events by means of sets\nof states that are less complete than the full set of consequence-relevant\nstates of Nature. We supplement our review with a brief discussion of\nunresolved matters in both approaches. \n\n"}
{"id": "1407.1497", "contents": "Title: Content-Aware Network Coding over Device-to-Device Networks Abstract: Consider a scenario of broadcasting a common content to a group of\ncooperating mobile devices that are within proximity of each other. Devices in\nthis group may receive partial content from the source due to packet losses\nover wireless broadcast links. We further consider that packet losses are\ndifferent for different devices. The remaining missing content at each device\ncan then be recovered, thanks to cooperation among the devices by exploiting\ndevice-to-device (D2D) connections. In this context, the minimum amount of time\nthat can guarantee a complete acquisition of the common content at every device\nis referred to as the \"completion time\". It has been shown that instantly\ndecodable network coding (IDNC) reduces the completion time as compared to no\nnetwork coding in this scenario. Yet, for applications such as video streaming,\nnot all packets have the same importance and not all devices are interested in\nthe same quality of content. This problem is even more interesting when\nadditional, but realistic constraints, such as strict deadline, bandwidth, or\nlimited energy are added in the problem formulation. We assert that direct\napplication of IDNC in such a scenario yields poor performance in terms of\ncontent quality and completion time. In this paper, we propose a novel Content\nand Loss-Aware IDNC scheme that improves content quality and network coding\nopportunities jointly by taking into account importance of each packet towards\nthe desired quality of service (QoS) as well as the channel losses over D2D\nlinks. Our proposed Content and Loss-Aware IDNC (i) maximizes the quality under\nthe completion time constraint, and (ii) minimizes the completion time under\nthe quality constraint. We demonstrate the benefits of Content and Loss-Aware\nIDNC through simulations. \n\n"}
{"id": "1407.1660", "contents": "Title: Estimating Traffic and Anomaly Maps via Network Tomography Abstract: Mapping origin-destination (OD) network traffic is pivotal for network\nmanagement and proactive security tasks. However, lack of sufficient flow-level\nmeasurements as well as potential anomalies pose major challenges towards this\ngoal. Leveraging the spatiotemporal correlation of nominal traffic, and the\nsparse nature of anomalies, this paper brings forth a novel framework to map\nout nominal and anomalous traffic, which treats jointly important network\nmonitoring tasks including traffic estimation, anomaly detection, and traffic\ninterpolation. To this end, a convex program is first formulated with nuclear\nand $\\ell_1$-norm regularization to effect sparsity and low rank for the\nnominal and anomalous traffic with only the link counts and a {\\it small}\nsubset of OD-flow counts. Analysis and simulations confirm that the proposed\nestimator can {\\em exactly} recover sufficiently low-dimensional nominal\ntraffic and sporadic anomalies so long as the routing paths are sufficiently\n\"spread-out\" across the network, and an adequate amount of flow counts are\nrandomly sampled. The results offer valuable insights about data acquisition\nstrategies and network scenaria giving rise to accurate traffic estimation. For\npractical networks where the aforementioned conditions are possibly violated,\nthe inherent spatiotemporal traffic patterns are taken into account by adopting\na Bayesian approach along with a bilinear characterization of the nuclear and\n$\\ell_1$ norms. The resultant nonconvex program involves quadratic regularizers\nwith correlation matrices, learned systematically from (cyclo)stationary\nhistorical data. Alternating-minimization based algorithms with provable\nconvergence are also developed to procure the estimates. Insightful tests with\nsynthetic and real Internet data corroborate the effectiveness of the novel\nschemes. \n\n"}
{"id": "1407.1931", "contents": "Title: Deterministic Near-Optimal P2P Streaming Abstract: We consider streaming over a peer-to-peer network with homogeneous nodes in\nwhich a single source broadcasts a data stream to all the users in the system.\nPeers are allowed to enter or leave the system (adversarially) arbitrarily.\nPrevious approaches for streaming in this setting have either used randomized\ndistribution graphs or structured trees with randomized maintenance algorithms.\nRandomized graphs handle peer churn well but have poor connectivity guarantees,\nwhile structured trees have good connectivity but have proven hard to maintain\nunder peer churn. We improve upon both approaches by presenting a novel\ndistribution structure with a deterministic and distributed algorithm for\nmaintenance under peer churn; our result is inspired by a recent work proposing\ndeterministic algorithms for rumor spreading in graphs. A key innovation in our\napproach is in having redundant links in the distribution structure. While this\nleads to a reduction in the maximum streaming rate possible, we show that for\nthe amount of redundancy used, the delay guarantee of the proposed algorithm is\nnear optimal. We introduce a tolerance parameter that captures the worst-case\ntransient streaming rate received by the peers during churn events and\ncharacterize the fundamental tradeoff between rate, delay and tolerance. A\nnatural generalization of the deterministic algorithm achieves this tradeoff\nnear optimally. Finally, the proposed deterministic algorithm is robust enough\nto handle various generalizations: ability to deal with heterogeneous node\ncapacities of the peers and more complicated streaming patterns where multiple\nsource transmissions are present. \n\n"}
{"id": "1407.3652", "contents": "Title: Forecasting future oil production in Norway and the UK: a general\n  improved methodology Abstract: We present a new Monte-Carlo methodology to forecast the crude oil production\nof Norway and the U.K. based on a two-step process, (i) the nonlinear\nextrapolation of the current/past performances of individual oil fields and\n(ii) a stochastic model of the frequency of future oil field discoveries.\nCompared with the standard methodology that tends to underestimate remaining\noil reserves, our method gives a better description of future oil production,\nas validated by our back-tests starting in 2008. Specifically, we predict\nremaining reserves extractable until 2030 to be 188 +/- 10 million barrels for\nNorway and 98 +/- 10 million barrels for the UK, which are respectively 45% and\n66% above the predictions using the standard methodology. \n\n"}
{"id": "1407.4694", "contents": "Title: Distributed Pricing-Based User Association for Downlink Heterogeneous\n  Cellular Networks Abstract: This paper considers the optimization of the user and base-station (BS)\nassociation in a wireless downlink heterogeneous cellular network under the\nproportional fairness criterion. We first consider the case where each BS has a\nsingle antenna and transmits at fixed power, and propose a distributed price\nupdate strategy for a pricing-based user association scheme, in which the users\nare assigned to the BS based on the value of a utility function minus a price.\nThe proposed price update algorithm is based on a coordinate descent method for\nsolving the dual of the network utility maximization problem, and it has a\nrigorous performance guarantee. The main advantage of the proposed algorithm as\ncompared to the existing subgradient method for price update is that the\nproposed algorithm is independent of parameter choices and can be implemented\nasynchronously. Further, this paper considers the joint user association and BS\npower control problem, and proposes an iterative dual coordinate descent and\nthe power optimization algorithm that significantly outperforms existing\napproaches. Finally, this paper considers the joint user association and BS\nbeamforming problem for the case where the BSs are equipped with multiple\nantennas and spatially multiplex multiple users. We incorporate dual coordinate\ndescent with the weighted minimum mean-squared error (WMMSE) algorithm, and\nshow that it achieves nearly the same performance as a computationally more\ncomplex benchmark algorithm (which applies the WMMSE algorithm on the entire\nnetwork for BS association), while avoiding excessive BS handover. \n\n"}
{"id": "1407.5305", "contents": "Title: The dynamics of the leverage cycle Abstract: We present a simple agent-based model of a financial system composed of\nleveraged investors such as banks that invest in stocks and manage their risk\nusing a Value-at-Risk constraint, based on historical observations of asset\nprices. The Value-at-Risk constraint implies that when perceived risk is low,\nleverage is high and vice versa, a phenomenon that has been dubbed pro-cyclical\nleverage. We show that this leads to endogenous irregular oscillations, in\nwhich gradual increases in stock prices and leverage are followed by drastic\nmarket collapses, i.e. a leverage cycle. This phenomenon is studied using\nsimplified models that give a deeper understanding of the dynamics and the\nnature of the feedback loops and instabilities underlying the leverage cycle.\nWe introduce a flexible leverage regulation policy in which it is possible to\ncontinuously tune from pro-cyclical to countercyclical leverage. When the\npolicy is sufficiently countercyclical and bank risk is sufficiently low the\nendogenous oscillation disappears and prices go to a fixed point. While there\nis always a leverage ceiling above which the dynamics are unstable,\ncountercyclical leverage can be used to raise the ceiling. We also study the\nimpact on leverage cycles of direct, temporal control of the bank's riskiness\nvia the bank's required Value-at-Risk quantile. Under such a rule the regulator\nrelaxes the Value-at-Risk quantile following a negative stock price shock and\ntightens it following a positive shock. While such a policy rule can reduce the\namplitude of leverage cycles, its effectiveness is highly dependent on the\nchoice of parameters. Finally, we investigate fixed limits on leverage and show\nhow they can control the leverage cycle. \n\n"}
{"id": "1407.5429", "contents": "Title: Bank-firm credit network in Japan. An analysis of a bipartite network Abstract: We present an analysis of the credit market of Japan. The analysis is\nperformed by investigating the bipartite network of banks and firms which is\nobtained by setting a link between a bank and a firm when a credit relationship\nis present in a given time window. In our investigation we focus on a community\ndetection algorithm which is identifying communities composed by both banks and\nfirms. We show that the clusters obtained by directly working on the bipartite\nnetwork carry information about the networked nature of the Japanese credit\nmarket. Our analysis is performed for each calendar year during the time period\nfrom 1980 to 2011. Specifically, we obtain communities of banks and networks\nfor each of the 32 investigated years, and we introduce a method to track the\ntime evolution of these communities on a statistical basis. We then\ncharacterize communities by detecting the simultaneous over-expression of\nattributes of firms and banks. Specifically, we consider as attributes the\neconomic sector and the geographical location of firms and the type of banks.\nIn our 32 year long analysis we detect a persistence of the over-expression of\nattributes of clusters of banks and firms together with a slow dynamics of\nchanges from some specific attributes to new ones. Our empirical observations\nshow that the credit market in Japan is a networked market where the type of\nbanks, geographical location of firms and banks and economic sector of the firm\nplay a role in shaping the credit relationships between banks and firms. \n\n"}
{"id": "1407.6731", "contents": "Title: Optimal User-Cell Association for Massive MIMO Wireless Networks Abstract: The use of a very large number of antennas at each base station site\n(referred to as \"Massive MIMO\") is one of the most promising approaches to cope\nwith the predicted wireless data traffic explosion. In combination with Time\nDivision Duplex and with simple per-cell processing, it achieves large\nthroughput per cell, low latency, and attractive power efficiency performance.\nFollowing the current wireless technology trend of moving to higher frequency\nbands and denser small cell deployments, a large number of antennas can be\nimplemented within a small form factor even in small cell base stations. In a\nheterogeneous network formed by large (macro) and small cell BSs, a key system\noptimization problem consists of \"load balancing\", that is, associating users\nto BSs in order to avoid congested hot-spots and/or under-utilized\ninfrastructure. In this paper, we consider the user-BS association problem for\na massive MIMO heterogeneous network. We formulate the problem as a network\nutility maximization, and provide a centralized solution in terms of the\nfraction of transmission resources (time-frequency slots) over which each user\nis served by a given BS. Furthermore, we show that such a solution is\nphysically realizable, i.e., there exists a sequence of integer scheduling\nconfigurations realizing (by time-sharing) the optimal fractions. While this\nsolution is optimal, it requires centralized computation. Then, we also\nconsider decentralized user-centric schemes, formulated as non-cooperative\ngames where each user makes individual selfish association decisions based only\non its local information. We identify a class of schemes such that their Nash\nequilibrium is very close to the global centralized optimum. Hence, these\nuser-centric algorithms are attractive not only for their simplicity and fully\ndecentralized implementation, but also because they operate near the system\n\"social\" optimum. \n\n"}
{"id": "1407.7237", "contents": "Title: Grid Integration Costs of Fluctuating Renewable Energy Sources Abstract: The grid integration of intermittent Renewable Energy Sources (RES) causes\ncosts for grid operators due to forecast uncertainty and the resulting\nproduction schedule mismatches. These so-called profile service costs are\nmarginal cost components and can be understood as an insurance fee against RES\nproduction schedule uncertainty that the system operator incurs due to the\nobligation to always provide sufficient control reserve capacity for power\nimbalance mitigation. This paper studies the situation for the German power\nsystem and the existing German RES support schemes. The profile service costs\nincurred by German Transmission System Operators (TSOs) are quantified and\nmeans for cost reduction are discussed. In general, profile service costs are\ndependent on the RES prediction error and the specific workings of the power\nmarkets via which the prediction error is balanced. This paper shows both how\nthe prediction error can be reduced in daily operation as well as how profile\nservice costs can be reduced via optimization against power markets and/or\nactive curtailment of RES generation. \n\n"}
{"id": "1407.7447", "contents": "Title: Study of a model for the distribution of wealth Abstract: An equation for the evolution of the distribution of wealth in a population\nof economic agents making binary transactions with a constant total amount of\n\"money\" has recently been proposed by one of us (RLR). This equation takes the\nform of an iterated nonlinear map of the distribution of wealth. The\nequilibrium distribution is known and takes a rather simple form. If this\ndistribution is such that, at some time, the higher momenta of the distribution\nexist, one can find exactly their law of evolution. A seemingly simple\nextension of the laws of exchange yields also explicit iteration formulae for\nthe higher momenta, but with a major difference with the original iteration\nbecause high order momenta grow indefinitely. This provides a quantitative\nmodel where the spreading of wealth, namely the difference between the rich and\nthe poor, tends to increase with time. \n\n"}
{"id": "1407.8309", "contents": "Title: An Alternating Direction Method Approach to Cloud Traffic Management Abstract: In this paper, we introduce a unified framework for studying various cloud\ntraffic management problems, ranging from geographical load balancing to\nbackbone traffic engineering. We first abstract these real-world problems as a\nmulti-facility resource allocation problem, and then present two distributed\noptimization algorithms by exploiting the special structure of the problem. Our\nalgorithms are inspired by Alternating Direction Method of Multipliers (ADMM),\nenjoying a number of unique features. Compared to dual decomposition, they\nconverge with non-strictly convex objective functions; compared to other\nADMM-type algorithms, they not only achieve faster convergence under weaker\nassumptions, but also have lower computational complexity and lower\nmessage-passing overhead. The simulation results not only confirm these\ndesirable features of our algorithms, but also highlight several additional\nadvantages, such as scalability and fault-tolerance. \n\n"}
{"id": "1408.1365", "contents": "Title: Kinetic Exchange Models in Economics and Sociology Abstract: In this article, we briefly review the different aspects and applications of\nkinetic exchange models in economics and sociology. Our main aim is to show in\nwhat manner the kinetic exchange models for closed economic systems were\ninspired by the kinetic theory of gas molecules. The simple yet powerful\nframework of kinetic theory, first proposed in 1738, led to the successful\ndevelopment of statistical physics of gases towards the end of the 19th\ncentury. This framework was successfully adapted to modeling of wealth\ndistributions in the early 2000's. In later times, it was applied to other\nareas like firm dynamics and opinion formation in the society, as well. We have\ntried to present the flavour of the several models proposed and their\napplications, intentionally leaving out the intricate mathematical and\ntechnical details. \n\n"}
{"id": "1408.3079", "contents": "Title: A Lightweight Approach for Improving the Lookup Performance in\n  Kademlia-type Systems Abstract: Discovery of nodes and content in large-scale distributed systems is\ngenerally based on Kademlia, today. Understanding Kademlia-type systems to\nimprove their performance is essential for maintaining a high service quality\nfor an increased number of participants, particularly when those systems are\nadopted by latency-sensitive applications.\n  This paper contributes to the understanding of Kademlia by studying the\nimpact of \\emph{diversifying} neighbours' identifiers within each routing table\nbucket on the lookup performance. We propose a new, yet backward-compatible,\nneighbour selection scheme that attempts to maximize the aforementioned\ndiversity. The scheme does not cause additional overhead except negligible\ncomputations for comparing the diversity of identifiers. We present a\ntheoretical model for the actual impact of the new scheme on the lookup's hop\ncount and validate it against simulations of three exemplary Kademlia-type\nsystems. We also measure the performance gain enabled by a partial deployment\nfor the scheme in the real KAD system. The results confirm the superiority of\nthe systems that incorporate our scheme. \n\n"}
{"id": "1408.3469", "contents": "Title: Properties of an Aloha-like stability region Abstract: A well-known inner bound on the stability region of the finite-user slotted\nAloha protocol is the set of all arrival rates for which there exists some\nchoice of the contention probabilities such that the associated worst-case\nservice rate for each user exceeds the user's arrival rate, denoted $\\Lambda$.\nAlthough testing membership in $\\Lambda$ of a given arrival rate can be posed\nas a convex program, it is nonetheless of interest to understand the properties\nof this set. In this paper we develop new results of this nature, including\n$i)$ an equivalence between membership in $\\Lambda$ and the existence of a\npositive root of a given polynomial, $ii)$ a method to construct a vector of\ncontention probabilities to stabilize any stabilizable arrival rate vector,\n$iii)$ the volume of $\\Lambda$, $iv)$ explicit polyhedral, spherical, and\nellipsoid inner and outer bounds on $\\Lambda$, and $v)$ characterization of the\ngeneralized convexity properties of a natural ``excess rate'' function\nassociated with $\\Lambda$, including the convexity of the set of contention\nprobabilities that stabilize a given arrival rate vector. \n\n"}
{"id": "1408.6973", "contents": "Title: How structurally stable are global socioeconomic systems? Abstract: The stability analysis of socioeconomic systems has been centered on\nanswering whether small perturbations when a system is in a given quantitative\nstate will push the system permanently to a different quantitative state.\nHowever, typically the quantitative state of socioeconomic systems is subject\nto constant change. Therefore, a key stability question that has been\nunder-investigated is how strong the conditions of a system itself can change\nbefore the system moves to a qualitatively different behavior, i.e., how\nstructurally stable the systems is. Here, we introduce a framework to\ninvestigate the structural stability of socioeconomic systems formed by the\nnetwork of interactions among agents competing for resources. We measure the\nstructural stability of the system as the range of conditions in the\ndistribution and availability of resources compatible with the qualitative\nbehavior in which all the constituent agents can be self-sustained across time.\nTo illustrate our framework, we study an empirical representation of the global\nsocioeconomic system formed by countries sharing and competing for\nmultinational companies used as proxy for resources. We demonstrate that the\nstructural stability of the system is inversely associated with the level of\ncompetition and the level of heterogeneity in the distribution of resources.\nImportantly, we show that the qualitative behavior of the observed global\nsocioeconomic system is highly sensitive to changes in the distribution of\nresources. We believe this work provides a methodological basis to develop\nsustainable strategies for socioeconomic systems subject to constantly changing\nconditions. \n\n"}
{"id": "1409.1606", "contents": "Title: Power Optimal Non-contiguous Spectrum Access in Multi Front End Radio\n  Enabled Point-to-Point Link Abstract: Non-contiguous spectrum chunks allow wireless links to flexibly access a wide\namount of bandwidth. Multi- Channel Multi-Radio (MC-MR) and Non-Contiguous\nOrthogonal Frequency Division Multiplexing (NC-OFDM) are the two commercially\nviable strategies to access non-contiguous spectrum chunks. MC-MR accesses\nmultiple non-contiguous chunks by activating multiple front ends which, in\nturn, increases the circuit power consumption of each of the activated front\nends. NC-OFDM accesses non-contiguous spectrum chunks with a single front end\nby nulling remaining subchannels but increases spectrum span which, in turn,\nincreases the power consumption of ADC and DAC. This work focuses on a\npoint-to-point link where transmitter and receiver have multiple front ends and\ncan employ NC-OFDM technology. We investigate optimal spectrum fragmentation in\neach front end from a system power (summation of transmit power and circuit\npower) perspective. We formulate a mixed integer non-linear program (MINLP) to\nperform power control and scheduling, and minimize system power by providing a\ngreedy algorithm (O(M^3 I)) where M and I denote the number of channels and\nradio front ends respectively. \n\n"}
{"id": "1409.1661", "contents": "Title: Rate Optimal design of a Wireless Backhaul Network using TV White Space Abstract: The penetration of wireless broadband services in remote areas has primarily\nbeen limited due to the lack of economic incentives that service providers\nencounter in sparsely populated areas. Besides, wireless backhaul links like\nsatellite and microwave are either expensive or require strict line of sight\ncommunication making them unattractive. TV white space channels with their\ndesirable radio propagation characteristics can provide an excellent\nalternative for engineering backhaul networks in areas that lack abundant\ninfrastructure. Specifically, TV white space channels can provide \"free\nwireless backhaul pipes\" to transport aggregated traffic from broadband sources\nto fiber access points. In this paper, we investigate the feasibility of\nmulti-hop wireless backhaul in the available white space channels by using\nnoncontiguous Orthogonal Frequency Division Multiple Access (NC-OFDMA)\ntransmissions between fixed backhaul towers. Specifically, we consider joint\npower control, scheduling and routing strategies to maximize the minimum rate\nacross broadband towers in the network. Depending on the population density and\ntraffic demands of the location under consideration, we discuss the suitable\nchoice of cell size for the backhaul network. Using the example of available TV\nwhite space channels in Wichita, Kansas (a small city located in central USA),\nwe provide illustrative numerical examples for designing such wireless backhaul\nnetwork. \n\n"}
{"id": "1409.2246", "contents": "Title: DCT${^2}$Gen: A Versatile TCP Traffic Generator for Data Centers Abstract: Only little is publicly known about traffic in non-educational data centers.\nRecent studies made some knowledge available, which gives us the opportunity to\ncreate more realistic traffic models for data center research. We used this\nknowledge to create the first publicly available traffic generator that\nproduces realistic traffic between hosts in data centers of arbitrary size. We\ncharacterize traffic by using six probability distribution functions and\nconcentrate on the generation of traffic on flow-level. The distribution\nfunctions are described as step functions, which makes our generator highly\nconfigurable to generate traffic for different kinds of data centers. Moreover,\nin data centers, traffic between hosts in the same rack and hosts in different\nracks have different properties. We model this phenomenon, making our generated\ntraffic very realistic. We carefully evaluated our approach and conclude that\nit reproduces these characteristics with high accuracy. \n\n"}
{"id": "1409.2835", "contents": "Title: Power Estimation in LTE systems with the General Framework of Standard\n  Interference Mappings Abstract: We devise novel techniques to obtain the downlink power inducing a given load\nin long-term evolution (LTE) systems, where we define load as the fraction of\nresource blocks in the time-frequency grid being requested by users from a\ngiven base station. These techniques are particularly important because\nprevious studies have proved that the data rate requirement of users can be\nsatisfied with lower transmit energy if we allow the load to increase. Those\nstudies have also shown that obtaining the power assignment from a desired load\nprofile can be posed as a fixed point problem involving standard interference\nmappings, but so far the mappings have not been obtained explicitly. One of our\nmain contributions in this study is to close this gap. We derive an\ninterference mapping having as its fixed point the power assignment inducing a\ndesired load, assuming that such an assignment exists. Having this mapping in\nclosed form, we simplify the proof of the aforementioned known results, and we\nalso devise novel iterative algorithms for power computation that have many\nnumerical advantages over previous methods. \n\n"}
{"id": "1409.4499", "contents": "Title: Toward Fully-Shared Access: Designing ISP Service Plans Leveraging\n  Excess Bandwidth Allocation Abstract: Shaping subscriber traffic based on token bucket filter (TBF) by Internet\nservice providers (ISPs) results in waste of network resources in shared access\nwhen there are few active subscribers, because it cannot allocate excess\nbandwidth in the long term. New traffic control schemes have been recently\nproposed to allocate excess bandwidth among active subscribers proportional to\ntheir token generation rates. In this paper we report the current status of our\nresearch on designing flexible yet practical service plans exploiting excess\nbandwidth allocation enabled by the new traffic control schemes in shared\naccess networks, which are attractive to both ISP and its subscribers in terms\nof revenue and quality of service (QoS) and serve as a stepping stone to\nfully-shared access in the future. \n\n"}
{"id": "1409.4739", "contents": "Title: Wireless networks appear Poissonian due to strong shadowing Abstract: Geographic locations of cellular base stations sometimes can be well fitted\nwith spatial homogeneous Poisson point processes. In this paper we make a\ncomplementary observation: In the presence of the log-normal shadowing of\nsufficiently high variance, the statistics of the propagation loss of a single\nuser with respect to different network stations are invariant with respect to\ntheir geographic positioning, whether regular or not, for a wide class of\nempirically homogeneous networks. Even in perfectly hexagonal case they appear\nas though they were realized in a Poisson network model, i.e., form an\ninhomogeneous Poisson point process on the positive half-line with a power-law\ndensity characterized by the path-loss exponent. At the same time, the\nconditional distances to the corresponding base stations, given their observed\npropagation losses, become independent and log-normally distributed, which can\nbe seen as a decoupling between the real and model geometry. The result applies\nalso to Suzuki (Rayleigh-log-normal) propagation model. We use\nKolmogorov-Smirnov test to empirically study the quality of the Poisson\napproximation and use it to build a linear-regression method for the\nstatistical estimation of the value of the path-loss exponent. \n\n"}
{"id": "1409.6001", "contents": "Title: Reconfigurable Wireless Networks Abstract: Driven by the advent of sophisticated and ubiquitous applications, and the\never-growing need for information, wireless networks are without a doubt\nsteadily evolving into profoundly more complex and dynamic systems. The user\ndemands are progressively rampant, while application requirements continue to\nexpand in both range and diversity. Future wireless networks, therefore, must\nbe equipped with the ability to handle numerous, albeit challenging\nrequirements. Network reconfiguration, considered as a prominent network\nparadigm, is envisioned to play a key role in leveraging future network\nperformance and considerably advancing current user experiences. This paper\npresents a comprehensive overview of reconfigurable wireless networks and an\nin-depth analysis of reconfiguration at all layers of the protocol stack. Such\nnetworks characteristically possess the ability to reconfigure and adapt their\nhardware and software components and architectures, thus enabling flexible\ndelivery of broad services, as well as sustaining robust operation under highly\ndynamic conditions. The paper offers a unifying framework for research in\nreconfigurable wireless networks. This should provide the reader with a\nholistic view of concepts, methods, and strategies in reconfigurable wireless\nnetworks. Focus is given to reconfigurable systems in relatively new and\nemerging research areas such as cognitive radio networks, cross-layer\nreconfiguration and software-defined networks. In addition, modern networks\nhave to be intelligent and capable of self-organization. Thus, this paper\ndiscusses the concept of network intelligence as a means to enable\nreconfiguration in highly complex and dynamic networks. Finally, the paper is\nsupported with several examples and case studies showing the tremendous impact\nof reconfiguration on wireless networks. \n\n"}
{"id": "1409.6281", "contents": "Title: Roaming charges for customers of cellular-wireless entrant and incumbent\n  providers Abstract: We consider a simple two-player game involving a large incumbent and small\nentrant into a cellular wireless access provider marketplace. The entrant's\ncustomers must pay roaming charges. We assume that the roaming charges are\nregulated, because if they are dictated by the incumbent then they could be set\nso high so as to be a barrier to entry in the marketplace. The game is studied\nat its Nash equilibrium. A roaming charge is identified that is arguably fair\nin the sense that revenues for the access providers are proportionate to their\ninfrastructure costs. \n\n"}
{"id": "1409.6649", "contents": "Title: A GDP-driven model for the binary and weighted structure of the\n  International Trade Network Abstract: Recent events such as the global financial crisis have renewed the interest\nin the topic of economic networks. One of the main channels of shock\npropagation among countries is the International Trade Network (ITN). Two\nimportant models for the ITN structure, the classical gravity model of trade\n(more popular among economists) and the fitness model (more popular among\nnetworks scientists), are both limited to the characterization of only one\nrepresentation of the ITN. The gravity model satisfactorily predicts the volume\nof trade between connected countries, but cannot reproduce the observed missing\nlinks (i.e. the topology). On the other hand, the fitness model can\nsuccessfully replicate the topology of the ITN, but cannot predict the volumes.\nThis paper tries to make an important step forward in the unification of those\ntwo frameworks, by proposing a new GDP-driven model which can simultaneously\nreproduce the binary and the weighted properties of the ITN. Specifically, we\nadopt a maximum-entropy approach where both the degree and the strength of each\nnode is preserved. We then identify strong nonlinear relationships between the\nGDP and the parameters of the model. This ultimately results in a weighted\ngeneralization of the fitness model of trade, where the GDP plays the role of a\n`macroeconomic fitness' shaping the binary and the weighted structure of the\nITN simultaneously. Our model mathematically highlights an important asymmetry\nin the role of binary and weighted network properties, namely the fact that\nbinary properties can be inferred without the knowledge of weighted ones, while\nthe opposite is not true. \n\n"}
{"id": "1409.7368", "contents": "Title: Census: Fast, scalable and robust data aggregation in MANETs Abstract: This paper describes Census, a protocol for data aggregation and statistical\ncounting in MANETs. Census operates by circulating a set of tokens in the\nnetwork using biased random walks such that each node is visited by at least\none token. The protocol is structure-free so as to avoid high messaging\noverhead for maintaining structure in the presence of node mobility. It biases\nthe random walks of tokens so as to achieve fast cover time; the bias involves\nshort albeit multi-hop gradients that guide the tokens towards hitherto\nunvisited nodes. Census thus achieves a cover time of O(N/k) and message\noverhead of O(Nlog(N)/k) where N is the number of nodes and k the number of\ntokens in the network. Notably, it enjoys scalability and robustness, which we\ndemonstrate via simulations in networks ranging from 100 to 4000 nodes under\ndifferent network densities and mobility models. \n\n"}
{"id": "1409.8269", "contents": "Title: Fact Sheet Research on Bayesian Decision Theory Abstract: In this fact sheet we give some preliminary research results on the Bayesian\nDecision Theory. This theory has been under construction for the past two\nyears. But what started as an intuitive enough idea, now seems to have the\nmakings of something more fundamental. \n\n"}
{"id": "1409.8321", "contents": "Title: Sudden Trust Collapse in Networked Societies Abstract: Trust is a collective, self-fulfilling phenomenon that suggests analogies\nwith phase transitions. We introduce a stylized model for the build-up and\ncollapse of trust in networks, which generically displays a first order\ntransition. The basic assumption of our model is that whereas trust begets\ntrust, panic also begets panic, in the sense that a small decrease in trust may\nbe amplified and ultimately lead to a sudden and catastrophic drop of trust. We\nshow, using both numerical simulations and mean-field analytic arguments, that\nthere are extended regions of the parameter space where two equilibrium states\ncoexist: a well-connected network where confidence is high, and a poorly\nconnected network where confidence is low. In these coexistence regions,\nspontaneous jumps from the well-connected state to the poorly connected state\ncan occur, corresponding to a sudden collapse of trust that is not caused by\nany major external catastrophe. In large systems, spontaneous crises are\nreplaced by history dependence: whether the system is found in one state or in\nthe other essentially depends on initial conditions. Finally, we document a new\nphase, in which agents are connected yet distrustful. \n\n"}
{"id": "1410.1255", "contents": "Title: Multi-resource Fair Allocation with Bounded Number of Tasks in Cloud\n  Computing Systems Abstract: Dominant resource fairness (DRF) is a popular mechanism for multi-resource\nallocation in cloud computing systems. In this paper, we consider a problem of\nmulti-resource fair allocation with bounded number of tasks. Firstly, we\npropose the lexicographically max-min normalized share (LMMNS) fair allocation\nmechanism, which is a natural generalization of DRF, and design a non-trivial\noptimal algorithm to find a LMMNS fair allocation, whose running time is linear\nin the number of users. Secondly, we prove that LMMNS satisfies envy-freeness\n(EF) and group strategy-proofness (GSP), and analysis the approximation ratios\nof LMMNS, by exploiting the properties of the optimal solution. Thirdly, we\npropose a modified version of LMMNS, which is the second mechanism satisfying\nsharing incentive, EF, and GSP. Finally, we have implemented LMMNS, and show\nthat it has a good average-case performance, especially when the number of\nresources is 2. \n\n"}
{"id": "1410.4694", "contents": "Title: Global Value Trees Abstract: The fragmentation of production across countries has become an important\nfeature of the globalization in recent decades and is often conceptualized by\nthe term, global value chains (GVCs). When empirically investigating the GVCs,\nprevious studies are mainly interested in knowing how global the GVCs are\nrather than how the GVCs look like. From a complex networks perspective, we use\nthe World Input-Output Database (WIOD) to study the global production system.\nWe find that the industry-level GVCs are indeed not chain-like but are better\ncharacterized by the tree topology. Hence, we compute the global value trees\n(GVTs) for all the industries available in the WIOD. Moreover, we compute an\nindustry importance measure based on the GVTs and compare it with other network\ncentrality measures. Finally, we discuss some future applications of the GVTs. \n\n"}
{"id": "1410.8432", "contents": "Title: Cycling in stochastic general equilibrium Abstract: By generalizing the measurements on the game experiments of mixed strategy\nNash equilibrium, we study the dynamical pattern in a representative dynamic\nstochastic general equilibrium (DSGE). The DSGE model describes the\nentanglements of the three variables (output gap [$y$], inflation [$\\pi$] and\nnominal interest rate [$r$]) which can be presented in 3D phase space. We find\nthat, even though the trajectory of $\\pi\\!-\\!y\\!-\\!r$ in phase space appears\nhighly stochastic, it can be visualized and quantified. It exhibits as\nclockwise cycles, counterclockwise cycles and weak cycles, respectively, when\nprojected onto $\\pi\\!-\\!y$, $y\\!-\\!r$ and $r\\!-\\!\\pi$ phase planes. We find\nalso that empirical data of United State (1960-2013) significantly exhibit same\ncycles. The resemblance between the cycles in general equilibrium and the\ncycles in mixed strategy Nash equilibrium suggest that, there generally exists\ndynamical fine structures accompanying with equilibrium. The fine structure,\ndescribing the entanglement of the non-equilibrium (the constantly deviating\nfrom the equilibrium), displays as endless cycles. \n\n"}
{"id": "1411.0143", "contents": "Title: Estimating the Spatial Reuse with Configuration Models Abstract: We propose a new methodology to estimate the spatial reuse of CSMA-like\nscheduling. Instead of focusing on spatial configurations of users, we model\nthe interferences between users as a random graph. Using configuration models\nfor random graphs, we show how the properties of the medium access mechanism\nare captured by some deterministic differential equations, when the size of the\ngraph gets large. Performance indicators such as the probability of connection\nof a given node can then be efficiently computed from these equations. We also\nperform simulations to illustrate the results on different types of random\ngraphs. Even on spatial structures, these estimates get very accurate as soon\nas the variance of the interference is not negligible. \n\n"}
{"id": "1411.1263", "contents": "Title: How Well Can Graphs Represent Wireless Interference? Abstract: Efficient use of a wireless network requires that transmissions be grouped\ninto feasible sets, where feasibility means that each transmission can be\nsuccessfully decoded in spite of the interference caused by simultaneous\ntransmissions. Feasibility is most closely modeled by a\nsignal-to-interference-plus-noise (SINR) formula, which unfortunately is\nconceptually complicated, being an asymmetric, cumulative, many-to-one\nrelationship. We re-examine how well graphs can capture wireless receptions as\nencoded in SINR relationships, placing them in a framework in order to\nunderstand the limits of such modelling. We seek for each wireless instance a\npair of graphs that provide upper and lower bounds on the feasibility relation,\nwhile aiming to minimize the gap between the two graphs. The cost of a graph\nformulation is the worst gap over all instances, and the price of (graph)\nabstraction is the smallest cost of a graph formulation. We propose a family of\nconflict graphs that is parameterized by a non-decreasing sub-linear function,\nand show that with a judicious choice of functions, the graphs can capture\nfeasibility with a cost of $O(\\log^* \\Delta)$, where $\\Delta$ is the ratio\nbetween the longest and the shortest link length. This holds on the plane and\nmore generally in doubling metrics. We use this to give greatly improved\n$O(\\log^* \\Delta)$-approximation for fundamental link scheduling problems with\narbitrary power control. We explore the limits of graph representations and\nfind that our upper bound is tight: the price of graph abstraction is\n$\\Omega(\\log^* \\Delta)$. We also give strong impossibility results for general\nmetrics, and for approximations in terms of the number of links. \n\n"}
{"id": "1411.3757", "contents": "Title: When do wireless network signals appear Poisson? Abstract: We consider the point process of signal strengths from transmitters in a\nwireless network observed from a fixed position under models with general\nsignal path loss and random propagation effects. We show via coupling arguments\nthat under general conditions this point process of signal strengths can be\nwell-approximated by an inhomogeneous Poisson or a Cox point processes on the\npositive real line. We also provide some bounds on the total variation distance\nbetween the laws of these point processes and both Poisson and Cox point\nprocesses. Under appropriate conditions, these results support the use of a\nspatial Poisson point process for the underlying positioning of transmitters in\nmodels of wireless networks, even if in reality the positioning does not appear\nPoisson. We apply the results to a number of models with popular choices for\npositioning of transmitters, path loss functions, and distributions of\npropagation effects. \n\n"}
{"id": "1411.3908", "contents": "Title: Distributed Discovery Clients for Spectrum Allocation Abstract: By using a distributed P2P system where the agents reside in in\nmicroprocessors already present in most radio nodes like Wi-Fi access points,\nbase stations, TVs connected to Internet etc, these agents can discover other\nagents over the back-haul network. As a result, client node lists (similar to\nneighbor lists used in 3GPP) are created. An alternative is to use a\ncentralized database. Why this best is done by distributed agents is discussed\nin this paper along with security considerations. Examples of other\napplications which will benefit from this system is also presented. \n\n"}
{"id": "1411.5323", "contents": "Title: Genetic Algorithms in Wireless Networking: Techniques, Applications, and\n  Issues Abstract: In recent times, wireless access technology is becoming increasingly\ncommonplace due to the ease of operation and installation of untethered\nwireless media. The design of wireless networking is challenging due to the\nhighly dynamic environmental condition that makes parameter optimization a\ncomplex task. Due to the dynamic, and often unknown, operating conditions,\nmodern wireless networking standards increasingly rely on machine learning and\nartificial intelligence algorithms. Genetic algorithms (GAs) provide a\nwell-established framework for implementing artificial intelligence tasks such\nas classification, learning, and optimization. GAs are well-known for their\nremarkable generality and versatility, and have been applied in a wide variety\nof settings in wireless networks. In this paper, we provide a comprehensive\nsurvey of the applications of GAs in wireless networks. We provide both an\nexposition of common GA models and configuration and provide a broad ranging\nsurvey of GA techniques in wireless networks. We also point out open research\nissues and define potential future work. While various surveys on GAs exist in\nliterature, our paper is the first paper, to the best of our knowledge, which\nfocuses on their application in wireless networks. \n\n"}
{"id": "1411.5739", "contents": "Title: The Online Disjoint Set Cover Problem and its Applications Abstract: Given a universe $U$ of $n$ elements and a collection of subsets\n$\\mathcal{S}$ of $U$, the maximum disjoint set cover problem (DSCP) is to\npartition $\\mathcal{S}$ into as many set covers as possible, where a set cover\nis defined as a collection of subsets whose union is $U$. We consider the\nonline DSCP, in which the subsets arrive one by one (possibly in an order\nchosen by an adversary), and must be irrevocably assigned to some partition on\narrival with the objective of minimizing the competitive ratio. The competitive\nratio of an online DSCP algorithm $A$ is defined as the maximum ratio of the\nnumber of disjoint set covers obtained by the optimal offline algorithm to the\nnumber of disjoint set covers obtained by $A$ across all inputs. We propose an\nonline algorithm for solving the DSCP with competitive ratio $\\ln n$. We then\nshow a lower bound of $\\Omega(\\sqrt{\\ln n})$ on the competitive ratio for any\nonline DSCP algorithm. The online disjoint set cover problem has wide ranging\napplications in practice, including the online crowd-sourcing problem, the\nonline coverage lifetime maximization problem in wireless sensor networks, and\nin online resource allocation problems. \n\n"}
{"id": "1411.6359", "contents": "Title: Packet-Level Network Compression: Realization and Scaling of the\n  Network-Wide Benefits Abstract: The existence of considerable amount of redundancy in the Internet traffic at\nthe packet level has stimulated the deployment of packet-level redundancy\nelimination techniques within the network by enabling network nodes to memorize\ndata packets. Redundancy elimination results in traffic reduction which in turn\nimproves the efficiency of network links. In this paper, the concept of network\ncompression is introduced that aspires to exploit the statistical correlation\nbeyond removing large duplicate strings from the flow to better suppress\nredundancy.\n  In the first part of the paper, we introduce \"memory-assisted compression\",\nwhich utilizes the memorized content within the network to learn the statistics\nof the information source generating the packets which can then be used toward\nreducing the length of codewords describing the packets emitted by the source.\nUsing simulations on data gathered from real network traces, we show that\nmemory-assisted compression can result in significant traffic reduction.\n  In the second part of the paper, we study the scaling of the average\nnetwork-wide benefits of memory-assisted compression. We discuss routing and\nmemory placement problems in network for the reduction of overall traffic. We\nderive a closed-form expression for the scaling of the gain in Erdos-Renyi\nrandom network graphs, where obtain a threshold value for the number of\nmemories deployed in a random graph beyond which network-wide benefits start to\nshine. Finally, the network-wide benefits are studied on Internet-like\nscale-free networks. We show that non-vanishing network compression gain is\nobtained even when only a tiny fraction of the total number of nodes in the\nnetwork are memory-enabled. \n\n"}
{"id": "1411.7711", "contents": "Title: Detour Planning for Fast and Reliable Failure Recovery in SDN with\n  OpenState Abstract: A reliable and scalable mechanism to provide protection against a link or\nnode failure has additional requirements in the context of SDN and OpenFlow.\nNot only it has to minimize the load on the controller, but it must be able to\nreact even when the controller is unreachable. In this paper we present a\nprotection scheme based on precomputed backup paths and inspired by MPLS\ncrankback routing, that guarantees instantaneous recovery times and aims at\nzero packet-loss after failure detection, regardless of controller\nreachability, even when OpenFlow's \"fast-failover\" feature cannot be used. The\nproposed mechanism is based on OpenState, an OpenFlow extension that allows a\nprogrammer to specify how forwarding rules should autonomously adapt in a\nstateful fashion, reducing the need to rely on remote controllers. We present\nthe scheme as well as two different formulations for the computation of backup\npaths. \n\n"}
{"id": "1411.7785", "contents": "Title: Performance laws of large heterogeneous cellular networks Abstract: We propose a model for heterogeneous cellular networks assuming a space-time\nPoisson process of call arrivals, independently marked by data volumes, and\nserved by different types of base stations (having different transmission\npowers) represented by the superposition of independent Poisson processes on\nthe plane. Each station applies a processor sharing policy to serve users\narriving in its vicinity, modeled by the Voronoi cell perturbed by some random\nsignal propagation effects (shadowing). Users' peak service rates depend on\ntheir signal-to-interference-and-noise ratios (SINR) with respect to the\nserving station. The mutual-dependence of the cells (due to the extra-cell\ninterference) is captured via some system of cell-load equations impacting the\nspatial distribution of the SINR. We use this model to study in a semi-analytic\nway (involving only static simulations, with the temporal evolution handled by\nthe queuing theoretic results) network performance metrics (cell loads, mean\nnumber of users) and the quality of service perceived by the users (mean\nthroughput) served by different types of base stations. Our goal is to identify\nmacroscopic laws regarding these performance metrics, involving averaging both\nover time and the network geometry. The reveled laws are validated against real\nfield measurement in an operational network. \n\n"}
{"id": "1412.0127", "contents": "Title: A biased view of a few possible components when reflecting on the\n  present decade financial and economic crisis Abstract: Is the present economic and financial crisis similar to some previous one? It\nwould be so nice to prove that universality laws exist for predicting such rare\nevents under a minimum set of realistic hypotheses. First, I briefly recall\nwhether patterns, like business cycles, are indeed found, and can be modeled\nwithin a statistical physics, or econophysics, framework. I point to a\nsimulation model for describing such so called business cycles, under exo- and\nendo-genous conditions I discuss self-organized and provoked crashes and their\npredictions. I emphasize the role of an of- ten forgotten ingredient: the time\ndelay in the information flow. I wonder about the information content of\nfinancial data, its mis-interpretation and market manipulation. \n\n"}
{"id": "1412.1618", "contents": "Title: Spanning trees of the World Trade Web: real-world data and the gravity\n  model of trade Abstract: In this paper, we investigate the statistical features of the weighted\ninternational-trade network. By finding the maximum weight spanning trees for\nthis network we make the extraction of the truly relevant connections forming\nthe network's backbone. We discuss the role of large-sized countries (strongest\neconomies) in the tree. Finally, we compare the topological properties of this\nbackbone to the maximum weight spanning trees obtained from the gravity model\nof trade. We show that the model correctly reproduces the backbone of the\nreal-world economy. \n\n"}
{"id": "1412.1898", "contents": "Title: Joint Rate and SINR Coverage Analysis for Decoupled Uplink-Downlink\n  Biased Cell Associations in HetNets Abstract: Load balancing by proactively offloading users onto small and otherwise\nlightly-loaded cells is critical for tapping the potential of dense\nheterogeneous cellular networks (HCNs). Offloading has mostly been studied for\nthe downlink, where it is generally assumed that a user offloaded to a small\ncell will communicate with it on the uplink as well. The impact of coupled\ndownlink-uplink offloading is not well understood. Uplink power control and\nspatial interference correlation further complicate the mathematical analysis\nas compared to the downlink. We propose an accurate and tractable model to\ncharacterize the uplink SINR and rate distribution in a multi-tier HCN as a\nfunction of the association rules and power control parameters. Joint\nuplink-downlink rate coverage is also characterized. Using the developed\nanalysis, it is shown that the optimal degree of channel inversion (for uplink\npower control) increases with load imbalance in the network. In sharp contrast\nto the downlink, minimum path loss association is shown to be optimal for\nuplink rate. Moreover, with minimum path loss association and full channel\ninversion, uplink SIR is shown to be invariant of infrastructure density. It is\nfurther shown that a decoupled association---employing differing association\nstrategies for uplink and downlink---leads to significant improvement in joint\nuplink-downlink rate coverage over the standard coupled association in HCNs. \n\n"}
{"id": "1412.2455", "contents": "Title: Location Verification Systems for VANETs in Rician Fading Channels Abstract: In this work we propose and examine Location Verification Systems (LVSs) for\nVehicular Ad Hoc Networks (VANETs) in the realistic setting of Rician fading\nchannels. In our LVSs, a single authorized Base Station (BS) equipped with\nmultiple antennas aims to detect a malicious vehicle that is spoofing its\nclaimed location. We first determine the optimal attack strategy of the\nmalicious vehicle, which in turn allows us to analyze the optimal LVS\nperformance as a function of the Rician $K$-factor of the channel between the\nBS and a legitimate vehicle. Our analysis also allows us to formally prove that\nthe LVS performance limit is independent of the properties of the channel\nbetween the BS and the malicious vehicle, provided the malicious vehicle's\nantenna number is above a specified value. We also investigate how tracking\ninformation on a vehicle quantitatively improves the detection performance of\nan LVS, showing how optimal performance is obtained under the assumption of the\ntracking length being randomly selected. The work presented here can be readily\nextended to multiple BS scenarios, and therefore forms the foundation for all\noptimal location authentication schemes within the context of Rician fading\nchannels. Our study closes important gaps in the current understanding of LVS\nperformance within the context of VANETs, and will be of practical value to\ncertificate revocation schemes within IEEE 1609.2. \n\n"}
{"id": "1412.8416", "contents": "Title: Joint Optimization of Radio and Computational Resources for Multicell\n  Mobile-Edge Computing Abstract: Migrating computational intensive tasks from mobile devices to more\nresourceful cloud servers is a promising technique to increase the\ncomputational capacity of mobile devices while saving their battery energy. In\nthis paper, we consider a MIMO multicell system where multiple mobile users\n(MUs) ask for computation offloading to a common cloud server. We formulate the\noffloading problem as the joint optimization of the radio resources-the\ntransmit precoding matrices of the MUs-and the computational resources-the CPU\ncycles/second assigned by the cloud to each MU-in order to minimize the overall\nusers' energy consumption, while meeting latency constraints. The resulting\noptimization problem is nonconvex (in the objective function and constraints).\nNevertheless, in the single-user case, we are able to express the global\noptimal solution in closed form. In the more challenging multiuser scenario, we\npropose an iterative algorithm, based on a novel successive convex\napproximation technique, converging to a local optimal solution of the original\nnonconvex problem. Then, we reformulate the algorithm in a distributed and\nparallel implementation across the radio access points, requiring only a\nlimited coordination/signaling with the cloud. Numerical results show that the\nproposed schemes outperform disjoint optimization algorithms. \n\n"}
{"id": "1412.8501", "contents": "Title: Formation Games of Reliable Networks Abstract: We establish a network formation game for the Internet's Autonomous System\n(AS) interconnection topology. The game includes different types of players,\naccounting for the heterogeneity of ASs in the Internet. We incorporate\nreliability considerations in the player's utility function, and analyze static\nproperties of the game as well as its dynamic evolution. We provide dynamic\nanalysis of its topological quantities, and explain the prevalence of some\n\"network motifs\" in the Internet graph. We assess our predictions with\nreal-world data. \n\n"}
{"id": "1412.8708", "contents": "Title: Full Duplex Operation for Small Cells Abstract: Full duplex (FD) communications has the potential to double the capacity of a\nhalf duplex (HD) system at the link level. However, FD operation increases the\naggregate interference on each communication link, which limits the capacity\nimprovement. In this paper, we investigate how much of the potential doubling\ncan be practically achieved in the resource-managed, small multi-cellular\nsystem, similar to the TDD variant of LTE, both in indoor and outdoor\nenvironments, assuming FD base stations (BSs) and HD user equipment (UEs). We\nfocus on low-powered small cellular systems, because they are more suitable for\nFD operation given practical self-interference cancellation limits. A joint UE\nselection and power allocation method for a multi-cell scenario is presented,\nwhere a hybrid scheduling policy assigns FD timeslots when it provides a\nthroughput advantage by pairing UEs with appropriate power levels to mitigate\nthe mutual interference, but otherwise defaults to HD operation. Due to the\ncomplexity of finding the globally optimum solution of the proposed algorithm,\na sub-optimal method based on a heuristic greedy algorithm for UE selection,\nand a novel solution using geometric programming for power allocation, is\nproposed. With practical self-interference cancellation, antennas and circuits,\nit is shown that the proposed hybrid FD system achieves as much as 94%\nthroughput improvement in the downlink, and 92% in the uplink, compared to a HD\nsystem in an indoor multi-cell scenario and 54% in downlink and 61% in uplink\nin an outdoor multi-cell scenario. Further, we also compare the energy\nefficiency of FD operation. \n\n"}
{"id": "1501.00434", "contents": "Title: Monetary Policy and Dark Corners in a stylized Agent-Based Model Abstract: We extend in a minimal way the stylized model introduced in in \"Tipping\nPoints in Macroeconomic Agent Based Models\" [JEDC 50, 29-61 (2015)], with the\naim of investigating the role and efficacy of monetary policy of a `Central\nBank' that sets the interest rate such as to steer the economy towards a\nprescribed inflation and employment level. Our major finding is that provided\nits policy is not too aggressive (in a sense detailed in the paper) the Central\nBank is successful in achieving its goals. However, the existence of different\nequilibrium states of the economy, separated by phase boundaries (or \"dark\ncorners\"), can cause the monetary policy itself to trigger instabilities and be\ncounter-productive. In other words, the Central Bank must navigate in a narrow\nwindow: too little is not enough, too much leads to instabilities and wildly\noscillating economies. This conclusion strongly contrasts with the prediction\nof DSGE models. \n\n"}
{"id": "1501.04703", "contents": "Title: Graph-based Framework for Flexible Baseband Function Splitting and\n  Placement in C-RAN Abstract: The baseband-up centralization architecture of radio access networks (C-RAN)\nhas recently been proposed to support efficient cooperative communications and\nreduce deployment and operational costs. However, the massive fronthaul\nbandwidth required to aggregate baseband samples from remote radio heads (RRHs)\nto the central office incurs huge fronthauling cost, and existing baseband\ncompression algorithms can hardly solve this issue. In this paper, we propose a\ngraphbased framework to effectively reduce fronthauling cost through properly\nsplitting and placing baseband processing functions in the network. Baseband\ntransceiver structures are represented with directed graphs, in which nodes\ncorrespond to baseband functions, and edges to the information flows between\nfunctions. By mapping graph weighs to computational and fronthauling costs, we\ntransform the problem of finding the optimum location to place some baseband\nfunctions into the problem of finding the optimum clustering scheme for graph\nnodes. We then solve this problem using a genetic algorithm with customized\nfitness function and mutation module. Simulation results show that proper\nsplitting and placement schemes can significantly reduce fronthauling cost at\nthe expense of increased computational cost. We also find that cooperative\nprocessing structures and stringent delay requirements will increase the\npossibility of centralized placement. \n\n"}
{"id": "1501.05751", "contents": "Title: Interbank markets and multiplex networks: centrality measures and\n  statistical null models Abstract: The interbank market is considered one of the most important channels of\ncontagion. Its network representation, where banks and claims/obligations are\nrepresented by nodes and links (respectively), has received a lot of attention\nin the recent theoretical and empirical literature, for assessing systemic risk\nand identifying systematically important financial institutions. Different\ntypes of links, for example in terms of maturity and collateralization of the\nclaim/obligation, can be established between financial institutions. Therefore\na natural representation of the interbank structure which takes into account\nmore features of the market, is a multiplex, where each layer is associated\nwith a type of link. In this paper we review the empirical structure of the\nmultiplex and the theoretical consequences of this representation. We also\ninvestigate the betweenness and eigenvector centrality of a bank in the\nnetwork, comparing its centrality properties across different layers and with\nMaximum Entropy null models. \n\n"}
{"id": "1502.02111", "contents": "Title: Exploiting the power of multiplicity: a holistic survey of network-layer\n  multipath Abstract: The Internet is inherently a multipath network---for an underlying network\nwith only a single path connecting various nodes would have been debilitatingly\nfragile. Unfortunately, traditional Internet technologies have been designed\naround the restrictive assumption of a single working path between a source and\na destination. The lack of native multipath support constrains network\nperformance even as the underlying network is richly connected and has\nredundant multiple paths. Computer networks can exploit the power of\nmultiplicity to unlock the inherent redundancy of the Internet. This opens up a\nnew vista of opportunities promising increased throughput (through concurrent\nusage of multiple paths) and increased reliability and fault-tolerance (through\nthe use of multiple paths in backup/ redundant arrangements). There are many\nemerging trends in networking that signify that the Internet's future will be\nunmistakably multipath, including the use of multipath technology in datacenter\ncomputing; multi-interface, multi-channel, and multi-antenna trends in\nwireless; ubiquity of mobile devices that are multi-homed with heterogeneous\naccess networks; and the development and standardization of multipath transport\nprotocols such as MP-TCP.\n  The aim of this paper is to provide a comprehensive survey of the literature\non network-layer multipath solutions. We will present a detailed investigation\nof two important design issues, namely the control plane problem of how to\ncompute and select the routes, and the data plane problem of how to split the\nflow on the computed paths. The main contribution of this paper is a systematic\narticulation of the main design issues in network-layer multipath routing along\nwith a broad-ranging survey of the vast literature on network-layer\nmultipathing. We also highlight open issues and identify directions for future\nwork. \n\n"}
{"id": "1502.03455", "contents": "Title: Dynamic Bandwidth-Efficient BCube Topologies for Virtualized Data Center\n  Networks Abstract: Network virtualization enables computing networks and data center (DC)\nproviders to manage their networking resources in a flexible manner using\nsoftware running on physical computers. In this paper, we address the existing\nissues with the classic DC network topologies in virtualized environment, and\ninvestigate a set of DC network topologies with the capability of providing\ndynamic structures according to the service-level required by the active\ntraffic in a virtual DC network. In particular, we propose three main\napproaches to modify the structure of a classic BCube topology as a topology\nbenchmark, and investigate their associated structural features and maximum\nachievable interconnected bandwidth for different routing scenarios. Finally,\nwe run an extensive simulation program to check the performance of the proposed\nmodified topologies in a simulation environment which considers failure\nanalysis and also traffic congestion. Our simulation experiments, which are\nconsistent to our design goals, show the efficiency of the proposed modified\ntopologies comparing to the classic BCube in terms of bandwidth availability\nand failure resiliency. \n\n"}
{"id": "1502.03978", "contents": "Title: Non Parametric Estimates of Option Prices Using Superhedging Abstract: We propose a new non parametric technique to estimate the CALL function based\non the superhedging principle. Our approach does not require absence of\narbitrage and easily accommodates bid/ask spreads and other market\nimperfections. We prove some optimal statistical properties of our estimates.\nAs an application we first test the methodology on a simulated sample of option\nprices and then on the S\\&P 500 index options. \n\n"}
{"id": "1502.05367", "contents": "Title: One- and two-sample nonparametric tests for the signal-to-noise ratio\n  based on record statistics Abstract: A new family of nonparametric statistics, the r-statistics, is introduced. It\nconsists of counting the number of records of the cumulative sum of the sample.\nThe single-sample r-statistic is almost as powerful as Student's t-statistic\nfor Gaussian and uniformly distributed variables, and more powerful than the\nsign and Wilcoxon signed-rank statistics as long as the data are not too\nheavy-tailed.\n  Three two-sample parametric r-statistics are proposed, one with a higher\nspecificity but a smaller sensitivity than Mann-Whitney U-test and the other\none a higher sensitivity but a smaller specificity. A nonparametric two-sample\nr-statistic is introduced, whose power is very close to that of Welch statistic\nfor Gaussian or uniformly distributed variables. \n\n"}
{"id": "1502.05578", "contents": "Title: Network Geometry Inference using Common Neighbors Abstract: We introduce and explore a new method for inferring hidden geometric\ncoordinates of nodes in complex networks based on the number of common\nneighbors between the nodes. We compare this approach to the HyperMap method,\nwhich is based only on the connections (and disconnections) between the nodes,\ni.e., on the links that the nodes have (or do not have). We find that for high\ndegree nodes the common-neighbors approach yields a more accurate inference\nthan the link-based method, unless heuristic periodic adjustments (or\n\"correction steps\") are used in the latter. The common-neighbors approach is\ncomputationally intensive, requiring $O(t^4)$ running time to map a network of\n$t$ nodes, versus $O(t^3)$ in the link-based method. But we also develop a\nhybrid method with $O(t^3)$ running time, which combines the common-neighbors\nand link-based approaches, and explore a heuristic that reduces its running\ntime further to $O(t^2)$, without significant reduction in the mapping\naccuracy. We apply this method to the Autonomous Systems (AS) Internet, and\nreveal how soft communities of ASes evolve over time in the similarity space.\nWe further demonstrate the method's predictive power by forecasting future\nlinks between ASes. Taken altogether, our results advance our understanding of\nhow to efficiently and accurately map real networks to their latent geometric\nspaces, which is an important necessary step towards understanding the laws\nthat govern the dynamics of nodes in these spaces, and the fine-grained\ndynamics of network connections. \n\n"}
{"id": "1502.06809", "contents": "Title: Optimal Linear and Cyclic Locally Repairable Codes over Small Fields Abstract: We consider locally repairable codes over small fields and propose\nconstructions of optimal cyclic and linear codes in terms of the dimension for\na given distance and length. Four new constructions of optimal linear codes\nover small fields with locality properties are developed. The first two\napproaches give binary cyclic codes with locality two. While the first\nconstruction has availability one, the second binary code is characterized by\nmultiple available repair sets based on a binary Simplex code. The third\napproach extends the first one to q-ary cyclic codes including (binary)\nextension fields, where the locality property is determined by the properties\nof a shortened first-order Reed-Muller code. Non-cyclic optimal binary linear\ncodes with locality greater than two are obtained by the fourth construction. \n\n"}
{"id": "1502.06899", "contents": "Title: Towards a Tractable Analysis of Localization Fundamentals in Cellular\n  Networks Abstract: When dedicated positioning systems, such as GPS, are unavailable, a mobile\ndevice has no choice but to fall back on its cellular network for localization.\nDue to random variations in the channel conditions to its surrounding base\nstations (BS), the mobile device is likely to face a mix of both favorable and\nunfavorable geometries for localization. Analytical studies of localization\nperformance (e.g., using the Cram\\'{e}r-Rao lower bound) usually require that\none fix the BS geometry, and favorable geometries have always been the\npreferred choice in the literature. However, not only are the resulting\nanalytical results constrained to the selected geometry, this practice is\nlikely to lead to overly-optimistic expectations of typical localization\nperformance. Ideally, localization performance should be studied across all\npossible geometric setups, thereby also removing any selection bias. This,\nhowever, is known to be hard and has been carried out only in simulation. In\nthis paper, we develop a new tractable approach where we endow the BS locations\nwith a distribution by modeling them as a Poisson point process (PPP), and use\ntools from stochastic geometry to obtain easy-to-use expressions for key\nperformance metrics. In particular, we focus on the probability of detecting\nsome minimum number of BSs, which is shown to be closely coupled with a network\noperator's ability to obtain satisfactory localization performance (e.g., meet\nFCC E911 requirements). This metric is indifferent to the localization\ntechnique (e.g., TOA, TDOA, AOA, or hybrids thereof), though different\ntechniques will presumably lead to different BS hearability requirements. In\norder to mitigate excessive interference due to the presence of dominant\ninterferers in the form of other BSs, we incorporate both BS coordination and\nfrequency reuse in the proposed framework and quantify the resulting\nperformance gains analytically. \n\n"}
{"id": "1503.02332", "contents": "Title: Robust Anomaly Detection in Dynamic Networks Abstract: We propose two robust methods for anomaly detection in dynamic networks in\nwhich the properties of normal traffic are time-varying. We formulate the\nrobust anomaly detection problem as a binary composite hypothesis testing\nproblem and propose two methods: a model-free and a model-based one, leveraging\ntechniques from the theory of large deviations. Both methods require a family\nof Probability Laws (PLs) that represent normal properties of traffic. We\ndevise a two-step procedure to estimate this family of PLs. We compare the\nperformance of our robust methods and their vanilla counterparts, which assume\nthat normal traffic is stationary, on a network with a diurnal normal pattern\nand a common anomaly related to data exfiltration. Simulation results show that\nour robust methods perform better than their vanilla counterparts in dynamic\nnetworks. \n\n"}
{"id": "1503.02955", "contents": "Title: Low-Delay Adaptive Video Streaming Based on Short-Term TCP Throughput\n  Prediction Abstract: Recently, HTTP-Based Adaptive Streaming has become the de facto standard for\nvideo streaming over the Internet. It allows the client to adapt media\ncharacteristics to varying network conditions in order to maximize Quality of\nExperience (QoE). In the case of live streaming this task becomes particularly\nchallenging. An important factor than might help improving performance is the\ncapability to correctly predict network throughput dynamics on short to medium\ntimescales. It becomes notably difficult in wireless networks that are often\nsubject to continuous throughput fluctuations.\n  In the present work, we develop an adaptation algorithm for HTTP-Based\nAdaptive Live Streaming that, for each adaptation decision, maximizes a\nQoE-based utility function depending on the probability of playback\ninterruptions, average video quality, and the amount of video quality\nfluctuations. To compute the utility function the algorithm leverages\nthroughput predictions, and dynamically estimated prediction accuracy.\n  We are trying to close the gap created by the lack of studies analyzing TCP\nthroughput on short to medium timescales. We study several time series\nprediction methods and their error distributions. We observe that Simple Moving\nAverage performs best in most cases. We also observe that the relative\nunderestimation error is best represented by a truncated normal distribution,\nwhile the relative overestimation error is best represented by a Lomax\ndistribution. Moreover, underestimations and overestimations exhibit a temporal\ncorrelation that we use to further improve prediction accuracy.\n  We compare the proposed algorithm with a baseline approach that uses a fixed\nmargin between past throughput and selected media bit rate, and an oracle-based\napproach that has perfect knowledge over future throughput for a certain time\nhorizon. \n\n"}
{"id": "1503.03366", "contents": "Title: Are Heterogeneous Cloud-Based Radio Access Networks Cost Effective? Abstract: Mobile networks of the future are predicted to be much denser than today's\nnetworks in order to cater to increasing user demands. In this context, cloud\nbased radio access networks have garnered significant interest as a cost\neffective solution to the problem of coping with denser networks and providing\nhigher data rates. However, to the best knowledge of the authors, a\nquantitative analysis of the cost of such networks is yet to be undertaken.\nThis paper develops a theoretic framework that enables computation of the\ndeployment cost of a network (modeled using various spatial point processes) to\nanswer the question posed by the paper's title. Then, the framework obtained is\nused along with a complexity model, which enables computing the information\nprocessing costs of a network, to compare the deployment cost of a cloud based\nnetwork against that of a traditional LTE network, and to analyze why they are\nmore economical. Using this framework and an exemplary budget, this paper shows\nthat cloud-based radio access networks require approximately 10 to 15% less\ncapital expenditure per square kilometer than traditional LTE networks. It also\ndemonstrates that the cost savings depend largely on the costs of base stations\nand the mix of backhaul technologies used to connect base stations with data\ncenters. \n\n"}
{"id": "1503.04317", "contents": "Title: HybridTE: Traffic Engineering for Very Low-Cost Software-Defined\n  Data-Center Networks Abstract: The size of modern data centers is constantly increasing. As it is not\neconomic to interconnect all machines in the data center using a\nfull-bisection-bandwidth network, techniques have to be developed to increase\nthe efficiency of data-center networks. The Software-Defined Network paradigm\nopened the door for centralized traffic engineering (TE) in such environments.\nUp to now, there were already a number of TE proposals for SDN-controlled data\ncenters that all work very well. However, these techniques either use a high\namount of flow table entries or a high flow installation rate that overwhelms\navailable switching hardware, or they require custom or very expensive\nend-of-line equipment to be usable in practice.\n  We present HybridTE, a TE technique that uses (uncertain) information about\nlarge flows. Using this extra information, our technique has very low hardware\nrequirements while maintaining better performance than existing TE techniques.\nThis enables us to build very low-cost, high performance data-center networks. \n\n"}
{"id": "1503.05098", "contents": "Title: Randomizing bipartite networks: the case of the World Trade Web Abstract: Within the last fifteen years, network theory has been successfully applied\nboth to natural sciences and to socioeconomic disciplines. In particular,\nbipartite networks have been recognized to provide a particularly insightful\nrepresentation of many systems, ranging from mutualistic networks in ecology to\ntrade networks in economy, whence the need of a pattern detection-oriented\nanalysis in order to identify statistically-significant structural properties.\nSuch an analysis rests upon the definition of suitable null models, i.e. upon\nthe choice of the portion of network structure to be preserved while\nrandomizing everything else. However, quite surprisingly, little work has been\ndone so far to define null models for real bipartite networks. The aim of the\npresent work is to fill this gap, extending a recently-proposed method to\nrandomize monopartite networks to bipartite networks. While the proposed\nformalism is perfectly general, we apply our method to the binary, undirected,\nbipartite representation of the World Trade Web, comparing the observed values\nof a number of structural quantities of interest with the expected ones,\ncalculated via our randomization procedure. Interestingly, the behavior of the\nWorld Trade Web in this new representation is strongly different from the\nmonopartite analogue, showing highly non-trivial patterns of self-organization. \n\n"}
{"id": "1503.05365", "contents": "Title: Caching at the Edge: a Green Perspective for 5G Networks Abstract: Endowed with context-awareness and proactive capabilities, caching users'\ncontent locally at the edge of the network is able to cope with increasing data\ntraffic demand in 5G wireless networks. In this work, we focus on the energy\nconsumption aspects of cache-enabled wireless cellular networks, specifically\nin terms of area power consumption (APC) and energy efficiency (EE). We assume\nthat both base stations (BSs) and mobile users are distributed according to\nhomogeneous Poisson point processes (PPPs) and we introduce a detailed power\nmodel that takes into account caching. We study the conditions under which the\narea power consumption is minimized with respect to BS transmit power, while\nensuring a certain quality of service (QoS) in terms of coverage probability.\nFurthermore, we provide the optimal BS transmit power that maximizes the area\nspectral efficiency per unit total power spent. The main takeaway of this paper\nis that caching seems to be an energy efficient solution. \n\n"}
{"id": "1503.05448", "contents": "Title: A Transfer Learning Approach for Cache-Enabled Wireless Networks Abstract: Locally caching contents at the network edge constitutes one of the most\ndisruptive approaches in $5$G wireless networks. Reaping the benefits of edge\ncaching hinges on solving a myriad of challenges such as how, what and when to\nstrategically cache contents subject to storage constraints, traffic load,\nunknown spatio-temporal traffic demands and data sparsity. Motivated by this,\nwe propose a novel transfer learning-based caching procedure carried out at\neach small cell base station. This is done by exploiting the rich contextual\ninformation (i.e., users' content viewing history, social ties, etc.) extracted\nfrom device-to-device (D2D) interactions, referred to as source domain. This\nprior information is incorporated in the so-called target domain where the goal\nis to optimally cache strategic contents at the small cells as a function of\nstorage, estimated content popularity, traffic load and backhaul capacity. It\nis shown that the proposed approach overcomes the notorious data sparsity and\ncold-start problems, yielding significant gains in terms of users'\nquality-of-experience (QoE) and backhaul offloading, with gains reaching up to\n$22\\%$ in a setting consisting of four small cell base stations. \n\n"}
{"id": "1503.06377", "contents": "Title: On Orchestrating Virtual Network Functions in NFV Abstract: Middleboxes or network appliances like firewalls, proxies and WAN optimizers\nhave become an integral part of today's ISP and enterprise networks. Middlebox\nfunctionalities are usually deployed on expensive and proprietary hardware that\nrequire trained personnel for deployment and maintenance. Middleboxes\ncontribute significantly to a network's capital and operational costs. In\naddition, organizations often require their traffic to pass through a specific\nsequence of middleboxes for compliance with security and performance policies.\nThis makes the middlebox deployment and maintenance tasks even more\ncomplicated. Network Function Virtualization (NFV) is an emerging and promising\ntechnology that is envisioned to overcome these challenges. It proposes to move\npacket processing from dedicated hardware middleboxes to software running on\ncommodity servers. In NFV terminology, software middleboxes are referred to as\nVirtualized Network Functions (VNFs). It is a challenging problem to determine\nthe required number and placement of VNFs that optimizes network operational\ncosts and utilization, without violating service level agreements. We call this\nthe VNF Orchestration Problem (VNF-OP) and provide an Integer Linear\nProgramming (ILP) formulation with implementation in CPLEX. We also provide a\ndynamic programming based heuristic to solve larger instances of VNF-OP. Trace\ndriven simulations on real-world network topologies demonstrate that the\nheuristic can provide solutions that are within 1.3 times of the optimal\nsolution. Our experiments suggest that a VNF based approach can provide more\nthan 4x reduction in the operational cost of a network. \n\n"}
{"id": "1503.07568", "contents": "Title: Router-level community structure of the Internet Autonomous Systems Abstract: The Internet is composed of routing devices connected between them and\norganized into independent administrative entities: the Autonomous Systems. The\nexistence of different types of Autonomous Systems (like large connectivity\nproviders, Internet Service Providers or universities) together with\ngeographical and economical constraints, turns the Internet into a complex\nmodular and hierarchical network. This organization is reflected in many\nproperties of the Internet topology, like its high degree of clustering and its\nrobustness.\n  In this work, we study the modular structure of the Internet router-level\ngraph in order to assess to what extent the Autonomous Systems satisfy some of\nthe known notions of community structure. We show that the modular structure of\nthe Internet is much richer than what can be captured by the current community\ndetection methods, which are severely affected by resolution limits and by the\nheterogeneity of the Autonomous Systems. Here we overcome this issue by using a\nmultiresolution detection algorithm combined with a small sample of nodes. We\nalso discuss recent work on community structure in the light of our results. \n\n"}
{"id": "1504.03503", "contents": "Title: Backhaul-Aware User Association and Resource Allocation for\n  Energy-Constrained HetNets Abstract: Growing attentions have been paid to renewable energy or hybrid energy\npowered heterogeneous networks (HetNets). In this paper, focusing on\nbackhaul-aware joint user association and resource allocation for this type of\nHetNets, we formulate an online optimization problem to maximize the network\nutility reflecting proportional fairness. Since user association and resource\nallocation are tightly coupled not only on resource consumption of the base\nstations (BSs), but also in the constraints of their available energy and\nbackhaul, the closed-form solution is quite difficult to obtain. Thus, we solve\nthe problem distributively via employing some decomposition methods.\nSpecifically, at first, by adopting primal decomposition method, we decompose\nthe original problem into a lower-level resource allocation problem for each\nBS, and a higher-level user association problem. For the optimal resource\nallocation, we prove that a BS either assigns equal normalized resources or\nprovides equal long-term service rate to its served users. Then, the user\nassociation problem is solved by Lagrange dual decomposition method, and a\ncompletely distributed algorithm is developed. Moreover, applying results of\nthe subgradient method, we demonstrate the convergence of the proposed\ndistributed algorithm. Furthermore, in order to efficiently and reliably apply\nthe proposed algorithm to the future wireless networks with an extremely dense\nBS deployment, we design a virtual user association and resource allocation\nscheme based on the software-defined networking architecture. Lastly, numerical\nresults validate the convergence of the proposed algorithm and the significant\nimprovement on network utility, load balancing and user fairness. \n\n"}
{"id": "1504.04076", "contents": "Title: End-to-End Service Delivery with QoS Guarantee in Software Defined\n  Networks Abstract: Software-Defined Network (SDN) is expected to have a significant impact on\nfuture networking. Although exciting progress has been made toward realizing\nSDN, application of this new networking paradigm in the future Internet to\nsupport end-to-end QoS provisioning faces some new challenges. The autonomous\nnetwork domains coexisting in the Internet and the diverse user applications\ndeployed upon the Internet call for a uniform Service Delivery Platform (SDP)\nthat enables high-level network abstraction and inter-domain collaboration for\nend-to-end service provisioning. However, the currently available SDN\ntechnologies lack effective mechanisms for supporting such a platform. In this\npaper, we first present a SDP framework that applies the Network-as-a-Service\n(NaaS) principle to provide network abstraction and orchestration for\nend-to-end service provisioning in SDN-based future Internet. Then we focus our\nstudy on two enabling technologies for such a SDP to achieve QoS guarantee;\nnamely a network abstraction model and an end-to-end resource allocation\nscheme. Specifically we propose a general model for abstracting the service\ncapabilities offered by network domains and develop a technique for determining\nthe required amounts of bandwidth in network domains for end-to-end service\ndelivery with QoS guarantee. Both the analytical and numerical results obtained\nin this paper indicate that the NaaS-based SDP not only simplifies SDN service\nand resource management but also enhances bandwidth utilization for end-to-end\nQoS provisioning. \n\n"}
{"id": "1504.06262", "contents": "Title: 40 Gbps Access for Metro networks: Implications in terms of\n  Sustainability and Innovation from an LCA Perspective Abstract: In this work, the implications of new technologies, more specifically the new\noptical FTTH technologies, are studied both from the functional and\nnon-functional perspectives. In particular, some direct impacts are listed in\nthe form of abandoning non-functional technologies, such as micro-registration,\nwhich would be implicitly required for having a functioning operation before\narrival the new high-bandwidth access technologies. It is shown that such\nabandonment of non-functional best practices, which are mainly at the\nmanagement level of ICT, immediately results in additional consumption and\nenvironmental footprint, and also there is a chance that some other new\ninnovations might be 'missed.' Therefore, unconstrained deployment of these\naccess technologies is not aligned with a possible sustainable ICT picture,\nexcept if they are regulated. An approach to pricing the best practices,\nincluding both functional and non-functional technologies, is proposed in order\nto develop a regulation and policy framework for a sustainable broadband\naccess. \n\n"}
{"id": "1504.06262", "contents": "Title: 40 Gbps Access for Metro networks: Implications in terms of\n  Sustainability and Innovation from an LCA Perspective Abstract: In this work, the implications of new technologies, more specifically the new\noptical FTTH technologies, are studied both from the functional and\nnon-functional perspectives. In particular, some direct impacts are listed in\nthe form of abandoning non-functional technologies, such as micro-registration,\nwhich would be implicitly required for having a functioning operation before\narrival the new high-bandwidth access technologies. It is shown that such\nabandonment of non-functional best practices, which are mainly at the\nmanagement level of ICT, immediately results in additional consumption and\nenvironmental footprint, and also there is a chance that some other new\ninnovations might be 'missed.' Therefore, unconstrained deployment of these\naccess technologies is not aligned with a possible sustainable ICT picture,\nexcept if they are regulated. An approach to pricing the best practices,\nincluding both functional and non-functional technologies, is proposed in order\nto develop a regulation and policy framework for a sustainable broadband\naccess. \n\n"}
{"id": "1504.06316", "contents": "Title: Interactive Communication with Unknown Noise Rate Abstract: Alice and Bob want to run a protocol over a noisy channel, where a certain\nnumber of bits are flipped adversarially. Several results take a protocol\nrequiring $L$ bits of noise-free communication and make it robust over such a\nchannel. In a recent breakthrough result, Haeupler described an algorithm that\nsends a number of bits that is conjectured to be near optimal in such a model.\nHowever, his algorithm critically requires $a \\ priori$ knowledge of the number\nof bits that will be flipped by the adversary.\n  We describe an algorithm requiring no such knowledge. If an adversary flips\n$T$ bits, our algorithm sends $L + O\\left(\\sqrt{L(T+1)\\log L} + T\\right)$ bits\nin expectation and succeeds with high probability in $L$. It does so without\nany $a \\ priori$ knowledge of $T$. Assuming a conjectured lower bound by\nHaeupler, our result is optimal up to logarithmic factors.\n  Our algorithm critically relies on the assumption of a private channel. We\nshow that privacy is necessary when the amount of noise is unknown. \n\n"}
{"id": "1504.06957", "contents": "Title: Full-duplex MAC Protocol Design and Analysis Abstract: The idea of in-band full-duplex (FD) communications revives in recent years\nowing to the significant progress in the self-interference cancellation and\nhardware design techniques, offering the potential to double spectral\nefficiency. The adaptations in upper layers are highly demanded in the design\nof FD communication systems. In this letter, we propose a novel medium access\ncontrol (MAC) using FD techniques that allows transmitters to monitor the\nchannel usage while transmitting, and backoff as soon as collision happens.\nAnalytical saturation throughput of the FD-MAC protocol is derived with the\nconsideration of imperfect sensing brought by residual self- interference (RSI)\nin the PHY layer. Both analytical and simulation results indicate that the\nnormalized saturation throughput of the proposed FD-MAC can significantly\noutperforms conventional CSMA/CA under various network conditions. \n\n"}
{"id": "1504.07566", "contents": "Title: Designing Wireless Broadband Access for Energy Efficiency: Are Small\n  Cells the Only Answer? Abstract: The main usage of cellular networks has changed from voice to data traffic,\nmostly requested by static users. In this paper, we analyze how a cellular\nnetwork should be designed to provide such wireless broadband access with\nmaximal energy efficiency (EE). Using stochastic geometry and a detailed power\nconsumption model, we optimize the density of access points (APs), number of\nantennas and users per AP, and transmission power for maximal EE. Small cells\nare of course a key technology in this direction, but the analysis shows that\nthe EE improvement of a small-cell network saturates quickly with the AP\ndensity and then \"massive MIMO\" techniques can further improve the EE. \n\n"}
{"id": "1504.07704", "contents": "Title: Accelerating the Development of Software-Defined Network Optimization\n  Applications Using SOL Abstract: Software-defined networking (SDN) can enable diverse network management\napplications such as traffic engineering, service chaining, network function\noutsourcing, and topology reconfiguration. Realizing the benefits of SDN for\nthese applications, however, entails addressing complex network optimizations\nthat are central to these problems. Unfortunately, such optimization problems\nrequire significant manual effort and expertise to express and non-trivial\ncomputation and/or carefully crafted heuristics to solve. Our vision is to\nsimplify the deployment of SDN applications using general high-level\nabstractions for capturing optimization requirements from which we can\nefficiently generate optimal solutions. To this end, we present SOL, a\nframework that demonstrates that it is indeed possible to simultaneously\nachieve generality and efficiency. The insight underlying SOL is that SDN\napplications can be recast within a unifying path-based optimization\nabstraction, from which it efficiently generates near-optimal solutions, and\ndevice configurations to implement those solutions. We illustrate the\ngenerality of SOL by prototyping diverse and new applications. We show that SOL\nsimplifies the development of SDN-based network optimization applications and\nprovides comparable or better scalability than custom optimization solutions. \n\n"}
{"id": "1505.00471", "contents": "Title: Phase Transitions, Renormalization and Yang-Lee Zeros in Stock Markets Abstract: The present paper analyses the formal parallelism existing between the laws\nof thermodynamics and some economic principles. Based on previous works, we\nshall show how the existence in Economics of principles analogous to those in\nthermodynamics involves the occurrence of economic events that remind of\nwell-known phenomenological thermodynamic paradigms (i.e., the magnetocaloric\neffect and population inversion). We shall also show how the phase transition\nand renormalization theory provides a natural framework to understand and\npredict trend changes in stock markets. Finally, current negotiation strategies\nin financial markets are briefly reviewed. \n\n"}
{"id": "1505.03653", "contents": "Title: Timed Consistent Network Updates Abstract: Network updates such as policy and routing changes occur frequently in\nSoftware Defined Networks (SDN). Updates should be performed consistently,\npreventing temporary disruptions, and should require as little overhead as\npossible. Scalability is increasingly becoming an essential requirement in SDN.\nIn this paper we propose to use time-triggered network updates to achieve\nconsistent updates. Our proposed solution requires lower overhead than existing\nupdate approaches, without compromising the consistency during the update. We\ndemonstrate that accurate time enables far more scalable consistent updates in\nSDN than previously available. In addition, it provides the SDN programmer with\nfine-grained control over the tradeoff between consistency and scalability. \n\n"}
{"id": "1505.05646", "contents": "Title: A mechanized proof of loop freedom of the (untimed) AODV routing\n  protocol Abstract: The Ad hoc On-demand Distance Vector (AODV) routing protocol allows the nodes\nin a Mobile Ad hoc Network (MANET) or a Wireless Mesh Network (WMN) to know\nwhere to forward data packets. Such a protocol is 'loop free' if it never leads\nto routing decisions that forward packets in circles. This paper describes the\nmechanization of an existing pen-and-paper proof of loop freedom of AODV in the\ninteractive theorem prover Isabelle/HOL. The mechanization relies on a novel\ncompositional approach for lifting invariants to networks of nodes. We exploit\nthe mechanization to analyse several improvements of AODV and show that\nIsabelle/HOL can re-establish most proof obligations automatically and identify\nexactly the steps that are no longer valid. \n\n"}
{"id": "1505.06004", "contents": "Title: Vehicular Communications: Survey and Challenges of Channel and\n  Propagation Models Abstract: Vehicular communication is characterized by a dynamic environment, high\nmobility, and comparatively low antenna heights on the communicating entities\n(vehicles and roadside units). These characteristics make vehicular propagation\nand channel modeling particularly challenging. In this article, we classify and\ndescribe the most relevant vehicular propagation and channel models, with a\nparticular focus on the usability of the models for the evaluation of protocols\nand applications. We first classify the models based on the propagation\nmechanisms they employ and their implementation approach. We also classify the\nmodels based on the channel properties they implement and pay special attention\nto the usability of the models, including the complexity of implementation,\nscalability, and the input requirements (e.g., geographical data input). We\nalso discuss the less-explored aspects in vehicular channel modeling, including\nmodeling specific environments (e.g., tunnels, overpasses, and parking lots)\nand types of communicating vehicles (e.g., scooters and public transportation\nvehicles). We conclude by identifying the underresearched aspects of vehicular\npropagation and channel modeling that require further modeling and measurement\nstudies. \n\n"}
{"id": "1506.04657", "contents": "Title: A Non-stationary Service Curve Model for Performance Analysis of\n  Transient Phases Abstract: Steady-state solutions for a variety of relevant queueing systems are known\ntoday, e.g., from queueing theory, effective bandwidths, and network calculus.\nThe behavior during transient phases, on the other hand, is understood to a\nmuch lesser extent as its analysis poses significant challenges. Considering\nthe majority of short-lived flows, transient effects that have diverse causes,\nsuch as TCP slow start, sleep scheduling in wireless networks, or signalling in\ncellular networks, are, however, predominant. This paper contributes a general\nmodel of regenerative service processes to characterize the transient behavior\nof systems. The model leads to a notion of non-stationary service curves that\ncan be conveniently integrated into the framework of the stochastic network\ncalculus. We derive respective models of sleep scheduling and show the\nsignificant impact of transient phases on backlogs and delays. We also consider\nmeasurement methods that estimate the service of an unknown system from\nobservations of selected probe traffic. We find that the prevailing rate\nscanning method does not recover the service during transient phases well. This\nlimitation is fundamental as it is explained by the non-convexity of\nnon-stationary service curves. A second key difficulty is proven to be due to\nthe super-additivity of network service processes. We devise a novel two-phase\nprobing technique that first determines a minimal pattern of probe traffic.\nThis probe is used to obtain an accurate estimate of the unknown transient\nservice. \n\n"}
{"id": "1506.07468", "contents": "Title: Coexistence Analysis between Radar and Cellular System in LoS Channel Abstract: Sharing spectrum with incumbents such as radar systems is an attractive\nsolution for cellular operators in order to meet the ever growing bandwidth\nrequirements and ease the spectrum crunch problem. In order to realize\nefficient spectrum sharing, interference mitigation techniques are required. In\nthis letter we address techniques to mitigate MIMO radar interference at MIMO\ncellular base stations (BSs). We specifically look at the amount of power\nreceived at BSs when radar uses null space projection (NSP)-based interference\nmitigation method. NSP reduces the amount of projected power at targets that\nare in-close vicinity to BSs. We study this issue and show that this can be\navoided if radar employs a larger transmit array. In addition, we compute the\ncoherence time of channel between radar and BSs and show that the coherence\ntime of channel is much larger than the pulse repetition interval of radars.\nTherefore, NSP-based interference mitigation techniques which depends on\naccurate channel state information (CSI) can be effective as the problem of CSI\nbeing outdated does not occur for most practical scenarios. \n\n"}
{"id": "1506.07911", "contents": "Title: Dynamic Time-domain Duplexing for Self-backhauled Millimeter Wave\n  Cellular Networks Abstract: Millimeter wave (mmW) bands between 30 and 300 GHz have attracted\nconsiderable attention for next-generation cellular networks due to vast\nquantities of available spectrum and the possibility of very high-dimensional\nantenna ar-rays. However, a key issue in these systems is range: mmW signals\nare extremely vulnerable to shadowing and poor high-frequency propagation.\nMulti-hop relaying is therefore a natural technology for such systems to\nimprove cell range and cell edge rates without the addition of wired access\npoints. This paper studies the problem of scheduling for a simple\ninfrastructure cellular relay system where communication between wired base\nstations and User Equipment follow a hierarchical tree structure through fixed\nrelay nodes. Such a systems builds naturally on existing cellular mmW backhaul\nby adding mmW in the access links. A key feature of the proposed system is that\nTDD duplexing selections can be made on a link-by-link basis due to directional\nisolation from other links. We devise an efficient, greedy algorithm for\ncentralized scheduling that maximizes network utility by jointly optimizing the\nduplexing schedule and resources allocation for dense, relay-enhanced OFDMA/TDD\nmmW networks. The proposed algorithm can dynamically adapt to loading, channel\nconditions and traffic demands. Significant throughput gains and improved\nresource utilization offered by our algorithm over the static,\nglobally-synchronized TDD patterns are demonstrated through simulations based\non empirically-derived channel models at 28 GHz. \n\n"}
{"id": "1506.07942", "contents": "Title: A Comprehensive Survey of Potential Game Approaches to Wireless Networks Abstract: Potential games form a class of non-cooperative games where unilateral\nimprovement dynamics are guaranteed to converge in many practical cases. The\npotential game approach has been applied to a wide range of wireless network\nproblems, particularly to a variety of channel assignment problems. In this\npaper, the properties of potential games are introduced, and games in wireless\nnetworks that have been proven to be potential games are comprehensively\ndiscussed. \n\n"}
{"id": "1507.04658", "contents": "Title: Tractable Resource Management in Millimeter-Wave Overlaid Ultra-Dense\n  Cellular Networks Abstract: What does millimeter-wave (mmW) seek assistance for from micro-wave ({\\mu}W)\nin a mmW overlaid 5G cellular network? This paper raises the question of\nwhether to complement downlink (DL) or uplink (UL) transmissions, and concludes\nthat {\\mu}W should aid UL more. Such dedication to UL results from the low mmW\nUL rate due to high peak-to-average power ratio (PAPR) at mobile users. The\nDL/UL allocations are tractably provided based on a novel closed-form mm-{\\mu}W\nspectral efficiency (SE) derivation via stochastic geometry. The findings\nexplicitly indicate: (i) both DL/UL mmW (or {\\mu}W) SEs coincidentally converge\non the same value in an ultra-dense cellular network (UDN) and (ii) such a mmW\n(or {\\mu}W) UDN SE is a logarithmic function of BS-to-user density ratio. The\ncorresponding mm-{\\mu}W resource management is evaluated by utilizing a three\ndimensional (3D) blockage model with real geography in Seoul, Korea. \n\n"}
{"id": "1507.06295", "contents": "Title: The Service-Bond Paradigm - Potentials for a Sustainable, ICT-enabled\n  Future Abstract: The service paradigm as we know it has gone through a long journey of\nevolution and improvement, and it seems that a service-oriented vision to\nactivities in general could serve as a potential platform for the global\ntransition to a sustainable future. However, it is also apparent that the\nservices themselves are required to move beyond their traditional definition in\norder to prevent any secondary side effect. Here, a new paradigm is proposed\nbased on bonding between entities involved in a service interaction, service\nchaining, or service orchestration. It is purposed to serve as a vehicle to\napproach sustainability at the global level in a manner that is thoughtful,\ncollaborative, and incremental. Time-modulated implementation of the proposed\nservice-bond paradigm is considered in order to reduce the associated risks and\nliabilities. The service bonds are then simply generalized toward representing\nbonding among more than two entities. Finally, a practical application of ICT\nagents in enabling the service bonds is presented in a use case related to\nsmart houses. In this use case, some ICT-based agents (federal regulars, among\nother ICT agents) are considered to represent and govern services and service\nbonds of a household with external entities such as utilities. \n\n"}
{"id": "1507.07159", "contents": "Title: Optimal Power Allocation for LTE Users with Different Modulations Abstract: In this paper, we demonstrate the optimal power allocation for QPSK, 16-QAM,\nand 64-QAM modulation schedules and the role of channel quality indicator\n(CQI). We used sigmoidal-like utility functions to represent the probability of\nsuccessful reception of packets at user equipment (UE). CQI as a feedback to\nthe base station (BS) indicates the data rate that a downlink channel can\nsupport. With Levenberg-Marquardt (LM) Optimization method, we present utility\nfunctions of different CQI values for standardized 15 Modulation order and\nCoding Scheme (MCS) in $3^{rd}$ Generation Partnership Project (3GPP). Finally,\nwe simulate and show the results of the optimal power allocation algorithm. \n\n"}
{"id": "1507.07216", "contents": "Title: Model Risk Analysis via Investment Structuring Abstract: \"What are the origins of risks?\" and \"How material are they?\" -- these are\nthe two most fundamental questions of any risk analysis. Quantitative\nStructuring -- a technology for building financial products -- provides\neconomically meaningful answers for both of these questions. It does so by\nconsidering risk as an investment opportunity. The structure of the investment\nreveals the precise sources of risk and its expected performance measures\nmateriality. We demonstrate these capabilities of Quantitative Structuring\nusing a concrete practical example -- model risk in options on vol-targeted\nindices. \n\n"}
{"id": "1507.08834", "contents": "Title: Response-Time-Optimised Service Deployment: MILP Formulations of\n  Piece-wise Linear Functions Approximating Non-linear Bivariate Mixed-integer\n  Functions Abstract: A current trend in networking and cloud computing is to provide compute\nresources at widely dispersed places; this is exemplified by developments such\nas Network Function Virtualisation. This paves the way for wide-area service\ndeployments with improved service quality: e.g, a nearby server can reduce the\nuser-perceived response times. But always using the nearest server can be a bad\ndecision if that server is already highly utilised. This paper formalises the\ntwo related problems of allocating resources at different locations and\nassigning users to them with the goal of minimising the response times for a\ngiven number of resources to use -- a non-linear capacitated facility location\nproblem with integrated queuing systems. To efficiently handle the\nnon-linearity, we introduce five linear problem approximations and adapt the\ncurrently best heuristic for a similar problem to our scenario. All six\napproaches are compared in experiments for solution quality and solving time.\nSurprisingly, our best optimisation formulation outperforms the heuristic in\nboth time and quality. Additionally, we evaluate the influence ot resource\ndistributions in the network on the response time: Cut by half for some\nconfigurations. The presented formulations are applicable to a broader\noptimisation domain. \n\n"}
{"id": "1508.00527", "contents": "Title: On the Base Station Association Problem in HetSNets Abstract: The dense deployment of small-cell base stations in HetSNets requires\nefficient resource allocation techniques. More precisely, the problem of\nassociating users to SBSs must be revised and carefully studied. This problem\nis NP-hard and requires solving an integer optimization problem. In order to\nefficiently solve this problem, we model it using non-cooperative game theory.\nFirst, we design two non-cooperative games to solve the problem and show the\nexistence of pure Nash equilibria (PNE) in both games. These equilibria are\nshown to be far from the social optimum. Hence, we propose a better game design\nin order to approach this optimum. This new game is proved to have no PNE in\ngeneral. However, simulations show, for Rayleigh fading channels, that a PNE\nalways exists for all instances of the game. In addition, we show that its\nprices of anarchy and stability are close to one. We propose a best response\ndynamics (BRD) algorithm that converges to a PNE when it exists. Because of the\nhigh information exchange of BRD, a completely distributed algorithm, based on\nthe theory of learning, is proposed. Simulations show that this algorithm has\ntight-to-optimal performance and further it converges to a PNE (when existing)\nwith high probability. \n\n"}
{"id": "1508.03011", "contents": "Title: Matching-based Spectrum Allocation in Cognitive Radio Networks Abstract: In this paper, a novel spectrum association approach for cognitive radio\nnetworks (CRNs) is proposed. Based on a measure of both inference and\nconfidence as well as on a measure of quality-of-service, the association\nbetween secondary users (SUs) in the network and frequency bands licensed to\nprimary users (PUs) is investigated. The problem is formulated as a matching\ngame between SUs and PUs. In this game, SUs employ a soft-decision Bayesian\nframework to detect PUs' signals and, eventually, rank them based on the\nlogarithm of the a posteriori ratio. A performance measure that captures both\nthe ranking metric and rate is further computed by the SUs. Using this\nperformance measure, a PU evaluates its own utility function that it uses to\nbuild its own association preferences. A distributed algorithm that allows both\nSUs and PUs to interact and self-organize into a stable match is proposed.\nSimulation results show that the proposed algorithm can improve the sum of SUs'\nrates by up to 20 % and 60 % relative to the deferred acceptance algorithm and\nrandom channel allocation approach, respectively. The results also show an\nimproved convergence time. \n\n"}
{"id": "1508.03533", "contents": "Title: Detecting early signs of the 2007-2008 crisis in the world trade Abstract: Since 2007, several contributions have tried to identify early-warning\nsignals of the financial crisis. However, the vast majority of analyses has\nfocused on financial systems and little theoretical work has been done on the\neconomic counterpart. In the present paper we fill this gap and employ the\ntheoretical tools of network theory to shed light on the response of world\ntrade to the financial crisis of 2007 and the economic recession of 2008-2009.\nWe have explored the evolution of the bipartite World Trade Web (WTW) across\nthe years 1995-2010, monitoring the behavior of the system both before and\nafter 2007. Our analysis shows early structural changes in the WTW topology:\nsince 2003, the WTW becomes increasingly compatible with the picture of a\nnetwork where correlations between countries and products are progressively\nlost. Moreover, the WTW structural modification can be considered as concluded\nin 2010, after a seemingly stationary phase of three years. We have also\nrefined our analysis by considering specific subsets of countries and products:\nthe most statistically significant early-warning signals are provided by the\nmost volatile macrosectors, especially when measured on developing countries,\nsuggesting the emerging economies as being the most sensitive ones to the\nglobal economic cycles. \n\n"}
{"id": "1508.03605", "contents": "Title: Reliable Prediction of Channel Assignment Performance in Wireless Mesh\n  Networks Abstract: The advancements in wireless mesh networks (WMN), and the surge in\nmulti-radio multi-channel (MRMC) WMN deployments have spawned a multitude of\nnetwork performance issues. These issues are intricately linked to the adverse\nimpact of endemic interference. Thus, interference mitigation is a primary\ndesign objective in WMNs. Interference alleviation is often effected through\nefficient channel allocation (CA) schemes which fully utilize the potential of\nMRMC environment and also restrain the detrimental impact of interference.\nHowever, numerous CA schemes have been proposed in research literature and\nthere is a lack of CA performance prediction techniques which could assist in\nchoosing a suitable CA for a given WMN. In this work, we propose a reliable\ninterference estimation and CA performance prediction approach. We demonstrate\nits efficacy by substantiating the CA performance predictions for a given WMN\nwith experimental data obtained through rigorous simulations on an ns-3 802.11g\nenvironment. \n\n"}
{"id": "1508.06366", "contents": "Title: Wireless Powered Communication Networks: An Overview Abstract: Wireless powered communication network (WPCN) is a new networking paradigm\nwhere the battery of wireless communication devices can be remotely replenished\nby means of microwave wireless power transfer (WPT) technology. WPCN eliminates\nthe need of frequent manual battery replacement/recharging, and thus\nsignificantly improves the performance over conventional battery-powered\ncommunication networks in many aspects, such as higher throughput, longer\ndevice lifetime, and lower network operating cost. However, the design and\nfuture application of WPCN is essentially challenged by the low WPT efficiency\nover long distance and the complex nature of joint wireless information and\npower transfer within the same network. In this article, we provide an overview\nof the key networking structures and performance enhancing techniques to build\nan efficient WPCN. Besides, we point out new and challenging future research\ndirections for WPCN. \n\n"}
{"id": "1508.06586", "contents": "Title: Financial Market Modeling with Quantum Neural Networks Abstract: Econophysics has developed as a research field that applies the formalism of\nStatistical Mechanics and Quantum Mechanics to address Economics and Finance\nproblems. The branch of Econophysics that applies of Quantum Theory to\nEconomics and Finance is called Quantum Econophysics. In Finance, Quantum\nEconophysics' contributions have ranged from option pricing to market dynamics\nmodeling, behavioral finance and applications of Game Theory, integrating the\nempirical finding, from human decision analysis, that shows that nonlinear\nupdate rules in probabilities, leading to non-additive decision weights, can be\ncomputationally approached from quantum computation, with resulting quantum\ninterference terms explaining the non-additive probabilities. The current work\ndraws on these results to introduce new tools from Quantum Artificial\nIntelligence, namely Quantum Artificial Neural Networks as a way to build and\nsimulate financial market models with adaptive selection of trading rules,\nleading to turbulence and excess kurtosis in the returns distributions for a\nwide range of parameters. \n\n"}
{"id": "1509.00268", "contents": "Title: AMON: An Open Source Architecture for Online Monitoring, Statistical\n  Analysis and Forensics of Multi-gigabit Streams Abstract: The Internet, as a global system of interconnected networks, carries an\nextensive array of information resources and services. Key requirements include\ngood quality-of-service and protection of the infrastructure from nefarious\nactivity (e.g. distributed denial of service--DDoS--attacks). Network\nmonitoring is essential to network engineering, capacity planning and\nprevention / mitigation of threats. We develop an open source architecture,\nAMON (All-packet MONitor), for online monitoring and analysis of multi-gigabit\nnetwork streams. It leverages the high-performance packet monitor PF RING and\nis readily deployable on commodity hardware. AMON examines all packets,\npartitions traffic into sub-streams by using rapid hashing and computes certain\nreal-time data products. The resulting data structures provide views of the\nintensity and connectivity structure of network traffic at the time-scale of\nrouting. The proposed integrated framework includes modules for the\nidentification of heavy-hitters as well as for visualization and statistical\ndetection at the time-of-onset of high impact events such as DDoS. This allows\noperators to quickly visualize and diagnose attacks, and limit offline and time\nconsuming post-mortem analysis. We demonstrate our system in the context of\nreal-world attack incidents, and validate it against state-of-the-art\nalternatives. AMON has been deployed and is currently processing 10Gbps+ live\nInternet traffic at Merit Network. It is extensible and allows the addition of\nfurther statistical and filtering modules for real-time forensics. \n\n"}
{"id": "1509.00338", "contents": "Title: User Association in 5G Networks: A Survey and an Outlook Abstract: The fifth generation (5G) mobile networks are envisioned to support the\ndeluge of data traffic with reduced energy consumption and improved quality of\nservice (QoS) provision. To this end, the key enabling technologies, such as\nheterogeneous networks (HetNets), massive multiple-input multiple-output (MIMO)\nand millimeter wave (mmWave) techniques, are identified to bring 5G to\nfruition. Regardless of the technology adopted, a user association mechanism is\nneeded to determine whether a user is associated with a particular base station\n(BS) before the data transmission commences. User association plays a pivotal\nrole in enhancing the load balancing, the spectrum efficiency and the energy\nefficiency of networks. The emerging 5G networks introduce numerous challenges\nand opportunities for the design of sophisticated user association mechanisms.\nHence, substantial research efforts are dedicated to the issues of user\nassociation in HetNets, massive MIMO networks, mmWave networks and energy\nharvesting networks. We introduce a taxonomy as a framework for systematically\nstudying the existing user association algorithms. Based on the proposed\ntaxonomy, we then proceed to present an extensive overview of the\nstate-of-the-art in user association conceived for HetNets, massive MIMO,\nmmWave and energy harvesting networks. Finally, we summarize the challenges as\nwell as opportunities of user association in 5G and provide design guidelines\nand potential solutions for sophisticated user association mechanisms. \n\n"}
{"id": "1509.01038", "contents": "Title: Multi-Source Cooperative Communication with Opportunistic Interference\n  Cancelling Relays Abstract: In this paper we present a multi-user cooperative protocol for wireless\nnetworks. Two sources transmit simultaneously their information blocks and\nrelays employ opportunistically successive interference cancellation (SIC) in\nan effort to decode them. An adaptive decode/amplify-and-forward scheme is\napplied at the relays to the decoded blocks or their sufficient statistic if\ndecoding fails. The main feature of the protocol is that SIC is exploited in a\nnetwork since more opportunities arise for each block to be decoded as the\nnumber of used relays NRU is increased. This feature leads to benefits in terms\nof diversity and multiplexing gains that are proven with the help of an\nanalytical outage model and a diversity-multiplexing tradeoff (DMT) analysis.\nThe performance improvements are achieved without any network synchronization\nand coordination. In the final part of this work the closed-form outage\nprobability model is used by a novel approach for offline pre-selection of the\nNRU relays, that have the best SIC performance, from a larger number of NR\nnodes. The analytical results are corroborated with extensive simulations,\nwhile the protocol is compared with orthogonal and multi-user protocols\nreported in the literature. \n\n"}
{"id": "1509.01094", "contents": "Title: An Ant Colonization Routing Algorithm to Minimize Network Power\n  Consumption Abstract: Rising energy consumption of IT infrastructure concerns have spurred the\ndevelopment of more power efficient networking equipment and algorithms. When\n\\emph{old} equipment just drew an almost constant amount of power regardless of\nthe traffic load, there were some efforts to minimize the total energy usage by\nmodifying routing decisions to aggregate traffic in a minimal set of links,\ncreating the opportunity to power off some unused equipment during low traffic\nperiods. New equipment, with power profile functions depending on the offered\nload, presents new challenges for optimal routing. The goal now is not just to\npower some links down, but to aggregate and/or spread the traffic so that\ndevices operate in their sweet spot in regards to network usage. In this paper\nwe present an algorithm that, making use of the ant colonization algorithm,\ncomputes, in a decentralized manner, the routing tables so as to minimize\nglobal energy consumption. Moreover, the resulting algorithm is also able to\ntrack changes in the offered load and react to them in real time. \n\n"}
{"id": "1509.03000", "contents": "Title: Full-Duplex Transceiver for Future Cellular Network: A Smart Antenna\n  Approach Abstract: In this paper, we propose a transceiver architecture for full-duplex (FD)\neNodeB (eNB) and FD user equipment (UE) transceiver. For FD\ncommunication,.i.e., simultaneous in-band uplink and downlink operation, same\nsubcarriers can be allocated to UE in both uplink and downlink. Hence, contrary\nto traditional LTE, we propose using single-carrier frequency division multiple\naccesses (SC-FDMA) for downlink along with the conventional method of using it\nfor uplink. The use of multiple antennas at eNB and singular value\ndecomposition (SVD) in the downlink allows multiple users (MU) to operate on\nthe same set of ubcarriers. In the uplink, successive interference cancellation\nwith optimal ordering (SSIC-OO) algorithm is used to decouple signals of UEs\noperating in the same set of subcarriers. A smart antenna approach is adopted\nwhich prevents interference, in downlink of a UE, from uplink signals of other\nUEs sharing same subcarriers. The approach includes using multiple antennas at\nUEs to form directed beams towards eNode and nulls towards other UEs. The\nproposed architecture results in significant improvement of the overall\nspectrum efficiency per cell of the cellular network. \n\n"}
{"id": "1509.04333", "contents": "Title: An Introduction to Business Mathematics Abstract: These lecture notes provide a self-contained introduction to the mathematical\nmethods required in a Bachelor degree programme in Business, Economics, or\nManagement. In particular, the topics covered comprise real-valued vector and\nmatrix algebra, systems of linear algebraic equations, Leontief's stationary\ninput-output matrix model, linear programming, elementary financial\nmathematics, as well as differential and integral calculus of real-valued\nfunctions of one real variable. A special focus is set on applications in\nquantitative economical modelling. \n\n"}
{"id": "1509.04366", "contents": "Title: Neighbor discovery latency in BLE-like duty-cycled protocols Abstract: Neighbor discovery is the procedure using which two wireless devices initiate\na first contact. In low power ad-hoc networks, radios are duty-cycled and the\nlatency until a packet meets a reception phase of another device is determined\nby a random process. Most research considers slotted protocols, in which the\npoints in time for reception are temporally coupled to beacon transmissions. In\ncontrast, many recent protocols, such as ANT/ANT+ and Bluetooth Low Energy\n(BLE) use a slotless, periodic-interval based scheme for neighbor discovery.\nHere, one device periodically broadcasts packets, whereas the other device\nperiodically listens to the channel. Both periods are independent from each\nother and drawn over continuous time. Such protocols provide 3 degrees of\nfreedom (viz., the intervals for advertising and scanning and the duration of\neach scan phase). Though billions of existing BLE devices rely on these\nprotocols, neither their expected latencies nor beneficial configurations with\ngood latency-duty-cycle relations are known. Parametrizations for the\nparticipating devices are usually determined based on a \"good guess\". In this\npaper, we for the first time present a mathematical theory which can compute\nthe neighbor discovery latencies for all possible parametrizations. Further,\nour theory shows that upper bounds on the latency can be guaranteed for all\nparametrizations, except for a finite number of singularities. Therefore,\nslotless, periodic interval-based protocols can be used in applications with\ndeterministic latency demands, which have been reserved for slotted protocols\nuntil now. Our proposed theory can be used for analyzing the neighbor discovery\nlatencies, for tweaking protocol parameters and for developing new protocols. \n\n"}
{"id": "1509.06524", "contents": "Title: Option contracts for a privacy-aware market Abstract: Suppliers (including companies and individual prosumers) may wish to protect\ntheir private information when selling items they have in stock. A market is\nenvisaged where private information can be protected through the use of\ndifferential privacy and option contracts, while privacy-aware suppliers\ndeliver their stock at a reduced price. In such a marketplace a broker acts as\nintermediary between privacy-aware suppliers and end customers, providing the\nextra items possibly needed to fully meet the customers' demand, while end\ncustomers book the items they need through an option contract. All stakeholders\nmay benefit from such a marketplace. A formula is provided for the option\nprice, and a budget equation is set for the mechanism to be profitable for the\nbroker/producer. \n\n"}
{"id": "1509.06611", "contents": "Title: Stochastic Content-Centric Multicast Scheduling for Cache-Enabled\n  Heterogeneous Cellular Networks Abstract: Caching at small base stations (SBSs) has demonstrated significant benefits\nin alleviating the backhaul requirement in heterogeneous cellular networks\n(HetNets). While many existing works focus on what contents to cache at each\nSBS, an equally important problem is what contents to deliver so as to satisfy\ndynamic user demands given the cache status. In this paper, we study optimal\ncontent delivery in cache-enabled HetNets by taking into account the inherent\nmulticast capability of wireless medium. We consider stochastic content\nmulticast scheduling to jointly minimize the average network delay and power\ncosts under a multiple access constraint. We establish a content-centric\nrequest queue model and formulate this stochastic optimization problem as an\ninfinite horizon average cost Markov decision process (MDP). By using\n\\emph{relative value iteration} and special properties of the request queue\ndynamics, we characterize some properties of the value function of the MDP.\nBased on these properties, we show that the optimal multicast scheduling policy\nis of threshold type. Then, we propose a structure-aware optimal algorithm to\nobtain the optimal policy. We also propose a low-complexity suboptimal policy,\nwhich possesses similar structural properties to the optimal policy, and\ndevelop a low-complexity algorithm to obtain this policy. \n\n"}
{"id": "1509.07684", "contents": "Title: A Path Generation Approach to Embedding of Virtual Networks Abstract: As the virtualization of networks continues to attract attention from both\nindustry and academia, the Virtual Network Embedding (VNE) problem remains a\nfocus of researchers. This paper proposes a one-shot, unsplittable flow VNE\nsolution based on column generation. We start by formulating the problem as a\npath-based mathematical program called the primal, for which we derive the\ncorresponding dual problem. We then propose an initial solution which is used,\nfirst, by the dual problem and then by the primal problem to obtain a final\nsolution. Unlike most approaches, our focus is not only on embedding accuracy\nbut also on the scalability of the solution. In particular, the one-shot nature\nof our formulation ensures embedding accuracy, while the use of column\ngeneration is aimed at enhancing the computation time to make the approach more\nscalable. In order to assess the performance of the proposed solution, we\ncompare it against four state of the art approaches as well as the optimal\nlink-based formulation of the one-shot embedding problem. Experiments on a\nlarge mix of Virtual Network (VN) requests show that our solution is near\noptimal (achieving about 95% of the acceptance ratio of the optimal solution),\nwith a clear improvement over existing approaches in terms of VN acceptance\nratio and average Substrate Network (SN) resource utilization, and a\nconsiderable improvement (92% for a SN of 50 nodes) in time complexity compared\nto the optimal solution. \n\n"}
{"id": "1509.07989", "contents": "Title: Resource allocation in Peer-to-Peer Networks: A Control-Theoretical\n  Perspective Abstract: P2P system rely on voluntary allocation of resources by its members due to\nabsence of any central controlling authority. This resource allocation can be\nviewed as classical control problem where feedback is the amount of resource\nreceived, which controls the output i.e. the amount of resources shared back to\nthe network by the node. The motivation behind the use of control system in\nresource allocation is to exploit already existing tools in control theory to\nimprove the overall allocation process and thereby solving the problem of\nfreeriding and whitewashing in the network. At the outset, we have derived the\ntransfer function to model the P2P system. Subsequently, through the simulation\nresults we have shown that transfer function was able to provide optimal value\nof resource sharing for the peers during the normal as well as high degree of\noverloading in the network. Thereafter we verified the accuracy of the transfer\nfunction derived by comparing its output with the simulated P2P network. To\ndemonstrate how control system reduces free riding it has been shown through\nsimulations how the control systems penalizes the nodes indulging in different\nlevels of freeriding. Our proposed control system shows considerable gain over\nexisting state of art algorithm. This improvement is achieved through PI action\nof controller. Since low reputation peers usually subvert reputation system by\nwhitewashing. We propose and substantiate a technique modifying transfer\nfunction such that systems' sluggishness becomes adaptive in such a way that it\nencourage genuine new comers to enter network and discourages member peers to\nwhitewash. \n\n"}
{"id": "1509.08346", "contents": "Title: UB-ANC Drone: A Flexible Airborne Networking and Communications Testbed Abstract: We present the University at Buffalo's Airborne Networking and Communications\nTestbed (UB-ANC Drone). UB-ANC Drone is an open software/hardware platform that\naims to facilitate rapid testing and repeatable comparative evaluation of\nairborne networking and communications protocols at different layers of the\nprotocol stack. It combines quadcopters capable of autonomous flight with\nsophisticated command and control capabilities and embedded software-defined\nradios (SDRs), which enable flexible deployment of novel communications and\nnetworking protocols. This is in contrast to existing airborne network\ntestbeds, which rely on standard inflexible wireless technologies, e.g., Wi-Fi\nor Zigbee. UB-ANC Drone is designed with emphasis on modularity and\nextensibility, and is built around popular open-source projects and standards\ndeveloped by the research and hobby communities. This makes UB-ANC Drone highly\ncustomizable, while also simplifying its adoption. In this paper, we describe\nUB-ANC Drone's hardware and software architecture. \n\n"}
{"id": "1509.08388", "contents": "Title: A Simple Multipath OpenFlow Controller using topology-based algorithm\n  for Multipath TCP Abstract: Multipath TCP, or MPTCP, is a widely-researched mechanism that allows a\nsingle application-level connection to be split to more than one TCP stream,\nand consequently more than one network interface, as opposed to the traditional\nTCP/IP model. Being a transport layer protocol, MPTCP can easily interact\nbetween the application using it and the network supporting it. However, MPTCP\ndoes not have control of its own route. Default IP routing behavior generally\ntakes all traffic through the shortest or best-metric path. However, this\nbehavior may actually cause paths to collide with each other, creating\ncontention for bandwidth in a number of edges. This can result in a bottleneck\nwhich limits the throughput of the network. Therefore, a multipath routing\nmechanism is necessary to ensure smooth operation of MPTCP. We created smoc, a\nSimple Multipath OpenFlow Controller, that uses only topology information of\nthe network to avoid collision where possible. Evaluation of smoc in a virtual\nlocal-area and a physical wide-area SDNs showed favorable results as smoc\nprovided better performance than simple or spanning-tree routing mechanisms. \n\n"}
{"id": "1509.08778", "contents": "Title: The Impact of Dual Prediction Schemes on the Reduction of the Number of\n  Transmissions in Sensor Networks Abstract: Future Internet of Things (IoT) applications will require that billions of\nwireless devices transmit data to the cloud frequently. However, the wireless\nmedium access is pointed as a problem for the next generations of wireless\nnetworks; hence, the number of data transmissions in Wireless Sensor Networks\n(WSNs) can quickly become a bottleneck, disrupting the exponential growth in\nthe number of interconnected devices, sensors, and amount of produced data.\nTherefore, keeping a low number of data transmissions is critical to\nincorporate new sensor nodes and measure a great variety of parameters in\nfuture generations of WSNs. Thanks to the high accuracy and low complexity of\nstate-of-the-art forecasting algorithms, Dual Prediction Schemes (DPSs) are\npotential candidates to optimize the data transmissions in WSNs at the finest\nlevel because they facilitate for sensor nodes to avoid unnecessary\ntransmissions without affecting the quality of their measurements. In this\nwork, we present a sensor network model that uses statistical theorems to\ndescribe the expected impact of DPSs and data aggregation in WSNs. We aim to\nprovide a foundation for future works by characterizing the theoretical gains\nof processing data in sensors and conditioning its transmission to the\npredictions' accuracy. Our simulation results show that the number of\ntransmissions can be reduced by almost 98% in the sensor nodes with the highest\nworkload. We also detail the impact of predicting and aggregating transmissions\naccording to the parameters that can be observed in common scenarios, such as\nsensor nodes' transmission ranges, the correlation between measurements of\ndifferent sensors, and the period between two consecutive measurements in a\nsensor. \n\n"}
{"id": "1510.04488", "contents": "Title: Performance Analysis of a Heterogeneous Traffic Scheduler using Large\n  Deviation Principle Abstract: In this paper, we study the stability of light traffic achieved by a\nscheduling algorithm which is suitable for heterogeneous traffic networks.\nSince analyzing a scheduling algorithm is intractable using the conventional\nmathematical tool, our goal is to minimize the largest queue-overflow\nprobability achieved by the algorithm. In the large deviation setting, this\nproblem is equivalent to maximizing the asymptotic decay rate of the largest\nqueue-overflow probability. We first derive an upper bound on the decay rate of\nthe queue overflow probability as the queue overflow threshold approaches\ninfinity. Then, we study several structural properties of the minimum-cost-path\nto overflow of the queue with the largest length, which is basically equivalent\nto the decay rate of the largest queue-overflow probability. Given these\nproperties, we prove that the queue with the largest length follows a sample\npath with linear increment. For certain parameter value, the scheduling\nalgorithm is asymptotically optimal in reducing the largest queue length.\nThrough numerical results, we have shown the large deviation properties of the\nqueue length typically used in practice while varying one parameter of the\nalgorithm. \n\n"}
{"id": "1510.04772", "contents": "Title: Analysis of Path Loss mitigation through Dynamic Spectrum Access:\n  Software Defined Radio Abstract: In this paper, an analysis is carried out for a method to mitigate the path\nloss through the dynamic spectrum access (DSA) method. The path loss is a major\ncomponent which determines the QoS of a wireless link. Its effect is\ncomplemented by the presence of obstruction between the transmitter and\nreceiver. The future cellular network (5G) focuses on operating with the\nmillimeter-wave (mmW). In higher frequency, path loss can play a significant\nrole in degrading the link quality due to higher attenuation. In a scenario,\nwhere the operating environment is changing dynamically, sudden degradation of\noperating conditions or arrival of obstruction between transmitter and receiver\nmay result in link failure. The method analyzed here includes dynamically\nallocating spectrum at a lower frequency band for a link suffering from high\npath loss. For the analysis, a wireless link was set up using Universal\nSoftware Radio Peripherals (USRPs). The received power is observed to increase\nby dynamically changing the operating frequency from 1.9 GHz to 830 MHz.\nFinally the utility of software defined radio (SDR) in the RF front end, to\ncombat the path loss in the future cellular networks, is studied. \n\n"}
{"id": "1510.05205", "contents": "Title: Asymptotic Scaling Laws of Wireless Adhoc Network with Physical Layer\n  Caching Abstract: We propose a physical layer (PHY) caching scheme for wireless adhoc networks.\nThe PHY caching exploits cache-assisted multihop gain and cache-induced\ndual-layer CoMP gain, which substantially improves the throughput of wireless\nadhoc networks. In particular, the PHY caching scheme contains a novel PHY\ntransmission mode called the cache-induced dual-layer CoMP which can support\nhomogeneous opportunistic CoMP in the wireless adhoc network. Compared with\ntraditional per-node throughput scaling results of\n\\Theta\\left(1/\\sqrt{N}\\right), we can achieve O(1) per node throughput for a\ncached wireless adhoc network with N nodes. Moreover, we analyze the throughput\nof the PHY caching scheme for regular wireless adhoc networks and study the\nimpact of various system parameters on the PHY caching gain. \n\n"}
{"id": "1510.05938", "contents": "Title: Ultra Dense Networks: The New Wireless Frontier for Enabling 5G Access Abstract: The extreme traffic load that future wireless networks are expected to\naccommodate requires a re-thinking of the system design. Initial estimations\nindicate that, different from the evolutionary path of previous cellular\ngenerations that was based on spectral efficiency improvements, the most\nsubstantial amount of future system performance gains will be obtained by means\nof network infrastructure densification. By increasing the density of\noperator-deployed infrastructure elements, along with incorporation of\nuser-deployed access nodes and mobile user devices acting as \"infrastructure\nprosumers\", it is expected that having one or more access nodes exclusively\ndedicated to each user will become feasible, introducing the ultra dense\nnetwork (UDN) paradigm. Although it is clear that UDNs are able to take\nadvantage of the significant benefits provided by proximal transmissions and\nincreased spatial reuse of system resources, at the same time, large node\ndensity and irregular deployment introduce new challenges, mainly due to the\ninterference environment characteristics that are vastly different from\nprevious cellular deployments. This article attempts to provide insights on\nfundamental issues related to UDN deployment, such as determining the\ninfrastructure density required to support given traffic load requirements and\nthe benefits of network-wise coordination, demonstrating the potential of UDNs\nfor 5G wireless networks. \n\n"}
{"id": "1511.05892", "contents": "Title: Analysis and Optimization of Sparse Random Linear Network Coding for\n  Reliable Multicast Services Abstract: Point-to-multipoint communications are expected to play a pivotal role in\nnext-generation networks. This paper refers to a cellular system transmitting\nlayered multicast services to a multicast group of users. Reliability of\ncommunications is ensured via different Random Linear Network Coding (RLNC)\ntechniques. We deal with a fundamental problem: the computational complexity of\nthe RLNC decoder. The higher the number of decoding operations is, the more the\nuser's computational overhead grows and, consequently, the faster the battery\nof mobile devices drains. By referring to several sparse RLNC techniques, and\nwithout any assumption on the implementation of the RLNC decoder in use, we\nprovide an efficient way to characterize the performance of users targeted by\nultra-reliable layered multicast services. The proposed modeling allows to\nefficiently derive the average number of coded packet transmissions needed to\nrecover one or more service layers. We design a convex resource allocation\nframework that allows to minimize the complexity of the RLNC decoder by jointly\noptimizing the transmission parameters and the sparsity of the code. The\ndesigned optimization framework also ensures service guarantees to\npredetermined fractions of users. The performance of the proposed optimization\nframework is then investigated in a LTE-A eMBMS network multicasting H.264/SVC\nvideo services. \n\n"}
{"id": "1511.06734", "contents": "Title: A Generalized Probability Framework to Model Economic Agents' Decisions\n  Under Uncertainty Abstract: The applications of techniques from statistical (and classical) mechanics to\nmodel interesting problems in economics and finance has produced valuable\nresults. The principal movement which has steered this research direction is\nknown under the name of `econophysics'. In this paper, we illustrate and\nadvance some of the findings that have been obtained by applying the\nmathematical formalism of quantum mechanics to model human decision making\nunder `uncertainty' in behavioral economics and finance. Starting from\nEllsberg's seminal article, decision making situations have been experimentally\nverified where the application of Kolmogorovian probability in the formulation\nof expected utility is problematic. Those probability measures which by\nnecessity must situate themselves in Hilbert space (such as `quantum\nprobability') enable a faithful representation of experimental data. We thus\nprovide an explanation for the effectiveness of the mathematical framework of\nquantum mechanics in the modeling of human decision making. We want to be\nexplicit though that we are not claiming that decision making has microscopic\nquantum mechanical features. \n\n"}
{"id": "1511.08631", "contents": "Title: Dynamic Clustering and ON/OFF Strategies for Wireless Small Cell\n  Networks Abstract: In this paper, a novel cluster-based approach for maximizing the energy\nefficiency of wireless small cell networks is proposed. A dynamic mechanism is\nproposed to group locally-coupled small cell base stations (SBSs) into clusters\nbased on location and traffic load. Within each formed cluster, SBSs coordinate\ntheir transmission parameters to minimize a cost function which captures the\ntradeoffs between energy efficiency and flow level performance, while\nsatisfying their users' quality-of-service requirements. Due to the lack of\ninter-cluster communications, clusters compete with one another in order to\nimprove the overall network's energy efficiency. This inter-cluster competition\nis formulated as a noncooperative game between clusters that seek to minimize\ntheir respective cost functions. To solve this game, a distributed learning\nalgorithm is proposed using which clusters autonomously choose their optimal\ntransmission strategies based on local information. It is shown that the\nproposed algorithm converges to a stationary mixed-strategy distribution which\nconstitutes an epsilon-coarse correlated equilibrium for the studied game.\nSimulation results show that the proposed approach yields significant\nperformance gains reaching up to 36% of reduced energy expenditures and up to\n41% of reduced fractional transfer time compared to conventional approaches. \n\n"}
{"id": "1512.00137", "contents": "Title: SVC-based Multi-user Streamloading for Wireless Networks Abstract: In this paper, we present an approach for joint rate allocation and quality\nselection for a novel video streaming scheme called streamloading.\nStreamloading is a recently developed method for delivering high quality video\nwithout violating copyright enforced restrictions on content access for video\nstreaming. In regular streaming services, content providers restrict the amount\nof viewable video that users can download prior to playback. This approach can\ncause inferior user experience due to bandwidth variations, especially in\nmobile networks with varying capacity. In streamloading, the video is encoded\nusing Scalable Video Coding, and users are allowed to pre-fetch enhancement\nlayers and store them on the device, while base layers are streamed in a near\nreal-time fashion ensuring that buffering constraints on viewable content are\nmet.\n  We begin by formulating the offline problem of jointly optimizing rate\nallocation and quality selection for streamloading in a wireless network. This\nmotivates our proposed online algorithms for joint scheduling at the base\nstation and segment quality selection at receivers. The results indicate that\nstreamloading outperforms state-of-the-art streaming schemes in terms of the\nnumber of additional streams we can admit for a given video quality.\nFurthermore, the quality adaptation mechanism of our proposed algorithm\nachieves a higher performance than baseline algorithms with no (or limited)\nvideo-centric optimization of the base station's allocation of resources, e.g.,\nproportional fairness. \n\n"}
{"id": "1512.01271", "contents": "Title: Costly Circuits, Submodular Schedules: Hybrid Switch Scheduling for Data\n  Centers Abstract: Hybrid switching - in which a high bandwidth circuit switch (optical or\nwireless) is used in conjunction with a low bandwidth packet switch - is a\npromising alternative to interconnect servers in today's large scale\ndata-centers. Circuit switches offer a very high link rate, but incur a\nnon-trivial reconfiguration delay which makes their scheduling challenging. In\nthis paper, we demonstrate a lightweight, simple and nearly-optimal scheduling\nalgorithm that trades-off configuration costs with the benefits of\nreconfiguration that match the traffic demands. The algorithm has strong\nconnections to submodular optimization, has performance at least half that of\nthe optimal schedule and strictly outperforms state of the art in a variety of\ntraffic demand settings. These ideas naturally generalize: we see that indirect\nrouting leads to exponential connectivity; this is another phenomenon of the\npower of multi hop routing, distinct from the well-known load balancing\neffects. \n\n"}
{"id": "1512.02328", "contents": "Title: Node-based Service-Balanced Scheduling for Provably Guaranteed\n  Throughput and Evacuation Time Performance Abstract: This paper focuses on the design of provably efficient online link scheduling\nalgorithms for multi-hop wireless networks. We consider single-hop traffic and\nthe one-hop interference model. The objective is twofold: 1) maximizing the\nthroughput when the flow sources continuously inject packets into the network,\nand 2) minimizing the evacuation time when there are no future packet arrivals.\nThe prior work mostly employs the link-based approach, which leads to\nthroughput-efficient algorithms but often does not guarantee satisfactory\nevacuation time performance. In this paper, we propose a novel Node-based\nService-Balanced (NSB) online scheduling algorithm. NSB aims to give scheduling\nopportunities to heavily congested nodes in a balanced manner, by maximizing\nthe total weight of the scheduled nodes in each scheduling cycle, where the\nweight of a node is determined by its workload and whether the node was\nscheduled in the previous scheduling cycle(s). We rigorously prove that NSB\nguarantees to achieve an efficiency ratio no worse (or no smaller) than 2/3 for\nthe throughput and an approximation ratio no worse (or no greater) than 3/2 for\nthe evacuation time. It is remarkable that NSB is both throughput-optimal and\nevacuation-time-optimal if the underlying network graph is bipartite. Further,\nwe develop a lower-complexity NSB algorithm, called LC-NSB, which provides the\nsame performance guarantees as NSB. Finally, we conduct numerical experiments\nto elucidate our theoretical results. \n\n"}
{"id": "1512.02454", "contents": "Title: The double role of GDP in shaping the structure of the International\n  Trade Network Abstract: The International Trade Network (ITN) is the network formed by trade\nrelationships between world countries. The complex structure of the ITN impacts\nimportant economic processes such as globalization, competitiveness, and the\npropagation of instabilities. Modeling the structure of the ITN in terms of\nsimple macroeconomic quantities is therefore of paramount importance. While\ntraditional macroeconomics has mainly used the Gravity Model to characterize\nthe magnitude of trade volumes, modern network theory has predominantly focused\non modeling the topology of the ITN. Combining these two complementary\napproaches is still an open problem. Here we review these approaches and\nemphasize the double role played by GDP in empirically determining both the\nexistence and the volume of trade linkages. Moreover, we discuss a unified\nmodel that exploits these patterns and uses only the GDP as the relevant\nmacroeconomic factor for reproducing both the topology and the link weights of\nthe ITN. \n\n"}
{"id": "1512.02859", "contents": "Title: The network structure of city-firm relations Abstract: How are economic activities linked to geographic locations? To answer this\nquestion, we use a data-driven approach that builds on the information about\nlocation, ownership and economic activities of the world's 3,000 largest firms\nand their almost one million subsidiaries. From this information we generate a\nbipartite network of cities linked to economic activities. Analysing the\nstructure of this network, we find striking similarities with nested networks\nobserved in ecology, where links represent mutualistic interactions between\nspecies. This motivates us to apply ecological indicators to identify the\nunbalanced deployment of economic activities. Such deployment can lead to an\nover-representation of specific economic sectors in a given city, and poses a\nsignificant thread for the city's future especially in times when the\nover-represented activities face economic uncertainties. If we compare our\nanalysis with external rankings about the quality of life in a city, we find\nthat the nested structure of the city-firm network also reflects such\ninformation about the quality of life, which can usually be assessed only via\ndedicated survey-based indicators. \n\n"}
{"id": "1512.04602", "contents": "Title: Wisent: Robust Downstream Communication and Storage for Computational\n  RFIDs Abstract: Computational RFID (CRFID) devices are emerging platforms that can enable\nperennial computation and sensing by eliminating the need for batteries.\nAlthough much research has been devoted to improving upstream (CRFID to RFID\nreader) communication rates, the opposite direction has so far been neglected,\npresumably due to the difficulty of guaranteeing fast and error-free transfer\namidst frequent power interruptions of CRFID. With growing interest in the\nmarket where CRFIDs are forever-embedded in many structures, it is necessary\nfor this void to be filled. Therefore, we propose Wisent-a robust downstream\ncommunication protocol for CRFIDs that operates on top of the legacy UHF RFID\ncommunication protocol: EPC C1G2. The novelty of Wisent is its ability to\nadaptively change the frame length sent by the reader, based on the length\nthrottling mechanism, to minimize the transfer times at varying channel\nconditions. We present an implementation of Wisent for the WISP 5 and an\noff-the-shelf RFID reader. Our experiments show that Wisent allows transfer up\nto 16 times faster than a baseline, non-adaptive shortest frame case, i.e.\nsingle word length, at sub-meter distance. As a case study, we show how Wisent\nenables wireless CRFID reprogramming, demonstrating the world's first\nwirelessly reprogrammable (software defined) CRFID. \n\n"}
{"id": "1512.05172", "contents": "Title: On the performance overhead tradeoff of distributed principal component\n  analysis via data partitioning Abstract: Principal component analysis (PCA) is not only a fundamental dimension\nreduction method, but is also a widely used network anomaly detection\ntechnique. Traditionally, PCA is performed in a centralized manner, which has\npoor scalability for large distributed systems, on account of the large network\nbandwidth cost required to gather the distributed state at a fusion center.\nConsequently, several recent works have proposed various distributed PCA\nalgorithms aiming to reduce the communication overhead incurred by PCA without\nlosing its inferential power. This paper evaluates the tradeoff between\ncommunication cost and solution quality of two distributed PCA algorithms on a\nreal domain name system (DNS) query dataset from a large network. We also apply\nthe distributed PCA algorithm in the area of network anomaly detection and\ndemonstrate that the detection accuracy of both distributed PCA-based methods\nhas little degradation in quality, yet achieves significant savings in\ncommunication bandwidth. \n\n"}
{"id": "1512.05343", "contents": "Title: European Union gas market development Abstract: The recently announced Energy Union by the European Commission is the most\nrecent step in a series of developments aiming at integrating the EU's gas\nmarkets to increase social welfare (SW) and security of gas supply. Based on a\nspatial partial equilibrium model, we analyze the changes in consumption,\nprices, and SW up to 2022 induced by the infrastructure expansions planned for\nthis period. We find that wholesale prices decrease slightly and converge at\nWestern European levels, the potential of suppliers to exert market power\ndecreases significantly, and consumer surplus increases by 15.9% in the EU. Our\nresults allow us to distinguish three categories of projects: (i) New gas\nsources developed and brought to the EU markets. These projects decrease prices\nand increase SW in a large number of countries. The only project in this\ncategory is the Trans-Anatolian Gas Pipeline; (ii) Existing gas sources made\navailable to additional countries. This leads to an increase of SW in the newly\nconnected countries, and a decrease everywhere else. These projects mainly\ninvolve pipeline and regasification terminal capacity enhancements; (iii)\nProjects with a marginal effect on the (fully functioning) market. Most storage\nexpansion projects fall into this category, plus the recently announced Turkish\nStream. Our results indicate that if all proposed infrastructure projects are\nrealized, the EU's single market will become a reality in 2019. However, we\nalso find that SW can only be increased significantly for the EU as a whole if\nnew gas sources become accessible. Consequently, we suggest that the EU should\nemphasize on measures to increase the available volumes, in particular once the\nintegration of the market is completed. At the same time, efficiency gains,\nalbeit decreasing SW, help to improve the situation of consumers and decrease\nthe dependency of the EU as a whole on external suppliers. \n\n"}
{"id": "1512.05429", "contents": "Title: DNA-GA: A New Approach of Network Performance Analysis Abstract: In this paper, we propose a new approach of network performance analysis,\nwhich is based on our previous works on the deterministic network analysis\nusing the Gaussian approximation (DNA-GA). First, we extend our previous works\nto a signal-to-interference ratio (SIR) analysis, which makes our DNA-GA\nanalysis a formal microscopic analysis tool. Second, we show two approaches for\nupgrading the DNA-GA analysis to a macroscopic analysis tool. Finally, we\nperform a comparison between the proposed DNA-GA analysis and the existing\nmacroscopic analysis based on stochastic geometry. Our results show that the\nDNA-GA analysis possesses a few special features: (i) shadow fading is\nnaturally considered in the DNAGA analysis; (ii) the DNA-GA analysis can handle\nnon-uniform user distributions and any type of multi-path fading; (iii) the\nshape and/or the size of cell coverage areas in the DNA-GA analysis can be made\narbitrary for the treatment of hotspot network scenarios. Thus, DNA-GA analysis\nis very useful for the network performance analysis of the 5th generation (5G)\nsystems with general cell deployment and user distribution, both on a\nmicroscopic level and on a macroscopic level. \n\n"}
{"id": "1512.08792", "contents": "Title: The Role of Time in Making Risky Decisions and the Function of Choice Abstract: The prospects of Kahneman and Tversky, Mega Million and Powerball lotteries,\nSt. Petersburg paradox, premature profits and growing losses criticized by\nLivermore are reviewed under an angle of view comparing mathematical\nexpectations with awards received. Original prospects have been formulated as a\none time opportunity. An award value depends on the number of times the game is\nplayed. The random sample mean is discussed as a universal award. The role of\ntime in making a risky decision is important as long as the frequency of games\nand playing time affect their number. A function of choice mapping properties\nof two-point random variables to fractions of respondents choosing them is\nproposed. \n\n"}
{"id": "1601.02763", "contents": "Title: Bounds and Constructions of Codes with Multiple Localities Abstract: This paper studies bounds and constructions of locally repairable codes\n(LRCs) with multiple localities so-called multiple-locality LRCs (ML-LRCs). In\nthe simplest case of two localities some code symbols of an ML-LRC have a\ncertain locality while the remaining code symbols have another one. We extend\ntwo bounds, the Singleton and the alphabet-dependent upper bound on the\ndimension of Cadambe--Mazumdar for LRCs, to the case of ML-LRCs with more than\ntwo localities. Furthermore, we construct Singleton-optimal ML-LRCs as well as\ncodes that achieve the extended alphabet-dependent bound. We give a family of\nbinary ML-LRCs based on generalized code concatenation that is optimal with\nrespect to the alphabet-dependent bound. \n\n"}
{"id": "1601.02990", "contents": "Title: The invisible hand and the rational agent are behind bubbles and crashes Abstract: The substantial turmoil created by both 2000 dot-com crash and 2008 subprime\ncrisis has fueled the belief that the two classical paradigms of economics,\nwhich are the invisible hand and the rational agent, are not appropriate to\ndescribe market dynamics and should be abandoned at the benefit of alternative\nnew theoretical concepts. At odd with such a view, using a simple model of\nchoice dynamics from sociophysics, the invisible hand and the rational agent\nparadigms are given a new legitimacy. Indeed, it is sufficient to introduce the\nholding of a few intermediate mini market aggregations by agents sharing their\nown private information, to recenter the invisible hand and the rational agent\nat the heart of market self regulation including the making of bubbles and\ntheir subsequent crashes. In so doing, an elasticity is discovered in the\nmarket efficiency mechanism due to the existence of agents anticipation. This\nelasticity is found to create spontaneous bubbles, which are rationally\nfounded, and at the same time, it provokes crashes when the limit of elasticity\nis reached. Although the findings disclose a path to put an end to the\nbubble-crash phenomena, it is argued to be rationality not feasible. \n\n"}
{"id": "1601.03147", "contents": "Title: Online Algorithms for Information Aggregation from Distributed and\n  Correlated Sources Abstract: There is a fundamental trade-off between the communication cost and latency\nin information aggregation. Aggregating multiple communication messages over\ntime can alleviate overhead and improve energy efficiency on one hand, but\ninevitably incurs information delay on the other hand. In the presence of\nuncertain future inputs, this trade-off should be balanced in an online manner,\nwhich is studied by the classical dynamic TCP ACK problem for a single\ninformation source. In this paper, we extend dynamic TCP ACK problem to a\ngeneral setting of collecting aggregate information from distributed and\ncorrelated information sources. In this model, distributed sources observe\ncorrelated events, whereas only a small number of reports are required from the\nsources. The sources make online decisions about their reporting operations in\na distributed manner without prior knowledge of the local observations at\nothers. Our problem captures a wide range of applications, such as in-situ\nsensing, anycast acknowledgement and distributed caching. We present simple\nthreshold-based competitive distributed online algorithms under different\nsettings of intercommunication. Our algorithms match the theoretical lower\nbounds in order of magnitude. We observe that our algorithms can produce\nsatisfactory performance in simulations and practical testbed. \n\n"}
{"id": "1601.03876", "contents": "Title: Streaming Big Data meets Backpressure in Distributed Network Computation Abstract: We study network response to queries that require computation of remotely\nlocated data and seek to characterize the performance limits in terms of\nmaximum sustainable query rate that can be satisfied. The available resources\ninclude (i) a communication network graph with links over which data is routed,\n(ii) computation nodes, over which computation load is balanced, and (iii)\nnetwork nodes that need to schedule raw and processed data transmissions. Our\naim is to design a universal methodology and distributed algorithm to\nadaptively allocate resources in order to support maximum query rate. The\nproposed algorithms extend in a nontrivial way the backpressure (BP) algorithm\nto take into account computations operated over query streams. They contribute\nto the fundamental understanding of network computation performance limits when\nthe query rate is limited by both the communication bandwidth and the\ncomputation capacity, a classical setting that arises in streaming big data\napplications in network clouds and fogs. \n\n"}
{"id": "1601.04043", "contents": "Title: Fighting Uncertainty with Uncertainty: A Baby Step Abstract: We can overcome uncertainty with uncertainty. Using randomness in our choices\nand in what we control, and hence in the decision making process, could\npotentially offset the uncertainty inherent in the environment and yield better\noutcomes. The example we develop in greater detail is the news-vendor inventory\nmanagement problem with demand uncertainty. We briefly discuss areas, where\nsuch an approach might be helpful, with the common prescription, \"Don't Simply\nOptimize, Also Randomize; perhaps best described by the term -\nRandoptimization\".\n  1. News-vendor Inventory Management\n  2. School Admissions\n  3. Journal Submissions\n  4. Job Candidate Selection\n  5. Stock Picking\n  6. Monetary Policy\n  This methodology is suitable for the social sciences since the primary source\nof uncertainty are the members of the system themselves and presently, no\nmethods are known to fully determine the outcomes in such an environment, which\nperhaps would require being able to read the minds of everyone involved and to\nanticipate their actions continuously. Admittedly, we are not qualified to\nrecommend whether such an approach is conducive for the natural sciences,\nunless perhaps, bounds can be established on the levels of uncertainty in a\nsystem and it is shown conclusively that a better understanding of the system\nand hence improved decision making will not alter the outcomes. \n\n"}
{"id": "1601.04219", "contents": "Title: BOOST: Base station on-off switching strategy for energy efficient\n  massive MIMO HetNets Abstract: In this paper, we investigate the problem of optimal base station (BS) ON-OFF\nswitching and user association in a heterogeneous network (HetNet) with massive\nMIMO, with the objective to maximize the system energy efficiency (EE). The\njoint BS ON-OFF switching and user association problem is formulated as an\ninteger programming problem. We first develop a centralized scheme, in which we\nrelax the integer constraints and employ a series of Lagrangian dual methods\nthat transform the original problem into a standard linear programming (LP)\nproblem. Due to the special structure of the LP, we prove that the optimal\nsolution to the relaxed LP is also feasible and optimal to the original\nproblem. We then propose a distributed scheme by formulating a repeated bidding\ngame for users and BS's, and prove that the game converges to a Nash\nEquilibrium (NE). Simulation studies demonstrate that the proposed schemes can\nachieve considerable gains in EE over several benchmark schemes in all the\nscenarios considered. \n\n"}
{"id": "1601.05648", "contents": "Title: Safe and Secure Wireless Power Transfer Networks: Challenges and\n  Opportunities in RF-Based Systems Abstract: RF-based wireless power transfer networks (WPTNs) are deployed to transfer\npower to embedded devices over the air via RF waves. Up until now, a\nconsiderable amount of effort has been devoted by researchers to design WPTNs\nthat maximize several objectives such as harvested power, energy outage and\ncharging delay. However, inherent security and safety issues are generally\noverlooked and these need to be solved if WPTNs are to be become widespread.\nThis article focuses on safety and security problems related WPTNs and\nhighlight their cruciality in terms of efficient and dependable operation of\nRF-based WPTNs. We provide a overview of new research opportunities in this\nemerging domain. \n\n"}
{"id": "1601.06065", "contents": "Title: Adaptive CSMA under the SINR Model: Efficient Approximation Algorithms\n  for Throughput and Utility Maximization Abstract: We consider a Carrier Sense Multiple Access (CSMA) based scheduling algorithm\nfor a single-hop wireless network under a realistic\nSignal-to-interference-plus-noise ratio (SINR) model for the interference. We\npropose two local optimization based approximation algorithms to efficiently\nestimate certain attempt rate parameters of CSMA called fugacities. It is known\nthat adaptive CSMA can achieve throughput optimality by sampling feasible\nschedules from a Gibbs distribution, with appropriate fugacities.\nUnfortunately, obtaining these optimal fugacities is an NP-hard problem.\nFurther, the existing adaptive CSMA algorithms use a stochastic gradient\ndescent based method, which usually entails an impractically slow (exponential\nin the size of the network) convergence to the optimal fugacities. To address\nthis issue, we first propose an algorithm to estimate the fugacities, that can\nsupport a given set of desired service rates. The convergence rate and the\ncomplexity of this algorithm are independent of the network size, and depend\nonly on the neighborhood size of a link. Further, we show that the proposed\nalgorithm corresponds exactly to performing the well-known Bethe approximation\nto the underlying Gibbs distribution. Then, we propose another local algorithm\nto estimate the optimal fugacities under a utility maximization framework, and\ncharacterize its accuracy. Numerical results indicate that the proposed methods\nhave a good degree of accuracy, and achieve extremely fast convergence to\nnear-optimal fugacities, and often outperform the convergence rate of the\nstochastic gradient descent by a few orders of magnitude. \n\n"}
{"id": "1601.06439", "contents": "Title: Who Ordered This?: Exploiting Implicit User Tag Order Preferences for\n  Personalized Image Tagging Abstract: What makes a person pick certain tags over others when tagging an image? Does\nthe order that a person presents tags for a given image follow an implicit bias\nthat is personal? Can these biases be used to improve existing automated image\ntagging systems? We show that tag ordering, which has been largely overlooked\nby the image tagging community, is an important cue in understanding user\ntagging behavior and can be used to improve auto-tagging systems. Inspired by\nthe assumption that people order their tags, we propose a new way of measuring\ntag preferences, and also propose a new personalized tagging objective function\nthat explicitly considers a user's preferred tag orderings. We also provide a\n(partially) greedy algorithm that produces good solutions to our new objective\nand under certain conditions produces an optimal solution. We validate our\nmethod on a subset of Flickr images that spans 5000 users, over 5200 tags, and\nover 90,000 images. Our experiments show that exploiting personalized tag\norders improves the average performance of state-of-art approaches both on\nper-image and per-user bases. \n\n"}
{"id": "1602.03635", "contents": "Title: Optimization of Caching Devices with Geometric Constraints Abstract: It has been recently advocated that in large communication systems it is\nbeneficial both for the users and for the network as a whole to store content\ncloser to users. One particular implementation of such an approach is to\nco-locate caches with wireless base stations. In this paper we study\ngeographically distributed caching of a fixed collection of files. We model\ncache placement with the help of stochastic geometry and optimize the\nallocation of storage capacity among files in order to minimize the cache miss\nprobability. We consider both per cache capacity constraints as well as an\naverage capacity constraint over all caches. The case of per cache capacity\nconstraints can be efficiently solved using dynamic programming, whereas the\ncase of the average constraint leads to a convex optimization problem. We\ndemonstrate that the average constraint leads to significantly smaller cache\nmiss probability. Finally, we suggest a simple LRU-based policy for\ngeographically distributed caching and show that its performance is close to\nthe optimal. \n\n"}
{"id": "1602.03706", "contents": "Title: SDxVPN: A Software-Defined Solution for VPN Service Providers Abstract: BGP/MPLS IP VPN and VPLS services are considered to be widely used in IP/MPLS\nnetworks for connecting customers' remote sites. However, service providers\nstruggle with many challenges to provide these services. Management complexity,\nequipment costs, and last but not least, scalability issues emerging as the\ncustomers increase in number, are just some of these problems. Software-defined\nnetworking (SDN) is an emerging paradigm that can solve aforementioned issues\nusing a logically centralized controller for network devices. In this paper, we\npropose a SDN-based solution called SDxVPN which considerably lowers the\ncomplexity of VPN service definition and management. Our method eliminates\ncomplex and costly device interactions that used to be done through several\ncontrol plane protocols and enables customers to determine their service\nspecifications, define restriction policies and even interconnect with other\ncustomers automatically without operator's intervention. We describe our\nprototype implementation of SDxVPN and its scalability evaluations under\nseveral representative scenarios. The results indicate the effectiveness of the\nproposed solution for deployment to provide large scale VPN services. \n\n"}
{"id": "1602.03906", "contents": "Title: How to group wireless nodes together? Abstract: This report presents a survey on how to group together in a static way planar\nnodes, that may belong to a wireless network (ad hoc or cellular). The aim is\nto identify appropriate methods that could also be applied for Point Processes.\nSpecifically matching pairs and algorithms are initially discussed. Next,\nspecifically for Point Processes, the Nearest Neighbour and Lilypond models are\npresented. Properties and results for the two models are stated. Original\nbounds are given for the value of the so-called generation number, which is\nrelated to the size of the nearest neighbour cluster. Finally, a variation of\nthe nearest neighbour grouping is proposed and an original metric is\nintroduced, named here the ancestor number. This is used to facilitate the\nanalysis of the distribution of cluster size. Based on this certain related\nbounds are derived. The report and the analysis included show clearly the\ndifficulty of working in point processes with static clusters of size greater\nthan two, when these are defined by proximity criteria. \n\n"}
{"id": "1602.05883", "contents": "Title: Pathways towards instability in financial networks Abstract: Following the financial crisis of 2007-2008, a deep analogy between the\norigins of instability in financial systems and complex ecosystems has been\npointed out: in both cases, topological features of network structures\ninfluence how easily distress can spread within the system. However, in\nfinancial network models, the details of how financial institutions interact\ntypically play a decisive role, and a general understanding of precisely how\nnetwork topology creates instability remains lacking. Here we show how\nprocesses that are widely believed to stabilise the financial system, i.e.\nmarket integration and diversification, can actually drive it towards\ninstability, as they contribute to create cyclical structures which tend to\namplify financial distress, thereby undermining systemic stability and making\nlarge crises more likely. This result holds irrespective of the details of how\ninstitutions interact, showing that policy-relevant analysis of the factors\naffecting financial stability can be carried out while abstracting away from\nsuch details. \n\n"}
{"id": "1602.06045", "contents": "Title: Programmable Packet Scheduling Abstract: Switches today provide a small set of scheduling algorithms. While we can\ntweak scheduling parameters, we cannot modify algorithmic logic, or add a\ncompletely new algorithm, after the switch has been designed. This paper\npresents a design for a programmable packet scheduler, which allows scheduling\nalgorithms---potentially algorithms that are unknown today---to be programmed\ninto a switch without requiring hardware redesign.\n  Our design builds on the observation that scheduling algorithms make two\ndecisions: in what order to schedule packets and when to schedule them.\nFurther, in many scheduling algorithms these decisions can be made when packets\nare enqueued. We leverage this observation to build a programmable scheduler\nusing a single abstraction: the push-in first-out queue (PIFO), a priority\nqueue that maintains the scheduling order and time for such algorithms.\n  We show that a programmable scheduler using PIFOs lets us program a wide\nvariety of scheduling algorithms. We present a detailed hardware design for\nthis scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area\noverhead on a 16-nm standard-cell library. Our design lets us program many\nsophisticated algorithms, such as a 5-level hierarchical scheduler with\nprogrammable scheduling algorithms at each level. \n\n"}
{"id": "1602.07731", "contents": "Title: Initial Access in 5G mm-Wave Cellular Networks Abstract: The massive amounts of bandwidth available at millimeter-wave frequencies\n(roughly above 10 GHz) have the potential to greatly increase the capacity of\nfifth generation cellular wireless systems. However, to overcome the high\nisotropic pathloss experienced at these frequencies, high directionality will\nbe required at both the base station and the mobile user equipment to establish\nsufficient link budget in wide area networks. This reliance on directionality\nhas important implications for control layer procedures. Initial access in\nparticular can be significantly delayed due to the need for the base station\nand the user to find the proper alignment for directional transmission and\nreception. This paper provides a survey of several recently proposed techniques\nfor this purpose. A coverage and delay analysis is performed to compare various\ntechniques including exhaustive and iterative search, and Context Information\nbased algorithms. We show that the best strategy depends on the target SNR\nregime, and provide guidelines to characterize the optimal choice as a function\nof the system parameters. \n\n"}
{"id": "1602.08156", "contents": "Title: Capacitated Kinetic Clustering in Mobile Networks by Optimal\n  Transportation Theory Abstract: We consider the problem of capacitated kinetic clustering in which $n$ mobile\nterminals and $k$ base stations with respective operating capacities are given.\nThe task is to assign the mobile terminals to the base stations such that the\ntotal squared distance from each terminal to its assigned base station is\nminimized and the capacity constraints are satisfied. This paper focuses on the\ndevelopment of \\emph{distributed} and computationally efficient algorithms that\nadapt to the motion of both terminals and base stations. Suggested by the\noptimal transportation theory, we exploit the structural property of the\noptimal solution, which can be represented by a power diagram on the base\nstations such that the total usage of nodes within each power cell equals the\ncapacity of the corresponding base station. We show by using the kinetic data\nstructure framework the first analytical upper bound on the number of changes\nin the optimal solution, i.e., its stability. On the algorithm side, using the\npower diagram formulation we show that the solution can be represented in size\nproportional to the number of base stations and can be solved by an iterative,\nlocal algorithm. In particular, this algorithm can naturally exploit the\ncontinuity of motion and has orders of magnitude faster than existing solutions\nusing min-cost matching and linear programming, and thus is able to handle\nlarge scale data under mobility. \n\n"}
{"id": "1602.08429", "contents": "Title: No such thing as a risk-neutral market Abstract: A very brief history of relative valuation in neoclassical finance since 1973\nis presented, with attention to core currency issues for emerging economies.\nPrice formation is considered in the context of hierarchical causality, with\ndiscussion focussed on identifying mathematical modelling challenges for robust\nand transparent regulation of interactions. \n\n"}
{"id": "1602.08710", "contents": "Title: Dynamic Channel Access Scheme for Interference Mitigation in\n  Relay-assisted Intra-WBANs Abstract: This work addresses problems related to interference mitigation in a single\nwireless body area network (WBAN). In this paper, We propose a distributed\n\\textit{C}ombined carrier sense multiple access with collision avoidance\n(CSMA/CA) with \\textit{F}lexible time division multiple access (\\textit{T}DMA)\nscheme for \\textit{I}nterference \\textit{M}itigation in relay-assisted\nintra-WBAN, namely, CFTIM. In CFTIM scheme, non interfering sources\n(transmitters) use CSMA/CA to communicate with relays. Whilst, high interfering\nsources and best relays use flexible TDMA to communicate with coordinator (C)\nthrough using stable channels. Simulation results of the proposed scheme are\ncompared to other schemes and consequently CFTIM scheme outperforms in all\ncases. These results prove that the proposed scheme mitigates interference,\nextends WBAN energy lifetime and improves the throughput. To further reduce the\ninterference level, we analytically show that the outage probability can be\neffectively reduced to the minimal. \n\n"}
{"id": "1603.01315", "contents": "Title: Ecology-Based DoS Attack in Cognitive Radio Networks Abstract: Cognitive radio technology, which is designed to enhance spectrum\nutilization, depends on the success of opportunistic access, where secondary\nusers (SUs) exploit spectrum void unoccupied by primary users (PUs) for\ntransmissions. We note that the system behaviors are very similar to the\ninteractions among different species coexisting in an ecosystem. However, SUs\nof a selfish nature or of misleading information may make concurrent\ntransmissions with PUs for additional incentives, and thus disrupt the entire\necosystem. By exploiting this vulnerability, this paper proposes a novel\ndistributed denial-of-service (DoS) attack where invasive species, i.e.,\nmalicious users (MUs), induce originally normal-behaved SUs to execute\nconcurrent transmissions with PUs and thus collapse the cognitive radio\nnetwork. We adopt stochastic geometry to model the spatial distributions of\nPUs, SUs, and MUs for the analysis of the mutual interference among them. The\naccess strategy of each SU in the spectrum sharing ecosystem, which evolves\nwith the experienced payoffs and interference, is modeled by an evolutionary\ngame. Based on the evolutionary stable strategy concept, we could efficiently\nidentify the fragile operating region at which normal-behaved SUs are\neventually evolved to conduct concurrent transmissions and thus to cause the\nruin of the network. \n\n"}
{"id": "1603.01921", "contents": "Title: Optimal Geographic Caching in Finite Wireless Networks Abstract: Cache-enabled device-to-device (D2D) networks turn memory of the devices at\nthe network edge, such as smart phones and tablets, into bandwidth by enabling\nasynchronous content sharing directly between proximate devices. Limited\nstorage capacity of the mobile devices necessitates the determination of\noptimal set of contents to be cached on each device. In order to study the\nproblem of optimal cache placement, we model the locations of devices in a\nfinite region (e.g., coffee shop, sports bar, library) as a uniform binomial\npoint process (BPP). For this setup, we first develop a generic framework to\nanalyze the coverage probability of the target receiver (target-Rx) when the\nrequested content is available at the $k^{th}$ closest device to it. Using this\ncoverage probability result, we evaluate optimal caching probability of the\npopular content to maximize the total hit probability. Our analysis concretely\ndemonstrates that optimal caching probability strongly depends on the number of\nsimultaneously active devices in the network. \n\n"}
{"id": "1603.03834", "contents": "Title: Comparative Advantage Driven Resource Allocation for Virtual Network\n  Functions Abstract: As Communication Service Providers (CSPs) adopt the Network Function\nVirtualization (NFV) paradigm, they need to transition their network function\ncapacity to a virtualized infrastructure with different Network Functions\nrunning on a set of heterogeneous servers. This abstract describes a novel\ntechnique for allocating server resources (compute, storage and network) for a\ngiven set of Virtual Network Function (VNF) requirements. Our approach helps\nthe telco providers decide the most effective way to run several VNFs on\nservers with different performance characteristics. Our analysis of prior VNF\nperformance characterization on heterogeneous/different server resource\nallocations shows that the ability to arbitrarily create many VNFs among\ndifferent servers' resource allocations leads to a comparative advantage among\nservers. We propose a VNF resource allocation method called COMPARE that\nmaximizes the total throughput of the system by formulating this resource\nallocation problem as a comparative advantage problem among heterogeneous\nservers. There several applications for using the VNF resource allocation from\nCOMPARE including transitioning current Telco deployments to NFV based\nsolutions and providing initial VNF placement for Service Function Chain (SFC)\nprovisioning. \n\n"}
{"id": "1603.05181", "contents": "Title: Strength of weak layers in cascading failures on multiplex networks:\n  case of the international trade network Abstract: Many real-world complex systems across natural, social, and economical\ndomains consist of manifold layers to form multiplex networks. The multiple\nnetwork layers give rise to nonlinear effect for the emergent dynamics of\nsystems. Especially, weak layers that can potentially play significant role in\namplifying the vulnerability of multiplex networks might be shadowed in the\naggregated single-layer network framework which indiscriminately accumulates\nall layers. Here we present a simple model of cascading failure on multiplex\nnetworks of weight-heterogeneous layers. By simulating the model on the\nmultiplex network of international trades, we found that the multiplex model\nproduces more catastrophic cascading failures which are the result of emergent\ncollective effect of coupling layers, rather than the simple sum thereof.\nTherefore risks can be systematically underestimated in single-layer network\nanalyses because the impact of weak layers can be overlooked. We anticipate\nthat our simple theoretical study can contribute to further investigation and\ndesign of optimal risk-averse real-world complex systems. \n\n"}
{"id": "1603.05752", "contents": "Title: Optimal Response to Burstable Billing under Demand Uncertainty Abstract: Burstable billing is widely adopted in practice, e.g., by colocation data\ncenter providers, to charge for their users, e.g., data centers, for data\ntransferring. However, there is still a lack of research on what the best way\nis for a user to manage its workload in response to burstable billing. To\novercome this shortcoming, we propose a novel method to optimally respond to\nburstable billing under demand uncertainty. First, we develop a tractable\nmathematical expression to calculate the 95th percentile usage of a user, who\nis charged by provider via burstable billing for bandwidth usage. This model is\nthen used to formulate a new bandwidth allocation problem to maximize the\nuser's surplus, i.e., its net utility minus cost. Additionally, we examine\ndifferent non-convex solution methods for the formulated stochastic\noptimization problem. We also extend our design to the case where a user can\nreceive service from multiple providers, who all employ burstable billing.\nUsing real-world workload traces, we show that our proposed method can reduce\nuser's bandwidth cost by 26% and increase its total surplus by 23%, compared to\nthe current practice of allocating bandwidth on-demand. \n\n"}
{"id": "1603.07052", "contents": "Title: Cluster Content Caching: An Energy-Efficient Approach to Improve Quality\n  of Service in Cloud Radio Access Networks Abstract: In cloud radio access networks (C-RANs), a substantial amount of data must be\nexchanged in both backhaul and fronthaul links, which causes high power\nconsumption and poor quality of service (QoS) experience for real-time\nservices. To solve this problem, a cluster content caching structure is\nproposed in this paper, which takes full advantage of distributed caching and\ncentralized signal processing. In particular, redundant traffic on the backhaul\ncan be reduced because the cluster content cache provides a part of required\ncontent objects for remote radio heads (RRHs) connected to a common edge cloud.\nTractable expressions for both effective capacity and energy efficiency\nperformance are derived, which show that the proposed structure can improve QoS\nguarantees with a lower power cost of local storage. Furthermore, to fully\nexplore the potential of the proposed cluster content caching structure, the\njoint design of resource allocation and RRH association is optimized, and two\ndistributed algorithms are accordingly proposed. Simulation results verify the\naccuracy of the analytical results and show the performance gains achieved by\ncluster content caching in C-RANs. \n\n"}
{"id": "1603.07650", "contents": "Title: Decoding and File Transfer Delay Balancing in Network Coding Broadcast Abstract: Network Coding is a packet encoding technique which has recently been shown\nto improve network performance (by reducing delays and increasing throughput)\nin broadcast and multicast communications. The cost for such an improvement\ncomes in the form of increased decoding complexity (and thus delay) at the\nreceivers end. Before delivering the file to higher layers, the receiver should\nfirst decode those packets. In our work we consider the broadcast transmission\nof a large file to N wireless users. The file is segmented into a number of\nblocks (each containing K packets - the Coding Window Size). The packets of\neach block are encoded using Random Linear Network Coding (RLNC).We obtain the\nminimum coding window size so that the completion time of the file transmission\nis upper bounded by a used defined delay constraint. \n\n"}
{"id": "1603.08062", "contents": "Title: Optimal Traffic Aggregation in Multi-RAT Heterogeneous Wireless Networks Abstract: Traffic load balancing and radio resource management is key to harness the\ndense and increasingly heterogeneous deployment of next generation \"$5$G\"\nwireless infrastructure. Strategies for aggregating user traffic from across\nmultiple radio access technologies (RATs) and/or access points (APs) would be\ncrucial in such heterogeneous networks (HetNets), but are not well\ninvestigated. In this paper, we develop a low complexity solution for\nmaximizing an $\\alpha$-optimal network utility leveraging the multi-link\naggregation (simultaneous connectivity to multiple RATs/APs) capability of\nusers in the network. The network utility maximization formulation has\nmaximization of sum rate ($\\alpha=0$), maximization of minimum rate ($\\alpha\n\\to \\infty$), and proportional fair ($\\alpha=1$) as its special cases. A closed\nform is also developed for the special case where a user aggregates traffic\nfrom at most two APs/RATs, and hence can be applied to practical scenarios like\nLTE-WLAN aggregation (LWA) and LTE dual-connectivity solutions. It is shown\nthat the required objective may also be realized through a decentralized\nimplementation requiring a series of message exchanges between the users and\nnetwork. Using comprehensive system level simulations, it is shown that optimal\nleveraging of multi-link aggregation leads to substantial throughput gains over\nsingle RAT/AP selection techniques. \n\n"}
{"id": "1603.09537", "contents": "Title: Will 5G See its Blind Side? Evolving 5G for Universal Internet Access Abstract: Internet has shown itself to be a catalyst for economic growth and social\nequity but its potency is thwarted by the fact that the Internet is off limits\nfor the vast majority of human beings. Mobile phones---the fastest growing\ntechnology in the world that now reaches around 80\\% of humanity---can enable\nuniversal Internet access if it can resolve coverage problems that have\nhistorically plagued previous cellular architectures (2G, 3G, and 4G). These\nconventional architectures have not been able to sustain universal service\nprovisioning since these architectures depend on having enough users per cell\nfor their economic viability and thus are not well suited to rural areas (which\nare by definition sparsely populated). The new generation of mobile cellular\ntechnology (5G), currently in a formative phase and expected to be finalized\naround 2020, is aimed at orders of magnitude performance enhancement. 5G offers\na clean slate to network designers and can be molded into an architecture also\namenable to universal Internet provisioning. Keeping in mind the great social\nbenefits of democratizing Internet and connectivity, we believe that the time\nis ripe for emphasizing universal Internet provisioning as an important goal on\nthe 5G research agenda. In this paper, we investigate the opportunities and\nchallenges in utilizing 5G for global access to the Internet for all (GAIA). We\nhave also identified the major technical issues involved in a 5G-based GAIA\nsolution and have set up a future research agenda by defining open research\nproblems. \n\n"}
{"id": "1604.00074", "contents": "Title: Waveform Design for Wireless Power Transfer Abstract: Far-field Wireless Power Transfer (WPT) has attracted significant attention\nin recent years. Despite the rapid progress, the emphasis of the research\ncommunity in the last decade has remained largely concentrated on improving the\ndesign of energy harvester (so-called rectenna) and has left aside the effect\nof transmitter design. In this paper, we study the design of transmit waveform\nso as to enhance the DC power at the output of the rectenna. We derive a\ntractable model of the non-linearity of the rectenna and compare with a linear\nmodel conventionally used in the literature. We then use those models to design\nnovel multisine waveforms that are adaptive to the channel state information\n(CSI). Interestingly, while the linear model favours narrowband transmission\nwith all the power allocated to a single frequency, the non-linear model\nfavours a power allocation over multiple frequencies. Through realistic\nsimulations, waveforms designed based on the non-linear model are shown to\nprovide significant gains (in terms of harvested DC power) over those designed\nbased on the linear model and over non-adaptive waveforms. We also compute\nanalytically the theoretical scaling laws of the harvested energy for various\nwaveforms as a function of the number of sinewaves and transmit antennas. Those\nscaling laws highlight the benefits of CSI knowledge at the transmitter in WPT\nand of a WPT design based on a non-linear rectenna model over a linear model.\nResults also motivate the study of a promising architecture relying on\nlarge-scale multisine multi-antenna waveforms for WPT. As a final note, results\nstress the importance of modeling and accounting for the non-linearity of the\nrectenna in any system design involving wireless power. \n\n"}
{"id": "1604.00148", "contents": "Title: Market Integration in the Prewar Japanese Rice Markets Abstract: This paper examines the integration process of the Japanese major rice\nmarkets (Tokyo and Osaka) from 1881 to 1932. Using a non-Bayesian time-varying\nvector error correction model, we argue that the process strongly depended on\nthe government's policy on the network system of the telegram and telephone;\nrice traders with an intention to use modern communication tools were usually\naffected by the changes in policy. We find that (i) the Japanese rice markets\nhad been integrated in the 1910s; (ii) increasing use of telegraphs had\naccelerated rice market integration from the Meiji period in Japan; and (iii)\nlocal telephone system, which reduced the time spent by urban users sending and\nreceiving telegrams, promoted market integration. \n\n"}
{"id": "1604.00565", "contents": "Title: A Statistical Block Fading Channel Model for Multiuser Massive MIMO\n  System Abstract: This paper presents a statistical block fading channel model for multiuser\nmassive MIMO system. The proposed channel model is evolved from correlation\nbased stochastic channel model (CBSCM) but in addition to the properties of\nCBSCM, it has capability of capturing channel variations along time or\nfrequency and along space simultaneously. It has a simplified analytical\nexpression, still being able to simulate underlying physical phenomena which\notherwise need a complex geometry based stochastic channel model (GBSCM). The\nchannel model is verified with reported measurement data of channel for massive\nMIMO. Spatial determinism in channel, the basic cause of unfavorable\npropagation, is modeled into controlling parameters of channel model. Channel\nmodel uses only three controlling parameters; one parameter describes variation\nin channel along resource block (along time or frequency) and remaining two\nparameters describe spatial variation in channel. Modeling of simultaneous\nvariation along time and space belongs to a very common scenario where mobility\nof mobile terminal and angular power distribution at base station receiver, are\nkey parameters. Additionally, simulation results reveal the hidden advantages\nof spatial determinism in channel for multiuser massive MIMO. \n\n"}
{"id": "1604.00913", "contents": "Title: BLISP: Enhancing Backscatter Radio with Active Radio for Computational\n  RFIDs Abstract: We demonstrate the world's first hybrid radio platform which combines the\nstrengths of active radio (long range and robustness to interference) and\nComputational RFIDs (low power consumption). We evaluate the Wireless\nIdentification and Sensing Platform (WISP), an EPC C1G2 standard-based,\nComputational RFID backscatter radio, against Bluetooth Low Energy (BLE) and\nshow (theoretically and experimentally) that WISP in high channel attenuation\nconditions is less energy efficient per received byte than BLE. Exploiting this\nobservation we design a simple switching mechanisms that backs off to BLE when\nradio conditions for WISP are unfavorable. By a set of laboratory experiments,\nwe show that our proposed hybrid active/backscatter radio obtains higher\ngoodput than WISP and lower energy consumption than BLE as stand-alone\nplatforms, especially when WISP is in range of an RFID interrogator for the\nmajority of the time. Simultaneously, our proposed platform is as energy\nefficient as BLE when user is mostly out of RFID interrogator range. \n\n"}
{"id": "1604.01720", "contents": "Title: Reading Between the Pixels: Photographic Steganography for Camera\n  Display Messaging Abstract: We exploit human color metamers to send light-modulated messages less visible\nto the human eye, but recoverable by cameras. These messages are a key\ncomponent to camera-display messaging, such as handheld smartphones capturing\ninformation from electronic signage. Each color pixel in the display image is\nmodified by a particular color gradient vector. The challenge is to find the\ncolor gradient that maximizes camera response, while minimizing human response.\nThe mismatch in human spectral and camera sensitivity curves creates an\nopportunity for hidden messaging. Our approach does not require knowledge of\nthese sensitivity curves, instead we employ a data-driven method. We learn an\nellipsoidal partitioning of the six-dimensional space of colors and color\ngradients. This partitioning creates metamer sets defined by the base color at\nthe display pixel and the color gradient direction for message encoding. We\nsample from the resulting metamer sets to find color steps for each base color\nto embed a binary message into an arbitrary image with reduced visible\nartifacts. Unlike previous methods that rely on visually obtrusive intensity\nmodulation, we embed with color so that the message is more hidden. Ordinary\ndisplays and cameras are used without the need for expensive LEDs or high speed\ndevices. The primary contribution of this work is a framework to map the pixels\nin an arbitrary image to a metamer pair for steganographic photo messaging. \n\n"}
{"id": "1604.04009", "contents": "Title: System Design of Internet-of-Things for Residential Smart Grid Abstract: Internet-of-Things (IoTs) envisions to integrate, coordinate, communicate,\nand collaborate real-world objects in order to perform daily tasks in a more\nintelligent and efficient manner. To comprehend this vision, this paper studies\nthe design of a large scale IoT system for smart grid application, which\nconstitutes a large number of home users and has the requirement of fast\nresponse time. In particular, we focus on the messaging protocol of a universal\nIoT home gateway, where our cloud enabled system consists of a backend server,\nunified home gateway (UHG) at the end users, and user interface for mobile\ndevices. We discuss the features of such IoT system to support a large scale\ndeployment with a UHG and real-time residential smart grid applications. Based\non the requirements, we design an IoT system using the XMPP protocol, and\nimplemented in a testbed for energy management applications. To show the\neffectiveness of the designed testbed, we present some results using the\nproposed IoT architecture. \n\n"}
{"id": "1604.05622", "contents": "Title: Understanding Noise and Interference Regimes in 5G Millimeter-Wave\n  Cellular Networks Abstract: With the severe spectrum shortage in conventional cellular bands,\nmillimeter-wave (mmWave) frequencies have been attracting growing attention for\nnext-generation micro- and picocellular wireless networks. A fundamental and\nopen question is whether mmWave cellular networks are likely to be noise- or\ninterference-limited. Identifying in which regime a network is operating is\ncritical for the design of MAC and physical-layer procedures and to provide\ninsights on how transmissions across cells should be coordinated to cope with\ninterference. This work uses the latest measurement-based statistical channel\nmodels to accurately assess the Interference-to-Noise Ratio (INR) in a wide\nrange of deployment scenarios. In addition to cell density, we also study\nantenna array size and antenna patterns, whose effects are critical in the\nmmWave regime. The channel models also account for blockage, line-of-sight and\nnon-line-of-sight regimes as well as local scattering, that significantly\naffect the level of spatial isolation. \n\n"}
{"id": "1604.06284", "contents": "Title: The Impact of Services on Economic Complexity: Service Sophistication as\n  Route for Economic Growth Abstract: Economic complexity reflects the amount of knowledge that is embedded in the\nproductive structure of an economy. By combining tools from network science and\neconometrics, a robust and stable relationship between a country's productive\nstructure and its economic growth has been established. Here we report that not\nonly goods but also services are important for predicting the rate at which\ncountries will grow. By adopting a terminology which classifies manufactured\ngoods and delivered services as products, we investigate the influence of\nservices on the country's productive structure. In particular, we provide\nevidence that complexity indices for services are in general higher than those\nfor goods, which is reflected in a general tendency to rank countries with\ndeveloped service sector higher than countries with economy centred on\nmanufacturing of goods. By focusing on country dynamics based on experimental\ndata, we investigate the impact of services on the economic complexity of\ncountries measured in the product space (consisting of both goods and\nservices). Importantly, we show that diversification of service exports and its\nsophistication can provide an additional route for economic growth in both\ndeveloping and developed countries. \n\n"}
{"id": "1604.08618", "contents": "Title: Stringer: Balancing Latency and Resource Usage in Service Function Chain\n  Provisioning Abstract: Network Functions Virtualization, or NFV, enables telecommunications\ninfrastructure providers to replace special-purpose networking equipment with\ncommodity servers running virtualized network functions (VNFs). A service\nprovider utilizing NFV technology faces the SFC provisioning problem of\nassigning VNF instances to nodes in the physical infrastructure (e.g., a\ndatacenter), and routing Service Function Chains (sequences of functions\nrequired by customers, a.k.a. SFCs) in the physical network. In doing so, the\nprovider must balance between various competing goals of performance and\nresource usage. We present an approach for SFC provisioning, consisting of\nthree elements. The first element is a fast, scalable round-robin heuristic.\nThe second element is a Mixed Integer Programming (MIP) based approach. The\nthird element is a queueing-theoretic model to estimate the average latency\nassociated with any SFC provisioning solution. Combined, these elements create\nan approach that generates a set of SFC provisioning solutions, reflecting\ndifferent tradeoffs between resource usage and performance. \n\n"}
{"id": "1604.08937", "contents": "Title: User Selection and Power Allocation in Full Duplex Multi-Cell Networks Abstract: Full duplex (FD) communications has the potential to double the capacity of a\nhalf duplex (HD) system at the link level. However, in a cellular network, FD\noperation is not a straightforward extension of half duplex operations. The\nincreased interference due to a large number of simultaneous transmissions in\nFD operation and realtime traffic conditions limits the capacity improvement.\nRealizing the potential of FD requires careful coordination of resource\nallocation among the cells as well as within the cell. In this paper, we\npropose a distributed resource allocation, i.e., joint user selection and power\nallocation for a FD multi-cell system, assuming FD base stations (BSs) and HD\nuser equipment (UEs). Due to the complexity of finding the globally optimum\nsolution, a sub-optimal solution for UE selection, and a novel geometric\nprogramming based solution for power allocation, are proposed. The proposed\ndistributed approach converges quickly and performs almost as well as a\ncentralized solution, but with much lower signaling overhead. It provides a\nhybrid scheduling policy which allows FD operations whenever it is\nadvantageous, but otherwise defaults to HD operation. We focus on small cell\nsystems because they are more suitable for FD operation, given practical\nself-interference cancellation limits.With practical self-interference\ncancellation, it is shown that the proposed hybrid FD system achieves nearly\ntwo times throughput improvement for an indoor multi-cell scenario, and about\n65% improvement for an outdoor multi-cell scenario compared to the HD system. \n\n"}
{"id": "1605.00057", "contents": "Title: Distributed Cell Association for Energy Harvesting IoT Devices in Dense\n  Small Cell Networks: A Mean-Field Multi-Armed Bandit Approach Abstract: The emerging Internet of Things (IoT)-driven ultra-dense small cell networks\n(UD-SCNs) will need to combat a variety of challenges. On one hand, massive\nnumber of devices sharing the limited wireless resources will render\ncentralized control mechanisms infeasible due to the excessive cost of\ninformation acquisition and computations. On the other hand, to reduce energy\nconsumption from fixed power grid and/or battery, many IoT devices may need to\ndepend on the energy harvested from the ambient environment (e.g., from RF\ntransmissions, environmental sources). However, due to the opportunistic nature\nof energy harvesting, this will introduce uncertainty in the network operation.\nIn this article, we study the distributed cell association problem for energy\nharvesting IoT devices in UD-SCNs. After reviewing the state-of-the-art\nresearch on the cell association problem in small cell networks, we outline the\nmajor challenges for distributed cell association in IoT-driven UD-SCNs where\nthe IoT devices will need to perform cell association in a distributed manner\nin presence of uncertainty (e.g., limited knowledge on channel/network) and\nlimited computational capabilities. To this end, we propose an approach based\non mean-field multi-armed bandit games to solve the uplink cell association\nproblem for energy harvesting IoT devices in a UD-SCN. This approach is\nparticularly suitable to analyze large multi-agent systems under uncertainty\nand lack of information. We provide some theoretical results as well as\npreliminary performance evaluation results for the proposed approach. \n\n"}
{"id": "1605.00634", "contents": "Title: Why have asset price properties changed so little in 200 years Abstract: We first review empirical evidence that asset prices have had episodes of\nlarge fluctuations and been inefficient for at least 200 years. We briefly\nreview recent theoretical results as well as the neurological basis of trend\nfollowing and finally argue that these asset price properties can be attributed\nto two fundamental mechanisms that have not changed for many centuries: an\ninnate preference for trend following and the collective tendency to exploit as\nmuch as possible detectable price arbitrage, which leads to destabilizing\nfeedback loops. \n\n"}
{"id": "1605.00716", "contents": "Title: Radio Transformer Networks: Attention Models for Learning to Synchronize\n  in Wireless Systems Abstract: We introduce learned attention models into the radio machine learning domain\nfor the task of modulation recognition by leveraging spatial transformer\nnetworks and introducing new radio domain appropriate transformations. This\nattention model allows the network to learn a localization network capable of\nsynchronizing and normalizing a radio signal blindly with zero knowledge of the\nsignals structure based on optimization of the network for classification\naccuracy, sparse representation, and regularization. Using this architecture we\nare able to outperform our prior results in accuracy vs signal to noise ratio\nagainst an identical system without attention, however we believe such an\nattention model has implication far beyond the task of modulation recognition. \n\n"}
{"id": "1605.01557", "contents": "Title: On the Aloha throughput-fairness tradeoff Abstract: A well-known inner bound of the stability region of the slotted Aloha\nprotocol on the collision channel with n users assumes worst-case service rates\n(all user queues non-empty). Using this inner bound as a feasible set of\nachievable rates, a characterization of the throughput--fairness tradeoff over\nthis set is obtained, where throughput is defined as the sum of the individual\nuser rates, and two definitions of fairness are considered: the Jain-Chiu-Hawe\nfunction and the sum-user alpha-fair (isoelastic) utility function. This\ncharacterization is obtained using both an equality constraint and an\ninequality constraint on the throughput, and properties of the optimal\ncontrols, the optimal rates, and the fairness as a function of the target\nthroughput are established. A key fact used in all theorems is the observation\nthat all contention probability vectors that extremize the fairness functions\ntake at most two non-zero values. \n\n"}
{"id": "1605.01930", "contents": "Title: Context Information Based Initial Cell Search for Millimeter Wave 5G\n  Cellular Networks Abstract: Millimeter wave (mmWave) communication is envisioned as a cornerstone to\nfulfill the data rate requirements for fifth generation (5G) cellular networks.\nIn mmWave communication, beamforming is considered as a key technology to\ncombat the high path-loss, and unlike in conventional microwave communication,\nbeamforming may be necessary even during initial access/cell search. Among the\nproposed beamforming schemes for initial cell search, analog beamforming is a\npower efficient approach but suffers from its inherent search delay during\ninitial access. In this work, we argue that analog beamforming can still be a\nviable choice when context information about mmWave base stations (BS) is\navailable at the mobile station (MS). We then study how the performance of\nanalog beamforming degrades in case of angular errors in the available context\ninformation. Finally, we present an analog beamforming receiver architecture\nthat uses multiple arrays of Phase Shifters and a single RF chain to combat the\neffect of angular errors, showing that it can achieve the same performance as\nhybrid beamforming. \n\n"}
{"id": "1605.02238", "contents": "Title: Latency Analysis of Systems with Multiple Interfaces for Ultra-Reliable\n  M2M Communication Abstract: One of the ways to satisfy the requirements of ultra-reliable low latency\ncommunication for mission critical Machine-type Communications (MTC)\napplications is to integrate multiple communication interfaces. In order to\nestimate the performance in terms of latency and reliability of such an\nintegrated communication system, we propose an analysis framework that combines\ntraditional reliability models with technology-specific latency probability\ndistributions. In our proposed model we demonstrate how failure correlation\nbetween technologies can be taken into account. We show for the considered\nscenario with fiber and different cellular technologies how up to 5-nines\nreliability can be achieved and how packet splitting can be used to reduce\nlatency substantially while keeping 4-nines reliability. The model has been\nvalidated through simulation. \n\n"}
{"id": "1605.03772", "contents": "Title: SplitBox: Toward Efficient Private Network Function Virtualization Abstract: This paper presents SplitBox, a scalable system for privately processing\nnetwork functions that are outsourced as software processes to the cloud.\nSpecifically, providers processing the network functions do not learn the\nnetwork policies instructing how the functions are to be processed. We first\npropose an abstract model of a generic network function based on match-action\npairs, assuming that this is processed in a distributed manner by multiple\nhonest-but-curious providers. Then, we introduce our SplitBox system for\nprivate network function virtualization and present a proof-of-concept\nimplementation on FastClick -- an extension of the Click modular router --\nusing a firewall as a use case. Our experimental results show that SplitBox\nachieves a throughput of over 2 Gbps with 1 kB-sized packets on average,\ntraversing up to 60 firewall rules. \n\n"}
{"id": "1605.04602", "contents": "Title: Spectrum and Infrastructure Sharing in Millimeter Wave Cellular\n  Networks: An Economic Perspective Abstract: The licensing model for millimeter wave bands has been the subject of\nconsiderable debate, with some industry players advocating for unlicensed use\nand others for traditional geographic area exclusive use licenses. Meanwhile,\nthe massive bandwidth, highly directional antennas, high penetration loss and\nsusceptibility to shadowing in these bands suggest certain advantages to\nspectrum and infrastructure sharing. However, even when sharing is technically\nbeneficial (as recent research in this area suggests that it is), it may not be\nprofitable. In this paper, both the technical and economic implications of\nresource sharing in millimeter wave networks are studied. Millimeter wave\nservice is considered in the economic framework of a network good, where\nconsumers' utility depends on the size of the network, and the strategic\ndecisions of consumers and service providers are connected to detailed network\nsimulations. The results suggest that \"open\" deployments of neutral small cells\nthat serve subscribers of any service provider encourage market entry by making\nit easier for networks to reach critical mass, more than \"open\" (unlicensed)\nspectrum would. The conditions under which competitive service providers would\nprefer to share resources or not are also described. \n\n"}
{"id": "1605.04945", "contents": "Title: Extended nonlinear feedback model for describing episodes of high\n  inflation Abstract: An extension of the nonlinear feedback (NLF) formalism to describe regimes of\nhyper- and high-inflation in economy is proposed in the present work. In the\nNLF model the consumer price index (CPI) exhibits a finite time singularity of\nthe type $1/(t_c -t)^{(1- \\beta)/\\beta}$, with $\\beta>0$, predicting a blow up\nof the economy at a critical time $t_c$. However, this model fails in\ndetermining $t_c$ in the case of weak hyperinflation regimes like, e.g., that\noccurred in Israel. To overcome this trouble, the NLF model is extended by\nintroducing a parameter $\\gamma$, which multiplies all therms with past growth\nrate index (GRI). In this novel approach the solution for CPI is also analytic\nbeing proportional to the Gaussian hypergeometric function\n$_2F_1(1/\\beta,1/\\beta,1+1/\\beta;z)$, where $z$ is a function of $\\beta$,\n$\\gamma$, and $t_c$. For $z \\to 1$ this hypergeometric function diverges\nleading to a finite time singularity, from which a value of $t_c$ can be\ndetermined. This singularity is also present in GRI. It is shown that the\ninterplay between parameters $\\beta$ and $\\gamma$ may produce phenomena of\nmultiple equilibria. An analysis of the severe hyperinflation occurred in\nHungary proves that the novel model is robust. When this model is used for\nexamining data of Israel a reasonable $t_c$ is got. High-inflation regimes in\nMexico and Iceland, which exhibit weaker inflations than that of Israel, are\nalso successfully described. \n\n"}
{"id": "1605.05227", "contents": "Title: Revisiting XOR-based Network Coding for Energy Efficient Broadcasting in\n  Mobile Ad Hoc Networks Abstract: Network coding is commonly used to improve the energy efficiency of\nnetwork-wide broadcasting in wireless multi-hop networks. In this work, we\nfocus on XOR-based broadcasting in mobile ad hoc networks with multiple\nsources. We make the observation that the common approach, which is to benefit\nfrom the synergy of XOR network coding with a CDS-based broadcast algorithm,\nsuffers performance breakdowns. After delving into the details of this synergy,\nwe attribute this behavior to an important mechanism of the underlying\nbroadcast algorithm, known as the \"termination criterion\". To tackle the\nproblem, we propose a termination criterion that is fully compatible with XOR\ncoding. In addition to that, we revisit the internals of XOR coding. We first\nenhance the synergy of XOR coding with the underlying broadcast algorithm by\nallowing each mechanism to benefit from information available by the other. In\nthis way, we manage to improve the pruning efficiency of the CDS-based\nalgorithm while at the same time we come up with a method for detecting coding\nopportunities that has minimal storage and processing requirements compared to\ncurrent approaches. Then, for the first time, we use XOR coding as a mechanism\nnot only for enhancing energy efficiency but also for reducing the\nend-to-end-delay. We validate the effectiveness of our proposed algorithm\nthrough extensive simulations on a diverse set of scenarios. \n\n"}
{"id": "1605.05614", "contents": "Title: Slotless Protocols for Fast and Energy-Efficient Neighbor Discovery Abstract: In mobile ad-hoc networks, neighbor discovery protocols are used to find\nsurrounding devices and to establish a first contact between them. Since the\nclocks of the devices are not synchronized and their energy-budgets are\nlimited, usually duty-cycled, asynchronous discovery protocols are applied.\nOnly if two devices are awake at the same point in time, they can rendezvous.\nCurrently, time-slotted protocols, which subdivide time into multiple intervals\nwith equal lengths (slots), are considered to be the most efficient discovery\nschemes. In this paper, we break away from the assumption of slotted time. We\npropose a novel, continuous-time discovery protocol, which temporally decouples\nbeaconing and listening. Each device periodically sends packets with a certain\ninterval, and periodically listens for a given duration with a different\ninterval. By optimizing these interval lengths, we show that this scheme can,\nto the best of our knowledge, outperform all known protocols such as DISCO,\nU-Connect or Searchlight significantly. For example, Searchlight takes up to\n740 % longer than our proposed technique to discover a device with the same\nduty-cycle. Further, our proposed technique can also be applied in widely-used\nasymmetric purely interval-based protocols such as ANT or Bluetooth Low Energy. \n\n"}
{"id": "1605.06467", "contents": "Title: Browser Feature Usage on the Modern Web Abstract: Modern web browsers are incredibly complex, with millions of lines of code\nand over one thousand JavaScript functions and properties available to website\nauthors. This work investigates how these browser features are used on the\nmodern, open web. We find that JavaScript features differ wildly in popularity,\nwith over 50% of provided features never used in the Alexa 10k.\n  We also look at how popular ad and tracking blockers change the distribution\nof features used by sites, and identify a set of approximately 10% of features\nthat are disproportionately blocked (prevented from executing by these\nextensions at least 90% of the time they are used). We additionally find that\nin the presence of these blockers, over 83% of available features are executed\non less than 1% of the most popular 10,000 websites.\n  We additionally measure a variety of aspects of browser feature usage on the\nweb, including how complex sites have become in terms of feature usage, how the\nlength of time a browser feature has been in the browser relates to its usage\non the web, and how many security vulnerabilities have been associated with\nrelated browser features. \n\n"}
{"id": "1605.06884", "contents": "Title: Mobile Cloud Computing with a UAV-Mounted Cloudlet: Optimal Bit\n  Allocation for Communication and Computation Abstract: Mobile cloud computing relieves the tension between compute-intensive mobile\napplications and battery-constrained mobile devices by enabling the offloading\nof computing tasks from mobiles to a remote processors. This paper considers a\nmobile cloud computing scenario in which the \"cloudlet\" processor that provides\noffloading opportunities to mobile devices is mounted on unmanned aerial\nvehicles (UAVs) to enhance coverage. Focusing on a slotted communication system\nwith frequency division multiplexing between mobile and UAV, the joint\noptimization of the number of input bits transmitted in the uplink by the\nmobile to the UAV, the number of input bits processed by the cloudlet at the\nUAV, and the number of output bits returned by the cloudlet to the mobile in\nthe downlink in each slot is carried out by means of dual decomposition under\nmaximum latency constraints with the aim of minimizing the mobile energy\nconsumption. Numerical results reveal the critical importance of an optimized\nbit allocation in order to enable significant energy savings as compared to\nlocal mobile execution for stringent latency constraints. \n\n"}
{"id": "1605.09002", "contents": "Title: Coded Caching Clusters with Device-to-Device Communications Abstract: We consider a geographically constrained caching community where popular data\nfiles are cached on mobile terminals and distributed through Device-to-Device\n(D2D) communications. Further, to ensure availability, data files are protected\nagainst user mobility, or churn, with erasure coding. Communication and storage\ncosts (in units of energy) are considered. We focus on finding the coding\nmethod that minimizes the overall cost in the network. Closed-form expressions\nfor the expected energy consumption incurred by data delivery and redundancy\nmaintenance are derived, and it is shown that coding significantly decreases\nthe overall energy consumption -- by more than 90% in a realistic scenario. It\nis further shown that D2D caching can also yield notable economical savings for\ntelecommunication operators. Our results are illustrated by numerical examples\nand verified by extensive computer simulations. \n\n"}
{"id": "1605.09011", "contents": "Title: A Self-Managed Architecture for Sensor Networks Based on Real Time Data\n  Analysis Abstract: Wireless sensor networks (WSNs) have been adopted as merely data producers\nfor years. However, the data collected by WSNs can also be used to manage their\noperation and avoid unnecessary measurements that do not provide any new\nknowledge about the environment. The benefits are twofold because wireless\nsensor nodes may save their limited energy resources and also reduce the\nwireless medium occupancy. We present a self-managed platform that collects and\nstores data from sensor nodes, analyzes its contents and uses the built\nknowledge to adjust the operation of the entire network. The system\narchitecture facilitates the incorporation of traditional WSNs into the\nInternet of Things by abstracting the lower communication layers and allowing\ndecisions based on the data relevance. Finally, we demonstrate the platform\noptimizing a WSN's operation at runtime, based on different real-time data\nanalysis. \n\n"}
{"id": "1605.09350", "contents": "Title: Computing backup forwarding rules in Software-Defined Networks Abstract: The past century of telecommunications has shown that failures in networks\nare prevalent. Although much has been done to prevent failures, network nodes\nand links are bound to fail eventually. Failure recovery processes are\ntherefore needed. Failure recovery is mainly influenced by (1) detection of the\nfailure, and (2) circumvention of the detected failure. However, especially in\nSDNs where controllers recompute network state reactively, this leads to high\ndelays. Hence, next to primary rules, backup rules should be installed in the\nswitches to quickly detour traffic once a failure occurs. In this work, we\npropose algorithms for computing an all-to-all primary and backup network\nforwarding configuration that is capable of circumventing link and node\nfailures. Omitting the high delay invoked by controller recomputation through\npreconfiguration, our proposal's recovery delay is close to the detection time\nwhich is significantly below the 50 ms rule of thumb. After initial recovery,\nwe recompute network configuration to guarantee protection from future\nfailures. Our algorithms use packet-labeling to guarantee correct and shortest\ndetour forwarding. The algorithms and labeling technique allow packets to\nreturn to the primary path and are able to discriminate between link and node\nfailures. The computational complexity of our solution is comparable to that of\nall-to-all-shortest paths computations. Our experimental evaluation on both\nreal and generated networks shows that network configuration complexity highly\ndecreases compared to classic disjoint paths computations. Finally, we provide\na proof-of-concept OpenFlow controller in which our proposed configuration is\nimplemented, demonstrating that it readily can be applied in production\nnetworks. \n\n"}
{"id": "1606.00202", "contents": "Title: OWL: a Reliable Online Watcher for LTE Control Channel Measurements Abstract: Reliable network measurements are a fundamental component of networking\nresearch as they enable network analysis, system debugging, performance\nevaluation and optimization. In particular, decoding the LTE control channel\nwould give access to the full base station traffic at a 1 ms granularity, thus\nallowing for traffic profiling and accurate measurements. Although a few\nopen-source implementations of LTE are available, they do not provide tools to\nreliably decoding the LTE control channel and, thus, accessing the scheduling\ninformation. In this paper, we present OWL, an Online Watcher for LTE that is\nable to decode all the resource blocks in more than 99% of the system frames,\nsignificantly outperforming existing non-commercial prior decoders. Compared to\nprevious attempts, OWL grounds the decoding procedure on information obtained\nfrom the LTE random access mechanism. This makes it possible to run our\nsoftware on inexpensive hardware coupled with almost any software defined radio\ncapable of sampling the LTE signal with sufficient accuracy. \n\n"}
{"id": "1606.01581", "contents": "Title: Big Data Caching for Networking: Moving from Cloud to Edge Abstract: In order to cope with the relentless data tsunami in $5G$ wireless networks,\ncurrent approaches such as acquiring new spectrum, deploying more base stations\n(BSs) and increasing nodes in mobile packet core networks are becoming\nineffective in terms of scalability, cost and flexibility. In this regard,\ncontext-aware $5$G networks with edge/cloud computing and exploitation of\n\\emph{big data} analytics can yield significant gains to mobile operators. In\nthis article, proactive content caching in $5$G wireless networks is\ninvestigated in which a big data-enabled architecture is proposed. In this\npractical architecture, vast amount of data is harnessed for content popularity\nestimation and strategic contents are cached at the BSs to achieve higher\nusers' satisfaction and backhaul offloading. To validate the proposed solution,\nwe consider a real-world case study where several hours of mobile data traffic\nis collected from a major telecom operator in Turkey and a big data-enabled\nanalysis is carried out leveraging tools from machine learning. Based on the\navailable information and storage capacity, numerical studies show that several\ngains are achieved both in terms of users' satisfaction and backhaul\noffloading. For example, in the case of $16$ BSs with $30\\%$ of content ratings\nand $13$ Gbyte of storage size ($78\\%$ of total library size), proactive\ncaching yields $100\\%$ of users' satisfaction and offloads $98\\%$ of the\nbackhaul. \n\n"}
{"id": "1606.01833", "contents": "Title: Analyzing Distributed Join-Idle-Queue: A Fluid Limit Approach Abstract: In the context of load balancing, Lu et al. introduced the distributed\nJoin-Idle-Queue algorithm, where a group of dispatchers distribute jobs to a\ncluster of parallel servers. Each dispatcher maintains a queue of idle servers;\nwhen a job arrives to a dispatcher, it sends it to a server on its queue, or to\na random server if the queue is empty. In turn, when a server has no jobs, it\nrequests to be placed on the idle queue of a randomly chosen dispatcher.\n  Although this algorithm was shown to be quite effective, the original\nasymptotic analysis makes simplifying assumptions that become increasingly\ninaccurate as the system load increases. Further, the analysis does not\nnaturally generalize to interesting variations, such as having a server request\nto be placed on the idle queue of a dispatcher before it has completed all\njobs, which can be beneficial under high loads.\n  We provide a new asymptotic analysis of Join-Idle-Queue systems based on mean\nfield fluid limit methods, deriving families of differential equations that\ndescribe these systems. Our analysis avoids previous simplifying assumptions,\nis empirically more accurate, and generalizes naturally to the variation\ndescribed above, as well as other simple variations. Our theoretical and\nempirical analyses shed further light on the performance of Join-Idle-Queue,\nincluding potential performance pitfalls under high load. \n\n"}
{"id": "1606.04202", "contents": "Title: Improved Approximation of Storage-Rate Tradeoff for Caching with\n  Multiple Demands Abstract: Caching at the network edge has emerged as a viable solution for alleviating\nthe severe capacity crunch in modern content centric wireless networks by\nleveraging network load-balancing in the form of localized content storage and\ndelivery. In this work, we consider a cache-aided network where the cache\nstorage phase is assisted by a central server and users can demand multiple\nfiles at each transmission interval. To service these demands, we consider two\ndelivery models - $(1)$ centralized content delivery where user demands at each\ntransmission interval are serviced by the central server via multicast\ntransmissions; and $(2)$ device-to-device (D2D) assisted distributed delivery\nwhere users multicast to each other in order to service file demands. For such\ncache-aided networks, we present new results on the fundamental cache storage\nvs. transmission rate tradeoff. Specifically, we develop a new technique for\ncharacterizing information theoretic lower bounds on the storage-rate tradeoff\nand show that the new lower bounds are strictly tighter than cut-set bounds\nfrom literature. Furthermore, using the new lower bounds, we establish the\noptimal storage-rate tradeoff to within a constant multiplicative gap. We show\nthat, for multiple demands per user, achievable schemes based on repetition of\nschemes for single demands are order-optimal under both delivery models. \n\n"}
{"id": "1606.04236", "contents": "Title: Context-Aware Proactive Content Caching with Service Differentiation in\n  Wireless Networks Abstract: Content caching in small base stations or wireless infostations is considered\nto be a suitable approach to improve the efficiency in wireless content\ndelivery. Placing the optimal content into local caches is crucial due to\nstorage limitations, but it requires knowledge about the content popularity\ndistribution, which is often not available in advance. Moreover, local content\npopularity is subject to fluctuations since mobile users with different\ninterests connect to the caching entity over time. Which content a user prefers\nmay depend on the user's context. In this paper, we propose a novel algorithm\nfor context-aware proactive caching. The algorithm learns context-specific\ncontent popularity online by regularly observing context information of\nconnected users, updating the cache content and observing cache hits\nsubsequently. We derive a sublinear regret bound, which characterizes the\nlearning speed and proves that our algorithm converges to the optimal cache\ncontent placement strategy in terms of maximizing the number of cache hits.\nFurthermore, our algorithm supports service differentiation by allowing\noperators of caching entities to prioritize customer groups. Our numerical\nresults confirm that our algorithm outperforms state-of-the-art algorithms in a\nreal world data set, with an increase in the number of cache hits of at least\n14%. \n\n"}
{"id": "1606.04405", "contents": "Title: Fundamentals of Modeling Finite Wireless Networks using Binomial Point\n  Process Abstract: Modeling the locations of nodes as a uniform binomial point process (BPP), we\npresent a generic mathematical framework to characterize the performance of an\narbitrarily-located reference receiver in a finite wireless network. Different\nfrom most of the prior works where the serving transmitter (TX) node is located\nat the fixed distance from the reference receiver, we consider two general\nTX-selection policies: i) uniform TX-selection: the serving node is chosen\nuniformly at random amongst transmitting nodes, and ii) k-closest TX-selection:\nthe serving node is the k-th closest node out of transmitting nodes to the\nreference receiver. The key intermediate step in our analysis is the derivation\nof a new set of distance distributions that lead not only to the tractable\nanalysis of coverage probability but also enable the analyses of wide range of\nclassical and currently trending problems in wireless networks. Using this new\nset of distance distributions, we first investigate the diversity loss due to\nSIR correlation in a finite network. We then obtain the optimal number of links\nthat can be simultaneously activated to maximize network spectral efficiency.\nFinally, we evaluate optimal caching probability to maximize the total hit\nprobability in cache-enabled finite networks. \n\n"}
{"id": "1606.05391", "contents": "Title: Technique Report: Scheduling Flows with Multiple Service Frequency\n  Constraints Abstract: With the fast development of wireless technologies, wireless applications\nhave invaded various areas in people's lives with a wide range of capabilities.\nGuaranteeing Quality-of-Service (QoS) is the key to the success of those\napplications. One of the QoS requirements, service frequency, is very important\nfor tasks including multimedia transmission in the Internet of Things. A\nservice frequency constraint denotes the length of time period during which a\nlink can transmit at least once. Unfortunately, it has not been well addressed\nyet. Therefore, this paper proposes a new framework to schedule multi\ntransmitting flows in wireless networks considering service frequency\nconstraint for each link. In our model, the constraints for flows are\nheterogeneous due to the diversity of users' behaviors. We first introduce a\nnew definition for network stability with service frequency constraints and\ndemonstrate that the novel scheduling policy is throughput-optimal in one\nfundamental category of network models. After that, we discuss the performance\nof a wireless network with service frequency constraints from the views of\ncapacity region and total queue length. Finally, a series of evaluations\nindicate the proposed scheduling policy can guarantee service frequency and\nachieve a good performance on the aspect of queue length of each flow. \n\n"}
{"id": "1606.05768", "contents": "Title: Network-Coded Macrocell Offloading in Femtocaching-Assisted Cellular\n  Networks Abstract: The femtocaching idea was proposed as a solution to compensate for the weak\nbackhaul capacity, by deploying coverage-limited nodes with high storage\ncapacity called femtocaches (FCs). In this paper, the macrocell offloading\nproblem in femtocaching-assisted cellular networks is investigated. The\nobjective is to minimize the number of transmissions by the macrocell base\nstation (MBS) given that all requests should be served simultaneously to\nsatisfy quality-of-experience (QoE) of the clients. We first formulate this MBS\noffloading problem as an optimization problem over a network coding graph, and\nshow that it is NP-hard. Therefore, we propose an ONC-broadcast offloading\nscheme that exploits both broadcasting and opportunistic network coding (ONC)\nto minimize the number of required MBS transmissions. We utilize a random graph\nmodel to approximate the performance of the proposed ONC-broadcast scheme in\nterms of the resultant average number of transmissions by the MBS. Moreover,\ndespite the complexity of finding the optimal solution for each and every case,\nwe prove that this ONC-broadcast scheme is asymptotically optimal, i.e., for\nlarge number of requests, the ONC-broadcast scheme achieves a similar macrocell\noffloading performance to that of the optimal solution. To implement the\nONC-broadcast scheme, we devise a heuristic that employs a dual conflict graph\nor broadcasting at the FCs such that the remaining requests can be served using\nthe minimum number of transmissions at the MBS. Simulations show that the dual\ngraph scheme improves MBS offloading as compared to the traditional separate\ngraph scheme. Furthermore, the simple heuristic proposed to implement the\nONC-broadcast scheme achieves a very close performance to the optimal\nONC-broadcast scheme. \n\n"}
{"id": "1606.06051", "contents": "Title: Physicists' approach to studying socio-economic inequalities: Can humans\n  be modelled as atoms? Abstract: A brief overview of the models and data analyses of income, wealth,\nconsumption distributions by the physicists, are presented here. It has been\nfound empirically that the distributions of income and wealth possess fairly\nrobust features, like the bulk of both the income and wealth distributions seem\nto reasonably fit both the log-normal and Gamma distributions, while the tail\nof the distribution fits well to a power law (as first observed by sociologist\nPareto). We also present our recent studies of the unit-level expenditure on\nconsumption across multiple countries and multiple years, where it was found\nthat there exist invariant features of consumption distribution: the bulk is\nlog-normally distributed, followed by a power law tail at the limit. The\nmechanisms leading to such inequalities and invariant features for the\ndistributions of socio-economic variables are not well-understood. We also\npresent some simple models from physics and demonstrate how they can be used to\nexplain some of these findings and their consequences. \n\n"}
{"id": "1606.06692", "contents": "Title: A Delay Optimal MAC and Packet Scheduler for Heterogeneous M2M Uplink Abstract: The uplink data arriving at the Machine-to-Machine (M2M) Application Server\n(AS) via M2M Aggregators (MAs) is fairly heterogeneous along several dimensions\nsuch as maximum tolerable packet delay, payload size and arrival rate, thus\nnecessitating the design of Quality-of-Service (QoS) aware packet scheduler. In\nthis paper, we classify the M2M uplink data into multiple QoS classes and use\nsigmoidal function to map the delay requirements of each class onto utility\nfunctions. We propose a proportionally fair delay-optimal multiclass packet\nscheduler at AS that maximizes a system utility metric. We note that the\naverage class delay under any work-conserving scheduling policy can be realized\nby appropriately time-sharing between all possible preemptive priority\npolicies. Therefore the optimal scheduler is determined using an iterative\nprocess to determine the optimal time-sharing between all priority scheduling\npolicies, such that it results in maximum system utility. The proposed\nscheduler can be implemented online with reduced complexity due to the\niterative optimization process. We then extend this work to determine jointly\noptimal MA-AS channel allocation and packet scheduling scheme at the MAs and\nAS. We first formulate a joint optimization problem that is solved centrally at\nthe AS and then propose a low complexity distributed optimization problem\nsolved independently at MAs and AS. We show that the distributed optimization\nsolution converges quickly to the centralized optimization result with minimal\ninformation exchange overhead between MAs and AS. Using Monte-Carlo\nsimulations, we verify the optimality of the proposed scheduler and show that\nit outperforms other state-of-the-art packet schedulers such as weighted round\nrobin, max-weight scheduler etc. Another desirable feature of proposed\nscheduler is low delay jitter for delay-sensitive traffic. \n\n"}
{"id": "1606.07079", "contents": "Title: SICS: Secure In-Cloud Service Function Chaining Abstract: There is an increasing trend that enterprises outsource their network\nfunctions to the cloud for lower cost and ease of management. However, network\nfunction outsourcing brings threats to the privacy of enterprises since the\ncloud is able to access the traffic and rules of in-cloud network functions.\nCurrent tools for secure network function outsourcing either incur large\nperformance overhead or do not support real-time updates. In this paper, we\npresent SICS, a secure service function chain outsourcing framework. SICS\nencrypts each packet header and use a label for in-cloud rule matching, which\nenables the cloud to perform its functionalities correctly with minimum header\ninformation leakage. Evaluation results show that SICS achieves higher\nthroughput, faster construction and update speed, and lower resource overhead\nat both enterprise and cloud sides, compared to existing solutions. \n\n"}
{"id": "1606.07360", "contents": "Title: Low Power Wide Area Networks: An Overview Abstract: Low Power Wide Area (LPWA) networks are attracting a lot of attention\nprimarily because of their ability to offer affordable connectivity to the\nlow-power devices distributed over very large geographical areas. In realizing\nthe vision of the Internet of Things (IoT), LPWA technologies complement and\nsometimes supersede the conventional cellular and short range wireless\ntechnologies in performance for various emerging smart city and\nmachine-to-machine (M2M) applications. This review paper presents the design\ngoals and the techniques, which different LPWA technologies exploit to offer\nwide-area coverage to low-power devices at the expense of low data rates. We\nsurvey several emerging LPWA technologies and the standardization activities\ncarried out by different standards development organizations (e.g., IEEE, IETF,\n3GPP, ETSI) as well as the industrial consortia built around individual LPWA\ntechnologies (e.g., LORa Alliance,WEIGHTLESS-SIG, and DASH7 Alliance). We\nfurther note that LPWA technologies adopt similar approaches, thus sharing\nsimilar limitations and challenges. This paper expands on these research\nchallenges and identifies potential directions to address them. While the\nproprietary LPWA technologies are already hitting the market with large\nnationwide roll-outs, this paper encourages an active engagement of the\nresearch community in solving problems that will shape the connectivity of tens\nof billions of devices in the next decade. \n\n"}
{"id": "1606.07613", "contents": "Title: Carrier-Grade Anomaly Detection Using Time-to-Live Header Information Abstract: Time-to-Live data in the IP header offers two interesting characteristics:\nFirst, different IP stacks pick different start TTL values. Second, each\ntraversed router should decrement the TTL value. The combination of both offers\nhost and route fingerprinting options. We present the first work to investigate\nInternet-wide TTL behavior at carrier scale and evaluate its fit to detect\nanomalies, predominantly spoofed source IP addresses. Using purpose-built\nsoftware, we capture 2 weeks of raw TTL data at a 40 Gbit/s Internet uplink.\nFor further insight, we actively measure observed hosts and conduct large-scale\nhitlist-based measurements, which yields three complementary data sets for IPv4\nand IPv6. A majority (69% IPv4; 81% IPv6) of passively observed multi-packet\nhosts exhibit one stable TTL value. Active measurements on unstable hosts yield\na stable anchor TTL value for more than 85% of responsive hosts. We develop a\nstructure to further classify unstable hosts taking, for example, temporal\nstability into account. Correlation of TTL values with BGP data is clear, yet\nunpredictive. The results indicate that carrier-grade TTL anomaly detection can\nyield significant insights in the following categories: First, the method can\nflag anomalies based on TTL observations (yet likely at a difficult false\npositive/false negative trade-off). Second, the method can establish trust that\na packet originates from its acclaimed source. \n\n"}
{"id": "1606.08562", "contents": "Title: Complex Systems and a Computational Social Science Perspective on the\n  Labor Market Abstract: Labor market institutions are central for modern economies, and their polices\ncan directly affect unemployment rates and economic growth. At the individual\nlevel, unemployment often has a detrimental impact on people's well-being and\nhealth. At the national level, high employment is one of the central goals of\nany economic policy, due to its close association with national prosperity. The\nmain goal of this thesis is to highlight the need for frameworks that take into\naccount the complex structure of labor market interactions. In particular, we\nexplore the benefits of leveraging tools from computational social science,\nnetwork science, and data-driven theories to measure the flow of opportunities\nand information in the context of the labor market. First, we investigate our\nkey hypothesis, which is that opportunity/information flow through weak ties,\nand this is a key determinant of the length of unemployment. We then extend the\nidea of opportunity/information flow to clusters of other economic activities,\nwhere we expect the flow within clusters of related activities to be higher\nthan within isolated activities. This captures the intuition that within\nrelated activities there are more \"capitals\" involved and that such activities\nrequire similar \"capabilities.\" Therefore, more extensive clusters of economic\nactivities should generate greater growth through exploiting the greater flow\nof opportunities and information. We quantify the opportunity/information flow\nusing a complexity measure of two economic activities (i.e. jobs and exports). \n\n"}
{"id": "1607.00500", "contents": "Title: Spatio-Temporal Network Dynamics Framework for Energy-Efficient\n  Ultra-Dense Cellular Networks Abstract: This article investigates the performance of an ultra-dense network (UDN)\nfrom an energy-efficiency (EE) standpoint leveraging the interplay between\nstochastic geometry (SG) and mean-field game (MFG) theory. In this setting,\nbase stations (BSs) (resp. users) are uniformly distributed over a\ntwo-dimensional plane as two independent homogeneous Poisson point processes\n(PPPs), where users associate to their nearest BSs. The goal of every BS is to\nmaximize its own energy efficiency subject to channel uncertainty, random BS\nlocation, and interference levels. Due to the coupling in interference, the\nproblem is solved in the mean-field (MF) regime where each BS interacts with\nthe whole BS population via time-varying MF interference. As a main\ncontribution, the asymptotic convergence of MF interference to zero is\nrigorously proved in a UDN with multiple transmit antennas. It allows us to\nderive a closed-form EE representation, yielding a tractable EE optimal power\ncontrol policy. This proposed power control achieves more than 1.5 times higher\nEE compared to a fixed power baseline. \n\n"}
{"id": "1607.02481", "contents": "Title: Inferring monopartite projections of bipartite networks: an\n  entropy-based approach Abstract: Bipartite networks are currently regarded as providing a major insight into\nthe organization of many real-world systems, unveiling the mechanisms driving\nthe interactions occurring between distinct groups of nodes. One of the most\nimportant issues encountered when modeling bipartite networks is devising a way\nto obtain a (monopartite) projection on the layer of interest, which preserves\nas much as possible the information encoded into the original bipartite\nstructure. In the present paper we propose an algorithm to obtain\nstatistically-validated projections of bipartite networks, according to which\nany two nodes sharing a statistically-significant number of neighbors are\nlinked. Since assessing the statistical significance of nodes similarity\nrequires a proper statistical benchmark, here we consider a set of four null\nmodels, defined within the exponential random graph framework. Our algorithm\noutputs a matrix of link-specific p-values, from which a validated projection\nis straightforwardly obtainable, upon running a multiple hypothesis testing\nprocedure. Finally, we test our method on an economic network (i.e. the\ncountries-products World Trade Web representation) and a social network (i.e.\nMovieLens, collecting the users' ratings of a list of movies). In both cases\nnon-trivial communities are detected: while projecting the World Trade Web on\nthe countries layer reveals modules of similarly-industrialized nations,\nprojecting it on the products layer allows communities characterized by an\nincreasing level of complexity to be detected; in the second case, projecting\nMovieLens on the films layer allows clusters of movies whose affinity cannot be\nfully accounted for by genre similarity to be individuated. \n\n"}
{"id": "1607.03205", "contents": "Title: Stock Market Market Crash of 2008: an empirical study of the deviation\n  of share prices from company fundamentals Abstract: The aim of this study is to investigate quantitatively whether share prices\ndeviated from company fundamentals in the stock market crash of 2008. For this\npurpose, we use a large database containing the balance sheets and share prices\nof 7,796 worldwide companies for the period 2004 through 2013. We develop a\npanel regression model using three financial indicators--dividends per share,\ncash flow per share, and book value per share--as explanatory variables for\nshare price. We then estimate individual company fundamentals for each year by\nremoving the time fixed effects from the two-way fixed effects model, which we\nidentified as the best of the panel regression models. One merit of our model\nis that we are able to extract unobservable factors of company fundamentals by\nusing the individual fixed effects.\n  Based on these results, we analyze the market anomaly quantitatively using\nthe divergence rate--the rate of the deviation of share price from a company's\nfundamentals. We find that share prices on average were overvalued in the\nperiod from 2005 to 2007, and were undervalued significantly in 2008, when the\nglobal financial crisis occurred. Share prices were equivalent to the\nfundamentals on average in the subsequent period. Our empirical results clearly\ndemonstrate that the worldwide stock market fluctuated excessively in the time\nperiod before and just after the global financial crisis of 2008. \n\n"}
{"id": "1607.03607", "contents": "Title: Cloud Empowered Self-Managing WSNs Abstract: Wireless Sensor Networks (WSNs) are composed of low powered and\nresource-constrained wireless sensor nodes that are not capable of performing\nhigh-complexity algorithms. Integrating these networks into the Internet of\nThings (IoT) facilitates their real-time optimization based on remote data\nvisualization and analysis. This work describes the design and implementation\nof a scalable system architecture that integrates WSNs and cloud services to\nwork autonomously in an IoT environment. The implementation relies on Software\nDefined Networking features to simplify the WSN management and exploits data\nanalytics tools to execute a reinforcement learning algorithm that takes\ndecisions based on the environment's evolution. It can automatically configure\nwireless sensor nodes to measure and transmit the temperature only at periods\nwhen the environment changes more often. Without any human intervention, the\nsystem could reduce nearly 85% the number of transmissions, showing the\npotential of this mechanism to extend WSNs lifetime without compromising the\ndata quality. Besides attending to similar use cases, such a WSN autonomic\nmanagement could promote a new business model to offer sensing tasks as a\nservice, which is also introduced in this work. \n\n"}
{"id": "1607.05543", "contents": "Title: Decentralized Opportunistic Access for D2D Underlaid Cellular Networks Abstract: We propose a decentralized access control scheme for interference management\nin D2D (device-to-device) underlaid cellular networks. Our method combines\nSIR-aware link activation with cellular exclusion regions in a case where D2D\nlinks opportunistically access the licensed cellular spectrum. Analytical\nexpressions and tight approximations for the coverage probabilities of cellular\nand D2D links are derived. We characterize the impact of the guard zone radius\nand the SIR threshold on the D2D area spectral efficiency and cellular\ncoverage. A tractable approach was proposed in order to find the SIR threshold\nand guard zone radius, which maximize the area spectral efficiency of the D2D\ncommunication while ensuring sufficient coverage probability for cellular\nuplink users. Simulations validate the accuracy of our analytical results and\nshow the performance gain of our proposed scheme compared to existing\nstate-of-the-art solutions. \n\n"}
{"id": "1608.00726", "contents": "Title: Infinite Unlimited Churn Abstract: We study unlimited infinite churn in peer-to-peer overlay networks. Under\nthis churn, arbitrary many peers may concurrently request to join or leave the\noverlay network; moreover these requests may never stop coming. We prove that\nunlimited adversarial churn, where processes may just exit the overlay network,\nis unsolvable. We focus on cooperative churn where exiting processes\nparticipate in the churn handling algorithm. We define the problem of unlimited\ninfinite churn in this setting. We distinguish the fair version of the problem,\nwhere each request is eventually satisfied, from the unfair version that just\nguarantees progress. We focus on local solutions to the problem, and prove that\na local solution to the Fair Infinite Unlimited Churn is impossible. We then\npresent and prove correct an algorithm UIUC that solves the Unfair Infinite\nUnlimited Churn Problem for a linearized peer-to-peer overlay network. We\nextend this solution to skip lists and skip graphs. \n\n"}
{"id": "1608.02032", "contents": "Title: Unique coverage in Boolean models Abstract: Consider a wireless cellular network consisting of small, densely scattered\nbase stations. A user $u$ is {\\em uniquely covered} by a base station $b$ if\n$u$ is the only user within distance $r$ of $b$. This makes it possible to\nassign the user $u$ to the base station $b$ without interference from any other\nuser $u'$. We investigate the maximum possible proportion of users who are\nuniquely covered. We solve this problem completely in one dimension and provide\nbounds, approximations and simulation results for the two-dimensional case. \n\n"}
{"id": "1608.02427", "contents": "Title: Maximum-Likelihood Detection for Energy-Efficient Timing Acquisition in\n  NB-IoT Abstract: Initial timing acquisition in narrow-band IoT (NB-IoT) devices is done by\ndetecting a periodically transmitted known sequence. The detection has to be\ndone at lowest possible latency, because the RF-transceiver, which dominates\ndownlink power consumption of an NB-IoT modem, has to be turned on throughout\nthis time. Auto-correlation detectors show low computational complexity from a\nsignal processing point of view at the price of a higher detection latency. In\ncontrast a maximum likelihood cross-correlation detector achieves low latency\nat a higher complexity as shown in this paper. We present a hardware\nimplementation of the maximum likelihood cross-correlation detection. The\ndetector achieves an average detection latency which is a factor of two below\nthat of an auto-correlation method and is able to reduce the required energy\nper timing acquisition by up to 34%. \n\n"}
{"id": "1608.02911", "contents": "Title: Correlated Interference from Uncorrelated Users in Bounded Ad Hoc\n  Networks with Blockage Abstract: In this letter, we study the joint impact of user density, blockage density\nand deployment area on the temporal correlation of interference for static and\nhighly mobile users. We show that even if the user locations become\nuncorrelated, the interference level can still be correlated when the\ndeployment area is bounded and/or there is blockage. In addition, we study how\nthe correlation coefficients of interference scale at a high density of\nblockage. \n\n"}
{"id": "1608.03429", "contents": "Title: Information-Centric Offloading in Cellular Networks with Coordinated\n  Device-to-Device Communication Abstract: In this paper, we develop a comprehensive analytical framework for cache\nenabled cellular networks overlaid with coordinated device-to-device (D2D)\ncommunication. We follow an approach similar to LTE Direct, where the base\nstation (BS) is responsible for establishing D2D links. We consider that an\narbitrary requesting user is offloaded to D2D mode to communicate with one of\nits 'k' closest D2D helpers within the macrocell subject to content\navailability and helper selection scheme. We consider two different D2D helper\nselection schemes: 1) uniform selection (US), where the D2D helper is selected\nuniformly and 2) nearest selection (NS), where the nearest helper possessing\nthe content is selected. Employing tools from stochastic geometry, we model the\nlocations of BSs and D2D helpers using independent homogeneous Poisson point\nprocesses (HPPPs). We characterize the D2D mode probability of an arbitrary\nuser for both the NS and US schemes. The distribution of the distance between\nan arbitrary user and its ith neighboring D2D helper within the macrocell is\nderived using disk approximation for the Voronoi cell, which is shown to be\nreasonably accurate. We fully characterize the overall coverage probability and\nthe average ergodic rate of an arbitrary user requesting a particular content.\nWe show that significant performance gains can be achieved compared to\nconventional cellular communication under both the NS and US schemes when\npopular contents are requested and NS scheme always outperforms the US scheme.\nOur analysis reveals an interesting trade off between the performance metrics\nand the number of candidate D2D helpers 'k'. We conclude that enhancing D2D\nopportunities for the users does not always result in better performance and\nthe network parameters have to be carefully tuned to harness maximum gains. \n\n"}
{"id": "1608.03521", "contents": "Title: Emergent organization in a model market Abstract: We study the collective behavior of interacting agents in a simple model of\nmarket economics originally introduced by N{\\o}rrelykke and Bak. A general\ntheoretical framework for interacting traders on an arbitrary network is\npresented, with the interaction consisting of buying (namely, consumption) and\nselling (namely, production) of commodities. Extremal dynamics is introduced by\nhaving the agent with least profit in the market readjust prices, causing the\nmarket to self--organize. We study this model market on regular lattices in\ntwo--dimension as well as on random complex networks; in the critical state\nfluctuations in an activity signal exhibit properties that are characteristic\nof avalanches observed in models of self-organized criticality, and these can\nbe described by power--law distributions. \n\n"}
{"id": "1608.04024", "contents": "Title: A Non-stationary Service Curve Model for Estimation of Cellular Sleep\n  Scheduling Abstract: While steady-state solutions of backlog and delay have been derived for\nessential wireless systems, the analysis of transient phases still poses\nsignificant challenges. Considering the majority of short-lived and interactive\nflows, transient startup effects, as caused by sleep scheduling in cellular\nnetworks, have, however, a substantial impact on the performance. To facilitate\nreasoning about the transient behavior of systems, this paper contributes a\nnotion of non-stationary service curves. Models of systems with sleep\nscheduling are derived and transient backlogs and delays are analyzed. Further,\nmeasurement methods that estimate the service of an unknown system from\nobservations of selected probe traffic are developed. Fundamental limitations\nof existing measurement methods are explained by the non-convexity of the\ntransient service and further difficulties are shown to be due to the\nsuper-additivity of network service processes. A novel two-phase probing\ntechnique is devised that first determines the shape of a minimal probe and\nsubsequently obtains an accurate estimate of the unknown service. In a\ncomprehensive measurement campaign, the method is used to evaluate the service\nof cellular networks with sleep scheduling (2G, 3G, and 4G), revealing\nconsiderable transient backlog and delay overshoots that persist for long\nrelaxation times. \n\n"}
{"id": "1608.05493", "contents": "Title: Network Volume Anomaly Detection and Identification in Large-scale\n  Networks based on Online Time-structured Traffic Tensor Tracking Abstract: This paper addresses network anomography, that is, the problem of inferring\nnetwork-level anomalies from indirect link measurements. This problem is cast\nas a low-rank subspace tracking problem for normal flows under incomplete\nobservations, and an outlier detection problem for abnormal flows. Since\ntraffic data is large-scale time-structured data accompanied with noise and\noutliers under partial observations, an efficient modeling method is essential.\nTo this end, this paper proposes an online subspace tracking of a Hankelized\ntime-structured traffic tensor for normal flows based on the Candecomp/PARAFAC\ndecomposition exploiting the recursive least squares (RLS) algorithm. We\nestimate abnormal flows as outlier sparse flows via sparsity maximization in\nthe underlying under-constrained linear-inverse problem. A major advantage is\nthat our algorithm estimates normal flows by low-dimensional matrices with\ntime-directional features as well as the spatial correlation of multiple links\nwithout using the past observed measurements and the past model parameters.\nExtensive numerical evaluations show that the proposed algorithm achieves\nfaster convergence per iteration of model approximation, and better volume\nanomaly detection performance compared to state-of-the-art algorithms. \n\n"}
{"id": "1608.05783", "contents": "Title: Non-Orthogonal Multiple Access (NOMA) in Cellular Uplink and Downlink:\n  Challenges and Enabling Techniques Abstract: By combining the concepts of superposition coding at the transmitter(s) and\nsuccessive interference cancellation (SIC) at the receiver(s), non-orthogonal\nmultiple access (NOMA) has recently emerged as a promising multiple access\ntechnique for 5G wireless technology. In this article, we first discuss the\nfundamentals of uplink and downlink NOMA transmissions and outline their key\ndistinctions (in terms of implementation complexity, detection and decoding at\nthe SIC receiver(s), incurred intra-cell and inter-cell interferences). Later,\nfor both downlink and uplink NOMA, we theoretically derive the NOMA dominant\ncondition for each individual user in a two-user NOMA cluster. NOMA dominant\ncondition refers to the condition under which the spectral efficiency gains of\nNOMA are guaranteed compared to conventional orthogonal multiple access (OMA).\nThe derived conditions provide direct insights on selecting appropriate users\nin two-user NOMA clusters. The conditions are distinct for uplink and downlink\nas well as for each individual user. Numerical results show the significance of\nthe derived conditions for the user selection in uplink/downlink NOMA clusters\nand provide a comparison to the random user selection. A brief overview of the\nrecent research investigations is then provided to highlight the existing\nresearch gaps. Finally, we discuss the potential applications and key\nchallenges of NOMA transmissions. \n\n"}
{"id": "1608.07949", "contents": "Title: Learning-Based Resource Allocation Scheme for TDD-Based CRAN System Abstract: Explosive growth in the use of smart wireless devices has necessitated the\nprovision of higher data rates and always-on connectivity, which are the main\nmotivators for designing the fifth generation (5G) systems. To achieve higher\nsystem efficiency, massive antenna deployment with tight coordination is one\npotential strategy for designing 5G systems, but has two types of associated\nsystem overhead. First is the synchronization overhead, which can be reduced by\nimplementing a cloud radio access network (CRAN)-based architecture design,\nthat separates the baseband processing and radio access functionality to\nachieve better system synchronization. Second is the overhead for acquiring\nchannel state information (CSI) of the users present in the system, which,\nhowever, increases tremendously when instantaneous CSI is used to serve\nhigh-mobility users. To serve a large number of users, a CRAN system with a\ndense deployment of remote radio heads (RRHs) is considered, such that each\nuser has a line-of-sight (LOS) link with the corresponding RRH. Since, the\ntrajectory of movement for high-mobility users is predictable; therefore,\nfairly accurate position estimates for those users can be obtained, and can be\nused for resource allocation to serve the considered users. The resource\nallocation is dependent upon various correlated system parameters, and these\ncorrelations can be learned using well-known \\emph{machine learning}\nalgorithms. This paper proposes a novel \\emph{learning-based resource\nallocation scheme} for time division duplex (TDD) based 5G CRAN systems with\ndense RRH deployment, by using only the users' position estimates for resource\nallocation, thus avoiding the need for CSI acquisition. This reduces the\noverall system overhead significantly, while still achieving near-optimal\nsystem performance; thus, better (effective) system efficiency is achieved.\n(See the paper for full abstract) \n\n"}
{"id": "1608.07953", "contents": "Title: Coexistence of OFDM and FBMC for Underlay D2D Communication in 5G\n  Networks Abstract: Device-to-device (D2D) communication is being heralded as an important part\nof the solution to the capacity problem in future networks, and is expected to\nbe natively supported in 5G. Given the high network complexity and required\nsignalling overhead associated with achieving synchronization in D2D networks,\nit is necessary to study asynchronous D2D communications. In this paper, we\nconsider a scenario whereby asynchronous D2D communication underlays an OFDMA\nmacro-cell in the uplink. Motivated by the superior performance of new\nwaveforms with increased spectral localization in the presence of frequency and\ntime misalignments, we compare the system-level performance of a set-up for\nwhen D2D pairs use either OFDM or FBMC/OQAM. We first demonstrate that\ninter-D2D interference, resulting from misaligned communications, plays a\nsignificant role in clustered D2D topologies. We then demonstrate that the\nresource allocation procedure can be simplified when D2D pairs use FBMC/OQAM,\nsince the high spectral localization of FBMC/OQAM results in negligible\ninter-D2D interference. Specifically, we identify that FBMC/OQAM is best suited\nto scenarios consisting of small, densely populated D2D clusters located near\nthe encompassing cell's edge. \n\n"}
{"id": "1608.08521", "contents": "Title: Profitable Task Allocation in Mobile Cloud Computing Abstract: We propose a game theoretic framework for task allocation in mobile cloud\ncomputing that corresponds to offloading of compute tasks to a group of nearby\nmobile devices. Specifically, in our framework, a distributor node holds a\nmultidimensional auction for allocating the tasks of a job among nearby mobile\nnodes based on their computational capabilities and also the cost of\ncomputation at these nodes, with the goal of reducing the overall job\ncompletion time. Our proposed auction also has the desired incentive\ncompatibility property that ensures that mobile devices truthfully reveal their\ncapabilities and costs and that those devices benefit from the task allocation.\nTo deal with node mobility, we perform multiple auctions over adaptive time\nintervals. We develop a heuristic approach to dynamically find the best time\nintervals between auctions to minimize unnecessary auctions and the\naccompanying overheads. We evaluate our framework and methods using both real\nworld and synthetic mobility traces. Our evaluation results show that our game\ntheoretic framework improves the job completion time by a factor of 2-5 in\ncomparison to the time taken for executing the job locally, while minimizing\nthe number of auctions and the accompanying overheads. Our approach is also\nprofitable for the nearby nodes that execute the distributor's tasks with these\nnodes receiving a compensation higher than their actual costs. \n\n"}
{"id": "1609.00424", "contents": "Title: Multi-Path Low Delay Network Codes Abstract: The capability of mobile devices to use multiple interfaces to support a\nsingle session is becoming more prevalent. Prime examples include the desire to\nimplement WiFi offloading and the introduction of 5G. Furthermore, an\nincreasing fraction of Internet traffic is becoming delay sensitive. These two\ntrends drive the need to investigate methods that enable communication over\nmultiple parallel heterogeneous networks, while also ensuring that delay\nconstraints are met. This paper approaches these challenges using a multi-path\nstreaming code that uses forward error correction to reduce the in-order\ndelivery delay of packets in networks with poor link quality and transient\nconnectivity. A simple analysis is developed that provides a good approximation\nof the in-order delivery delay. Furthermore, numerical results help show that\nthe delay penalty of communicating over multiple paths is insignificant when\nconsidering the potential throughput gains obtained through the fusion of\nmultiple networks. \n\n"}
{"id": "1609.00664", "contents": "Title: Transparent Clouds: An Enhancement to Abstraction Abstract: With the introduction of various hardware/software technologies such as Cloud\nTechnologies or Virtualization technologies, there has been a great potential\nto reuse ICT artifacts thanks to Abstraction and also Exchangeability features\nachieved via these technologies. These technologies also provide various\nadvantages with respect to sustainability including resource consumption\nreduction (in the use phase only or in the whole life cycle). However, there is\nan additional but untapped potential associated with the anonymization of\nresources introduced by both abstraction and exchangeability features. By\nrealizing on this potential, we can improve cloud solutions and reduce their\nby-product opacity, which usually prevents leveraging on the specialized but\ntweakable (i.e., nonessential modifications without changing the main function)\nfeatures of components that are captured in the component models. This is\nespecially a challenge in the case heterogeneous/disaggregated infrastructure\nwhere developing models to cover everything is practically impossible. In this\nwork, by leveraging on the concept of pathways, we develop a few mechanisms\nthat enable transparency and therefore tweakability of features even in the\npresence of abstraction and heterogeneity. In particular, the layered-stack\napproach to system decomposition is considered because of its role in both\nsoftware defined networking (SDN) and Network Function Virtualization (NFV)\nsystem decompositions. For a concrete example, the case of dynamic frequency\nscaling of processors is considered and it is shown that the associated\nconsumption could be considerably reduced without requiring additional changes\nto the middle components. \n\n"}
{"id": "1609.00852", "contents": "Title: Joint Caching and Pricing Strategies for Popular Content in Information\n  Centric Networks Abstract: We develop an analytical framework for distribution of popular content in an\nInformation Centric Network (ICN) that comprises of Access ICNs, a Transit ICN\nand a Content Provider. Using a generalized Zipf distribution to model content\npopularity, we devise a game theoretic approach to jointly determine caching\nand pricing strategies in such an ICN. Under the assumption that the caching\ncost of the access and transit ICNs is inversely proportional to popularity, we\nshow that the Nash caching strategies in the ICN are 0-1 (all or nothing)\nstrategies. Further, for the case of symmetric Access ICNs, we show that the\nNash equilibrium is unique and the caching policy (0 or 1) is determined by a\nthreshold on the popularity of the content (reflected by the Zipf probability\nmetric), i.e., all content more popular than the threshold value is cached. We\nalso show that the resulting threshold of the Access and Transit ICNs, as well\nas all prices can be obtained by a decomposition of the joint caching and\npricing problem into two independent caching only and pricing only problems. \n\n"}
{"id": "1609.04552", "contents": "Title: SCTP User Message Interleaving Integration and Validation Abstract: The Stream Control Transmission Protocol (SCTP) is a connection and message\noriented transport protocol. It supports multiple uni-directional streams in\neach direction allowing user message sequence preservation within each stream.\nThis minimizes the re-sequencing delay at the receiver side in case of message\nloss. The base protocol, although being optimized for small messages, supports\narbitrary large user messages by using fragmentation and reassembly at the cost\nof adding delays at the sender side. To overcome this limitation, a protocol\nextension called User Message Interleaving is currently being specified by the\nInternet Engineering Task Force (IETF). This paper describes the new extension,\nits integration and validation in the context of the INET framework. \n\n"}
{"id": "1609.05362", "contents": "Title: Mobile Edge Computing via a UAV-Mounted Cloudlet: Optimization of Bit\n  Allocation and Path Planning Abstract: Unmanned Aerial Vehicles (UAVs) have been recently considered as means to\nprovide enhanced coverage or relaying services to mobile users (MUs) in\nwireless systems with limited or no infrastructure. In this paper, a UAV-based\nmobile cloud computing system is studied in which a moving UAV is endowed with\ncomputing capabilities to offer computation offloading opportunities to MUs\nwith limited local processing capabilities. The system aims at minimizing the\ntotal mobile energy consumption while satisfying quality of service\nrequirements of the offloaded mobile application. Offloading is enabled by\nuplink and downlink communications between the mobile devices and the UAV that\ntake place by means of frequency division duplex (FDD) via orthogonal or\nnon-orthogonal multiple access (NOMA) schemes. The problem of jointly\noptimizing the bit allocation for uplink and downlink communication as well as\nfor computing at the UAV, along with the cloudlet's trajectory under latency\nand UAV's energy budget constraints is formulated and addressed by leveraging\nsuccessive convex approximation (SCA) strategies. Numerical results demonstrate\nthe significant energy savings that can be accrued by means of the proposed\njoint optimization of bit allocation and cloudlet's trajectory as compared to\nlocal mobile execution as well as to partial optimization approaches that\ndesign only the bit allocation or the cloudlet's trajectory. \n\n"}
{"id": "1609.05670", "contents": "Title: Load-aware Performance Analysis of Cell Center/Edge Users in Random\n  HetNets Abstract: For real-time traffic, the link quality and call blocking probability (both\nderived from coverage probability) are realized to be poor for cell edge users\n(CEUs) compared to cell center users (CCUs) as the signal reception in the cell\ncenter region is better compared to the cell edge region. In heterogeneous\nnetworks (HetNets), the uncoordinated channel access by different types of base\nstations determine the interference statistics that further arbitrates the\ncoverage probability. Thus, the spectrum allocation techniques have major\nimpact on the performance of CCU and CEU. In this paper, the performance of\nCCUs and CEUs in a random two-tier network is studied for two spectrum\nallocation techniques namely: 1) co-channel (CSA), and 2) shared (SSA). For\nperformance analysis, the widely accepted conception of modeling the tiers of\nHetNet using independent homogeneous Poisson point process (PPP) is considered\nto accommodate the spatial randomness in location of BSs. To incorporate the\nspatial randomness in the arrival of service and to aid the load-aware\nanalysis, the cellular traffic is modeled using spatio-temporal PPP. Under this\nscenario, we have developed an analytical framework to evaluate the load-aware\nperformance, including coverage and blocking probabilities, of CCUs and CEUs\nunder both spectrum allocation techniques. Further, we provide insight into\nachievable area energy efficiency for SSA and CSA. The developed analytical\nframework is validated through extensive simulations. Next, we demonstrate the\nimpact of traffic load and femto access points density on the performance of\nCCUs/CEUs under CSA and SSA. \n\n"}
{"id": "1609.06888", "contents": "Title: Analysis of Network Robustness for Finite Sized Wireless Sensor Networks Abstract: Studying network robustness for wireless sensor networks(WSNs) is an exciting\ntopic of research as sensor nodes often fail due to hardware degradation,\nresource constraints, and environmental changes. The application of spectral\ngraph theory to networked systems has generated several important results.\nHowever, previous research has often failed to consider the network parameters,\nwhich is crucial to study the real network applications. Network criticality is\none of the effective metrics to quantify the network robustness against such\nfailures and attacks. In this work, we derive the exact formulas of network\ncriticality for WSNs using r-nearest neighbor networks and we show the effect\nof nearest neighbors and network dimension on robustness using analytical and\nnumerical evaluations. Furthermore, we also show how symmetric and static\napproximations can wrongly designate the network robustness when implemented to\nWSNs. \n\n"}
{"id": "1609.09682", "contents": "Title: Soft Cache Hits and the Impact of Alternative Content Recommendations on\n  Mobile Edge Caching Abstract: Caching popular content at the edge of future mobile networks has been widely\nconsidered in order to alleviate the impact of the data tsunami on both the\naccess and backhaul networks. A number of interesting techniques have been\nproposed, including femto-caching and \"delayed\" or opportunistic cache access.\nNevertheless, the majority of these approaches suffer from the rather limited\nstorage capacity of the edge caches, compared to the tremendous and rapidly\nincreasing size of the Internet content catalog. We propose to depart from the\nassumption of hard cache misses, common in most existing works, and consider\n\"soft\" cache misses, where if the original content is not available, an\nalternative content that is locally cached can be recommended. Given that\nInternet content consumption is increasingly entertainment-oriented, we believe\nthat a related content could often lead to complete or at least partial user\nsatisfaction, without the need to retrieve the original content over expensive\nlinks. In this paper, we formulate the problem of optimal edge caching with\nsoft cache hits, in the context of delayed access, and analyze the expected\ngains. We then show using synthetic and real datasets of related video contents\nthat promising caching gains could be achieved in practice. \n\n"}
{"id": "1609.09773", "contents": "Title: An Overview of Sustainable Green 5G Networks Abstract: The stringent requirements of a 1,000 times increase in data traffic and one\nmillisecond round trip latency have made limiting the potentially tremendous\nensuing energy consumption one of the most challenging problems for the design\nof the upcoming fifth-generation (5G) networks. To enable sustainable 5G\nnetworks, new technologies have been proposed to improve the system energy\nefficiency and alternative energy sources are introduced to reduce our\ndependence on traditional fossil fuels. In particular, various 5G techniques\ntarget the reduction of the energy consumption without sacrificing the\nquality-of-service. Meanwhile, energy harvesting technologies, which enable\ncommunication transceivers to harvest energy from various renewable resources\nand ambient radio frequency signals for communi- cation, have drawn significant\ninterest from both academia and industry. In this article, we provide an\noverview of the latest research on both green 5G techniques and energy\nharvesting for communication. In addition, some technical challenges and\npotential research topics for realizing sustainable green 5G networks are also\nidentified. \n\n"}
{"id": "1610.00564", "contents": "Title: End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural\n  Networks Abstract: We investigate sequence machine learning techniques on raw radio signal\ntime-series data. By applying deep recurrent neural networks we learn to\ndiscriminate between several application layer traffic types on top of a\nconstant envelope modulation without using an expert demodulation algorithm. We\nshow that complex protocol sequences can be learned and used for both\nclassification and generation tasks using this approach. \n\n"}
{"id": "1610.02728", "contents": "Title: Lying Your Way to Better Traffic Engineering Abstract: To optimize the flow of traffic in IP networks, operators do traffic\nengineering (TE), i.e., tune routing-protocol parameters in response to traffic\ndemands. TE in IP networks typically involves configuring static link weights\nand splitting traffic between the resulting shortest-paths via the\nEqual-Cost-MultiPath (ECMP) mechanism. Unfortunately, ECMP is a notoriously\ncumbersome and indirect means for optimizing traffic flow, often leading to\npoor network performance. Also, obtaining accurate knowledge of traffic demands\nas the input to TE is elusive, and traffic conditions can be highly variable,\nfurther complicating TE. We leverage recently proposed schemes for increasing\nECMP's expressiveness via carefully disseminated bogus information (\"lies\") to\ndesign COYOTE, a readily deployable TE scheme for robust and efficient network\nutilization. COYOTE leverages new algorithmic ideas to configure (static)\ntraffic splitting ratios that are optimized with respect to all (even\nadversarially chosen) traffic scenarios within the operator's \"uncertainty\nbounds\". Our experimental analyses show that COYOTE significantly outperforms\ntoday's prevalent TE schemes in a manner that is robust to traffic uncertainty\nand variation. We discuss experiments with a prototype implementation of\nCOYOTE. \n\n"}
{"id": "1610.04203", "contents": "Title: Maximizing Broadcast Throughput Under Ultra-Low-Power Constraints Abstract: Wireless object tracking applications are gaining popularity and will soon\nutilize emerging ultra-low-power device-to-device communication. However,\nsevere energy constraints require much more careful accounting of energy usage\nthan what prior art provides. In particular, the available energy, the\ndiffering power consumption levels for listening, receiving, and transmitting,\nas well as the limited control bandwidth must all be considered. Therefore, we\nformulate the problem of maximizing the throughput among a set of heterogeneous\nbroadcasting nodes with differing power consumption levels, each subject to a\nstrict ultra-low-power budget. We obtain the oracle throughput (i.e., maximum\nthroughput achieved by an oracle) and use Lagrangian methods to design EconCast\n- a simple asynchronous distributed protocol in which nodes transition between\nsleep, listen, and transmit states, and dynamically change the transition\nrates. EconCast can operate in groupput or anyput modes to respectively\nmaximize two alternative throughput measures. We show that EconCast approaches\nthe oracle throughput. The performance is also evaluated numerically and via\nextensive simulations and it is shown that EconCast outperforms prior art by 6x\n- 17x under realistic assumptions. Moreover, we evaluate EconCast's latency\nperformance and consider design tradeoffs when operating in groupput and anyput\nmodes. Finally, we implement EconCast using the TI eZ430-RF2500-SEH energy\nharvesting nodes and experimentally show that in realistic environments it\nobtains 57% - 77% of the achievable throughput. \n\n"}
{"id": "1610.04793", "contents": "Title: Low Power Wide Area Network Analysis: Can LoRa Scale? Abstract: Low Power Wide Area (LPWA) networks are making spectacular progress from\ndesign, standardisation, to commercialisation. At this time of fast-paced\nadoption, it is of utmost importance to analyse how well these technologies\nwill scale as the number of devices connected to the Internet of Things (IoT)\ninevitably grows. In this letter, we provide a stochastic geometry framework\nfor modelling the performance of a single gateway LoRa network, a leading LPWA\ntechnology. Our analysis formulates unique peculiarities of LoRa, including its\nchirp spread-spectrum modulation technique, regulatory limitations on radio\nduty cycle, and use of ALOHA protocol on top, all of which are not as common in\ntoday's commercial cellular networks. We show that the coverage probability\ndrops exponentially as the number of end-devices grows due to interfering\nsignals using the same spreading sequence. We conclude that this fundamental\nlimiting factor is perhaps more significant towards LoRa scalability than for\ninstance spectrum restrictions. Our derivations for co-spreading factor\ninterference found in LoRa networks enables rigorous scalability analysis of\nsuch networks. \n\n"}
{"id": "1610.04836", "contents": "Title: An Efficient Uplink Multi-Connectivity Scheme for 5G mmWave Control\n  Plane Applications Abstract: The millimeter wave (mmWave) frequencies offer the potential of orders of\nmagnitude increases in capacity for next-generation cellular systems. However,\nlinks in mmWave networks are susceptible to blockage and may suffer from rapid\nvariations in quality. Connectivity to multiple cells - at mmWave and/or\ntraditional frequencies - is considered essential for robust communication. One\nof the challenges in supporting multi-connectivity in mmWaves is the\nrequirement for the network to track the direction of each link in addition to\nits power and timing. To address this challenge, we implement a novel uplink\nmeasurement system that, with the joint help of a local coordinator operating\nin the legacy band, guarantees continuous monitoring of the channel propagation\nconditions and allows for the design of efficient control plane applications,\nincluding handover, beam tracking and initial access. We show that an\nuplink-based multi-connectivity approach enables less consuming, better\nperforming, faster and more stable cell selection and scheduling decisions with\nrespect to a traditional downlink-based standalone scheme. Moreover, we argue\nthat the presented framework guarantees (i) efficient tracking of the user in\nthe presence of the channel dynamics expected at mmWaves, and (ii) fast\nreaction to situations in which the primary propagation path is blocked or not\navailable. \n\n"}
{"id": "1610.06138", "contents": "Title: Fundamental Limits on Throughput Capacity in Information-Centric Network Abstract: Wireless information-centric networks consider storage as one of the network\nprimitives, and propose to cache data within the network in order to improve\nlatency and reduce bandwidth consumption. We study the throughput capacity and\nlatency in an information-centric network when the data cached in each node has\na limited lifetime. The results show that with some fixed request and cache\nexpiration rates, the order of the data access time does not change with\nnetwork growth, and the maximum throughput order is not changing with the\nnetwork growth in grid networks, and is inversely proportional to the number of\nnodes in one cell in random networks. Comparing these values with the\ncorresponding throughput and latency with no cache capability (throughput\ninversely proportional to the network size, and latency of order $\\sqrt{n}$ and\nthe inverse of the transmission range in grid and random networks,\nrespectively), we can actually quantify the asymptotic advantage of caching.\nMoreover, we compare these scaling laws for different content discovery\nmechanisms and illustrate that not much gain is lost when a simple path search\nis used. \n\n"}
{"id": "1610.06212", "contents": "Title: RadioHound: A Pervasive Sensing Network for Sub-6 GHz Dynamic Spectrum\n  Monitoring Abstract: We design a custom spectrum sensing network, called RadioHound, capable of\ntuning from 25 MHz to 6 GHz, which covers nearly all widely-deployed wireless\nactivity. We describe the system hardware and network infrastructure in detail\nwith a view towards driving the cost, size, and power usage of the sensors as\nlow as possible. The system estimates the spatial variation of radio-frequency\npower from an unknown random number of sources. System performance is measured\nby computing the mean square error against a simulated radio-frequency\nenvironment. We find that the system performance depends heavily on the\ndeployment density of the sensors. Consequently, we derive an expression for\nthe sensor density as a function of environmental characteristics and\nconfidence in measurement quality. \n\n"}
{"id": "1610.06754", "contents": "Title: A Localization Approach for Crowdsourced Air Traffic Communication\n  Networks Abstract: In this work, we argue that current state-of-the-art methods of aircraft\nlocalization such as multilateration are insufficient, in particular for modern\ncrowdsourced air traffic networks with random, unplanned deployment geometry.\nWe propose an alternative, a grid-based localization approach using the\nk-Nearest Neighbor algorithm, to deal with the identified shortcomings. Our\nproposal does not require any changes to the existing air traffic protocols and\ntransmitters, and is easily implemented using only low-cost,\ncommercial-off-the-shelf hardware.\n  Using an algebraic multilateration algorithm for comparison, we evaluate our\napproach using real-world flight data collected with our collaborative sensor\nnetwork OpenSky. We quantify its effectiveness in terms of aircraft location\naccuracy, surveillance coverage, and the verification of false position data.\nOur results show that the grid-based approach can increase the effective air\ntraffic surveillance coverage compared to multilateration by a factor of up to\n2.5. As it does not suffer from dilution of precision, it is much more robust\nin noisy environments and performs better in pre-existing, unplanned receiver\ndeployments.\n  We further find that the mean aircraft location accuracy can be increased by\nup to 41% in comparison with multilateration while also being able to pinpoint\nthe origin of potential spoofing attacks conducted from the ground. \n\n"}
{"id": "1610.06995", "contents": "Title: Modeling and Analysis of Uplink Non-Orthogonal Multiple Access (NOMA) in\n  Large-Scale Cellular Networks Using Poisson Cluster Processes Abstract: Non-orthogonal multiple access (NOMA) serves multiple users by superposing\ntheir distinct message signals. The desired message signal is decoded at the\nreceiver by applying successive interference cancellation (SIC). Using the\ntheory of Poisson cluster process (PCP), we provide a framework to analyze\nmulti-cell uplink NOMA systems. Specifically, we characterize the rate coverage\nprobability of a NOMA user who is at rank $m$ (in terms of the distance from\nits serving BS) among all users in a cell and the mean rate coverage\nprobability of all users in a cell. Since the signal-to-interference-plus-noise\nratio (SINR) of $m$-th user relies on efficient SIC, we consider three\nscenarios, i.e., perfect SIC (in which the signals of $m-1$ interferers who are\nstronger than $m$-th user are decoded successfully), imperfect SIC (in which\nthe signals of of $m-1$ interferers who are stronger than $m$-th user may or\nmay not be decoded successfully), and imperfect worst case SIC (in which the\ndecoding of the signal of $m$-th user is always unsuccessful whenever the\ndecoding of its relative $m-1$ stronger users is unsuccessful). The derived\nexpressions are customized to capture the performance of a user at rank $m$ in\nan equivalent orthogonal multiple access (OMA) system. Finally, numerical\nresults are presented to validate the derived expressions. \n\n"}
{"id": "1610.09884", "contents": "Title: Isolation probabilities in dynamic soft random geometric graphs Abstract: We consider soft random geometric graphs, constructed by distributing points\n(nodes) randomly according to a Poisson Point Process, and forming links\nbetween pairs of nodes with a probability that depends on their mutual\ndistance, the \"connection function.\" Each node has a probability of being\nisolated depending on the locations of the other nodes; we give analytic\nexpressions for the distribution of isolation probabilities. Keeping the node\nlocations fixed, the links break and reform over time, making a dynamic\nnetwork; this is a good model of a wireless ad-hoc network with communication\nchannels undergoing rapid fading. We use the above isolation probabilities to\ninvestigate the distribution of the time to transmit information to all the\nnodes, finding good agreement with numerics. \n\n"}
{"id": "1611.00688", "contents": "Title: Mitigating Inter-network Interference in LoRa Networks Abstract: Long Range (LoRa) is a popular technology used to construct Low-Power\nWide-Area Network (LPWAN) networks. Given the popularity of LoRa it is likely\nthat multiple independent LoRa networks are deployed in close proximity. In\nthis situation, neighbouring networks interfere and methods have to be found to\ncombat this interference. In this paper we investigate the use of directional\nantennae and the use of multiple base stations as methods of dealing with\ninter-network interference. Directional antennae increase signal strength at\nreceivers without increasing transmission energy cost. Thus, the probability of\nsuccessfully decoding the message in an interference situation is improved.\nMultiple base stations can alternatively be used to improve the probability of\nreceiving a message in a noisy environment. We compare the effectiveness of\nthese two approaches via simulation. Our findings show that both methods are\nable to improve LoRa network performance in interference settings. However, the\nresults show that the use of multiple base stations clearly outperforms the use\nof directional antennae. For example, in a setting where data is collected from\n600 nodes which are interfered by four networks with 600 nodes each, using\nthree base stations improves the Data Extraction Rate (DER) from 0.24 to 0.56\nwhile the use of directional antennae provides an increase to only 0.32. \n\n"}
{"id": "1611.07238", "contents": "Title: Time and Space Optimal Counting in Population Protocols Abstract: This work concerns the general issue of combined optimality in terms of time\nand space complexity. In this context, we study the problem of (exact) counting\nresource-limited and passively mobile nodes in the model of population\nprotocols, in which the space complexity is crucial. The counted nodes are\nmemory-limited anonymous devices (called agents) communicating asynchronously\nin pairs (according to a fairness condition). Moreover, we assume that these\nagents are prone to failures so that they cannot be correctly initialized. This\nstudy considers two classical fairness conditions, and for each we investigate\nthe issue of time optimality of counting given the optimal space per agent. In\nthe case of randomly interacting agents (probabilistic fairness), as usual, the\nconvergence time is measured in terms of parallel time (or parallel\ninteractions), which is defined as the number of pairwise interactions until\nconvergence, divided by n (the number of agents). In case of weak fairness,\nwhere it is only required that every pair of agents interacts infinitely often,\nthe convergence time is defined in terms of non-null transitions, i.e, the\ntransitions that affect the states of the interacting agents.First, assuming\nprobabilistic fairness, we present a \"non-guessing\" time optimal protocol of\nO(n log n) expected time given an optimal space of only one bit, and we prove\nthe time optimality of this protocol. Then, for weak fairness, we show that a\nspace optimal (semi-uniform) solution cannot converge faster than in\n$\\Omega$(2^n) time (non-null transitions). This result, together with the time\ncomplexity analysis of an already known space optimal protocol, shows that it\nis also optimal in time (given the optimal space constrains). \n\n"}
{"id": "1611.08703", "contents": "Title: Multi-hop Communication in the Uplink for LPWANs Abstract: Low-Power Wide Area Networks (LPWANs) have arisen as a promising\ncommunication technology for supporting Internet of Things (IoT) services due\nto their low power operation, wide coverage range, low cost and scalability.\nHowever, most LPWAN solutions like SIGFOX or LoRaWAN rely on star topology\nnetworks, where stations (STAs) transmit directly to the gateway (GW), which\noften leads to rapid battery depletion in STAs located far from it. In this\nwork, we analyze the impact on LPWANs energy consumption of multi-hop\ncommunication in the uplink, allowing STAs to transmit data packets in lower\npower levels and higher data rates to closer parent STAs, reducing their energy\nconsumption consequently. To that aim, we introduce the Distance-Ring\nExponential Stations Generator (DRESG) framework, designed to evaluate the\nperformance of the so-called optimal-hop routing model, which establishes\noptimal routing connections in terms of energy efficiency, aiming to balance\nthe consumption among all the STAs in the network. Results show that enabling\nsuch multi-hop connections entails higher network lifetimes, reducing\nsignificantly the bottleneck consumption in LPWANs with up to thousands of\nSTAs. These results lead to foresee multi-hop communication in the uplink as a\npromising routing alternative for extending the lifetime of LPWAN deployments. \n\n"}
{"id": "1611.09011", "contents": "Title: Scalable Fine-grained Path Control in Software Defined Networks Abstract: The OpenFlow-based SDN is widely studied to better network performance\nthrough planning fine-grained paths. However, being designed to configure path\nhop-by-hop, it faces the scalability issue that both the flow table overhead\nand path setup delay are unacceptable for large-scale networks. In this paper,\nwe propose PACO, a framework based on Source Routing to address that problem\nthrough quickly pushing paths into the packet header at network edges and\npre-installing few rules at the network core. The straightforward\nimplementation of SR is inefficient as it would incur too many path labels;\nother efficient approaches would sacrifice path flexibility (e.g., DEFO). To\nimplement SR efficiently and flexibly, PACO presents each path as a\nconcatenation of pathlets and introduces algorithms to compute pathlets and\nconcatenate paths with minimum path labels. Our extensive simulations confirm\nthe scalability of PACO as it saves the flow table overhead up to 94% compared\nwith OpenFlow-SDN solutions and show that PACO outperforms SR-SDN solutions by\nsupporting more than 40% paths with few label overhead. \n\n"}
{"id": "1612.00397", "contents": "Title: Maximal Sections of Sheaves of Data over an Abstract Simplicial Complex Abstract: We employ techniques from topological data analysis to model sensor networks.\nOur approach to sensor integration uses the topological method of sheaves over\ncell complexes. The internal consistency of data from individual sensors is\ndetermined by a set of consistency functions assigned to elements of the\ncomplex. Using these functions we determine, for any collection of data, the\nunique set of maximal sections of consistent data received from the sensors. We\noffer a proof for the existence and uniqueness of these sections and illustrate\nthe ideas with examples. \n\n"}
{"id": "1612.01264", "contents": "Title: New Generation Value Networks for Content Delivery Abstract: In this paper we paint a broad picture of the Internet content delivery\nmarket, by taking into consideration both economical and technical challenges\nthat might drive the interactions among the stakeholders in the future. We\nfocus on a few disrupting factors, namely ubiquitous encryption, traffic boost,\nnetwork scalability, latency needs and network control; and try to figure out\nwhether the current model (CDN) is robust against their variation, which\noptimization can be envisioned and how the most accredited option (ICN) can be\nof help. \n\n"}
{"id": "1612.03985", "contents": "Title: Restless Video Bandits: Optimal SVC Streaming in a Multi-user Wireless\n  Network Abstract: In this paper, we consider the problem of optimal scalable video delivery to\nmobile users in wireless networks given arbitrary Quality Adaptation (QA)\nmechanisms. In current practical systems, QA and scheduling are performed\nindependently by the content provider and network operator, respectively. While\nmost research has been focused on jointly optimizing these two tasks, the high\ncomplexity that comes with a joint approach makes the implementation\nimpractical. Therefore, we present a scheduling mechanism that takes the QA\nlogic of each user as input and optimizes the scheduling accordingly. Hence,\nthere is no need for centralized QA and cross-layer interactions are minimized.\nWe model the QA-adaptive scheduling and the jointly optimal problem as a\nRestless Bandit and a Multi-user Semi Markov Decision Process, respectively in\norder to compare the loss incurred by not employing a jointly optimal scheme.\nWe then present heuristic algorithms in order to achieve the optimal outcome of\nthe Restless Bandit solution assuming the base station has knowledge of the\nunderlying quality adaptation of each user (QA-Aware). We also present a\nsimplified heuristic without the need for any higher layer knowledge at the\nbase station (QA-Blind). We show that our QA-Aware strategy can achieve up to\ntwo times improvement in user network utilization compared to popular baseline\nalgorithms such as Proportional Fairness. We also provide a testbed\nimplementation of the QA-Blind scheme in order to compare it with baseline\nalgorithms in a real network setting. \n\n"}
{"id": "1612.05539", "contents": "Title: Greedy Routing and the Algorithmic Small-World Phenomenom Abstract: The algorithmic small-world phenomenon, empirically established by Milgram's\nletter forwarding experiments from the 60s, was theoretically explained by\nKleinberg in 2000. However, from today's perspective his model has several\nsevere shortcomings that limit the applicability to real-world networks. In\norder to give a more convincing explanation of the algorithmic small-world\nphenomenon, we study decentralized greedy routing in a more flexible random\ngraph model (geometric inhomogeneous random graphs) which overcomes all\nprevious shortcomings. Apart from exhibiting good properties in theory, it has\nalso been extensively experimentally validated that this model reasonably\ncaptures real-world networks.\n  In this model, the greedy routing protocol is purely distributed as each\nvertex only needs to know information about its direct neighbors. We prove that\nit succeeds with constant probability, and in case of success almost surely\nfinds an almost shortest path of length {\\theta}(loglog n), where our bound is\ntight including the leading constant. Moreover, we study natural local patching\nmethods which augment greedy routing by backtracking and which do not require\nany global knowledge. We show that such methods can ensure success probability\n1 in an asymptotically tight number of steps.\n  These results also address the question of Krioukov et al. whether there are\nefficient local routing protocols for the internet graph. There were promising\nexperimental studies, but the question remained unsolved theoretically. Our\nresults give for the first time a rigorous and analytical affirmative answer. \n\n"}
{"id": "1701.01212", "contents": "Title: Downlink Coverage Analysis for a Finite 3D Wireless Network of Unmanned\n  Aerial Vehicles Abstract: In this paper, we consider a finite network of unmanned aerial vehicles\n(UAVs) serving a given region. Modeling this network as a uniform binomial\npoint process (BPP), we derive the downlink coverage probability of a reference\nreceiver located at an arbitrary position on the ground assuming Nakagami-$m$\nfading for all wireless links. The reference receiver is assumed to connect to\nits closest transmitting node as is usually the case in cellular systems. After\nderiving the distribution of distances from the reference receiver to the\nserving and interfering nodes, we derive an exact expression for downlink\ncoverage probability in terms of the derivative of Laplace transform of\ninterference power distribution. In the downlink of this system, it is not\nunusual to encounter scenarios in which the line-of-sight (LOS) component is\nsignificantly stronger than the reflected multipath components. To emulate such\nscenarios, we also derive the coverage probability in the absence of fading\nfrom the results of Nakagami-$m$ fading by taking the limit $m \\to \\infty$.\nUsing asymptotic expansion of incomplete gamma function, we concretely show\nthat this limit reduces to a redundant condition. Consequently, we derive an\naccurate coverage probability approximation for this case using dominant\ninterferer-based approach in which the effect of dominant interferer is exactly\ncaptured and the residual interference from other interferers is carefully\napproximated. We then derive the bounds of the approximate coverage probability\nusing Berry-Esseen theorem. Our analyses reveal several useful trends in\ncoverage probability as a function of height of the transmitting nodes and the\nlocation of reference receiver on the ground. \n\n"}
{"id": "1701.02979", "contents": "Title: Multi-Antenna Coded Caching Abstract: In this paper we consider a single-cell downlink scenario where a\nmultiple-antenna base station delivers contents to multiple cache-enabled user\nterminals. Based on the multicasting opportunities provided by the so-called\nCoded Caching technique, we investigate three delivery approaches. Our baseline\nscheme employs the coded caching technique on top of max-min fair multicasting.\nThe second one consists of a joint design of Zero-Forcing (ZF) and coded\ncaching, where the coded chunks are formed in the signal domain (complex\nfield). The third scheme is similar to the second one with the difference that\nthe coded chunks are formed in the data domain (finite field). We derive\nclosed-form rate expressions where our results suggest that the latter two\nschemes surpass the first one in terms of Degrees of Freedom (DoF). However, at\nthe intermediate SNR regime forming coded chunks in the signal domain results\nin power loss, and will deteriorate throughput of the second scheme. The main\nmessage of our paper is that the schemes performing well in terms of DoF may\nnot be directly appropriate for intermediate SNR regimes, and modified schemes\nshould be employed. \n\n"}
{"id": "1701.03968", "contents": "Title: Attention Allocation Aid for Visual Search Abstract: This paper outlines the development and testing of a novel, feedback-enabled\nattention allocation aid (AAAD), which uses real-time physiological data to\nimprove human performance in a realistic sequential visual search task. Indeed,\nby optimizing over search duration, the aid improves efficiency, while\npreserving decision accuracy, as the operator identifies and classifies targets\nwithin simulated aerial imagery. Specifically, using experimental eye-tracking\ndata and measurements about target detectability across the human visual field,\nwe develop functional models of detection accuracy as a function of search\ntime, number of eye movements, scan path, and image clutter. These models are\nthen used by the AAAD in conjunction with real time eye position data to make\nprobabilistic estimations of attained search accuracy and to recommend that the\nobserver either move on to the next image or continue exploring the present\nimage. An experimental evaluation in a scenario motivated from human\nsupervisory control in surveillance missions confirms the benefits of the AAAD. \n\n"}
{"id": "1701.04562", "contents": "Title: A Game-Theoretic Model for Analysis and Design of Self-Organization\n  Mechanisms in IoT Abstract: We propose a framework based on Network Formation Game for self-organization\nin the Internet of Things (IoT), in which heterogeneous and multi-interface\nnodes are modeled as self-interested agents who individually decide on\nestablishment and severance of links to other agents. Through analysis of the\nstatic game, we formally confirm the emergence of realistic topologies from our\nmodel, and analytically establish the criteria that lead to stable multi-hop\nnetwork structures. \n\n"}
{"id": "1701.04581", "contents": "Title: Modeling and Performance Studies of Data Communication Networks using\n  Dynamic Complex Networks Abstract: All the existing real world networks are evolving, hence, study of traffic\ndynamics in these enlarged networks is a challenging task. The critical issue\nis to optimize the network structure to improve network capacity and avoid\ntraffic congestion. We are interested in taking user's routes such that it is\nleast congested with optimal network capacity. Network capacity may be improved\neither by optimizing network topology or enhancing in routing approach. In this\ncontext, we propose and design a model of the time varying data communication\nnetworks (TVCN) based on the dynamics of in-flowing links. Newly appeared node\nprefers to attach with most influential node present in the network. In this\npaper, influence is termed as \\textit{reputation} and is applied for computing\noverall congestion at any node. User path with least betweenness centrality and\nmost reputation is preferred for routing. Kelly's optimization formulation for\na rate allocation problem is used for obtaining optimal rates of distinct users\nat different time instants and it is found that the user's path with lowest\nbetweenness centrality and highest reputation will always give maximum rate at\nstable point. \n\n"}
{"id": "1701.04673", "contents": "Title: HARE: Supporting efficient uplink multi-hop communications in\n  self-organizing LPWANs Abstract: The emergence of low-power wide area networks (LPWANs) as a new agent in the\nInternet of Things (IoT) will result in the incorporation into the digital\nworld of low-automated processes from a wide variety of sectors. The single-hop\nconception of typical LPWAN deployments, though simple and robust, overlooks\nthe self-organization capabilities of network devices, suffers from lack of\nscalability in crowded scenarios, and pays little attention to energy\nconsumption. Aimed to take the most out of devices' capabilities, the HARE\nprotocol stack is proposed in this paper as a new LPWAN technology flexible\nenough to adopt uplink multi-hop communications when proving energetically more\nefficient. In this way, results from a real testbed show energy savings of up\nto 15% when using a multi-hop approach while keeping the same network\nreliability. System's self-organizing capability and resilience have been also\nvalidated after performing numerous iterations of the association mechanism and\ndeliberately switching off network devices. \n\n"}
{"id": "1701.05007", "contents": "Title: IoTScanner: Detecting and Classifying Privacy Threats in IoT\n  Neighborhoods Abstract: In the context of the emerging Internet of Things (IoT), a proliferation of\nwireless connectivity can be expected. That ubiquitous wireless communication\nwill be hard to centrally manage and control, and can be expected to be opaque\nto end users. As a result, owners and users of physical space are threatened to\nlose control over their digital environments.\n  In this work, we propose the idea of an IoTScanner. The IoTScanner integrates\na range of radios to allow local reconnaissance of existing wireless\ninfrastructure and participating nodes. It enumerates such devices, identifies\nconnection patterns, and provides valuable insights for technical support and\nhome users alike. Using our IoTScanner, we attempt to classify actively\nstreaming IP cameras from other non-camera devices using simple heuristics. We\nshow that our classification approach achieves a high accuracy in an IoT\nsetting consisting of a large number of IoT devices. While related work usually\nfocuses on detecting either the infrastructure, or eavesdropping on traffic\nfrom a specific node, we focus on providing a general overview of operations in\nall observed networks. We do not assume prior knowledge of used SSIDs,\npreshared passwords, or similar. \n\n"}
{"id": "1701.05114", "contents": "Title: A geometrical imaging of the real gap between economies of China and the\n  United States Abstract: GDP of China is about 11 trillion dollars and GDP of the United States is\nabout 18 trillion dollars. Suppose that we know for the coming years, economy\nof the US will experience a real growth rate equal to \\%3 and economy of China\nwill experience a real growth as of \\%6. Now, the question is how long does it\ntake for economy of China to catch the economy of the United States. The early\nimpression is that the desired time is the answer of the equation\n$11\\times1.06^X=18\\times1.03^X$. The correct answer however is quite different.\nGDP is not a simple number and the gap between two countries can not be\naddressed simply through their sizes. It is rather a geometrical object.\nCountries pass different paths in the space of production. The gaps between GDP\nof different countries depend on the path that each country passes through and\nlocal metric. To address distance between economies of China and of the US we\nneed to know their utility preferences and the path that China passes to reach\nthe US size. The true gap then can be found if we calculate local metric along\nthis path. It resembles impressions about measurements in the General Theory of\nRelativity. Path dependency of aggregate indexes is widely discussed in the\nIndex Number Theory. Our aim is to stick to the geometrical view presented in\nthe General Relativity to provide a visual understanding of the matter. We show\nthat different elements in the general relativity have their own counterparts\nin economics. We claim that national agencies who provide aggregate data\nresemble falling observers into a curved space time. It is while the World Bank\nor international organizations are outside observers. The vision provided here,\nleaves readers with a clear conclusion. If China keeps its growth rate, then\nthe economy of China should catch the economy of the United States sooner than\nwhat we expect. \n\n"}
{"id": "1701.05711", "contents": "Title: Age-Optimal Information Updates in Multihop Networks Abstract: The problem of reducing the age-of-information has been extensively studied\nin the single-hop networks. In this paper, we minimize the age-of-information\nin general multihop networks. If the packet transmission times over the network\nlinks are exponentially distributed, we prove that a preemptive Last Generated\nFirst Served (LGFS) policy results in smaller age processes at all nodes of the\nnetwork (in a stochastic ordering sense) than any other causal policy. In\naddition, for arbitrary general distributions of packet transmission times, the\nnon-preemptive LGFS policy is shown to minimize the age processes at all nodes\nof the network among all non-preemptive work-conserving policies (again in a\nstochastic ordering sense). It is surprising that such simple policies can\nachieve optimality of the joint distribution of the age processes at all nodes\neven under arbitrary network topologies, as well as arbitrary packet generation\nand arrival times. These optimality results not only hold for the age\nprocesses, but also for any non-decreasing functional of the age processes. \n\n"}
{"id": "1701.06823", "contents": "Title: Graph Analytics for anomaly detection in homogeneous wireless networks -\n  A Simulation Approach Abstract: In the Internet of Things (IoT) devices are exposed to various kinds of\nattacks when connected to the Internet. An attack detection mechanism that\nunderstands the limitations of these severely resource-constrained devices is\nnecessary. This is important since current approaches are either customized for\nwireless networks or for the conventional Internet with heavy data\ntransmission. Also, the detection mechanism need not always be as\nsophisticated. Simply signaling that an attack is taking place may be enough in\nsome situations, for example in NIDS using anomaly detection. In graph\nnetworks, central nodes are the nodes that bear the most influence in the\nnetwork. The purpose of this research is to explore experimentally the\nrelationship between the behavior of central nodes and anomaly detection when\nan attack spreads through a network. As a result, we propose a novel anomaly\ndetection approach using this unique methodology which has been unexplored so\nfar in communication networks. Also, in the experiment, we identify presence of\nan attack originating and propagating throughout a network of IoT using our\nmethodology. \n\n"}
{"id": "1701.07153", "contents": "Title: Throughput Maximization for Wireless Powered Communications Harvesting\n  from Non-dedicated Sources Abstract: We consider the wireless powered communications where users harvest energy\nfrom non-dedicated sources. The user follows a harvest-then-transmit protocol:\nin first phase of a slot time the source node harvests energy from a nearby\nconventional Access Point, then transmit information to its destination node or\nrelay node in the second phase. We obtain the optimal\\textit{ harvesting ratio}\nto maximize the expected throughput for direct transmission (DT )and decode\nforward (DF) relay under outage constraint, respectively. Our results reveal\nthat the optimal harvest ratio for DT is dominated by the outage constraint\nwhile for DF relay, by the data causality . \n\n"}
{"id": "1701.07363", "contents": "Title: E2M2: Energy Efficient Mobility Management in Dense Small Cells with\n  Mobile Edge Computing Abstract: Merging mobile edge computing with the dense deployment of small cell base\nstations promises enormous benefits such as a real proximity, ultra-low latency\naccess to cloud functionalities. However, the envisioned integration creates\nmany new challenges and one of the most significant is mobility management,\nwhich is becoming a key bottleneck to the overall system performance. Simply\napplying existing solutions leads to poor performance due to the highly\noverlapped coverage areas of multiple base stations in the proximity of the\nuser and the co-provisioning of radio access and computing services. In this\npaper, we develop a novel user-centric mobility management scheme, leveraging\nLyapunov optimization and multi-armed bandits theories, in order to maximize\nthe edge computation performance for the user while keeping the user's\ncommunication energy consumption below a constraint. The proposed scheme\neffectively handles the uncertainties present at multiple levels in the system\nand provides both short-term and long-term performance guarantee. Simulation\nresults show that our proposed scheme can significantly improve the computation\nperformance (compared to state of the art) while satisfying the communication\nenergy constraint. \n\n"}
{"id": "1701.08680", "contents": "Title: Fog-Assisted wIoT: A Smart Fog Gateway for End-to-End Analytics in\n  Wearable Internet of Things Abstract: Today, wearable internet-of-things (wIoT) devices continuously flood the\ncloud data centers at an enormous rate. This increases a demand to deploy an\nedge infrastructure for computing, intelligence, and storage close to the\nusers. The emerging paradigm of fog computing could play an important role to\nmake wIoT more efficient and affordable. Fog computing is known as the cloud on\nthe ground. This paper presents an end-to-end architecture that performs data\nconditioning and intelligent filtering for generating smart analytics from\nwearable data. In wIoT, wearable sensor devices serve on one end while the\ncloud backend offers services on the other end. We developed a prototype of\nsmart fog gateway (a middle layer) using Intel Edison and Raspberry Pi. We\ndiscussed the role of the smart fog gateway in orchestrating the process of\ndata conditioning, intelligent filtering, smart analytics, and selective\ntransfer to the cloud for long-term storage and temporal variability\nmonitoring. We benchmarked the performance of developed prototypes on\nreal-world data from smart e-textile gloves. Results demonstrated the usability\nand potential of proposed architecture for converting the real-world data into\nuseful analytics while making use of knowledge-based models. In this way, the\nsmart fog gateway enhances the end-to-end interaction between wearables (sensor\ndevices) and the cloud. \n\n"}
{"id": "1701.09011", "contents": "Title: Overlay Routing for Fast Video Transfers in CDN Abstract: Content Delivery Networks (CDN) are witnessing the outburst of video\nstreaming (e.g., personal live streaming or Video-on-Demand) where the video\ncontent, produced or accessed by mobile phones, must be quickly transferred\nfrom a point to another of the network. Whenever a user requests a video not\ndirectly available at the edge server, the CDN network must 1) identify the\nbest location in the network where the content is stored, 2) set up a\nconnection and 3) deliver the video as quickly as possible. For this reason,\nexisting CDNs are adopting an overlay structure to reduce latency, leveraging\nthe flexibility introduced by the Software Defined Networking (SDN) paradigm.\nIn order to guarantee a satisfactory Quality of Experience (QoE) to users, the\nconnection must respect several Quality of Service (QoS) constraints. In this\npaper, we focus on the sub-problem 2), by presenting an approach to efficiently\ncompute and maintain paths in the overlay network. Our approach allows to speed\nup the transfer of video segments by finding minimum delay overlay paths under\nconstraints on hop count, jitter, packet loss and relay processing capacity.\nThe proposed algorithm provides a near-optimal solution, while drastically\nreducing the execution time. We show on traces collected in a real CDN that our\nsolution allows to maximize the number of fast video transfers. \n\n"}
{"id": "1702.00832", "contents": "Title: An Introduction to Deep Learning for the Physical Layer Abstract: We present and discuss several novel applications of deep learning for the\nphysical layer. By interpreting a communications system as an autoencoder, we\ndevelop a fundamental new way to think about communications system design as an\nend-to-end reconstruction task that seeks to jointly optimize transmitter and\nreceiver components in a single process. We show how this idea can be extended\nto networks of multiple transmitters and receivers and present the concept of\nradio transformer networks as a means to incorporate expert domain knowledge in\nthe machine learning model. Lastly, we demonstrate the application of\nconvolutional neural networks on raw IQ samples for modulation classification\nwhich achieves competitive accuracy with respect to traditional schemes relying\non expert features. The paper is concluded with a discussion of open challenges\nand areas for future investigation. \n\n"}
{"id": "1702.03741", "contents": "Title: Random walk based in-network computation of arbitrary functions Abstract: We study in-network computation on general network topologies. Specifically,\nwe are given the description of a function, and a network with distinct nodes\nat which the operands of the function are made available, and a designated sink\nwhere the computed value of the function is to be consumed. We want to compute\nthe function during the process of moving the data towards the sink. Such\nsettings have been studied in the literature, but mainly for symmetric\nfunctions, e.g. average, parity etc., which have the specific property that the\noutput is invariant to permutation of the operands. To the best of our\nknowledge, we present the first fully decentralised algorithms for arbitrary\nfunctions, which we model as those functions whose computation schema is\nstructured as a binary tree. We propose two algorithms, Fixed Random-Compute\nand Flexible Random-Compute, for this problem, both of which use simple random\nwalks on the network as their basic primitive. Assuming a stochastic model for\nthe generation of streams of data at each source, we provide a lower and an\nupper bound on the rate at which Fixed Random-Compute can compute the stream of\nassociated function values. Note that the lower bound on rate though computed\nfor our algorithm serves as a general lower bound for the function computation\nproblem and to the best of our knowledge is first such lower bound for\nasymmetric functions. We also provide upper bounds on the average time taken to\ncompute the function, characterising this time in terms of the fundamental\nparameters of the random walk on the network: the hitting time in the case of\nFixed Random-Compute, and the mixing time in the case of Flexible\nRandom-Compute. \n\n"}
{"id": "1702.04642", "contents": "Title: Prediction defaults for networked-guarantee loans Abstract: Networked-guarantee loans may cause the systemic risk related concern of the\ngovernment and banks in China. The prediction of default of enterprise loans is\na typical extremely imbalanced prediction problem, and the networked-guarantee\nmake this problem more difficult to solve. Since the guaranteed loan is a debt\nobligation promise, if one enterprise in the guarantee network falls into a\nfinancial crisis, the debt risk may spread like a virus across the guarantee\nnetwork, even lead to a systemic financial crisis. In this paper, we propose an\nimbalanced network risk diffusion model to forecast the enterprise default risk\nin a short future. Positive weighted k-nearest neighbors (p-wkNN) algorithm is\ndeveloped for the stand-alone case -- when there is no default contagious; then\na data-driven default diffusion model is integrated to further improve the\nprediction accuracy. We perform the empirical study on a real-world three-years\nloan record from a major commercial bank. The results show that our proposed\nmethod outperforms conventional credit risk methods in terms of AUC. In\nsummary, our quantitative risk evaluation model shows promising prediction\nperformance on real-world data, which could be useful to both regulators and\nstakeholders. \n\n"}
{"id": "1702.04943", "contents": "Title: Femto-Caching with Soft Cache Hits: Improving Performance through\n  Recommendation and Delivery of Related Content Abstract: Pushing popular content to cheap \"helper\" nodes (e.g., small cells) during\noff-peak hours has recently been proposed to cope with the increase in mobile\ndata traffic. User requests can be served locally from these helper nodes, if\nthe requested content is available in at least one of the nearby helpers.\nNevertheless, the collective storage of a few nearby helper nodes does not\nusually suffice to achieve a high enough hit rate in practice. We propose to\ndepart from the assumption of hard cache hits, common in existing works, and\nconsider \"soft\" cache hits, where if the original content is not available,\nsome related contents that are locally cached can be recommended instead. Given\nthat Internet content consumption is entertainment-oriented, we argue that\nthere exist scenarios where a user might accept an alternative content (e.g.,\nbetter download rate for alternative content, low rate plans, etc.), thus\navoiding to access expensive/congested links. We formulate the problem of\noptimal edge caching with soft cache hits in a relatively generic setup,\npropose efficient algorithms, and analyze the expected gains. We then show\nusing synthetic and real datasets of related video contents that promising\ncaching gains could be achieved in practice. \n\n"}
{"id": "1702.05031", "contents": "Title: Peak Transmission Rate Resilient Crosslayer Broadcast for Body Area\n  Networks Abstract: Wireless Body Area Networks (WBAN) open an interdisciplinary area within\nWireless Sensor Networks (WSN) research, with a tremendous impact in healthcare\narea where sensors are used to monitor, collect and transmit biological\nparameters of the human body. We propose the rst network-MAC cross-layer\nbroadcast protocol in WBAN. Our protocol, evaluated in the OMNET++ simulator\nenriched with realistic human body mobility models and channel models issued\nfrom the recent research on biomedical and health informatics, outperforms\nexisting at broadcast strategies in terms of percentage of covered nodes,\nenergy consumption and correct reception of causally-ordered packets (i.e.\npackets are received in the same order as they were sent by the sink).\nFurthermore, we investigate the resilience of both existing at broadcast\nstrategies and our new protocol face to various transmission rates and human\nbody mobility. Existing at broadcast strategies without exception start to have\na drastic drop of performances for transmission rates above 11Kb/s while our\ncross-layer protocol performances maintains its good performances for\ntransmission rates up to 190Kb/s. \n\n"}
{"id": "1702.05197", "contents": "Title: Throughput-Optimal Broadcast in Wireless Networks with\n  Point-to-Multipoint Transmissions Abstract: We consider the problem of efficient packet dissemination in wireless\nnetworks with point-to-multi-point wireless broadcast channels. We propose a\ndynamic policy, which achieves the broadcast capacity of the network. This\npolicy is obtained by first transforming the original multi-hop network into a\nprecedence-relaxed virtual single-hop network and then finding an optimal\nbroadcast policy for the relaxed network. The resulting policy is shown to be\nthroughput-optimal for the original wireless network using a sample-path\nargument. We also prove the NP-completeness of the finite-horizon broadcast\nproblem, which is in contrast with the polynomial time solvability of the\nproblem with point-to-point channels. Illustrative simulation results\ndemonstrate the efficacy of the proposed broadcast policy in achieving the full\nbroadcast capacity with low delay. \n\n"}
{"id": "1702.06645", "contents": "Title: Resource Sharing Among mmWave Cellular Service Providers in a Vertically\n  Differentiated Duopoly Abstract: With the increasing interest in the use of millimeter wave bands for 5G\ncellular systems comes renewed interest in resource sharing. Properties of\nmillimeter wave bands such as massive bandwidth, highly directional antennas,\nhigh penetration loss, and susceptibility to shadowing, suggest technical\nadvantages to spectrum and infrastructure sharing in millimeter wave cellular\nnetworks. However, technical advantages do not necessarily translate to\nincreased profit for service providers, or increased consumer surplus. In this\npaper, detailed network simulations are used to better understand the economic\nimplications of resource sharing in a vertically differentiated duopoly market\nfor cellular service. The results suggest that resource sharing is less often\nprofitable for millimeter wave service providers compared to microwave cellular\nservice providers, and does not necessarily increase consumer surplus. \n\n"}
{"id": "1702.06772", "contents": "Title: Efficient CSMA using Regional Free Energy Approximations Abstract: CSMA (Carrier Sense Multiple Access) algorithms based on Gibbs sampling can\nachieve throughput optimality if certain parameters called the fugacities are\nappropriately chosen. However, the problem of computing these fugacities is\nNP-hard. In this work, we derive estimates of the fugacities by using a\nframework called the regional free energy approximations. In particular, we\nderive explicit expressions for approximate fugacities corresponding to any\nfeasible service rate vector. We further prove that our approximate fugacities\nare exact for the class of chordal graphs. A distinguishing feature of our work\nis that the regional approximations that we propose are tailored to conflict\ngraphs with small cycles, which is a typical characteristic of wireless\nnetworks. Numerical results indicate that the fugacities obtained by the\nproposed method are quite accurate and significantly outperform the existing\nBethe approximation based techniques. \n\n"}
{"id": "1702.07151", "contents": "Title: Replication of Virtual Network Functions: Optimizing Link Utilization\n  and Resource Costs Abstract: Network Function Virtualization (NFV) is enabling the softwarization of\ntraditional network services, commonly deployed in dedicated hardware, into\ngeneric hardware in form of Virtual Network Functions (VNFs), which can be\nlocated flexibly in the network. However, network load balancing can be\ncritical for an ordered sequence of VNFs, also known as Service Function Chains\n(SFCs), a common cloud and network service approach today. The placement of\nthese chained functions increases the ping-pong traffic between VNFs, directly\naffecting to the efficiency of bandwidth utilization. The optimization of the\nplacement of these VNFs is a challenge as also other factors need to be\nconsidered, such as the resource utilization. To address this issue, we study\nthe problem of VNF placement with replications, and especially the potential of\nVNFs replications to help load balance the network, while the server\nutilization is minimized. In this paper we present a Linear Programming (LP)\nmodel for the optimum placement of functions finding a trade-off between the\nminimization of two objectives: the link utilization and CPU resource usage.\nThe results show how the model load balance the utilization of all links in the\nnetwork using minimum resources. \n\n"}
{"id": "1702.08395", "contents": "Title: Backhaul-aware Robust 3D Drone Placement in 5G+ Wireless Networks Abstract: Using drones as flying base stations is a promising approach to enhance the\nnetwork coverage and area capacity by moving supply towards demand when\nrequired. However deployment of such base stations can face some restrictions\nthat need to be considered. One of the limitations in drone base stations\n(drone-BSs) deployment is the availability of reliable wireless backhaul link.\nThis paper investigates how different types of wireless backhaul offering\nvarious data rates would affect the number of served users. Two approaches,\nnamely, network-centric and user-centric, are introduced and the optimal 3D\nbackhaul-aware placement of a drone-BS is found for each approach. To this end,\nthe total number of served users and sum-rates are maximized in the\nnetwork-centric and user-centric frameworks, respectively. Moreover, as it is\npreferred to decrease drone-BS movements to save more on battery and increase\nflight time and to reduce the channel variations, the robustness of the network\nis examined as how sensitive it is with respect to the users displacements. \n\n"}
{"id": "1703.00123", "contents": "Title: DTNC: A New Server-side Data Cleansing Framework for Cellular Trajectory\n  Services Abstract: It is essential for the cellular network operators to provide cellular\nlocation services to meet the needs of their users and mobile applications.\nHowever, cellular locations, estimated by network-based methods at the\nserver-side, bear with {\\it high spatial errors} and {\\it arbitrary missing\nlocations}. Moreover, auxiliary sensor data at the client-side are not\navailable to the operators. In this paper, we study the {\\em cellular\ntrajectory cleansing problem} and propose an innovative data cleansing\nframework, namely \\underline{D}ynamic \\underline{T}ransportation\n\\underline{N}etwork based \\underline{C}leansing (DTNC) to improve the quality\nof cellular locations delivered in online cellular trajectory services. We\nmaintain a dynamic transportation network (DTN), which associates a network\nedge with a probabilistic distribution of travel times updated continuously. In\naddition, we devise an object motion model, namely, {\\em travel-time-aware\nhidden semi-Markov model} ({\\em TT-HsMM}), which is used to infer the most\nprobable traveled edge sequences on DTN. To validate our ideas, we conduct a\ncomprehensive evaluation using real-world cellular data provided by a major\ncellular network operator and a GPS dataset collected by smartphones as the\nground truth. In the experiments, DTNC displays significant advantages over six\nstate-of-the-art techniques. \n\n"}
{"id": "1703.00134", "contents": "Title: Collision Resolution and Interference Elimination in Multiaccess\n  Communication Networks Abstract: We define a multiaccess communication scheme that effectively eliminates\ninterference and resolves collisions in many-to-one and many-to-many\ncommunication scenarios. Each transmitter is uniquely identified by a steering\nvector. All signals issued from a specific transmitter will be steered into the\nsame single-dimensional or double-dimensional subspace at all receivers hearing\nthis transmission. This subspace is orthogonal to the noise subspace at a\nreceiver and the signals within the subspace can be extracted using the\nroot-MUSIC method. At high SNR, local channel knowledge and strict\nsynchronization, the algorithm asymptotically achieves full network capacity on\ncondition that a channel remains constant within a single time slot. Without\nsynchronization, the worst case asymptotic performance is still greater than\nthe $50\\%$ throughput achieved by collision resolution algorithms and\ninterference management techniques like interference alignment. \n\n"}
{"id": "1703.00731", "contents": "Title: Local Voting: Optimal Distributed Node Scheduling Algorithm for Multihop\n  Wireless Networks Abstract: An efficient and fair node scheduling is a big challenge in multihop wireless\nnetworks. In this work, we propose a distributed node scheduling algorithm,\ncalled Local Voting. The idea comes from the finding that the shortest delivery\ntime or delay is obtained when the load is equalized throughout the network.\nSimulation results demonstrate that Local Voting achieves better performance in\nterms of average delay, maximum delay, and fairness compared to several\nrepresentative scheduling algorithms from the literature. Despite being\ndistributed, Local Voting has a very close performance to a centralized\nalgorithm that is considered to have the optimal performance. \n\n"}
{"id": "1703.01038", "contents": "Title: Ultra-Dense Edge Caching under Spatio-Temporal Demand and Network\n  Dynamics Abstract: This paper investigates a cellular edge caching design under an extremely\nlarge number of small base stations (SBSs) and users. In this ultra-dense edge\ncaching network (UDCN), SBS-user distances shrink, and each user can request a\ncached content from multiple SBSs. Unfortunately, the complexity of existing\ncaching controls' mechanisms increases with the number of SBSs, making them\ninapplicable for solving the fundamental caching problem: How to maximize local\ncaching gain while minimizing the replicated content caching? Furthermore,\nspatial dynamics of interference is no longer negligible in UDCNs due to the\nsurge in interference. In addition, the caching control should consider\ntemporal dynamics of user demands. To overcome such difficulties, we propose a\nnovel caching algorithm weaving together notions of mean-field game theory and\nstochastic geometry. These enable our caching algorithm to become independent\nof the number of SBSs and users, while incorporating spatial interference\ndynamics as well as temporal dynamics of content popularity and storage\nconstraints. Numerical evaluation validates the fact that the proposed\nalgorithm reduces not only the long run average cost by at least 24% but also\nthe number of replicated content by 56% compared to a popularity-based\nalgorithm. \n\n"}
{"id": "1703.02149", "contents": "Title: Decentralized Consistent Network Updates in SDN with ez-Segway Abstract: We present ez-Segway, a decentralized mechanism to consistently and quickly\nupdate the network state while preventing forwarding anomalies (loops and\nblack-holes) and avoiding link congestion. In our design, the centralized SDN\ncontroller only pre-computes information needed by the switches during the\nupdate execution. This information is distributed to the switches, which use\npartial knowledge and direct message passing to efficiently realize the update.\nThis separation of concerns has the key benefit of improving update performance\nas the communication and computation bottlenecks at the controller are removed.\nOur evaluations via network emulations and large-scale simulations demonstrate\nthe efficiency of ez-Segway, which compared to a centralized approach, improves\nnetwork update times by up to 45% and 57% at the median and the 99th\npercentile, respectively. A deployment of a system prototype in a real OpenFlow\nswitch and an implementation in P4 demonstrate the feasibility and low overhead\nof implementing simple network update functionality within switches. \n\n"}
{"id": "1703.05174", "contents": "Title: On the Connectivity Problem of ETSI DCC Algorithm Abstract: The intelligent transportation systems (ITS) framework from European\nTelecommunication Standards Institute (ETSI) imposes requirements on the\nexchange of periodic safety messages between components of ITS such as\nvehicles. In particular, it requires ETSI standardized Decentralized Congestion\nControl (DCC) algorithm to regulate the beaconing activity of vehicles based on\nwireless channel utilization. However, the DCC state that defines the beaconing\nbehavior under heavy channel congestion, i.e., the Restrictive state, has a\nserious connectivity problem that safety beacons do not reach other vehicles in\nsafety-critical distances. In this paper, we demonstrate the problem through\nanalysis, simulation, and on-road measurements. We suggest that DCC change the\ntransmit power setting for the Restrictive state before a full-scale deployment\nof the ETSI ITS framework starts, and we discuss its consequences in terms of\nchanges in communicability and channel utilization. \n\n"}
{"id": "1703.06568", "contents": "Title: Evaluating the Stream Control Transmission Protocol Using Uppaal Abstract: The Stream Control Transmission Protocol (SCTP) is a Transport Layer protocol\nthat has been proposed as an alternative to the Transmission Control Protocol\n(TCP) for the Internet of Things (IoT). SCTP, with its four-way handshake\nmechanism, claims to protect the Server from a Denial-of-Service (DoS) attack\nby ensuring the legitimacy of the Client, which has been a known issue\npertaining to the three-way handshake of TCP. This paper compares the\nhandshakes of TCP and SCTP to discuss its shortcomings and strengths. We\npresent an Uppaal model of the TCP three-way handshake and SCTP four-way\nhandshake and show that SCTP is able to cope with the presence of an\nIllegitimate Client, while TCP fails. The results confirm that SCTP is better\nequipped to deal with this type of attack. \n\n"}
{"id": "1703.08109", "contents": "Title: Cayley graphs and symmetric interconnection networks Abstract: These lecture notes are on automorphism groups of Cayley graphs and their\napplications to optimal fault-tolerance of some interconnection networks. We\nfirst give an introduction to automorphisms of graphs and an introduction to\nCayley graphs. We then discuss automorphism groups of Cayley graphs. We prove\nthat the vertex-connectivity of edge-transitive graphs is maximum possible. We\ninvestigate the automorphism group and vertex-connectivity of some families of\nCayley graphs that have been considered for interconnection networks; we focus\non the hypercubes, folded hypercubes, Cayley graphs generated by\ntranspositions, and Cayley graphs from linear codes. New questions and open\nproblems are also discussed. \n\n"}
{"id": "1703.08985", "contents": "Title: TCP in 5G mmWave Networks: Link Level Retransmissions and MP-TCP Abstract: MmWave communications, one of the cornerstones of future 5G mobile networks,\nare characterized at the same time by a potential multi-gigabit capacity and by\na very dynamic channel, sensitive to blockage, wide fluctuations in the\nreceived signal quality, and possibly also sudden link disruption. While the\nperformance of physical and MAC layer schemes that address these issues has\nbeen thoroughly investigated in the literature, the complex interactions\nbetween mmWave links and transport layer protocols such as TCP are still\nrelatively unexplored. This paper uses the ns-3 mmWave module, with its channel\nmodel based on real measurements in New York City, to analyze the performance\nof the Linux TCP/IP stack (i) with and without link-layer retransmissions,\nshowing that they are fundamental to reach a high TCP throughput on mmWave\nlinks and (ii) with Multipath TCP (MP-TCP) over multiple LTE and mmWave links,\nillustrating which are the throughput-optimal combinations of secondary paths\nand congestion control algorithms in different conditions. \n\n"}
{"id": "1703.09025", "contents": "Title: Service Overlay Forest Embedding for Software-Defined Cloud Networks Abstract: Network Function Virtualization (NFV) on Software-Defined Networks (SDN) can\neffectively optimize the allocation of Virtual Network Functions (VNFs) and the\nrouting of network flows simultaneously. Nevertheless, most previous studies on\nNFV focus on unicast service chains and thereby are not scalable to support a\nlarge number of destinations in multicast. On the other hand, the allocation of\nVNFs has not been supported in the current SDN multicast routing algorithms. In\nthis paper, therefore, we make the first attempt to tackle a new challenging\nproblem for finding a service forest with multiple service trees, where each\ntree contains multiple VNFs required by each destination. Specifically, we\nformulate a new optimization, named Service Overlay Forest (SOF), to minimize\nthe total cost of all allocated VNFs and all multicast trees in the forest. We\ndesign a new $3\\rho_{ST}$-approximation algorithm to solve the problem, where\n$\\rho_{ST}$ denotes the best approximation ratio of the Steiner Tree problem,\nand the distributed implementation of the algorithm is also presented.\nSimulation results on real networks for data centers manifest that the proposed\nalgorithm outperforms the existing ones by over 25%. Moreover, the\nimplementation of an experimental SDN with HP OpenFlow switches indicates that\nSOF can significantly improve the QoE of the Youtube service. \n\n"}
{"id": "1703.10750", "contents": "Title: A Survey on Mobile Edge Networks: Convergence of Computing, Caching and\n  Communications Abstract: As the explosive growth of smart devices and the advent of many new\napplications, traffic volume has been growing exponentially. The traditional\ncentralized network architecture cannot accommodate such user demands due to\nheavy burden on the backhaul links and long latency. Therefore, new\narchitectures which bring network functions and contents to the network edge\nare proposed, i.e., mobile edge computing and caching. Mobile edge networks\nprovide cloud computing and caching capabilities at the edge of cellular\nnetworks. In this survey, we make an exhaustive review on the state-of-the-art\nresearch efforts on mobile edge networks. We first give an overview of mobile\nedge networks including definition, architecture and advantages. Next, a\ncomprehensive survey of issues on computing, caching and communication\ntechniques at the network edge is presented respectively. The applications and\nuse cases of mobile edge networks are discussed. Subsequently, the key enablers\nof mobile edge networks such as cloud technology, SDN/NFV and smart devices are\ndiscussed. Finally, open research challenges and future directions are\npresented as well. \n\n"}
{"id": "1704.00616", "contents": "Title: Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance\n  for Action Classification and Detection Abstract: General human action recognition requires understanding of various visual\ncues. In this paper, we propose a network architecture that computes and\nintegrates the most important visual cues for action recognition: pose, motion,\nand the raw images. For the integration, we introduce a Markov chain model\nwhich adds cues successively. The resulting approach is efficient and\napplicable to action classification as well as to spatial and temporal action\nlocalization. The two contributions clearly improve the performance over\nrespective baselines. The overall approach achieves state-of-the-art action\nclassification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover,\nit yields state-of-the-art spatio-temporal action localization results on\nUCF101 and J-HMDB. \n\n"}
{"id": "1704.00890", "contents": "Title: Analysis of Device-to-Device Communications in Uplink Cellular Networks\n  with Lognormal Fading Abstract: In this paper, using the stochastic geometry theory, we present a framework\nfor analyzing the performance of device-to-device (D2D) communications\nunderlaid uplink (UL) cellular networks. In our analysis, we consider a D2D\nmode selection criterion based on an energy threshold for each user equipment\n(UE). Specifically, a UE will operate in a cellular mode, if its received\nsignal strength from the strongest base station (BS) is large than a threshold\n\\beta. Otherwise, it will operate in a D2D mode. Furthermore, we consider a\ngeneralized log-normal shadowing in our analysis. The coverage probability and\nthe area spectral efficiency (ASE) are derived for both the cellular network\nand the D2D one. Through our theoretical and numerical analyses, we quantify\nthe performance gains brought by D2D communications and provide guidelines of\nselecting the parameters for network operations. \n\n"}
{"id": "1704.01179", "contents": "Title: The Wandering of Corn Abstract: Time and Sales of corn futures traded electronically on the CME Group Globex\nare studied. Theories of continuous prices turn upside down reality of\nintra-day trading. Prices and their increments are discrete and obey lattice\nprobability distributions. A function for systematic evolution of futures\ntrading volume is proposed. Dependence between sample skewness and kurtosis of\nwaiting times does not support hypothesis of Weibull distribution. Kumaraswamy\ndistribution is more suitable for waiting times. Relationships between trading\nvolume and maximum profit strategies are presented. Frequencies of absolute\nb-increments are approximated by a Hurwitz Zeta distribution. Relative\nb-increments are non-Gaussian too. Dependence between b- and a-increments\nallows to interpret the sample variances of b-increments as a stochastic\nprocess. Mean sample variance of b-increments vs. a-increments is presented.\nThe L1 distance and Log-likelihood statistics for independence between a- and\nb-increments are controversial. Corn price jumps remind of chain branching\nreactions. Bi-logarithmic plots of the empirical frequencies of extreme\nb-increments vs. ranks are presented. Corresponding distributions resemble\nsnakes forked tongues. The maximum profit strategy is discussed as a measure of\nnon-equilibrium. \n\n"}
{"id": "1704.02790", "contents": "Title: Performance Analysis of Reliable Video Streaming with Strict Playout\n  Deadline in Multi-Hop Wireless Networks Abstract: Motivated by emerging vision-based intelligent services, we consider the\nproblem of rate adaptation for high quality and low delay visual information\ndelivery over wireless networks using scalable video coding. Rate adaptation in\nthis setting is inherently challenging due to the interplay between the\nvariability of the wireless channels, the queuing at the network nodes and the\nframe-based decoding and playback of the video content at the receiver at very\nshort time scales. To address the problem, we propose a low-complexity,\nmodel-based rate adaptation algorithm for scalable video streaming systems,\nbuilding on a novel performance model based on stochastic network calculus. We\nvalidate the model using extensive simulations. We show that it allows fast,\nnear optimal rate adaptation for fixed transmission paths, as well as\ncross-layer optimized routing and video rate adaptation in mesh networks, with\nless than $10$\\% quality degradation compared to the best achievable\nperformance. \n\n"}
{"id": "1704.03704", "contents": "Title: Full-Duplex Device-to-Device Collaboration for Low-Latency Wireless\n  Video Distribution Abstract: Growing demand for video services is the main driver for increasing traffic\nin wireless cellular data networks. Wireless video distribution schemes have\nrecently been proposed to offload data via Device-to-Device (D2D)\ncommunications. These offloading schemes increase capacity and reduce\nend-to-end delay in cellular networks and help to serve the dramatically\nincreasing demand for high quality video. In this paper, we propose a new\nscheme for video distribution over cellular networks by exploiting full-duplex\n(FD) D2D communication in two scenarios; scenario one: two nodes exchange their\ndesired video files simultaneously with each other, and scenario two: each node\ncan concurrently transmit to and receive from two different nodes. In the\nlatter case, an intermediate transceiver can serve one or multiple users' file\nrequests whilst capturing its desired file from another device in the vicinity.\nAnalytic and simulation results are used to compare the proposed scheme with\nits half-duplex (HD) counterpart under the same transmitter establishment\ncriteria to show the achievable gain of FD-D2D scheme in video content\ndelivery, in terms of sum throughput and latency. \n\n"}
{"id": "1704.04146", "contents": "Title: Traffic Minimizing Caching and Latent Variable Distributions of Order\n  Statistics Abstract: Given a statistical model for the request frequencies and sizes of data\nobjects in a caching system, we derive the probability density of the size of\nthe file that accounts for the largest amount of data traffic. This is\nequivalent to finding the required size of the cache for a caching placement\nthat maximizes the expected byte hit ratio for given file size and popularity\ndistributions. The file that maximizes the expected byte hit ratio is the file\nfor which the product of its size and popularity is the highest -- thus, it is\nthe file that incurs the greatest load on the network. We generalize this\ntheoretical problem to cover factors and addends of arbitrary order statistics\nfor given parent distributions. Further, we study the asymptotic behavior of\nthese distributions. We give several factor and addend densities of widely-used\ndistributions, and verify our results by extensive computer simulations. \n\n"}
{"id": "1704.04595", "contents": "Title: Exploiting Non-Causal CPU-State Information for Energy-Efficient Mobile\n  Cooperative Computing Abstract: Scavenging the idling computation resources at the enormous number of mobile\ndevices can provide a powerful platform for local mobile cloud computing. The\nvision can be realized by peer-to-peer cooperative computing between edge\ndevices, referred to as co-computing. This paper considers a co-computing\nsystem where a user offloads computation of input-data to a helper. The helper\ncontrols the offloading process for the objective of minimizing the user's\nenergy consumption based on a predicted helper's CPU-idling profile that\nspecifies the amount of available computation resource for co-computing.\nConsider the scenario that the user has one-shot input-data arrival and the\nhelper buffers offloaded bits. The problem for energy-efficient co-computing is\nformulated as two sub-problems: the slave problem corresponding to adaptive\noffloading and the master one to data partitioning. Given a fixed offloaded\ndata size, the adaptive offloading aims at minimizing the energy consumption\nfor offloading by controlling the offloading rate under the deadline and buffer\nconstraints. By deriving the necessary and sufficient conditions for the\noptimal solution, we characterize the structure of the optimal policies and\npropose algorithms for computing the policies. Furthermore, we show that the\nproblem of optimal data partitioning for offloading and local computing at the\nuser is convex, admitting a simple solution using the sub-gradient method.\nLast, the developed design approach for co-computing is extended to the\nscenario of bursty data arrivals at the user accounting for data causality\nconstraints. Simulation results verify the effectiveness of the proposed\nalgorithms. \n\n"}
{"id": "1704.08535", "contents": "Title: TFDASH: A Fairness, Stability, and Efficiency Aware Rate Control\n  Approach for Multiple Clients over DASH Abstract: Dynamic adaptive streaming over HTTP (DASH) has recently been widely deployed\nin the Internet and adopted in the industry. It, however, does not impose any\nadaptation logic for selecting the quality of video fragments requested by\nclients and suffers from lackluster performance with respect to a number of\ndesirable properties: efficiency, stability, and fairness when multiple players\ncompete for a bottleneck link. In this paper, we propose a throughput-friendly\nDASH (TFDASH) rate control scheme for video streaming with multiple clients\nover DASH to well balance the trade-offs among efficiency, stability, and\nfairness. The core idea behind guaranteeing fairness and high efficiency\n(bandwidth utilization) is to avoid OFF periods during the downloading process\nfor all clients, i.e., the bandwidth is in perfect-subscription or\nover-subscription with bandwidth utilization approach to 100\\%. We also propose\na dual-threshold buffer model to solve the instability problem caused by the\nabove idea. As a result, by integrating these novel components, we also propose\na probability-driven rate adaption logic taking into account several key\nfactors that most influence visual quality, including buffer occupancy, video\nplayback quality, video bit-rate switching frequency and amplitude, to\nguarantee high-quality video streaming. Our experiments evidently demonstrate\nthe superior performance of the proposed method. \n\n"}
{"id": "1705.00055", "contents": "Title: Charting the Complexity Landscape of Waypoint Routing Abstract: Modern computer networks support interesting new routing models in which\ntraffic flows from a source s to a destination t can be flexibly steered\nthrough a sequence of waypoints, such as (hardware) middleboxes or\n(virtualized) network functions, to create innovative network services like\nservice chains or segment routing. While the benefits and technological\nchallenges of providing such routing models have been articulated and studied\nintensively over the last years, much less is known about the underlying\nalgorithmic traffic routing problems. This paper shows that the waypoint\nrouting problem features a deep combinatorial structure, and we establish\ninteresting connections to several classic graph theoretical problems. We find\nthat the difficulty of the waypoint routing problem depends on the specific\nsetting, and chart a comprehensive landscape of the computational complexity.\nIn particular, we derive several NP-hardness results, but we also demonstrate\nthat exact polynomial-time algorithms exist for a wide range of practically\nrelevant scenarios. \n\n"}
{"id": "1705.00462", "contents": "Title: Spectrum Monitoring for Radar Bands using Deep Convolutional Neural\n  Networks Abstract: In this paper, we present a spectrum monitoring framework for the detection\nof radar signals in spectrum sharing scenarios. The core of our framework is a\ndeep convolutional neural network (CNN) model that enables Measurement Capable\nDevices to identify the presence of radar signals in the radio spectrum, even\nwhen these signals are overlapped with other sources of interference, such as\ncommercial LTE and WLAN. We collected a large dataset of RF measurements, which\ninclude the transmissions of multiple radar pulse waveforms, downlink LTE,\nWLAN, and thermal noise. We propose a pre-processing data representation that\nleverages the amplitude and phase shifts of the collected samples. This\nrepresentation allows our CNN model to achieve a classification accuracy of\n99.6% on our testing dataset. The trained CNN model is then tested under\nvarious SNR values, outperforming other models, such as spectrogram-based CNN\nmodels. \n\n"}
{"id": "1705.02777", "contents": "Title: D2D-Based Grouped Random Access to Mitigate Mobile Access Congestion in\n  5G Sensor Networks Abstract: The Fifth Generation (5G) wireless service of sensor networks involves\nsignificant challenges when dealing with the coordination of ever-increasing\nnumber of devices accessing shared resources. This has drawn major interest\nfrom the research community as many existing works focus on the radio access\nnetwork congestion control to efficiently manage resources in the context of\ndevice-to-device (D2D) interaction in huge sensor networks. In this context,\nthis paper pioneers a study on the impact of D2D link reliability in\ngroup-assisted random access protocols, by shedding the light on beneficial\nperformance and potential limitations of approaches of this kind against\ntunable parameters such as group size, number of sensors and reliability of D2D\nlinks. Additionally, we leverage on the association with a Geolocation Database\n(GDB) capability to assist the grouping decisions by drawing parallels with\nrecent regulatory-driven initiatives around GDBs and arguing benefits of the\nsuggested proposal. Finally, the proposed method is approved to significantly\nreduce the delay over random access channels, by means of an exhaustive\nsimulation campaign. \n\n"}
{"id": "1705.03304", "contents": "Title: A Distributed Approach for Networked Flying Platform Association with\n  Small Cells in 5G+ Networks Abstract: The densification of small-cell base stations in a 5G architecture is a\npromising approach to enhance the coverage area and facilitate the ever\nincreasing capacity demand of end users. However, the bottleneck is an\nintelligent management of a backhaul/fronthaul network for these small-cell\nbase stations. This involves efficient association and placement of the\nbackhaul hubs that connects these small-cells with the core network.\nTerrestrial hubs suffer from an inefficient non line of sight link limitations\nand unavailability of a proper infrastructure in an urban area. Seeing the\npopularity of flying platforms, we employ here an idea of using networked\nflying platform (NFP) such as unmanned aerial vehicles (UAVs), drones, unmanned\nballoons flying at different altitudes, as aerial backhaul hubs. The\nassociation problem of these NFP-hubs and small-cell base stations is\nformulated considering backhaul link and NFP related limitations such as\nmaximum number of supported links and bandwidth. Then, this paper presents an\nefficient and distributed solution of the designed problem, which performs a\ngreedy search in order to maximize the sum rate of the overall network. A\nfavorable performance is observed via a numerical comparison of our proposed\nmethod with optimal exhaustive search algorithm in terms of sum rate and\nrun-time speed. \n\n"}
{"id": "1705.03848", "contents": "Title: Propensity to spending of an average consumer over a brief period Abstract: Understanding consumption dynamics and its impact on the whole economy and\nwelfare within the present economic crisis is not an easy task. Indeed the\nlevel of consumer demand for different goods varies with the prices, consumer\nincomes and demographic factors. Furthermore crisis may trigger different\nbehaviors which result in distortions and amplification effects. In the present\nwork we propose a simple model to quantitatively describe the time evolution\nover a brief period of the amount of money an average consumer decides to\nspend, depending on his/her available budget. A simple hydrodynamical analog of\nthe model is discussed. Finally, perspectives of this work are briefly\noutlined. \n\n"}
{"id": "1705.04289", "contents": "Title: Joint Spectrum Allocation and Structure Optimization in Green Powered\n  Heterogeneous Cognitive Radio Networks Abstract: We aim at maximizing the sum rate of secondary users (SUs) in OFDM-based\nHeterogeneous Cognitive Radio (CR) Networks using RF energy harvesting.\nAssuming SUs operate in a time switching fashion, each time slot is partitioned\ninto three non-overlapping parts devoted for energy harvesting, spectrum\nsensing and data transmission. The general problem of joint resource allocation\nand structure optimization is formulated as a Mixed Integer Nonlinear\nProgramming task which is NP-hard and intractable. Thus, we propose to tackle\nit by decomposing it into two subproblems. We first propose a sub-channel\nallocation scheme to approximately satisfy SUs' rate requirements and remove\nthe integer constraints. For the second step, we prove that the general\noptimization problem is reduced to a convex optimization task. Considering the\ntrade-off among fractions of each time slot, we focus on optimizing the time\nslot structures of SUs that maximize the total throughput while guaranteeing\nthe rate requirements of both real-time and non-real-time SUs. Since the\nreduced optimization problem does not have a simple closed-form solution, we\nthus propose a near optimal closed-form solution by utilizing Lambert-W\nfunction. We also exploit iterative gradient method based on Lagrangian dual\ndecomposition to achieve near optimal solutions. Simulation results are\npresented to validate the optimality of the proposed schemes. \n\n"}
{"id": "1705.04527", "contents": "Title: sOFTDP: Secure and Efficient Topology Discovery Protocol for SDN Abstract: Topology discovery is one of the most critical tasks of Software-Defined\nNetwork (SDN) controllers. Current SDN controllers use the OpenFlow Discovery\nProtocol (OFDP) as the de-facto protocol for discovering the underlying network\ntopology. In a previous work, we have shown the functional, performance and\nsecurity limitations of OFDP. In this paper, we introduce and detail a novel\nprotocol called secure and efficient OpenFlow Discovery Protocol sOTDP. sOFTDP\nrequires minimal changes to OpenFlow switch design, eliminates major\nvulnerabilities in the topology discovery process and improves its performance.\nWe have implemented sOFTDP as a topology discovery module in Floodlight for\nevaluation. The results show that our implementation is more secure than OFDP\nand previous security workarounds. Also, sOFTDP reduces the topology discovery\ntime several orders of magnitude compared to the original OFDP and existing\nOFDP improvements. \n\n"}
{"id": "1705.05899", "contents": "Title: Scalability analysis of large-scale LoRaWAN networks in ns-3 Abstract: As LoRaWAN networks are actively being deployed in the field, it is important\nto comprehend the limitations of this Low Power Wide Area Network technology.\nPrevious work has raised questions in terms of the scalability and capacity of\nLoRaWAN networks as the number of end devices grows to hundreds or thousands\nper gateway. Some works have modeled LoRaWAN networks as pure ALOHA networks,\nwhich fails to capture important characteristics such as the capture effect and\nthe effects of interference. Other works provide a more comprehensive model by\nrelying on empirical and stochastic techniques. This work uses a different\napproach where a LoRa error model is constructed from extensive complex\nbaseband bit error rate simulations and used as an interference model. The\nerror model is combined with the LoRaWAN MAC protocol in an ns-3 module that\nenables to study multi channel, multi spreading factor, multi gateway,\nbi-directional LoRaWAN networks with thousands of end devices. Using the\nlorawan ns-3 module, a scalability analysis of LoRaWAN shows the detrimental\nimpact of downstream traffic on the delivery ratio of confirmed upstream\ntraffic. The analysis shows that increasing gateway density can ameliorate but\nnot eliminate this effect, as stringent duty cycle requirements for gateways\ncontinue to limit downstream opportunities. \n\n"}
{"id": "1705.05953", "contents": "Title: LoRa Backscatter: Enabling The Vision of Ubiquitous Connectivity Abstract: The vision of embedding connectivity into billions of everyday objects runs\ninto the reality of existing communication technologies --- there is no\nexisting wireless technology that can provide reliable and long-range\ncommunication at tens of microwatts of power as well as cost less than a dime.\nWhile backscatter is low-power and low-cost, it is known to be limited to short\nranges. This paper overturns this conventional wisdom about backscatter and\npresents the first wide-area backscatter system. Our design can successfully\nbackscatter from any location between an RF source and receiver, separated by\n475 m, while being compatible with commodity LoRa hardware. Further, when our\nbackscatter device is co-located with the RF source, the receiver can be as far\nas 2.8 km away. We deploy our system in a 4,800 $ft^{2}$ (446 $m^{2}$) house\nspread across three floors, a 13,024 $ft^{2}$ (1210 $m^{2}$) office area\ncovering 41 rooms, as well as a one-acre (4046 $m^{2}$) vegetable farm and show\nthat we can achieve reliable coverage, using only a single RF source and\nreceiver. We also build a contact lens prototype as well as a flexible\nepidermal patch device attached to the human skin. We show that these devices\ncan reliably backscatter data across a 3,328 $ft^{2}$ (309 $m^{2}$) room.\nFinally, we present a design sketch of a LoRa backscatter IC that shows that it\ncosts less than a dime at scale and consumes only 9.25 $\\mu$W of power, which\nis more than 1000x lower power than LoRa radio chipsets. \n\n"}
{"id": "1705.05954", "contents": "Title: Convergence Results on Pulse Coupled Oscillator Protocols in Locally\n  Connected Networks Abstract: This work provides new insights on the convergence of a locally connected\nnetwork of pulse coupled oscillator (PCOs) (i.e., a bio-inspired model for\ncommunication networks) to synchronous and desynchronous states, and their\nimplication in terms of the decentralized synchronization and scheduling in\ncommunication networks. Bio-inspired techniques have been advocated by many as\nfault-tolerant and scalable alternatives to produce self-organization in\ncommunication networks. The PCO dynamics in particular have been the source of\ninspiration for many network synchronization and scheduling protocols. However,\ntheir convergence properties, especially in locally connected networks, have\nnot been fully understood, prohibiting the migration into mainstream standards.\nThis work provides further results on the convergence of PCOs in locally\nconnected networks and the achievable convergence accuracy under propagation\ndelays. For synchronization, almost sure convergence is proved for $3$ nodes\nand accuracy results are obtained for general locally connected networks\nwhereas, for scheduling (or desynchronization), results are derived for locally\nconnected networks with mild conditions on the overlapping set of maximal\ncliques. These issues have not been fully addressed before in the literature. \n\n"}
{"id": "1705.06882", "contents": "Title: QuickTalk: An Association-Free Communication Method for IoT Devices in\n  Proximity Abstract: IoT devices are in general considered to be straightforward to use. However,\nwe find that there are a number of situations where the usability becomes poor.\nThe situations include but not limited to the followings: 1) when initializing\nan IoT device, 2) when trying to control an IoT device which is initialized and\nregistered by another person, and 3) when trying to control an IoT device out\nof many of the same type. We tackle these situations by proposing a new\nassociation-free communication method, QuickTalk. QuickTalk lets a user device\nsuch as a smartphone pinpoint and activate an IoT device with the help of an IR\ntransmitter and communicate with the pinpointed IoT device through the\nbroadcast channel of WiFi. By the nature of its association-free communication,\nQuickTalk allows a user device to immediately give a command to a specific IoT\ndevice in proximity even when the IoT device is uninitialized, unregistered to\nthe control interface of the user, or registered but being physically confused\nwith others. Our experiments of QuickTalk implemented on Raspberry Pi 2 devices\nshow that the end-to-end delay of QuickTalk is upper bounded by 2.5 seconds and\nits median is only about 0.74 seconds. We further confirm that even when an IoT\ndevice has ongoing data sessions, QuickTalk can still establish a reliable\ncommunication channel to the IoT device with little impact to the ongoing\nsessions. \n\n"}
{"id": "1705.07273", "contents": "Title: Responsive Action-based Video Synthesis Abstract: We propose technology to enable a new medium of expression, where video\nelements can be looped, merged, and triggered, interactively. Like audio, video\nis easy to sample from the real world but hard to segment into clean reusable\nelements. Reusing a video clip means non-linear editing and compositing with\nnovel footage. The new context dictates how carefully a clip must be prepared,\nso our end-to-end approach enables previewing and easy iteration.\n  We convert static-camera videos into loopable sequences, synthesizing them in\nresponse to simple end-user requests. This is hard because a) users want\nessentially semantic-level control over the synthesized video content, and b)\nautomatic loop-finding is brittle and leaves users limited opportunity to work\nthrough problems. We propose a human-in-the-loop system where adding effort\ngives the user progressively more creative control. Artists help us evaluate\nhow our trigger interfaces can be used for authoring of videos and\nvideo-performances. \n\n"}
{"id": "1705.08240", "contents": "Title: Herding boosts too-connected-to-fail risk in stock market of China Abstract: The crowd panic and its contagion play non-negligible roles at the time of\nthe stock crash, especially for China where inexperienced investors dominate\nthe market. However, existing models rarely consider investors in networking\nstocks and accordingly miss the exact knowledge of how panic contagion leads to\nabrupt crash. In this paper, by networking stocks of sharing common mutual\nfunds, a new methodology of investigating the market crash is presented. It is\nsurprisingly revealed that the herding, which origins in the mimic of seeking\nfor high diversity across investment strategies to lower individual risk, will\nproduce too-connected-to-fail stocks and reluctantly boosts the systemic risk\nof the entire market. Though too-connected stocks might be relatively stable\nduring the crisis, they are so influential that a small downward fluctuation\nwill cascade to trigger severe drops of massive successor stocks, implying that\ntheir falls might be unexpectedly amplified by the collective panic and result\nin the market crash. Our findings suggest that the whole picture of portfolio\nstrategy has to be carefully supervised to reshape the stock network. \n\n"}
{"id": "1705.08684", "contents": "Title: MmWave System for Future ITS: A MAC-layer Approach for V2X Beam Steering Abstract: Millimeter Waves (mmWave) systems have the potential of enabling\nmulti-gigabit-per-second communications in future Intelligent Transportation\nSystems (ITSs). Unfortunately, because of the increased vehicular mobility,\nthey require frequent antenna beam realignments - thus significantly increasing\nthe in-band Beamforming (BF) overhead. In this paper, we propose Smart\nMotion-prediction Beam Alignment (SAMBA), a MAC-layer algorithm that exploits\nthe information broadcast via DSRC beacons by all vehicles. Based on this\ninformation, overhead-free BF is achieved by estimating the position of the\nvehicle and predicting its motion. Moreover, adapting the beamwidth with\nrespect to the estimated position can further enhance the performance. Our\ninvestigation shows that SAMBA outperforms the IEEE 802.11ad BF strategy,\nincreasing the data rate by more than twice for sparse vehicle density while\nenhancing the network throughput proportionally to the number of vehicles.\nFurthermore, SAMBA was proven to be more efficient compared to legacy BF\nalgorithm under highly dynamic vehicular environments and hence, a viable\nsolution for future ITS services. \n\n"}
{"id": "1705.10305", "contents": "Title: Near Optimal Online Distortion Minimization for Energy Harvesting Nodes Abstract: We consider online scheduling for an energy harvesting communication system\nwhere a sensor node collects samples from a Gaussian source and sends them to a\ndestination node over a Gaussian channel. The sensor is equipped with a\nfinite-sized battery that is recharged by an independent and identically\ndistributed (i.i.d.) energy harvesting process over time. The goal is to\nminimize the long term average distortion of the source samples received at the\ndestination. We study two problems: the first is when sampling is cost-free,\nand the second is when there is a sampling cost incurred whenever samples are\ncollected. We show that fixed fraction policies [Shaviv-Ozgur], in which a\nfixed fraction of the battery state is consumed in each time slot, are\nnear-optimal in the sense that they achieve a long term average distortion that\nlies within a constant additive gap from the optimal solution for all energy\narrivals and battery sizes. For the problem with sampling costs, the\ntransmission policy is bursty; the sensor can collect samples and transmit for\nonly a portion of the time. \n\n"}
{"id": "1706.00307", "contents": "Title: Energy Harvesting Networks with General Utility Functions: Near Optimal\n  Online Policies Abstract: We consider online scheduling policies for single-user energy harvesting\ncommunication systems, where the goal is to characterize online policies that\nmaximize the long term average utility, for some general concave and\nmonotonically increasing utility function. In our setting, the transmitter\nrelies on energy harvested from nature to send its messages to the receiver,\nand is equipped with a finite-sized battery to store its energy. Energy packets\nare independent and identically distributed (i.i.d.) over time slots, and are\nrevealed causally to the transmitter. Only the average arrival rate is known a\npriori. We first characterize the optimal solution for the case of Bernoulli\narrivals. Then, for general i.i.d. arrivals, we first show that fixed fraction\npolicies [Shaviv-Ozgur] are within a constant multiplicative gap from the\noptimal solution for all energy arrivals and battery sizes. We then derive a\nset of sufficient conditions on the utility function to guarantee that fixed\nfraction policies are within a constant additive gap as well from the optimal\nsolution. \n\n"}
{"id": "1706.01215", "contents": "Title: DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems\n  with a Compressor-Critic Framework Abstract: Recent advances in deep learning motivate the use of deep neutral networks in\nsensing applications, but their excessive resource needs on constrained\nembedded devices remain an important impediment. A recently explored solution\nspace lies in compressing (approximating or simplifying) deep neural networks\nin some manner before use on the device. We propose a new compression solution,\ncalled DeepIoT, that makes two key contributions in that space. First, unlike\ncurrent solutions geared for compressing specific types of neural networks,\nDeepIoT presents a unified approach that compresses all commonly used deep\nlearning structures for sensing applications, including fully-connected,\nconvolutional, and recurrent neural networks, as well as their combinations.\nSecond, unlike solutions that either sparsify weight matrices or assume linear\nstructure within weight matrices, DeepIoT compresses neural network structures\ninto smaller dense matrices by finding the minimum number of non-redundant\nhidden elements, such as filters and dimensions required by each layer, while\nkeeping the performance of sensing applications the same. Importantly, it does\nso using an approach that obtains a global view of parameter redundancies,\nwhich is shown to produce superior compression. We conduct experiments with\nfive different sensing-related tasks on Intel Edison devices. DeepIoT\noutperforms all compared baseline algorithms with respect to execution time and\nenergy consumption by a significant margin. It reduces the size of deep neural\nnetworks by 90% to 98.9%. It is thus able to shorten execution time by 71.4% to\n94.5%, and decrease energy consumption by 72.2% to 95.7%. These improvements\nare achieved without loss of accuracy. The results underscore the potential of\nDeepIoT for advancing the exploitation of deep neural networks on\nresource-constrained embedded devices. \n\n"}
{"id": "1706.02795", "contents": "Title: A Deep Causal Inference Approach to Measuring the Effects of Forming\n  Group Loans in Online Non-profit Microfinance Platform Abstract: Kiva is an online non-profit crowdsouring microfinance platform that raises\nfunds for the poor in the third world. The borrowers on Kiva are small business\nowners and individuals in urgent need of money. To raise funds as fast as\npossible, they have the option to form groups and post loan requests in the\nname of their groups. While it is generally believed that group loans pose less\nrisk for investors than individual loans do, we study whether this is the case\nin a philanthropic online marketplace. In particular, we measure the effect of\ngroup loans on funding time while controlling for the loan sizes and other\nfactors. Because loan descriptions (in the form of texts) play an important\nrole in lenders' decision process on Kiva, we make use of this information\nthrough deep learning in natural language processing. In this aspect, this is\nthe first paper that uses one of the most advanced deep learning techniques to\ndeal with unstructured data in a way that can take advantage of its superior\nprediction power to answer causal questions. We find that on average, forming\ngroup loans speeds up the funding time by about 3.3 days. \n\n"}
{"id": "1706.02936", "contents": "Title: Principal-Agent Problem with Common Agency without Communication Abstract: In this paper, we consider a problem of contract theory in which several\nPrincipals hire a common Agent and we study the model in the continuous time\nsetting. We show that optimal contracts should satisfy some equilibrium\nconditions and we reduce the optimisation problem of the Principals to a system\nof coupled Hamilton-Jacobi-Bellman (HJB) equations. We provide conditions\nensuring that for risk-neutral Principals, the system of coupled HJB equations\nadmits a solution. Further, we apply our study in a more specific\nlinear-quadratic model where two interacting Principals hire one common Agent.\nIn this continuous time model, we extend the result of Bernheim and Whinston\n(1986) in which the authors compare the optimal effort of the Agent in a\nnon-cooperative Principals model and that in the aggregate model, by showing\nthat these two optimisations coincide only in the first best case. We also\nstudy the sensibility of the optimal effort and the optimal remunerations with\nrespect to appetence parameters and the correlation between the projects. \n\n"}
{"id": "1706.03086", "contents": "Title: LoRaWAN in the Wild: Measurements from The Things Network Abstract: The Long-Range Wide-Area Network (LoRaWAN) specification was released in\n2015, primarily to support the Internet-of-Things by facilitating wireless\ncommunication over long distances. Since 2015, the role-out and adoption of\nLoRaWAN has seen a steep growth. To the best of our knowledge, we are the first\nto have extensively measured, analyzed, and modeled the performance, features,\nand use cases of an operational LoRaWAN, namely The Things Network. Our\nmeasurement data, as presented in this paper, cover the early stages up to the\nproduction-level deployment of LoRaWAN. In particular, we analyze packet\npayloads, radio-signal quality, and spatio-temporal aspects, to model and\nestimate the performance of LoRaWAN. We also use our empirical findings in\nsimulations to estimate the packet-loss. \n\n"}
{"id": "1706.03444", "contents": "Title: Secret-Key-Aided Scheme for Securing Untrusted DF Relaying Networks Abstract: This paper proposes a new scheme to secure the transmissions in an untrusted\ndecode-and-forward (DF) relaying network. A legitimate source node, Alice,\nsends her data to a legitimate destination node, Bob, with the aid of an\nuntrusted DF relay node, Charlie. To secure the transmissions from Charlie\nduring relaying time slots, each data codeword is secured using a secret-key\ncodeword that has been previously shared between Alice and Bob during the\nperfectly secured time slots (i.e., when the channel secrecy rate is positive).\nThe secret-key bits exchanged between Alice and Bob are stored in a\nfinite-length buffer and are used to secure data transmission whenever needed.\nWe model the secret-key buffer as a queueing system and analyze its Markov\nchain. Our numerical results show the gains of our proposed scheme relative to\nbenchmarks. Moreover, the proposed scheme achieves an upper bound on the secure\nthroughput. \n\n"}
{"id": "1706.05877", "contents": "Title: General Equilibrium Under Convex Portfolio Constraints and Heterogeneous\n  Risk Preferences Abstract: This paper characterizes the equilibrium in a continuous time financial\nmarket populated by heterogeneous agents who differ in their rate of relative\nrisk aversion and face convex portfolio constraints. The model is studied in an\napplication to margin constraints and found to match real world observations\nabout financial variables and leverage cycles. It is shown how margin\nconstraints increase the market price of risk and decrease the interest rate by\nforcing more risk averse agents to hold more risky assets, producing a higher\nequity risk premium. In addition, heterogeneity and margin constraints are\nshown to produce both pro- and counter-cyclical leverage cycles. Beyond two\ntypes, it is shown how constraints can cascade and how leverage can exhibit\nhighly non-linear dynamics. Finally, empirical results are given, documenting a\nnovel stylized fact which is predicted by the model, namely that the leverage\ncycle is both pro- and counter-cyclical. \n\n"}
{"id": "1706.07544", "contents": "Title: Simultaneous Transmit and Receive Operation in Next Generation IEEE\n  802.11 WLANs: A MAC Protocol Design Approach Abstract: Full-duplex (FD) technology is likely to be adopted in various legacy\ncommunications standards. The IEEE 802.11ax working group has been considering\na simultaneous transmit and receive (STR) mode for the next generation wireless\nlocal area networks (WLANs). Enabling STR mode (FD communication mode) in\n802.11 networks creates bi-directional FD (BFD) and uni-directional FD (UFD)\nlinks. The key challenge is to integrate STR mode with minimal protocol\nmodifications, while considering the co-existence of FD and legacy half-duplex\n(HD) stations (STAs) and backwards compatibility. This paper proposes a simple\nand practical approach to enable STR mode in 802.11 networks with co-existing\nFD and HD STAs. The protocol explicitly accounts for the peculiarities of FD\nenvironments and backwards compatibility. Key aspects of the proposed solution\ninclude FD capability discovery, handshake mechanism for channel access, node\nselection for UFD transmission, adaptive acknowledgement (ACK) timeout for STAs\nengaged in BFD or UFD transmission, and mitigation of contention unfairness.\nPerformance evaluation demonstrates the effectiveness of the proposed solution\nin realizing the gains of FD technology for next generation WLANs. \n\n"}
{"id": "1706.10209", "contents": "Title: Storage, Communication, and Load Balancing Trade-off in Distributed\n  Cache Networks Abstract: We consider load balancing in a network of caching servers delivering\ncontents to end users. Randomized load balancing via the so-called power of two\nchoices is a well-known approach in parallel and distributed systems. In this\nframework, we investigate the tension between storage resources, communication\ncost, and load balancing performance. To this end, we propose a randomized load\nbalancing scheme which simultaneously considers cache size limitation and\nproximity in the server redirection process.\n  In contrast to the classical power of two choices setup, since the memory\nlimitation and the proximity constraint cause correlation in the server\nselection process, we may not benefit from the power of two choices. However,\nwe prove that in certain regimes of problem parameters, our scheme results in\nthe maximum load of order $\\Theta(\\log\\log n)$ (here $n$ is the network size).\nThis is an exponential improvement compared to the scheme which assigns each\nrequest to the nearest available replica. Interestingly, the extra\ncommunication cost incurred by our proposed scheme, compared to the nearest\nreplica strategy, is small. Furthermore, our extensive simulations show that\nthe trade-off trend does not depend on the network topology and library\npopularity profile details. \n\n"}
{"id": "1707.00513", "contents": "Title: Interference Coordination via Power Domain Channel Estimation Abstract: A novel technique is proposed which enables each transmitter to acquire\nglobal channel state information (CSI) from the sole knowledge of individual\nreceived signal power measurements, which makes dedicated feedback or\ninter-transmitter signaling channels unnecessary. To make this possible, we\nresort to a completely new technique whose key idea is to exploit the transmit\npower levels as symbols to embed information and the observed interference as a\ncommunication channel the transmitters can use to exchange coordination\ninformation. Although the used technique allows any kind of {low-rate}\ninformation to be exchanged among the transmitters, the focus here is to\nexchange local CSI. The proposed procedure also comprises a phase which allows\nlocal CSI to be estimated. Once an estimate of global CSI is acquired by the\ntransmitters, it can be used to optimize any utility function which depends on\nit. While algorithms which use the same type of measurements such as the\niterative water-filling algorithm (IWFA) implement the sequential best-response\ndynamics (BRD) applied to individual utilities, here, thanks to the\navailability of global CSI, the BRD can be applied to the sum-utility.\nExtensive numerical results show that significant gains can be obtained and,\nthis, by requiring no additional online signaling. \n\n"}
{"id": "1707.00832", "contents": "Title: Modeling the Internet of Things: a simulation perspective Abstract: This paper deals with the problem of properly simulating the Internet of\nThings (IoT). Simulating an IoT allows evaluating strategies that can be\nemployed to deploy smart services over different kinds of territories. However,\nthe heterogeneity of scenarios seriously complicates this task. This imposes\nthe use of sophisticated modeling and simulation techniques. We discuss novel\napproaches for the provision of scalable simulation scenarios, that enable the\nreal-time execution of massively populated IoT environments. Attention is given\nto novel hybrid and multi-level simulation techniques that, when combined with\nagent-based, adaptive Parallel and Distributed Simulation (PADS) approaches,\ncan provide means to perform highly detailed simulations on demand. To support\nthis claim, we detail a use case concerned with the simulation of vehicular\ntransportation systems. \n\n"}
{"id": "1707.02482", "contents": "Title: On the Interplay Between Edge Caching and HARQ in Fog-RAN Abstract: In a Fog Radio Access Network (Fog-RAN), edge caching is combined with\ncloud-aided transmission in order to compensate for the limited hit probability\nof the caches at the base stations (BSs). Unlike the typical wired scenarios\nstudied in the networking literature in which entire files are typically\ncached, recent research has suggested that fractional caching at the BSs of a\nwireless system can be beneficial. This paper investigates the benefits of\nfractional caching in a scenario with a cloud processor connected via a\nwireless fronthaul link to a BS, which serves a number of mobile users on a\nwireless downlink channel using orthogonal spectral resources. The fronthaul\nand downlink channels occupy orthogonal frequency bands. The end-to-end\ndelivery latency for given requests of the users depends on the HARQ processes\nrun on the two links to counteract fading-induced outages. An analytical\nframework based on theory of Markov chains with rewards is provided that\nenables the optimization of fractional edge caching at the BSs. Numerical\nresults demonstrate meaningful advantages for fractional caching due to the\ninterplay between caching and HARQ transmission. The gains are observed in the\ntypical case in which the performance is limited by the wireless downlink\nchannel and the file popularity distribution is not too skewed. \n\n"}
{"id": "1707.02701", "contents": "Title: Case For Static AMSDU Aggregation in WLANs Abstract: Frame aggregation is a mechanism by which multiple frames are combined into a\nsingle transmission unit over the air. Frames aggregated at the AMSDU level use\na common CRC check to enforce integrity. For longer aggregated AMSDU frames,\nthe packet error rate increases significantly for the same bit error rate.\nHence, multiple studies have proposed doing AMSDU aggregation adaptively based\non the error rate. This study evaluates if there is a \\emph{practical}\nadvantage in doing adaptive AMSDU aggregation based on the link bit error rate.\nEvaluations on a model show that instead of implementing a complex adaptive\nAMSDU frame aggregation mechanism which impact queuing and other implementation\naspects, it is easier to influence packet error rate with traditional\nmechanisms while keeping the AMSDU aggregation logic simple. \n\n"}
{"id": "1707.03203", "contents": "Title: Multi-antenna Enabled Cluster-based Cooperation in Wireless Powered\n  Communication Networks Abstract: In this paper, we consider a wireless powered communication network (WPCN)\nconsisting of a multi-antenna hybrid access point (HAP) that transfers wireless\nenergy to and receives sensing data from a cluster of low-power wireless\ndevices (WDs). To enhance the throughput performance of some far-away WDs, we\nallow one of the WDs to act as the cluster head (CH) that helps forward the\nmessages of the other cluster members (CMs). However, the performance of the\nproposed cluster-based cooperation is fundamentally limited by the high energy\nconsumption of the CH, who needs to transmit all the WDs' messages including\nits own. To tackle this issue, we exploit the capability of multi-antenna\nenergy beamforming (EB) at the HAP, which can focus more transferred power to\nthe CH to balance its energy consumption in assisting the other WDs.\nSpecifically, we first derive the throughput performance of each individual WD\nunder the proposed scheme. Then, we jointly optimize the EB design, the\ntransmit time allocation among the HAP and the WDs, and the transmit power\nallocation of the CH to maximize the minimum data rate achievable among all the\nWDs (the max-min throughput) for improved throughput fairness among the WDs. An\nefficient optimal algorithm is proposed to solve the joint optimization\nproblem. Moreover, we simulate under practical network setups and show that the\nproposed multi-antenna enabled cluster-based cooperation can effectively\nimprove the throughput fairness of WPCN. \n\n"}
{"id": "1707.03269", "contents": "Title: Q-Learning Algorithm for VoLTE Closed-Loop Power Control in Indoor Small\n  Cells Abstract: We propose a reinforcement learning (RL) based closed loop power control\nalgorithm for the downlink of the voice over LTE (VoLTE) radio bearer for an\nindoor environment served by small cells. The main contributions of our paper\nare to 1) use RL to solve performance tuning problems in an indoor cellular\nnetwork for voice bearers and 2) show that our derived lower bound loss in\neffective signal to interference plus noise ratio due to neighboring cell\nfailure is sufficient for VoLTE power control purposes in practical cellular\nnetworks. In our simulation, the proposed RL-based power control algorithm\nsignificantly improves both voice retainability and mean opinion score compared\nto current industry standards. The improvement is due to maintaining an\neffective downlink signal to interference plus noise ratio against adverse\nnetwork operational issues and faults. \n\n"}
{"id": "1707.04282", "contents": "Title: Polynomial Counting in Anonymous Dynamic Networks with Applications to\n  Anonymous Dynamic Algebraic Computations Abstract: Starting with Michail, Chatzigiannakis, and Spirakis work, the problem of\nCounting the number of nodes in Anonymous Dynamic Networks has attracted a lot\nof attention. The problem is challenging because nodes are indistinguishable\n(they lack identifiers and execute the same program) and the topology may\nchange arbitrarily from round to round of communication, as long as the network\nis connected in each round. The problem is central in distributed computing as\nthe number of participants is frequently needed to make important decisions,\nsuch as termination, agreement, synchronization, and many others. A variety of\nalgorithms built on top of mass-distribution techniques have been presented,\nanalyzed, and also experimentally evaluated; some of them assumed additional\nknowledge of network characteristics, such as bounded degree or given upper\nbound on the network size. However, the question of whether Counting can be\nsolved deterministically in sub-exponential time remained open. In this work,\nwe answer this question positively by presenting Methodical Counting, which\nruns in polynomial time and requires no knowledge of network characteristics.\nMoreover, we also show how to extend Methodical Counting to compute the sum of\ninput values and more complex functions without extra cost. Our analysis\nleverages previous work on random walks in evolving graphs, combined with\ncarefully chosen alarms in the algorithm that control the process and its\nparameters. To the best of our knowledge, our Counting algorithm and its\nextensions to other algebraic and Boolean functions are the first that can be\nimplemented in practice with worst-case guarantees. \n\n"}
{"id": "1707.05836", "contents": "Title: Domain-Sharding for Faster HTTP/2 in Lossy Cellular Networks Abstract: HTTP/2 (h2) is a new standard for Web communications that already delivers a\nlarge share of Web traffic. Unlike HTTP/1, h2 uses only one underlying TCP\nconnection. In a cellular network with high loss and sudden spikes in latency,\nwhich the TCP stack might interpret as loss, using a single TCP connection can\nnegatively impact Web performance. In this paper, we perform an extensive\nanalysis of real world cellular network traffic and design a testbed to emulate\nloss characteristics in cellular networks. We use the emulated cellular network\nto measure h2 performance in comparison to HTTP/1.1, for webpages synthesized\nfrom HTTP Archive repository data.\n  Our results show that, in lossy conditions, h2 achieves faster page load\ntimes (PLTs) for webpages with small objects. For webpages with large objects,\nh2 degrades the PLT. We devise a new domain-sharding technique that isolates\nlarge and small object downloads on separate connections. Using sharding, we\nshow that under lossy cellular conditions, h2 over multiple connections\nimproves the PLT compared to h2 with one connection and HTTP/1.1 with six\nconnections. Finally, we recommend content providers and content delivery\nnetworks to apply h2-aware domain-sharding on webpages currently served over h2\nfor improved mobile Web performance. \n\n"}
{"id": "1707.05866", "contents": "Title: Asymptotically Optimal Load Balancing Topologies Abstract: We consider a system of $N$ servers inter-connected by some underlying graph\ntopology $G_N$. Tasks arrive at the various servers as independent Poisson\nprocesses of rate $\\lambda$. Each incoming task is irrevocably assigned to\nwhichever server has the smallest number of tasks among the one where it\nappears and its neighbors in $G_N$. Tasks have unit-mean exponential service\ntimes and leave the system upon service completion.\n  The above model has been extensively investigated in the case $G_N$ is a\nclique. Since the servers are exchangeable in that case, the queue length\nprocess is quite tractable, and it has been proved that for any $\\lambda < 1$,\nthe fraction of servers with two or more tasks vanishes in the limit as $N \\to\n\\infty$. For an arbitrary graph $G_N$, the lack of exchangeability severely\ncomplicates the analysis, and the queue length process tends to be worse than\nfor a clique. Accordingly, a graph $G_N$ is said to be $N$-optimal or\n$\\sqrt{N}$-optimal when the occupancy process on $G_N$ is equivalent to that on\na clique on an $N$-scale or $\\sqrt{N}$-scale, respectively.\n  We prove that if $G_N$ is an Erd\\H{o}s-R\\'enyi random graph with average\ndegree $d(N)$, then it is with high probability $N$-optimal and\n$\\sqrt{N}$-optimal if $d(N) \\to \\infty$ and $d(N) / (\\sqrt{N} \\log(N)) \\to\n\\infty$ as $N \\to \\infty$, respectively. This demonstrates that optimality can\nbe maintained at $N$-scale and $\\sqrt{N}$-scale while reducing the number of\nconnections by nearly a factor $N$ and $\\sqrt{N} / \\log(N)$ compared to a\nclique, provided the topology is suitably random. It is further shown that if\n$G_N$ contains $\\Theta(N)$ bounded-degree nodes, then it cannot be $N$-optimal.\nIn addition, we establish that an arbitrary graph $G_N$ is $N$-optimal when its\nminimum degree is $N - o(N)$, and may not be $N$-optimal even when its minimum\ndegree is $c N + o(N)$ for any $0 < c < 1/2$. \n\n"}
{"id": "1707.06729", "contents": "Title: Predictive networking and optimization for flow-based networks Abstract: Artificial Neural Networks (ANNs) were used to classify neural network flows\nby flow size. After training the neural network was able to predict the size of\na flows with 87% accuracy with a Feed Forward Neural Network. This demonstrates\nthat flow based routers can prioritize candidate flows with a predicted large\nnumber of packets for priority insertion into hardware content-addressable\nmemory. \n\n"}
{"id": "1707.08074", "contents": "Title: Optimal Sensing and Data Estimation in a Large Sensor Network Abstract: An energy efficient use of large scale sensor networks necessitates\nactivating a subset of possible sensors for estimation at a fusion center. The\nproblem is inherently combinatorial; to this end, a set of iterative,\nrandomized algorithms are developed for sensor subset selection by exploiting\nthe underlying statistics. Gibbs sampling-based methods are designed to\noptimize the estimation error and the mean number of activated sensors. The\noptimality of the proposed strategy is proven, along with guarantees on their\nconvergence speeds. Also, another new algorithm exploiting stochastic\napproximation in conjunction with Gibbs sampling is derived for a constrained\nversion of the sensor selection problem. The methodology is extended to the\nscenario where the fusion center has access to only a parametric form of the\njoint statistics, but not the true underlying distribution. Therein,\nexpectation-maximization is effectively employed to learn the distribution.\nStrategies for iid time-varying data are also outlined. Numerical results show\nthat the proposed methods converge very fast to the respective optimal\nsolutions, and therefore can be employed for optimal sensor subset selection in\npractical sensor networks. \n\n"}
{"id": "1707.08569", "contents": "Title: Wisture: RNN-based Learning of Wireless Signals for Gesture Recognition\n  in Unmodified Smartphones Abstract: This paper introduces Wisture, a new online machine learning solution for\nrecognizing touch-less dynamic hand gestures on a smartphone. Wisture relies on\nthe standard Wi-Fi Received Signal Strength (RSS) using a Long Short-Term\nMemory (LSTM) Recurrent Neural Network (RNN), thresholding filters and traffic\ninduction. Unlike other Wi-Fi based gesture recognition methods, the proposed\nmethod does not require a modification of the smartphone hardware or the\noperating system, and performs the gesture recognition without interfering with\nthe normal operation of other smartphone applications.\n  We discuss the characteristics of Wisture, and conduct extensive experiments\nto compare its performance against state-of-the-art machine learning solutions\nin terms of both accuracy and time efficiency. The experiments include a set of\ndifferent scenarios in terms of both spatial setup and traffic between the\nsmartphone and Wi-Fi access points (AP). The results show that Wisture achieves\nan online recognition accuracy of up to 94% (average 78%) in detecting and\nclassifying three hand gestures. \n\n"}
{"id": "1707.09422", "contents": "Title: Hyperprofile-based Computation Offloading for Mobile Edge Networks Abstract: In recent studies, researchers have developed various computation offloading\nframeworks for bringing cloud services closer to the user via edge networks.\nSpecifically, an edge device needs to offload computationally intensive tasks\nbecause of energy and processing constraints. These constraints present the\nchallenge of identifying which edge nodes should receive tasks to reduce\noverall resource consumption. We propose a unique solution to this problem\nwhich incorporates elements from Knowledge-Defined Networking (KDN) to make\nintelligent predictions about offloading costs based on historical data. Each\nserver instance can be represented in a multidimensional feature space where\neach dimension corresponds to a predicted metric. We compute features for a\n\"hyperprofile\" and position nodes based on the predicted costs of offloading a\nparticular task. We then perform a k-Nearest Neighbor (kNN) query within the\nhyperprofile to select nodes for offloading computation. This paper formalizes\nour hyperprofile-based solution and explores the viability of using machine\nlearning (ML) techniques to predict metrics useful for computation offloading.\nWe also investigate the effects of using different distance metrics for the\nqueries. Our results show various network metrics can be modeled accurately\nwith regression, and there are circumstances where kNN queries using Euclidean\ndistance as opposed to rectilinear distance is more favorable. \n\n"}
{"id": "1708.04186", "contents": "Title: Boundaries as an Enhancement Technique for Physical Layer Security Abstract: In this paper, we study the receiver performance with physical layer security\nin a Poisson field of interferers. We compare the performance in two deployment\nscenarios: (i) the receiver is located at the corner of a quadrant, (ii) the\nreceiver is located in the infinite plane. When the channel state information\n(CSI) of the eavesdropper is not available at the transmitter, we calculate the\nprobability of secure connectivity using the Wyner coding scheme, and we show\nthat hiding the receiver at the corner is beneficial at high rates of the\ntransmitted codewords and detrimental at low transmission rates. When the CSI\nis available, we show that the average secrecy capacity is higher when the\nreceiver is located at the corner, even if the intensity of interferers in this\ncase is four times higher than the intensity of interferers in the bulk.\nTherefore boundaries can also be used as a secrecy enhancement technique for\nhigh data rate applications. \n\n"}
{"id": "1708.05096", "contents": "Title: Will SDN be part of 5G? Abstract: For many, this is no longer a valid question and the case is considered\nsettled with SDN/NFV (Software Defined Networking/Network Function\nVirtualization) providing the inevitable innovation enablers solving many\noutstanding management issues regarding 5G. However, given the monumental task\nof softwarization of radio access network (RAN) while 5G is just around the\ncorner and some companies have started unveiling their 5G equipment already,\nthe concern is very realistic that we may only see some point solutions\ninvolving SDN technology instead of a fully SDN-enabled RAN. This survey paper\nidentifies all important obstacles in the way and looks at the state of the art\nof the relevant solutions. This survey is different from the previous surveys\non SDN-based RAN as it focuses on the salient problems and discusses solutions\nproposed within and outside SDN literature. Our main focus is on fronthaul,\nbackward compatibility, supposedly disruptive nature of SDN deployment,\nbusiness cases and monetization of SDN related upgrades, latency of general\npurpose processors (GPP), and additional security vulnerabilities,\nsoftwarization brings along to the RAN. We have also provided a summary of the\narchitectural developments in SDN-based RAN landscape as not all work can be\ncovered under the focused issues. This paper provides a comprehensive survey on\nthe state of the art of SDN-based RAN and clearly points out the gaps in the\ntechnology. \n\n"}
{"id": "1708.06598", "contents": "Title: Coverage Maximization for a Poisson Field of Drone Cells Abstract: The use of drone base stations to provide wireless connectivity for ground\nterminals is becoming a promising part of future technologies. The design of\nsuch aerial networks is however different compared to cellular 2D networks, as\nantennas from the drones are looking down, and the channel model becomes\nheight-dependent. In this paper, we study the effect of antenna patterns and\nheight-dependent shadowing. We consider a random network topology to capture\nthe effect of dynamic changes of the flying base stations. First we\ncharacterize the aggregate interference imposed by the co-channel neighboring\ndrones. Then we derive the link coverage probability between a ground user and\nits associated drone base station. The result is used to obtain the optimum\nsystem parameters in terms of drones antenna beamwidth, density and altitude.\nWe also derive the average LoS probability of the associated drone and show\nthat it is a good approximation and simplification of the coverage probability\nin low altitudes up to 500 m according to the required\nsignal-to-interference-plus-noise ratio (SINR). \n\n"}
{"id": "1708.06658", "contents": "Title: AACT: Application-Aware Cooperative Time Allocation for Internet of\n  Things Abstract: As the number of Internet of Things (IoT) devices keeps increasing, data is\nrequired to be communicated and processed by these devices at unprecedented\nrates. Cooperation among wireless devices by exploiting Device-to-Device (D2D)\nconnections is promising, where aggregated resources in a cooperative setup can\nbe utilized by all devices, which would increase the total utility of the\nsetup. In this paper, we focus on the resource allocation problem for\ncooperating IoT devices with multiple heterogeneous applications. In\nparticular, we develop Application-Aware Cooperative Time allocation (AACT)\nframework, which optimizes the time that each application utilizes the\naggregated system resources by taking into account heterogeneous device\nconstraints and application requirements. AACT is grounded on the concept of\nRolling Horizon Control (RHC) where decisions are made by iteratively solving a\nconvex optimization problem over a moving control window of estimated system\nparameters. The simulation results demonstrate significant performance gains. \n\n"}
{"id": "1708.06698", "contents": "Title: Optimal and Scalable Caching for 5G Using Reinforcement Learning of\n  Space-time Popularities Abstract: Small basestations (SBs) equipped with caching units have potential to handle\nthe unprecedented demand growth in heterogeneous networks. Through low-rate,\nbackhaul connections with the backbone, SBs can prefetch popular files during\noff-peak traffic hours, and service them to the edge at peak periods. To\nintelligently prefetch, each SB must learn what and when to cache, while taking\ninto account SB memory limitations, the massive number of available contents,\nthe unknown popularity profiles, as well as the space-time popularity dynamics\nof user file requests. In this work, local and global Markov processes model\nuser requests, and a reinforcement learning (RL) framework is put forth for\nfinding the optimal caching policy when the transition probabilities involved\nare unknown. Joint consideration of global and local popularity demands along\nwith cache-refreshing costs allow for a simple, yet practical asynchronous\ncaching approach. The novel RL-based caching relies on a Q-learning algorithm\nto implement the optimal policy in an online fashion, thus enabling the cache\ncontrol unit at the SB to learn, track, and possibly adapt to the underlying\ndynamics. To endow the algorithm with scalability, a linear function\napproximation of the proposed Q-learning scheme is introduced, offering faster\nconvergence as well as reduced complexity and memory requirements. Numerical\ntests corroborate the merits of the proposed approach in various realistic\nsettings. \n\n"}
{"id": "1708.08299", "contents": "Title: The Convergence of Machine Learning and Communications Abstract: The areas of machine learning and communication technology are converging.\nToday's communications systems generate a huge amount of traffic data, which\ncan help to significantly enhance the design and management of networks and\ncommunication components when combined with advanced machine learning methods.\nFurthermore, recently developed end-to-end training procedures offer new ways\nto jointly optimize the components of a communication system. Also in many\nemerging application fields of communication technology, e.g., smart cities or\ninternet of things, machine learning methods are of central importance. This\npaper gives an overview over the use of machine learning in different areas of\ncommunications and discusses two exemplar applications in wireless networking.\nFurthermore, it identifies promising future research topics and discusses their\npotential impact. \n\n"}
{"id": "1708.08594", "contents": "Title: Identifying relationship lending in the interbank market: A network\n  approach Abstract: Relationship lending is broadly interpreted as a strong partnership between a\nlender and a borrower. Nevertheless, we still lack consensus regarding how to\nquantify the strength of a lending relationship, while simple statistics such\nas the frequency and volume of loans have been used as proxies in previous\nstudies. Here, we propose statistical tests to identify relationship lending as\na significant tie between banks. Application of the proposed method to the\nItalian interbank networks reveals that the fraction of relationship lending\namong all bilateral trades has been quite stable and that the relationship\nlenders tend to impose high interest rates at the time of financial distress. \n\n"}
{"id": "1708.09087", "contents": "Title: A New Stable Peer-to-Peer Protocol with Non-persistent Peers Abstract: Recent studies have suggested that the stability of peer-to-peer networks may\nrely on persistent peers, who dwell on the network after they obtain the entire\nfile. In the absence of such peers, one piece becomes extremely rare in the\nnetwork, which leads to instability. Technological developments, however, are\npoised to reduce the incidence of persistent peers, giving rise to a need for a\nprotocol that guarantees stability with non-persistent peers. We propose a\nnovel peer-to-peer protocol, the group suppression protocol, to ensure the\nstability of peer-to-peer networks under the scenario that all the peers adopt\nnon-persistent behavior. Using a suitable Lyapunov potential function, the\ngroup suppression protocol is proven to be stable when the file is broken into\ntwo pieces, and detailed experiments demonstrate the stability of the protocol\nfor arbitrary number of pieces. We define and simulate a decentralized version\nof this protocol for practical applications. Straightforward incorporation of\nthe group suppression protocol into BitTorrent while retaining most of\nBitTorrent's core mechanisms is also presented. Subsequent simulations show\nthat under certain assumptions, BitTorrent with the official protocol cannot\nescape from the missing piece syndrome, but BitTorrent with group suppression\ndoes. \n\n"}
{"id": "1708.09410", "contents": "Title: Secure Communications for the Two-user Broadcast Channel with Random\n  Traffic Abstract: In this work, we study the stability region of the two-user broadcast channel\n(BC) with bursty data arrivals and security constraints. We consider the\nscenario, where one of the receivers has a secrecy constraint and its packets\nneed to be kept secret from the other receiver. This is achieved by employing\nfull-duplexing at the receiver with the secrecy constraint, so that it\ntransmits a jamming signal to impede the reception of the other receiver. In\nthis context, the stability region of the two-user BC is characterized for the\ngeneral decoding case. Then, assuming two different decoding schemes the\nrespective stability regions are derived. The effect of self-interference due\nto the full-duplex operation on the stability region is also investigated. The\nstability region of the BC with a secrecy constraint, where the receivers do\nnot have full duplex capability can be obtained as a special case of the\nresults derived in this paper. In addition, the paper considers the problem of\nmaximizing the saturated throughput of the queue, whose packets does not\nrequire to be kept secret under minimum service guarantees for the other queue.\nThe results provide new insights on the effect of the secrecy constraint on the\nstability region of the BC. In particular, it is shown that the stability\nregion with secrecy constraint is sensitive to the coefficient of\nself-interference cancelation under certain cases. \n\n"}
{"id": "1708.09561", "contents": "Title: Optimal Dynamic Cloud Network Control Abstract: Distributed cloud networking enables the deployment of a wide range of\nservices in the form of interconnected software functions instantiated over\ngeneral purpose hardware at multiple cloud locations distributed throughout the\nnetwork. We consider the problem of optimal service delivery over a distributed\ncloud network, in which nodes are equipped with both communication and\ncomputation resources. We address the design of distributed online solutions\nthat drive flow processing and routing decisions, along with the associated\nallocation of cloud and network resources. For a given set of services, each\ndescribed by a chain of service functions, we characterize the cloud network\ncapacity region and design a family of dynamic cloud network control (DCNC)\nalgorithms that stabilize the underlying queuing system, while achieving\narbitrarily close to minimum cost with a tradeoff in network delay. The\nproposed DCNC algorithms make local decisions based on the online minimization\nof linear and quadratic metrics obtained from an upper bound on the Lyapunov\ndrift-plus-penalty of the cloud network queuing system. Minimizing a quadratic\nvs. a linear metric is shown to improve the cost-delay tradeoff at the expense\nof increased computational complexity. Our algorithms are further enhanced with\na shortest transmission-plus-processing distance bias that improves delay\nperformance without compromising throughput or overall cloud network cost. We\nprovide throughput and cost optimality guarantees, convergence time analysis,\nand extensive simulations in representative cloud network scenarios. \n\n"}
{"id": "1708.09827", "contents": "Title: Walking Through Waypoints Abstract: We initiate the study of a fundamental combinatorial problem: Given a\ncapacitated graph $G=(V,E)$, find a shortest walk (\"route\") from a source $s\\in\nV$ to a destination $t\\in V$ that includes all vertices specified by a set\n$\\mathscr{W}\\subseteq V$: the \\emph{waypoints}. This waypoint routing problem\nfinds immediate applications in the context of modern networked distributed\nsystems. Our main contribution is an exact polynomial-time algorithm for graphs\nof bounded treewidth. We also show that if the number of waypoints is\nlogarithmically bounded, exact polynomial-time algorithms exist even for\ngeneral graphs. Our two algorithms provide an almost complete characterization\nof what can be solved exactly in polynomial-time: we show that more general\nproblems (e.g., on grid graphs of maximum degree 3, with slightly more\nwaypoints) are computationally intractable. \n\n"}
{"id": "1709.00717", "contents": "Title: Enhancing TCP End-to-End Performance in Millimeter-Wave Communications Abstract: Recently, millimeter-wave (mmWave) communications have received great\nattention due to the availability of large spectrum resources. Nevertheless,\ntheir impact on TCP performance has been overlooked, which is observed that the\nsaid TCP performance collapse occurs owing to the significant difference in\nsignal quality between LOS and NLOS links. We propose a novel TCP design for\nmmWave communications, a mmWave performance enhancing proxy (mmPEP), enabling\nnot only to overcome TCP performance collapse but also exploit the properties\nof mmWave channels. The base station installs the TCP proxy to operate the two\nfunctionalities called Ack management and batch retransmission. Specifically,\nthe proxy sends the said early-Ack to the server not to decrease its sending\nrate even in the NLOS status. In addition, when a packet-loss is detected, the\nproxy retransmits not only lost packets but also the certain number of the\nfollowing packets expected to be lost too. It is verified by ns-3 simulation\nthat compared with benchmark, mmPEP enhances the end-to-end rate and packet\ndelivery ratio by maintaining high sending rate with decreasing the loss\nrecovery time. \n\n"}
{"id": "1709.01494", "contents": "Title: Latency Optimal Broadcasting in Noisy Wireless Mesh Networks Abstract: In this paper, we adopt a new noisy wireless network model introduced very\nrecently by Censor-Hillel et al. in [ACM PODC 2017, CHHZ17]. More specifically,\nfor a given noise parameter $p\\in [0,1],$ any sender has a probability of $p$\nof transmitting noise or any receiver of a single transmission in its\nneighborhood has a probability $p$ of receiving noise.\n  In this paper, we first propose a new asymptotically latency-optimal\napproximation algorithm (under faultless model) that can complete\nsingle-message broadcasting task in $D+O(\\log^2 n)$ time units/rounds in any\nWMN of size $n,$ and diameter $D$. We then show this diameter-linear\nbroadcasting algorithm remains robust under the noisy wireless network model\nand also improves the currently best known result in CHHZ17 by a\n$\\Theta(\\log\\log n)$ factor.\n  In this paper, we also further extend our robust single-message broadcasting\nalgorithm to $k$ multi-message broadcasting scenario and show it can broadcast\n$k$ messages in $O(D+k\\log n+\\log^2 n)$ time rounds. This new robust\nmulti-message broadcasting scheme is not only asymptotically optimal but also\nanswers affirmatively the problem left open in CHHZ17 on the existence of an\nalgorithm that is robust to sender and receiver faults and can broadcast $k$\nmessages in $O(D+k\\log n + polylog(n))$ time rounds. \n\n"}
{"id": "1709.01786", "contents": "Title: An Efficient Loop-free Version of AODVv2 Abstract: Ad hoc On Demand distance Vector (AODV) routing protocol is one of the most\nprominent routing protocol used in Mobile Ad-hoc Networks (MANETs). Due to the\nmobility of nodes, there exists many revisions as scenarios leading to the loop\nformation were found. We demonstrate the loop freedom property violation of\nAODVv2-11, AODVv2-13, and AODVv2-16 through counterexamples. We present our\nproposed version of AODVv2 precisely which not only ensures loop freedom but\nalso improves the performance. \n\n"}
{"id": "1709.02667", "contents": "Title: Implementing Flexible Demand: Real-time Price vs. Market Integration Abstract: This paper proposes an agent-based model that combines both spot and\nbalancing electricity markets. From this model, we develop a multi-agent\nsimulation to study the integration of the consumers' flexibility into the\nsystem. Our study identifies the conditions that real-time prices may lead to\nhigher electricity costs, which in turn contradicts the usual claim that such a\npricing scheme reduces cost. We show that such undesirable behavior is in fact\nsystemic. Due to the existing structure of the wholesale market, the predicted\ndemand that is used in the formation of the price is never realized since the\nflexible users will change their demand according to such established price. As\nthe demand is never correctly predicted, the volume traded through the\nbalancing markets increases, leading to higher overall costs. In this case, the\nsystem can sustain, and even benefit from, a small number of flexible users,\nbut this solution can never upscale without increasing the total costs. To\navoid this problem, we implement the so-called \"exclusive groups.\" Our results\nillustrate the importance of rethinking the current practices so that\nflexibility can be successfully integrated considering scenarios with and\nwithout intermittent renewable sources. \n\n"}
{"id": "1709.04162", "contents": "Title: On the Accuracy of Formal Verification of Selective Defenses for TDoS\n  Attacks Abstract: Telephony Denial of Service (TDoS) attacks target telephony services, such as\nVoice over IP (VoIP), not allowing legitimate users to make calls. There are\nfew defenses that attempt to mitigate TDoS attacks, most of them using IP\nfiltering, with limited applicability. In our previous work, we proposed to use\nselective strategies for mitigating HTTP Application-Layer DDoS Attacks\ndemonstrating their effectiveness in mitigating different types of attacks.\nDeveloping such types of defenses is challenging as there are many design\noptions, eg, which dropping functions and selection algorithms to use. Our\nfirst contribution is to demonstrate both experimentally and by using formal\nverification that selective strategies are suitable for mitigating TDoS\nattacks. We used our formal model to help decide which selective strategies to\nuse with much less effort than carrying out experiments. Our second\ncontribution is a detailed comparison of the results obtained from our formal\nmodels and the results obtained by carrying out experiments. We demonstrate\nthat formal methods is a powerful tool for specifying defenses for mitigating\nDistributed Denial of Service attacks allowing to increase our confidence on\nthe proposed defense before actual implementation. \n\n"}
{"id": "1709.06002", "contents": "Title: NeuRoute: Predictive Dynamic Routing for Software-Defined Networks Abstract: This paper introduces NeuRoute, a dynamic routing framework for Software\nDefined Networks (SDN) entirely based on machine learning, specifically, Neural\nNetworks. Current SDN/OpenFlow controllers use a default routing based on\nDijkstra algorithm for shortest paths, and provide APIs to develop custom\nrouting applications. NeuRoute is a controller-agnostic dynamic routing\nframework that (i) predicts traffic matrix in real time, (ii) uses a neural\nnetwork to learn traffic characteristics and (iii) generates forwarding rules\naccordingly to optimize the network throughput. NeuRoute achieves the same\nresults as the most efficient dynamic routing heuristic but in much less\nexecution time. \n\n"}
{"id": "1709.07974", "contents": "Title: Infrastructure Sharing for Mobile Network Operators: Analysis of\n  Trade-offs and Market Abstract: The conflicting problems of growing mobile service demand and\nunderutilization of dedicated spectrum has given rise to a paradigm where\nmobile network operators (MNOs) share their infrastructure among themselves in\norder to lower their operational costs, while at the same time increase the\nusage of their existing network resources. We model and analyze such an\ninfrastructure sharing system considering a single buyer MNO and multiple\nseller MNOs. Assuming that the locations of the BSs can be modeled as a\nhomogeneous Poisson point process, we find the downlink\nsignal-to-interference-plus-noise ratio (SINR) coverage probability for a user\nserved by the buyer MNO in an infrastructure sharing environment. We analyze\nthe trade-off between increasing the transmit power of a BS and the intensity\nof BSs owned by the buyer MNO required to achieve a given quality-of-service\n(QoS) in terms of the SINR coverage probability. Also, for a seller MNO, we\nanalyze the power consumption of the network per unit area (i.e., areal power\nconsumption) which is shown to be a piecewise continuous function of BS\nintensity, composed of a linear and a convex function. Accordingly, the BS\nintensity of the seller MNO can be optimized to minimize the areal power\nconsumption while achieving a minimum QoS for the buyer MNO. We then use these\nresults to formulate a single-buyer multiple-seller BS infrastructure market.\nThe buyer MNO is concerned with finding which seller MNO to purchase from and\nwhat fraction of BSs to purchase. On the sellers' side, the problem of pricing\nand determining the fraction of infrastructure to be sold is formulated as a\nCournot oligopoly market. We prove that the iterative update of each seller's\nbest response always converges to the Nash Equilibrium. \n\n"}
{"id": "1709.08577", "contents": "Title: Coverage Analysis of a Vehicular Network Modeled as Cox Process Driven\n  by Poisson Line Process Abstract: In this paper, we consider a vehicular network in which the wireless nodes\nare located on a system of roads. We model the roadways, which are\npredominantly straight and randomly oriented, by a Poisson line process (PLP)\nand the locations of nodes on each road as a homogeneous 1D Poisson point\nprocess (PPP). Assuming that each node transmits independently, the locations\nof transmitting and receiving nodes are given by two Cox processes driven by\nthe same PLP. For this setup, we derive the coverage probability of a typical\nreceiver, which is an arbitrarily chosen receiving node, assuming independent\nNakagami-$m$ fading over all wireless channels. Assuming that the typical\nreceiver connects to its closest transmitting node in the network, we first\nderive the distribution of the distance between the typical receiver and the\nserving node to characterize the desired signal power. We then characterize\ncoverage probability for this setup, which involves two key technical\nchallenges. First, we need to handle several cases as the serving node can\npossibly be located on any line in the network and the corresponding\ninterference experienced at the typical receiver is different in each case.\nSecond, conditioning on the serving node imposes constraints on the spatial\nconfiguration of lines, which require careful analysis of the conditional\ndistribution of the lines. We address these challenges in order to accurately\ncharacterize the interference experienced at the typical receiver. We then\nderive an exact expression for coverage probability in terms of the derivative\nof Laplace transform of interference power distribution. We analyze the trends\nin coverage probability as a function of the network parameters: line density\nand node density. We also study the asymptotic behavior of this model and\ncompare the coverage performance with that of a homogeneous 2D PPP model with\nthe same node density. \n\n"}
{"id": "1709.09939", "contents": "Title: Modeling Transmission and Radiation Effects when Exploiting Power Line\n  Networks for Communication Abstract: Power distribution grids are exploited by Power Line Communication (PLC)\ntechnology to convey high frequency data signals. The natural conformation of\nsuch power line networks causes a relevant part of the high frequency signals\ntraveling through them to be radiated instead of being conducted. This causes\nnot only electromagnetic interference (EMI) with devices positioned next to\npower line cables, but also a consistent deterioration of the signal integrity.\nSince existing PLC channel models do not take into account losses due to\nradiation phenomena, this paper responds to the need of developing accurate\nnetwork simulators. A thorough analysis is herein presented about the conducted\nand radiated effects on the signal integrity, digging into differential mode to\ncommon mode signal conversion due to network imbalances. The outcome of this\nwork allows each network element to be described by a mixed-mode transmission\nmatrix. Furthermore, the classical per-unit-length equivalent circuit of\ntransmission lines is extended to incorporate radiation resistances. The\nresults of this paper lay the foundations for future developments of\ncomprehensive power line network models that incorporate conducted and radiated\nphenomena. \n\n"}
{"id": "1710.01476", "contents": "Title: A Comparative Taxonomy and Survey of Public Cloud Infrastructure Vendors Abstract: An increasing number of technology enterprises are adopting cloud-native\narchitectures to offer their web-based products, by moving away from\nprivately-owned data-centers and relying exclusively on cloud service\nproviders. As a result, cloud vendors have lately increased, along with the\nestimated annual revenue they share. However, in the process of selecting a\nprovider's cloud service over the competition, we observe a lack of universal\ncommon ground in terms of terminology, functionality of services and billing\nmodels. This is an important gap especially under the new reality of the\nindustry where each cloud provider has moved towards his own service taxonomy,\nwhile the number of specialized services has grown exponentially. This work\ndiscusses cloud services offered by four dominant, in terms of their current\nmarket share, cloud vendors. We provide a taxonomy of their services and\nsub-services that designates major service families namely computing, storage,\ndatabases, analytics, data pipelines, machine learning, and networking. The aim\nof such clustering is to indicate similarities, common design approaches and\nfunctional differences of the offered services. The outcomes are essential both\nfor individual researchers, and bigger enterprises in their attempt to identify\nthe set of cloud services that will utterly meet their needs without\ncompromises. While we acknowledge the fact that this is a dynamic industry,\nwhere new services arise constantly, and old ones experience important updates,\nthis study paints a solid image of the current offerings and gives prominence\nto the directions that cloud service providers are following. \n\n"}
{"id": "1710.01801", "contents": "Title: FOCAN: A Fog-supported Smart City Network Architecture for Management of\n  Applications in the Internet of Everything Environments Abstract: Smart city vision brings emerging heterogeneous communication technologies\nsuch as Fog Computing (FC) together to substantially reduce the latency and\nenergy consumption of Internet of Everything (IoE) devices running various\napplications. The key feature that distinguishes the FC paradigm for smart\ncities is that it spreads communication and computing resources over the\nwired/wireless access network (e.g., proximate access points and base stations)\nto provide resource augmentation (e.g., cyberforaging) for resource and\nenergy-limited wired/wireless (possibly mobile) things. Moreover, smart city\napplications are developed with the goal of improving the management of urban\nflows and allowing real-time responses to challenges that can arise in users'\ntransactional relationships. This article presents a Fog-supported smart city\nnetwork architecture called Fog Computing Architecture Network (FOCAN), a\nmulti-tier structure in which the applications running on things jointly\ncompute, route, and communicate with one another through the smart city\nenvironment to decrease latency and improve energy provisioning and the\nefficiency of services among things with different capabilities. An important\nconcern that arises with the introduction of FOCAN is the need to avoid\ntransferring data to/from distant things and instead to cover the nearest\nregion for an IoT application. We define three types of communications between\nFOCAN devices (e.g., interprimary, primary, and secondary communication) to\nmanage applications in a way that meets the quality of service standards for\nthe IoE. One of the main advantages of FOCAN is that the devices can provide\nthe services with low energy usage and in an efficient manner. Simulation\nresults for a selected case study demonstrate the tremendous impact of the\nFOCAN energy-efficient solution on the communication performance of various\ntypes of things in smart cities. \n\n"}
{"id": "1710.02459", "contents": "Title: Evaluation of the Performance of Adaptive HTTP Streaming Systems Abstract: Adaptive video streaming over HTTP is becoming omnipresent in our daily life.\nIn the past, dozens of research papers have proposed novel approaches to\naddress different aspects of adaptive streaming and a decent amount of player\nimplementations (commercial and open source) are available. However, state of\nthe art evaluations are sometimes superficial as many proposals only\ninvestigate a certain aspect of the problem or focus on a specific platform -\nplayer implementations used in actual services are rarely considered. HTML5 is\nnow available on many platforms and foster the deployment of adaptive media\nstreaming applications. We propose a common evaluation framework for adaptive\nHTML5 players and demonstrate its applicability by evaluating eight different\nplayers which are actually deployed in real-world services. \n\n"}
{"id": "1710.02966", "contents": "Title: Lightweight Joint Simulation of Vehicular Mobility and Communication\n  with LIMoSim Abstract: The provision of reliable and efficient communication is a key requirement\nfor the deployment of autonomous cars as well as for future Intelligent\nTransportation Systems (ITSs) in smart cities. Novel communications\ntechnologies will have to face highly-complex and extremely dynamic network\ntopologies in a Vehicle-to-Everything (V2X)-context and will require the\nconsideration of mobility information into decision processes for routing,\nhandover and resource allocation. Consequently, researches and developers\nrequire simulation tools that are capable of providing realistic\nrepresentations for both components as well as means for leveraging the\nconvergence of mobility and communication. In this paper, we present a\nlightweight framework for the simulation of vehicular mobility, which has a\ncommunications-oriented perspective by design and is intended to be used in\ncombination with a network simulator. In contrast to existing approaches, it\nworks without requiring Interprocess Communication (IPC) using an integrated\napproach and is therefore able to reduce the complexity of simulation setups\ndramatically. Since mobility and communication share the same codebase, it is\nable to model scenarios with a high level of interdependency between those two\ncomponents. In a proof-of-concept study, we evaluate the proposed simulator in\ndifferent example scenarios in an Long Term Evolution (LTE)- context using\nreal-world map data. \n\n"}
{"id": "1710.03473", "contents": "Title: Recent Advances in Information-Centric Networking based Internet of\n  Things (ICN-IoT) Abstract: Information-Centric Networking (ICN) is being realized as a promising\napproach to accomplish the shortcomings of current IP-address based networking.\nICN models are based on naming the content to get rid of address-space\nscarcity, accessing the content via name-based-routing, caching the content at\nintermediate nodes to provide reliable, efficient data delivery and\nself-certifying contents to ensure better security. Obvious benefits of ICN in\nterms of fast and efficient data delivery and improved reliability raises ICN\nas highly promising networking model for Internet of Things (IoTs) like\nenvironments. IoT aims to connect anyone and/or anything at any time by any\npath on any place. From last decade, IoTs attracts both industry and research\ncommunities. IoTs is an emerging research field and still in its infancy. Thus,\nthis paper presents the potential of ICN for IoTs by providing state-of-the-art\nliterature survey. We discuss briefly the feasibility of ICN features and their\nmodels (and architectures) in the context of IoT. Subsequently, we present a\ncomprehensive survey on ICN based caching, naming, security and mobility\napproaches for IoTs with appropriate classification. Furthermore, we present\noperating systems (OS) and simulation tools for ICNIoT. Finally, we provide\nimportant research challenges and issues faced by ICN for IoTs. \n\n"}
{"id": "1710.04771", "contents": "Title: Applications of Economic and Pricing Models for Resource Management in\n  5G Wireless Networks: A Survey Abstract: This paper presents a comprehensive literature review on applications of\neconomic and pricing theory for resource management in the evolving fifth\ngeneration (5G) wireless networks. The 5G wireless networks are envisioned to\novercome existing limitations of cellular networks in terms of data rate,\ncapacity, latency, energy efficiency, spectrum efficiency, coverage,\nreliability, and cost per information transfer. To achieve the goals, the 5G\nsystems will adopt emerging technologies such as massive Multiple-Input\nMultiple-Output (MIMO), mmWave communications, and dense Heterogeneous Networks\n(HetNets). However, 5G involves multiple entities and stakeholders that may\nhave different objectives, e.g., high data rate, low latency, utility\nmaximization, and revenue/profit maximization. This poses a number of\nchallenges to resource management designs of 5G. While the traditional\nsolutions may neither efficient nor applicable, economic and pricing models\nhave been recently developed and adopted as useful tools to achieve the\nobjectives. In this paper, we review economic and pricing approaches proposed\nto address resource management issues in the 5G wireless networks including\nuser association, spectrum allocation, and interference and power management.\nFurthermore, we present applications of economic and pricing models for\nwireless caching and mobile data offloading. Finally, we highlight important\nchallenges, open issues and future research directions of applying economic and\npricing models to the 5G wireless networks. \n\n"}
{"id": "1710.06089", "contents": "Title: Hierarchical Fog-Cloud Computing for IoT Systems: A Computation\n  Offloading Game Abstract: Fog computing, which provides low-latency computing services at the network\nedge, is an enabler for the emerging Internet of Things (IoT) systems. In this\npaper, we study the allocation of fog computing resources to the IoT users in a\nhierarchical computing paradigm including fog and remote cloud computing\nservices. We formulate a computation offloading game to model the competition\nbetween IoT users and allocate the limited processing power of fog nodes\nefficiently. Each user aims to maximize its own quality of experience (QoE),\nwhich reflects its satisfaction of using computing services in terms of the\nreduction in computation energy and delay. Utilizing a potential game approach,\nwe prove the existence of a pure Nash equilibrium and provide an upper bound\nfor the price of anarchy. Since the time complexity to reach the equilibrium\nincreases exponentially in the number of users, we further propose a\nnear-optimal resource allocation mechanism and prove that in a system with $N$\nIoT users, it can achieve an $\\epsilon$-Nash equilibrium in $O(N/\\epsilon)$\ntime. Through numerical studies, we evaluate the users' QoE as well as the\nequilibrium efficiency. Our results reveal that by utilizing the proposed\nmechanism, more users benefit from computing services in comparison to an\nexisting offloading mechanism. We further show that our proposed mechanism\nsignificantly reduces the computation delay and enables low-latency fog\ncomputing services for delay-sensitive IoT applications. \n\n"}
{"id": "1710.07450", "contents": "Title: Deep Learning Based NLOS Identification with Commodity WLAN Devices Abstract: Identifying line-of-sight (LOS) and non-LOS (NLOS) channel conditions can\nimprove the performance of many wireless applications, such as signal\nstrength-based localization algorithms. For this purpose, channel state\ninformation (CSI) obtained by commodity IEEE 802.11n devices can be used,\nbecause it contains information about channel impulse response (CIR). However,\nbecause of the limited sampling rate of the devices, a high-resolution CIR is\nnot available, and it is difficult to detect the existence of an LOS path from\na single CSI measurement, but it can be inferred from the variation pattern of\nCSI over time. To this end, we propose a recurrent neural network (RNN) model,\nwhich takes a series of CSI to identify the corresponding channel condition. We\ncollect numerous measurement data under an indoor office environment, train the\nproposed RNN model, and compare the performance with those of existing schemes\nthat use handcrafted features. The proposed method efficiently learns a\nnon-linear relationship between input and output, and thus, yields high\naccuracy even for data obtained in a very short period. \n\n"}
{"id": "1710.10033", "contents": "Title: Towards efficient coexistence of IEEE 802.15.4e TSCH and IEEE 802.11 Abstract: A major challenge in wide deployment of smart wireless devices, using\ndifferent technologies and sharing the same 2.4 GHz spectrum, is to achieve\ncoexistence across multiple technologies. The IEEE~802.11 (WLAN) and the IEEE\n802.15.4e TSCH (WSN) where designed with different goals in mind and both play\nimportant roles for respective applications. However, they cause mutual\ninterference and degraded performance while operating in the same space. To\nimprove this situation we propose an approach to enable a cooperative control\nwhich type of network is transmitting at given time, frequency and place.\n  We recognize that TSCH based sensor network is expected to occupy only small\nshare of time, and that the nodes are by design tightly synchronized. We\ndevelop mechanism enabling over-the-air synchronization of the Wi-Fi network to\nthe TSCH based sensor network. Finally, we show that Wi-Fi network can avoid\ntransmitting in the \"collision periods\". We provide full design and show\nprototype implementation based on the Commercial off-the-shelf (COTS) devices.\nOur solution does not require changes in any of the standards. \n\n"}
{"id": "1710.10356", "contents": "Title: Optimal Control of Wireless Computing Networks Abstract: Augmented information (AgI) services allow users to consume information that\nresults from the execution of a chain of service functions that process source\ninformation to create real-time augmented value. Applications include real-time\nanalysis of remote sensing data, real-time computer vision, personalized video\nstreaming, and augmented reality, among others. We consider the problem of\noptimal distribution of AgI services over a wireless computing network, in\nwhich nodes are equipped with both communication and computing resources. We\ncharacterize the wireless computing network capacity region and design a joint\nflow scheduling and resource allocation algorithm that stabilizes the\nunderlying queuing system while achieving a network cost arbitrarily close to\nthe minimum, with a tradeoff in network delay. Our solution captures the unique\nchaining and flow scaling aspects of AgI services, while exploiting the use of\nthe broadcast approach coding scheme over the wireless channel. \n\n"}
{"id": "1710.10911", "contents": "Title: Device-centric Energy Optimization for Edge Cloud Offloading Abstract: A wireless system is considered, where, computationally complex algorithms\nare offloaded from user devices to an edge cloud server, for the purpose of\nefficient battery usage. The main focus of this paper is to characterize and\nanalyze, the trade-off between the energy consumed for processing the data\nlocally, and for offloading. An analytical framework is presented, that\nminimizes the in-device energy consumption, by providing an optimal offloading\ndecision for multiple user devices. A closed form solution is obtained for the\noffloading decision. The solution also provides the amount of computational\ndata that should be offloaded, for the given computational and communication\nresources. Consequently, reduction in the energy consumption is observed. \n\n"}
{"id": "1710.11043", "contents": "Title: Isolation and connectivity in random geometric graphs with self-similar\n  intensity measures Abstract: Random geometric graphs consist of randomly distributed nodes (points), with\npairs of nodes within a given mutual distance linked. In the usual model the\ndistribution of nodes is uniform on a square, and in the limit of infinitely\nmany nodes and shrinking linking range, the number of isolated nodes is Poisson\ndistributed, and the probability of no isolated nodes is equal to the\nprobability the whole graph is connected. Here we examine these properties for\nseveral self-similar node distributions, including smooth and fractal, uniform\nand nonuniform, and finitely ramified or otherwise. We show that nonuniformity\ncan break the Poisson distribution property, but it strengthens the link\nbetween isolation and connectivity. It also stretches out the connectivity\ntransition. Finite ramification is another mechanism for lack of connectivity.\nThe same considerations apply to fractal distributions as smooth, with some\ntechnical differences in evaluation of the integrals and analytical arguments. \n\n"}
{"id": "1710.11403", "contents": "Title: Collaborative Spatial Reuse in Wireless Networks via Selfish Multi-Armed\n  Bandits Abstract: Next-generation wireless deployments are characterized by being dense and\nuncoordinated, which often leads to inefficient use of resources and poor\nperformance. To solve this, we envision the utilization of completely\ndecentralized mechanisms to enable Spatial Reuse (SR). In particular, we focus\non dynamic channel selection and Transmission Power Control (TPC). We rely on\nReinforcement Learning (RL), and more specifically on Multi-Armed Bandits\n(MABs), to allow networks to learn their best configuration. In this work, we\nstudy the exploration-exploitation trade-off by means of the\n$\\varepsilon$-greedy, EXP3, UCB and Thompson sampling action-selection, and\ncompare their performance. In addition, we study the implications of selecting\nactions simultaneously in an adversarial setting (i.e., concurrently), and\ncompare it with a sequential approach. Our results show that optimal\nproportional fairness can be achieved, even when no information about\nneighboring networks is available to the learners and Wireless Networks (WNs)\noperate selfishly. However, there is high temporal variability in the\nthroughput experienced by the individual networks, specially for\n$\\varepsilon$-greedy and EXP3. These strategies, contrary to UCB and Thompson\nsampling, base their operation on the absolute experienced reward, rather than\non its distribution. We identify the cause of this variability to be the\nadversarial setting of our setup in which the set of most played actions\nprovide intermittent good/poor performance depending on the neighboring\ndecisions. We also show that learning sequentially, even if using a selfish\nstrategy, contributes to minimize this variability. The sequential approach is\ntherefore shown to effectively deal with the challenges posed by the\nadversarial settings that are typically found in decentralized WNs. \n\n"}
{"id": "1711.01351", "contents": "Title: Uplink Performance Analysis of a Drone Cell in a Random Field of Ground\n  Interferers Abstract: Aerial base stations are a promising technology to increase the capabilities\nof the existing communication networks. However, the existing analytical\nframeworks do not sufficiently characterize the impact of ground interferers on\nthe aerial base stations. In order to address this issue, we model the effect\nof interference coming from the coexisting ground networks on the aerial link,\nwhich could be the uplink of an aerial cell served by a drone base station. By\nconsidering a Poisson field of ground interferers, we characterize the\naggregate interference experienced by the drone. This result includes the\neffect of the drone antenna pattern, the height-dependent shadowing, and\nvarious types of environment. We show that the benefits that a drone obtains\nfrom a better line-of-sight (LoS) at high altitudes is counteracted by a high\nvulnerability to the interference coming from the ground. However, by deriving\nthe link coverage probability and transmission rate we show that a drone base\nstation is still a promising technology if the overall system is properly\ndimensioned according to the given density and transmission power of the\ninterferers. Particularly, our results illustrate how the benefits of such\nnetwork is maximized by defining the optimal drone altitude and signal-to-\ninterference (SIR) requirement. \n\n"}
{"id": "1711.01478", "contents": "Title: OCDN: Oblivious Content Distribution Networks Abstract: As publishers increasingly use Content Distribution Networks (CDNs) to\ndistribute content across geographically diverse networks, CDNs themselves are\nbecoming unwitting targets of requests for both access to user data and content\ntakedown. From copyright infringement to moderation of online speech, CDNs have\nfound themselves at the forefront of many recent legal quandaries. At the heart\nof the tension, however, is the fact that CDNs have rich information both about\nthe content they are serving and the users who are requesting that content.\nThis paper offers a technical contribution that is relevant to this ongoing\ntension with the design of an Oblivious CDN (OCDN); the system is both\ncompatible with the existing Web ecosystem of publishers and clients and hides\nfrom the CDN both the content it is serving and the users who are requesting\nthat content. OCDN is compatible with the way that publishers currently host\ncontent on CDNs. Using OCDN, publishers can use multiple CDNs to publish\ncontent; clients retrieve content through a peer-to-peer anonymizing network of\nproxies. Our prototype implementation and evaluation of OCDN show that the\nsystem can obfuscate both content and clients from the CDN operator while still\ndelivering content with good performance. \n\n"}
{"id": "1711.01938", "contents": "Title: Single-Carrier Modulation versus OFDM for Millimeter-Wave Wireless MIMO Abstract: This paper presents results on the achievable spectral efficiency and on the\nenergy efficiency for a wireless multiple-input-multiple-output (MIMO) link\noperating at millimeter wave frequencies (mmWave) in a typical 5G scenario. Two\ndifferent single-carrier modem schemes are considered, i.e., a traditional\nmodulation scheme with linear equalization at the receiver, and a\nsingle-carrier modulation with cyclic prefix, frequency-domain equalization and\nFFT-based processing at the receiver; these two schemes are compared with a\nconventional MIMO-OFDM transceiver structure. Our analysis jointly takes into\naccount the peculiar characteristics of MIMO channels at mmWave frequencies,\nthe use of hybrid (analog-digital) pre-coding and post-coding beamformers, the\nfinite cardinality of the modulation structure, and the non-linear behavior of\nthe transmitter power amplifiers. Our results show that the best performance is\nachieved by single-carrier modulation with time-domain equalization, which\nexhibits the smallest loss due to the non-linear distortion, and whose\nperformance can be further improved by using advanced equalization schemes.\nResults also confirm that performance gets severely degraded when the link\nlength exceeds 90-100 meters and the transmit power falls below 0 dBW. \n\n"}
{"id": "1711.02666", "contents": "Title: Tensor-Generative Adversarial Network with Two-dimensional Sparse\n  Coding: Application to Real-time Indoor Localization Abstract: Localization technology is important for the development of indoor\nlocation-based services (LBS). Global Positioning System (GPS) becomes invalid\nin indoor environments due to the non-line-of-sight issue, so it is urgent to\ndevelop a real-time high-accuracy localization approach for smartphones.\nHowever, accurate localization is challenging due to issues such as real-time\nresponse requirements, limited fingerprint samples and mobile device storage.\nTo address these problems, we propose a novel deep learning architecture:\nTensor-Generative Adversarial Network (TGAN).\n  We first introduce a transform-based 3D tensor to model fingerprint samples.\nInstead of those passive methods that construct a fingerprint database as a\nprior, our model applies artificial neural network with deep learning to train\nnetwork classifiers and then gives out estimations. Then we propose a novel\ntensor-based super-resolution scheme using the generative adversarial network\n(GAN) that adopts sparse coding as the generator network and a residual\nlearning network as the discriminator. Further, we analyze the performance of\ntensor-GAN and implement a trace-based localization experiment, which achieves\nbetter performance. Compared to existing methods for smartphones indoor\npositioning, that are energy-consuming and high demands on devices, TGAN can\ngive out an improved solution in localization accuracy, response time and\nimplementation complexity. \n\n"}
{"id": "1711.02805", "contents": "Title: The (thin) Bridges of AS Connectivity: Measuring Dependency using AS\n  Hegemony Abstract: Inter-domain routing is a crucial part of the Internet designed for arbitrary\npolicies, economical models, and topologies. This versatility translates into a\nsubstantially complex system that is hard to comprehend. Monitoring the\ninter-domain routing infrastructure is however essential for understanding the\ncurrent state of the Internet and improving it. In this paper we design a\nmethodology to answer two simple questions: Which are the common transit\nnetworks used to reach a certain AS? How much does this AS depends on these\ntransit networks? To answer these questions we digest AS paths advertised with\nthe Border Gateway Protocol (BGP) into AS graphs and measure node centrality,\nthat is the likelihood of an AS to lie on paths between two other ASes. Our\nproposal relies solely on the AS hegemony metric, a new way to quantify node\ncentrality while taking into account the bias towards the partial view offered\nby BGP. Our analysis using 14 years of BGP data refines our knowledge on\nInternet flattening but also exhibits the consolidated position of tier-1\nnetworks in today's IPv4 and IPv6 Internet. We also study the connectivity to\ntwo content providers (Google and Akamai) and investigate the AS dependency of\nnetworks hosting DNS root servers. These case studies emphasize the benefits of\nthe proposed method to assist ISPs in planning and assessing infrastructure\ndeployment. \n\n"}
{"id": "1711.03941", "contents": "Title: A TTL-based Approach for Content Placement in Edge Networks Abstract: Edge networks are promising to provide better services to users by\nprovisioning computing and storage resources at the edge of networks. However,\ndue to the uncertainty and diversity of user interests, content popularity,\ndistributed network structure, cache sizes, it is challenging to decide where\nto place the content, and how long it should be cached. In this paper, we study\nthe utility optimization of content placement at edge networks through\ntimer-based (TTL) policies. We propose provably optimal distributed algorithms\nthat operate at each network cache to maximize the overall network utility. Our\nTTL-based optimization model provides theoretical answers to how long each\ncontent must be cached, and where it should be placed in the edge network.\nExtensive evaluations show that our algorithm significantly outperforms path\nreplication with conventional caching algorithms over some network topologies. \n\n"}
{"id": "1711.05932", "contents": "Title: A Design-Time/Run-Time Application Mapping Methodology for Predictable\n  Execution Time in MPSoCs Abstract: Executing multiple applications on a single MPSoC brings the major challenge\nof satisfying multiple quality requirements regarding real-time, energy, etc.\nHybrid application mapping denotes the combination of design-time analysis with\nrun-time application mapping. In this article, we present such a methodology,\nwhich comprises a design space exploration coupled with a formal performance\nanalysis. This results in several resource reservation configurations,\noptimized for multiple objectives, with verified real-time guarantees for each\nindividual application. The Pareto-optimal configurations are handed over to\nrun-time management which searches for a suitable mapping according to this\ninformation. To provide any real-time guarantees, the performance analysis\nneeds to be composable and the influence of the applications on each other has\nto be bounded. We achieve this either by spatial or a novel temporal isolation\nfor tasks and by exploiting composable NoCs. With the proposed temporal\nisolation, tasks of different applications can be mapped to the same resource\nwhile with spatial isolation, one computing resource can be exclusively used by\nonly one application. The experiments reveal that the success rate in finding\nfeasible application mappings can be increased by the proposed temporal\nisolation by up to 30% and energy consumption can be reduced compared to\nspatial isolation. \n\n"}
{"id": "1711.05969", "contents": "Title: Physical-Layer Schemes for Wireless Coded Caching Abstract: We investigate the potentials of applying the coded caching paradigm in\nwireless networks. In order to do this, we investigate physical layer schemes\nfor downlink transmission from a multiantenna transmitter to several\ncache-enabled users. As the baseline scheme we consider employing coded caching\non top of max-min fair multicasting, which is shown to be far from optimal at\nhigh SNR values. Our first proposed scheme, which is near-optimal in terms of\nDoF, is the natural extension of multiserver coded caching to Gaussian\nchannels. As we demonstrate, its finite SNR performance is not satisfactory,\nand thus we propose a new scheme in which the linear combination of messages is\nimplemented in the finite field domain, and the one-shot precoding for the MISO\ndownlink is implemented in the complex field. While this modification results\nin the same near-optimal DoF performance, we show that this leads to\nsignificant performance improvement at finite SNR. Finally, we extend our\nscheme to the previously considered cache-enabled interference channels, and\nmoreover, we provide an Ergodic rate analysis of our scheme. Our results convey\nthe important message that although directly translating schemes from the\nnetwork coding ideas to wireless networks may work well at high SNR values,\ncareful modifications need to be considered for acceptable finite SNR\nperformance. \n\n"}
{"id": "1711.06154", "contents": "Title: Reliable Video Streaming over mmWave with Multi Connectivity and Network\n  Coding Abstract: The next generation of multimedia applications will require the\ntelecommunication networks to support a higher bitrate than today, in order to\ndeliver virtual reality and ultra-high quality video content to the users. Most\nof the video content will be accessed from mobile devices, prompting the\nprovision of very high data rates by next generation (5G) cellular networks. A\npossible enabler in this regard is communication at mmWave frequencies, given\nthe vast amount of available spectrum that can be allocated to mobile users;\nhowever, the harsh propagation environment at such high frequencies makes it\nhard to provide a reliable service. This paper presents a reliable video\nstreaming architecture for mmWave networks, based on multi connectivity and\nnetwork coding, and evaluates its performance using a novel combination of the\nns-3 mmWave module, real video traces and the network coding library Kodo. The\nresults show that it is indeed possible to reliably stream video over cellular\nmmWave links, while the combination of multi connectivity and network coding\ncan support high video quality with low latency. \n\n"}
{"id": "1711.08045", "contents": "Title: Standards for enabling heterogeneous IaaS cloud federations Abstract: Technology market is continuing a rapid growth phase where different resource\nproviders and Cloud Management Frameworks are positioning to provide ad-hoc\nsolutions -in terms of management interfaces, information discovery or billing-\ntrying to differentiate from competitors but that as a result remain\nincompatible between them when addressing more complex scenarios like federated\nclouds. Grasping interoperability problems present in current infrastructures\nis then a must-do, tackled by studying how existing and emerging standards\ncould enhance user experience in the cloud ecosystem. In this paper we will\nreview the current open challenges in Infrastructure as a Service cloud\ninteroperability and federation, as well as point to the potential standards\nthat should alleviate these problems. \n\n"}
{"id": "1711.10079", "contents": "Title: Towards Provably Invisible Network Flow Fingerprints Abstract: Network traffic analysis reveals important information even when messages are\nencrypted. We consider active traffic analysis via flow fingerprinting by\ninvisibly embedding information into packet timings of flows. In particular,\nassume Alice wishes to embed fingerprints into flows of a set of network input\nlinks, whose packet timings are modeled by Poisson processes, without being\ndetected by a watchful adversary Willie. Bob, who receives the set of\nfingerprinted flows after they pass through the network modeled as a collection\nof independent and parallel $M/M/1$ queues, wishes to extract Alice's embedded\nfingerprints to infer the connection between input and output links of the\nnetwork. We consider two scenarios: 1) Alice embeds fingerprints in all of the\nflows; 2) Alice embeds fingerprints in each flow independently with probability\n$p$. Assuming that the flow rates are equal, we calculate the maximum number of\nflows in which Alice can invisibly embed fingerprints while having those\nfingerprints successfully decoded by Bob. Then, we extend the construction and\nanalysis to the case where flow rates are distinct, and discuss the extension\nof the network model. \n\n"}
{"id": "1712.00237", "contents": "Title: Demystifying Mobile Web Browsing under Multiple Protocols Abstract: With the popularity of mobile devices, such as smartphones, tablets, users\nprefer visiting Web pages on mobile devices. Meanwhile, HTTP(S) plays as the\nmajor protocol to deliver Web contents, and has served the Web well for more\nthan 15 years. However, as the Web pages grow increasingly complex to provide\nmore content and functionality, the shortcomings and inflexibility of HTTP\nbecome more and more urgent to solve, e.g., the sluggish page load, insecure\ncontent, redundant transfer, etc. SPDY and HTTP/2 are promoted to solve the\nshortcomings and inflexibilities of HTTP/1.x. We are interested in how Web\npages perform on smartphones with different protocols, including HTTP, HTTPS,\nSPDY, and HTTP/2. In this paper, we divide our experiments into two parts.\nFirst, in order to simplify our analysis, we develop our own HTTP client\nignoring complicated process in real browsers to fetch synthetic Web pages with\npre-specified object sizes and object numbers with different protocols,\nrespectively. Meanwhile, we emulate different network conditions between client\nand server using Traffic Control. In order to test with real browsers, we clone\nAlexa top 200 websites, which have the corresponding mobile version, into our\nlocal host. Meanwhile, we control mobile Chrome browser to load those Web pages\nwith different protocols and emulate different network conditions using Traffic\nControl. We identify how Web page characteristics and network conditions affect\nWeb performance on smartphones for each protocol. We also conduct experiments\non a low-end device to observe if a less powerful processor could affect Web\npage performance for each protocol. \n\n"}
{"id": "1712.01905", "contents": "Title: Simulating Opportunistic Networks: Survey and Future Directions Abstract: Simulation is one of the most powerful tools we have for evaluating the\nperformance of Opportunistic Networks. In this survey, we focus on available\ntools and models, compare their performance and precision and experimentally\nshow the scalability of different simulators. We also perform a gap analysis of\nstate-of-the-art Opportunistic Network simulations and sketch out possible\nfurther development and lines of research. This survey is targeted at students\nstarting work and research in this area while also serving as a valuable source\nof information for experienced researchers. \n\n"}
{"id": "1712.02951", "contents": "Title: Software Defined Applications in Cellular and Optical Networks Abstract: Small wireless cells have the potential to overcome bottlenecks in wireless\naccess through the sharing of spectrum resources. A novel access backhaul\nnetwork architecture based on a Smart Gateway (Sm-GW) between the small cell\nbase stations, e.g., LTE eNBs, and the conventional backhaul gateways, e.g.,\nLTE Servicing/Packet Gateways (S/P-GWs) has been introduced to address the\nbottleneck. The Sm-GW flexibly schedules uplink transmissions for the eNBs.\nBased on software defined networking (SDN) a management mechanism that allows\nmultiple operator to flexibly inter-operate via multiple Sm-GWs with a\nmultitude of small cells has been proposed. This dissertation also\ncomprehensively survey the studies that examine the SDN paradigm in optical\nnetworks. Along with the PHY functional split improvements, the performance of\nDistributed Converged Cable Access Platform (DCCAP) in the cable architectures\nespecially for the Remote-PHY and Remote-MACPHY nodes has been evaluated. In\nthe PHY functional split, in addition to the re-use of infrastructure with a\ncommon FFT module for multiple technologies, a novel cross functional split\ninteraction to cache the repetitive QAM symbols across time at the remote node\nto reduce the transmission rate requirement of the fronthaul link has been\nproposed. \n\n"}
{"id": "1712.03530", "contents": "Title: Datacenter Traffic Control: Understanding Techniques and Trade-offs Abstract: Datacenters provide cost-effective and flexible access to scalable compute\nand storage resources necessary for today's cloud computing needs. A typical\ndatacenter is made up of thousands of servers connected with a large network\nand usually managed by one operator. To provide quality access to the variety\nof applications and services hosted on datacenters and maximize performance, it\ndeems necessary to use datacenter networks effectively and efficiently.\nDatacenter traffic is often a mix of several classes with different priorities\nand requirements. This includes user-generated interactive traffic, traffic\nwith deadlines, and long-running traffic. To this end, custom transport\nprotocols and traffic management techniques have been developed to improve\ndatacenter network performance.\n  In this tutorial paper, we review the general architecture of datacenter\nnetworks, various topologies proposed for them, their traffic properties,\ngeneral traffic control challenges in datacenters and general traffic control\nobjectives. The purpose of this paper is to bring out the important\ncharacteristics of traffic control in datacenters and not to survey all\nexisting solutions (as it is virtually impossible due to massive body of\nexisting research). We hope to provide readers with a wide range of options and\nfactors while considering a variety of traffic control mechanisms. We discuss\nvarious characteristics of datacenter traffic control including management\nschemes, transmission control, traffic shaping, prioritization, load balancing,\nmultipathing, and traffic scheduling. Next, we point to several open challenges\nas well as new and interesting networking paradigms. At the end of this paper,\nwe briefly review inter-datacenter networks that connect geographically\ndispersed datacenters which have been receiving increasing attention recently\nand pose interesting and novel research problems. \n\n"}
{"id": "1712.04201", "contents": "Title: On the Energy-Efficient Deployment for Ultra-Dense Heterogeneous\n  Networks with NLoS and LoS Transmissions Abstract: We investigate network performance of ultra-dense heterogeneous networks\n(HetNets) and study the maximum energy-efficient base station (BS) deployment\nincorporating probabilistic non-line-of-sight (NLoS) and line-of-sight (LoS)\ntransmissions. First, we develop an analytical framework with the maximum\ninstantaneous received power (MIRP) and the maximum average received power\n(MARP) association schemes to model the coverage probability and related\nperformance metrics, e.g., the potential throughput (PT) and the energy\nefficiency (EE). Second, we formulate two optimization problems to achieve the\nmaximum energy-efficient deployment solution with specific service criteria.\nSimulation results show that there are tradeoffs among the coverage\nprobability, the total power consumption, and the EE. To be specific, the\nmaximum coverage probability with ideal power consumption is superior to that\nwith practical power consumption when the total power constraint is small and\ninferior to that with practical power consumption when the total power\nconstraint becomes large. Moreover, the maximum EE is a decreasing function\nwith respect to the coverage probability constraint. \n\n"}
{"id": "1712.04687", "contents": "Title: Can Balloons Produce Li-Fi? A Disaster Management Perspective Abstract: Natural calamities and disasters disrupt the conventional communication\nsetups and the wireless bandwidth becomes constrained. A safe and\ncost-effective solution for communication and data access in such scenarios is\nlong needed. Light-Fidelity (Li-Fi) which promises wireless access to data at\nhigh speeds using visible light can be a good option. Visible light being safe\nto use for wireless access in such affected environments also provides\nillumination. Importantly, when a Li-Fi unit is attached to an air balloon and\na network of such Li-Fi balloons are coordinated to form a Li-Fi balloon\nnetwork, data can be accessed anytime and anywhere required and hence many\nlives can be tracked and saved. We propose this idea of a Li-Fi balloon and\ngive an overview of its design using the Philips Li-Fi hardware. Further, we\npropose the concept of a balloon network and coin it with an acronym, the\nLiBNet. We consider the balloons to be arranged as a homogeneous Poisson point\nprocess in the LiBNet and we derive the mean co-channel interference for such\nan arrangement. \n\n"}
{"id": "1712.05457", "contents": "Title: 60 GHz Blockage Study Using Phased Arrays Abstract: The millimeter wave (mmWave) frequencies offer the potential for enormous\ncapacity wireless links. However, designing robust communication systems at\nthese frequencies requires that we understand the channel dynamics over both\ntime and space: mmWave signals are extremely vulnerable to blocking and the\nchannel can thus rapidly appear and disappear with small movement of obstacles\nand reflectors. In rich scattering environments, different paths may experience\ndifferent blocking trajectories and understanding these multi-path blocking\ndynamics is essential for developing and assessing beamforming and\nbeam-tracking algorithms. This paper presents the design and experimental\nresults of a novel measurement system which uses phased arrays to perform\nmmWave dynamic channel measurements. Specifically, human blockage and its\neffects across multiple paths are investigated with only several microseconds\nbetween successive measurements. From these measurements we develop a modeling\ntechnique which uses low-rank tensor factorization to separate the available\npaths so that their joint statistics can be understood. \n\n"}
{"id": "1712.05677", "contents": "Title: Timely-Throughput Optimal Scheduling with Prediction Abstract: Motivated by the increasing importance of providing delay-guaranteed services\nin general computing and communication systems, and the recent wide adoption of\nlearning and prediction in network control, in this work, we consider a general\nstochastic single-server multi-user system and investigate the fundamental\nbenefit of predictive scheduling in improving timely-throughput, being the rate\nof packets that are delivered to destinations before their deadlines. By\nadopting an error rate-based prediction model, we first derive a Markov\ndecision process (MDP) solution to optimize the timely-throughput objective\nsubject to an average resource consumption constraint. Based on a packet-level\ndecomposition of the MDP, we explicitly characterize the optimal scheduling\npolicy and rigorously quantify the timely-throughput improvement due to\npredictive-service, which scales as\n$\\Theta(p\\left[C_{1}\\frac{(a-a_{\\max}q)}{p-q}\\rho^{\\tau}+C_{2}(1-\\frac{1}{p})\\right](1-\\rho^{D}))$,\nwhere $a, a_{\\max}, \\rho\\in(0, 1), C_1>0, C_2\\ge0$ are constants, $p$ is the\ntrue-positive rate in prediction, $q$ is the false-negative rate, $\\tau$ is the\npacket deadline and $D$ is the prediction window size. We also conduct\nextensive simulations to validate our theoretical findings. Our results provide\nnovel insights into how prediction and system parameters impact performance and\nprovide useful guidelines for designing predictive low-latency control\nalgorithms. \n\n"}
{"id": "1712.06358", "contents": "Title: The Saga of KPR: Theoretical and Experimental developments Abstract: In this article, we present a brief narration of the origin and the overview\nof the recent developments done on the Kolkata Paise Restaurant (KPR) problem,\nwhich can serve as a prototype for a broader class of resource allocation\nproblems in the presence of a large number of competing agents, typically\nstudied using coordination and anti-coordination games. We discuss the KPR and\nits several extensions, as well as its applications in many economic and social\nphenomena. We end the article with some discussions on our ongoing experimental\nanalysis of the same problem. We demonstrate that this provides an interesting\npicture of how people analyze complex situations, and design their strategies\nor react to them. \n\n"}
{"id": "1712.06634", "contents": "Title: Better Algorithms for Hybrid Circuit and Packet Switching in Data\n  Centers Abstract: Hybrid circuit and packet switching for data center networking (DCN) has\nreceived considerable research attention recently. A hybrid-switched DCN\nemploys a much faster circuit switch that is reconfigurable with a nontrivial\ncost, and a much slower packet switch that is reconfigurable with no cost, to\ninterconnect its racks of servers. The research problem is, given a traffic\ndemand matrix (between the racks), how to compute a good circuit switch\nconfiguration schedule so that the vast majority of the traffic demand is\nremoved by the circuit switch, leaving a remaining demand matrix that contains\nonly small elements for the packet switch to handle. In this paper, we propose\ntwo new hybrid switch scheduling algorithms under two different scheduling\nconstraints. Our first algorithm, called 2-hop Eclipse, strikes a much better\ntradeoff between the resulting performance (of the hybrid switch) and the\ncomputational complexity (of the algorithm) than the state of the art solution\nEclipse/Eclipse++. Our second algorithm, called BFF (best first fit), is the\nfirst hybrid switching solution that exploits the potential partial\nreconfiguration capability of the circuit switch for performance gains. \n\n"}
{"id": "1712.07021", "contents": "Title: Cache-Aided Private Information Retrieval with Partially Known Uncoded\n  Prefetching: Fundamental Limits Abstract: We consider the problem of private information retrieval (PIR) from $N$\nnon-colluding and replicated databases, when the user is equipped with a cache\nthat holds an uncoded fraction $r$ of the symbols from each of the $K$ stored\nmessages in the databases. This model operates in a two-phase scheme, namely,\nthe prefetching phase where the user acquires side information and the\nretrieval phase where the user privately downloads the desired message. In the\nprefetching phase, the user receives $\\frac{r}{N}$ uncoded fraction of each\nmessage from the $n$th database. This side information is known only to the\n$n$th database and unknown to the remaining databases, i.e., the user possesses\n\\emph{partially known} side information. We investigate the optimal normalized\ndownload cost $D^*(r)$ in the retrieval phase as a function of $K$, $N$, $r$.\nWe develop lower and upper bounds for the optimal download cost. The bounds\nmatch in general for the cases of very low caching ratio ($r \\leq\n\\frac{1}{N^{K-1}}$) and very high caching ratio ($r \\geq\n\\frac{K-2}{N^2-3N+KN}$). We fully characterize the optimal download cost\ncaching ratio tradeoff for $K=3$. For general $K$, $N$, and $r$, we show that\nthe largest gap between the achievability and the converse bounds is\n$\\frac{5}{32}$. \n\n"}
{"id": "1712.07307", "contents": "Title: Network Cache Design under Stationary Requests: Exact Analysis and\n  Poisson Approximation Abstract: The design of caching algorithms to maximize hit probability has been\nextensively studied. In this paper, we associate each content with a utility,\nwhich is a function of either the corresponding content hit rate or hit\nprobability. We formulate a cache optimization problem to maximize the sum of\nutilities over all contents under stationary and ergodic request processes.\nThis problem is non-convex in general but we reformulate it as a convex\noptimization problem when the inter-request time (irt) distribution has a\nnon-increasing hazard rate function. We provide explicit optimal solutions for\nsome irt distributions, and compare the solutions of the hit-rate based (HRB)\nand hit-probability based (HPB) problems. We formulate a reverse engineering\nbased dual implementation of LRU under stationary arrivals. We also propose\ndecentralized algorithms that can be implemented using limited information and\nuse a discrete time Lyapunov technique (DTLT) to correctly characterize their\nstability. We find that decentralized algorithms that solve HRB are more robust\nthan decentralized HPB algorithms. Informed by these results, we further\npropose lightweight Poisson approximate decentralized and online algorithms\nthat are accurate and efficient in achieving optimal hit rates and hit\nprobabilities. \n\n"}
{"id": "1712.07458", "contents": "Title: Disruptive events in high-density cellular networks Abstract: Stochastic geometry models are used to study wireless networks, particularly\ncellular phone networks, but most of the research focuses on the typical user,\noften ignoring atypical events, which can be highly disruptive and of interest\nto network operators. We examine atypical events when a unexpected large\nproportion of users are disconnected or connected by proposing a hybrid\napproach based on ray launching simulation and point process theory. This work\nis motivated by recent results using large deviations theory applied to the\nsignal-to-interference ratio. This theory provides a tool for the stochastic\nanalysis of atypical but disruptive events, particularly when the density of\ntransmitters is high. For a section of a European city, we introduce a new\nstochastic model of a single network cell that uses ray launching data\ngenerated with the open source RaLaNS package, giving deterministic path loss\nvalues. We collect statistics on the fraction of (dis)connected users in the\nuplink, and observe that the probability of an unexpected large proportion of\ndisconnected users decreases exponentially when the transmitter density\nincreases. This observation implies that denser networks become more stable in\nthe sense that the probability of the fraction of (dis)connected users\ndeviating from its mean, is exponentially small. We also empirically obtain and\nillustrate the density of users for network configurations in the disruptive\nevent, which highlights the fact that such bottleneck behaviour not only stems\nfrom too many users at the cell boundary, but also from the near-far effect of\nmany users in the immediate vicinity of the base station. We discuss the\nimplications of these findings and outline possible future research directions. \n\n"}
{"id": "1712.07649", "contents": "Title: Trading Strategies with Position Limits Abstract: Whether you trade futures for yourself or a hedge fund, your strategy is\ncounted. Long and short position limits make the number of unique strategies\nfinite. Formulas of the numbers of strategies, transactions, do nothing actions\nare derived. A discrete distribution of actions, corresponding probability\nmass, cumulative distribution and characteristic functions, moments, extreme\nvalues are presented. Strategies time slice distributions are determined.\nVector properties of trading strategies are studied. Algebraic not associative,\ncommutative, initial magmas with invertible elements control trading positions\nand strategies. Maximum profit strategies, MPS, and optimal trading elements\ncan define trading patterns. Dynkin introduced the term interpreted in English\nas \"Markov time\" in 1963. Neftci applied it for the formalization of Technical\nAnalysis in 1991. \n\n"}
{"id": "1712.08768", "contents": "Title: Learning-Based Computation Offloading for IoT Devices with Energy\n  Harvesting Abstract: Internet of Things (IoT) devices can apply mobile-edge computing (MEC) and\nenergy harvesting (EH) to provide the satisfactory quality of experiences for\ncomputation intensive applications and prolong the battery lifetime. In this\narticle, we investigate the computation offloading for IoT devices with energy\nharvesting in wireless networks with multiple MEC devices such as base stations\nand access points, each with different computation resource and radio\ncommunication capability. We propose a reinforcement learning based computation\noffloading framework for an IoT device to choose the MEC device and determine\nthe offloading rate according to the current battery level, the previous radio\nbandwidth to each MEC device and the predicted amount of the harvested energy.\nA \"hotbooting\" Q-learning based computation offloading scheme is proposed for\nan IoT device to achieve the optimal offloading performance without being aware\nof the MEC model, the energy consumption and computation latency model. We also\npropose a fast deep Q-network (DQN) based offloading scheme, which combines the\ndeep learning and hotbooting techniques to accelerate the learning speed of\nQ-learning. We show that the proposed schemes can achieve the optimal\noffloading policy after sufficiently long learning time and provide their\nperformance bounds under two typical MEC scenarios. Simulations are performed\nfor IoT devices that use wireless power transfer to capture the ambient\nradio-frequency signals to charge the IoT batteries. Simulation results show\nthat the fast DQN-based offloading scheme reduces the energy consumption,\ndecreases the computation delay and the task drop ratio, and increases the\nutility of the IoT device in dynamic MEC, compared with the benchmark\nQ-learning based offloading. \n\n"}
{"id": "1712.09315", "contents": "Title: Who is Smarter? Intelligence Measure of Learning-based Cognitive Radios Abstract: Cognitive radio (CR) is considered as a key enabling technology for dynamic\nspectrum access to improve spectrum efficiency. Although the CR concept was\ninvented with the core idea of realizing cognition, the research on measuring\nCR cognitive capabilities and intelligence is largely open. Deriving the\nintelligence measure of CR not only can lead to the development of new CR\ntechnologies, but also makes it possible to better configure the networks by\nintegrating CRs with different cognitive capabilities. In this paper, for the\nfirst time, we propose a data-driven methodology to quantitatively measure the\nintelligence factors of the CR with learning capabilities. The basic idea of\nour methodology is to run various tests on the CR in different spectrum\nenvironments under different settings and obtain various performance data on\ndifferent metrics. Then we apply factor analysis on the performance data to\nidentify and quantize the intelligence factors and cognitive capabilities of\nthe CR. More specifically, we present a case study consisting of 144 different\ntypes of CRs. The CRs are different in terms of learning-based dynamic spectrum\naccess strategies, number of sensors, sensing accuracy, processing speed, and\nalgorithmic complexity. Five intelligence factors are identified for the CRs\nthrough our data analysis.We show that these factors comply well with the\nnature of the tested CRs, which validates the proposed intelligence measure\nmethodology. \n\n"}
{"id": "1712.10041", "contents": "Title: Caching under Content Freshness Constraints Abstract: Several real-time delay-sensitive applications pose varying degrees of\nfreshness demands on the requested content. The performance of cache\nreplacement policies that are agnostic to these demands is likely to be\nsub-optimal. Motivated by this concern, in this paper, we study caching\npolicies under a request arrival process which incorporates user freshness\ndemands. We consider the performance metric to be the steady-state cache hit\nprobability. We first provide a universal upper bound on the performance of any\ncaching policy. We then analytically obtain the content-wise hit-rates for the\nLeast Popular (LP) policy and provide sufficient conditions for the asymptotic\noptimality of cache performance under this policy. Next, we obtain an accurate\napproximation for the LRU hit-rates in the regime of large content population.\nTo this end, we map the characteristic time of a content in the LRU policy to\nthe classical Coupon Collector's Problem and show that it sharply concentrates\naround its mean. Further, we develop modified versions of these policies which\neject cache redundancies present in the form of stale contents. Finally, we\npropose a new policy which outperforms the above policies by explicitly using\nfreshness specifications of user requests to prioritize among the cached\ncontents. We corroborate our analytical insights with extensive simulations. \n\n"}
{"id": "1712.10061", "contents": "Title: The Age of Information in Multihop Networks Abstract: Information updates in multihop networks such as Internet of Things (IoT) and\nintelligent transportation systems have received significant recent attention.\nIn this paper, we minimize the age of a single information flow in\ninterference-free multihop networks. When preemption is allowed and the packet\ntransmission times are exponentially distributed, we prove that a preemptive\nLast-Generated, First-Served (LGFS) policy results in smaller age processes\nacross all nodes in the network than any other causal policy (in a stochastic\nordering sense). In addition, for the class of New-Better-than-Used (NBU)\ndistributions, we show that the non-preemptive LGFS policy is within a constant\nage gap from the optimum average age. In contrast, our numerical result shows\nthat the preemptive LGFS policy can be very far from the optimum for some NBU\ntransmission time distributions. Finally, when preemption is prohibited and the\npacket transmission times are arbitrarily distributed, the non-preemptive LGFS\npolicy is shown to minimize the age processes across all nodes in the network\namong all work-conserving policies (again in a stochastic ordering sense).\nInterestingly, these results hold under quite general conditions, including (i)\narbitrary packet generation and arrival times, and (ii) for minimizing both the\nage processes in stochastic ordering and any non-decreasing functional of the\nage processes. \n\n"}
{"id": "1801.02099", "contents": "Title: Joint Data Compression and Caching: Approaching Optimality with\n  Guarantees Abstract: We consider the problem of optimally compressing and caching data across a\ncommunication network. Given the data generated at edge nodes and a routing\npath, our goal is to determine the optimal data compression ratios and caching\ndecisions across the network in order to minimize average latency, which can be\nshown to be equivalent to maximizing the compression and caching gain under an\nenergy consumption constraint. We show that this problem is NP-hard in general\nand the hardness is caused by the caching decision subproblem, while the\ncompression sub-problem is polynomial-time solvable. We then propose an\napproximation algorithm that achieves a $(1-1/e)$-approximation solution to the\noptimum in strongly polynomial time. We show that our proposed algorithm\nachieve the near-optimal performance in synthetic-based evaluations. In this\npaper, we consider a tree-structured network as an illustrative example, but\nour results easily extend to general network topology at the expense of more\ncomplicated notations. \n\n"}
{"id": "1801.02134", "contents": "Title: FlexONC: Joint Cooperative Forwarding and Network Coding with Precise\n  Encoding Conditions Abstract: In recent years, network coding has emerged as an innovative method that\nhelps a wireless network approach its maximum capacity, by combining multiple\nunicasts in one broadcast. However, the majority of research conducted in this\narea is yet to fully utilize the broadcasting nature of wireless networks, and\nstill assumes fixed route between the source and destination that every packet\nshould travel through. This assumption not only limits coding opportunities,\nbut can also cause buffer overflow in some specific intermediate nodes.\nAlthough some studies considered scattering of the flows dynamically in the\nnetwork, they still face some limitations. This paper explains pros and cons of\nsome prominent research in network coding and proposes a Flexible and\nOpportunistic Network Coding scheme (FlexONC) as a solution to such issues.\nFurthermore, this research discovers that the conditions used in previous\nstudies to combine packets of different flows are overly optimistic and would\naffect the network performance adversarially. Therefore, we provide a more\naccurate set of rules for packet encoding. The experimental results show that\nFlexONC outperforms previous methods especially in networks with high bit error\nrate, by better utilizing redundant packets spread in the network. \n\n"}
{"id": "1801.02344", "contents": "Title: Optimal Time Scheduling for Wireless-Powered Backscatter Communication\n  Networks Abstract: This letter introduces a novel wireless-powered backscatter communication\nsystem which allows sensors to utilize RF signals transmitted from a dedicated\nRF energy source to transmit data. In the proposed system, when the RF energy\nsource transmits RF signals, the sensors are able to backscatter the RF signals\nto transmit date to the gateway and/or harvest energy from the RF signals for\ntheir operations. By integrating backscattering and energy harvesting\ntechniques, we can optimize the network throughput of the system. In\nparticular, we first formulate the time scheduling problem for the system, and\nthen propose an optimal solution using convex optimization to maximize the\noverall network throughput. Numerical results show a significant throughput\ngain achieved by our proposed design over two other baseline schemes. \n\n"}
{"id": "1801.02741", "contents": "Title: Towards Stability Analysis of Data Transport Mechanisms: a Fluid Model\n  and an Application Abstract: The Transmission Control Protocol (TCP) utilizes congestion avoidance and\ncontrol mechanisms as a preventive measure against congestive collapse and as\nan adaptive measure in the presence of changing network conditions. The set of\navailable congestion control algorithms is diverse, and while many have been\nstudied from empirical and simulation perspectives, there is a notable lack of\nanalytical work for some variants. To gain more insight into the dynamics of\nthese algorithms, we: (1) propose a general modeling scheme consisting of a set\nof functional differential equations of retarded type (RFDEs) and of the\ncongestion window as a function of time; (2) apply this scheme to TCP Reno and\ndemonstrate its equivalence to a previous, well known model for TCP Reno; (3)\nshow an application of the new framework to the widely-deployed congestion\ncontrol algorithm TCP CUBIC, for which analytical models are few and limited;\nand (4) validate the model using simulations. Our modeling framework yields a\nfluid model for TCP CUBIC. From a theoretical analysis of this model, we\ndiscover that TCP CUBIC is locally uniformly asymptotically stable -- a\nproperty of the algorithm previously unknown. \n\n"}
{"id": "1801.04035", "contents": "Title: EdgeChain: Blockchain-based Multi-vendor Mobile Edge Application\n  Placement Abstract: The state-of-the-art mobile edge applications are generating intense traffic\nand posing rigorous latency requirements to service providers. While resource\nsharing across multiple service providers can be a way to maximize the\nutilization of limited resources at the network edge, it requires a centralized\nrepository maintained by all parties for service providers to share status.\nMoreover, service providers have to trust each other for resource allocation\nfairness, which is difficult because of potential conflicts of interest. We\npropose EdgeChain, a blockchain-based architecture to make mobile edge\napplication placement decisions for multiple service providers. We first\nformulate a stochastic programming problem minimizing the placement cost for\nmobile edge application placement scenarios. Based on our model, we present a\nheuristic mobile edge application placement algorithm. As a decentralized\npublic ledger, the blockchain then takes the logic of our algorithm as the\nsmart contract, with the consideration of resources from all mobile edge hosts\nparticipating in the system. The algorithm is agreed by all parties and the\nresults will only be accepted by majority of the mining nodes on the\nblockchain. When a placement decision is made, an edge host meeting the\nconsumer's latency and budget requirements will be selected at the lowest cost.\nAll placement transactions are stored on the blockchain and are traceable by\nevery mobile edge service provider and application vendor who consumes\nresources at the mobile edge. \n\n"}
{"id": "1801.04907", "contents": "Title: Sending Information Through Status Updates Abstract: We consider an energy harvesting transmitter sending status updates regarding\na physical phenomenon it observes to a receiver. Different from the existing\nliterature, we consider a scenario where the status updates carry information\nabout an independent message. The transmitter encodes this message into the\ntimings of the status updates. The receiver needs to extract this encoded\ninformation, as well as update the status of the observed phenomenon. The\ntimings of the status updates, therefore, determine both the age of information\n(AoI) and the message rate (rate). We study the tradeoff between the achievable\nmessage rate and the achievable average AoI. We propose several achievable\nschemes and compare their rate-AoI performances. \n\n"}
{"id": "1801.05756", "contents": "Title: Content Placement in Cache-Enabled Sub-6 GHz and Millimeter-Wave\n  Multi-antenna Dense Small Cell Networks Abstract: This paper studies the performance of cache-enabled dense small cell networks\nconsisting of multi-antenna sub-6 GHz and millimeter-wave base stations.\nDifferent from the existing works which only consider a single antenna at each\nbase station, the optimal content placement is unknown when the base stations\nhave multiple antennas. We first derive the successful content delivery\nprobability by accounting for the key channel features at sub-6 GHz and mmWave\nfrequencies. The maximization of the successful content delivery probability is\na challenging problem. To tackle it, we first propose a constrained\ncross-entropy algorithm which achieves the near-optimal solution with moderate\ncomplexity. We then develop another simple yet effective heuristic\nprobabilistic content placement scheme, termed two-stair algorithm, which\nstrikes a balance between caching the most popular contents and achieving\ncontent diversity. Numerical results demonstrate the superior performance of\nthe constrained cross-entropy method and that the two-stair algorithm yields\nsignificantly better performance than only caching the most popular contents.\nThe comparisons between the sub-6 GHz and mmWave systems reveal an interesting\ntradeoff between caching capacity and density for the mmWave system to achieve\nsimilar performance as the sub-6 GHz system. \n\n"}
{"id": "1801.05823", "contents": "Title: Device Caching for Network Offloading: Delay Minimization with Presence\n  of User Mobility Abstract: A delay-optimal caching problem (DOCP) in deviceto- device (D2D) networks\nwith moblity is modelled. The problem arises in the context of achieving\noffloading using device caching, and the offloading effect is represented by\nthe expected network load ratio (NLR) which is the percentage of data that has\nto be downloaded from the network. Compared with the related studies, this work\nconsiders minimizing delay with NLR guarantee in mobility scenarios. A lower\nbound of global optimum is derived, thus enabling performance benchmarking of\nany sub-optimal algorithm. For problem-solving, an effective search algorithm\n(ESA) is proposed based on the bound. Simulations are conducted to evaluate the\neffectiveness of the ESA algorithm. \n\n"}
{"id": "1801.06326", "contents": "Title: Some aspects of physical prototyping in Pervasive Computing Abstract: This document summarises the results of several research campaigns over the\npast seven years. The main connecting theme is the physical layer of widely\ndeployed sensors in Pervasive Computing domains. In particular, we have focused\non the RF-channel or on ambient audio.\n  The initial problem from which we started this work was that of distributed\nadaptive transmit beamforming. We have been looking for a simple method to\nalign the phases of jointly transmitting nodes (e.g. sensor or IoT nodes). The\nalgorithmic solution to this problem was to implement a distributed random\noptimisation method on the participating nodes in which the transmitters and\nthe receiver follow an iterative question-and-answer scheme. We have been able\nto derive sharp asymptotic bounds on the expected optimisation time of an\nevolutionary random optimiser and presented an asymptotically optimal approach.\n  One thing that we have learned from the work on these physical layer\nalgorithms was that the signals we work on are fragile and perceptive to\nphysical environmental changes. These could be obstacles such as furniture,\nopened or closed windows or doors as well as movement of individuals. This\nobservation motivated us to view the wireless interface as a sensor for\nenvironmental changes in Pervasive Computing environments.\n  Another use of physical layer RF-signals is for security applications.\n  We are currently working to further push these mentioned directions and novel\nfields of physical prototyping. In particular, the calculation of mathematical\noperations on the wireless channel at the time of transmission appears to\ncontain good potential for gains in efficiency for communication and\ncomputation in Pervasive Computing domains. \n\n"}
{"id": "1801.06623", "contents": "Title: Promises and Caveats of Uplink IoT Ultra-Dense Networks Abstract: In this paper, by means of simulations, we evaluate the uplink (UL)\nperformance of an Internet of Things (IoT) capable ultra-dense network (UDN) in\nterms of the coverage probability and the density of reliably working user\nequipments (UEs). From our study, we show the benefits and challenges that UL\nIoT UDNs will bring about in the future. In more detail, for a low-reliability\ncriterion, such as achieving a UL signal-to-interference-plus-noise ratio\n(SINR) above 0 dB, the density of reliably working UEs grows quickly with the\nnetwork densification, showing the potential of UL IoT UDNs. In contrast, for a\nhigh-reliability criterion, such as achieving a UL SINR above 10 dB, the\ndensity of reliably working UEs remains to be low in UDNs due to excessive\ninter-cell interference, which should be considered when operating UL IoT UDNs.\nMoreover, considering the existence of a non-zero antenna height difference\nbetween base stations (BSs) and UEs, the density of reliably working UEs could\neven decrease as we deploy more BSs. This calls for the usage of sophisticated\ninterference management schemes and/or beam steering/shaping technologies in UL\nIoT UDNs. \n\n"}
{"id": "1801.07379", "contents": "Title: Secure Mobile Crowdsensing with Deep Learning Abstract: In order to stimulate secure sensing for Internet of Things (IoT)\napplications such as healthcare and traffic monitoring, mobile crowdsensing\n(MCS) systems have to address security threats, such as jamming, spoofing and\nfaked sensing attacks, during both the sensing and the information exchange\nprocesses in large-scale dynamic and heterogenous networks. In this article, we\ninvestigate secure mobile crowdsensing and present how to use deep learning\n(DL) methods such as stacked autoencoder (SAE), deep neural network (DNN), and\nconvolutional neural network (CNN) to improve the MCS security approaches\nincluding authentication, privacy protection, faked sensing countermeasures,\nintrusion detection and anti-jamming transmissions in MCS. We discuss the\nperformance gain of these DL-based approaches compared with traditional\nsecurity schemes and identify the challenges that need to be addressed to\nimplement them in practical MCS systems. \n\n"}
{"id": "1801.07472", "contents": "Title: Efficient 3D Aerial Base Station Placement Considering Users Mobility by\n  Reinforcement Learning Abstract: This paper considers an aerial base station (aerial-BS) assisted terrestrial\nnetwork where user mobility is taken into account. User movement changes the\nnetwork dynamically which may result in performance loss. To avoid this loss,\nguarantee a minimum quality-of-service (QoS) and possibly increase the QoS, we\nadd an aerial-BS to the network. For a fair comparison between the conventional\nterrestrial network and the aerial-BS assisted one, we keep the total number of\nBSs identical in both networks. Obtaining the best performance in such networks\nhighly depends on the optimal placement of the aerial-BS. To this end, an\nalgorithm which can rely on general and realistic assumptions and can decide\nwhere to go based on the past experiences is required. The proposed approach\nfor this goal is based on a discounted reward reinforcement learning which is\nknown as Q-learning. Simulation results show this method provides an effective\nplacement strategy which increases the QoS of wireless networks when it is\nneeded and promises to find the optimum position of the aerial-BS in discrete\nenvironments. \n\n"}
{"id": "1801.07992", "contents": "Title: XZero: On Practical Cross-Technology Interference-Nulling for LTE-U/WiFi\n  Coexistence Abstract: LTE-U/WiFi coexistence can be significantly improved by placing so-called\ncoexistence gaps in space through cross-technology interference-nulling (CTIN)\nfrom LTE-U BS towards WiFi nodes. Such coordinated co-existence scheme\nrequires, for the exchange of control messages, a cross-technology control\nchannel (CTC) between LTE-U and WiFi networks which was presented recently.\nHowever, it is unclear how a practical CTIN operates in the absence of channel\nstate information which is needed for CTIN but cannot be obtained from the CTC.\nWe present XZero, the first practical CTIN system that is able to quickly find\nthe suitable precoding configuration used for interference nulling without\nhaving to search the whole space of angular orientations. XZero performs a\ntree-based search to find the direction for the null beam(s) by exploiting the\nfeedback received from the WiFi AP on the tested null directions. We have\nimplemented a prototype of XZero using SDR platform for LTE-U and commodity\nhardware for WiFi and evaluated its performance in a large indoor testbed.\nEvaluation results reveal on average a reduction by 15.7 dB in\ninterference-to-noise ratio at the nulled WiFi nodes when using a ULA with four\nantennas. Moreover, XZero has a sub-second reconfiguration delay which is up to\n10x smaller as compared to naive exhaustive linear search. \n\n"}
{"id": "1801.08560", "contents": "Title: A Tractable Analysis of the Blind-spot Probability in Localization\n  Networks under Correlated Blocking Abstract: In localization applications, the line-of-sight between anchors and targets\nmay be blocked by obstacles in the environment. A target that is invisible\n(i.e., without line-of-sight) to a sufficient number of anchors cannot be\nunambiguously localized and is, therefore, said to be in a blind spot. In this\npaper, we analyze the blind spot probability of a typical target by using\nstochastic geometry to model the randomness in the obstacle and anchor\nlocations. In doing so, we handle correlated anchor blocking induced by the\nobstacles, unlike previous works that assume independent anchor blocking. We\nfirst characterize the regime over which the independent blocking assumption\nunderestimates the blind spot probability of the typical target, which in turn,\nis characterized as a function of the distribution of the visible area,\nsurrounding the target location. Since this distribution is difficult to\ncharacterize exactly, we formulate the nearest two-obstacle approximation,\nwhich is equivalent to considering correlated blocking for only the nearest two\nobstacles from the target and assuming independent blocking for the remaining\nobstacles. Based on this, we derive an approximate expression for the blind\nspot probability, which helps determine the anchor deployment intensity needed\nfor the blind spot probability of a typical target to be at most a threshold,\n$\\mu$. \n\n"}
{"id": "1801.10500", "contents": "Title: Analysis of Coded Selective-Repeat ARQ via Matrix Signal-Flow Graphs Abstract: We propose two schemes for selective-repeat ARQ protocols over packet erasure\nchannels with unreliable feedback: (i) a hybrid ARQ protocol with soft\ncombining at the receiver, and (ii) a coded ARQ protocol, by building on the\nuncoded baseline scheme for ARQ, developed by Ausavapattanakun and Nosratinia.\nOur method leverages discrete-time queuing and coding theory to analyze the\nperformance of the proposed data transmission methods. We incorporate forward\nerror-correction to reduce in-order delivery delay, and exploit a matrix\nsignal-flow graph approach to analyze the throughput and delay of the\nprotocols. We demonstrate and contrast the performance of the coded protocols\nwith that of the uncoded scheme, illustrating the benefits of coded\ntransmissions. \n\n"}
{"id": "1802.01563", "contents": "Title: Age-Minimal Online Policies for Energy Harvesting Sensors with Random\n  Battery Recharges Abstract: We consider an energy harvesting sensor that is sending measurement updates\nregarding some physical phenomenon to a destination. The sensor relies on\nenergy harvested from nature to measure and send its updates, and is equipped\nwith a battery of finite size to collect its harvested energy. The energy\nharvesting process is Poisson with unit rate, and arrives in amounts that fully\nrecharge the battery. Our setting is online in the sense that the times of\nenergy arrivals are revealed causally to the sensor after the energy is\nharvested; only the statistics of the arrival process is known a priori.\nUpdates need to be sent in a timely manner to the destination, namely, such\nthat the long term average age of information is minimized over the course of\ncommunication. The age of information is defined as the time elapsed since the\nfreshest update has reached the destination. We first show that the optimal\nscheduling update policy is a renewal policy, and then show that it has a multi\nthreshold structure: the sensor sends an update only if the age of information\ngrows above a certain threshold that depends on the available energy. \n\n"}
{"id": "1802.02895", "contents": "Title: Adaptive Coded Caching for Fair Delivery over Fading Channels Abstract: The performance of existing coded caching schemes is sensitive to the worst\nchannel quality, a problem which is exacerbated when communicating over fading\nchannels. In this paper, we address this limitation in the following manner: in\nshort-term, we allow transmissions to subsets of users with good channel\nquality, avoiding users with fades, while in long-term we ensure fairness among\nusers. Our online scheme combines (i) the classical decentralized coded caching\nscheme \\cite{maddah2013decentralized} with (ii) joint scheduling and power\ncontrol for the fading broadcast channel, as well as (iii) congestion control\nfor ensuring the optimal long-term average performance. We prove that our\nonline delivery scheme maximizes the alpha-fair utility among all schemes\nrestricted to decentralized placement. By tuning the value of alpha, the\nproposed scheme can achieve different operating points on the average delivery\nrate region and tune performance according to an operator's choice.\n  We demonstrate via simulations that our scheme outperforms two baseline\nschemes: (a) standard coded caching with multicast transmission, limited by the\nworst channel user yet exploiting the global caching gain; (b) opportunistic\nscheduling with unicast transmissions exploiting the fading diversity but\nlimited to local caching gain. \n\n"}
{"id": "1802.05113", "contents": "Title: Multi-Criteria Virtual Machine Placement in Cloud Computing\n  Environments: A literature Review Abstract: Cloud computing is a revolutionary process that has impacted the manner of\nusing networks. It allows a high level of flexibility as Virtual Machines (VMs)\nrun elastically workloads on physical machines in data centers. The issue of\nplacing virtual machines (VMP) in cloud environments is an important challenge\nthat has been thoroughly addressed, although not yet completely resolved. This\narticle discusses the different problems that may disrupt the placement of VMs\nand Virtual Network Functions (VNFs), and classifies the existing solutions\ninto five major objective functions based on multiple performance metrics such\nas energy consumption, Quality of Service, Service Level Agreement, and\nincurred cost. The existing solutions are also classified based on whether they\nadopt heuristic, deterministic, meta-heuristic or approximation algorithms. The\nVNF placement in 5G network is also discussed to highlight the convergence\ntoward optimal usage of mobile services by including\nNFV/Software-Defined-Network technologies. \n\n"}
{"id": "1802.06706", "contents": "Title: Integration of Carrier Aggregation and Dual Connectivity for the ns-3\n  mmWave Module Abstract: Thanks to the wide availability of bandwidth, the millimeter wave (mmWave)\nfrequencies will provide very high data rates to mobile users in next\ngeneration 5G cellular networks. However, mmWave links suffer from high\nisotropic pathloss and blockage from common materials, and are subject to an\nintermittent channel quality. Therefore, protocols and solutions at different\nlayers in the cellular network and the TCP/IP protocol stack have been proposed\nand studied. A valuable tool for the end-to-end performance analysis of mmWave\ncellular networks is the ns-3 mmWave module, which already models in detail the\nchannel, Physical (PHY) and Medium Access Control (MAC) layers, and extends the\nLong Term Evolution (LTE) stack for the higher layers. In this paper we present\nan implementation for the ns-3 mmWave module of multi connectivity techniques\nfor 3GPP New Radio (NR) at mmWave frequencies, namely Carrier Aggregation (CA)\nand Dual Connectivity (DC), and discuss how they can be integrated to increase\nthe functionalities offered by the ns-3 mmWave module. \n\n"}
{"id": "1802.07475", "contents": "Title: Car-to-Cloud Communication Traffic Analysis Based on the Common Vehicle\n  Information Model Abstract: Although connectivity services have been introduced already today in many of\nthe most recent car models, the potential of vehicles serving as highly mobile\nsensor platform in the Internet of Things (IoT) has not been sufficiently\nexploited yet. The European AutoMat project has therefore defined an open\nCommon Vehicle Information Model (CVIM) in combination with a cross-industry,\ncloud-based big data marketplace. Thereby, vehicle sensor data can be leveraged\nfor the design of entirely new services even beyond traffic-related\napplications (such as localized weather forecasts). This paper focuses on the\nprediction of the achievable data rate making use of an analytical model based\non empirical measurements. For an in-depth analysis, the CVIM has been\nintegrated in a vehicle traffic simulator to produce CVIM-complaint data\nstreams as a result of the individual behavior of each vehicle (speed, brake\nactivity, steering activity, etc.). In a next step, a simulation of vehicle\ntraffic in a realistically modeled, large-area street network has been used in\ncombination with a cellular Long Term Evolution (LTE) network to determine the\ncumulated amount of data produced within each network cell. As a result, a new\ncar-to-cloud communication traffic model has been derived, which quantifies the\ndata rate of aggregated car-to-cloud data producible by vehicles depending on\nthe current traffic situations (free flow and traffic jam). The results provide\na reference for network planning and resource scheduling for car-to-cloud type\nservices in the context of smart cities. \n\n"}
{"id": "1802.07855", "contents": "Title: RT-DAP: A Real-Time Data Analytics Platform for Large-scale Industrial\n  Process Monitoring and Control Abstract: In most process control systems nowadays, process measurements are\nperiodically collected and archived in historians. Analytics applications\nprocess the data, and provide results offline or in a time period that is\nconsiderably slow in comparison to the performance of the manufacturing\nprocess. Along with the proliferation of Internet-of-Things (IoT) and the\nintroduction of \"pervasive sensors\" technology in process industries,\nincreasing number of sensors and actuators are installed in process plants for\npervasive sensing and control, and the volume of produced process data is\ngrowing exponentially. To digest these data and meet the ever-growing\nrequirements to increase production efficiency and improve product quality,\nthere needs to be a way to both improve the performance of the analytics system\nand scale the system to closely monitor a much larger set of plant resources.\nIn this paper, we present a real-time data analytics platform, called RT-DAP,\nto support large-scale continuous data analytics in process industries. RT-DAP\nis designed to be able to stream, store, process and visualize a large volume\nof realtime data flows collected from heterogeneous plant resources, and\nfeedback to the control system and operators in a realtime manner. A prototype\nof the platform is implemented on Microsoft Azure. Our extensive experiments\nvalidate the design methodologies of RT-DAP and demonstrate its efficiency in\nboth component and system levels. \n\n"}
{"id": "1802.08776", "contents": "Title: Bandwidth Partitioning and Downlink Analysis in Millimeter Wave\n  Integrated Access and Backhaul for 5G Abstract: With the increasing network densification, it has become exceedingly\ndifficult to provide traditional fiber backhaul access to each cell site, which\nis especially true for small cell base stations (SBSs). The increasing maturity\nof millimeter wave (mm-wave) communication has opened up the possibility of\nproviding high-speed wireless backhaul to such cell sites. Since mm-wave is\nalso suitable for access links, the third generation partnership project (3GPP)\nis envisioning an integrated access and backhaul (IAB) architecture for the\nfifth generation (5G) cellular networks in which the same infrastructure and\nspectral resources will be used for both access and backhaul. In this paper, we\ndevelop an analytical framework for IAB-enabled cellular network using which\nits downlink rate coverage probability is accurately characterized. Using this\nframework, we study the performance of three backhaul bandwidth (BW) partition\nstrategies: 1) equal partition: when all SBSs obtain equal share of the\nbackhaul BW; 2) instantaneous load-based partition: when the backhaul BW share\nof an SBS is proportional to its instantaneous load; and 3) average load-based\npartition: when the backhaul BW share of an SBS is proportional to its average\nload. Our analysis shows that depending on the choice of the partition\nstrategy, there exists an optimal split of access and backhaul BW for which the\nrate coverage is maximized. Further, there exists a critical volume of\ncell-load (total number of users) beyond which the gains provided by the\nIAB-enabled network disappear and its performance converges to that of the\ntraditional macro-only network with no SBSs. \n\n"}
{"id": "1802.09262", "contents": "Title: tinyLTE: Lightweight, Ad-Hoc Deployable Cellular Network for Vehicular\n  Communication Abstract: The application of LTE technology has evolved from infrastructure-based\ndeployments in licensed bands to new use cases covering ad hoc,\ndevice-to-device communications and unlicensed band operation. Vehicular\ncommunication is an emerging field of particular interest for LTE, covering in\nour understanding both automotive (cars) as well as unmanned aerial vehicles.\nExisting commercial equipment is designed for infrastructure making it\nunsuitable for vehicular applications requiring low weight and unlicensed band\nsupport (e.g. 5.9 GHz ITS-band). In this work, we present tinyLTE, a system\ndesign which provides fully autonomous, multi-purpose and ultra-compact LTE\ncells by utilizing existing open source eNB and EPC implementations. Due to its\nsmall form factor and low weight, the tinyLTE system enables mobile deployment\non board of cars and drones as well as smooth integration with existing\nroadside infrastructure. Additionally, the standalone design allows for systems\nto be chained in a multi-hop configuration. The paper describes the lean and\nlow-cost design concept and implementation followed by a performance evaluation\nfor single and two-hop configurations at 5.9 GHz. The results from both lab and\nfield experiments validate the feasibility of the tinyLTE approach and\ndemonstrate its potential to even support real-time vehicular applications\n(e.g. with a lowest average end-to-end latency of around 7 ms in the lab\nexperiment). \n\n"}
{"id": "1802.10140", "contents": "Title: Towards a Socially Optimal Multi-Modal Routing Platform Abstract: The increasing rate of urbanization has added pressure on the already\nconstrained transportation networks in our communities. Ride-sharing platforms\nsuch as Uber and Lyft are becoming a more commonplace, particularly in urban\nenvironments. While such services may be deemed more convenient than riding\npublic transit due to their on-demand nature, reports show that they do not\nnecessarily decrease the congestion in major cities. One of the key problems is\nthat typically mobility decision support systems focus on individual utility\nand react only after congestion appears. In this paper, we propose socially\nconsiderate multi-modal routing algorithms that are proactive and consider, via\npredictions, the shared effect of riders on the overall efficacy of mobility\nservices. We have adapted the MATSim simulator framework to incorporate the\nproposed algorithms present a simulation analysis of a case study in Nashville,\nTennessee that assesses the effects of our routing models on the traffic\ncongestion for different levels of penetration and adoption of socially\nconsiderate routes. Our results indicate that even at a low penetration (social\nratio), we are able to achieve an improvement in system-level performance. \n\n"}
{"id": "1803.01136", "contents": "Title: Coverage and Connectivity Analysis of Millimeter Wave Vehicular Networks Abstract: The next generations of vehicles will require data transmission rates in the\norder of terabytes per driving hour, to support advanced automotive services.\nThis unprecedented amount of data to be exchanged goes beyond the capabilities\nof existing communication technologies for vehicular communication and calls\nfor new solutions. A possible answer to this growing demand for ultra-high\ntransmission speeds can be found in the millimeter-wave (mmWave) bands which,\nhowever, are subject to high signal attenuation and challenging propagation\ncharacteristics. In particular, mmWave links are typically directional, to\nbenefit from the resulting beamforming gain, and require precise alignment of\nthe transmitter and the receiver beams, an operation which may increase the\nlatency of the communication and lead to deafness due to beam misalignment. In\nthis paper, we propose a stochastic model for characterizing the beam coverage\nand connectivity probability in mmWave automotive networks. The purpose is to\nexemplify some of the complex and interesting tradeoffs that have to be\nconsidered when designing solutions for vehicular scenarios based on mmWave\nlinks. The results show that the performance of the automotive nodes in highly\nmobile mmWave systems strictly depends on the specific environment in which the\nvehicles are deployed, and must account for several automotive-specific\nfeatures such as the nodes speed, the beam alignment periodicity, the base\nstations density and the antenna geometry. \n\n"}
{"id": "1803.01462", "contents": "Title: Optimal Status Updating for an Energy Harvesting Sensor with a Noisy\n  Channel Abstract: Consider an energy harvesting sensor continuously monitors a system and sends\ntime-stamped status update to a destination. The destination keeps track of the\nsystem status through the received updates. Under the energy causality\nconstraint at the sensor, our objective is to design an optimal online status\nupdating policy to minimize the long-term average Age of Information (AoI) at\nthe destination. We focus on the scenario where the the channel between the\nsource and the destination is noisy, and each transmitted update may fail\nindependently with a constant probability. We assume there is no channel state\ninformation or transmission feedback available to the sensor. We prove that\nwithin a broadly defined class of online policies, the best-effort uniform\nupdating policy, which was shown to be optimal when the channel is perfect, is\nstill optimal in the presence of update failures. Our proof relies on tools\nfrom Martingale processes, and the construction of a sequence of virtual\npolicies. \n\n"}
{"id": "1803.02261", "contents": "Title: User-Centric 5G Cellular Networks: Resource Allocation and Comparison\n  with the Cell-Free Massive MIMO Approach Abstract: Recently, the so-called cell-free (CF) Massive MIMO architecture has been\nintroduced, wherein a very large number of distributed access points (APs)\nsimultaneously and jointly serve a much smaller number of mobile stations\n(MSs). The paper extends the CF approach to the case in which both the APs and\nthe MSs are equipped with multiple antennas, proposing a beamfoming scheme\nthat, relying on the channel hardening effect, does not require channel\nestimation at the MSs. We contrast the CF massive MIMO approach with a\nuser-centric (UC) approach wherein each MS is served only by a limited number\nof APs. Since far APs experience a bad SINR, it turns out that they are quite\nunhelpful in serving far users, and so, the UC approach, while requiring less\nbackhaul overhead with respect to the CF approach, is shown here to achieve\nbetter performance results, in terms of achievable rate-per-user, for the vast\nmajority of the MSs in the network. Furthermore, in the paper we propose two\npower allocation strategy for the uplink and downlink, one aimed at maximizing\nthe overall data-rate and another aimed at maximizing system fairness. \n\n"}
{"id": "1803.02791", "contents": "Title: Facebook (A)Live? Are live social broadcasts really broadcasts? Abstract: The era of live-broadcast is back but with two major changes. First, unlike\ntraditional TV broadcasts, content is now streamed over the Internet enabling\nit to reach a wider audience. Second, due to various user-generated content\nplatforms it has become possible for anyone to get involved, streaming their\nown content to the world. This emerging trend of going live usually happens via\nsocial platforms, where users perform live social broadcasts predominantly from\ntheir mobile devices, allowing their friends (and the general public) to engage\nwith the stream in real-time. With the growing popularity of such platforms,\nthe burden on the current Internet infrastructure is therefore expected to\nmultiply. With this in mind, we explore one such prominent platform - Facebook\nLive. We gather 3TB of data, representing one month of global activity and\nexplore the characteristics of live social broadcast. From this, we derive\nsimple yet effective principles which can decrease the network burden. We then\ndissect global and hyper-local properties of the video while on-air, by\ncapturing the geography of the broadcasters or the users who produce the video\nand the viewers or the users who interact with it. Finally, we study the social\nengagement while the video is live and distinguish the key aspects when the\nsame video goes on-demand. A common theme throughout the paper is that, despite\nits name, many attributes of Facebook Live deviate from both the concepts of\nlive and broadcast. \n\n"}
{"id": "1803.02816", "contents": "Title: Shedding Light on the Dark Corners of the Internet: A Survey of Tor\n  Research Abstract: Anonymity services have seen high growth rates with increased usage in the\npast few years. Among various services, Tor is one of the most popular\npeer-to-peer anonymizing service. In this survey paper, we summarize, analyze,\nclassify and quantify 26 years of research on the Tor network. Our research\nshows that `security' and `anonymity' are the most frequent keywords associated\nwith Tor research studies. Quantitative analysis shows that the majority of\nresearch studies on Tor focus on `deanonymization' the design of a breaching\nstrategy. The second most frequent topic is analysis of path selection\nalgorithms to select more resilient paths. Analysis shows that the majority of\nexperimental studies derived their results by deploying private testbeds while\nothers performed simulations by developing custom simulators. No consistent\nparameters have been used for Tor performance analysis. The majority of authors\nperformed throughput and latency analysis. \n\n"}
{"id": "1803.03285", "contents": "Title: Massive UAV-to-Ground Communication and its Stable Movement Control: A\n  Mean-Field Approach Abstract: This paper proposes a real-time movement control algorithm for massive\nunmanned aerial vehicles (UAVs) that provide emergency cellular connections in\nan urban disaster site. While avoiding the inter-UAV collision under temporal\nwind dynamics, the proposed algorithm minimizes each UAV's energy consumption\nper unit downlink rate. By means of a mean-field game theoretic flocking\napproach, the velocity control of each UAV only requires its own location and\nchannel states. Numerical results validate the performance of the algorithm in\nterms of the number of collisions and energy consumption per data rate, under a\nrealistic 3GPP UAV channel model. \n\n"}
{"id": "1803.03622", "contents": "Title: Virtual Network Embedding Approximations: Leveraging Randomized Rounding Abstract: The Virtual Network Embedding Problem (VNEP) captures the essence of many\nresource allocation problems of today's infrastructure providers, which offer\ntheir physical computation and networking resources to customers. Customers\nrequest resources in the form of Virtual Networks, i.e. as a directed graph\nwhich specifies computational requirements at the nodes and communication\nrequirements on the edges. An embedding of a Virtual Network on the shared\nphysical infrastructure is the joint mapping of (virtual) nodes to physical\nservers together with the mapping of (virtual) edges onto paths in the physical\nnetwork connecting the respective servers.\n  This work initiates the study of approximation algorithms for the VNEP.\nConcretely, we study the offline setting with admission control: given multiple\nrequest graphs the task is to embed the most profitable subset while not\nexceeding resource capacities. Our approximation is based on the randomized\nrounding of Linear Programming (LP) solutions. Interestingly, we uncover that\nthe standard LP formulation for the VNEP exhibits an inherent structural\ndeficit when considering general virtual network topologies: its solutions\ncannot be decomposed into valid embeddings. In turn, focusing on the class of\ncactus request graphs, we devise a novel LP formulation, whose solutions can be\ndecomposed into convex combinations of valid embedding. Proving performance\nguarantees of our rounding scheme, we obtain the first approximation algorithm\nfor the VNEP in the resource augmentation model.\n  We propose two types of rounding heuristics and evaluate their performance in\nan extensive computational study. Our results indicate that randomized rounding\ncan yield good solutions (even without augmentations). Specifically, heuristic\nrounding achieves 73.8% of the baseline's profit, while not exceeding\ncapacities. \n\n"}
{"id": "1803.03845", "contents": "Title: 5G NR Jamming, Spoofing, and Sniffing: Threat Assessment and Mitigation Abstract: In December 2017, the Third Generation Partnership Project (3GPP) released\nthe first set of specifications for 5G New Radio (NR), which is currently the\nmost widely accepted 5G cellular standard. 5G NR is expected to replace LTE and\nprevious generations of cellular technology over the next several years,\nproviding higher throughput, lower latency, and a host of new features. Similar\nto LTE, the 5G NR physical layer consists of several physical channels and\nsignals, most of which are vital to the operation of the network.\nUnfortunately, like for any wireless technology, disruption through radio\njamming is possible. This paper investigates the extent to which 5G NR is\nvulnerable to jamming and spoofing, by analyzing the physical downlink and\nuplink control channels and signals. We identify the weakest links in the 5G NR\nframe, and propose mitigation strategies that should be taken into account\nduring implementation of 5G NR chipsets and base stations. \n\n"}
{"id": "1803.04100", "contents": "Title: Multi-Hop Routing in Covert Wireless Networks Abstract: In covert communication, Alice tries to communicate with Bob without being\ndetected by a warden Willie. When the distance between Alice and Bob becomes\nlarge compared to the distance between Alice and Willie(s), the performance of\ncovert communication will be degraded. In this case, multi-hop message\ntransmission via intermediate relays can help to improve performance. Hence, in\nthis work multi-hop covert communication over a moderate size network and in\nthe presence of multiple collaborating Willies is considered. The relays can\ntransmit covertly using either a single key for all relays, or different\nindependent keys at the relays. For each case, we develop efficient algorithms\nto find optimal paths with maximum throughput and minimum end-to-end delay\nbetween Alice and Bob. As expected, employing multiple hops significantly\nimproves the ability to communicate covertly versus the case of a single-hop\ntransmission. Furthermore, at the expense of more shared key bits, analytical\nresults and numerical simulations demonstrate that multi-hop covert\ncommunication with different independent keys at the relays has better\nperformance than multi-hop covert communication with a single key. \n\n"}
{"id": "1803.04452", "contents": "Title: (FPT-)Approximation Algorithms for the Virtual Network Embedding Problem Abstract: Many resource allocation problems in the cloud can be described as a basic\nVirtual Network Embedding Problem (VNEP): finding mappings of request graphs\n(describing the workloads) onto a substrate graph (describing the physical\ninfrastructure). In the offline setting, the two natural objectives are profit\nmaximization, i.e., embedding a maximal number of request graphs subject to the\nresource constraints, and cost minimization, i.e., embedding all requests at\nminimal overall cost. The VNEP can be seen as a generalization of classic\nrouting and call admission problems, in which requests are arbitrary graphs\nwhose communication endpoints are not fixed. Due to its applications, the\nproblem has been studied intensively in the networking community. However, the\nunderlying algorithmic problem is hardly understood.\n  This paper presents the first fixed-parameter tractable approximation\nalgorithms for the VNEP. Our algorithms are based on randomized rounding. Due\nto the flexible mapping options and the arbitrary request graph topologies, we\nshow that a novel linear program formulation is required. Only using this novel\nformulation the computation of convex combinations of valid mappings is\nenabled, as the formulation needs to account for the structure of the request\ngraphs. Accordingly, to capture the structure of request graphs, we introduce\nthe graph-theoretic notion of extraction orders and extraction width and show\nthat our algorithms have exponential runtime in the request graphs' maximal\nwidth. Hence, for request graphs of fixed extraction width, we obtain the first\npolynomial-time approximations.\n  Studying the new notion of extraction orders we show that (i) computing\nextraction orders of minimal width is NP-hard and (ii) that computing\ndecomposable LP solutions is in general NP-hard, even when restricting request\ngraphs to planar ones. \n\n"}
{"id": "1803.05364", "contents": "Title: Temporal Correlation of Interference in Vehicular Networks with\n  Shifted-Exponential Time Headways Abstract: We consider a one-dimensional vehicular network where the time headway (time\ndifference between successive vehicles as they pass a point on the roadway)\nfollows the shifted-exponential distribution. We show that neglecting the\nimpact of shift in the deployment model, which degenerates the distribution of\nvehicles to a Poisson Point Process, overestimates the temporal correlation of\ninterference at the origin. The estimation error becomes large at high traffic\nconditions and small time-lags. \n\n"}
{"id": "1803.05663", "contents": "Title: Are Bitcoin Bubbles Predictable? Combining a Generalized Metcalfe's Law\n  and the LPPLS Model Abstract: We develop a strong diagnostic for bubbles and crashes in bitcoin, by\nanalyzing the coincidence (and its absence) of fundamental and technical\nindicators. Using a generalized Metcalfe's law based on network properties, a\nfundamental value is quantified and shown to be heavily exceeded, on at least\nfour occasions, by bubbles that grow and burst. In these bubbles, we detect a\nuniversal super-exponential unsustainable growth. We model this universal\npattern with the Log-Periodic Power Law Singularity (LPPLS) model, which\nparsimoniously captures diverse positive feedback phenomena, such as herding\nand imitation. The LPPLS model is shown to provide an ex-ante warning of market\ninstabilities, quantifying a high crash hazard and probabilistic bracket of the\ncrash time consistent with the actual corrections; although, as always, the\nprecise time and trigger (which straw breaks the camel's back) being exogenous\nand unpredictable. Looking forward, our analysis identifies a substantial but\nnot unprecedented overvaluation in the price of bitcoin, suggesting many months\nof volatile sideways bitcoin prices ahead (from the time of writing, March\n2018). \n\n"}
{"id": "1803.06025", "contents": "Title: CPVNF:Cost-efficient Proactive VNF Placement and Chaining for\n  Value-Added Services in Content Delivery Networks Abstract: Value-added services (e.g., overlaid video advertisements) have become an\nintegral part of today's Content Delivery Networks (CDNs). To offer\ncost-efficient, scalable and more agile provisioning of new value-added\nservices in CDNs, Network Functions Virtualization (NFV) paradigm may be\nleveraged to allow implementation of fine-grained services as a chain of\nVirtual Network Functions (VNFs) to be placed in CDN. The manner in which these\nchains are placed is critical as it both affects the quality of service (QoS)\nand provider cost. The problem is however, very challenging due to the\nspecifics of the chains (e.g.,one of their end-points is not known prior to the\nplacement). We formulate it as an Integer Linear Program (ILP) and propose a\ncost efficient Proactive VNF placement and chaining (CPVNF)algorithm. The\nobjective is to find the optimal number of VNFs along with their locations in\nsuch a manner that the cost is minimized while QoS is met. Apart from cost\nminimization, the support for large-scale CDNs with a large number of servers\nand end-users is an important feature of the proposed algorithm. Through\nsimulations, the algorithm's behaviour for small-scale to large-scale CDN\nnetworks is analyzed. \n\n"}
{"id": "1803.06596", "contents": "Title: Network Service Orchestration: A Survey Abstract: Business models of network service providers are undergoing an evolving\ntransformation fueled by vertical customer demands and technological advances\nsuch as 5G, Software Defined Networking~(SDN), and Network Function\nVirtualization~(NFV). Emerging scenarios call for agile network services\nconsuming network, storage, and compute resources across heterogeneous\ninfrastructures and administrative domains. Coordinating resource control and\nservice creation across interconnected domains and diverse technologies becomes\na grand challenge. Research and development efforts are being devoted to\nenabling orchestration processes to automate, coordinate, and manage the\ndeployment and operation of network services. In this survey, we delve into the\ntopic of Network Service Orchestration~(NSO) by reviewing the historical\nbackground, relevant research projects, enabling technologies, and\nstandardization activities. We define key concepts and propose a taxonomy of\nNSO approaches and solutions to pave the way towards a common understanding of\nthe various ongoing efforts around the realization of diverse NSO application\nscenarios. Based on the analysis of the state of affairs, we present a series\nof open challenges and research opportunities, altogether contributing to a\ntimely and comprehensive survey on the vibrant and strategic topic of network\nservice orchestration. \n\n"}
{"id": "1803.06854", "contents": "Title: MONICA in Hamburg: Towards Large-Scale IoT Deployments in a Smart City Abstract: Modern cities and metropolitan areas all over the world face new management\nchallenges in the 21st century primarily due to increasing demands on living\nstandards by the urban population. These challenges range from climate change,\npollution, transportation, and citizen engagement, to urban planning, and\nsecurity threats. The primary goal of a Smart City is to counteract these\nproblems and mitigate their effects by means of modern ICT to improve urban\nadministration and infrastructure. Key ideas are to utilise network\ncommunication to inter-connect public authorities; but also to deploy and\nintegrate numerous sensors and actuators throughout the city infrastructure -\nwhich is also widely known as the Internet of Things (IoT). Thus, IoT\ntechnologies will be an integral part and key enabler to achieve many\nobjectives of the Smart City vision.\n  The contributions of this paper are as follows. We first examine a number of\nIoT platforms, technologies and network standards that can help to foster a\nSmart City environment. Second, we introduce the EU project MONICA which aims\nfor demonstration of large-scale IoT deployments at public, inner-city events\nand give an overview on its IoT platform architecture. And third, we provide a\ncase-study report on SmartCity activities by the City of Hamburg and provide\ninsights on recent (on-going) field tests of a vertically integrated,\nend-to-end IoT sensor application. \n\n"}
{"id": "1803.07649", "contents": "Title: An Online Algorithm for Power-proportional Data Centers with Switching\n  Cost Abstract: Recent studies have shown that power-proportional data centers can save\nenergy cost by dynamically \"right-sizing\" the data centers based on real-time\nworkload. More servers are activated when the workload increases while some\nservers can be put into the sleep mode during periods of low load. In this\npaper, we revisit the dynamic right-sizing problem for heterogeneous data\ncenters with various operational cost and switching cost. We propose a new\nonline algorithm based on a regularization technique, which achieves a better\ncompetitive ratio compared to the state-of-the-art greedy algorithm. We further\nintroduce a switching cost offset into the model and extend our algorithm to\nthis new setting. Simulations based on real workload and renewable energy\ntraces show that our algorithms outperform the greedy algorithm in both\nsettings. \n\n"}
{"id": "1803.07976", "contents": "Title: An Overview on Application of Machine Learning Techniques in Optical\n  Networks Abstract: Today's telecommunication networks have become sources of enormous amounts of\nwidely heterogeneous data. This information can be retrieved from network\ntraffic traces, network alarms, signal quality indicators, users' behavioral\ndata, etc. Advanced mathematical tools are required to extract meaningful\ninformation from these data and take decisions pertaining to the proper\nfunctioning of the networks from the network-generated data. Among these\nmathematical tools, Machine Learning (ML) is regarded as one of the most\npromising methodological approaches to perform network-data analysis and enable\nautomated network self-configuration and fault management. The adoption of ML\ntechniques in the field of optical communication networks is motivated by the\nunprecedented growth of network complexity faced by optical networks in the\nlast few years. Such complexity increase is due to the introduction of a huge\nnumber of adjustable and interdependent system parameters (e.g., routing\nconfigurations, modulation format, symbol rate, coding schemes, etc.) that are\nenabled by the usage of coherent transmission/reception technologies, advanced\ndigital signal processing and compensation of nonlinear effects in optical\nfiber propagation. In this paper we provide an overview of the application of\nML to optical communications and networking. We classify and survey relevant\nliterature dealing with the topic, and we also provide an introductory tutorial\non ML for researchers and practitioners interested in this field. Although a\ngood number of research papers have recently appeared, the application of ML to\noptical networks is still in its infancy: to stimulate further work in this\narea, we conclude the paper proposing new possible research directions. \n\n"}
{"id": "1803.07993", "contents": "Title: Age of Information in a Network of Preemptive Servers Abstract: A source submits status updates to a network for delivery to a destination\nmonitor. Updates follow a route through a series of network nodes. Each node is\na last-come-first-served queue supporting preemption in service. We\ncharacterize the average age of information at the input and output of each\nnode in the route induced by the updates passing through. For Poisson arrivals\nto a line network of preemptive memoryless servers, we show that average age\naccumulates through successive network nodes. \n\n"}
{"id": "1803.08433", "contents": "Title: Dyloc: Dynamic and Collaborative User-controlled AOA based Localizing\n  System with your laptops Abstract: Currently, accurate localization system based on commodity WiFi devices is\nnot broadly available yet. In the literature, the solutions are based on either\nnetwork infrastructure like WiFi router, which have at least three antennas, or\nsacrifice accuracy with coarse grained information like RSSI. In this work, we\ndesign a new localizing system which is accurate based on AOA estimation and\ninstantly deployable on users' devices.\n  Dyloc is designed to be dynamically constructed with user's devices as\nnetwork nodes without any network infrastructure. On the platform of laptops,\nour system achieve comparable localization accuracy with state-of-the-art work\ndespite of the limitation of less number and large separation of antennas. We\ndesign multi-stage signal processing to resolve the ambiguity issue arisen in\nthis scenario. To enable dynamic and collaborative construction, our system can\naccurately conduct self-localization and also eliminate the need of\ninfrastructure anchors, which is due to the dedicated two-layer algorithm\ndesign. \n\n"}
{"id": "1803.11512", "contents": "Title: Joint Communication, Computation, Caching, and Control in Big Data\n  Multi-access Edge Computing Abstract: The concept of multi-access edge computing (MEC) has been recently introduced\nto supplement cloud computing by deploying MEC servers to the network edge so\nas to reduce the network delay and alleviate the load on cloud data centers.\nHowever, compared to a resourceful cloud, an MEC server has limited resources.\nWhen each MEC server operates independently, it cannot handle all of the\ncomputational and big data demands stemming from the users devices.\nConsequently, the MEC server cannot provide significant gains in overhead\nreduction due to data exchange between users devices and remote cloud.\nTherefore, joint computing, caching, communication, and control (4C) at the\nedge with MEC server collaboration is strongly needed for big data\napplications. In order to address these challenges, in this paper, the problem\nof joint 4C in big data MEC is formulated as an optimization problem whose goal\nis to maximize the bandwidth saving while minimizing delay, subject to the\nlocal computation capability of user devices, computation deadline, and MEC\nresource constraints. However, the formulated problem is shown to be\nnon-convex. To make this problem convex, a proximal upper bound problem of the\noriginal formulated problem that guarantees descent to the original problem is\nproposed. To solve the proximal upper bound problem, a block successive upper\nbound minimization (BSUM) method is applied. Simulation results show that the\nproposed approach increases bandwidth-saving and minimizes delay while\nsatisfying the computation deadlines. \n\n"}
{"id": "1804.00500", "contents": "Title: Hybrid Cell Outage Compensation in 5G Networks: Sky-Ground Approach Abstract: Unmanned Aerial Vehicles (UAVs) enabled communications is a novel and\nattractive area of research in cellular communications. It provides several\ndegrees of freedom in time, space and it can be used for multiple purposes.\nThis is why wide deployment of UAVs has the potential to be integrated in the\nupcoming 5G standard. In this paper, we present a novel cell outage\ncompensation (COC) framework to mitigate the effect of the failure of any\noutdoor Base Station (BS) in 5G networks. Within our framework, the outage\ncompensation is done with the assistance of sky BSs (UAVs) and Ground BSs\n(GBSs). An optimization problem is formulated to jointly minimize the energy of\nthe Drone BSs (DBSs) and GBSs involved in the healing process which accordingly\nwill minimize the number of DBSs and determine their optimal 2D positions. In\naddition, the DBSs will mainly heal the users that the GBS cannot heal due to\ncapacity issues. Simulation results show that the proposed hybrid approach\noutperforms the conventional COC approach. Moreover, all users receive the\nminimum quality of service in addition to minimizing the UAVs' consumed energy. \n\n"}
{"id": "1804.00709", "contents": "Title: Generative Adversarial Learning for Spectrum Sensing Abstract: A novel approach of training data augmentation and domain adaptation is\npresented to support machine learning applications for cognitive radio. Machine\nlearning provides effective tools to automate cognitive radio functionalities\nby reliably extracting and learning intrinsic spectrum dynamics. However, there\nare two important challenges to overcome, in order to fully utilize the machine\nlearning benefits with cognitive radios. First, machine learning requires\nsignificant amount of truthed data to capture complex channel and emitter\ncharacteristics, and train the underlying algorithm (e.g., a classifier).\nSecond, the training data that has been identified for one spectrum environment\ncannot be used for another one (e.g., after channel and emitter conditions\nchange). To address these challenges, a generative adversarial network (GAN)\nwith deep learning structures is used to 1)~generate additional synthetic\ntraining data to improve classifier accuracy, and 2) adapt training data to\nspectrum dynamics. This approach is applied to spectrum sensing by assuming\nonly limited training data without knowledge of spectrum statistics. Machine\nlearning classifiers are trained with limited, augmented and adapted training\ndata to detect signals. Results show that training data augmentation increases\nthe classifier accuracy significantly and this increase is sustained with\ndomain adaptation as spectrum conditions change. \n\n"}
{"id": "1804.01747", "contents": "Title: Survey of Communication Protocols for Internet-of-Things and Related\n  Challenges of Fog and Cloud Computing Integration Abstract: The fast increment in the number of IoT (Internet of Things) devices is\naccelerating the research on new solutions to make cloud services scalable. In\nthis context, the novel concept of fog computing as well as the combined\nfog-to-cloud computing paradigm is becoming essential to decentralize the\ncloud, while bringing the services closer to the end-system. This paper surveys\non the application layer communication protocols to fulfil the IoT\ncommunication requirements, and their potential for implementation in fog- and\ncloud-based IoT systems. To this end, the paper first presents a comparative\nanalysis of the main characteristics of IoT communication protocols, including\nrequest-reply and publish-subscribe protocols. After that, the paper surveys\nthe protocols that are widely adopted and implemented in each segment of the\nsystem (IoT, fog, cloud), and thus opens up the discussion on their\ninteroperability and wider system integration. Finally, the paper reviews the\nmain performance issues, including latency, energy consumption and network\nthroughput. The survey is expected to be useful to system architects and\nprotocol designers when choosing the communication protocols in an integrated\nIoT-to-fog-to-cloud system architecture. \n\n"}
{"id": "1804.01895", "contents": "Title: Implicit Coordination of Caches in Small Cell Networks under Unknown\n  Popularity Profiles Abstract: We focus on a dense cellular network, in which a limited-size cache is\navailable at every Base Station (BS). In order to optimize the overall\nperformance of the system in such scenario, where a significant fraction of the\nusers is covered by several BSs, a tight coordination among nearby caches is\nneeded. To this end, this pape introduces a class of simple and fully\ndistributed caching policies, which require neither direct communication among\nBSs, nor a priori knowledge of content popularity. Furthermore, we propose a\nnovel approximate analytical methodology to assess the performance of\ninteracting caches under such policies. Our approach builds upon the well known\ncharacteristic time approximation and provides predictions that are\nsurprisingly accurate (hardly distinguishable from the simulations) in most of\nthe scenarios. Both synthetic and trace-driven results show that the our\ncaching policies achieve excellent performance (in some cases provably\noptimal). They outperform state-of-the-art dynamic policies for interacting\ncaches, and, in some cases, also the greedy content placement, which is known\nto be the best performing polynomial algorithm under static and perfectly-known\ncontent popularity profiles. \n\n"}
{"id": "1804.01908", "contents": "Title: A Tutorial on Beam Management for 3GPP NR at mmWave Frequencies Abstract: The millimeter wave (mmWave) frequencies offer the availability of huge\nbandwidths to provide unprecedented data rates to next-generation cellular\nmobile terminals. However, mmWave links are highly susceptible to rapid channel\nvariations and suffer from severe free-space pathloss and atmospheric\nabsorption. To address these challenges, the base stations and the mobile\nterminals will use highly directional antennas to achieve sufficient link\nbudget in wide area networks. The consequence is the need for precise alignment\nof the transmitter and the receiver beams, an operation which may increase the\nlatency of establishing a link, and has important implications for control\nlayer procedures, such as initial access, handover and beam tracking. This\ntutorial provides an overview of recently proposed measurement techniques for\nbeam and mobility management in mmWave cellular networks, and gives insights\ninto the design of accurate, reactive and robust control schemes suitable for a\n3GPP NR cellular network. We will illustrate that the best strategy depends on\nthe specific environment in which the nodes are deployed, and give guidelines\nto inform the optimal choice as a function of the system parameters. \n\n"}
{"id": "1804.03219", "contents": "Title: Dynamic Pricing and Learning with Competition: Insights from the Dynamic\n  Pricing Challenge at the 2017 INFORMS RM & Pricing Conference Abstract: This paper presents the results of the Dynamic Pricing Challenge, held on the\noccasion of the 17th INFORMS Revenue Management and Pricing Section Conference\non June 29-30, 2017 in Amsterdam, The Netherlands. For this challenge,\nparticipants submitted algorithms for pricing and demand learning of which the\nnumerical performance was analyzed in simulated market environments. This\nallows consideration of market dynamics that are not analytically tractable or\ncan not be empirically analyzed due to practical complications. Our findings\nimplicate that the relative performance of algorithms varies substantially\nacross different market dynamics, which confirms the intrinsic complexity of\npricing and learning in the presence of competition. \n\n"}
{"id": "1804.04365", "contents": "Title: A Survey of Fog Computing and Communication: Current Researches and\n  Future Directions Abstract: In this survey, we discuss the evolution of distributed computing from the\nutility computing to the fog computing, various research challenges for the\ndevelopment of fog computing environments, the current status on fog computing\nresearch along with a taxonomy of various existing works in this direction.\nThen, we focus on the architectures of fog computing systems, technologies for\nenabling fog, fog computing features, security and privacy of fog, the QoS\nparameters, applications of fog, and give critical insights of various works\ndone on this domain. Lastly, we briefly discuss about different fog computing\nassociations that closely work on the development of fog based platforms and\nservices, and give a summary of various types of overheads associated with fog\ncomputing platforms. Finally, we provide a thorough discussion on the future\nscopes and open research areas in fog computing as an enabler for the next\ngeneration computing paradigm. \n\n"}
{"id": "1804.05019", "contents": "Title: Automatic Detection and Query of Wireless Spectrum Events from Streaming\n  Data Abstract: Several alternatives for more efficient spectrum management have been\nproposed over the last decade, resulting in new techniques for automatic\nwideband spectrum sensing. However, while spectrum sensing technology is\nimportant, understanding, using and taking actions on this data for better\nspectrum and network resource management is at least equally important. In this\npaper, we propose a system that is able to automatically detect wireless\nspectrum events from streaming spectrum sensing data, and enables the\nconsumption of the events as they are produced, as a statistical report or on a\nper-query basis. The proposed system is referred to as spectrum streamer and is\nwireless technology agnostic, scalable, able to deliver actionable information\nto humans and machines and also enables application development by custom\nquerying of the detected events. \n\n"}
{"id": "1804.05183", "contents": "Title: Scheduling Advertisement Delivery in Vehicular Networks Abstract: Vehicular users are emerging as a prime market for targeted advertisement,\nwhere advertisements (ads) are sent from network points of access to vehicles,\nand displayed to passengers only if they are relevant to them. In this study,\nwe take the viewpoint of a broker managing the advertisement system, and\ngetting paid every time a relevant ad is displayed to an interested user. The\nbroker selects the ads to broadcast at each point of access so as to maximize\nits revenue. In this context, we observe that choosing the ads that best fit\nthe users' interest could actually hurt the broker's revenue. In light of this\nconflict, we present Volfied, an algorithm allowing for conflict-free,\nnear-optimal ad selection with very low computational complexity. Our\nperformance evaluation, carried out through real-world vehicular traces, shows\nthat Volfied increases the broker revenue by up to 70% with provably low\ncomputational complexity, compared to state-of-the-art alternatives. \n\n"}
{"id": "1804.06368", "contents": "Title: Ultra-Reliable and Low-Latency Vehicular Transmission: An Extreme Value\n  Theory Approach Abstract: Considering a Manhattan mobility model in vehicle-to-vehicle networks, this\nwork studies a power minimization problem subject to second-order statistical\nconstraints on latency and reliability, captured by a network-wide maximal data\nqueue length. We invoke results in extreme value theory to characterize\nstatistics of extreme events in terms of the maximal queue length.\nSubsequently, leveraging Lyapunov stochastic optimization to deal with network\ndynamics, we propose two queue-aware power allocation solutions. In contrast\nwith the baseline, our approaches achieve lower mean and variance of the\nmaximal queue length. \n\n"}
{"id": "1804.06593", "contents": "Title: Coexistence of URLLC and eMBB services in the C-RAN Uplink: An\n  Information-Theoretic Study Abstract: The performance of orthogonal and non-orthogonal multiple access is studied\nfor the multiplexing of enhanced Mobile BroadBand (eMBB) and Ultra-Reliable\nLow-Latency Communications (URLLC) users in the uplink of a multi-cell Cloud\nRadio Access Network (C-RAN) architecture. While eMBB users can operate over\nlong codewords spread in time and frequency, URLLC users' transmissions are\nrandom and localized in time due to their low-latency requirements. These\nrequirements also call for decoding of their packets to be carried out at the\nedge nodes (ENs), whereas eMBB traffic can leverage the interference management\ncapabilities of centralized decoding at the cloud. Using information-theoretic\narguments, the performance trade-offs between eMBB and URLLC traffic types are\ninvestigated in terms of rate for the former, and rate, access latency, and\nreliability for the latter. The analysis includes non-orthogonal multiple\naccess (NOMA) with different decoding architectures, such as puncturing and\nsuccessive interference cancellation (SIC). The study sheds light into\neffective design choices as a function of inter-cell interference,\nsignal-to-noise ratio levels, and fronthaul capacity constraints. \n\n"}
{"id": "1804.06711", "contents": "Title: The CCI30 Index Abstract: We describe the design of the CCI30 cryptocurrency index. \n\n"}
{"id": "1804.08121", "contents": "Title: Cellular Connectivity for UAVs: Network Modeling, Performance Analysis\n  and Design Guidelines Abstract: The growing use of aerial user equipments (UEs) in various applications\nrequires ubiquitous and reliable connectivity for safe control and data\nexchange between these devices and ground stations. Key questions that need to\nbe addressed when planning the deployment of aerial UEs are whether the\ncellular network is a suitable candidate for enabling such connectivity, and\nhow the inclusion of aerial UEs might impact the overall network efficiency.\nThis paper provides an in-depth analysis of user and network level performance\nof a cellular network that serves both unmanned aerial vehicles (UAVs) and\nground users in the downlink. Our results show that the favorable propagation\nconditions that UAVs enjoy due to their height often backfire on them, as the\nincreased co-channel interference received from neighboring ground BSs is not\ncompensated by the improved signal strength. When compared with a ground user\nin an urban area, our analysis shows that a UAV flying at 100 meters can\nexperience a throughput decrease of a factor 10 and a coverage drop from 76% to\n30%. Motivated by these findings, we develop UAV and network based solutions to\nenable an adequate integration of UAVs into cellular networks. In particular,\nwe show that an optimal tilting of the UAV antenna can increase their coverage\nand throughput from 23% to 89% and from 3.5 b/s/Hz to 5.8 b/s/Hz, respectively,\noutperforming ground UEs. Furthermore, our findings reveal that depending on\nUAV altitude, the aerial user performance can scale with respect to the network\ndensity better than that of a ground user. Finally, our results show that\nnetwork densification and the use of micro cells limit UAV performance. While\nUAV usage has the potential to increase area spectral efficiency (ASE) of\ncellular networks with moderate number of cells, they might hamper the\ndevelopment of future ultra dense networks. \n\n"}
{"id": "1804.08138", "contents": "Title: Complex Network Analysis of Men Single ATP Tennis Matches Abstract: Who are the most significant players in the history of men tennis? Is the\nofficial ATP ranking system fair in evaluating players scores? Which players\ndeserved the most contemplation looking at their match records? Which players\nhave never faced yet and are likely to play against in the future? Those are\njust some of the questions developed in this paper supported by data updated at\nApril 2018. In order to give an answer to the aforementioned questions, complex\nnetwork science techniques have been applied to some representations of the\nnetwork of men singles tennis matches. Additionally, a new predictive algorithm\nis proposed in order to forecast the winner of a match. \n\n"}
{"id": "1804.08409", "contents": "Title: Stochastic Geometry Modeling of Cellular V2X Communication Over Shared\n  Channels Abstract: To overcome the limitations of DSRC with short range, non-supportability of\nhigh density networks, unreliable broadcast services, signal congestion and\nconnectivity disruptions, cellular vehicle-to-everything (C-V2X) communication\nnetworks, standardized in 3rd Generation Partnership Project (3GPP) Release 14,\nhave been recently introduced to cover broader vehicular communication\nscenarios including vehicle-to-vehicle (V2V), vehicle-to-pedestrian (V2P) and\nvehicle-to-infrastructure/network (V2I/N). In C-V2X, vehicles can directly\ncommunicate over PC5 based dedicated sidelinks called direct mode or V2V\ncommunication. However, high vehicle densities may require reuse of cellular\nspectrum for V2V. Moreover, infrastructure mode communication through V2I/N\nlinks can augment V2V communication by enhancing communication range and\nreliability for enhanced safety along with consistent performance under traffic\ncongestions. Motivated by the stringent connection reliability, spectral\nefficiency, and coverage requirements in C-V2X, this paper presents the first\ncomprehensive and tractable analytical framework for performance of C-V2X\nnetworks over shared V2V and cellular uplink channels, where the transmitting\nvehicles can deliver their information via infrastructure or direct mode, based\non their distances, propagation environments and the bias factor. By\npractically modeling the vehicles on the roads using the doubly stochastic Cox\nprocess and the base-stations, we derive new association probabilities, new\nsuccess probabilities of infrastructure and direct mode, and overall success\nprobability of the C-V2X communication over shared channels, which are\nvalidated by the simulations results. Our results reveal the benefits of our\nproposed model (possibility of selecting both direct and infrastructure modes\nover shared channels) compared to V2V network in terms of success probability. \n\n"}
{"id": "1804.08415", "contents": "Title: On the Number and 3D Placement of Drone Base Stations in Wireless\n  Cellular Networks Abstract: Using drone base stations (drone-BSs) in wireless networks has started\nattracting attention. Drone-BSs can assist the ground BSs in both capacity and\ncoverage enhancement. One of the important problems about integrating drone-BSs\nto cellular networks is the management of their placement to satisfy the\ndynamic system requirements. In this paper, we propose a method to find the\npositions of drone-BSs in an area with different user densities using a\nheuristic algorithm. The goal is to find the minimum number of drone-BSs and\ntheir 3D placement so that all the users are served. Our simulation results\nshow that the proposed approach can satisfy the quality-of-service requirements\nof the network. \n\n"}
{"id": "1804.08787", "contents": "Title: Price and Performance of Cloud-hosted Virtual Network Functions:\n  Analysis and Future Challenges Abstract: The concept of Network Function Virtualization (NFV) has been introduced as a\nnew paradigm in the recent few years. NFV offers a number of benefits including\nsignificantly increased maintainability and reduced deployment overhead.\nSeveral works have been done to optimize deployment (also called embedding) of\nvirtual network functions (VNFs). However, no work to date has looked into\noptimizing the selection of cloud instances for a given VNF and its specific\nrequirements. In this paper, we evaluate the performance of VNFs when embedded\non different Amazon EC2 cloud instances. Specifically, we evaluate three VNFs\n(firewall, IDS, and NAT) in terms of arrival packet rate, resources\nutilization, and packet loss. Our results indicate that performance varies\nacross instance types, departing from the intuition of \"you get what you pay\nfor\" with cloud instances. We also find out that CPU is the critical resource\nfor the tested VNFs, although their peak packet processing capacities differ\nconsiderably from each other. Finally, based on the obtained results, we\nidentify key research challenges related to VNF instance selection and service\nchain provisioning. \n\n"}
{"id": "1804.08986", "contents": "Title: Feedback Control Goes Wireless: Guaranteed Stability over Low-power\n  Multi-hop Networks Abstract: Closing feedback loops fast and over long distances is key to emerging\napplications; for example, robot motion control and swarm coordination require\nupdate intervals of tens of milliseconds. Low-power wireless technology is\npreferred for its low cost, small form factor, and flexibility, especially if\nthe devices support multi-hop communication. So far, however, feedback control\nover wireless multi-hop networks has only been shown for update intervals on\nthe order of seconds. This paper presents a wireless embedded system that tames\nimperfections impairing control performance (e.g., jitter and message loss),\nand a control design that exploits the essential properties of this system to\nprovably guarantee closed-loop stability for physical processes with linear\ntime-invariant dynamics. Using experiments on a cyber-physical testbed with 20\nwireless nodes and multiple cart-pole systems, we are the first to demonstrate\nand evaluate feedback control and coordination over wireless multi-hop networks\nfor update intervals of 20 to 50 milliseconds. \n\n"}
{"id": "1804.09201", "contents": "Title: Resource Allocation and HARQ Optimization for URLLC Traffic in 5G\n  Wireless Networks Abstract: 5G wireless networks are expected to support Ultra Reliable Low Latency\nCommunications (URLLC) traffic which requires very low packet delays ( < 1\nmsec.) and extremely high reliability ($\\sim$99.999\\%). In this paper we focus\non the design of a wireless system supporting downlink URLLC traffic. Using a\nqueuing network based model for the wireless system we characterize the effect\nof various design choices on the maximum URLLC load it can support, including:\n1) system parameters such as the bandwidth, link SINR , and QoS requirements;\n2) resource allocation schemes in Orthogonal Frequency Division Multiple Access\n(OFDMA) based systems; and 3) Hybrid Automatic Repeat Request (HARQ) schemes.\nKey contributions of this paper which are of practical interest are: 1) study\nof how the the minimum required system bandwidth to support a given URLLC load\nscales with associated QoS constraints; 2) characterization of optimal OFDMA\nresource allocation schemes which maximize the admissible URLLC load; and 3)\noptimization of a repetition code based HARQ scheme which approximates Chase\nHARQ combining. \n\n"}
{"id": "1804.10654", "contents": "Title: Resolving SINR Queries in a Dynamic Setting Abstract: We consider a set of transmitters broadcasting simultaneously on the same\nfrequency under the SINR model. Transmission power may vary from one\ntransmitter to another, and a transmitter's signal strength at a given point is\nmodeled by the transmitter's power divided by some constant power $\\alpha$ of\nthe distance it traveled. Roughly, a receiver at a given location can hear a\nspecific transmitter only if the transmitter's signal is stronger by a\nspecified ratio than the signals of all other transmitters combined. An SINR\nquery is to determine whether a receiver at a given location can hear any\ntransmitter, and if yes, which one.\n  An approximate answer to an SINR query is such that one gets a definite YES\nor definite NO, when the ratio between the strongest signal and all other\nsignals combined is well above or well below the reception threshold, while the\nanswer in the intermediate range is allowed to be either YES or NO.\n  We describe compact data structures that support approximate SINR queries in\nthe plane in a dynamic context, i.e., where transmitters may be inserted and\ndeleted over time. We distinguish between two main variants --- uniform power\nand non-uniform power. In both variants the preprocessing time is $O(n\n\\mathop{\\textrm{polylog}} n)$ and the amortized update time is\n$O(\\mathop{\\textrm{polylog}} n)$, while the query time is\n$O(\\mathop{\\textrm{polylog}} n)$ for uniform power, and randomized time\n$O(\\sqrt{n} \\mathop{\\textrm{polylog}} n)$ with high probability for non-uniform\npower.\n  Finally, we observe that in the static context the latter data structure can\nbe implemented differently, so that the query time is also\n$O(\\mathop{\\textrm{polylog}} n)$, thus significantly improving all previous\nresults for this problem. \n\n"}
{"id": "1804.10778", "contents": "Title: Hidden Vehicle Sensing via Asynchronous V2V Transmission: A\n  Multi-Path-Geometry Approach Abstract: Accurate vehicular sensing is a basic and important operation in autonomous\ndriving. Unfortunately, the existing techniques have their own limitations. For\ninstance, the communication-based approach (e.g., transmission of GPS\ninformation) has high latency and low reliability while the reflection-based\napproach (e.g., RADAR) is incapable of detecting hidden vehicles (HVs) without\nline-of-sight. This is arguably the reason behind some recent fatal accidents\ninvolving autonomous vehicles. To address this issue, this paper presents a\nnovel HV-sensing technology that exploits multi-path transmission from a HV to\na sensing vehicle (SV). The powerful technology enables the SV to detect\nmultiple HV-state parameters including position, orientation of driving\ndirection, and size. Its implementation does not even require\ntransmitter-receiver synchronization like conventional mobile positioning\ntechniques. Our design approach leverages estimated information on multi-path\n(AoA/AoD/ToA) and their geometric relations. As a result, a complex system of\nequations or optimization problems, where the desired HV-state parameters are\nvariables, can be formulated for different channel-noise conditions. The\ndevelopment of intelligent solution methods ranging from least-square estimator\nto disk/box minimization yields a set of practical HV-sensing techniques. We\nstudy their feasibility conditions in terms of the required number of paths.\nFurthermore, practical solutions, including sequential path combining and\nrandom directional beamforming, are proposed to enable HV-sensing given\ninsufficient paths. Last, realistic simulation of driving in both highway and\nrural scenarios demonstrates the effectiveness of the proposed techniques. In\nsummary, the proposed technique will enhance the capabilities of existing\nvehicular sensing technologies by enabling HV-sensing. \n\n"}
{"id": "1804.10856", "contents": "Title: Efficient Calculation of Meta Distributions and the Performance of User\n  Percentiles Abstract: Meta distributions (MDs) are refined performance metrics in wireless networks\nmodeled using point processes. While there is no known method to directly\ncalculate MDs, the moments of the underlying conditional distributions (given\nthe point process) can often be expressed in exact analytical form. The problem\nof finding the MD given the moments has several solutions, but the standard\napproaches are inefficient and sensitive to the choices of a number of\nparameters. Here we propose and explore the use of a method based on binomial\nmixtures, which has several key advantages over other methods, since it is\nbased on a simple linear transform of the moments. \n\n"}
{"id": "1804.11147", "contents": "Title: FIRST: A Framework for Optimizing Information Quality in Mobile\n  Crowdsensing Systems Abstract: Mobile crowdsensing allows data collection at a scale and pace that was once\nimpossible. One of the biggest challenges in mobile crowdsensing is that\nparticipants may exhibit malicious or unreliable behavior. Therefore, it\nbecomes imperative to design algorithms to accurately classify between reliable\nand unreliable sensing reports. To this end, we propose a novel Framework for\noptimizing Information Reliability in Smartphone-based participaTory sensing\n(FIRST), that leverages mobile trusted participants (MTPs) to securely assess\nthe reliability of sensing reports. FIRST models and solves the challenging\nproblem of determining before deployment the minimum number of MTPs to be used\nin order to achieve desired classification accuracy. We extensively evaluate\nFIRST through an implementation in iOS and Android of a room occupancy\nmonitoring system, and through simulations with real-world mobility traces.\nExperimental results demonstrate that FIRST reduces significantly the impact of\nthree security attacks (i.e., corruption, on/off, and collusion), by achieving\na classification accuracy of almost 80% in the considered scenarios. Finally,\nwe discuss our ongoing research efforts to test the performance of FIRST as\npart of the National Map Corps project. \n\n"}
{"id": "1805.00041", "contents": "Title: LBP: Robust Rate Adaptation Algorithm for SVC Video Streaming Abstract: Video streaming today accounts for up to 55\\% of mobile traffic. In this\npaper, we explore streaming videos encoded using Scalable Video Coding scheme\n(SVC) over highly variable bandwidth conditions such as cellular networks.\nSVC's unique encoding scheme allows the quality of a video chunk to change\nincrementally, making it more flexible and adaptive to challenging network\nconditions compared to other encoding schemes. Our contribution is threefold.\nFirst, we formulate the quality decisions of video chunks constrained by the\navailable bandwidth, the playback buffer, and the chunk deadlines as an\noptimization problem. The objective is to optimize a novel QoE metric that\nmodels a combination of the three objectives of minimizing the stall/skip\nduration of the video, maximizing the playback quality of every chunk, and\nminimizing the number of quality switches. Second, we develop Layered Bin\nPacking (LBP) Adaptation Algorithm, a novel algorithm that solves the proposed\noptimization problem. Moreover, we show that LBP achieves the optimal solution\nof the proposed optimization problem with linear complexity in the number of\nvideo chunks. Third, we propose an online algorithm (online LBP) where several\nchallenges are addressed including handling bandwidth prediction errors, and\nshort prediction duration. Extensive simulations with real bandwidth traces of\npublic datasets reveal the robustness of our scheme and demonstrate its\nsignificant performance improvement as compared to the state-of-the-art SVC\nstreaming algorithms. The proposed algorithm is also implemented on a TCP/IP\nemulation test bed with real LTE bandwidth traces, and the emulation confirms\nthe simulation results and validates that the algorithm can be implemented and\ndeployed on today's mobile devices. \n\n"}
{"id": "1805.01559", "contents": "Title: Computational Optimal Transport for 5G Massive C-RAN Device Association Abstract: The massive scale of future wireless networks will create computational\nbottlenecks in performance optimization. In this paper, we study the problem of\nconnecting mobile traffic to Cloud RAN (C-RAN) stations. To balance station\nload, we steer the traffic by designing device association rules. The baseline\nassociation rule connects each device to the station with the strongest signal,\nwhich does not account for interference or traffic hot spots, and leads to load\nimbalances and performance deterioration. Instead, we can formulate an\noptimization problem to decide centrally the best association rule at each time\ninstance. However, in practice this optimization has such high dimensions, that\neven linear programming solvers fail to solve. To address the challenge of\nmassive connectivity, we propose an approach based on the theory of optimal\ntransport, which studies the economical transfer of probability between two\ndistributions. Our proposed methodology can further inspire scalable algorithms\nfor massive optimization problems in wireless networks. \n\n"}
{"id": "1805.02719", "contents": "Title: Mobility-Aware Analysis of 5G and B5G Cellular Networks: A Tutorial Abstract: Providing network connectivity to mobile users is a key requirement for\ncellular wireless networks. User mobility impacts network performance as well\nas user perceived service quality. For efficient network dimensioning and\noptimization, it is therefore required to characterize the mobility-aware\nnetwork performance metrics such as the handoff rate, handoff probability,\nsojourn time, direction switch rate, and users' throughput or coverage. This\ncharacterization is particularly challenging for heterogeneous,\ndense/ultra-dense, and random cellular networks such as the emerging 5G and\nbeyond 5G (B5G) networks. In this article, we provide a tutorial on\nmobility-aware performance analysis of both the spatially random and\nnon-random, single-tier and multi-tier cellular networks. We first provide a\nsummary of the different mobility models which include purely random models,\nspatially correlated, and temporally correlated models. The differences among\nvarious mobility models, their statistical properties, and their pros and cons\nare presented. We then describe two main analytical approaches for\nmobility-aware performance analysis of both random and non-random cellular\nnetworks. For the first approach, we describe a general methodology and present\nseveral case studies for different cellular network tessellations such as\nsquare lattice, hexagon lattice, single-tier and multi-tier models in which\nbase-stations (BSs) follow a homogeneous Poisson Point Process (PPP). For the\nsecond approach, we also outline the general methodology. In addition, we\ndiscuss some limitations/imperfections of the existing techniques and provide\ncorrections to these imperfections. Finally, we point out specific 5G\napplication scenarios where the impact of mobility would be significant and\noutline the challenges associated with mobility-aware analysis of those\nscenarios. \n\n"}
{"id": "1805.03573", "contents": "Title: A Novel PMU Fog based Early Anomaly Detection for an Efficient Wide Area\n  PMU Network Abstract: Based on phasor measurement units (PMUs), a synchronphasor system is widely\nrecognized as a promising smart grid measurement system. It is able to provide\nhigh-frequency, high-accuracy phasor measurements sampling for Wide Area\nMonitoring and Control (WAMC) applications. However, the high sampling\nfrequency of measurement data under strict latency constraints introduces new\nchallenges for real time communication. It would be very helpful if the\ncollected data can be prioritized according to its importance such that the\nexisting quality of service (QoS) mechanisms in the communication networks can\nbe leveraged. To achieve this goal, certain anomaly detection functions should\nbe conducted by the PMUs. Inspired by the recent emerging edge-fog-cloud\ncomputing hierarchical architecture, which allows computing tasks to be\nconducted at the network edge, a novel PMU fog is proposed in this paper. Two\nanomaly detection approaches, Singular Spectrum Analysis (SSA) and K-Nearest\nNeighbors (KNN), are evaluated in the PMU fog using the IEEE 16-machine 68-bus\nsystem. The simulation experiments based on Riverbed Modeler demonstrate that\nthe proposed PMU fog can effectively reduce the data flow end-to-end (ETE)\ndelay without sacrificing data completeness. \n\n"}
{"id": "1805.04271", "contents": "Title: Performance Study of LTE and mmWave in Vehicle-to-Network Communications Abstract: A key enabler for the emerging autonomous and cooperative driving services is\nhigh-throughput and reliable Vehicle-to-Network (V2N) communication. In this\nrespect, the millimeter wave (mmWave) frequencies hold great promises because\nof the large available bandwidth which may provide the required link capacity.\nHowever, this potential is hindered by the challenging propagation\ncharacteristics of high-frequency channels and the dynamic topology of the\nvehicular scenarios, which affect the reliability of the connection. Moreover,\nmmWave transmissions typically leverage beamforming gain to compensate for the\nincreased path loss experienced at high frequencies. This, however, requires\nfine alignment of the transmitting and receiving beams, which may be difficult\nin vehicular scenarios. Those limitations may undermine the performance of V2N\ncommunications and pose new challenges for proper vehicular communication\ndesign. In this paper, we study by simulation the practical feasibility of some\nmmWave-aware strategies to support V2N, in comparison to the traditional LTE\nconnectivity below 6 GHz. The results show that the orchestration among\ndifferent radios represents a viable solution to enable both high-capacity and\nrobust V2N communications. \n\n"}
{"id": "1805.04460", "contents": "Title: Network-based indicators of Bitcoin bubbles Abstract: The functioning of the cryptocurrency Bitcoin relies on the open availability\nof the entire history of its transactions. This makes it a particularly\ninteresting socio-economic system to analyse from the point of view of network\nscience. Here we analyse the evolution of the network of Bitcoin transactions\nbetween users. We achieve this by using the complete transaction history from\nDecember 5th 2011 to December 23rd 2013. This period includes three bubbles\nexperienced by the Bitcoin price. In particular, we focus on the global and\nlocal structural properties of the user network and their variation in relation\nto the different period of price surge and decline. By analysing the temporal\nvariation of the heterogeneity of the connectivity patterns we gain insights on\nthe different mechanisms that take place during bubbles, and find that hubs\n(i.e., the most connected nodes) had a fundamental role in triggering the burst\nof the second bubble. Finally, we examine the local topological structures of\ninteractions between users, we discover that the relative frequency of triadic\ninteractions experiences a strong change before, during and after a bubble, and\nsuggest that the importance of the hubs grows during the bubble. These results\nprovide further evidence that the behaviour of the hubs during bubbles\nsignificantly increases the systemic risk of the Bitcoin network, and discuss\nthe implications on public policy interventions. \n\n"}
{"id": "1805.06670", "contents": "Title: Show me the Cache: Optimizing Cache-Friendly Recommendations for\n  Sequential Content Access Abstract: Caching has been successfully applied in wired networks, in the context of\nContent Distribution Networks (CDNs), and is quickly gaining ground for\nwireless systems. Storing popular content at the edge of the network (e.g. at\nsmall cells) is seen as a `win-win' for both the user (reduced access latency)\nand the operator (reduced load on the transport network and core servers).\nNevertheless, the much smaller size of such edge caches, and the volatility of\nuser preferences suggest that standard caching methods do not suffice in this\ncontext. What is more, simple popularity-based models commonly used (e.g. IRM)\nare becoming outdated, as users often consume multiple contents in sequence\n(e.g. YouTube, Spotify), and this consumption is driven by recommendation\nsystems. The latter presents a great opportunity to bias the recommender to\nminimize content access cost (e.g. maximizing cache hit rates). To this end, in\nthis paper we first propose a Markovian model for recommendation-driven user\nrequests. We then formulate the problem of biasing the recommendation algorithm\nto minimize access cost, while maintaining acceptable recommendation quality.\nWe show that the problem is non-convex, and propose an iterative ADMM-based\nalgorithm that outperforms existing schemes, and shows significant potential\nfor performance improvement on real content datasets. \n\n"}
{"id": "1805.06752", "contents": "Title: Scheduling Policies for Age Minimization in Wireless Networks with\n  Unknown Channel State Abstract: Age of information (AoI) is a recently proposed metric that measures the time\nelapsed since the generation of the last received information update. We\nconsider the problem of AoI minimization for a network under general\ninterference constraints, and time varying channel. We study the case where the\nchannel statistics are known, but the current channel state is unknown. We\npropose two scheduling policies, namely, the virtual queue based policy and\nage-based policy. In the virtual queue based policy, the scheduler schedules\nlinks with maximum weighted sum of the virtual queue lengths, while in the\nage-based policy, the scheduler schedules links with maximum weighted sum of a\nfunction of link AoI. We prove that the virtual queue based policy is peak age\noptimal, up to an additive constant, while the age-based policy is at most\nfactor 4 away from the optimal age. Numerical results suggest that both the\nproposed policies are, in fact, very close to the optimal. \n\n"}
{"id": "1805.06912", "contents": "Title: Fast reinforcement learning for decentralized MAC optimization Abstract: In this paper, we propose a novel decentralized framework for optimizing the\ntransmission strategy of Irregular Repetition Slotted ALOHA (IRSA) protocol in\nsensor networks. We consider a hierarchical communication framework that\nensures adaptivity to changing network conditions and does not require\ncentralized control. The proposed solution is inspired by the reinforcement\nlearning literature, and, in particular, Q-learning. To deal with sensor nodes'\nlimited lifetime and communication range, we allow them to decide how many\npacket replicas to transmit considering only their own buffer state. We show\nthat this information is sufficient and can help avoiding packets' collisions\nand improving the throughput significantly. We solve the problem using the\ndecentralized partially observable Markov Decision Process (Dec-POMDP)\nframework, where we allow each node to decide independently of the others how\nmany packet replicas to transmit. We enhance the proposed Q-learning based\nmethod with the concept of virtual experience, and we theoretically and\nexperimentally prove that convergence time is, thus, significantly reduced. The\nexperiments prove that our method leads to large throughput gains, in\nparticular when network traffic is heavy, and scales well with the size of the\nnetwork. To comprehend the effect of the problem's nature on the learning\ndynamics and vice versa, we investigate the waterfall effect, a severe\ndegradation in performance above a particular traffic load, typical for\ncodes-on-graphs and prove that our algorithm learns to alleviate it. \n\n"}
{"id": "1805.08357", "contents": "Title: Multi-UAV Cooperative Trajectory for Servicing Dynamic Demands and\n  Charging Battery Abstract: Unmanned Aerial Vehicle (UAV) technology is a promising solution for\nproviding high-quality mobile services to ground users, where a UAV with\nlimited service coverage travels among multiple geographical user locations\n(e.g., hotspots) for servicing their demands locally. How to dynamically\ndetermine a UAV swarm's cooperative path planning to best meet many users'\nspatio-temporally distributed demands is an important question but is\nunaddressed in the literature. To our best knowledge, this paper is the first\nto design and analyze cooperative path planning algorithms of a large UAV swarm\nfor optimally servicing many spatial locations, where ground users' demands are\nreleased dynamically in the long time horizon. Regarding a single UAV's path\nplanning design, we manage to substantially simplify the traditional dynamic\nprogram and propose an optimal algorithm of low computation complexity, which\nis only polynomial with respect to both the numbers of spatial locations and\nuser demands. After coordinating a large number $K$ of UAVs, this simplified\ndynamic optimization problem becomes intractable and we alternatively present a\nfast iterative cooperation algorithm with provable approximation ratio\n$1-(1-\\frac{1}{K})^{K}$ in the worst case, which is proved to obviously\noutperform the traditional approach of partitioning UAVs to serve different\nlocation clusters separately. To relax UAVs' battery capacity limit for\nsustainable service provisioning, we further allow UAVs to travel to charging\nstations in the mean time and thus jointly design UAVs' path planning over\nusers' locations and charging stations. Despite of the problem difficulty, for\nthe optimal solution, we successfully transform the problem to an integer\nlinear program by creating novel directed acyclic graph of the UAV-state\ntransition diagram, and propose an iterative algorithm with constant\napproximation ratio. \n\n"}
{"id": "1805.08550", "contents": "Title: Anticipating cryptocurrency prices using machine learning Abstract: Machine learning and AI-assisted trading have attracted growing interest for\nthe past few years. Here, we use this approach to test the hypothesis that the\ninefficiency of the cryptocurrency market can be exploited to generate abnormal\nprofits. We analyse daily data for $1,681$ cryptocurrencies for the period\nbetween Nov. 2015 and Apr. 2018. We show that simple trading strategies\nassisted by state-of-the-art machine learning algorithms outperform standard\nbenchmarks. Our results show that nontrivial, but ultimately simple,\nalgorithmic mechanisms can help anticipate the short-term evolution of the\ncryptocurrency market. \n\n"}
{"id": "1805.09253", "contents": "Title: Federated Learning for Ultra-Reliable Low-Latency V2V Communications Abstract: In this paper, a novel joint transmit power and resource allocation approach\nfor enabling ultra-reliable low-latency communication (URLLC) in vehicular\nnetworks is proposed. The objective is to minimize the network-wide power\nconsumption of vehicular users (VUEs) while ensuring high reliability in terms\nof probabilistic queuing delays. In particular, a reliability measure is\ndefined to characterize extreme events (i.e., when vehicles' queue lengths\nexceed a predefined threshold with non-negligible probability) using extreme\nvalue theory (EVT). Leveraging principles from federated learning (FL), the\ndistribution of these extreme events corresponding to the tail distribution of\nqueues is estimated by VUEs in a decentralized manner. Finally, Lyapunov\noptimization is used to find the joint transmit power and resource allocation\npolicies for each VUE in a distributed manner. The proposed solution is\nvalidated via extensive simulations using a Manhattan mobility model. It is\nshown that FL enables the proposed distributed method to estimate the tail\ndistribution of queues with an accuracy that is very close to a centralized\nsolution with up to 79\\% reductions in the amount of data that need to be\nexchanged. Furthermore, the proposed method yields up to 60\\% reductions of\nVUEs with large queue lengths, without an additional power consumption,\ncompared to an average queue-based baseline. Compared to systems with fixed\npower consumption and focusing on queue stability while minimizing average\npower consumption, the reduction in extreme events of the proposed method is\nabout two orders of magnitude. \n\n"}
{"id": "1805.10115", "contents": "Title: On incremental deployability Abstract: Motivated by the difficulty of effecting fundamental change in the\narchitecture of the Internet, in this paper, we study from a theoretical\nperspective the question of how individuals can join forces toward collective\nventures. To that end, we draw on an elementary concept in Internet systems\nengineering, namely, that of incremental deployability, which we study\nmathematically and computationally. For example, we show that incremental\ndeployability is at least as general a concept as the Nash equilibrium (in that\nthe latter can be derived from the former). We then draw on this foundation to\ndesign and analyze institutional mechanisms that are not only promising to\nbootstrap emerging Internet architectures but they also have broader\napplications in social organization beyond its predominant market (and\nfinance)-based character. \n\n"}
{"id": "1805.10539", "contents": "Title: Implementation of Epidemic Routing with IP Convergence Layer in ns-3 Abstract: We present the Epidemic routing protocol implementation in ns-3. It is a\nfull-featured DTN protocol in that it supports the message abstraction and\nstore-and-haul behavior. We compare the performance of our Epidemic routing\nns-3 implementation with the existing implementation of Epidemic in the ONE\nsimulator, and discuss the differences. \n\n"}
{"id": "1805.11606", "contents": "Title: Exploring Server-side Blocking of Regions Abstract: One of the Internet's greatest strengths is the degree to which it\nfacilitates access to any of its resources from users anywhere in the world.\nHowever, users in the developing world have complained of websites blocking\ntheir countries. We explore this phenomenon using a measurement study. With a\ncombination of automated page loads, manual checking, and traceroutes, we can\nsay, with high confidence, that some websites do block users from some regions.\nWe cannot say, with high confidence, why, or even based on what criteria, they\ndo so except for in some cases where the website states a reason. We do report\nqualitative evidence that fears of abuse and the costs of serving requests to\nsome regions may play a role. \n\n"}
{"id": "1805.11721", "contents": "Title: The Role of Caching in Future Communication Systems and Networks Abstract: This paper has the following ambitious goal: to convince the reader that\ncontent caching is an exciting research topic for the future communication\nsystems and networks. Caching has been studied for more than 40 years, and has\nrecently received increased attention from industry and academia. Novel caching\ntechniques promise to push the network performance to unprecedented limits, but\nalso pose significant technical challenges. This tutorial provides a brief\noverview of existing caching solutions, discusses seminal papers that open new\ndirections in caching, and presents the contributions of this Special Issue. We\nanalyze the challenges that caching needs to address today, considering also an\nindustry perspective, and identify bottleneck issues that must be resolved to\nunleash the full potential of this promising technique. \n\n"}
{"id": "1805.12090", "contents": "Title: Problem-Adapted Artificial Intelligence for Online Network Optimization Abstract: Future 5G wireless networks will rely on agile and automated network\nmanagement, where the usage of diverse resources must be jointly optimized with\nsurgical accuracy. A number of key wireless network functionalities (e.g.,\ntraffic steering, power control) give rise to hard optimization problems. What\nis more, high spatio-temporal traffic variability coupled with the need to\nsatisfy strict per slice/service SLAs in modern networks, suggest that these\nproblems must be constantly (re-)solved, to maintain close-to-optimal\nperformance. To this end, we propose the framework of Online Network\nOptimization (ONO), which seeks to maintain both agile and efficient control\nover time, using an arsenal of data-driven, online learning, and AI-based\ntechniques. Since the mathematical tools and the studied regimes vary widely\namong these methodologies, a theoretical comparison is often out of reach.\nTherefore, the important question `what is the right ONO technique?' remains\nopen to date. In this paper, we discuss the pros and cons of each technique and\npresent a direct quantitative comparison for a specific use case, using real\ndata. Our results suggest that carefully combining the insights of problem\nmodeling with state-of-the-art AI techniques provides significant advantages at\nreasonable complexity. \n\n"}
{"id": "1806.00605", "contents": "Title: Trade Network Reconstruction and Simulation with Changes in Trade Policy Abstract: The interdependent nature of the global economy has become stronger with\nincreases in international trade and investment. We propose a new model to\nreconstruct the international trade network and associated cost network by\nmaximizing entropy based on local information about inward and outward trade.\nWe show that the trade network can be successfully reconstructed using the\nproposed model. In addition to this reconstruction, we simulated structural\nchanges in the international trade network caused by changing trade tariffs in\nthe context of the government's trade policy. The simulation for the FOOD\ncategory shows that import of FOOD from the U.S. to Japan increase drastically\nby halving the import cost. Meanwhile, the simulation for the MACHINERY\ncategory shows that exports from Japan to the U.S. decrease drastically by\ndoubling the export cost, while exports to the EU increased. \n\n"}
{"id": "1806.00676", "contents": "Title: A Geometric Approach for Real-time Monitoring of Dynamic Large Scale\n  Graphs: AS-level graphs illustrated Abstract: The monitoring of large dynamic networks is a major chal- lenge for a wide\nrange of application. The complexity stems from properties of the underlying\ngraphs, in which slight local changes can lead to sizable variations of global\nprop- erties, e.g., under certain conditions, a single link cut that may be\noverlooked during monitoring can result in splitting the graph into two\ndisconnected components. Moreover, it is often difficult to determine whether a\nchange will propagate globally or remain local. Traditional graph theory\nmeasure such as the centrality or the assortativity of the graph are not\nsatisfying to characterize global properties of the graph. In this paper, we\ntackle the problem of real-time monitoring of dynamic large scale graphs by\ndeveloping a geometric approach that leverages notions of geometric curvature\nand recent development in graph embeddings using Ollivier-Ricci curvature [47].\nWe illustrate the use of our method by consid- ering the practical case of\nmonitoring dynamic variations of global Internet using topology changes\ninformation provided by combining several BGP feeds. In particular, we use our\nmethod to detect major events and changes via the geometry of the embedding of\nthe graph. \n\n"}
{"id": "1806.00750", "contents": "Title: Location Privacy in Cognitive Radio Networks: A Survey Abstract: Cognitive radio networks (CRNs) have emerged as an essential technology to\nenable dynamic and opportunistic spectrum access which aims to exploit\nunderutilized licensed channels to solve the spectrum scarcity problem. Despite\nthe great benefits that CRNs offer in terms of their ability to improve\nspectrum utilization efficiency, they suffer from user location privacy issues.\nKnowing that their whereabouts may be exposed can discourage users from joining\nand participating in the CRNs, thereby potentially hindering the adoption and\ndeployment of this technology in future generation networks. The location\ninformation leakage issue in the CRN context has recently started to gain\nattention from the research community due to its importance, and several\nresearch efforts have been made to tackle it. However, to the best of our\nknowledge, none of these works have tried to identify the vulnerabilities that\nare behind this issue or discuss the approaches that could be deployed to\nprevent it. In this paper, we try to fill this gap by providing a comprehensive\nsurvey that investigates the various location privacy risks and threats that\nmay arise from the different components of this CRN technology, and explores\nthe different privacy attacks and countermeasure solutions that have been\nproposed in the literature to cope with this location privacy issue. We also\ndiscuss some open research problems, related to this issue, that need to be\novercome by the research community to take advantage of the benefits of this\nkey CRN technology without having to sacrifice the users' privacy. \n\n"}
{"id": "1806.02088", "contents": "Title: Architectures and Key Technical Challenges for 5G Systems Incorporating\n  Satellites Abstract: Satellite Communication systems are a promising solution to extend and\ncomplement terrestrial networks in unserved or under-served areas. This aspect\nis reflected by recent commercial and standardisation endeavours. In\nparticular, 3GPP recently initiated a Study Item for New Radio-based, i.e., 5G,\nNon-Terrestrial Networks aimed at deploying satellite systems either as a\nstand-alone solution or as an integration to terrestrial networks in mobile\nbroadband and machine-type communication scenarios. However, typical satellite\nchannel impairments, as large path losses, delays, and Doppler shifts, pose\nsevere challenges to the realisation of a satellite-based NR network. In this\npaper, based on the architecture options currently being discussed in the\nstandardisation fora, we discuss and assess the impact of the satellite channel\ncharacteristics on the physical and Medium Access Control layers, both in terms\nof transmitted waveforms and procedures for enhanced Mobile BroadBand (eMBB)\nand NarrowBand-Internet of Things (NB-IoT) applications. The proposed analysis\nshows that the main technical challenges are related to the PHY/MAC procedures,\nin particular Random Access (RA), Timing Advance (TA), and Hybrid Automatic\nRepeat reQuest (HARQ) and, depending on the considered service and\narchitecture, different solutions are proposed. \n\n"}
{"id": "1806.03255", "contents": "Title: Automatically Generating a Large, Culture-Specific Blocklist for China Abstract: Internet censorship measurements rely on lists of websites to be tested, or\n\"block lists\" that are curated by third parties. Unfortunately, many of these\nlists are not public, and those that are tend to focus on a small group of\ntopics, leaving other types of sites and services untested. To increase and\ndiversify the set of sites on existing block lists, we use natural language\nprocessing and search engines to automatically discover a much wider range of\nwebsites that are censored in China. Using these techniques, we create a list\nof 1125 websites outside the Alexa Top 1,000 that cover Chinese politics,\nminority human rights organizations, oppressed religions, and more.\nImportantly, $\\textit{none of the sites we discover are present on the current\nlargest block list}$. The list that we develop not only vastly expands the set\nof sites that current Internet measurement tools can test, but it also deepens\nour understanding of the nature of content that is censored in China. We have\nreleased both this new block list and the code for generating it. \n\n"}
{"id": "1806.03328", "contents": "Title: Transient Delay Bounds for Multi-Hop Wireless Networks Abstract: In this article, we investigate the transient behavior of a sequence of\npackets/bits traversing a multi-hop wireless network. Our work is motivated by\nnovel applications from the domain of process automation, Machine-Type\nCommunication (MTC) and cyber-physical systems, where short messages are\ncommunicated and statistical guarantees need to be provided on a per-message\nlevel. In order to optimize such a network, apart from understanding the\nstationary system dynamics, an understanding of the short-term dynamics (i.e.,\ntransient behavior) is also required. To this end, we derive novel Wireless\nTransient Bounds (WTB) for end-to-end delay and backlog in a multi-hop wireless\nnetwork using stochastic network calculus approach. WTB depends on the initial\nbacklog at each node as well as the instantaneous channel states. We\nnumerically compare WTB with State-Of-The-Art Transient bounds (SOTAT), that\ncan be obtained by adapting existing stationary bounds, as well as simulation\nof the network. While SOTAT and stationary bounds are not able to capture the\nshort-term system dynamics well, WTB provides relatively tight upper bound and\nhas a decay rate that closely matches the simulation. This is achieved by WTB\nonly with a slight increase in the computational complexity, by a factor of O(T\n+ N), where T is the duration of the arriving sequence and N is the number of\nhops in the network. We believe that the presented analysis and the bounds can\nbe used as base for future work on transient network optimization, e.g., in\nmassive MTC, critical MTC, edge computing and autonomous vehicle. \n\n"}
{"id": "1806.04193", "contents": "Title: Stochastic Geometric Coverage Analysis in mmWave Cellular Networks with\n  Realistic Channel and Antenna Radiation Models Abstract: Millimeter-wave (mmWave) bands will play an important role in 5G wireless\nsystems. The system performance can be assessed by using models from stochastic\ngeometry that cater for the directivity in the desired signal transmissions as\nwell as the interference, and by calculating the\nsignal-to-interference-plus-noise ratio (SINR) coverage. Nonetheless, the\ncorrectness of the existing coverage expressions derived through stochastic\ngeometry may be questioned, as it is not clear whether they capture the impact\nof the detailed mmWave channel and antenna features. In this study, we propose\nan SINR coverage analysis framework that includes realistic channel model (from\nNYU) and antenna element radiation patterns (with isotropic/directional\nradiation). We first introduce two parameters, aligned gain and misaligned\ngain, associated with the desired signal beam and the interfering signal beam,\nrespectively. We provide the distributions of the aligned and misaligned gains\nthrough curve fitting of system-simulation results. The distribution of these\ngains is used to determine the distribution of the SINR. We compare the\nobtained analytical SINR coverage with the corresponding SINR coverage\ncalculated via system-level simulations. The results show that both aligned and\nmisaligned gains can be modeled as exponential-logarithmically distributed\nrandom variables with the highest accuracy, and can further be approximated as\nexponentially distributed random variables with reasonable accuracy. These\napproximations are thus expected to be useful to evaluate the system\nperformance under ultra-reliable and low-latency communication (URLLC) and\nevolved mobile broadband (eMBB) scenarios, respectively. \n\n"}
{"id": "1806.04583", "contents": "Title: Network-Connected UAV Communications: Potentials and Challenges Abstract: This article explores the use of network-connected unmanned aerial vehicle\n(UAV) communications as a compelling solution to achieve high-rate information\ntransmission and support ultra-reliable UAV remote command and control. We\nfirst discuss the use cases of UAVs and the resulting communication\nrequirements, accompanied with a flexible architecture for network-connected\nUAV communications. Then, the signal transmission and interference\ncharacteristics are theoretically analyzed, and subsequently we highlight the\ndesign and optimization considerations, including antenna design,\nnon-orthogonal multiple access communications, as well as network selection and\nassociation optimization. Finally, case studies are provided to show the\nfeasibility of network-connected UAV communications. \n\n"}
{"id": "1806.05776", "contents": "Title: Tiny Codes for Guaranteeable Delay Abstract: Future 5G systems will need to support ultra-reliable low-latency\ncommunications scenarios. From a latency-reliability viewpoint, it is\ninefficient to rely on average utility-based system design. Therefore, we\nintroduce the notion of guaranteeable delay which is the average delay plus\nthree standard deviations of the mean. We investigate the trade-off between\nguaranteeable delay and throughput for point-to-point wireless erasure links\nwith unreliable and delayed feedback, by bringing together signal flow\ntechniques to the area of coding. We use tiny codes, i.e. sliding window by\ncoding with just 2 packets, and design three variations of selective-repeat ARQ\nprotocols, by building on the baseline scheme, i.e. uncoded ARQ, developed by\nAusavapattanakun and Nosratinia: (i) Hybrid ARQ with soft combining at the\nreceiver; (ii) cumulative feedback-based ARQ without rate adaptation; and (iii)\nCoded ARQ with rate adaptation based on the cumulative feedback. Contrasting\nthe performance of these protocols with uncoded ARQ, we demonstrate that HARQ\nperforms only slightly better, cumulative feedback-based ARQ does not provide\nsignificant throughput while it has better average delay, and Coded ARQ can\nprovide gains up to about 40% in terms of throughput. Coded ARQ also provides\ndelay guarantees, and is robust to various challenges such as imperfect and\ndelayed feedback, burst erasures, and round-trip time fluctuations. This\nfeature may be preferable for meeting the strict end-to-end latency and\nreliability requirements of future use cases of ultra-reliable low-latency\ncommunications in 5G, such as mission-critical communications and industrial\ncontrol for critical control messaging. \n\n"}
{"id": "1806.07055", "contents": "Title: Capacitor Based Activity Sensing for Kinetic Powered Wearable IoTs Abstract: We propose a novel use of the conventional energy storage component, i.e.,\ncapacitor, in kinetic-powered wearable IoTs as a sensor to detect human\nactivities. Since different activities accumulate energies in the capacitor at\ndifferent rates, these activities can be detected directly by observing the\ncharging rate of the capacitor. The key advantage of the proposed capacitor\nbased activity sensing mechanism, called CapSense, is that it obviates the need\nfor sampling the motion signal during the activity detection period thus\nsignificantly saving power consumption of the wearable device. A challenge we\nface is that capacitors are inherently non-linear energy accumulators, which,\neven for the same activity, leads to significant variations in charging rates\nat different times depending on the current charge level of the capacitor. We\nsolve this problem by jointly configuring the parameters of the capacitor and\nthe associated energy harvesting circuits, which allows us to operate on\ncharging cycles that are approximately linear. We design and implement a\nkinetic-powered shoe sole and conduct experiments with 10 subjects. Our results\nshow that CapSense can classify five different daily activities with 95%\naccuracy while consuming 73% less system power compared to conventional motion\nsignal based activity detection. \n\n"}
{"id": "1806.07302", "contents": "Title: Trust Anchors in Software Defined Networks Abstract: Advances in software virtualization and network processing lead to increasing\nnetwork softwarization. Software network elements running on commodity\nplatforms replace or complement hardware components in cloud and mobile network\ninfrastructure. However, such com- modity platforms have a large attack surface\nand often lack granular control and tight integration of the underlying\nhardware and software stack. Often, software network elements are either\nthemselves vulnerable to software attacks or can be compromised through the\nbloated trusted computing base. To address this, we protect the core security\nassets of network elements - authentication credentials and cryptographic\ncontext - by provisioning them to and maintaining them exclusively in isolated\nexecution environments. We complement this with a secure and scalable mechanism\nto enroll network elements into software defined networks. Our evaluation\nresults show a negligible impact on run-time performance and only a moderate\nperformance impact at the deployment stage. \n\n"}
{"id": "1806.07476", "contents": "Title: CommunityWatch: The Swiss-Army Knife of BGP Anomaly Detection Abstract: We present CommunityWatch, an open-source system that enables timely and\naccurate detection of BGP routing anomalies. CommunityWatch leverages meta-data\nencoded by AS operators on their advertised routes through the BGP Communities\nattribute. The BGP Communities values lack standardized semantics, offering the\nflexibility to attach a wide range of information, including AS relationships,\nlocation data, and route redistribution policies. Therefore, parsing and\ncorrelating Community values and their dynamics enables the detection and\ntracking of a variety of routing anomalies. We exhibit the efficacy of\nCommunityWatch through the detection of three different types of anomalies:\ninfrastructure outages, route leaks, and traffic blackholing. \n\n"}
{"id": "1806.07761", "contents": "Title: Quick and Plenty: Achieving Low Delay and High Rate in 802.11ac Edge\n  Networks Abstract: We consider transport layer approaches for achieving high rate, low delay\ncommunication over edge paths where the bottleneck is an 802.11ac WLAN. We\nfirst show that by regulating send rate so as to maintain a target aggregation\nlevel it is possible to realise high rate, low delay communication over\n802.11ac WLANs. We then address two important practical issues arising in\nproduction networks, namely that (i) many client devices are non-rooted mobile\nhandsets/tablets and (ii) the bottleneck may lie in the backhaul rather than\nthe WLAN, or indeed vary between the two over time. We show that both these\nissues can be resolved by use of simple and robust machine learning techniques.\nWe present a prototype transport layer implementation of our low delay rate\nallocation approach and use this to evaluate performance under real radio\nconditions. \n\n"}
{"id": "1806.10521", "contents": "Title: Reliable Wireless Multi-Hop Networks with Decentralized Slot Management:\n  An Analysis of IEEE 802.15.4 DSME Abstract: Wireless communication is a key element in the realization of the Industrial\nInternet of Things for flexible and cost-efficient monitoring and control of\nindustrial processes. Wireless mesh networks using IEEE 802.15.4 have a high\npotential for executing monitoring and control tasks with low energy\nconsumption and low costs for deployment and maintenance. However, conventional\nmedium access techniques based on carrier sensing cannot provide the required\nreliability for industrial applications. Therefore, the standard was extended\nwith techniques for time-slotted medium access on multiple channels. In this\npaper, we present openDSME, a comprehensive implementation of the Deterministic\nand Synchronous Multi-channel Extension (DSME) and propose a method for\ntraffic-aware and decentralized slot scheduling to enable scalable wireless\nindustrial networks. The performance of DSME and our implementation is\ndemonstrated in the OMNeT++ simulator and on a physically deployed wireless\nnetwork in the FIT/IoT-LAB. It is shown that in the given scenarios, twice as\nmuch traffic can be delivered reliably by using DSME instead of CSMA/CA and\nthat the energy consumption can be reduced significantly. The paper is\ncompleted by presenting important trade-offs for parameter selection and by\nuncovering open issues of the current specification that call for further\neffort in research and standardization. \n\n"}
{"id": "1806.10964", "contents": "Title: Effective Wireless Scheduling via Hypergraph Sketches Abstract: An overarching issue in resource management of wireless networks is assessing\ntheir capacity: How much communication can be achieved in a network, utilizing\nall the tools available: power control, scheduling, routing, channel assignment\nand rate adjustment? We propose the first framework for approximation\nalgorithms in the physical model of wireless interference that addresses these\nquestions in full. The approximations obtained are at most doubly logarithmic\nin the link length and rate diversity. Where previous bounds are known, this\ngives an exponential improvement (or better).\n  A key contribution is showing that the complex interference relationship of\nthe physical model can be simplified, at a small cost, into a novel type of\namenable conflict graphs. We also show that the approximation obtained is\nprovably the best possible for any conflict graph formulation. \n\n"}
{"id": "1807.00078", "contents": "Title: Over-the-Air Time Synchronization for URLLC: Requirements, Challenges\n  and Possible Enablers Abstract: Ultra-reliable and low-latency communications (URLLC) is an emerging feature\nin 5G and beyond wireless systems, which is introduced to support stringent\nlatency and reliability requirements of mission-critical industrial\napplications. In many potential applications, multiple sensors/actuators\ncollaborate and require isochronous operation with strict and bounded jitter,\ne.g., \\SI{1}{\\micro\\second}. To this end, network time synchronization becomes\ncrucial for real-time and isochronous communication between a controller and\nthe sensors/actuators. In this paper, we look at different applications in\nfactory automation and smart grids to reveal the requirements of device-level\ntime synchronization and the challenges in extending the high-granularity\ntiming information to the devices. Also, we identify the potential over-the-air\nsynchronization mechanisms in 5G radio interface, and discuss the needed\nenhancements to meet the jitter constraints of time-sensitive URLLC\napplications. \n\n"}
{"id": "1807.00207", "contents": "Title: Multi-agent Learning for Cooperative Large-scale Caching Networks Abstract: Caching networks are designed to reduce traffic load at backhaul links, by\nserving demands from edge-nodes. In the past decades, many studies have been\ndone to address the caching problem. However, in practice, finding an optimal\ncaching policy is still challenging due to dynamicity of traffic and\nscalability caused by complex impact of caching strategy chosen by each\nindividual cache on other parts of network. In this paper, we focus on cache\nplacement to optimize the performance metrics such as hit ratio in cooperative\nlarge-scale caching networks. Our proposed solution, cooperative multi-agent\nbased cache placement (CoM-Cache) is based on multi-agent reinforcement\nlearning framework and can seamlessly track the content popularity dynamics in\nan on-line fashion. CoM-Cache is enable to solve the problems over a spectrum\nfrom isolated to interconnected caches and is designed flexibly to fit any\ncaching networks. To deal with dimensionality issue, CoM-Cache exploits the\nproperty of locality of interactions among caches. The experimental results\nreport CoM-Cache outperforms base-line schemes, however at the expense of\nreasonable additional complexity. \n\n"}
{"id": "1807.00464", "contents": "Title: Leveraging the Channel as a Sensor: Real-time Vehicle Classification\n  Using Multidimensional Radio-fingerprinting Abstract: Upcoming Intelligent Transportation Systems (ITSs) will transform roads from\nstatic resources to dynamic Cyber Physical Systems (CPSs) in order to satisfy\nthe requirements of future vehicular traffic in smart city environments.\nUp-to-date information serves as the basis for changing street directions as\nwell as guiding individual vehicles to a fitting parking slot. In this context,\nnot only abstract indicators like traffic flow and density are required, but\nalso data about mobility parameters and class information of individual\nvehicles. Consequently, accurate and reliable systems that are capable of\nproviding these kinds of information in real-time are highly demanded. In this\npaper, we present a system for classifying vehicles based on their\nradio-fingerprints which applies cutting-edge machine learning models and can\nbe non-intrusively installed into the existing road infrastructure in an ad-hoc\nmanner. In contrast to other approaches, it is able to provide accurate\nclassification results without causing privacy-violations or being vulnerable\nto challenging weather conditions. Moreover, it is a promising candidate for\nlarge-scale city deployments due to its cost-efficient installation and\nmaintenance properties. The proposed system is evaluated in a comprehensive\nfield evaluation campaign within an experimental live deployment on a German\nhighway, where it is able to achieve a binary classification success ratio of\nmore than 99% and an overall accuracy of 89.15% for a fine-grained\nclassification task with nine different classes. \n\n"}
{"id": "1807.00851", "contents": "Title: On Non-Preemptive VM Scheduling in the Cloud Abstract: We study the problem of scheduling VMs (Virtual Machines) in a distributed\nserver platform, motivated by cloud computing applications. The VMs arrive\ndynamically over time to the system, and require a certain amount of resources\n(e.g. memory, CPU, etc) for the duration of their service. To avoid costly\npreemptions, we consider non-preemptive scheduling: Each VM has to be assigned\nto a server which has enough residual capacity to accommodate it, and once a VM\nis assigned to a server, its service \\textit{cannot} be disrupted (preempted).\nPrior approaches to this problem either have high complexity, require\nsynchronization among the servers, or yield queue sizes/delays which are\nexcessively large. We propose a non-preemptive scheduling algorithm that\nresolves these issues. In general, given an approximation algorithm to Knapsack\nwith approximation ratio $r$, our scheduling algorithm can provide $r\\beta$\nfraction of the throughput region for $\\beta < r$. In the special case of a\ngreedy approximation algorithm to Knapsack, we further show that this condition\ncan be relaxed to $\\beta<1$. The parameters $\\beta$ and $r$ can be tuned to\nprovide a tradeoff between achievable throughput, delay, and computational\ncomplexity of the scheduling algorithm. Finally extensive simulation results\nusing both synthetic and real traffic traces are presented to verify the\nperformance of our algorithm. \n\n"}
{"id": "1807.01093", "contents": "Title: Hierarchical Capacity Provisioning for Fog Computing Abstract: The concept of fog computing is centered around providing computation\nresources at the edge of network, thereby reducing the latency and improving\nthe quality of service. However, it is still desirable to investigate how and\nwhere at the edge of the network the computation capacity should be\nprovisioned. To this end, we propose a hierarchical capacity provisioning\nscheme. In particular, we consider a two-tier network architecture consisting\nof shallow and deep cloudlets and explore the benefits of hierarchical capacity\nbased on queueing analysis. Moreover, we explore two different network\nscenarios in which the network delay between the two tiers is negligible as\nwell as the case that the deep cloudlet is located somewhere deeper in the\nnetwork and thus the delay is significant. More importantly, we model the first\nnetwork delay scenario with bufferless shallow cloudlets as well as the second\nscenario with finite-size buffer shallow cloudlets, and formulate an\noptimization problem for each model. We also use stochastic ordering to solve\nthe optimization problem formulated for the first model and an upper bound\nbased technique is proposed for the second model. The performance of the\nproposed scheme is evaluated via simulations in which we show the accuracy of\nthe proposed upper bound technique as well as the queue length estimation\napproach for both randomly generated input and real trace data. \n\n"}
{"id": "1807.01147", "contents": "Title: FastTrack: Minimizing Stalls for CDN-based Over-the-top Video Streaming\n  Systems Abstract: Traffic for internet video streaming has been rapidly increasing and is\nfurther expected to increase with the higher definition videos and IoT\napplications, such as 360 degree videos and augmented virtual reality\napplications. While efficient management of heterogeneous cloud resources to\noptimize the quality of experience is important, existing work in this problem\nspace often left out important factors. In this paper, we present a model for\ndescribing a today's representative system architecture for video streaming\napplications, typically composed of a centralized origin server and several CDN\nsites. Our model comprehensively considers the following factors: limited\ncaching spaces at the CDN sites, allocation of CDN for a video request, choice\nof different ports from the CDN, and the central storage and bandwidth\nallocation. With the model, we focus on minimizing a performance metric, stall\nduration tail probability (SDTP), and present a novel, yet efficient, algorithm\nto solve the formulated optimization problem. The theoretical bounds with\nrespect to the SDTP metric are also analyzed and presented. Our extensive\nsimulation results demonstrate that the proposed algorithms can significantly\nimprove the SDTP metric, compared to the baseline strategies. Small-scale video\nstreaming system implementation in a real cloud environment further validates\nour results. \n\n"}
{"id": "1807.01163", "contents": "Title: Inter-cluster Cooperation for Wireless D2D Caching Networks Abstract: Proactive wireless caching and device to device (D2D) communication have\nemerged as promising techniques for enhancing users' quality of service and\nnetwork performance. In this paper, we propose a new architecture for D2D\ncaching with inter-cluster cooperation. We study a cellular network in which\nusers cache popular files and share them with other users either in their\nproximity via D2D communication or with remote users using cellular\ntransmission. We characterize the network average delay per request from a\nqueuing perspective. Specifically, we formulate the delay minimization problem\nand show that it is NP-hard. Furthermore, we prove that the delay minimization\nproblem is equivalent to the minimization of a non-increasing monotone\nsupermodular function subject to a uniform partition matroid constraint. A\ncomputationally efficient greedy algorithm is proposed which is proven to be\nlocally optimal within a factor 0.63 of the optimum. We analyze the average per\nrequest throughput for different caching schemes and conduct the scaling\nanalysis for the average sum throughput. We show how throughput scaling depends\non video content popularity when the number of files grows asymptotically\nlarge. Simulation results show a delay reduction of 45% to 80% compared to a\nD2D caching system without inter-cluster cooperation. \n\n"}
{"id": "1807.02567", "contents": "Title: Deep Learning for Launching and Mitigating Wireless Jamming Attacks Abstract: An adversarial machine learning approach is introduced to launch jamming\nattacks on wireless communications and a defense strategy is presented. A\ncognitive transmitter uses a pre-trained classifier to predict the current\nchannel status based on recent sensing results and decides whether to transmit\nor not, whereas a jammer collects channel status and ACKs to build a deep\nlearning classifier that reliably predicts the next successful transmissions\nand effectively jams them. This jamming approach is shown to reduce the\ntransmitter's performance much more severely compared with random or\nsensing-based jamming. The deep learning classification scores are used by the\njammer for power control subject to an average power constraint. Next, a\ngenerative adversarial network (GAN) is developed for the jammer to reduce the\ntime to collect the training dataset by augmenting it with synthetic samples.\nAs a defense scheme, the transmitter deliberately takes a small number of wrong\nactions in spectrum access (in form of a causative attack against the jammer)\nand therefore prevents the jammer from building a reliable classifier. The\ntransmitter systematically selects when to take wrong actions and adapts the\nlevel of defense to mislead the jammer into making prediction errors and\nconsequently increase its throughput. \n\n"}
{"id": "1807.03454", "contents": "Title: Using Complex Network Theory for Temporal Locality in Network Traffic\n  Flows Abstract: Monitoring the interaction behaviors of network traffic flows and detecting\nunwanted Internet applications and anomalous flows have become a challenging\nproblem, since many applications obfuscate their network traffic flow using\nunregistered port numbers or payload encryption. In this paper, the temporal\nlocality complex network model--TLCN is proposed as a way to monitor, analyze\nand visualize network traffic flows. TLCNs model the interaction behaviors of\nlarge-scale network traffic flows, where the nodes and the edges can be defined\nto represent different flow levels and flow interactions separately. Then, the\nstatistical characteristics and dynamic behaviors of the TLCNs are studied to\nrepresent TLCN's structure representing ability to the flow interactions.\nAccording to the analysis of TLCN statistical characteristics with different\nInternet applications, we found that the weak interaction flows prefer to form\nthe small-world TLCN and the strong interaction flows prefer to the scale-free\nTLCN. In the studies of anomaly behaviors of TLCNs, the network structure of\nattacked TLCNs can have a remarkable feature for three attack patterns, and the\nevolution of TLCNs exhibits a good consistency between TLCN structure and\nattack process. With the introduction of TLCNs, we are able to harness a wealth\nof tools and graph modeling techniques from a diverse set of disciplines. \n\n"}
{"id": "1807.03930", "contents": "Title: Robust Beamforming Design in a NOMA Cognitive Radio Network Relying on\n  SWIPT Abstract: This paper studies a multiple-input single-output non-orthogonal multiple\naccess cognitive radio network relying on simultaneous wireless information and\npower transfer. A realistic non-linear energy harvesting model is applied and a\npower splitting architecture is adopted at each secondary user (SU). Since it\nis difficult to obtain perfect channel state information (CSI) in practice,\ninstead either a bounded or gaussian CSI error model is considered. Our robust\nbeamforming and power splitting ratio are jointly designed for two problems\nwith different objectives, namely that of minimizing the transmission power of\nthe cognitive base station and that of maximizing the total harvested energy of\nthe SUs, respectively. The optimization problems are challenging to solve,\nmainly because of the non-linear structure of the energy harvesting and CSI\nerrors models. We converted them into convex forms by using semi-definite\nrelaxation. For the minimum transmission power problem, we obtain the rank-2\nsolution under the bounded CSI error model, while for the maximum energy\nharvesting problem, a two-loop procedure using a one-dimensional search is\nproposed. Our simulation results show that the proposed scheme significantly\noutperforms its traditional orthogonal multiple access counterpart.\nFurthermore, the performance using the gaussian CSI error model is generally\nbetter than that using the bounded CSI error model. \n\n"}
{"id": "1807.04449", "contents": "Title: Cross-Sender Bit-Mixing Coding Abstract: Scheduling to avoid packet collisions is a long-standing challenge in\nnetworking, and has become even trickier in wireless networks with multiple\nsenders and multiple receivers. In fact, researchers have proved that even {\\em\nperfect} scheduling can only achieve $\\mathbf{R} = O(\\frac{1}{\\ln N})$. Here\n$N$ is the number of nodes in the network, and $\\mathbf{R}$ is the {\\em medium\nutilization rate}. Ideally, one would hope to achieve $\\mathbf{R} = \\Theta(1)$,\nwhile avoiding all the complexities in scheduling. To this end, this paper\nproposes {\\em cross-sender bit-mixing coding} ({\\em BMC}), which does not rely\non scheduling. Instead, users transmit simultaneously on suitably-chosen slots,\nand the amount of overlap in different user's slots is controlled via coding.\nWe prove that in all possible network topologies, using BMC enables us to\nachieve $\\mathbf{R}=\\Theta(1)$. We also prove that the space and time\ncomplexities of BMC encoding/decoding are all low-order polynomials. \n\n"}
{"id": "1807.04618", "contents": "Title: Proofs and Performance Evaluation of Greedy Multi-Channel Neighbor\n  Discovery Approaches Abstract: The accelerating penetration of physical environments by objects with\ninformation processing and wireless communication capabilities requires\napproaches to find potential communication partners and discover services. In\nthe present work, we focus on passive discovery approaches in multi-channel\nwireless networks based on overhearing periodic beacon transmissions of\nneighboring devices which are otherwise agnostic to the discovery process. We\npropose a family of low-complexity algorithms that generate listening schedules\nguaranteed to discover all neighbors. The presented approaches simultaneously\ndepending on the beacon periods optimize the worst case discovery time, the\nmean discovery time, and the mean number of neighbors discovered until any\narbitrary in time. The presented algorithms are fully compatible with\ntechnologies such as IEEE 802.11 and IEEE 802.15.4. Complementing the proposed\nlow-complexity algorithms, we formulate the problem of computing discovery\nschedules that minimize the mean discovery time for arbitrary beacon periods as\nan integer linear problem. We study the performance of the proposed approaches\nanalytically, by means of numerical experiments, and by extensively simulating\nthem under realistic conditions. We observe that the generated listening\nschedules significantly - by up to factor 4 for the mean discovery time, and by\nup to 300% for the mean number of neighbors discovered until each point in time\n- outperform the Passive Scan, a discovery approach defined in the IEEE\n802.15.4 standard. Based on the gained insights, we discuss how the selection\nof the beacon periods influences the efficiency of the discovery process, and\nprovide recommendations for the design of systems and protocols. \n\n"}
{"id": "1807.05220", "contents": "Title: Greedy Multi-Channel Neighbor Discovery Abstract: The accelerating penetration of physical environments by objects with\ninformation processing and wireless communication capabilities requires\napproaches to find potential communication partners and discover services. In\nthe present work, we focus on passive discovery approaches in multi-channel\nwireless networks based on overhearing periodic beacon transmissions of\nneighboring devices which are otherwise agnostic to the discovery process. We\npropose a family of low-complexity algorithms that generate listening schedules\nguaranteed to discover all neighbors. The presented approaches simultaneously\ndepending on the beacon periods optimize the worst case discovery time, the\nmean discovery time, and the mean number of neighbors discovered until any\narbitrary in time. The presented algorithms are fully compatible with\ntechnologies such as IEEE 802.11 and IEEE 802.15.4. Complementing the proposed\nlow-complexity algorithms, we formulate the problem of computing discovery\nschedules that minimize the mean discovery time for arbitrary beacon periods as\nan integer linear problem. We study the performance of the proposed approaches\nanalytically, by means of numerical experiments, and by extensively simulating\nthem under realistic conditions. We observe that the generated listening\nschedules significantly - by up to factor 4 for the mean discovery time, and by\nup to 300% for the mean number of neighbors discovered until each point in time\n- outperform the Passive Scan, a discovery approach defined in the IEEE\n802.15.4 standard. Based on the gained insights, we discuss how the selection\nof the beacon periods influences the efficiency of the discovery process, and\nprovide recommendations for the design of systems and protocols. \n\n"}
{"id": "1807.05523", "contents": "Title: Improving the Performance of WLANs by Reducing Unnecessary Active Scans Abstract: We consider the problem of excessive and unnecessary active scans in heavily\nutilized WLANs during which low rate probe requests and responses are\nbroadcast. These management frames severely impact the goodput. Our analysis of\ntwo production WLANs reveals that lesser number of non-overlapping channels in\n$2.4$ GHz makes it more prone to the effects of increased probe frames than $5$\nGHz. We find that not only up to $90$% of probe responses carry redundant\ninformation but the probe traffic can be as high as $60$\\% of the management\ntraffic. Furthermore, active scanning severely impacts real-time applications\nat a client as it increases the latency by $91$ times.\n  We present a detailed analysis of the impact of active scans on an individual\nclient and the whole network. We discuss three ways to control the probe\ntraffic in production WLANs -- access point configurations, network planning,\nand client modification. Our proposals for access point configuration are in\nline with current WLAN deployments, better network planning is device agnostic\nin nature, and client modification reduces the average number of probe requests\nper client by up to $50$% without hampering the ongoing WiFi connection. \n\n"}
{"id": "1807.08087", "contents": "Title: Capacity Analysis for Full Duplex Self-backhauled Small Cells Abstract: Full duplex (FD) communication enables simultaneous transmission and\nreception on the same frequency band. Though it has the potential of doubling\nthe throughput on isolated links, in reality, higher interference and\nasymmetric traffic demands in the uplink and downlink could significantly\nreduce the gains of FD operations. In this paper, we consider the application\nof FD operation in self-backhauled small cells, where multiple FD capable small\ncell base stations (SBS) are wirelessly backhauled by a FD capable macro-cell\nBS (MBS). To increase the capacity of the backhaul link, the MBS is equipped\nwith multiple antennas to enable space division multiple access (SDMA). A\nscheduling method using back-pressure algorithm and geometric programming is\nproposed for link selection and interference mitigation. Simulation results\nshow that with FD SDMA backhaul links, the proposed scheduler almost doubles\nthroughput under asymmetric traffic demand and various network conditions. \n\n"}
{"id": "1807.08088", "contents": "Title: Learning Optimal Resource Allocations in Wireless Systems Abstract: This paper considers the design of optimal resource allocation policies in\nwireless communication systems which are generically modeled as a functional\noptimization problem with stochastic constraints. These optimization problems\nhave the structure of a learning problem in which the statistical loss appears\nas a constraint, motivating the development of learning methodologies to\nattempt their solution. To handle stochastic constraints, training is\nundertaken in the dual domain. It is shown that this can be done with small\nloss of optimality when using near-universal learning parameterizations. In\nparticular, since deep neural networks (DNN) are near-universal their use is\nadvocated and explored. DNNs are trained here with a model-free primal-dual\nmethod that simultaneously learns a DNN parametrization of the resource\nallocation policy and optimizes the primal and dual variables. Numerical\nsimulations demonstrate the strong performance of the proposed approach on a\nnumber of common wireless resource allocation problems. \n\n"}
{"id": "1807.08615", "contents": "Title: Short-term and Long-term Cell Outage Compensation Using UAVs in 5G\n  Networks Abstract: The use of Unmanned Aerial Vehicles (UAVs) has gained interest in wireless\nnetworks for its many uses and advantages such as rapid deployment and\nmulti-purpose functionality. This is why wide deployment of UAVs has the\npotential to be integrated in the upcoming 5G standard. They can be used as\nflying base-stations, which can be deployed in case of ground Base-Stations\n(GBSs) failures. Such failures can be short-term or long-term. Based on the\ntype and duration of the failure, we propose a framework that uses drones or\nhelikites to mitigate GBS failures. Our proposed short-term and long-term cell\noutage compensation framework aims to mitigate the effect of the failure of any\nGBS in 5G networks. Within our framework, outage compensation is done with the\nassistance of sky BSs (UAVs). An optimization problem is formulated to jointly\nminimize communication power of the UAVs and maximize the minimum rates of the\nUsers' Equipment (UEs) affected by the failure. Also, the optimal placement of\nthe UAVs is determined. Simulation results show that the proposed framework\nguarantees the minimum quality of service for each UE in addition to minimizing\nthe UAVs' consumed energy. \n\n"}
{"id": "1807.09614", "contents": "Title: On the analysis of partially homogeneous nearest-neighbour random walks\n  in the quarter plane Abstract: This work deals with the stationary analysis of two-dimensional partially\nhomogeneous nearest-neighbour random walks. Such type of random walks in the\nquarter plane are characterized by the fact that the one-step transition\nprobabilities are functions of the state-space. We show that its stationary\nbehavior is investigated by solving a finite system of linear equations, and a\nfunctional equation with the aid of the theory of Riemann(-Hilbert) boundary\nvalue problems. This work is strongly motivated by emerging applications in\nmultiple access systems as well as in the study of a general class of queueing\nsystems with state dependent parameters. A simple numerical illustration\nproviding useful information about a queue-aware multiple access system is also\npresented. \n\n"}
{"id": "1807.10617", "contents": "Title: Temporal connectivity in finite networks with non-uniform measures Abstract: Soft Random Geometric Graphs (SRGGs) have been widely applied to various\nmodels including those of wireless sensor, communication, social and neural\nnetworks. SRGGs are constructed by randomly placing nodes in some space and\nmaking pairwise links probabilistically using a connection function that is\nsystem specific and usually decays with distance. In this paper we focus on the\napplication of SRGGs to wireless communication networks where information is\nrelayed in a multi hop fashion, although the analysis is more general and can\nbe applied elsewhere by using different distributions of nodes and/or\nconnection functions. We adopt a general non-uniform density which can model\nthe stationary distribution of different mobility models, with the interesting\ncase being when the density goes to zero along the boundaries. The global\nconnectivity properties of these non-uniform networks are likely to be\ndetermined by highly isolated nodes, where isolation can be caused by the\nspatial distribution or the local geometry (boundaries). We extend the analysis\nto temporal-spatial networks where we fix the underlying non-uniform\ndistribution of points and the dynamics are caused by the temporal variations\nin the link set, and explore the probability a node near the corner is isolated\nat time $T$. This work allows for insight into how non-uniformity (caused by\nmobility) and boundaries impact the connectivity features of temporal-spatial\nnetworks. We provide a simple method for approximating these probabilities for\na range of different connection functions and verify them against simulations.\nBoundary nodes are numerically shown to dominate the connectivity properties of\nthese finite networks with non-uniform measure. \n\n"}
{"id": "1807.11059", "contents": "Title: MPTCP meets FEC: Supporting Latency-Sensitive Applications over\n  Heterogeneous Networks Abstract: Over the past years, TCP has gone through numerous updates to provide\nperformance enhancement under diverse network conditions. However, with respect\nto losses, little can be achieved with legacy TCP detection and recovery\nmechanisms. Both fast retransmission and retransmission timeout take at least\none extra round trip time to perform, and this might significantly impact\nperformance of latency-sensitive applications, especially in lossy or high\ndelay networks. While forward error correction (FEC) is not a new initiative in\nthis direction, the majority of the approaches consider FEC inside the\napplication. In this paper, we design and implement a framework, where FEC is\nintegrated within TCP. Our main goal with this design choice is to enable\nlatency sensitive applications over TCP in high delay and lossy networks, but\nremaining application agnostic. We further incorporate this design into\nmultipath TCP (MPTCP), where we focus particularly on heterogeneous settings,\nconsidering the fact that TCP recovery mechanisms further escalate head-of-line\nblocking in multipath. We evaluate the performance of the proposed framework\nand show that such a framework can bring significant benefits compared to\nlegacy TCP and MPTCP for latency-sensitive real application traffic, such as\nvideo streaming and web services. \n\n"}
{"id": "1807.11329", "contents": "Title: EIQIS: Toward an Event-Oriented Indexable and Queryable Intelligent\n  Surveillance System Abstract: Edge computing provides the ability to link distributor users for multimedia\ncontent, while retaining the power of significant data storage and access at a\ncentralized computer. Two requirements of significance include: what\ninformation show be processed at the edge and how the content should be stored.\nAnswers to these questions require a combination of query-based search, access,\nand response as well as indexed-based processing, storage, and distribution. A\nmeasure of intelligence is not what is known, but is recalled, hence, future\nedge intelligence must provide recalled information for dynamic response. In\nthis paper, a novel event-oriented indexable and queryable intelligent\nsurveillance (EIQIS) system is introduced leveraging the on-site edge devices\nto collect the information sensed in format of frames and extracts useful\nfeatures to enhance situation awareness. The design principles are discussed\nand a preliminary proof-of-concept prototype is built that validated the\nfeasibility of the proposed idea. \n\n"}
{"id": "1807.11621", "contents": "Title: On the Security Analysis of a Cooperative Incremental Relaying Protocol\n  in the Presence of an Active Eavesdropper Abstract: Physical layer security offers an efficient means to decrease the risk of\nconfidential information leakage through wiretap links. In this paper, we\naddress the physical-layer security in a cooperative wireless subnetwork that\nincludes a source-destination pair and multiple relays, exchanging information\nin the presence of a malevolent eavesdropper. Specifically, the eavesdropper is\nactive in the network and transmits artificial noise (AN) with a\nmultiple-antenna transmitter to confound both the relays and the destination.\nWe first analyse the secrecy capacity of the direct source-to-destination\ntransmission in terms of intercept probability (IP) and secrecy outage\nprobability (SOP). A decode-and-forward incremental relaying (IR) protocol is\nthen introduced to improve reliability and security of communications in the\npresence of the active eavesdropper. Within this context, and depending on the\navailability of channel state information, three different schemes (one optimal\nand two sub-optimal) are proposed to select a trusted relay to improve the\nachievable secrecy rate. For each one of these schemes, and for both selection\nand maximum ratio combining at the destination and eavesdropper, we derive new\nand exact closed-form expressions for the IP and SOP. Our analysis and\nsimulation results demonstrate the superior performance of the proposed\nIR-based selection schemes for secure communication. They also confirm the\nexistence of a floor phenomenon for the SOP in the absence of AN. \n\n"}
{"id": "1808.00876", "contents": "Title: Normalization Before Shaking Toward Learning Symmetrically Distributed\n  Representation Without Margin in Speech Emotion Recognition Abstract: Regularization is crucial to the success of many practical deep learning\nmodels, in particular in a more often than not scenario where there are only a\nfew to a moderate number of accessible training samples. In addition to weight\ndecay, data augmentation and dropout, regularization based on multi-branch\narchitectures, such as Shake-Shake regularization, has been proven successful\nin many applications and attracted more and more attention. However, beyond\nmodel-based representation augmentation, it is unclear how Shake-Shake\nregularization helps to provide further improvement on classification tasks,\nlet alone the baffling interaction between batch normalization and shaking. In\nthis work, we present our investigation on Shake-Shake regularization, drawing\nconnections to the vicinal risk minimization principle and discriminative\nfeature learning in verification tasks. Furthermore, we identify a strong\nresemblance between batch normalized residual blocks and batch normalized\nrecurrent neural networks, where both of them share a similar convergence\nbehavior, which could be mitigated by a proper initialization of batch\nnormalization. Based on the findings, our experiments on speech emotion\nrecognition demonstrate simultaneously an improvement on the classification\naccuracy and a reduction on the generalization gap both with statistical\nsignificance. \n\n"}
{"id": "1808.01977", "contents": "Title: Deep Reinforcement Learning for Online Computation Offloading in\n  Wireless Powered Mobile-Edge Computing Networks Abstract: Wireless powered mobile-edge computing (MEC) has recently emerged as a\npromising paradigm to enhance the data processing capability of low-power\nnetworks, such as wireless sensor networks and internet of things (IoT). In\nthis paper, we consider a wireless powered MEC network that adopts a binary\noffloading policy, so that each computation task of wireless devices (WDs) is\neither executed locally or fully offloaded to an MEC server. Our goal is to\nacquire an online algorithm that optimally adapts task offloading decisions and\nwireless resource allocations to the time-varying wireless channel conditions.\nThis requires quickly solving hard combinatorial optimization problems within\nthe channel coherence time, which is hardly achievable with conventional\nnumerical optimization methods. To tackle this problem, we propose a Deep\nReinforcement learning-based Online Offloading (DROO) framework that implements\na deep neural network as a scalable solution that learns the binary offloading\ndecisions from the experience. It eliminates the need of solving combinatorial\noptimization problems, and thus greatly reduces the computational complexity\nespecially in large-size networks. To further reduce the complexity, we propose\nan adaptive procedure that automatically adjusts the parameters of the DROO\nalgorithm on the fly. Numerical results show that the proposed algorithm can\nachieve near-optimal performance while significantly decreasing the computation\ntime by more than an order of magnitude compared with existing optimization\nmethods. For example, the CPU execution latency of DROO is less than $0.1$\nsecond in a $30$-user network, making real-time and optimal offloading truly\nviable even in a fast fading environment. \n\n"}
{"id": "1808.02342", "contents": "Title: A Very Brief Introduction to Machine Learning With Applications to\n  Communication Systems Abstract: Given the unprecedented availability of data and computing resources, there\nis widespread renewed interest in applying data-driven machine learning methods\nto problems for which the development of conventional engineering solutions is\nchallenged by modelling or algorithmic deficiencies. This tutorial-style paper\nstarts by addressing the questions of why and when such techniques can be\nuseful. It then provides a high-level introduction to the basics of supervised\nand unsupervised learning. For both supervised and unsupervised learning,\nexemplifying applications to communication networks are discussed by\ndistinguishing tasks carried out at the edge and at the cloud segments of the\nnetwork at different layers of the protocol stack. \n\n"}
{"id": "1808.02975", "contents": "Title: Auto-Scaling Network Resources using Machine Learning to Improve QoS and\n  Reduce Cost Abstract: Virtualization of network functions (as virtual routers, virtual firewalls,\netc.) enables network owners to efficiently respond to the increasing\ndynamicity of network services. Virtual Network Functions (VNFs) are easy to\ndeploy, update, monitor, and manage. The number of VNF instances, similar to\ngeneric computing resources in cloud, can be easily scaled based on load.\nHence, auto-scaling (of resources without human intervention) has been\nreceiving attention. Prior studies on auto-scaling use measured network traffic\nload to dynamically react to traffic changes. In this study, we propose a\nproactive Machine Learning (ML) based approach to perform auto-scaling of VNFs\nin response to dynamic traffic changes. Our proposed ML classifier learns from\npast VNF scaling decisions and seasonal/spatial behavior of network traffic\nload to generate scaling decisions ahead of time. Compared to existing\napproaches for ML-based auto-scaling, our study explores how the properties\n(e.g., start-up time) of underlying virtualization technology impacts Quality\nof Service (QoS) and cost savings. We consider four different virtualization\ntechnologies: Xen and KVM, based on hypervisor virtualization, and Docker and\nLXC, based on container virtualization. Our results show promising accuracy of\nthe ML classifier using real data collected from a private ISP. We report\nin-depth analysis of the learning process (learning-curve analysis), feature\nranking (feature selection, Principal Component Analysis (PCA), etc.), impact\nof different sets of features, training time, and testing time. Our results\nshow how the proposed methods improve QoS and reduce operational cost for\nnetwork owners. We also demonstrate a practical use-case example\n(Software-Defined Wide Area Network (SD-WAN) with VNFs and backbone network) to\nshow that our ML methods save significant cost for network service leasers. \n\n"}
{"id": "1808.06254", "contents": "Title: SABRE: Protecting Bitcoin against Routing Attacks Abstract: Routing attacks remain practically effective in the Internet today as\nexisting countermeasures either fail to provide protection guarantees or are\nnot easily deployable. Blockchain systems are particularly vulnerable to such\nattacks as they rely on Internet-wide communication to reach consensus. In\nparticular, Bitcoin -the most widely-used cryptocurrency- can be split in half\nby any AS-level adversary using BGP hijacking. In this paper, we present SABRE,\na secure and scalable Bitcoin relay network which relays blocks worldwide\nthrough a set of connections that are resilient to routing attacks. SABRE runs\nalongside the existing peer-to-peer network and is easily deployable. As a\ncritical system, SABRE design is highly resilient and can efficiently handle\nhigh bandwidth loads, including Denial of Service attacks. We built SABRE\naround two key technical insights. First, we leverage fundamental properties of\ninter-domain routing (BGP) policies to host relay nodes: (i) in locations that\nare inherently protected against routing attacks; and (ii) on paths that are\neconomically preferred by the majority of Bitcoin clients. These properties are\ngeneric and can be used to protect other Blockchain-based systems. Second, we\nleverage the fact that relaying blocks is communication-heavy, not\ncomputation-heavy. This enables us to offload most of the relay operations to\nprogrammable network hardware (using the P4 programming language). Thanks to\nthis hardware/software co-design, SABRE nodes operate seamlessly under high\nload while mitigating the effects of malicious clients. We present a complete\nimplementation of SABRE together with an extensive evaluation. Our results\ndemonstrate that SABRE is effective at securing Bitcoin against routing\nattacks, even with deployments as small as 6 nodes. \n\n"}
{"id": "1808.07293", "contents": "Title: Invisible Pixels Are Dead, Long Live Invisible Pixels! Abstract: Privacy has deteriorated in the world wide web ever since the 1990s. The\ntracking of browsing habits by different third-parties has been at the center\nof this deterioration. Web cookies and so-called web beacons have been the\nclassical ways to implement third-party tracking. Due to the introduction of\nmore sophisticated technical tracking solutions and other fundamental\ntransformations, the use of classical image-based web beacons might be expected\nto have lost their appeal. According to a sample of over thirty thousand images\ncollected from popular websites, this paper shows that such an assumption is a\nfallacy: classical 1 x 1 images are still commonly used for third-party\ntracking in the contemporary world wide web. While it seems that ad-blockers\nare unable to fully block these classical image-based tracking beacons, the\npaper further demonstrates that even limited information can be used to\naccurately classify the third-party 1 x 1 images from other images. An average\nclassification accuracy of 0.956 is reached in the empirical experiment. With\nthese results the paper contributes to the ongoing attempts to better\nunderstand the lack of privacy in the world wide web, and the means by which\nthe situation might be eventually improved. \n\n"}
{"id": "1808.07356", "contents": "Title: Study on Base Station Topology in Cellular Networks: Take Advantage of\n  Alpha Shapes, Betti Numbers, and Euler Characteristics Abstract: Faced with the ever-increasing trend of the cellular network scale, how to\nquantitatively evaluate the effectiveness of the large-scale deployment of base\nstations (BSs) has become a challenging topic. To this end, a deeper\nunderstanding of the cellular network topology is of fundamental significance\nto be achieved. In this paper, $ \\alpha $-Shape, a powerful algebraic geometric\ntool, is integrated into the analysis of real BS location data for six Asian\ncountries and six European countries, respectively. Firstly, the BS spatial\ndeployments of both Asian and European countries express fractal features based\non two different testifying metrics, namely the Betti numbers and the Hurst\ncoefficients. Secondly, it is found out that the log-normal distribution\npresents the best match to the cellular network topology when the practical BS\ndeployment is characterized by the Euler characteristics. \n\n"}
{"id": "1808.10217", "contents": "Title: Competitive Data Trading in Wireless-Powered Internet of Things (IoT)\n  Crowdsensing Systems with Blockchain Abstract: With the explosive growth of smart IoT devices at the edge of the Internet,\nembedding sensors on mobile devices for massive data collection and collective\nenvironment sensing has been envisioned as a cost-effective solution for IoT\napplications. However, existing IoT platforms and framework rely on dedicated\nmiddleware for (semi-) centralized task dispatching, data storage and incentive\nprovision. Consequently, they are usually expensive to deploy, have limited\nadaptability to diverse requirements, and face a series of data security and\nprivacy issues. In this paper, we employ permissionless blockchains to\nconstruct a purely decentralized platform for data storage and trading in a\nwireless-powered IoT crowdsensing system. In the system, IoT sensors use power\nwirelessly transferred from RF-energy beacons for data sensing and transmission\nto an access point. The data is then forwarded to the blockchain for\ndistributed ledger services, i.e., data/transaction verification, recording,\nand maintenance. Due to coupled interference of wireless transmission and\ntransaction fee incurred from blockchain's distributed ledger services,\nrational sensors have to decide on their transmission rates to maximize\nindividual utility. Thus, we formulate a noncooperative game model to analyze\nthis competitive situation among the sensors. We provide the analytical\ncondition for the existence of the Nash equilibrium as well as a series of\ninsightful numerical results about the equilibrium strategies in the game. \n\n"}
{"id": "1809.02826", "contents": "Title: Delay-Constrained Input-Queued Switch Abstract: In this paper, we study the delay-constrained input-queued switch where each\npacket has a deadline and it will expire if it is not delivered before its\ndeadline. Such new scenario is motivated by the proliferation of real-time\napplications in multimedia communication systems, tactile Internet, networked\ncontrolled systems, and cyber-physical systems. The delay-constrained\ninput-queued switch is completely different from the well-understood\ndelay-unconstrained one and thus poses new challenges. We focus on three\nfundamental problems centering around the performance metric of timely\nthroughput: (i) how to characterize the capacity region? (ii) how to design a\nfeasibility/throughput-optimal scheduling policy? and (iii) how to design a\nnetwork-utility-maximization scheduling policy? We use three different\napproaches to solve these three fundamental problems. The first approach is\nbased on Markov Decision Process (MDP) theory, which can solve all three\nproblems. However, it suffers from the curse of dimensionality. The second\napproach breaks the curse of dimensionality by exploiting the combinatorial\nfeatures of the problem. It gives a new capacity region characterization with\nonly a polynomial number of linear constraints. The third approach is based on\nthe framework of Lyapunov optimization, where we design a polynomial-time\nmaximum-weight T-disjoint-matching scheduling policy which is proved to be\nfeasibility/throughput-optimal. Our three approaches apply to the\nframe-synchronized traffic pattern but our MDP-based approach can be extended\nto more general traffic patterns. \n\n"}
{"id": "1809.03222", "contents": "Title: Colombian export capabilities: building the firms-products network Abstract: In this paper we analyse the bipartite Colombian firms-products network,\nthroughout a period of five years, from 2010 to 2014. Our analysis depicts a\nstrongly modular system, with several groups of firms specializing in the\nexport of specific categories of products. These clusters have been detected by\nrunning the bipartite variant of the traditional modularity maximization,\nrevealing a bi-modular structure. Interestingly, this finding is refined by\napplying a recently-proposed algorithm for projecting bipartite networks on the\nlayer of interest and, then, running the Louvain algorithm on the resulting\nmonopartite representations. Important structural differences emerge upon\ncomparing the Colombian firms-products network with the World Trade Web, in\nparticular, the bipartite representation of the latter is not characterized by\na similar block-structure, as the modularity maximization fails in revealing\n(bipartite) nodes clusters. This points out that economic systems behave\ndifferently at different scales: while countries tend to diversify their\nproduction --potentially exporting a large number of different products-- firms\nspecialize in exporting (substantially very limited) baskets of basically\nhomogeneous products. \n\n"}
{"id": "1809.03953", "contents": "Title: 5G Massive MIMO Architectures: Self-Backhauled Small Cells versus Direct\n  Access Abstract: In this paper, we focus on one of the key technologies for the\nfifth-generation wireless communication networks, massive\nmultiple-input-multiple-output (mMIMO), by investigating two of its most\nrelevant architectures: 1) to provide in-band backhaul for the ultra-dense\nnetwork (UDN) of self-backhauled small cells (SCs), and 2) to provide direct\naccess (DA) to user equipments (UEs). Through comprehensive 3GPP-based\nsystem-level simulations and analytical formulations, we show the end-to-end UE\nrates achievable with these two architectures. Differently from the existing\nworks, we provide results for two strategies of self-backhauled SC deployments,\nnamely random and ad-hoc, where in the latter SCs are purposely positioned\nclose to UEs to achieve line-of-sight (LoS) access links. We also evaluate the\noptimal backhaul and access time resource partition due to the in-band\nself-backhauling (s-BH) operations. Our results show that the ad-hoc deployment\nof self-backhauled SCs closer to the UEs with optimal resource partition and\nwith directive antenna patterns, provides rate improvements for cell-edge UEs\nthat amount to 30% and tenfold gain, as compared to mMIMO DA architecture with\npilot reuse 3 and reuse 1, respectively. On the other hand, mMIMO s-BH\nunderperforms mMIMO DA above the median value of the UE rates when the effect\nof pilot contamination is less severe, and the LoS probability of the DA links\nimproves. \n\n"}
{"id": "1809.05088", "contents": "Title: High Throughput Cryptocurrency Routing in Payment Channel Networks Abstract: Despite growing adoption of cryptocurrencies, making fast payments at scale\nremains a challenge. Payment channel networks (PCNs) such as the Lightning\nNetwork have emerged as a viable scaling solution. However, completing payments\non PCNs is challenging: payments must be routed on paths with sufficient funds.\nAs payments flow over a single channel (link) in the same direction, the\nchannel eventually becomes depleted and cannot support further payments in that\ndirection; hence, naive routing schemes like shortest-path routing can deplete\nkey payment channels and paralyze the system. Today's PCNs also route payments\natomically, worsening the problem. In this paper, we present Spider, a routing\nsolution that \"packetizes\" transactions and uses a multi-path transport\nprotocol to achieve high-throughput routing in PCNs. Packetization allows\nSpider to complete even large transactions on low-capacity payment channels\nover time, while the multi-path congestion control protocol ensures balanced\nutilization of channels and fairness across flows. Extensive simulations\ncomparing Spider with state-of-the-art approaches shows that Spider requires\nless than 25% of the funds to successfully route over 95% of transactions on\nbalanced traffic demands, and offloads 4x more transactions onto the PCN on\nimbalanced demands. \n\n"}
{"id": "1809.05628", "contents": "Title: On the Integrity of Cross-Origin JavaScripts Abstract: The same-origin policy is a fundamental part of the Web. Despite the\nrestrictions imposed by the policy, embedding of third-party JavaScript code is\nallowed and commonly used. Nothing is guaranteed about the integrity of such\ncode. To tackle this deficiency, solutions such as the subresource integrity\nstandard have been recently introduced. Given this background, this paper\npresents the first empirical study on the temporal integrity of cross-origin\nJavaScript code. According to the empirical results based on a ten day polling\nperiod of over 35 thousand scripts collected from popular websites, (i)\ntemporal integrity changes are relatively common; (ii) the adoption of the\nsubresource integrity standard is still in its infancy; and (iii) it is\npossible to statistically predict whether a temporal integrity change is likely\nto occur. With these results and the accompanying discussion, the paper\ncontributes to the ongoing attempts to better understand security and privacy\nin the current Web. \n\n"}
{"id": "1809.06970", "contents": "Title: FastDeepIoT: Towards Understanding and Optimizing Neural Network\n  Execution Time on Mobile and Embedded Devices Abstract: Deep neural networks show great potential as solutions to many sensing\napplication problems, but their excessive resource demand slows down execution\ntime, pausing a serious impediment to deployment on low-end devices. To address\nthis challenge, recent literature focused on compressing neural network size to\nimprove performance. We show that changing neural network size does not\nproportionally affect performance attributes of interest, such as execution\ntime. Rather, extreme run-time nonlinearities exist over the network\nconfiguration space. Hence, we propose a novel framework, called FastDeepIoT,\nthat uncovers the non-linear relation between neural network structure and\nexecution time, then exploits that understanding to find network configurations\nthat significantly improve the trade-off between execution time and accuracy on\nmobile and embedded devices. FastDeepIoT makes two key contributions. First,\nFastDeepIoT automatically learns an accurate and highly interpretable execution\ntime model for deep neural networks on the target device. This is done without\nprior knowledge of either the hardware specifications or the detailed\nimplementation of the used deep learning library. Second, FastDeepIoT informs a\ncompression algorithm how to minimize execution time on the profiled device\nwithout impacting accuracy. We evaluate FastDeepIoT using three different\nsensing-related tasks on two mobile devices: Nexus 5 and Galaxy Nexus.\nFastDeepIoT further reduces the neural network execution time by $48\\%$ to\n$78\\%$ and energy consumption by $37\\%$ to $69\\%$ compared with the\nstate-of-the-art compression algorithms. \n\n"}
{"id": "1809.07407", "contents": "Title: Unfolding the complexity of the global value chain: Strengths and\n  entropy in the single-layer, multiplex, and multi-layer international trade\n  networks Abstract: The worldwide trade network has been widely studied through different data\nsets and network representations with a view to better understanding\ninteractions among countries and products. Here we investigate international\ntrade through the lenses of the single-layer, multiplex, and multi-layer\nnetworks. We discuss differences among the three network frameworks in terms of\ntheir relative advantages in capturing salient topological features of trade.\nWe draw on the World Input-Output Database to build the three networks. We then\nuncover sources of heterogeneity in the way strength is allocated among\ncountries and transactions by computing the strength distribution and entropy\nin each network. Additionally, we trace how entropy evolved, and show how the\nobserved peaks can be associated with the onset of the global economic\ndownturn. Findings suggest how more complex representations of trade, such as\nthe multi-layer network, enable us to disambiguate the distinct roles of intra-\nand cross-industry transactions in driving the evolution of entropy at a more\naggregate level. We discuss our results and the implications of our comparative\nanalysis of networks for research on international trade and other empirical\ndomains across the natural and social sciences. \n\n"}
{"id": "1809.07681", "contents": "Title: Fundamentals on Base Stations in Cellular Networks: From the Perspective\n  of Algebraic Topology Abstract: In recent decades, the deployments of cellular networks have been going\nthrough an unprecedented expansion. In this regard, it is beneficial to acquire\nprofound knowledge of cellular networks from the view of topology so that\nprominent network performances can be achieved by means of appropriate\nplacements of base stations (BSs). In our researches, practical location data\nof BSs in eight representative cities are processed with classical algebraic\ngeometric instruments, including $ \\alpha $-Shapes, Betti numbers, and Euler\ncharacteristics. At first, the fractal nature is revealed in the BS topology\nfrom both perspectives of the Betti numbers and the Hurst coefficients.\nFurthermore, log-normal distribution is affirmed to provide the optimal fitness\nto the Euler characteristics of real BS deployments. \n\n"}
{"id": "1809.07857", "contents": "Title: In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and\n  Communication by Federated Learning Abstract: Recently, along with the rapid development of mobile communication\ntechnology, edge computing theory and techniques have been attracting more and\nmore attentions from global researchers and engineers, which can significantly\nbridge the capacity of cloud and requirement of devices by the network edges,\nand thus can accelerate the content deliveries and improve the quality of\nmobile services. In order to bring more intelligence to the edge systems,\ncompared to traditional optimization methodology, and driven by the current\ndeep learning techniques, we propose to integrate the Deep Reinforcement\nLearning techniques and Federated Learning framework with the mobile edge\nsystems, for optimizing the mobile edge computing, caching and communication.\nAnd thus, we design the \"In-Edge AI\" framework in order to intelligently\nutilize the collaboration among devices and edge nodes to exchange the learning\nparameters for a better training and inference of the models, and thus to carry\nout dynamic system-level optimization and application-level enhancement while\nreducing the unnecessary system communication load. \"In-Edge AI\" is evaluated\nand proved to have near-optimal performance but relatively low overhead of\nlearning, while the system is cognitive and adaptive to the mobile\ncommunication systems. Finally, we discuss several related challenges and\nopportunities for unveiling a promising upcoming future of \"In-Edge AI\". \n\n"}
{"id": "1809.07859", "contents": "Title: Post-disaster 4G/5G Network Rehabilitation using Drones: Solving Battery\n  and Backhaul Issues Abstract: Drone-based communications is a novel and attractive area of research in\ncellular networks. It provides several degrees of freedom in time (available on\ndemand), space (mobile) and it can be used for multiple purposes (self-healing,\noffloading, coverage extension or disaster recovery). This is why the wide\ndeployment of drone-based communications has the potential to be integrated in\nthe 5G standard. In this paper, we utilize a grid of drones to provide cellular\ncoverage to disaster-struck regions where the terrestrial infrastructure is\ntotally damaged due to earthquake, flood, etc. We propose solutions for the\nmost challenging issues facing drone networks which are limited battery energy\nand limited backhauling. Our proposed solution based mainly on using three\ntypes of drones; tethered backhaul drone (provides high capacity backhauling),\nuntethered powering drone (provides on the fly battery charging) and untethered\ncommunication drone (provides cellular connectivity). Hence, an optimization\nproblem is formulated to minimize the energy consumption of drones in addition\nto determining the placement of these drones and guaranteeing a minimum rate\nfor the users. The simulation results show that we can provide unlimited\ncellular service to the disaster-affected region under certain conditions with\na guaranteed minimum rate for each user. \n\n"}
{"id": "1809.07864", "contents": "Title: Enabling Ultra-Low Delay Teleorchestras using Software Defined\n  Networking Abstract: Ultra-low delay sensitive applications can afford delay only at the level of\nmsec. An example of this application class are the Networked Music Performance\n(NMP) systems that describe a live music performance by geographically separate\nmusicians over the Internet. The present work proposes a novel architecture for\nNMP systems, where the key-innovation is the close collaboration between the\nnetwork and the application. Using SDN principles, the applications are enabled\nto adapt their internal audio signal processing, in order to cope with network\ndelay increase. Thus, affordable end-to-end delay is provided to NMP users,\neven under considerable network congestion. \n\n"}
{"id": "1809.08325", "contents": "Title: The Rise of Certificate Transparency and Its Implications on the\n  Internet Ecosystem Abstract: In this paper, we analyze the evolution of Certificate Transparency (CT) over\ntime and explore the implications of exposing certificate DNS names from the\nperspective of security and privacy. We find that certificates in CT logs have\nseen exponential growth. Website support for CT has also constantly increased,\nwith now 33% of established connections supporting CT. With the increasing\ndeployment of CT, there are also concerns of information leakage due to all\ncertificates being visible in CT logs. To understand this threat, we introduce\na CT honeypot and show that data from CT logs is being used to identify targets\nfor scanning campaigns only minutes after certificate issuance. We present and\nevaluate a methodology to learn and validate new subdomains from the vast\nnumber of domains extracted from CT logged certificates. \n\n"}
{"id": "1809.08514", "contents": "Title: Fundamental Limits of Invisible Flow Fingerprinting Abstract: Network flow fingerprinting can be used to de-anonymize communications on\nanonymity systems such as Tor by linking the ingress and egress segments of\nanonymized connections. Assume Alice and Bob have access to the input and the\noutput links of an anonymous network, respectively, and they wish to\ncollaboratively reveal the connections between the input and the output links\nwithout being detected by Willie who protects the network. Alice generates a\ncodebook of fingerprints, where each fingerprint corresponds to a unique\nsequence of inter-packet delays and shares it only with Bob. For each input\nflow, she selects a fingerprint from the codebook and embeds it in the flow,\ni.e., changes the packet timings of the flow to follow the packet timings\nsuggested by the fingerprint, and Bob extracts the fingerprints from the output\nflows. We model the network as parallel $M/M/1$ queues where each queue is\nshared by a flow from Alice to Bob and other flows independent of the flow from\nAlice to Bob. The timings of the flows are governed by independent Poisson\npoint processes. Assuming all input flows have equal rates and that Bob\nobserves only flows with fingerprints, we first present two scenarios: 1) Alice\nfingerprints all the flows; 2) Alice fingerprints a subset of the flows,\nunknown to Willie. Then, we extend the construction and analysis to the case\nwhere flow rates are arbitrary as well as the case where not all the flows that\nBob observes have a fingerprint. For each scenario, we derive the number of\nflows that Alice can fingerprint and Bob can trace by fingerprinting. \n\n"}
{"id": "1809.08536", "contents": "Title: From Megabits to CPU~Ticks: Enriching a Demand Trace in the Age of MEC Abstract: All the content consumed by mobile users, be it a web page or a live stream,\nundergoes some processing along the way; as an example, web pages and videos\nare transcoded to fit each device's screen. The recent multi-access edge\ncomputing (MEC) paradigm envisions performing such processing within the\ncellular network, as opposed to resorting to a cloud server on the Internet.\nDesigning a MEC network, i.e., placing and dimensioning the computational\nfacilities therein, requires information on how much computational power is\nrequired to produce the contents needed by the users. However, real-world\ndemand traces only contain information on how much data is downloaded. In this\npaper, we demonstrate how to {\\em enrich} demand traces with information about\nthe computational power needed to process the different types of content, and\nwe show the substantial benefit that can be obtained from using such enriched\ntraces for the design of MEC-based networks. \n\n"}
{"id": "1809.08748", "contents": "Title: A Survey of Conventional and Artificial Intelligence / Learning based\n  Resource Allocation and Interference Mitigation Schemes in D2D Enabled\n  Networks Abstract: 5th generation networks are envisioned to provide seamless and ubiquitous\nconnection to 1000-fold more devices and is believed to provide ultra-low\nlatency and higher data rates up to tens of Gbps. Different technologies\nenabling these requirements are being developed including mmWave\ncommunications, Massive MIMO and beamforming, Device to Device (D2D)\ncommunications and Heterogeneous Networks. D2D communication is a promising\ntechnology to enable applications requiring high bandwidth such as online\nstreaming and online gaming etc. It can also provide ultra- low latencies\nrequired for applications like vehicle to vehicle communication for autonomous\ndriving. D2D communication can provide higher data rates with high energy\nefficiency and spectral efficiency compared to conventional communication. The\nperformance benefits of D2D communication can be best achieved when D2D users\nreuses the spectrum being utilized by the conventional cellular users. This\nspectrum sharing in a multi-tier heterogeneous network will introduce complex\ninterference among D2D users and cellular users which needs to be resolved.\nMotivated by limited number of surveys for interference mitigation and resource\nallocation in D2D enabled heterogeneous networks, we have surveyed different\nconventional and artificial intelligence based interference mitigation and\nresource allocation schemes developed in recent years. Our contribution lies in\nthe analysis of conventional interference mitigation techniques and their\nshortcomings. Finally, the strengths of AI based techniques are determined and\nopen research challenges deduced from the recent research are presented. \n\n"}
{"id": "1809.09470", "contents": "Title: SS5G: Collision Resolution Protocol for Delay and Energy Efficient LoRa\n  Networks Abstract: Future 5G and Internet of Things (IoT) applications will heavily rely on\nlong-range communication technologies such as low-power wireless area networks\n(LPWANs). In particular, LoRaWAN built on LoRa physical layer is gathering\nincreasing interests, both from academia and industries, for enabling low-cost\nenergy efficient IoT wireless sensor networks for, e.g., environmental\nmonitoring over wide areas. While its communication range may go up to 20\nkilometers, the achievable bit rates in LoRaWAN are limited to a few kilobits\nper second. In the event of collisions, the perceived rate is further reduced\ndue to packet loss and retransmissions. Firstly, to alleviate the harmful\nimpacts of collisions, we propose a decoding algorithm that enables to resolve\nseveral superposed LoRa signals. Our proposed method exploits the slight\ndesynchronization of superposed signals and specific features of LoRa physical\nlayer. Secondly, we design a full MAC protocol enabling collision resolution.\nThe simulation results demonstrate that the proposed method outperforms\nconventional LoRaWAN jointly in terms of system throughput, energy efficiency\nas well as delay. These results show that our scheme is well suited for 5G and\nIoT systems, as one of their major goals is to provide the best trade-off among\nthese performance objectives. \n\n"}
{"id": "1809.10781", "contents": "Title: Methods and Concepts in Economic Complexity Abstract: Knowhow in societies accumulates as it gets transmitted from group to group,\nand from generation to generation. However, we lack of a unified quantitative\nformalism that takes into account the structured process for how this\naccumulation occurs, and this has precluded the development of a unified view\nof human development in the past and in the present. Here, we summarize a\nparadigm to understand and model this process. The paradigm goes under the\ngeneral name of the Theory of Economic Complexity (TEC). Based on it, we\npresent a combination of analytical, numerical and empirical results that\nillustrate how to characterize the process of development, providing measurable\nquantities that can be used to predict future developments. The emphasis is the\nquantification of the collective knowhow an economy has accumulated, and what\nare the directions in which it is likely to expand. As a case study we consider\ndata on trade, which provides consistent data on the technological\ndiversification of 200 countries across more than 50 years. The paradigm\nrepresented by TEC should be relevant for anthropologists, sociologists, and\neconomists interested in the role of collective knowhow as the main determinant\nof the success and welfare of a society. \n\n"}
{"id": "1810.00104", "contents": "Title: Temporal Cliques Admit Sparse Spanners Abstract: Let $G=(V,E)$ be an undirected graph on $n$ vertices and $\\lambda:E\\to\n2^{\\mathbb{N}}$ a mapping that assigns to every edge a non-empty set of integer\nlabels (times). Such a graph is {\\em temporally connected} if a path exists\nwith non-decreasing times from every vertex to every other vertex. In a seminal\npaper, Kempe, Kleinberg, and Kumar \\cite{KKK02} asked whether, given such a\ntemporal graph, a {\\em sparse} subset of edges always exists whose labels\nsuffice to preserve temporal connectivity -- a {\\em temporal spanner}. Axiotis\nand Fotakis \\cite{AF16} answered negatively by exhibiting a family of\n$\\Theta(n^2)$-dense temporal graphs which admit no temporal spanner of density\n$o(n^2)$. In this paper, we give the first positive answer as to the existence\nof $o(n^2)$-sparse spanners in a dense class of temporal graphs, by showing\n(constructively) that if $G$ is a complete graph, then one can always find a\ntemporal spanner of density $O(n \\log n)$. \n\n"}
{"id": "1810.00349", "contents": "Title: IDMoB: IoT Data Marketplace on Blockchain Abstract: Today, Internet of Things (IoT) devices are the powerhouse of data generation\nwith their ever-increasing numbers and widespread penetration. Similarly,\nartificial intelligence (AI) and machine learning (ML) solutions are getting\nintegrated to all kinds of services, making products significantly more\n\"smarter\". The centerpiece of these technologies is \"data\". IoT device vendors\nshould be able keep up with the increased throughput and come up with new\nbusiness models. On the other hand, AI/ML solutions will produce better results\nif training data is diverse and plentiful.\n  In this paper, we propose a blockchain-based, decentralized and trustless\ndata marketplace where IoT device vendors and AI/ML solution providers may\ninteract and collaborate. By facilitating a transparent data exchange platform,\naccess to consented data will be democratized and the variety of services\ntargeting end-users will increase. Proposed data marketplace is implemented as\na smart contract on Ethereum blockchain and Swarm is used as the distributed\nstorage platform. \n\n"}
{"id": "1810.00356", "contents": "Title: DELMU: A Deep Learning Approach to Maximising the Utility of Virtualised\n  Millimetre-Wave Backhauls Abstract: Advances in network programmability enable operators to 'slice' the physical\ninfrastructure into independent logical networks. By this approach, each\nnetwork slice aims to accommodate the demands of increasingly diverse services.\nHowever, precise allocation of resources to slices across future 5G\nmillimetre-wave backhaul networks, to optimise the total network utility, is\nchallenging. This is because the performance of different services often\ndepends on conflicting requirements, including bandwidth, sensitivity to delay,\nor the monetary value of the traffic incurred. In this paper, we put forward a\ngeneral rate utility framework for slicing mm-wave backhaul links, encompassing\nall known types of service utilities, i.e. logarithmic, sigmoid, polynomial,\nand linear. We then introduce DELMU, a deep learning solution that tackles the\ncomplexity of optimising non-convex objective functions built upon arbitrary\ncombinations of such utilities. Specifically, by employing a stack of\nconvolutional blocks, DELMU can learn correlations between traffic demands and\nachievable optimal rate assignments. We further regulate the inferences made by\nthe neural network through a simple 'sanity check' routine, which guarantees\nboth flow rate admissibility within the network's capacity region and minimum\nservice levels. The proposed method can be trained within minutes, following\nwhich it computes rate allocations that match those obtained with\nstate-of-the-art global optimisation algorithms, yet orders of magnitude\nfaster. This confirms the applicability of DELMU to highly dynamic traffic\nregimes and we demonstrate up to 62% network utility gains over a baseline\ngreedy approach. \n\n"}
{"id": "1810.00847", "contents": "Title: Exploring the Performance Boundaries of NB-IoT Abstract: NarrowBand-IoT has just joined the LPWAN community. Unlike most of its\ncompetitors, NB-IoT did not emerge from a blank slate. Indeed, it is closely\nlinked to LTE, from which it inherits many of the features that undoubtedly\ndetermine its behavior. In this paper, we empirically explore the boundaries of\nthis technology, analyzing from a user's point of view critical characteristics\nsuch as energy consumption, reliability and delays. The results show that its\nperformance in terms of energy is comparable and even outperforms, in some\ncases, an LPWAN reference technology like LoRa, with the added benefit of\nguaranteeing delivery. However, the high variability observed in both energy\nexpenditure and network delays call into question its suitability for some\napplications, especially those subject to service-level agreements. \n\n"}
{"id": "1810.01310", "contents": "Title: The logic of uncertainty as a logic of experience and chance and the\n  co~event-based Bayes' theorem Abstract: The logic of uncertainty is not the logic of experience and as well as it is\nnot the logic of chance. It is the logic of experience and chance. Experience\nand chance are two inseparable poles. These are two dual reflections of one\nessence, which is called co~event. The theory of experience and chance is the\ntheory of co~events. To study the co~events, it is not enough to study the\nexperience and to study the chance. For this, it is necessary to study the\nexperience and chance as a single entire, a co~event. In other words, it is\nnecessary to study their interaction within a co~event. The new co~event\naxiomatics and the theory of co~events following from it were created precisely\nfor these purposes. In this work, I am going to demonstrate the effectiveness\nof the new theory of co~events in a studying the logic of uncertainty. I will\ndo this by the example of a co~event splitting of the logic of the Bayesian\nscheme, which has a long history of fierce debates between Bayesianists and\nfrequentists. I hope the logic of the theory of experience and chance will make\nits modest contribution to the application of these old dual debaters. \n\n"}
{"id": "1810.01534", "contents": "Title: Band Assignment in Dual Band Systems: A Learning-based Approach Abstract: We consider the band assignment problem in dual band systems, where the\nbase-station (BS) chooses one of the two available frequency bands\n(centimeter-wave and millimeter-wave bands) to communicate data to the mobile\nstation (MS). While the millimeter-wave band offers higher data rate when it is\navailable, there is a significant probability of outage during which the\ncommunication should be carried on the centimeter-wave band.\n  In this work, we use a machine learning framework to provide an efficient and\npractical solution to the band assignment problem. In particular, the BS trains\na Neural Network (NN) to predict the right band assignment decision using\nobserved channel information. We study the performance of the NN in two\nenvironments: (i) A stochastic channel model with correlated bands, and (ii)\nmicrocellular outdoor channels obtained by simulations with a commercial\nray-tracer. For the former case, for sake of comparison we also develop a\nthreshold based band assignment that relies on the optimal mean square error\nestimator of the best band. In addition, we study the performance of the\nNN-based solution with different NN structures and different observed\nparameters (position, field strength, etc.). We compare the achieved\nperformance to linear and logistic regression based solutions as well as the\nthreshold based solution. Under practical constraints, the learning based band\nassignment shows competitive or superior performance in both environments. \n\n"}
{"id": "1810.01548", "contents": "Title: Deep Learning Based Caching for Self-Driving Car in Multi-access Edge\n  Computing Abstract: Once self-driving car becomes a reality and passengers are no longer worry\nabout it, they will need to find new ways of entertainment. However, retrieving\nentertainment contents at the Data Center (DC) can hinder content delivery\nservice due to high delay of car-to-DC communication. To address these\nchallenges, we propose a deep learning based caching for self-driving car, by\nusing Deep Learning approaches deployed on the Multi-access Edge Computing\n(MEC) structure. First, at DC, Multi-Layer Perceptron (MLP) is used to predict\nthe probabilities of contents to be requested in specific areas. To reduce the\ncar-DC delay, MLP outputs are logged into MEC servers attached to roadside\nunits. Second, in order to cache entertainment contents stylized for car\npassengers' features such as age and gender, Convolutional Neural Network (CNN)\nis used to predict age and gender of passengers. Third, each car requests MLP\noutput from MEC server and compares its CNN and MLP outputs by using k-means\nand binary classification. Through this, the self-driving car can identify the\ncontents need to be downloaded from the MEC server and cached. Finally, we\nformulate deep learning based caching in the self-driving car that enhances\nentertainment services as an optimization problem whose goal is to minimize\ncontent downloading delay. To solve the formulated problem, a Block Successive\nMajorization-Minimization (BS-MM) technique is applied. The simulation results\nshow that the accuracy of our prediction for the contents need to be cached in\nthe areas of the self-driving car is achieved at 98.04% and our approach can\nminimize delay. \n\n"}
{"id": "1810.02276", "contents": "Title: Performance Analysis of NOMA for Ultra-Reliable and Low-Latency\n  Communications Abstract: Grant-free non-orthogonal multiple access (NOMA) has been regarded as a\nkey-enabler technology for ultra-reliable and low-latency communications\n(URLLC). In this paper, we analyse the performance of NOMA with short packet\ncommunications for URLLC. In this regard, the overall packet loss probability\nconsists of transmission error probability and queueing-delay violation\nprobability. Queueing-delay has been modelled using the effective bandwidth.\nDue to short transmission time, the infinite block-length has been replaced\nwith finite blocklength of the channel codes which rules out the application of\nShannon's formula. The achievable effective bandwidth of the system is derived,\nand then, the transmission error probability has been analysed. The derivations\nare validated through extensive simulations, which shows the variations of the\nsignal-to-noise ratio (SNR) requirements of the system for various\ntransmission-error probability, QoS exponent, and the transmission packet size. \n\n"}
{"id": "1810.02713", "contents": "Title: Optimizing groups of colluding strong attackers in mobile urban\n  communication networks with evolutionary algorithms Abstract: In novel forms of the Social Internet of Things, any mobile user within\ncommunication range may help routing messages for another user in the network.\nThe resulting message delivery rate depends both on the users' mobility\npatterns and the message load in the network. This new type of configuration,\nhowever, poses new challenges to security, amongst them, assessing the effect\nthat a group of colluding malicious participants can have on the global message\ndelivery rate in such a network is far from trivial. In this work, after\nmodeling such a question as an optimization problem, we are able to find quite\ninteresting results by coupling a network simulator with an evolutionary\nalgorithm. The chosen algorithm is specifically designed to solve problems\nwhose solutions can be decomposed into parts sharing the same structure. We\ndemonstrate the effectiveness of the proposed approach on two medium-sized\nDelay-Tolerant Networks, realistically simulated in the urban contexts of two\ncities with very different route topology: Venice and San Francisco. In all\nexperiments, our methodology produces attack patterns that greatly lower\nnetwork performance with respect to previous studies on the subject, as the\nevolutionary core is able to exploit the specific weaknesses of each target\nconfiguration. \n\n"}
{"id": "1810.03060", "contents": "Title: Eiffel: Efficient and Flexible Software Packet Scheduling Abstract: Packet scheduling determines the ordering of packets in a queuing data\nstructure with respect to some ranking function that is mandated by a\nscheduling policy. It is the core component in many recent innovations to\noptimize network performance and utilization. Our focus in this paper is on the\ndesign and deployment of packet scheduling in software. Software schedulers\nhave several advantages over hardware including shorter development cycle and\nflexibility in functionality and deployment location. We substantially improve\ncurrent software packet scheduling performance, while maintaining flexibility,\nby exploiting underlying features of packet ranking; namely, packet ranks are\nintegers and, at any point in time, fall within a limited range of values. We\nintroduce Eiffel, a novel programmable packet scheduling system. At the core of\nEiffel is an integer priority queue based on the Find First Set (FFS)\ninstruction and designed to support a wide range of policies and ranking\nfunctions efficiently. As an even more efficient alternative, we also propose a\nnew approximate priority queue that can outperform FFS-based queues for some\nscenarios. To support flexibility, Eiffel introduces novel programming\nabstractions to express scheduling policies that cannot be captured by current,\nstate-of-the-art scheduler programming models. We evaluate Eiffel in a variety\nof settings and in both kernel and userspace deployments. We show that it\noutperforms state of the art systems by 3-40x in terms of either number of\ncores utilized for network processing or number of flows given fixed processing\ncapacity. \n\n"}
{"id": "1810.03069", "contents": "Title: Spatio-temporal Edge Service Placement: A Bandit Learning Approach Abstract: Shared edge computing platforms deployed at the radio access network are\nexpected to significantly improve quality of service delivered by Application\nService Providers (ASPs) in a flexible and economic way. However, placing edge\nservice in every possible edge site by an ASP is practically infeasible due to\nthe ASP's prohibitive budget requirement. In this paper, we investigate the\nedge service placement problem of an ASP under a limited budget, where the ASP\ndynamically rents computing/storage resources in edge sites to host its\napplications in close proximity to end users. Since the benefit of placing edge\nservice in a specific site is usually unknown to the ASP a priori, optimal\nplacement decisions must be made while learning this benefit. We pose this\nproblem as a novel combinatorial contextual bandit learning problem. It is\n\"combinatorial\" because only a limited number of edge sites can be rented to\nprovide the edge service given the ASP's budget. It is \"contextual\" because we\nutilize user context information to enable finer-grained learning and decision\nmaking. To solve this problem and optimize the edge computing performance, we\npropose SEEN, a Spatial-temporal Edge sErvice placemeNt algorithm. Furthermore,\nSEEN is extended to scenarios with overlapping service coverage by\nincorporating a disjunctively constrained knapsack problem. In both cases, we\nprove that our algorithm achieves a sublinear regret bound when it is compared\nto an oracle algorithm that knows the exact benefit information. Simulations\nare carried out on a real-world dataset, whose results show that SEEN\nsignificantly outperforms benchmark solutions. \n\n"}
{"id": "1810.03298", "contents": "Title: Multi-Stream Opportunistic Network Decoupling: Relay Selection and\n  Interference Management Abstract: We study multi-stream transmission in the $K \\times N \\times K$ channel with\ninterfering relay nodes, consisting of $K$ multi-antenna source--destination\n(S--D) pairs and $N$ single-antenna half-duplex relay nodes between the S--D\npairs. We propose a new achievable scheme operating with partial effective\nchannel gain, termed multi-stream opportunistic network decoupling (MS-OND),\nwhich achieves the optimal degrees of freedom (DoF) under a certain relay\nscaling law. Our protocol is built upon the conventional OND that leads to\nvirtual full-duplex mode with one data stream transmission per S--D pair,\ngeneralizing the idea of OND to multi-stream scenarios by leveraging relay\nselection and interference management. Specifically, two subsets of relay nodes\nare opportunistically selected using alternate relaying in terms of producing\nor receiving the minimum total interference level. For interference management,\neach source node sends $S \\,(1 \\le S \\le M)$ data streams to selected relay\nnodes with random beamforming for the first hop, while each destination node\nreceives its desired $S$ streams from the selected relay nodes via\nopportunistic interference alignment for the second hop, where $M$ is the\nnumber of antennas at each source or destination node. Our analytical results\nare validated by numerical evaluation. \n\n"}
{"id": "1810.03355", "contents": "Title: Distributed Function Chaining with Anycast Routing Abstract: Current networks more and more rely on virtualized middleboxes to flexibly\nprovide security, protocol optimization, and policy compliance functionalities.\nAs such, delivering these services requires that the traffic be steered through\nthe desired sequence of virtual appliances. Current solutions introduce a new\nlogically centralized entity, often called orchestrator, needing to build its\nown holistic view of the whole network so to decide where to direct the\ntraffic. We advocate that such a centralized orchestration is not necessary and\nthat, on the contrary, the same objectives can be achieved by augmenting the\nnetwork layer routing so to include the notion of service and its chaining. In\nthis paper, we support our claim by designing such a system. We also present an\nimplementation and an early evaluation, showing that we can easily steer\ntraffic through available resources. This approach also offers promising\nfeatures such as incremental deployability, multi-domain service chaining,\nfailure resiliency, and easy maintenance. \n\n"}
{"id": "1810.03476", "contents": "Title: On the Benefits of Network-Level Cooperation in Millimeter-Wave\n  Communications Abstract: Relaying techniques for millimeter-wave wireless networks represent a\npowerful solution for improving the transmission performance. In this work, we\nquantify the benefits in terms of delay and throughput for a random-access\nmulti-user millimeter-wave wireless network, assisted by a full-duplex network\ncooperative relay. The relay is equipped with a queue for which we analyze the\nperformance characteristics (e.g., arrival rate, service rate, average size,\nand stability condition). Moreover, we study two possible transmission schemes:\nfully directional and broadcast. In the former, the source nodes transmit a\npacket either to the relay or to the destination by using narrow beams,\nwhereas, in the latter, the nodes transmit to both the destination and the\nrelay in the same timeslot by using a wider beam, but with lower beamforming\ngain. In our analysis, we also take into account the beam alignment phase that\noccurs every time a transmitter node changes the destination node. We show how\nthe beam alignment duration, as well as position and number of transmitting\nnodes, significantly affect the network performance. Moreover, we illustrate\nthe optimal transmission scheme (i.e., broadcast or fully directional) for\nseveral system parameters and show that a fully directional transmission is not\nalways beneficial, but, in some scenarios, broadcasting and relaying can\nimprove the performance in terms of throughput and delay. \n\n"}
{"id": "1810.03510", "contents": "Title: Fundamental Limits of Covert Bit Insertion in Packets Abstract: Covert communication is necessary when revealing the mere existence of a\nmessage leaks sensitive information to an attacker. Consider a network link\nwhere an authorized transmitter Jack sends packets to an authorized receiver\nSteve, and the packets visit Alice, Willie, and Bob, respectively, before they\nreach Steve. Covert transmitter Alice wishes to alter the packet stream in some\nway to send information to covert receiver Bob without watchful and capable\nadversary Willie being able to detect the presence of the message. In our\nprevious works, we addressed two techniques for such covert transmission from\nAlice to Bob: packet insertion and packet timing. In this paper, we consider\ncovert communication via bit insertion in packets with available space (e.g.,\nwith size less than the maximum transmission unit). We consider three\nscenarios: 1) packet sizes are independent and identically distributed (i.i.d.)\nwith a probability mass function (pmf) whose support is a set of one bit spaced\nvalues; 2) packet sizes are i.i.d. with a pmf whose support is arbitrary; 3)\npacket sizes may be dependent. For the first and second assumptions, we show\nthat Alice can covertly insert $\\mathcal{O}(\\sqrt{n})$ bits of information in a\nflow of $n$ packets; conversely, if she inserts $\\omega(\\sqrt{n})$ bits of\ninformation, Willie can detect her with arbitrarily small error probability.\nFor the third assumption, we prove Alice can covertly insert on average\n$\\mathcal{O}(c(n)/\\sqrt{n})$ bits in a sequence of $n$ packets, where $c(n)$ is\nthe average number of conditional pmf of packet sizes given the history, with a\nsupport of at least size two. \n\n"}
{"id": "1810.03556", "contents": "Title: A quantum network stack and protocols for reliable entanglement-based\n  networks Abstract: We present a stack model for breaking down the complexity of\nentanglement-based quantum networks. More specifically, we focus on the\nstructures and architectures of quantum networks and not on concrete physical\nimplementations of network elements. We construct the quantum network stack in\na hierarchical manner comprising several layers, similar to the classical\nnetwork stack, and identify quantum networking devices operating on each of\nthese layers. The layers responsibilities range from establishing\npoint-to-point connectivity, over intra-network graph state generation, to\ninter-network routing of entanglement. In addition we propose several protocols\noperating on these layers. In particular, we extend the existing intra-network\nprotocols for generating arbitrary graph states to ensure reliability inside a\nquantum network, where here reliability refers to the capability to compensate\nfor devices failures. Furthermore, we propose a routing protocol for quantum\nrouters which enables to generate arbitrary graph states across network\nboundaries. This protocol, in correspondence with classical routing protocols,\ncan compensate dynamically for failures of routers, or even complete networks,\nby simply re-routing the given entanglement over alternative paths. We also\nconsider how to connect quantum routers in a hierarchical manner to reduce\ncomplexity, as well as reliability issues arising in connecting these quantum\nnetworking devices. \n\n"}
{"id": "1810.03670", "contents": "Title: The Wireless Control Plane: An Overview and Directions for Future\n  Research Abstract: Software-defined networking (SDN), which has been successfully deployed in\nthe management of complex data centers, has recently been incorporated into a\nmyriad of 5G networks to intelligently manage a wide range of heterogeneous\nwireless devices, software systems, and wireless access technologies. Thus, the\nSDN control plane needs to communicate wirelessly with the wireless data plane\neither directly or indirectly. The uncertainties in the wireless SDN control\nplane (WCP) make its design challenging. Both WCP schemes (direct WCP, D-WCP,\nand indirect WCP, I-WCP) have been incorporated into recent 5G networks;\nhowever, a discussion of their design principles and their design limitations\nis missing. This paper introduces an overview of the WCP design (I-WCP and\nD-WCP) and discusses its intricacies by reviewing its deployment in recent 5G\nnetworks. Furthermore, to facilitate synthesizing a robust WCP, this paper\nproposes a generic WCP framework using deep reinforcement learning (DRL)\nprinciples and presents a roadmap for future research. \n\n"}
{"id": "1810.03857", "contents": "Title: A Novel Approach to Quality of Service Provisioning in Trusted Relay\n  Quantum Key Distribution Networks Abstract: In recent years, noticeable progress has been made in the development of\nquantum equipment, reflected through the number of successful demonstrations of\nQuantum Key Distribution (QKD) technology. Although they showcase the great\nachievements of QKD, many practical difficulties still need to be resolved.\nInspired by the significant similarity between mobile ad-hoc networks and QKD\ntechnology, we propose a novel quality of service (QoS) model including new\nmetrics for determining the states of public and quantum channels as well as a\ncomprehensive metric of the QKD link. We also propose a novel routing protocol\nto achieve high-level scalability and minimize consumption of cryptographic\nkeys. Given the limited mobility of nodes in QKD networks, our routing protocol\nuses the geographical distance and calculated link states to determine the\noptimal route. It also benefits from a caching mechanism and detection of\nreturning loops to provide effective forwarding while minimizing key\nconsumption and achieving the desired utilization of network links. Simulation\nresults are presented to demonstrate the validity and accuracy of the proposed\nsolutions. \n\n"}
{"id": "1810.04105", "contents": "Title: Multibeam for Joint Communication and Sensing Using Steerable Analog\n  Antenna Arrays Abstract: Beamforming has great potential for joint communication and sensing (JCAS),\nwhich is becoming a demanding feature on many emerging platforms such as\nunmanned aerial vehicles and smart cars. Although beamforming has been\nextensively studied for communication and radar sensing respectively, its\napplication in the joint system is not straightforward due to different\nbeamforming requirements by communication and sensing. In this paper, we\npropose a novel multibeam framework using steerable analog antenna arrays,\nwhich allows seamless integration of communication and sensing. Different to\nconventional JCAS schemes that support JCAS using a single beam, our framework\nis based on the key innovation of multibeam technology: providing fixed subbeam\nfor communication and packet-varying scanning subbeam for sensing,\nsimultaneously from a single transmitting array. We provide a system\narchitecture and protocols for the proposed framework, complying well with\nmodern packet communication systems with multicarrier modulation. We also\npropose low-complexity and effective multibeam design and generation methods,\nwhich offer great flexibility in meeting different communication and sensing\nrequirements. We further develop sensing parameter estimation algorithms using\nconventional digital Fourier transform and 1D compressive sensing techniques,\nmatching well with the multibeam framework. Simulation results are provided and\nvalidate the effectiveness of our proposed framework, beamforming design\nmethods and the sensing algorithms. \n\n"}
{"id": "1810.04118", "contents": "Title: Semi-supervised Deep Reinforcement Learning in Support of IoT and Smart\n  City Services Abstract: Smart services are an important element of the smart cities and the Internet\nof Things (IoT) ecosystems where the intelligence behind the services is\nobtained and improved through the sensory data. Providing a large amount of\ntraining data is not always feasible; therefore, we need to consider\nalternative ways that incorporate unlabeled data as well. In recent years, Deep\nreinforcement learning (DRL) has gained great success in several application\ndomains. It is an applicable method for IoT and smart city scenarios where\nauto-generated data can be partially labeled by users' feedback for training\npurposes. In this paper, we propose a semi-supervised deep reinforcement\nlearning model that fits smart city applications as it consumes both labeled\nand unlabeled data to improve the performance and accuracy of the learning\nagent. The model utilizes Variational Autoencoders (VAE) as the inference\nengine for generalizing optimal policies. To the best of our knowledge, the\nproposed model is the first investigation that extends deep reinforcement\nlearning to the semi-supervised paradigm. As a case study of smart city\napplications, we focus on smart buildings and apply the proposed model to the\nproblem of indoor localization based on BLE signal strength. Indoor\nlocalization is the main component of smart city services since people spend\nsignificant time in indoor environments. Our model learns the best action\npolicies that lead to a close estimation of the target locations with an\nimprovement of 23% in terms of distance to the target and at least 67% more\nreceived rewards compared to the supervised DRL model. \n\n"}
{"id": "1810.04371", "contents": "Title: Can Determinacy Minimize Age of Information? Abstract: Age-of-information (AoI) is a newly proposed performance metric of\ninformation freshness. It differs from the traditional delay metric, because it\nis destination centric and measures the time that elapsed since the last\nreceived fresh information update was generated at the source. AoI has been\nanalyzed for several queueing models, and the problem of optimizing AoI over\narrival and service rates has been studied in the literature. We consider the\nproblem of minimizing AoI over the space of update generation and service time\ndistributions. In particular, we ask whether determinacy, i.e. periodic\ngeneration of update packets and/or deterministic service, optimizes AoI. By\nconsidering several queueing systems, we show that in certain settings,\ndeterministic service can in fact result in the worst case AoI, while a\nheavy-tailed distributed service can yield the minimum AoI. This leads to an\ninteresting conclusion that, in some queueing systems, the service time\ndistribution that minimizes expected packet delay, or variance in packet delay\ncan, in fact, result in the worst case AoI. This exposes a fundamental\ndifference between AoI metrics and packet delay. \n\n"}
{"id": "1810.06930", "contents": "Title: Feedforward Neural Networks for Caching: Enough or Too Much? Abstract: We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications. \n\n"}
{"id": "1810.06938", "contents": "Title: Wireless Access in Ultra-Reliable Low-Latency Communication (URLLC) Abstract: The future connectivity landscape and, notably, the 5G wireless systems will\nfeature Ultra-Reliable Low Latency Communication (URLLC). The coupling of high\nreliability and low latency requirements in URLLC use cases makes the wireless\naccess design very challenging, in terms of both the protocol design and of the\nassociated transmission techniques. This paper aims to provide a broad\nperspective on the fundamental tradeoffs in URLLC as well as the principles\nused in building access protocols. Two specific technologies are considered in\nthe context of URLLC: massive MIMO and multi-connectivity, also termed\ninterface diversity. The paper also touches upon the important question of the\nproper statistical methodology for designing and assessing extremely high\nreliability levels. \n\n"}
{"id": "1810.07120", "contents": "Title: A Comprehensive Survey on Networking over TV White Spaces Abstract: TV white spaces refer to the allocated but locally unused TV spectrum and can\nbe used by unlicensed devices as secondary users. Thanks to their lower\nfrequencies (e.g., 54 -- 698 MHz in the US), communication over the TV spectrum\nhas excellent propagation characteristics over long distances and through\nobstacles. These characteristics along with their wide availability make the TV\nwhite spaces a great choice and alternative to many existing wireless\ntechnologies, especially the ones that need long range and high bandwidth\ncommunication. In the last decade, there have been numerous efforts from\nacademia, industries, and standards bodies for exploiting the potentials of the\nTV white spaces for several applications including wireless broadband Internet\naccess. Their characteristics and features also hold potentials for many new\napplications including sensing and monitoring, Internet of Things, wireless\ncontrol, smart utility, location-based services, and transportation and\nlogistics. In this paper, we perform a retrospective review and comparative\nstudy of existing work on networking in the TV white spaces. Additionally, we\ndiscuss the associated research challenges such as dealing with the\ninterference between primary and secondary TV spectrum users, TV white space\ntemporal and spatial variations and fragmentation, antenna design, mobility,\nand security. We also describe the future research directions to handle the\nabove challenges. To the best of our knowledge, this is the first comprehensive\nsurvey on the literature of TV white space networking research. \n\n"}
{"id": "1810.07690", "contents": "Title: Forecasting financial crashes with quantum computing Abstract: A key problem in financial mathematics is the forecasting of financial\ncrashes: if we perturb asset prices, will financial institutions fail on a\nmassive scale? This was recently shown to be a computationally intractable\n(NP-hard) problem. Financial crashes are inherently difficult to predict, even\nfor a regulator which has complete information about the financial system. In\nthis paper we show how this problem can be handled by quantum annealers. More\nspecifically, we map the equilibrium condition of a toy-model financial network\nto the ground-state problem of a spin-1/2 quantum Hamiltonian with 2-body\ninteractions, i.e., a quadratic unconstrained binary optimization (QUBO)\nproblem. The equilibrium market values of institutions after a sudden shock to\nthe network can then be calculated via adiabatic quantum computation and, more\ngenerically, by quantum annealers. Our procedure could be implemented on\nnear-term quantum processors, thus providing a potentially more efficient way\nto assess financial equilibrium and predict financial crashes. \n\n"}
{"id": "1810.07730", "contents": "Title: Implementation and Analysis of QUIC for MQTT Abstract: Transport and security protocols are essential to ensure reliable and secure\ncommunication between two parties. For IoT applications, these protocols must\nbe lightweight, since IoT devices are usually resource constrained.\nUnfortunately, the existing transport and security protocols -- namely TCP/TLS\nand UDP/DTLS -- fall short in terms of connection overhead, latency, and\nconnection migration when used in IoT applications. In this paper, after\nstudying the root causes of these shortcomings, we show how utilizing QUIC in\nIoT scenarios results in a higher performance. Based on these observations, and\ngiven the popularity of MQTT as an IoT application layer protocol, we integrate\nMQTT with QUIC. By presenting the main APIs and functions developed, we explain\nhow connection establishment and message exchange functionalities work. We\nevaluate the performance of MQTTw/QUIC versus MQTTw/TCP using wired, wireless,\nand long-distance testbeds. Our results show that MQTTw/QUIC reduces connection\noverhead in terms of the number of packets exchanged with the broker by up to\n56%. In addition, by eliminating half-open connections, MQTTw/QUIC reduces\nprocessor and memory usage by up to 83% and 50%, respectively. Furthermore, by\nremoving the head-of-line blocking problem, delivery latency is reduced by up\nto 55%. We also show that the throughput drops experienced by MQTTw/QUIC when a\nconnection migration happens is considerably lower than that of MQTTw/TCP. \n\n"}
{"id": "1810.09300", "contents": "Title: RCanopus: Making Canopus Resilient to Failures and Byzantine Faults Abstract: Distributed consensus is a key enabler for many distributed systems including\ndistributed databases and blockchains. Canopus is a scalable distributed\nconsensus protocol that ensures that live nodes in a system agree on an ordered\nsequence of operations (called transactions). Unlike most prior consensus\nprotocols, Canopus does not rely on a single leader. Instead, it uses a virtual\ntree overlay for message dissemination to limit network traffic across\noversubscribed links. It leverages hardware redundancies, both within a rack\nand inside the network fabric, to reduce both protocol complexity and\ncommunication overhead. These design decisions enable Canopus to support large\ndeployments without significant performance degradation.\n  The existing Canopus protocol is resilient in the face of node and\ncommunication failures, but its focus is primarily on performance, so does not\nrespond well to other types of failures. For example, the failure of a single\nrack of servers causes all live nodes to stall. The protocol is also open to\nattack by Byzantine nodes, which can cause different live nodes to conclude the\nprotocol with different transaction orders. In this paper, we describe RCanopus\n(`resilent Canopus') which extends Canopus to add liveness, that is, allowing\nlive nodes to make progress, when possible, despite many types of failures.\nThis requires RCanopus to accurately detect and recover from failure despite\nusing unreliable failure detectors, and tolerance of Byzantine attacks. Second,\nRCanopus guarantees safety, that is, agreement amongst live nodes of\ntransaction order, in the presence of Byzantine attacks and network\npartitioning. \n\n"}
{"id": "1810.10139", "contents": "Title: Joint Transaction Transmission and Channel Selection in Cognitive Radio\n  Based Blockchain Networks: A Deep Reinforcement Learning Approach Abstract: To ensure that the data aggregation, data storage, and data processing are\nall performed in a decentralized but trusted manner, we propose to use the\nblockchain with the mining pool to support IoT services based on cognitive\nradio networks. As such, the secondary user can send its sensing data, i.e.,\ntransactions, to the mining pools. After being verified by miners, the\ntransactions are added to the blocks. However, under the dynamics of the\nprimary channel and the uncertainty of the mempool state of the mining pool, it\nis challenging for the secondary user to determine an optimal transaction\ntransmission policy. In this paper, we propose to use the deep reinforcement\nlearning algorithm to derive an optimal transaction transmission policy for the\nsecondary user. Specifically, we adopt a Double Deep-Q Network (DDQN) that\nallows the secondary user to learn the optimal policy. The simulation results\nclearly show that the proposed deep reinforcement learning algorithm\noutperforms the conventional Q-learning scheme in terms of reward and learning\nspeed. \n\n"}
{"id": "1810.10247", "contents": "Title: Leveraging eBPF for programmable network functions with IPv6 Segment\n  Routing Abstract: With the advent of Software Defined Networks (SDN), Network Function\nVirtualisation (NFV) or Service Function Chaining (SFC), operators expect\nnetworks to support flexible services beyond the mere forwarding of packets.\nThe network programmability framework which is being developed within the IETF\nby leveraging IPv6 Segment Routing enables the realisation of in-network\nfunctions. In this paper, we demonstrate that this vision of in-network\nprogrammability can be realised. By leveraging the eBPF support in the Linux\nkernel, we implement a flexible framework that allows network operators to\nencode their own network functions as eBPF code that is automatically executed\nwhile processing specific packets. Our lab measurements indicate that the\noverhead of calling such eBPF functions remains acceptable. Thanks to eBPF,\noperators can implement a variety of network functions. We describe the\narchitecture of our implementation in the Linux kernel. This extension has been\nreleased with Linux 4.18. We illustrate the flexibility of our approach with\nthree different use cases: delay measurements, hybrid networks and network\ndiscovery. Our lab measurements also indicate that the performance penalty of\nrunning eBPF network functions on Linux routers does not incur a significant\noverhead. \n\n"}
{"id": "1810.11280", "contents": "Title: Virtual Network Embedding via Decomposable LP Formulations: Orientations\n  of Small Extraction Width and Beyond Abstract: The Virtual Network Embedding Problem (VNEP) considers the efficient\nallocation of resources distributed in a substrate network to a set of request\nnetworks. Many existing works discuss either heuristics or exact algorithms,\nresulting in a choice between quick runtimes and quality guarantees for the\nsolutions. Recently, the first fixed-parameter tractable (FPT) approximation\nalgorithm for the VNEP with arbitrary request and substrate topologies has been\npublished by Rost and Schmid. This algorithm is based on a LP formulation and\nis FPT in the newly introduced graph parameter extraction width (EW). It\ntherefore combines positive traits of heuristics and exact approaches: The\nruntime is polynomial for instances with bounded EW, and the algorithm returns\napproximate solutions with high probability.\n  We propose two extensions of this algorithm to optimize its runtime. Firstly,\nwe develop a new LP formulation related to tree decompositions. We show that\nthis LP formulation is always smaller, and that the resulting algorithm is FPT\nin the new parameter extraction label width (ELW). We improve on two important\nresults by Rost and Schmid: For centrally rooted half-wheel topologies, the EW\nscales linearly with request size, whereas the ELW is constant. Further, adding\nany number of paths parallel to an existing edge increases the EW by at most\nthe maximum degree of the request. By contrast, the ELW only increases by at\nmost one. Lastly, we show that finding the minimal ELW is NP-hard.\n  Secondly, we consider the approach of partitioning the request into subgraphs\nwhich are processed independently, yielding even smaller LP formulations. While\nthis algorithm may lead to higher ELW within the subgraphs, we show that this\nincrease is always smaller than the size of the inter-subgraph boundary. In\nparticular, the algorithm has zero additional cost when subgraphs are separated\nby a single node. \n\n"}
{"id": "1810.12209", "contents": "Title: Quality-of-Service in Multihop Wireless Networks: Diffusion\n  Approximation Abstract: We consider a multihop wireless system. There are multiple source-destination\npairs. The data from a source may have to pass through multiple nodes. We\nobtain a channel scheduling policy which can guarantee end-to-end mean delay\nfor the different traffic streams. We show the stability of the network for\nthis policy by convergence to a fluid limit. It is intractable to obtain the\nstationary distribution of this network. Thus we also provide a diffusion\napproximation for this scheme under heavy traffic. We show that the stationary\ndistribution of the scaled process of the network converges to that of the\nBrownian limit. This theoretically justifies the performance of the system. We\nprovide simulations to verify our claims. \n\n"}
{"id": "1810.12840", "contents": "Title: Asset Price Distributions and Efficient Markets Abstract: We explore a decomposition in which returns on a large class of portfolios\nrelative to the market depend on a smooth non-negative drift and changes in the\nasset price distribution. This decomposition is obtained using general\ncontinuous semimartingale price representations, and is thus consistent with\nvirtually any asset pricing model. Fluctuations in portfolio relative returns\ndepend on stochastic time-varying dispersion in asset prices. Thus, our\nframework uncovers an asset pricing factor whose existence emerges from an\naccounting identity universal across different economic and financial\nenvironments, a fact that has deep implications for market efficiency. In\nparticular, in a closed, dividend-free market in which asset price dispersion\nis relatively constant, a large class of portfolios must necessarily outperform\nthe market portfolio over time. We show that price dispersion in commodity\nfutures markets has increased only slightly, and confirm the existence of\nsubstantial excess returns that co-vary with changes in price dispersion as\npredicted by our theory. \n\n"}
{"id": "1810.12859", "contents": "Title: JavaScript Convolutional Neural Networks for Keyword Spotting in the\n  Browser: An Experimental Analysis Abstract: Used for simple commands recognition on devices from smart routers to mobile\nphones, keyword spotting systems are everywhere. Ubiquitous as well are web\napplications, which have grown in popularity and complexity over the last\ndecade with significant improvements in usability under cross-platform\nconditions. However, despite their obvious advantage in natural language\ninteraction, voice-enabled web applications are still far and few between. In\nthis work, we attempt to bridge this gap by bringing keyword spotting\ncapabilities directly into the browser. To our knowledge, we are the first to\ndemonstrate a fully-functional implementation of convolutional neural networks\nin pure JavaScript that runs in any standards-compliant browser. We also apply\nnetwork slimming, a model compression technique, to explore the\naccuracy-efficiency tradeoffs, reporting latency measurements on a range of\ndevices and software. Overall, our robust, cross-device implementation for\nkeyword spotting realizes a new paradigm for serving neural network\napplications, and one of our slim models reduces latency by 66% with a minimal\ndecrease in accuracy of 4% from 94% to 90%. \n\n"}
{"id": "1811.01498", "contents": "Title: DSIC: Deep Learning based Self-Interference Cancellation for In-Band\n  Full Duplex Wireless Abstract: In-band full duplex wireless is of utmost interest to future wireless\ncommunication and networking due to great potentials of spectrum efficiency.\nIBFD wireless, however, is throttled by its key challenge, namely\nself-interference. Therefore, effective self-interference cancellation is the\nkey to enable IBFD wireless. This paper proposes a real-time non-linear\nself-interference cancellation solution based on deep learning. In this\nsolution, a self-interference channel is modeled by a deep neural network\n(DNN). Synchronized self-interference channel data is first collected to train\nthe DNN of the self-interference channel. Afterwards, the trained DNN is used\nto cancel the self-interference at a wireless node. This solution has been\nimplemented on a USRP SDR testbed and evaluated in real world in multiple\nscenarios with various modulations in transmitting information including\nnumbers, texts as well as images. It results in the performance of 17dB in\ndigital cancellation, which is very close to the self-interference power and\nnearly cancels the self-interference at a SDR node in the testbed. The solution\nyields an average of 8.5% bit error rate (BER) over many scenarios and\ndifferent modulation schemes. \n\n"}
{"id": "1811.02330", "contents": "Title: Traversing Virtual Network Functions from the Edge to the Core: An\n  End-to-End Performance Analysis Abstract: Future mobile networks supporting Internet of Things are expected to provide\nboth high throughput and low latency to user-specific services. One way to\novercome this challenge is to adopt network function virtualization and\nMulti-access edge computing (MEC). In this paper, we analyze an end-to-end\ncommunication system that consists of both MEC servers and a server at the core\nnetwork hosting different types of virtual network functions. We develop a\nqueueing model for the performance analysis of the system consisting of both\nprocessing and transmission flows. The system is decomposed into subsystems\nwhich are independently analyzed in order to approximate the behaviour of the\noriginal system. We provide closed-form expressions of the performance metrics\nsuch as system drop rate and average number of tasks in the system. Simulation\nresults show that our approximation performs quite well. By evaluating the\nsystem under different scenarios, we provide insights for the decision making\non traffic flow control and its impact on critical performance metrics. \n\n"}
{"id": "1811.03410", "contents": "Title: Quantifying Link Stability in Ad Hoc Wireless Networks Subject to\n  Ornstein-Uhlenbeck Mobility Abstract: The performance of mobile ad hoc networks in general and that of the routing\nalgorithm, in particular, can be heavily affected by the intrinsic dynamic\nnature of the underlying topology. In this paper, we build a new\nanalytical/numerical framework that characterizes nodes' mobility and the\nevolution of links between them. This formulation is based on a stationary\nMarkov chain representation of link connectivity. The existence of a link\nbetween two nodes depends on their distance, which is governed by the mobility\nmodel. In our analysis, nodes move randomly according to an Ornstein-Uhlenbeck\nprocess using one tuning parameter to obtain different levels of randomness in\nthe mobility pattern. Finally, we propose an entropy-rate-based metric that\nquantifies link uncertainty and evaluates its stability. Numerical results show\nthat the proposed approach can accurately reflect the random mobility in the\nnetwork and fully captures the link dynamics. It may thus be considered a\nvaluable performance metric for the evaluation of the link stability and\nconnectivity in these networks. \n\n"}
{"id": "1811.03748", "contents": "Title: Adaptive Task Allocation for Mobile Edge Learning Abstract: This paper aims to establish a new optimization paradigm for implementing\nrealistic distributed learning algorithms, with performance guarantees, on\nwireless edge nodes with heterogeneous computing and communication capacities.\nWe will refer to this new paradigm as `Mobile Edge Learning (MEL)'. The problem\nof dynamic task allocation for MEL is considered in this paper with the aim to\nmaximize the learning accuracy, while guaranteeing that the total times of data\ndistribution/aggregation over heterogeneous channels, and local computing\niterations at the heterogeneous nodes, are bounded by a preset duration. The\nproblem is first formulated as a quadratically-constrained integer linear\nproblem. Being an NP-hard problem, the paper relaxes it into a non-convex\nproblem over real variables. We thus proposed two solutions based on deriving\nanalytical upper bounds of the optimal solution of this relaxed problem using\nLagrangian analysis and KKT conditions, and the use of suggest-and-improve\nstarting from equal batch allocation, respectively. The merits of these\nproposed solutions are exhibited by comparing their performances to both\nnumerical approaches and the equal task allocation approach. \n\n"}
{"id": "1811.03981", "contents": "Title: Ultra-Reliable Low-Latency Vehicular Networks: Taming the Age of\n  Information Tail Abstract: While the notion of age of information (AoI) has recently emerged as an\nimportant concept for analyzing ultra-reliable low-latency communications\n(URLLC), the majority of the existing works have focused on the average AoI\nmeasure. However, an average AoI based design falls short in properly\ncharacterizing the performance of URLLC systems as it cannot account for\nextreme events that occur with very low probabilities. In contrast, in this\npaper, the main objective is to go beyond the traditional notion of average AoI\nby characterizing and optimizing a URLLC system while capturing the AoI tail\ndistribution. In particular, the problem of vehicles' power minimization while\nensuring stringent latency and reliability constraints in terms of\nprobabilistic AoI is studied. To this end, a novel and efficient mapping\nbetween both AoI and queue length distributions is proposed. Subsequently,\nextreme value theory (EVT) and Lyapunov optimization techniques are adopted to\nformulate and solve the problem. Simulation results shows a nearly two-fold\nimprovement in terms of shortening the tail of the AoI distribution compared to\na baseline whose design is based on the maximum queue length among vehicles,\nwhen the number of vehicular user equipment (VUE) pairs is 80. The results also\nshow that this performance gain increases significantly as the number of VUE\npairs increases. \n\n"}
{"id": "1811.05308", "contents": "Title: On Distributed Routing in Underwater Optical Wireless Sensor Networks Abstract: Underwater optical wireless communication (UOWC) is becoming an attractive\ntechnology for underwater wireless sensor networks (UWSNs) since it offers\nhigh-speed communication links. Although UOWC overcomes the drawbacks of\nacoustic and radio frequency communication channels such as high latency and\nlow data rate, yet, it has its own limitations. One of the major limitations of\nUOWC is its limited transmission range which demands to develop a multi-hop\nnetwork with efficient routing protocols. Currently, the routing protocols for\nUOWSNs are centralized having high complexity and large end-to-end delay. In\nthis article, first, we present the existing routing protocols for UOWSNs.\nBased on the existing protocols, we then propose distributed routing protocols\nto address the problems of high complexity and large end-to-end delay.\nNumerical results have been provided to show that the proposed routing protocol\nis superior to the existing protocols in terms of complexity and end-to-end\ndelay. Finally, we have presented open research directions in UOWSNs. \n\n"}
{"id": "1811.05948", "contents": "Title: EdgeBench: Benchmarking Edge Computing Platforms Abstract: The emerging trend of edge computing has led several cloud providers to\nrelease their own platforms for performing computation at the 'edge' of the\nnetwork. We compare two such platforms, Amazon AWS Greengrass and Microsoft\nAzure IoT Edge, using a new benchmark comprising a suite of performance\nmetrics. We also compare the performance of the edge frameworks to cloud-only\nimplementations available in their respective cloud ecosystems. Amazon AWS\nGreengrass and Azure IoT Edge use different underlying technologies, edge\nLambda functions vs. containers, and so we also elaborate on platform features\navailable to developers. Our study shows that both of these edge platforms\nprovide comparable performance, which nevertheless differs in important ways\nfor key types of workloads used in edge applications. Finally, we discuss\nseveral current issues and challenges we faced in deploying these platforms. \n\n"}
{"id": "1811.06261", "contents": "Title: Efficient Edge Rewiring Strategies for Enhancement in Network Capacity Abstract: The structure of the network has great impact on its traffic dynamics. Most\nof the real world networks follow the heterogeneous structure and exhibit\nscale-free feature. In scale-free network, a new node prefers to connect with\nhub nodes and the network capacity is curtailed by smaller degree nodes.\nTherefore, we propose rewiring a fraction of links in the network, to improve\nthe network transport efficiency. In this paper, we discuss some efficient link\nrewiring strategies and perform simulations on scale-free networks, confirming\nthe effectiveness of these strategies. The rewiring strategies actually reduce\nthe centrality of the nodes having higher betweenness centrality. After the\nlink rewiring process, the degree distribution of the network remains the same.\nThis work will be beneficial for the enhancement of network performance. \n\n"}
{"id": "1811.09221", "contents": "Title: The typical cell in anisotropic tessellations Abstract: The typical cell is a key concept for stochastic-geometry based modeling in\ncommunication networks, as it provides a rigorous framework for describing\nproperties of a serving zone associated with a component selected at random in\na large network. We consider a setting where network components are located on\na large street network. While earlier investigations were restricted to street\nsystems without preferred directions, in this paper we derive the distribution\nof the typical cell in Manhattan-type systems characterized by a pattern of\nhorizontal and vertical streets. We explain how the mathematical description\ncan be turned into a simulation algorithm and provide numerical results\nuncovering novel effects when compared to classical isotropic networks. \n\n"}
{"id": "1811.12605", "contents": "Title: Minimizing Age-of-Information with Throughput Requirements in Multi-Path\n  Network Communication Abstract: We consider the scenario where a sender periodically sends a batch of data to\na receiver over a multi-hop network, possibly using multiple paths. Our\nobjective is to minimize peak/average Age-of-Information (AoI) subject to\nthroughput requirements. The consideration of batch generation and multi-path\ncommunication differentiates our AoI study from existing ones. We first show\nthat our AoI minimization problems are NP-hard, but only in the weak sense, as\nwe develop an optimal algorithm with a pseudo-polynomial time complexity. We\nthen prove that minimizing AoI and minimizing maximum delay are \"roughly\"\nequivalent, in the sense that any optimal solution of the latter is an\napproximate solution of the former with bounded optimality loss. We leverage\nthis understanding to design a general approximation framework for our\nproblems. It can build upon any $\\alpha$-approximation algorithm of the maximum\ndelay minimization problem, to construct an $(\\alpha+c)$-approximate solution\nfor minimizing AoI. Here $c$ is a constant depending on the throughput\nrequirements. Simulations over various network topologies validate the\neffectiveness of our approach. \n\n"}
{"id": "1811.12924", "contents": "Title: Joint Information Freshness and Completion Time Optimization for\n  Vehicular Networks Abstract: The demand for real-time cloud applications has seen an unprecedented growth\nover the past decade. These applications require rapidly data transfer and fast\ncomputations. This paper considers a scenario where multiple IoT devices update\ninformation on the cloud, and request a computation from the cloud at certain\ntimes. The time required to complete the request for computation includes the\ntime to wait for computation to start on busy virtual machines, performing the\ncomputation, waiting and service in the networking stage for delivering the\noutput to the end user. In this context, the freshness of the information is an\nimportant concern and is different from the completion time. This paper\nproposes novel scheduling strategies for both computation and networking\nstages. Based on these strategies, the age-of-information (AoI) metric and the\ncompletion time are characterized. A convex combination of the two metrics is\noptimized over the scheduling parameters. The problem is shown to be convex and\nthus can be solved optimally. Moreover, based on the offline policy, an online\nalgorithm for job scheduling is developed. Numerical results demonstrate\nsignificant improvement as compared to the considered baselines. \n\n"}
{"id": "1812.01830", "contents": "Title: Unified Analysis of HetNets using Poisson Cluster Process under\n  Max-Power Association Abstract: Owing to its flexibility in modeling real-world spatial configurations of\nusers and base stations (BSs), the Poisson cluster process (PCP) has recently\nemerged as an appealing way to model and analyze heterogeneous cellular\nnetworks (HetNets). Despite its undisputed relevance to HetNets -- corroborated\nby the models used in industry -- the PCP's use in performance analysis has\nbeen limited. This is primarily because of the lack of analytical tools to\ncharacterize performance metrics such as the coverage probability of a user\nconnected to the strongest BS. In this paper, we develop an analytical\nframework for the evaluation of the coverage probability, or equivalently the\ncomplementary cumulative density function (CCDF) of\nsignal-to-interference-and-noise-ratio (SINR), of a typical user in a K-tier\nHetNet under a max power-based association strategy, where the BS locations of\neach tier follow either a Poisson point process (PPP) or a PCP. The key\nenabling step involves conditioning on the parent PPPs of all the PCPs which\nallows us to express the coverage probability as a product of sum-product and\nprobability generating functionals (PGFLs) of the parent PPPs. In addition to\nseveral useful insights, our analysis provides a rigorous way to study the\nimpact of the cluster size on the SINR distribution, which was not possible\nusing existing PPP-based models. \n\n"}
{"id": "1812.02791", "contents": "Title: Multi-Hop Communication for nTorrent in a Wireless Ad Hoc Environment Abstract: nTorrent is a BitTorrent-like application that is based on NDN (Named Data\nNetworking). Ad hoc environments introduce additional challenges to the\ndissemination of files among peers. Some issues that we encounter are that not\nall peers in the neighborhood or environment run the nTorrent application or\ndesire the same torrent file. These issues cause nTorrent interests to be\nunable to be processed or prevent peers from downloading their desired torrent\nfiles. In order to solve this issue, I implemented pure forwarding nodes that\nrepresent peers that do not run the nTorrent application and also extended the\noriginal nTorrent application to be able to forward interests for torrent files\nother than their own desired torrent file. For this project, the solution is\nable to facilitate multi-hop communication through all nodes present in the\nenvironment whether or not they run nTorrent. \n\n"}
{"id": "1812.03633", "contents": "Title: Efficient Training Management for Mobile Crowd-Machine Learning: A Deep\n  Reinforcement Learning Approach Abstract: In this letter, we consider the concept of Mobile Crowd-Machine Learning\n(MCML) for a federated learning model. The MCML enables mobile devices in a\nmobile network to collaboratively train neural network models required by a\nserver while keeping data on the mobile devices. The MCML thus addresses data\nprivacy issues of traditional machine learning. However, the mobile devices are\nconstrained by energy, CPU, and wireless bandwidth. Thus, to minimize the\nenergy consumption, training time and communication cost, the server needs to\ndetermine proper amounts of data and energy that the mobile devices use for\ntraining. However, under the dynamics and uncertainty of the mobile\nenvironment, it is challenging for the server to determine the optimal\ndecisions on mobile device resource management. In this letter, we propose to\nadopt a deep- Q learning algorithm that allows the server to learn and find\noptimal decisions without any a priori knowledge of network dynamics.\nSimulation results show that the proposed algorithm outperforms the static\nalgorithms in terms of energy consumption and training latency. \n\n"}
{"id": "1812.04711", "contents": "Title: Joint Computation Offloading and Resource Allocation in Cloud Based\n  Wireless HetNets Abstract: In this paper, we study the joint computation offloading and resource\nallocation problem in the two-tier wireless heterogeneous network (HetNet). Our\ndesign aims to optimize the computation offloading to the cloud jointly with\nthe subchannel allocation to minimize the maximum (min-max) weighted energy\nconsumption subject to practical constraints on bandwidth, computing resource\nand allowable latency for the multi-user multitask computation system. To\ntackle this non-convex mixed integer non-linear problem (MINLP), we employ the\nbisection search method to solve it where we propose a novel approach to\ntransform and verify the feasibility of the underlying problem in each\niteration. In addition, we propose a low-complexity algorithm, which can\ndecrease the number of binary optimization variables and enable more scalable\ncomputation offloading optimization in the practical wireless HetNets.\nNumerical studies confirm that the proposed design achieves the energy saving\ngains about 55% in comparison with the local computation scheme under the\nstrict required latency of 0.1s. \n\n"}
{"id": "1812.05670", "contents": "Title: When to Preempt? Age of Information Minimization under Link Capacity\n  Constraint Abstract: In this paper, we consider a scenario where a source continuously monitors an\nobject and sends time-stamped status updates to a destination through a\nrate-limited link. We assume updates arrive randomly at the source according to\na Bernoulli process. Due to the link capacity constraint, it takes multiple\ntime slots for the source to complete the transmission of an update. Therefore,\nwhen a new update arrives at the source during the transmission of another\nupdate, the source needs to decide whether to skip the new arrival or to switch\nto it, in order to minimize the expected average age of information (AoI) at\nthe destination. We start with the setting where all updates are of the same\nsize, and prove that within a broadly defined class of online policies, the\noptimal policy should be a renewal policy, and has a sequential switching\nproperty. We then show that the optimal decision of the source in any time slot\nhas threshold structures, and only depends on the age of the update being\ntransmitted and the AoI at the destination. We then consider the setting where\nupdates are of different sizes, and show that the optimal Markovian policy also\nhas a multiple-threshold structure. For each of the settings, we explicitly\nidentify the thresholds by formulating the problem as a Markov Decision Process\n(MDP), and solve it through value iteration. Special structural properties of\nthe corresponding optimal policy are utilized to reduce the computational\ncomplexity of the value iteration algorithm. \n\n"}
{"id": "1812.06000", "contents": "Title: The Rank Effect Abstract: We decompose returns for portfolios of bottom-ranked, lower-priced assets\nrelative to the market into rank crossovers and changes in the relative price\nof those bottom-ranked assets. This decomposition is general and consistent\nwith virtually any asset pricing model. Crossovers measure changes in rank and\nare smoothly increasing over time, while return fluctuations are driven by\nvolatile relative price changes. Our results imply that in a closed,\ndividend-free market in which the relative price of bottom-ranked assets is\napproximately constant, a portfolio of those bottom-ranked assets will\noutperform the market portfolio over time. We show that bottom-ranked relative\ncommodity futures prices have increased only slightly, and confirm the\nexistence of substantial excess returns predicted by our theory. If these\nexcess returns did not exist, then top-ranked relative prices would have had to\nbe much higher in 2018 than those actually observed -- this would imply a\nradically different commodity price distribution. \n\n"}
{"id": "1812.06553", "contents": "Title: Fast and Efficient Bulk Multicasting over Dedicated Inter-Datacenter\n  Networks Abstract: Several organizations have built multiple datacenters connected via dedicated\nwide area networks over which large inter-datacenter transfers take place. This\nincludes tremendous volumes of bulk multicast traffic generated as a result of\ndata and content replication. Although one can perform these transfers using a\nsingle multicast forwarding tree, that can lead to poor performance as the\nslowest receiver on each tree dictates the completion time for all receivers.\nUsing multiple trees per transfer each connected to a subset of receivers\nalleviates this concern. The choice of multicast trees also determines the\ntotal bandwidth usage. To further improve the performance, bandwidth over\ndedicated inter-datacenter networks can be carved for different multicast trees\nover specific time periods to avoid congestion and minimize the average\nreceiver completion times.\n  In this paper, we break this problem into the three sub-problems of\npartitioning, tree selection, and rate allocation. We present an algorithm\ncalled QuickCast which is computationally fast and allows us to significantly\nspeed up multiple receivers per bulk multicast transfer with control over extra\nbandwidth consumption. We evaluate QuickCast against a variety of synthetic and\nreal traffic patterns as well as real WAN topologies. Compared to performing\nbulk multicast transfers as separate unicast transfers, QuickCast achieves up\nto $3.64\\times$ reduction in mean completion times while at the same time using\n$0.71\\times$ the bandwidth. Also, QuickCast allows the top $50\\%$ of receivers\nto complete between $3\\times$ to $35\\times$ faster on average compared with\nwhen a single forwarding multicast tree is used for data delivery. \n\n"}
{"id": "1812.06885", "contents": "Title: Optimizing Throughput Performance in Distributed MIMO Wi-Fi Networks\n  using Deep Reinforcement Learning Abstract: This paper explores the feasibility of leveraging concepts from deep\nreinforcement learning (DRL) to enable dynamic resource management in Wi-Fi\nnetworks implementing distributed multi-user MIMO (D-MIMO). D-MIMO is a\ntechnique by which a set of wireless access points are synchronized and grouped\ntogether to jointly serve multiple users simultaneously. This paper addresses\ntwo dynamic resource management problems pertaining to D-MIMO Wi-Fi networks:\n(i) channel assignment of D-MIMO groups, and (ii) deciding how to cluster\naccess points to form D-MIMO groups, in order to maximize user throughput\nperformance. These problems are known to be NP-Hard and only heuristic\nsolutions exist in literature. We construct a DRL framework through which a\nlearning agent interacts with a D-MIMO Wi-Fi network, learns about the network\nenvironment, and is successful in converging to policies which address the\naforementioned problems. Through extensive simulations and on-line training\nbased on D-MIMO Wi-Fi networks, this paper demonstrates the efficacy of DRL in\nachieving an improvement of 20% in user throughput performance compared to\nheuristic solutions, particularly when network conditions are dynamic. This\nwork also showcases the effectiveness of DRL in meeting multiple network\nobjectives simultaneously, for instance, maximizing throughput of users as well\nas fairness of throughput among them. \n\n"}
{"id": "1812.07961", "contents": "Title: Geobiodynamics and Roegenian Economic Systems Abstract: This mathematical essay brings together ideas from Economics, Geobiodynamics\nand Thermodynamics. Its purpose is to obtain real models of complex\nevolutionary systems. More specifically, the essay defines Roegenian Economy\nand links Geobiodynamics and Roegenian Economy. In this context, we discuss the\nisomorphism between the concepts and techniques of Thermodynamics and\nEconomics. Then we describe a Roegenian economic system like a Carnot group.\nAfter we analyse the phase equilibrium for two heterogeneous economic systems.\nThe European Union Economics appears like Cartesian product of Roegenian\neconomic systems and its Balance is analysed in details. A Section at the end\ndescribes the \"economic black holes\" as small parts of a a global economic\nsystem in which national income is so great that it causes others poor\nenrichment. These ideas can be used to improve our knowledge and understanding\nof the nature of development and evolution of thermodynamic-economic systems. \n\n"}
{"id": "1812.08148", "contents": "Title: Minimizing Age of Information with Soft Updates Abstract: We consider an information updating system where an information provider and\nan information receiver engage in an update process over time. Different from\nthe existing literature where updates are countable (hard) and take effect\neither immediately or after a delay, but $instantaneously$ in both cases, here\nupdates start taking effect right away but $gradually$ over time. We coin this\nsetting $soft$ $updates$. When the updating process starts, the age decreases\nuntil the soft update period ends. We constrain the number of times the\ninformation provider and the information receiver meet (number of update\nperiods) and the total duration of the update periods. We consider two models\nfor the decrease of age during an update period: In the first model, the rate\nof decrease of age is proportional to the current age, and in the second model,\nthe rate of decrease of age is constant. The first model results in an\nexponentially decaying age, and the second model results in a linearly decaying\nage. In both cases, we determine the optimum updating schemes, by determining\nthe optimum start times and optimum durations of the updates, subject to the\nconstraints on the number of update periods and the total update duration. \n\n"}
{"id": "1812.08464", "contents": "Title: Statistical Location and Rotation-Aware Beam Search for Millimeter-Wave\n  Networks Abstract: Beam training in dynamic millimeter-wave (mm-wave) networks with mobile\ndevices is highly challenging as devices must scan a large angular domain to\nmaintain alignment of their directional antennas under mobility. Device\nrotation is particularly challenging, as a handheld device may rotate\nsignificantly over a very short period of time, causing it to lose the\nconnection to the Access Point (AP) unless the rotation is accompanied by\nimmediate beam realignment. We study how to maintain the link to a mm-wave AP\nunder rotation and without any input from inertial sensors, exploiting the fact\nthat mm-wave devices will typically be multi-band. We present a model that maps\nTime-of-Flight measurements to rotation and propose a method to infer the\nrotation speed of the mobile terminal using only measurements from sub-6 GHz\nWiFi. We also use the same sub-6 GHz WiFi system to reduce the angle error\nestimate for link establishment, exploiting the spatial geometry of the\ndeployed APs and a statistical model that maps the user position's spatial\ndistribution to an angle error distribution. We leverage these findings to\nintroduce SLASH, a Statistical Location and rotation-Aware beam SearcH\nalgorithm that adaptively narrows the sector search space and accelerates both\nlink establishment and maintenance between mm-wave devices. We evaluate SLASH\nwith experiments conducted indoors with a sub-6 GHz WiFi Time-of-Flight\npositioning system and a 60-GHz testbed. SLASH can increase the data rate by\nmore than 41% for link establishment and 67% for link maintenance with respect\nto prior work. \n\n"}
{"id": "1812.09026", "contents": "Title: Deep Reinforcement Learning for Real-Time Optimization in NB-IoT\n  Networks Abstract: NarrowBand-Internet of Things (NB-IoT) is an emerging cellular-based\ntechnology that offers a range of flexible configurations for massive IoT radio\naccess from groups of devices with heterogeneous requirements. A configuration\nspecifies the amount of radio resource allocated to each group of devices for\nrandom access and for data transmission. Assuming no knowledge of the traffic\nstatistics, there exists an important challenge in \"how to determine the\nconfiguration that maximizes the long-term average number of served IoT devices\nat each Transmission Time Interval (TTI) in an online fashion\". Given the\ncomplexity of searching for optimal configuration, we first develop real-time\nconfiguration selection based on the tabular Q-learning (tabular-Q), the Linear\nApproximation based Q-learning (LA-Q), and the Deep Neural Network based\nQ-learning (DQN) in the single-parameter single-group scenario. Our results\nshow that the proposed reinforcement learning based approaches considerably\noutperform the conventional heuristic approaches based on load estimation\n(LE-URC) in terms of the number of served IoT devices. This result also\nindicates that LA-Q and DQN can be good alternatives for tabular-Q to achieve\nalmost the same performance with much less training time. We further advance\nLA-Q and DQN via Actions Aggregation (AA-LA-Q and AA-DQN) and via Cooperative\nMulti-Agent learning (CMA-DQN) for the multi-parameter multi-group scenario,\nthereby solve the problem that Q-learning agents do not converge in\nhigh-dimensional configurations. In this scenario, the superiority of the\nproposed Q-learning approaches over the conventional LE-URC approach\nsignificantly improves with the increase of configuration dimensions, and the\nCMA-DQN approach outperforms the other approaches in both throughput and\ntraining efficiency. \n\n"}
{"id": "1812.09761", "contents": "Title: How to Achieve High Classification Accuracy with Just a Few Labels: A\n  Semi-supervised Approach Using Sampled Packets Abstract: Network traffic classification, which has numerous applications from security\nto billing and network provisioning, has become a cornerstone of today's\ncomputer networks. Previous studies have developed traffic classification\ntechniques using classical machine learning algorithms and deep learning\nmethods when large quantities of labeled data are available. However, capturing\nlarge labeled datasets is a cumbersome and time-consuming process. In this\npaper, we propose a semi-supervised approach that obviates the need for large\nlabeled datasets. We first pre-train a model on a large unlabeled dataset where\nthe input is the time series features of a few sampled packets. Then the\nlearned weights are transferred to a new model that is re-trained on a small\nlabeled dataset. We show that our semi-supervised approach achieves almost the\nsame accuracy as a fully-supervised method with a large labeled dataset, though\nwe use only 20 samples per class. In tests based on a dataset generated from\nthe more challenging QUIC protocol, our approach yields 98% accuracy. To show\nits efficacy, we also test our approach on two public datasets. Moreover, we\nstudy three different sampling techniques and demonstrate that sampling packets\nfrom an arbitrary portion of a flow is sufficient for classification. \n\n"}
{"id": "1812.11145", "contents": "Title: Checking-in on Network Functions Abstract: When programming network functions, changes within a packet tend to have\nconsequences---side effects which must be accounted for by network programmers\nor administrators via arbitrary logic and an innate understanding of\ndependencies. Examples of this include updating checksums when a packet's\ncontents has been modified or adjusting a payload length field of a IPv6 header\nif another header is added or updated within a packet. While static-typing\ncaptures interface specifications and how packet contents should behave, it\ndoes not enforce precise invariants around runtime dependencies like the\nexamples above. Instead, during the design phase of network functions,\nprogrammers should be given an easier way to specify checks up front, all\nwithout having to account for and keep track of these consequences at each and\nevery step during the development cycle. In keeping with this view, we present\na unique approach for adding and generating both static checks and dynamic\ncontracts for specifying and checking packet processing operations. We develop\nour technique within an existing framework called NetBricks and demonstrate how\nour approach simplifies and checks common dependent packet and header\nprocessing logic that other systems take for granted, all without adding much\noverhead during development. \n\n"}
{"id": "1812.11233", "contents": "Title: Beam with Adaptive Divergence Angle in Free-Space Optical Communications\n  for High-Speed Trains Abstract: In this paper, we propose an adaptive beam that adapts its divergence angle\naccording to the receiver aperture diameter and the communication distance to\nimprove the received power and ease the alignment between the communicating\noptical transceivers in a free-space optical communications (FSOC) system for\nhigh-speed trains (HSTs). We compare the received power, signal-to-noise ratio,\nbit error rate, and the maximum communication distance of the proposed adaptive\nbeam with a beam that uses a fixed divergence angle of 1 mrad. The proposed\nadaptive beam yields a higher received power with an increase of 33 dB in\naverage over the fixed-divergence beam under varying visibility conditions and\ndistance. Moreover, the proposed adaptive divergence angle extends the\ncommunication distance of a FSOC system for HSTs to about three times under\ndifferent visibility conditions as compared to a fixed divergence beam. We also\npropose a new ground transceiver placement that places the ground transceivers\nof a FSOC system for HSTs on gantries placed above the train passage instead of\nplacing them next to track. The proposed transceiver placement provides a\nreceived-power increase of 3.8 dB in average over the conventional placement of\nground-station transceivers next to the track. \n\n"}
{"id": "1812.11429", "contents": "Title: Modeling, Simulating and Configuring Programmable Wireless Environments\n  for Multi-User Multi-Objective Networking Abstract: Programmable wireless environments enable the software-defined propagation of\nwaves within them, yielding exceptional performance potential. Several\nbuilding-block technologies have been implemented and evaluated at the physical\nlayer. The present work contributes a network-layer scheme to configure such\nenvironments for multiple users and objectives, and for any physical-layer\ntechnology. Supported objectives include any combination of Quality of Service\nand power transfer optimization, eavesdropping and Doppler effect mitigation,\nin multi-cast or uni-cast settings. Additionally, a graph-based model of\nprogrammable environments is proposed, which incorporates core physical\nobservations and efficiently separates physical and networking concerns.\nEvaluation takes place in a specially developed, free simulation tool, and in a\nvariety of environments. Performance gains over regular propagation are\nhighlighted, reaching important insights on the user capacity of programmable\nenvironments. \n\n"}
{"id": "1812.11652", "contents": "Title: Using Machine Learning for Handover Optimization in Vehicular Fog\n  Computing Abstract: Smart mobility management would be an important prerequisite for future fog\ncomputing systems. In this research, we propose a learning-based handover\noptimization for the Internet of Vehicles that would assist the smooth\ntransition of device connections and offloaded tasks between fog nodes. To\naccomplish this, we make use of machine learning algorithms to learn from\nvehicle interactions with fog nodes. Our approach uses a three-layer\nfeed-forward neural network to predict the correct fog node at a given location\nand time with 99.2 % accuracy on a test set. We also implement a dual stacked\nrecurrent neural network (RNN) with long short-term memory (LSTM) cells capable\nof learning the latency, or cost, associated with these service requests. We\ncreate a simulation in JAMScript using a dataset of real-world vehicle\nmovements to create a dataset to train these networks. We further propose the\nuse of this predictive system in a smarter request routing mechanism to\nminimize the service interruption during handovers between fog nodes and to\nanticipate areas of low coverage through a series of experiments and test the\nmodels' performance on a test set. \n\n"}
{"id": "1812.11826", "contents": "Title: UAV Base Station Location Optimization for Next Generation Wireless\n  Networks: Overview and Future Research Directions Abstract: Unmanned aerial vehicles mounted base stations (UAV-BSs) are expected to\nbecome one of the significant components of the Next Generation Wireless\nNetworks (NGWNs). Rapid deployment, mobility, higher chances of unobstructed\npropagation path, and flexibility features of UAV-BSs have attracted\nsignificant attention. Despite, potentially, high gains brought by UAV-BSs in\nNGWNs, many challenges are also introduced by them. Optimal location assignment\nto UAV-BSs, arguably, is the most widely investigated problem in the literature\non UAV-BSs in NGWNs. This paper presents a comprehensive survey of the\nliterature on the location optimization of UAV-BSs in NGWNs. A generic\noptimization framework through a universal Mixed Integer Non-Linear Programming\n(MINLP) formulation is constructed and the specifications of its constituents\nare elaborated. The generic problem is classified into a novel taxonomy. Due to\nthe highly challenging nature of the optimization problem a range of solutions\nare adopted in the literature which are also covered under the aforementioned\nclassification. Furthermore, future research directions on UAV-BS location\noptimization in 5G and beyond non-terrestrial aerial communication systems are\ndiscussed. \n\n"}
{"id": "1901.00406", "contents": "Title: Delay and Power Tradeoff with Consideration of Caching Capabilities in\n  Dense Wireless Networks Abstract: Enabling caching capabilities in dense small cell networks (DSCNs) has a\ndirect impact on file delivery delay and power consumption. Most existing work\nstudied these two performance metrics separately in cache-enabled DSCNs.\nHowever, file delivery delay and power consumption are coupled with each other\nand cannot be minimized simultaneously. In this paper, we investigate the\noptimal tradoff between these two performance metrics. Firstly, we formulate\nthe joint file delivery delay and power consumption optimization (JDPO) problem\nwhere power control, user association and file placement are jointly\nconsidered. Then we convert it to a form that can be handled by Generalized\nBenders Decomposition (GDB). with GDB, we decompose the converted JDPO problem\ninto two smaller problems, i.e., primal problem related to power control and\nmaster problem related to user association and file placement. An iterative\nalgorithm is proposed and proved to be $\\epsilon$-optimal, in which the primal\nproblem and master problem are solved iteratively to approach the optimal\nsolution. To further reduce the complexity of the master problem, an\naccelerated algorithm based on semi-definite relaxation is proposed. Finally,\nthe simulation results demonstrate that the proposed algorithm can approach the\noptimal tradeoff between file delivery delay and power consumption. \n\n"}
{"id": "1901.00963", "contents": "Title: Integrating Sub-6 GHz and Millimeter Wave to Combat Blockage:\n  Delay-Optimal Scheduling Abstract: Millimeter wave (mmWave) technologies have the potential to achieve very high\ndata rates, but suffer from intermittent connectivity. In this paper, we\nprovision an architecture to integrate sub-6 GHz and mmWave technologies, where\nwe incorporate the sub-6 GHz interface as a fallback data transfer mechanism to\ncombat blockage and intermittent connectivity of the mmWave communications. To\nthis end, we investigate the problem of scheduling data packets across the\nmmWave and sub-6 GHz interfaces such that the average delay of system is\nminimized. This problem can be formulated as Markov Decision Process. We first\ninvestigate the problem of discounted delay minimization, and prove that the\noptimal policy is of the threshold-type, i.e., data packets should always be\nrouted to the mmWave interface as long as the number of packets in the system\nis smaller than a threshold. Then, we show that the results of the discounted\ndelay problem hold for the average delay problem as well. Through numerical\nresults, we demonstrate that under heavy traffic, integrating sub-6 GHz with\nmmWave can reduce the average delay by up to 70%. Further, our scheduling\npolicy substantially reduces the delay over the celebrated MaxWeight policy. \n\n"}
{"id": "1901.01443", "contents": "Title: Towards Secure Slicing: Using Slice Isolation to Mitigate DDoS Attacks\n  on 5G Core Network Slices Abstract: In this paper, we propose a solution to proactively mitigate Distributed\nDenial-of-Service attacks in 5G core network slicing using slice isolation.\nNetwork slicing is one of the key technologies that allow 5G networks to offer\ndedicated resources to different industries (services). However, a Distributed\nDenial-of-Service attack could severely impact the performance and availability\nof the slices as they could share the same physical resources in a multi-tenant\nvirtualized networking infrastructure. Slice isolation is an essential\nrequirement for 5G network slicing.\n  In this paper, we use network isolation to tackle the challenging problem of\nDistributed Denial-of-Service attacks in 5G network slicing. We propose the use\nof a mathematical model that can provide on-demand slice isolation as well as\nguarantee end-to-end delay for 5G core network slices. We evaluate the proposed\nwork with a mix of simulation and experimental work. Our results show that the\nproposed isolation could mitigate Distributed Denial-of-Service attacks as well\nas increase the availability of the slices. We believe this work will encourage\nfurther research in securing 5G network slicing. \n\n"}
{"id": "1901.01863", "contents": "Title: Beyond socket options: making the Linux TCP stack truly extensible Abstract: The Transmission Control Protocol (TCP) is one of the most important\nprotocols in today's Internet. Its specification and implementations have been\nrefined for almost forty years. The Linux TCP stack is one of the most widely\nused TCP stacks given its utilisation on servers and Android smartphones and\ntablets. However, TCP and its implementations evolve very slowly. In this\npaper, we demonstrate how to leverage the eBPF virtual machine that is part of\nthe recent versions of the Linux kernel to make the TCP stack easier to extend.\n  We demonstrate a variety of use cases where the eBPF code is injected inside\na running kernel to update or tune the TCP implementation. We first implement\nthe TCP User Timeout Option. Then we propose a new option that enables a client\nto request a server to use a specific congestion control scheme. Our third\nextension is a TCP option that sets the initial congestion window. We then\ndemonstrate how eBPF code can be used to tune the acknowledgment strategy. \n\n"}
{"id": "1901.01976", "contents": "Title: The interconnected wealth of nations: Shock propagation on global\n  trade-investment multiplex networks Abstract: The increasing integration of world economies, which organize in complex\nmultilayer networks of interactions, is one of the critical factors for the\nglobal propagation of economic crises. We adopt the network science approach to\nquantify shock propagation on the global trade-investment multiplex network. To\nthis aim, we propose a model that couples a Susceptible-Infected-Recovered\nepidemic spreading dynamics, describing how economic distress propagates\nbetween connected countries, with an internal contagion mechanism, describing\nthe spreading of such economic distress within a given country. At the local\nlevel, we find that the interplay between trade and financial interactions\ninfluences the vulnerabilities of countries to shocks. At the large scale, we\nfind a simple linear relation between the relative magnitude of a shock in a\ncountry and its global impact on the whole economic system, albeit the strength\nof internal contagion is country-dependent and the intercountry propagation\ndynamics is non-linear. Interestingly, this systemic impact can be predicted on\nthe basis of intra-layer and inter-layer scale factors that we name network\nmultipliers, that are independent of the magnitude of the initial shock. Our\nmodel sets-up a quantitative framework to stress-test the robustness of\nindividual countries and of the world economy to propagating crashes. \n\n"}
{"id": "1901.02613", "contents": "Title: Dynamic Mobility-Aware Interference Avoidance for Aerial Base Stations\n  in Cognitive Radio Networks Abstract: Aerial base station (ABS) is a promising solution for public safety as it can\nbe deployed in coexistence with cellular networks to form a temporary\ncommunication network. However, the interference from the primary cellular\nnetwork may severely degrade the performance of an ABS network. With this\nconsideration, an adaptive dynamic interference avoidance scheme is proposed in\nthis work for ABSs coexisting with a primary network. In the proposed scheme,\nthe mobile ABSs can reconfigure their locations to mitigate the interference\nfrom the primary network, so as to better relay the data from the designated\nsource(s) to destination(s). To this end, the single/multi-commodity maximum\nflow problems are formulated and the weighted Cheeger constant is adopted as a\ncriterion to improve the maximum flow of the ABS network. In addition, a\ndistributed algorithm is proposed to compute the optimal ABS moving directions.\nMoreover, the trade-off between the maximum flow and the shortest path\ntrajectories is investigated and an energy-efficient approach is developed as\nwell. Simulation results show that the proposed approach is effective in\nimproving the maximum network flow and the energy-efficient approach can save\nup to 39% of the energy for the ABSs with marginal degradation in the maximum\nnetwork flow. \n\n"}
{"id": "1901.02636", "contents": "Title: On the Robustness of Distributed Computing Networks Abstract: Traffic flows in a distributed computing network require both transmission\nand processing, and can be interdicted by removing either communication or\ncomputation resources. We study the robustness of a distributed computing\nnetwork under the failures of communication links and computation nodes. We\ndefine cut metrics that measure the connectivity, and show a non-zero gap\nbetween the maximum flow and the minimum cut. Moreover, we study a network flow\ninterdiction problem that minimizes the maximum flow by removing communication\nand computation resources within a given budget. We develop mathematical\nprograms to compute the optimal interdiction, and polynomial-time approximation\nalgorithms that achieve near-optimal interdiction in simulation. \n\n"}
{"id": "1901.02691", "contents": "Title: The Arrival of News and Return Jumps in Stock Markets: A Nonparametric\n  Approach Abstract: This paper introduces a non-parametric framework to statistically examine how\nnews events, such as company or macroeconomic announcements, contribute to the\npre- and post-event jump dynamics of stock prices under the intraday\nseasonality of the news and jumps. We demonstrate our framework, which has\nseveral advantages over the existing methods, by using data for i) the S&P 500\nindex ETF, SPY, with macroeconomic announcements and ii) Nasdaq Nordic\nLarge-Cap stocks with scheduled and non-scheduled company announcements. We\nprovide strong evidence that non-scheduled company announcements and some\nmacroeconomic announcements contribute jumps that follow the releases and also\nsome evidence for pre-jumps that precede the scheduled arrivals of public\ninformation, which may indicate non-gradual information leakage. Especially\ninterim reports of Nordic large-cap companies are found containing important\ninformation to yield jumps in stock prices. Additionally, our results show that\nreleases of unexpected information are not reacted to uniformly across Nasdaq\nNordic markets, even if they are jointly operated and are based on the same\nexchange rules. \n\n"}
{"id": "1901.03239", "contents": "Title: 6G: The Next Frontier Abstract: The current development of 5G networks represents a breakthrough in the\ndesign of communication networks, for its ability to provide a single platform\nenabling a variety of different services, from enhanced mobile broadband\ncommunications, automated driving, Internet-of-Things, with its huge number of\nconnected devices, etc. Nevertheless, looking at the current development of\ntechnologies and new services, it is already possible to envision the need to\nmove beyond 5G with a new architecture incorporating new services and\ntechnologies. The goal of this paper is to motivate the need to move to a sixth\ngeneration (6G) of mobile communication networks, starting from a gap analysis\nof 5G, and predicting a new synthesis of near future services, like hologram\ninterfaces, ambient sensing intelligence, a pervasive introduction of\nartificial intelligence and the incorporation of technologies, like TeraHertz\n(THz) or Visible Light Communications (VLC), 3-dimensional coverage. \n\n"}
{"id": "1901.03931", "contents": "Title: Joint Placement and Allocation of VNF Nodes with Budget and Capacity\n  Constraints Abstract: With the advent of Network Function Virtualization (NFV), network services\nthat traditionally run on proprietary dedicated hardware can now be realized\nusing Virtual Network Functions (VNFs) that are hosted on general-purpose\ncommodity hardware. This new network paradigm offers a great flexibility to\nInternet service providers (ISPs) for efficiently operating their networks\n(collecting network statistics, enforcing management policies, etc.). However,\nintroducing NFV requires an investment to deploy VNFs at certain network nodes\n(called VNF-nodes), which has to account for practical constraints such as the\ndeployment budget and the VNF-node capacity. To that end, it is important to\ndesign a joint VNF-nodes placement and capacity allocation algorithm that can\nmaximize the total amount of network flows that are fully processed by the\nVNF-nodes while respecting such practical constraints. In contrast to most\nprior work that often neglects either the budget constraint or the capacity\nconstraint, we explicitly consider both of them. We prove that accounting for\nthese constraints introduces several new challenges. Specifically, we prove\nthat the studied problem is not only NP-hard but also non-submodular. To\naddress these challenges, we introduce a novel relaxation method such that the\nobjective function of the relaxed placement subproblem becomes submodular.\nLeveraging this useful submodular property, we propose two algorithms that\nachieve an approximation ratio of $\\frac{1}{2}(1-1/e)$ and $\\frac{1}{3}(1-1/e)$\nfor the original non-relaxed problem, respectively. Finally, we corroborate the\neffectiveness of the proposed algorithms through extensive evaluations using\ntrace-driven simulations. \n\n"}
{"id": "1901.05558", "contents": "Title: Framework for a Perceptive Mobile Network using Joint Communication and\n  Radar Sensing Abstract: In this paper, we develop a framework for a novel perceptive mobile/cellular\nnetwork that integrates radar sensing function into the mobile communication\nnetwork. We propose a unified system platform that enables downlink and uplink\nsensing, sharing the same transmitted signals with communications. We aim to\ntackle the fundamental sensing parameter estimation problem in perceptive\nmobile networks, by addressing two key challenges associated with sophisticated\nmobile signals and rich multipath in mobile networks. To extract sensing\nparameters from orthogonal frequency division multiple access (OFDMA) and\nspatial division multiple access (SDMA) communication signals, we propose two\napproaches to formulate it to problems that can be solved by compressive\nsensing techniques. Most sensing algorithms have limits on the number of\nmultipath signals for their inputs. To reduce the multipath signals, as well as\nremoving unwanted clutter signals, we propose a background subtraction method\nbased on simple recursive computation, and provide a closed-form expression for\nperformance characterization. The effectiveness of these methods is validated\nin simulations. \n\n"}
{"id": "1901.05741", "contents": "Title: RepChain: A Reputation-based Secure, Fast and High Incentive Blockchain\n  System via Sharding Abstract: In today's blockchain system, designing a secure and high throughput\nblockchain on par with a centralized payment system is a difficult task.\nSharding is one of the most worthwhile emerging technologies for improving the\nsystem throughput while maintain high security level. However, previous\nsharding related designs have two main limitations: Firstly, the throughput of\ntheir random-based sharding system is not high enough as they did not leverage\nthe heterogeneity among validators. Secondly, to design an incentive mechanism\nto promote cooperation could incur a huge overhead on their system. In this\npaper, we propose RepChain, a reputation-based secure and fast blockchain\nsystem via sharding, which also provides high incentive to stimulate node\ncooperation. RepChain utilizes reputation to explicitly characterize the\nheterogeneity among the validators and lay the foundation for the incentive\nmechanism. We propose a new double-chain architecture which includes\ntransaction chain and reputation chain. For transaction chain, a Raft-based\nsynchronous consensus that can achieve high throughput has been presented. For\nreputation chain, the synchronous Byzantine fault tolerance that combines\ncollective signing has been utilized to achieve a consensus on both reputation\nscore and the related transaction blocks. It supports a high throughput\ntransaction chain with moderate generation speed. Moreover, we propose a\nreputation-based sharding and leader selection scheme. To analyze the security\nof RepChain, we propose a recursive formula to calculate the epoch security\nwithin only O(km^2) time. Furthermore, we implement and evaluate RepChain on\nthe Amazon Web Service platform. The results show our solution can enhance both\nthroughout and security level of the existing sharding-based blockchain system. \n\n"}
{"id": "1901.06637", "contents": "Title: UAV Communications for 5G and Beyond: Recent Advances and Future Trends Abstract: Providing ubiquitous connectivity to diverse device types is the key\nchallenge for 5G and beyond 5G (B5G). Unmanned aerial vehicles (UAVs) are\nexpected to be an important component of the upcoming wireless networks that\ncan potentially facilitate wireless broadcast and support high rate\ntransmissions. Compared to the communications with fixed infrastructure, UAV\nhas salient attributes, such as flexible deployment, strong line-of-sight (LoS)\nconnection links, and additional design degrees of freedom with the controlled\nmobility. In this paper, a comprehensive survey on UAV communication towards\n5G/B5G wireless networks is presented. We first briefly introduce essential\nbackground and the space-air-ground integrated networks, as well as discuss\nrelated research challenges faced by the emerging integrated network\narchitecture. We then provide an exhaustive review of various 5G techniques\nbased on UAV platforms, which we categorize by different domains including\nphysical layer, network layer, and joint communication, computing and caching.\nIn addition, a great number of open research problems are outlined and\nidentified as possible future research directions. \n\n"}
{"id": "1901.07241", "contents": "Title: The spread of a financial virus through Europe and beyond Abstract: We analyse the importance of international relations between countries on the\nfinancial stability. The contagion effect in the network is tested by\nimplementing an epidemiological model, comprising a number of European\ncountries and using bilateral data on foreign claims between them. Banking\nstatistics of consolidated foreign claims on ultimate risk bases, obtained from\nthe Banks of International Settlements, allow us to measure the exposure of\ncontagion spreading from a particular country to the other national banking\nsystems. We show that the financial system of some countries, experiencing the\ndebt crisis, is a source of global systemic risk because they threaten the\nstability of a larger system, being a global threat to the intoxication of the\nworld economy and resulting in what we call a `financial virus'. Illustrative\nsimulations were done in the NetLogo multi-agent programmable modelling\nenvironment and in MATLAB. \n\n"}
{"id": "1901.07768", "contents": "Title: Cooperation Speeds Surfing: Use Co-Bandit! Abstract: In this paper, we explore the benefit of cooperation in adversarial bandit\nsettings. As a motivating example, we consider the problem of wireless network\nselection. Mobile devices are often required to choose the right network to\nassociate with for optimal performance, which is non-trivial. The excellent\ntheoretical properties of EXP3, a leading multi-armed bandit algorithm, suggest\nthat it should work well for this type of problem. Yet, it performs poorly in\npractice. A major limitation is its slow rate of stabilization. Bandit-style\nalgorithms perform better when global knowledge is available, i.e., when\ndevices receive feedback about all networks after each selection. But,\nunfortunately, communicating full information to all devices is expensive.\nTherefore, we address the question of how much information is adequate to\nachieve better performance. We propose Co-Bandit, a novel cooperative bandit\napproach, that allows devices to occasionally share their observations and\nforward feedback received from neighbors; hence, feedback may be received with\na delay. Devices perform network selection based on their own observation and\nfeedback from neighbors. As such, they speed up each other's rate of learning.\nWe prove that Co-Bandit is regret-minimizing and retains the convergence\nproperty of multiplicative weight update algorithms with full information.\nThrough simulation, we show that a very small amount of information, even with\na delay, is adequate to nudge each other to select the right network and yield\nsignificantly faster stabilization at the optimal state (about 630x faster than\nEXP3). \n\n"}
{"id": "1901.08113", "contents": "Title: Unveiling the potential of Graph Neural Networks for network modeling\n  and optimization in SDN Abstract: Network modeling is a critical component for building self-driving\nSoftware-Defined Networks, particularly to find optimal routing schemes that\nmeet the goals set by administrators. However, existing modeling techniques do\nnot meet the requirements to provide accurate estimations of relevant\nperformance metrics such as delay and jitter. In this paper we propose a novel\nGraph Neural Network (GNN) model able to understand the complex relationship\nbetween topology, routing and input traffic to produce accurate estimates of\nthe per-source/destination pair mean delay and jitter. GNN are tailored to\nlearn and model information structured as graphs and as a result, our model is\nable to generalize over arbitrary topologies, routing schemes and variable\ntraffic intensity. In the paper we show that our model provides accurate\nestimates of delay and jitter (worst case $R^2=0.86$) when testing against\ntopologies, routing and traffic not seen during training. In addition, we\npresent the potential of the model for network operation by presenting several\nuse-cases that show its effective use in per-source/destination pair\ndelay/jitter routing optimization and its generalization capabilities by\nreasoning in topologies and routing schemes not seen during training. \n\n"}
{"id": "1901.08326", "contents": "Title: A stack-vector routing protocol for automatic tunneling Abstract: In a network, a tunnel is a part of a path where a protocol is encapsulated\nin another one. A tunnel starts with an encapsulation and ends with the\ncorresponding decapsulation. Several tunnels can be nested at some stage,\nforming a protocol stack. Tunneling is very important nowadays and it is\ninvolved in several tasks: IPv4/IPv6 transition, VPNs, security (IPsec, onion\nrouting), etc. However, tunnel establishment is mainly performed manually or by\nscript, which present obvious scalability issues. Some works attempt to\nautomate a part of the process (e.g., TSP, ISATAP, etc.). However, the\ndetermination of the tunnel(s) endpoints is not fully automated, especially in\nthe case of an arbitrary number of nested tunnels. The lack of routing\nprotocols performing automatic tunneling is due to the unavailability of path\ncomputation algorithms taking into account encapsulations and decapsulations.\nThere is a polynomial centralized algorithm to perform the task. However, to\nthe best of our knowledge, no fully distributed path computation algorithm is\nknown. Here, we propose the first fully distributed algorithm for path\ncomputation with automatic tunneling, i.e., taking into account encapsulation,\ndecapsulation and conversion of protocols. Our algorithm is a generalization of\nthe distributed Bellman-Ford algorithm, where the distance vector is replaced\nby a protocol stack vector. This allows to know how to route a packet with some\nprotocol stack. We prove that the messages size of our algorithm is polynomial,\neven if the shortest path can be of exponential length. We also prove that the\nalgorithm converges after a polynomial number of steps in a synchronized\nsetting. We adapt our algorithm into a proto-protocol for routing with\nautomatic tunneling and we show its efficiency through simulations. \n\n"}
{"id": "1901.08329", "contents": "Title: When Machine Learning Meets Big Data: A Wireless Communication\n  Perspective Abstract: We have witnessed an exponential growth in commercial data services, which\nhas lead to the 'big data era'. Machine learning, as one of the most promising\nartificial intelligence tools of analyzing the deluge of data, has been invoked\nin many research areas both in academia and industry. The aim of this article\nis twin-fold. Firstly, we briefly review big data analysis and machine\nlearning, along with their potential applications in next-generation wireless\nnetworks. The second goal is to invoke big data analysis to predict the\nrequirements of mobile users and to exploit it for improving the performance of\n\"social network-aware wireless\". More particularly, a unified big data aided\nmachine learning framework is proposed, which consists of feature extraction,\ndata modeling and prediction/online refinement. The main benefits of the\nproposed framework are that by relying on big data which reflects both the\nspectral and other challenging requirements of the users, we can refine the\nmotivation, problem formulations and methodology of powerful machine learning\nalgorithms in the context of wireless networks. In order to characterize the\nefficiency of the proposed framework, a pair of intelligent practical\napplications are provided as case studies: 1) To predict the positioning of\ndrone-mounted areal base stations (BSs) according to the specific tele-traffic\nrequirements by gleaning valuable data from social networks. 2) To predict the\ncontent caching requirements of BSs according to the users' preferences by\nmining data from social networks. Finally, open research opportunities are\nidentified for motivating future investigations. \n\n"}
{"id": "1901.08919", "contents": "Title: Wireless Broadcast with short labelling Abstract: In this paper, we study the broadcast problem in wireless networks when the\nbroadcast is helped by a labelling scheme. We focus on two variants of\nbroadcast: broadcast without acknowledgment (i.e. the initiator of the\nbroadcast is not notified at the end of broadcast) and broadcast with\nacknowledgment. Our contribution is threefold. First, we improve in terms of\nmemory complexity a recent labelling-based broadcast scheme with acknowledgment\ndesigned for arbitrary networks.Second, we propose label optimal broadcast\nalgorithms in Level Separable Networks (a class of networks issued from recent\nstudies in Wireless Body Area Networks). In this class of networks we propose\nan acknowledgment-free broadcast strategy using 1-bit labels and broadcast with\nacknowledgment using 2-bits labels. In the class of level-separable networks,\nour algorithms finish within 2D rounds, where D is the eccentricity of the\nbroadcast initiator. Interestingly, the time complexity of broadcast in the\ncase of level-separable networks does not depend on the size of the network but\nrather on the initiator eccentricity which makes this class of graphs\ninteresting for further investigation. Finally, we study the hardness of\ndetermining that a graph is level separable. Our study shows that even though\nchecking that a separation is a level separation can be done in polynomial\ntime, determining that a graph has the level separable property is NP-complete.\nThis result opens interesting independent research directions. \n\n"}
{"id": "1901.08946", "contents": "Title: Joint Service Placement and Request Routing in Multi-cell Mobile Edge\n  Computing Networks Abstract: The proliferation of innovative mobile services such as augmented reality,\nnetworked gaming, and autonomous driving has spurred a growing need for\nlow-latency access to computing resources that cannot be met solely by existing\ncentralized cloud systems. Mobile Edge Computing (MEC) is expected to be an\neffective solution to meet the demand for low-latency services by enabling the\nexecution of computing tasks at the network-periphery, in proximity to\nend-users. While a number of recent studies have addressed the problem of\ndetermining the execution of service tasks and the routing of user requests to\ncorresponding edge servers, the focus has primarily been on the efficient\nutilization of computing resources, neglecting the fact that non-trivial\namounts of data need to be stored to enable service execution, and that many\nemerging services exhibit asymmetric bandwidth requirements. To fill this gap,\nwe study the joint optimization of service placement and request routing in\nMEC-enabled multi-cell networks with multidimensional\n(storage-computation-communication) constraints. We show that this problem\ngeneralizes several problems in literature and propose an algorithm that\nachieves close-to-optimal performance using randomized rounding. Evaluation\nresults demonstrate that our approach can effectively utilize the available\nresources to maximize the number of requests served by low-latency edge cloud\nservers. \n\n"}
{"id": "1901.09094", "contents": "Title: Derandomized Load Balancing using Random Walks on Expander Graphs Abstract: In a computing center with a huge amount of machines, when a job arrives, a\ndispatcher need to decide which machine to route this job to based on limited\ninformation. A classical method, called the power-of-$d$ choices algorithm is\nto pick $d$ servers independently at random and dispatch the job to the least\nloaded server among the $d$ servers. In this paper, we analyze a low-randomness\nvariant of this dispatching scheme, where $d$ queues are sampled through $d$\nindependent non-backtracking random walks on a $k$-regular graph $G$. Under\ncertain assumptions of the graph $G$ we show that under this scheme, the\ndynamics of the queuing system converges to the same deterministic ordinary\ndifferential equation (ODE) for the power-of-$d$ choices scheme. We also show\nthat the system is stable under the proposed scheme, and the stationary\ndistribution of the system converges to the fixed point of the ODE. \n\n"}
{"id": "1901.09129", "contents": "Title: Optimal $k$-Coverage Charging Problem Abstract: Wireless rechargeable sensor networks, consisting of sensor nodes with\nrechargeable batteries and mobile chargers to replenish their batteries, have\ngradually become a promising solution to the bottleneck of energy limitation\nthat hinders the wide deployment of wireless sensor networks (WSN). In this\npaper, we focus on the mobile charger scheduling and path optimization scenario\nin which the $k$-coverage ability of a network system needs to be maintained.\nWe formulate the optimal $k$-coverage charging problem of finding a feasible\npath for a mobile charger to charge a set of sensor nodes within their\nestimated charging deadlines under the constraint of maintaining the\n$k$-coverage ability of the network system, with an objective of minimizing the\nenergy consumption on traveling per tour. We show the hardness of the problem\nthat even finding a feasible path for the trivial case of the problem is an\nNP-complete one.\n  We model the problem and apply dynamic programming to design an algorithm\nthat finds an exact solution to the optimal $k$-coverage charging problem.\nHowever, the computational complexity is still prohibitive for large size\nnetworks. We then introduce Deep Q-learning, a reinforcement learning algorithm\nto tackle the problem. \n\n"}
{"id": "1901.09177", "contents": "Title: An Optimized BBR for Multipath Real Time Video Streaming Abstract: The multipath transmission scheme can work as an effective way to provide\nbetter quality of experiments to end users. Two key research points in the\nmultipath real time video transmission context are congestion control and\npacket scheduling. As Utility maximization theory shows, to provide better\nsatisfaction to end users is to provide higher throughput and lower\ntransmission delay. The congestion control is responsible to converge to the\nmaximum available bandwidth and avoid leading the network into congestion. A\ndelay response BBR (Delay-BBR) algorithm optimized for real time video\ntransmission is proposed, and the main idea is to reduce sending rate when the\nlink delay has exceeded certain threshold to let the intermediate routers drain\nthe occupied buffer. It can achieve better transmission delay and lower packet\nloss rate compared with QUIC-BBR and WebRTC-BBR by experiment. And a packet\nscheduling algorithm induced from Utility maximization theory works on top of\nthe congestion control algorithm is tested and achieves lower frame delivery\ndelay further compared with benchmark algorithms. \n\n"}
{"id": "1901.09247", "contents": "Title: Spectrum Data Poisoning with Adversarial Deep Learning Abstract: Machine learning has been widely applied in wireless communications. However,\nthe security aspects of machine learning in wireless applications have not been\nwell understood yet. We consider the case that a cognitive transmitter senses\nthe spectrum and transmits on idle channels determined by a machine learning\nalgorithm. We present an adversarial machine learning approach to launch a\nspectrum data poisoning attack by inferring the transmitter's behavior and\nattempting to falsify the spectrum sensing data over the air. For that purpose,\nthe adversary transmits for a short period of time when the channel is idle to\nmanipulate the input for the decision mechanism of the transmitter. The\ncognitive engine at the transmitter is a deep neural network model that\npredicts idle channels with minimum sensing error for data transmissions. The\ntransmitter collects spectrum sensing data and uses it as the input to its\nmachine learning algorithm. In the meantime, the adversary builds a cognitive\nengine using another deep neural network model to predict when the transmitter\nwill have a successful transmission based on its spectrum sensing data. The\nadversary then performs the over-the-air spectrum data poisoning attack, which\naims to change the channel occupancy status from idle to busy when the\ntransmitter is sensing, so that the transmitter is fooled into making incorrect\ntransmit decisions. This attack is more energy efficient and harder to detect\ncompared to jamming of data transmissions. We show that this attack is very\neffective and reduces the throughput of the transmitter substantially. \n\n"}
{"id": "cond-mat/0001353", "contents": "Title: \"Thermometers\" of Speculative Frenzy Abstract: Establishing unambiguously the existence of speculative bubbles is an\non-going controversy complicated by the need of defining a model of fundamental\nprices. Here, we present a novel empirical method which bypasses all the\ndifficulties of the previous approaches by monitoring external indicators of an\nanomalously growing interest in the public at times of bubbles. From the\ndefinition of a bubble as a self-fulfilling reinforcing price change, we\nidentify indicators of a possible self-reinforcing imitation between agents in\nthe market. We show that during the build-up phase of a bubble, there is a\ngrowing interest in the public for the commodity in question, whether it\nconsists in stocks, diamonds or coins. That interest can be estimated through\ndifferent indicators: increase in the number of books published on the topic,\nincrease in the subscriptions to specialized journals. Moreover, the well-known\nempirical rule according to which the volume of sales is growing during a bull\nmarket finds a natural interpretation in this framework: sales increases in\nfact reveal and pinpoint the progress of the bubble's diffusion throughout\nsociety. We also present a simple model of rational expectation which maps\nexactly onto the Ising model on a random graph. The indicators are then\ninterpreted as ``thermometers'', measuring the balance between idiosyncratic\ninformation (noise temperature) and imitation (coupling) strength. In this\ncontext, bubbles are interpreted as low or critical temperature phases, where\nthe imitation strength carries market prices up essentially independently of\nfundamentals. Contrary to the naive conception of a bubble and a crash as times\nof disorder, on the contrary, we show that bubbles and crashes are times where\nthe concensus is too strong. \n\n"}
{"id": "cond-mat/0005416", "contents": "Title: Market Ecology, Pareto Wealth Distribution and Leptokurtic Returns in\n  Microscopic Simulation of the LLS Stock Market Model Abstract: The LLS stock market model is a model of heterogeneous quasi-rational\ninvestors operating in a complex environment about which they have incomplete\ninformation. We review the main features of this model and several of its\nextensions. We study the effects of investor heterogeneity and show that\npredation, competition, or symbiosis may occur between different investor\npopulations. The dynamics of the LLS model lead to the empirically observed\nPareto wealth distribution. Many properties observed in actual markets appear\nas natural consequences of the LLS dynamics: truncated Levy distribution of\nshort-term returns, excess volatility, a return autocorrelation \"U-shape\"\npattern, and a positive correlation between volume and absolute returns. \n\n"}
{"id": "cond-mat/0011373", "contents": "Title: Universal Structure of the Personal Income Distribution Abstract: We investigate the Japanese personal income distribution in the high income\nrange over the 112 years 1887-1998, and that in the middle income range over\nthe 44 years 1955-98. It is observed that the distribution pattern of the\nlognormal with power law tail is the universal structure. However the indexes\nspecifying the distribution differ from year to year. One of the index\ncharacterizing the distribution is the mean value of the lognormal\ndistribution; the mean income in the middle income range. It is found that this\nvalue correlates linearly with the Gross Domestic Product (GDP). To clarify the\ntemporal change of the equality or inequality of the distribution, we analyze\nPareto and Gibrat indexes, which characterize the distribution in the high\nincome range and that in the middle income range respectively. It is found for\nsome years that there is no correlation between the high income and the middle\nincome. It is also shown that the mean value of Pareto index equals to 2, and\nthe change of this index is effected by the change of the asset price. From\nthese analysis we derive four constraints that must be satisfied by\nmathematical models. \n\n"}
{"id": "cond-mat/0103170", "contents": "Title: Finite market size as a source of extreme wealth inequality and market\n  instability Abstract: We study the finite-size effects in some scaling systems, and show that the\nfinite number of agents N leads to a cut-off in the upper value of the Pareto\nlaw for the relative individual wealth. The exponent $\\alpha$ of the Pareto law\nobtained in stochastic multiplicative market models is crucially affected by\nthe fact that N is always finite in real systems. We show that any finite value\nof N leads to properties which can differ crucially from the naive theoretical\nresults obtained by assuming an infinite N. In particular, finite N may cause\nin the absence of an appropriate social policy extreme wealth inequality\n$\\alpha < 1$ and market instability. \n\n"}
{"id": "cond-mat/0103544", "contents": "Title: Exponential and power-law probability distributions of wealth and income\n  in the United Kingdom and the United States Abstract: We present the data on wealth and income distributions in the United Kingdom,\nas well as on the income distributions in the individual states of the USA. In\nall of these data, we find that the great majority of population is described\nby an exponential distribution, whereas the high-end tail follows a power law.\nThe distributions are characterized by a dimensional scale analogous to\ntemperature. The values of temperature are determined for the UK and the USA,\nas well as for the individual states of the USA. \n\n"}
{"id": "cond-mat/0104127", "contents": "Title: Crashes : symptoms, diagnoses and remedies Abstract: A brief historical perspective is first given concerning financial crashes, -\nfrom the 17th till the 20th century. In modern times, it seems that log\nperiodic oscillations are found before crashes in several financial indices.\nThe same is found in sand pile avalanches on Sierpinski gaskets. A discussion\npertains to the after shock period with illustrations from the DAX index. The\nfactual financial observations and the laboratory ones allow us some conjecture\non symptoms and remedies for discussing financial crashes along econophysics\nlines. \n\n"}
{"id": "cond-mat/0106096", "contents": "Title: Statistical mechanics of complex networks Abstract: Complex networks describe a wide range of systems in nature and society, much\nquoted examples including the cell, a network of chemicals linked by chemical\nreactions, or the Internet, a network of routers and computers connected by\nphysical links. While traditionally these systems were modeled as random\ngraphs, it is increasingly recognized that the topology and evolution of real\nnetworks is governed by robust organizing principles. Here we review the recent\nadvances in the field of complex networks, focusing on the statistical\nmechanics of network topology and dynamics. After reviewing the empirical data\nthat motivated the recent interest in networks, we discuss the main models and\nanalytical tools, covering random graphs, small-world and scale-free networks,\nas well as the interplay between topology and the network's robustness against\nfailures and attacks. \n\n"}
{"id": "cond-mat/0109203", "contents": "Title: Risk aversion in economic transactions Abstract: Most people are risk-averse (risk-seeking) when they expect to gain (lose).\nBased on a generalization of ``expected utility theory'' which takes this into\naccount, we introduce an automaton mimicking the dynamics of economic\noperations. Each operator is characterized by a parameter q which gauges\npeople's attitude under risky choices; this index q is in fact the entropic one\nwhich plays a central role in nonextensive statistical mechanics. Different\nlong term patterns of average asset redistribution are observed according to\nthe distribution of parameter q (chosen once for ever for each operator) and\nthe rules (e.g., the probabilities involved in the gamble and the indebtedness\nrestrictions) governing the values that are exchanged in the transactions.\nAnalytical and numerical results are discussed in terms of how the sensitivity\nto risk affects the dynamics of economic transactions. \n\n"}
{"id": "cond-mat/0110201", "contents": "Title: Stability of money: Phase transitions in an Ising economy Abstract: The stability of money value is an important requisite for a functioning\neconomy, yet it critically depends on the actions of participants in the market\nthemselves. Here we model the value of money as a dynamical variable that\nresults from trading between agents. The basic trading scenario can be recast\ninto an Ising type spin model and is studied on the hierarchical network\nstructure of a Cayley tree. We solve this model analytically and observe a\nphase transition between a one state phase, always allowing for a stable money\nvalue, and a two state phase, where an unstable (inflationary) phase occurs.\nThe onset of inflation is discontinuous and follows a first order phase\ntransition. The stable phase provides a parameter region where money value is\nrobust and can be stabilized without fine tuning. \n\n"}
{"id": "cond-mat/0202388", "contents": "Title: Physics of Personal Income Abstract: We report empirical studies on the personal income distribution, and clarify\nthat the distribution pattern of the lognormal with power law tail is the\nuniversal structure. We analyze the temporal change of Pareto index and Gibrat\nindex to investigate the change of the inequality of the income distribution.\nIn addition some mathematical models which are proposed to explain the power\nlaw distribution are reviewed. \n\n"}
{"id": "cond-mat/0203227", "contents": "Title: Ising Model on Networks with an Arbitrary Distribution of Connections Abstract: We find the exact critical temperature $T_c$ of the nearest-neighbor\nferromagnetic Ising model on an `equilibrium' random graph with an arbitrary\ndegree distribution $P(k)$. We observe an anomalous behavior of the\nmagnetization, magnetic susceptibility and specific heat, when $P(k)$ is\nfat-tailed, or, loosely speaking, when the fourth moment of the distribution\ndiverges in infinite networks. When the second moment becomes divergent, $T_c$\napproaches infinity, the phase transition is of infinite order, and size effect\nis anomalously strong. \n\n"}
{"id": "cond-mat/0205520", "contents": "Title: Empirical nonextensive laws for the county distribution of total\n  personal income and gross domestic product Abstract: We analyze the cumulative distribution of total personal income of USA\ncounties, and gross domestic product of Brazilian, German and United Kingdom\ncounties, and also of world countries.\n  We verify that generalized exponential distributions, related to nonextensive\nstatistical mechanics, describe almost the whole spectrum of the distributions\n(within acceptable errors), ranging from the low region to the middle region,\nand, in some cases, up to the power-law tail.\n  The analysis over about 30 years (for USA and Brazil) shows a regular pattern\nof the parameters appearing in the present phenomenological approach,\nsuggesting a possible connection between the underlying dynamics of (at least\nsome aspects of) the economy of a country (or of the whole world) and\nnonextensive statistical mechanics.\n  We also introduce two additional examples related to geographical\ndistributions: land areas of counties and land prices, and the same kind of\nequations adjust the data in the whole range of the spectrum. \n\n"}
{"id": "cond-mat/0301289", "contents": "Title: Pareto Law in a Kinetic Model of Market with Random Saving Propensity Abstract: We have numerically simulated the ideal-gas models of trading markets, where\neach agent is identified with a gas molecule and each trading as an elastic or\nmoney-conserving two-body collision. Unlike in the ideal gas, we introduce\n(quenched) saving propensity of the agents, distributed widely between the\nagents ($0 \\le \\lambda < 1$). The system remarkably self-organizes to a\ncritical Pareto distribution of money $P(m) \\sim m^{-(\\nu + 1)}$ with $\\nu\n\\simeq 1$. We analyse the robustness (universality) of the distribution in the\nmodel. We also argue that although the fractional saving ingredient is a bit\nunnatural one in the context of gas models, our model is the simplest so far,\nshowing self-organized criticality, and combines two century-old distributions:\nGibbs (1901) and Pareto (1897) distributions. \n\n"}
{"id": "cond-mat/0302147", "contents": "Title: Ideal Gas-Like Distributions in Economics: Effects of Saving Propensity Abstract: We consider the ideal-gas models of trading markets, where each agent is\nidentified with a gas molecule and each trading as an elastic or\nmoney-conserving (two-body) collision. Unlike in the ideal gas, we introduce\nsaving propensity $\\lambda$ of agents, such that each agent saves a fraction\n$\\lambda$ of its money and trades with the rest. We show the steady-state money\nor wealth distribution in a market is Gibbs-like for $\\lambda=0$, has got a\nnon-vanishing most-probable value for $\\lambda \\ne 0$ and Pareto-like when\n$\\lambda$ is widely distributed among the agents. We compare these results with\nobservations on wealth distributions of various countries. \n\n"}
{"id": "cond-mat/0306496", "contents": "Title: Evidence of Fueling of the 2000 New Economy Bubble by Foreign Capital\n  Inflow: Implications for the Future of the US Economy and its Stock Market Abstract: Previous analyses of a large ensemble of stock markets have demonstrated that\na log-periodic power law (LPPL) behavior of the prices constitutes a qualifying\nsignature of speculative bubbles that often land with a crash. We detect such a\nLPPL signature in the foreign capital inflow during the bubble on the US\nmarkets culminating in March 2000. We detect a weak synchronization and lag\nwith the NASDAQ 100 LPPL pattern. We propose to rationalize these observations\nby the existence of positive feedback loops between market-appreciation /\nincreased-spending / increased-deficit-of-balance-of-payment /\nlarger-foreign-surplus / increased-foreign-capital-inflows and so on. Our\nanalysis suggests that foreign capital inflow have been following rather than\ncausing the bubble. We then combine a macroeconomic analysis of feedback\nprocesses occurring between the economy and the stock market with a technical\nanalysis of more than two hundred years of the DJIA to investigate possible\nscenarios for the future, three years after the end of the bubble and deep into\na bearish regime. We also detect a LPPL accelerating bubble on the EURO against\nthe US dollar and the Japanese Yen. In sum, our analyses is in line with our\nprevious work on the LPPL ``anti-bubble'' representing the bearish market that\nstarted in 2000. \n\n"}
{"id": "cond-mat/0311227", "contents": "Title: Money in Gas-Like Markets: Gibbs and Pareto Laws Abstract: We consider the ideal-gas models of trading markets, where each agent is\nidentified with a gas molecule and each trading as an elastic or\nmoney-conserving (two-body) collision. Unlike in the ideal gas, we introduce\nsaving propensity $\\lambda$ of agents, such that each agent saves a fraction\n$\\lambda$ of its money and trades with the rest. We show the steady-state money\nor wealth distribution in a market is Gibbs-like for $\\lambda=0$, has got a\nnon-vanishing most-probable value for $\\lambda \\ne 0$ and Pareto-like when\n$\\lambda$ is widely distributed among the agents. We compare these results with\nobservations on wealth distributions of various countries. \n\n"}
{"id": "cond-mat/0311235", "contents": "Title: Inelastically scattering particles and wealth distribution in an open\n  economy Abstract: Using the analogy with inelastic granular gasses we introduce a model for\nwealth exchange in society. The dynamics is governed by a kinetic equation,\nwhich allows for self-similar solutions. The scaling function has a power-law\ntail, the exponent being given by a transcendental equation. In the limit of\ncontinuous trading, closed form of the wealth distribution is calculated\nanalytically. \n\n"}
{"id": "cond-mat/0402075", "contents": "Title: A (reactive) lattice-gas approach to economic cycles Abstract: A microscopic approach to macroeconomic features is intended. A model for\nmacroeconomic behavior under heterogeneous spatial economic conditions is\nreviewed. A birth-death lattice gas model taking into account the influence of\nan economic environment on the fitness and concentration evolution of economic\nentities is numerically and analytically examined. The reaction-diffusion model\ncan be also mapped onto a high order logistic map. The role of the selection\npressure along various dynamics with entity diffusion on a square symmetry\nlattice has been studied by Monte-Carlo simulation. The model leads to a sort\nof phase transition for the fitness gap as a function of the selection pressure\nand to cycles. The control parameter is a (scalar) ''business plan''. The\nbusiness plan(s) allows for spin-offs or merging and enterprise survival\nevolution law(s), whence bifurcations, cycles and chaotic behavior. \n\n"}
{"id": "cond-mat/0402143", "contents": "Title: Personal Email Networks: An Effective Anti-Spam Tool Abstract: We provide an automated graph theoretic method for identifying individual\nusers' trusted networks of friends in cyberspace. We routinely use our social\nnetworks to judge the trustworthiness of outsiders, i.e., to decide where to\nbuy our next car, or to find a good mechanic for it. In this work, we show that\nan email user may similarly use his email network, constructed solely from\nsender and recipient information available in the email headers, to distinguish\nbetween unsolicited commercial emails, commonly called \"spam\", and emails\nassociated with his circles of friends. We exploit the properties of social\nnetworks to construct an automated anti-spam tool which processes an individual\nuser's personal email network to simultaneously identify the user's core\ntrusted networks of friends, as well as subnetworks generated by spams. In our\nempirical studies of individual mail boxes, our algorithm classified\napproximately 53% of all emails as spam or non-spam, with 100% accuracy. Some\nof the emails are left unclassified by this network analysis tool. However, one\ncan exploit two of the following useful features. First, it requires no user\nintervention or supervised training; second, it results in no false negatives\ni.e., spam being misclassified as non-spam, or vice versa. We demonstrate that\nthese two features suggest that our algorithm may be used as a platform for a\ncomprehensive solution to the spam problem when used in concert with more\nsophisticated, but more cumbersome, content-based filters. \n\n"}
{"id": "cond-mat/0403045", "contents": "Title: An out-of-equilibrium model of the distributions of wealth Abstract: The distribution of wealth among the members of a society is herein assumed\nto result from two fundamental mechanisms, trade and investment. An empirical\ndistribution of wealth shows an abrupt change between the low-medium range,\nthat may be fitted by a non-monotonic function with an exponential-like tail\nsuch as a Gamma distribution, and the high wealth range, that is well fitted by\na Pareto or inverse power-law function. We demonstrate that an appropriate\ntrade-investment model, depending on three adjustable parameters associated\nwith the total wealth of a society, a social differentiation among agents, and\neconomic volatility referred to as investment can successfully reproduce the\ndistribution of empirical wealth data in the low, medium and high ranges.\nFinally, we provide an economic interpretation of the mechanisms in the model\nand, in particular, we discuss the difference between Classical and\nNeoclassical theories regarding the concepts of {\\it value} and {\\it price}. We\nconsider the importance that out-of-equilibrium trade transactions, where the\nprices differ from values, have in real economic societies. \n\n"}
{"id": "cond-mat/0403070", "contents": "Title: Relations between a typical scale and averages in the breaking of\n  fractal distribution Abstract: We study distributions which have both fractal and non-fractal scale regions\nby introducing a typical scale into a scale invariant system. As one of models\nin which distributions follow power law in the large scale region and deviate\nfurther from the power law in the smaller scale region, we employ 2-dim quantum\ngravity modified by the $R^2$ term. As examples of distributions in the real\nworld which have similar property to this model, we consider those of personal\nincome in Japan over latest twenty fiscal years. We find relations between the\ntypical scale and several kinds of averages in this model, and observe that\nthese relations are also valid in recent personal income distributions in Japan\nwith sufficient accuracy. We show the existence of the fiscal years so called\nbubble term in which the gap has arisen in power law, by observing that the\ndata are away from one of these relations. We confirm, therefore, that the\ndistribution of this model has close similarity to those of personal income. In\naddition, we can estimate the value of Pareto index and whether a big gap\nexists in power law by using only these relations. As a result, we point out\nthat the typical scale is an useful concept different from average value and\nthat the distribution function derived in this model is an effective tool to\ninvestigate these kinds of distributions. \n\n"}
{"id": "cond-mat/0403167", "contents": "Title: Contagion Flow Through Banking Networks Abstract: Based on an empirical analysis of the network structure of the Austrian\ninter-bank market, we study the flow of funds through the banking network\nfollowing exogenous shocks to the system. These shocks are implemented by\nstochastic changes in variables like interest rates, exchange rates, etc. We\ndemonstrate that the system is relatively stable in the sence that defaults of\nindividual banks are unlikely to spread over the entire network. We study the\ncontagion impact of all individual banks, meaning the number of banks which are\ndriven into insolvency as a result of a single bank's default. We show that the\nvertex betweenness of individual banks is linearly related to their contagion\nimpact. \n\n"}
{"id": "cond-mat/0406385", "contents": "Title: Temporal evolution of the \"thermal\" and \"superthermal\" income classes in\n  the USA during 1983-2001 Abstract: Personal income distribution in the USA has a well-defined two-class\nstructure. The majority of population (97-99%) belongs to the lower class\ncharacterized by the exponential Boltzmann-Gibbs (\"thermal\") distribution,\nwhereas the upper class (1-3% of population) has a Pareto power-law\n(\"superthermal\") distribution. By analyzing income data for 1983-2001, we show\nthat the \"thermal\" part is stationary in time, save for a gradual increase of\nthe effective temperature, whereas the \"superthermal\" tail swells and shrinks\nfollowing the stock market. We discuss the concept of equilibrium inequality in\na society, based on the principle of maximal entropy, and quantitatively show\nthat it applies to the majority of population. \n\n"}
{"id": "cond-mat/0408013", "contents": "Title: Stock Price Clustering and Discreteness: The \"Compass Rose\" and\n  Predictability Abstract: In this letter we investigate the information provided by the \"compass rose\"\n(Crack, T.F. and Ledoit, O. (1996), Journal of Finance, 51(2), pg. 751-762)\npatterns revealed in phase portraits of daily stock returns. It has been\ninitially suggested that the compass rose is just a manifestation of price\nclustering and discreteness and the tick size, factors that can affect the\nunbiasedness of an array of statistical tests based on stock returns. We show\nthat this may not entirely be the case. \n\n"}
{"id": "cond-mat/0409329", "contents": "Title: An analytic treatment of the Gibbs-Pareto behavior in wealth\n  distribution Abstract: We develop a general framework, based on Boltzmann transport theory, to\nanalyze the distribution of wealth in societies. Within this framework we\nderive the distribution function of wealth by using a two-party trading model\nfor the poor people while for the rich people a new model is proposed where\ninteraction with wealthy entities (huge reservoir) is relevant. At equilibrium,\nthe interaction with wealthy entities gives a power-law (Pareto-like) behavior\nin the wealth distribution while the two-party interaction gives a\nBoltzmann-Gibbs distribution. \n\n"}
{"id": "cond-mat/0410059", "contents": "Title: Accuracy and Scaling Phenomena in Internet Mapping Abstract: A great deal of effort has been spent measuring topological features of the\nInternet. However, it was recently argued that sampling based on taking paths\nor traceroutes through the network from a small number of sources introduces a\nfundamental bias in the observed degree distribution. We examine this bias\nanalytically and experimentally. For Erdos-Renyi random graphs with mean degree\nc, we show analytically that traceroute sampling gives an observed degree\ndistribution P(k) ~ 1/k for k < c, even though the underlying degree\ndistribution is Poisson. For graphs whose degree distributions have power-law\ntails P(k) ~ k^-alpha, traceroute sampling from a small number of sources can\nsignificantly underestimate the value of \\alpha when the graph has a large\nexcess (i.e., many more edges than vertices). We find that in order to obtain a\ngood estimate of alpha it is necessary to use a number of sources which grows\nlinearly in the average degree of the underlying graph. Based on these\nobservations we comment on the accuracy of the published values of alpha for\nthe Internet. \n\n"}
{"id": "cond-mat/0503087", "contents": "Title: On the Bias of Traceroute Sampling; or, Power-law Degree Distributions\n  in Regular Graphs Abstract: Understanding the structure of the Internet graph is a crucial step for\nbuilding accurate network models and designing efficient algorithms for\nInternet applications. Yet, obtaining its graph structure is a surprisingly\ndifficult task, as edges cannot be explicitly queried. Instead, empirical\nstudies rely on traceroutes to build what are essentially single-source,\nall-destinations, shortest-path trees. These trees only sample a fraction of\nthe network's edges, and a recent paper by Lakhina et al. found empirically\nthat the resuting sample is intrinsically biased. For instance, the observed\ndegree distribution under traceroute sampling exhibits a power law even when\nthe underlying degree distribution is Poisson.\n  In this paper, we study the bias of traceroute sampling systematically, and,\nfor a very general class of underlying degree distributions, calculate the\nlikely observed distributions explicitly. To do this, we use a continuous-time\nrealization of the process of exposing the BFS tree of a random graph with a\ngiven degree distribution, calculate the expected degree distribution of the\ntree, and show that it is sharply concentrated. As example applications of our\nmachinery, we show how traceroute sampling finds power-law degree distributions\nin both delta-regular and Poisson-distributed random graphs. Thus, our work\nputs the observations of Lakhina et al. on a rigorous footing, and extends them\nto nearly arbitrary degree distributions. \n\n"}
{"id": "cond-mat/0504185", "contents": "Title: Disaster Management in Scale-Free Networks: Recovery from and Protection\n  Against Intentional Attacks Abstract: Susceptibility of scale free Power Law (PL) networks to attacks has been\ntraditionally studied in the context of what may be termed as {\\em\ninstantaneous attacks}, where a randomly selected set of nodes and edges are\ndeleted while the network is kept {\\em static}. In this paper, we shift the\nfocus to the study of {\\em progressive} and instantaneous attacks on {\\em\nreactive} grown and random PL networks, which can respond to attacks and take\nremedial steps. In the process, we present several techniques that managed\nnetworks can adopt to minimize the damages during attacks, and also to\nefficiently recover from the aftermath of successful attacks. For example, we\npresent (i) compensatory dynamics that minimize the damages inflicted by\ntargeted progressive attacks, such as linear-preferential deletions of nodes in\ngrown PL networks; the resulting dynamic naturally leads to the emergence of\nnetworks with PL degree distributions with exponential cutoffs; (ii)\ndistributed healing algorithms that can scale the maximum degree of nodes in a\nPL network using only local decisions, and (iii) efficient means of creating\ngiant connected components in a PL network that has been fragmented by attacks\non a large number of high-degree nodes. Such targeted attacks are considered to\nbe a major vulnerability of PL networks; however, our results show that the\nintroduction of only a small number of random edges, through a {\\em reverse\npercolation} process, can restore connectivity, which in turn allows\nrestoration of other topological properties of the original network. Thus, the\nscale-free nature of the networks can itself be effectively utilized for\nprotection and recovery purposes. \n\n"}
{"id": "cond-mat/0506330", "contents": "Title: A program generating homogeneous random graphs with given weights Abstract: We present a program package which generates homogeneous random graphs with\nprobabilities prescribed by the user. The statistical weight of a labeled graph\n$\\alpha$ is given in the form $W(\\alpha)=\\prod_{i=1}^N p(q_i)$, where $p(q)$ is\nan arbitrary user function and $q_i$ are the degrees of the graph nodes. The\nprogram can be used to generate two types of graphs (simple graphs and\npseudo-graphs) from three types of ensembles (micro-canonical, canonical and\ngrand-canonical). \n\n"}
{"id": "cond-mat/0602611", "contents": "Title: k-core (bootstrap) percolation on complex networks: Critical phenomena\n  and nonlocal effects Abstract: We develop the theory of the k-core (bootstrap) percolation on uncorrelated\nrandom networks with arbitrary degree distributions. We show that the k-core\npercolation is an unusual, hybrid phase transition with a jump emergence of the\nk-core as at a first order phase transition but also with a critical\nsingularity as at a continuous transition. We describe the properties of the\nk-core, explain the meaning of the order parameter for the k-core percolation,\nand reveal the origin of the specific critical phenomena. We demonstrate that a\nso-called ``corona'' of the k-core plays a crucial role (corona is a subset of\nvertices in the k-core which have exactly k neighbors in the k-core). It turns\nout that the k-core percolation threshold is at the same time the percolation\nthreshold of finite corona clusters. The mean separation of vertices in corona\nclusters plays the role of the correlation length and diverges at the critical\npoint. We show that a random removal of even one vertex from the k-core may\nresult in the collapse of a vast region of the k-core around the removed\nvertex. The mean size of this region diverges at the critical point. We find an\nexact mapping of the k-core percolation to a model of cooperative relaxation.\nThis model undergoes critical relaxation with a divergent rate at some critical\nmoment. \n\n"}
{"id": "cond-mat/0603861", "contents": "Title: Congestion-gradient driven transport on complex networks Abstract: We present a study of transport on complex networks with routing based on\nlocal information. Particles hop from one node of the network to another\naccording to a set of routing rules with different degrees of congestion\nawareness, ranging from random diffusion to rigid congestion-gradient driven\nflow. Each node can be either source or destination for particles and all nodes\nhave the same routing capacity, which are features of ad-hoc wireless networks.\nIt is shown that the transport capacity increases when a small amount of\ncongestion awareness is present in the routing rules, and that it then\ndecreases as the routing rules become too rigid when the flow becomes strictly\ncongestion-gradient driven. Therefore, an optimum value of the congestion\nawareness exists in the routing rules. It is also shown that, in the limit of a\nlarge number of nodes, networks using routing based on local information jam at\nany nonzero load. Finally, we study the correlation between congestion at node\nlevel and a betweenness centrality measure. \n\n"}
{"id": "cond-mat/0607017", "contents": "Title: Optimal routing on complex networks Abstract: We present a novel heuristic algorithm for routing optimization on complex\nnetworks. Previously proposed routing optimization algorithms aim at avoiding\nor reducing link overload. Our algorithm balances traffic on a network by\nminimizing the maximum node betweenness with as little path lengthening as\npossible, thus being useful in cases when networks are jamming due to queuing\noverload. By using the resulting routing table, a network can sustain\nsignificantly higher traffic without jamming than in the case of traditional\nshortest path routing. \n\n"}
{"id": "cond-mat/0609098", "contents": "Title: Synchronization in Weighted Uncorrelated Complex Networks in a Noisy\n  Environment: Optimization and Connections with Transport Efficiency Abstract: Motivated by synchronization problems in noisy environments, we study the\nEdwards-Wilkinson process on weighted uncorrelated scale-free networks. We\nconsider a specific form of the weights, where the strength (and the associated\ncost) of a link is proportional to $(k_{i}k_{j})^{\\beta}$ with $k_{i}$ and\n$k_{j}$ being the degrees of the nodes connected by the link. Subject to the\nconstraint that the total network cost is fixed, we find that in the mean-field\napproximation on uncorrelated scale-free graphs, synchronization is optimal at\n$\\beta^{*}$$=$-1. Numerical results, based on exact numerical diagonalization\nof the corresponding network Laplacian, confirm the mean-field results, with\nsmall corrections to the optimal value of $\\beta^{*}$. Employing our recent\nconnections between the Edwards-Wilkinson process and resistor networks, and\nsome well-known connections between random walks and resistor networks, we also\npursue a naturally related problem of optimizing performance in queue-limited\ncommunication networks utilizing local weighted routing schemes. \n\n"}
{"id": "cond-mat/0701184", "contents": "Title: Transport optimization on complex networks Abstract: We present a comparative study of the application of a recently introduced\nheuristic algorithm to the optimization of transport on three major types of\ncomplex networks. The algorithm balances network traffic iteratively by\nminimizing the maximum node betweenness with as little path lengthening as\npossible. We show that by using this optimal routing, a network can sustain\nsignificantly higher traffic without jamming than in the case of shortest path\nrouting. A formula is proved that allows quick computation of the average\nnumber of hops along the path and of the average travel times once the\nbetweennesses of the nodes are computed. Using this formula, we show that\nrouting optimization preserves the small-world character exhibited by networks\nunder shortest path routing, and that it significantly reduces the average\ntravel time on congested networks with only a negligible increase in the\naverage travel time at low loads. Finally, we study the correlation between the\nweights of the links in the case of optimal routing and the betweennesses of\nthe nodes connected by them. \n\n"}
{"id": "cond-mat/9909131", "contents": "Title: Patterns of consumption in socio-economic models with heterogeneous\n  interacting agents Abstract: We study consumption behaviour in systems with heterogeneous interacting\nagents. Two different models are introduced, respectively with long and short\nrange interactions among agents. At any time step an agent decides whether or\nnot to consume a good, doing so if this provides positive utility. Utility is\naffected by idiosyncratic preferences and costs as well as externalities from\nother agents. Agents are ranked in classes and recognize peer, distinction and\naspiration groups. We simulate the system for different choices of the\nparameters and identify different complex patterns: a steady state regime with\na variety of consumption modes of behaviour, and a wave/cycle regime. The cases\nof fad and value goods are both analyzed. \n\n"}
{"id": "cs/0302017", "contents": "Title: A Proposal to Separate Handles from Names on the Internet Abstract: Networked communications inherently depend on the ability of the sender of a\nmessage to indicate through some token how the message should be delivered to a\nparticular recipient. The tokens that refer messages to recipients are\nvariously known as routes, addresses,handles, and names} ordered by their\nrelative nearness to network topology vs. human meaning. All four sorts of\ntoken refer in some way to a recipient, but they are controlled by different\nauthorities and their meanings depend on different contextual parameters.\n  Today's global Internet employs dynamically determined routes, IP addresses,\nand domain names. Domain names combine the functions of handles and names. The\nhigh value of domain names as names leads to substantial social and legal\ndispute about their assignment, degrading their value as handles. The time has\ncome to provide a distinct open network handle system (ONHS), using handles\nthat are not meaningful in natural language and are therefore not subject to\nthe disputes surrounding the use of names.\n  A handle service may be deployed easily as a handle domain within the current\nDomain Name System. In order to minimize the administrative load, and maximize\ntheir own autonomy, netizens may use public-key cryptography to assign their\nown handles. \n\n"}
{"id": "cs/0304045", "contents": "Title: On a composition of digraphs Abstract: Many \"good\" topologies for interconnection networks are based on line\ndigraphs of regular digraphs. These digraphs support unitary matrices. We\npropose the property \"being the digraph of a unitary matrix\" as additional\ncriterion for the design of new interconnection networks. We define a\ncomposition of digraphs, which we call diagonal union. Diagonal union can be\nused to construct digraphs of unitary matrices. We remark that digraphs\nobtained via diagonal union are state split graphs, as defined in symbolic\ndynamics. Finally, we list some potential directions for future research. \n\n"}
{"id": "cs/0404010", "contents": "Title: On the universality of rank distributions of website popularity Abstract: We present an extensive analysis of long-term statistics of the queries to\nwebsites using logs collected on several web caches in Russian academic\nnetworks and on US IRCache caches. We check the sensitivity of the statistics\nto several parameters: (1) duration of data collection, (2) geographical\nlocation of the cache server collecting data, and (3) the year of data\ncollection. We propose a two-parameter modification of the Zipf law and\ninterpret the parameters. We find that the rank distribution of websites is\nstable when approximated by the modified Zipf law. We suggest that website\npopularity may be a universal property of Internet. \n\n"}
{"id": "cs/0405070", "contents": "Title: Traffic-driven model of the World Wide Web graph Abstract: We propose a model for the World Wide Web graph that couples the topological\ngrowth with the traffic's dynamical evolution. The model is based on a simple\ntraffic-driven dynamics and generates weighted directed graphs exhibiting the\nstatistical properties observed in the Web. In particular, the model yields a\nnon-trivial time evolution of vertices and heavy-tail distributions for the\ntopological and traffic properties. The generated graphs exhibit a complex\narchitecture with a hierarchy of cohesiveness levels similar to those observed\nin the analysis of real data. \n\n"}
{"id": "cs/0411013", "contents": "Title: Efficient Algorithms for Large-Scale Topology Discovery Abstract: There is a growing interest in discovery of internet topology at the\ninterface level. A new generation of highly distributed measurement systems is\ncurrently being deployed. Unfortunately, the research community has not\nexamined the problem of how to perform such measurements efficiently and in a\nnetwork-friendly manner. In this paper we make two contributions toward that\nend. First, we show that standard topology discovery methods (e.g., skitter)\nare quite inefficient, repeatedly probing the same interfaces. This is a\nconcern, because when scaled up, such methods will generate so much traffic\nthat they will begin to resemble DDoS attacks. We measure two kinds of\nredundancy in probing (intra- and inter-monitor) and show that both kinds are\nimportant. We show that straightforward approaches to addressing these two\nkinds of redundancy must take opposite tacks, and are thus fundamentally in\nconflict. Our second contribution is to propose and evaluate Doubletree, an\nalgorithm that reduces both types of redundancy simultaneously on routers and\nend systems. The key ideas are to exploit the tree-like structure of routes to\nand from a single point in order to guide when to stop probing, and to probe\neach path by starting near its midpoint. Our results show that Doubletree can\nreduce both types of measurement load on the network dramatically, while\npermitting discovery of nearly the same set of nodes and links. We then show\nhow to enable efficient communication between monitors through the use of Bloom\nfilters. \n\n"}
{"id": "cs/0503064", "contents": "Title: Minimum-Cost Multicast over Coded Packet Networks Abstract: We consider the problem of establishing minimum-cost multicast connections\nover coded packet networks, i.e. packet networks where the contents of outgoing\npackets are arbitrary, causal functions of the contents of received packets. We\nconsider both wireline and wireless packet networks as well as both static\nmulticast (where membership of the multicast group remains constant for the\nduration of the connection) and dynamic multicast (where membership of the\nmulticast group changes in time, with nodes joining and leaving the group).\n  For static multicast, we reduce the problem to a polynomial-time solvable\noptimization problem, and we present decentralized algorithms for solving it.\nThese algorithms, when coupled with existing decentralized schemes for\nconstructing network codes, yield a fully decentralized approach for achieving\nminimum-cost multicast. By contrast, establishing minimum-cost static multicast\nconnections over routed packet networks is a very difficult problem even using\ncentralized computation, except in the special cases of unicast and broadcast\nconnections.\n  For dynamic multicast, we reduce the problem to a dynamic programming problem\nand apply the theory of dynamic programming to suggest how it may be solved. \n\n"}
{"id": "cs/0506071", "contents": "Title: Signal transmission on lossy lines as a dissipative quantum state\n  propagation Abstract: The transmission of electric signals on a coupled line with distributed\nRLC-parameters is considered as a propagation of a dissipative quasi particle.\nA calculation technique is developed, alternative to the one, accepted for\nlumped lines. The relativistic wave equation for the transient response is\ndeduced following the common Ohm-low-type considerations. The exact expressions\nfor the Green function, for information transfer velocity and for time delay\nare obtained on this base. The fundamental restrictions on the measurement\naccuracy of the time delay are pointed out. The obtained results are naturally\ngeneralized for the multilevel networks of the arbitrary dimension. \n\n"}
{"id": "cs/0506099", "contents": "Title: DIMES: Let the Internet Measure Itself Abstract: Today's Internet maps, which are all collected from a small number of vantage\npoints, are falling short of being accurate. We suggest here a paradigm shift\nfor this task. DIMES is a distributed measurement infrastructure for the\nInternet that is based on the deployment of thousands of light weight\nmeasurement agents around the globe.\n  We describe the rationale behind DIMES deployment, discuss its design\ntrade-offs and algorithmic challenges, and analyze the structure of the\nInternet as it seen with DIMES. \n\n"}
{"id": "cs/0507072", "contents": "Title: Reliable Data Storage in Distributed Hash Tables Abstract: Distributed Hash Tables offer a resilient lookup service for unstable\ndistributed environments. Resilient data storage, however, requires additional\ndata replication and maintenance algorithms. These algorithms can have an\nimpact on both the performance and the scalability of the system. In this\npaper, we describe the goals and design space of these replication algorithms.\n  We examine an existing replication algorithm, and present a new analysis of\nits reliability. We then present a new dynamic replication algorithm which can\noperate in unstable environments. We give several possible replica placement\nstrategies for this algorithm, and show how they impact reliability and\nperformance.\n  Finally we compare all replication algorithms through simulation, showing\nquantitatively the difference between their bandwidth use, fault tolerance and\nperformance. \n\n"}
{"id": "cs/0508033", "contents": "Title: Lessons from Three Views of the Internet Topology Abstract: Network topology plays a vital role in understanding the performance of\nnetwork applications and protocols. Thus, recently there has been tremendous\ninterest in generating realistic network topologies. Such work must begin with\nan understanding of existing network topologies, which today typically consists\nof a relatively small number of data sources. In this paper, we calculate an\nextensive set of important characteristics of Internet AS-level topologies\nextracted from the three data sources most frequently used by the research\ncommunity: traceroutes, BGP, and WHOIS. We find that traceroute and BGP\ntopologies are similar to one another but differ substantially from the WHOIS\ntopology. We discuss the interplay between the properties of the data sources\nthat result from specific data collection mechanisms and the resulting topology\nviews. We find that, among metrics widely considered, the joint degree\ndistribution appears to fundamentally characterize Internet AS-topologies: it\nnarrowly defines values for other important metrics. We also introduce an\nevaluation criteria for the accuracy of topology generators and verify previous\nobservations that generators solely reproducing degree distributions cannot\ncapture the full spectrum of critical topological characteristics of any of the\nthree topologies. Finally, we release to the community the input topology\ndatasets, along with the scripts and output of our calculations. This\nsupplement should enable researchers to validate their models against real data\nand to make more informed selection of topology data sources for their specific\nneeds. \n\n"}
{"id": "cs/0511007", "contents": "Title: K-core decomposition of Internet graphs: hierarchies, self-similarity\n  and measurement biases Abstract: We consider the $k$-core decomposition of network models and Internet graphs\nat the autonomous system (AS) level. The $k$-core analysis allows to\ncharacterize networks beyond the degree distribution and uncover structural\nproperties and hierarchies due to the specific architecture of the system. We\ncompare the $k$-core structure obtained for AS graphs with those of several\nnetwork models and discuss the differences and similarities with the real\nInternet architecture. The presence of biases and the incompleteness of the\nreal maps are discussed and their effect on the $k$-core analysis is assessed\nwith numerical experiments simulating biased exploration on a wide range of\nnetwork models. We find that the $k$-core analysis provides an interesting\ncharacterization of the fluctuations and incompleteness of maps as well as\ninformation helping to discriminate the original underlying structure. \n\n"}
{"id": "cs/0511101", "contents": "Title: Chinese Internet AS-level Topology Abstract: We present the first complete measurement of the Chinese Internet topology at\nthe autonomous systems (AS) level based on traceroute data probed from servers\nof major ISPs in mainland China. We show that both the Chinese Internet AS\ngraph and the global Internet AS graph can be accurately reproduced by the\nPositive-Feedback Preference (PFP) model with the same parameters. This result\nsuggests that the Chinese Internet preserves well the topological\ncharacteristics of the global Internet. This is the first demonstration of the\nInternet's topological fractality, or self-similarity, performed at the level\nof topology evolution modeling. \n\n"}
{"id": "cs/0512095", "contents": "Title: The Internet AS-Level Topology: Three Data Sources and One Definitive\n  Metric Abstract: We calculate an extensive set of characteristics for Internet AS topologies\nextracted from the three data sources most frequently used by the research\ncommunity: traceroutes, BGP, and WHOIS. We discover that traceroute and BGP\ntopologies are similar to one another but differ substantially from the WHOIS\ntopology. Among the widely considered metrics, we find that the joint degree\ndistribution appears to fundamentally characterize Internet AS topologies as\nwell as narrowly define values for other important metrics. We discuss the\ninterplay between the specifics of the three data collection mechanisms and the\nresulting topology views. In particular, we show how the data collection\npeculiarities explain differences in the resulting joint degree distributions\nof the respective topologies. Finally, we release to the community the input\ntopology datasets, along with the scripts and output of our calculations. This\nsupplement should enable researchers to validate their models against real data\nand to make more informed selection of topology data sources for their specific\nneeds. \n\n"}
{"id": "cs/0607080", "contents": "Title: New Model of Internet Topology Using k-shell Decomposition Abstract: We introduce and use k-shell decomposition to investigate the topology of the\nInternet at the AS level. Our analysis separates the Internet into three\nsub-components: (a) a nucleus which is a small (~100 nodes) very well connected\nglobally distributed subgraph; (b) a fractal sub-component that is able to\nconnect the bulk of the Internet without congesting the nucleus, with self\nsimilar properties and critical exponents; and (c) dendrite-like structures,\nusually isolated nodes that are connected to the rest of the network through\nthe nucleus only. This unique decomposition is robust, and provides insight\ninto the underlying structure of the Internet and its functional consequences.\nOur approach is general and useful also when studying other complex networks. \n\n"}
{"id": "cs/0609030", "contents": "Title: Space Division Multiple Access with a Sum Feedback Rate Constraint Abstract: On a multi-antenna broadcast channel, simultaneous transmission to multiple\nusers by joint beamforming and scheduling is capable of achieving high\nthroughput, which grows double logarithmically with the number of users. The\nsum rate for channel state information (CSI) feedback, however, increases\nlinearly with the number of users, reducing the effective uplink capacity. To\naddress this problem, a novel space division multiple access (SDMA) design is\nproposed, where the sum feedback rate is upper-bounded by a constant. This\ndesign consists of algorithms for CSI quantization, threshold based CSI\nfeedback, and joint beamforming and scheduling. The key feature of the proposed\napproach is the use of feedback thresholds to select feedback users with large\nchannel gains and small CSI quantization errors such that the sum feedback rate\nconstraint is satisfied. Despite this constraint, the proposed SDMA design is\nshown to achieve a sum capacity growth rate close to the optimal one. Moreover,\nthe feedback overflow probability for this design is found to decrease\nexponentially with the difference between the allowable and the average sum\nfeedback rates. Numerical results show that the proposed SDMA design is capable\nof attaining higher sum capacities than existing ones, even though the sum\nfeedback rate is bounded. \n\n"}
{"id": "cs/0702156", "contents": "Title: Analysis of Steiner subtrees of Random Trees for Traceroute Algorithms Abstract: We consider in this paper the problem of discovering, via a traceroute\nalgorithm, the topology of a network, whose graph is spanned by an infinite\nbranching process. A subset of nodes is selected according to some criterion.\nAs a measure of efficiency of the algorithm, the Steiner distance of the\nselected nodes, i.e. the size of the spanning sub-tree of these nodes, is\ninvestigated. For the selection of nodes, two criteria are considered: A node\nis randomly selected with a probability, which is either independent of the\ndepth of the node (uniform model) or else in the depth biased model, is\nexponentially decaying with respect to its depth. The limiting behavior the\nsize of the discovered subtree is investigated for both models. \n\n"}
{"id": "math/0412429", "contents": "Title: On a kinetic model for a simple market economy Abstract: In this paper, we consider a simple kinetic model of economy involving both\nexchanges between agents and speculative trading. We show that the kinetic\nmodel admits non trivial quasi-stationary states with power law tails of Pareto\ntype. In order to do this we consider a suitable asymptotic limit of the model\nyielding a Fokker-Planck equation for the distribution of wealth among\nindividuals. For this equation the stationary state can be easily derived and\nshows a Pareto power law tail. Numerical results confirm the previous analysis. \n\n"}
{"id": "math/0510013", "contents": "Title: Network Kriging Abstract: Network service providers and customers are often concerned with aggregate\nperformance measures that span multiple network paths. Unfortunately, forming\nsuch network-wide measures can be difficult, due to the issues of scale\ninvolved. In particular, the number of paths grows too rapidly with the number\nof endpoints to make exhaustive measurement practical. As a result, it is of\ninterest to explore the feasibility of methods that dramatically reduce the\nnumber of paths measured in such situations while maintaining acceptable\naccuracy.\n  We cast the problem as one of statistical prediction--in the spirit of the\nso-called `kriging' problem in spatial statistics--and show that end-to-end\nnetwork properties may be accurately predicted in many cases using a\nsurprisingly small set of carefully chosen paths. More precisely, we formulate\na general framework for the prediction problem, propose a class of linear\npredictors for standard quantities of interest (e.g., averages, totals,\ndifferences) and show that linear algebraic methods of subset selection may be\nused to effectively choose which paths to measure. We characterize the\nperformance of the resulting methods, both analytically and numerically. The\nsuccess of our methods derives from the low effective rank of routing matrices\nas encountered in practice, which appears to be a new observation in its own\nright with potentially broad implications on network measurement generally. \n\n"}
{"id": "nlin/0008018", "contents": "Title: Models for the size distribution of businesses in a price driven market Abstract: A microscopic model of aggregation and fragmentation is introduced to\ninvestigate the size distribution of businesses. In the model, businesses are\nconstrained to comply with the market price, as expected by the customers,\nwhile customers can only buy at the prices offered by the businesses. We show\nnumerically and analytically that the size distribution scales like a\npower-law. A mean-field version of our model is also introduced and we\ndetermine for which value of the parameters the mean-field model agrees with\nthe microscopic model. We discuss to what extent our simple model and its\nresults compare with empirical data on company sizes in the U.S. and debt sizes\nin Japan. Finally, possible extensions of the mean-field model are discussed,\nto cope with other empirical data. \n\n"}
{"id": "nlin/0109015", "contents": "Title: Wealth redistribution with finite resources Abstract: We present a simplified model for the exploitation of finite resources by\ninteracting agents, where each agent receives a random fraction of the\navailable resources. An extremal dynamics ensures that the poorest agent has a\nchance to change its economic welfare. After a long transient, the system\nself-organizes into a critical state that maximizes the average performance of\neach participant. Our model exhibits a new kind of wealth condensation, where\nvery few extremely rich agents are stable in time and the rest stays in the\nmiddle class. \n\n"}
{"id": "nlin/0604061", "contents": "Title: Profit Maximization, Industry Structure, and Competition: A critique of\n  neoclassical theory Abstract: Neoclassical economics has two theories of competition between\nprofit-maximizing firms (Marshallian and Cournot-Nash) that start from\ndifferent premises about the degree of strategic interaction between firms, yet\nreach the same result, that market price falls as the number of firms in an\nindustry increases. The Marshallian argument is strictly false. We integrate\nthe different premises, and establish that the optimal level of strategic\ninteraction between competing firms is zero. Simulations support our analysis\nand reveal intriguing emergent behaviors. \n\n"}
{"id": "physics/0502081", "contents": "Title: Statistical Properties of Business Firms Structure and Growth Abstract: We analyze a database comprising quarterly sales of 55624 pharmaceutical\nproducts commercialized by 3939 pharmaceutical firms in the period 1992--2001.\nWe study the probability density function (PDF) of growth in firms and product\nsales and find that the width of the PDF of growth decays with the sales as a\npower law with exponent $\\beta = 0.20 \\pm 0.01$. We also find that the average\nsales of products scales with the firm sales as a power law with exponent\n$\\alpha = 0.57 \\pm 0.02$. And that the average number products of a firm scales\nwith the firm sales as a power law with exponent $\\gamma = 0.42 \\pm 0.02$. We\ncompare these findings with the predictions of models proposed till date on\ngrowth of business firms. \n\n"}
{"id": "physics/0504026", "contents": "Title: Let Your CyberAlter Ego Share Information and Manage Spam Abstract: Almost all of us have multiple cyberspace identities, and these {\\em\ncyber}alter egos are networked together to form a vast cyberspace social\nnetwork. This network is distinct from the world-wide-web (WWW), which is being\nqueried and mined to the tune of billions of dollars everyday, and until\nrecently, has gone largely unexplored. Empirically, the cyberspace social\nnetworks have been found to possess many of the same complex features that\ncharacterize its real counterparts, including scale-free degree distributions,\nlow diameter, and extensive connectivity. We show that these topological\nfeatures make the latent networks particularly suitable for explorations and\nmanagement via local-only messaging protocols. {\\em Cyber}alter egos can\ncommunicate via their direct links (i.e., using only their own address books)\nand set up a highly decentralized and scalable message passing network that can\nallow large-scale sharing of information and data. As one particular example of\nsuch collaborative systems, we provide a design of a spam filtering system, and\nour large-scale simulations show that the system achieves a spam detection rate\nclose to 100%, while the false positive rate is kept around zero. This system\nhas several advantages over other recent proposals (i) It uses an already\nexisting network, created by the same social dynamics that govern our daily\nlives, and no dedicated peer-to-peer (P2P) systems or centralized server-based\nsystems need be constructed; (ii) It utilizes a percolation search algorithm\nthat makes the query-generated traffic scalable; (iii) The network has a built\nin trust system (just as in social networks) that can be used to thwart\nmalicious attacks; iv) It can be implemented right now as a plugin to popular\nemail programs, such as MS Outlook, Eudora, and Sendmail. \n\n"}
{"id": "physics/0504121", "contents": "Title: How the rich get richer Abstract: In our model, $n$ traders interact with each other and with a central bank;\nthey are taxed on the money they make, some of which is dissipated away by\ncorruption. A generic feature of our model is that the richest trader always\nwins by 'consuming' all the others: another is the existence of a threshold\nwealth, below which all traders go bankrupt. The two-trader case is examined in\ndetail,in the socialist and capitalist limits, which generalise easily to\n$n>2$. In its mean-field incarnation, our model exhibits a two-time-scale\nglassy dynamics, as well as an astonishing universality.When preference is\ngiven to local interactions in finite neighbourhoods,a novel feature emerges:\ninstead of at most one overall winner in the system,finite numbers of winners\nemerge, each one the overlord of a particular region.The patterns formed by\nsuch winners (metastable states) are very much a consequence of initial\nconditions, so that the fate of the marketplace is ruled by its past history;\nhysteresis is thus also manifested. \n\n"}
{"id": "physics/0504161", "contents": "Title: Detailed simulation results for some wealth distribution models in\n  Econophysics Abstract: In this paper we present detailed simulation results on the wealth\ndistribution model with quenched saving propensities. Unlike other wealth\ndistribution models where the saving propensities are either zero or constant,\nthis model is not found to be ergodic and self-averaging. The wealth\ndistribution statistics with a single realization of quenched disorder is\nobserved to be significantly different in nature from that of the statistics\naveraged over a large number of independent quenched configurations. The\npeculiarities in the single realization statistics refuses to vanish\nirrespective of whatever large sample size is used. This implies that\npreviously observed Pareto law is essentially a convolution of the single\nmember distributions. \n\n"}
{"id": "physics/0504197", "contents": "Title: The Rich Are Different!: Pareto Law from asymmetric interactions in\n  asset exchange models Abstract: It is known that asset exchange models with symmetric interaction between\nagents show either a Gibbs/log-normal distribution of assets among the agents\nor condensation of the entire wealth in the hands of a single agent, depending\nupon the rules of exchange. Here we explore the effects of introducing\nasymmetry in the interaction between agents with different amounts of wealth\n(i.e., the rich behave differently from the poor). This can be implemented in\nseveral ways: e.g., (1) in the net amount of wealth that is transferred from\none agent to another during an exchange interaction, or (2) the probability of\ngaining vs. losing a net amount of wealth from an exchange interaction. We\npropose that, in general, the introduction of asymmetry leads to Pareto-like\npower law distribution of wealth. \n\n"}
{"id": "physics/0504217", "contents": "Title: Pareto's Law of Income Distribution: Evidence for Germany, the United\n  Kingdom, and the United States Abstract: We analyze three sets of income data: the US Panel Study of Income Dynamics\nPSID), the British Household Panel Survey (BHPS), and the German Socio-Economic\nPanel (GSOEP). It is shown that the empirical income distribution is consistent\nwith a two-parameter lognormal function for the low-middle income group\n(97%-99% of the population), and with a Pareto or power law function for the\nhigh income group (1%-3% of the population). This mixture of two qualitatively\ndifferent analytical distributions seems stable over the years covered by our\ndata sets, although their parameters significantly change in time. It is also\nfound that the probability density of income growth rates almost has the form\nof an exponential function. \n\n"}
{"id": "physics/0505157", "contents": "Title: Economic exchanges in a stratified society: End of the middle class? Abstract: We study the effect of the social stratification on the wealth distribution\non a system of interacting economic agents that are constrained to interact\nonly within their own economic class. The economical mobility of the agents is\nrelated to its success in exchange transactions. Different wealth distributions\nare obtained as a function of the width of the economic class. We find a range\nof widths in which the society is divided in two classes separated by a deep\ngap that prevents further exchange between poor and rich agents. As a\nconsequence, the middle wealth class is eliminated. The high values of the Gini\nindices obtained in these cases indicate a highly unequal society. On the other\nhand, lower and higher widths induce lower Gini indices and a fairer wealth\ndistribution. \n\n"}
{"id": "physics/0505173", "contents": "Title: Empirical study and model of personal income Abstract: Personal income distributions in Japan are analyzed empirically and a simple\nstochastic model of the income process is proposed. Based on empirical facts,\nwe propose a minimal two-factor model. Our model of personal income consists of\nan asset accumulation process and a wage process. We show that these simple\nprocesses can successfully reproduce the empirical distribution of income. In\nparticular, the model can reproduce the particular transition of the\ndistribution shape from the middle part to the tail part. This model also\nallows us to derive the tail exponent of the distribution analytically. \n\n"}
{"id": "physics/0506028", "contents": "Title: Influence of saving propensity on the power law tail of wealth\n  distribution Abstract: Some general features of kinetic multi-agent models are reviewed, with\nparticular attention to the relation between the agent saving propensities and\nthe form of the equilibrium wealth distribution. The effect of a finite cutoff\nof the saving propensity distribution on the corresponding wealth distribution\nis studied. Various results about kinetic multi-agent models are collected and\nused to construct a realistic wealth distribution with zero limit for small\nvalues of wealth, an exponential form at intermediate and a power law tail at\nlarger values of wealth. \n\n"}
{"id": "physics/0507101", "contents": "Title: Production networks and failure avalanches Abstract: Although standard economics textbooks are seldom interested in production\nnetworks, modern economies are more and more based upon suppliers/customers\ninteractions. One can consider entire sectors of the economy as generalised\nsupply chains. We will take this view in the present paper and study under\nwhich conditions local failures to produce or simply to deliver can result in\navalanches of shortage and bankruptcies across the network. We will show that a\nlarge class of models exhibit scale free distributions of production and wealth\namong firms and that metastable regions of high production are highly\nlocalised. \n\n"}
{"id": "physics/0507162", "contents": "Title: Dynamic Process of Money Transfer Models Abstract: We have studied numerically the statistical mechanics of the dynamic\nphenomena, including money circulation and economic mobility, in some transfer\nmodels. The models on which our investigations were performed are the basic\nmodel proposed by A. Dragulescu and V. Yakovenko [1], the model with uniform\nsaving rate developed by A. Chakraborti and B.K. Chakrabarti [2], and its\nextended model with diverse saving rate [3]. The velocity of circulation is\nfound to be inversely related with the average holding time of money. In order\nto check the nature of money transferring process in these models, we\ndemonstrated the probability distributions of holding time. In the model with\nuniform saving rate, the distribution obeys exponential law, which indicates\nmoney transfer here is a kind of Poisson process. But when the saving rate is\nset diversely, the holding time distribution follows a power law. The velocity\ncan also be deduced from a typical individual's optimal choice. In this way, an\napproach for building the micro-foundation of velocity is provided. In order to\nexpose the dynamic mechanism behind the distribution in microscope, we examined\nthe mobility by collecting the time series of agents' rank and measured it by\nemploying an index raised by economists. In the model with uniform saving rate,\nthe higher saving rate, the slower agents moves in the economy. Meanwhile, all\nof the agents have the same chance to be the rich. However, it is not the case\nin the model with diverse saving rate, where the assumed economy falls into\nstratification. The volatility distribution of the agents' ranks are also\ndemonstrated to distinguish the differences among these models. \n\n"}
{"id": "physics/0508178", "contents": "Title: Derivation of the distribution from extended Gibrat's law Abstract: Employing profits data of Japanese companies in 2002 and 2003, we identify\nthe non-Gibrat's law which holds in the middle profits region. From the law of\ndetailed balance in all regions, Gibrat's law in the high region and the\nnon-Gibrat's law in the middle region, we kinematically derive the profits\ndistribution function in the high and middle range uniformly. The distribution\nfunction accurately fits with empirical data without any fitting parameter. \n\n"}
{"id": "physics/0508206", "contents": "Title: The Network of Inter-Regional Direct Investment Stocks across Europe Abstract: We propose a methodological framework to study the dynamics of inter-regional\ninvestment flow in Europe from a Complex Networks perspective, an approach with\nrecent proven success in many fields including economics. In this work we study\nthe network of investment stocks in Europe at two different levels: first, we\ncompute the inward-outward investment stocks at the level of firms, based on\nownership shares and number of employees; then we estimate the inward-outward\ninvestment stock at the level of regions in Europe, by aggregating the\nownership network of firms, based on their headquarter location. Despite the\nintuitive value of this approach for EU policy making in economic development,\nto our knowledge there are no similar works in the literature yet. In this\npaper we focus on statistical distributions and scaling laws of activity,\ninvestment stock and connectivity degree both at the level of firms and at the\nlevel of regions. In particular we find that investment stock of firms is power\nlaw distributed with an exponent very close to the one found for firm activity.\nOn the other hand investment stock and activity of regions turn out to be\nlog-normal distributed. At both levels we find scaling laws relating investment\nto activity and connectivity. In particular, we find that investment stock\nscales with connectivity in a similar way as has been previously found for\nstock market data, calling for further investigations on a possible general\nscaling law holding true in economical networks. \n\n"}
{"id": "physics/0509172", "contents": "Title: Role of Selective Interaction in Wealth Distribution Abstract: In our simplified description `wealth' is money ($m$). A kinetic theory of\ngas like model of money is investigated where two agents interact (trade)\nselectively and exchange some amount of money between them so that sum of their\nmoney is unchanged and thus total money of all the agents remains conserved.\nThe probability distributions of individual money ($P(m)$ vs. $m$) is seen to\nbe influenced by certain ways of selective interactions. The distributions\nshift away from Boltzmann-Gibbs like exponential distribution and in some cases\ndistributions emerge with power law tails known as Pareto's law ($P(m) \\propto\nm^{-(1+\\alpha)}$). Power law is also observed in some other closely related\nconserved and discrete models. A discussion is provided with numerical support\nto have a dig into the emergence of power laws in such models. \n\n"}
{"id": "physics/0510038", "contents": "Title: A common origin of the power law distributions in models of market and\n  earthquake Abstract: We show that there is a common mode of origin for the power laws observed in\ntwo different models: (i) the Pareto law for the distribution of money among\nthe agents with random saving propensities in an ideal gas-like market model\nand (ii) the Gutenberg-Richter law for the distribution of overlaps in a\nfractal-overlap model for earthquakes. We find that the power laws appear as\nthe asymptotic forms of ever-widening log-normal distributions for the agents'\nmoney and the overlap magnitude respectively. The identification of the generic\norigin of the power laws helps in better understanding and in developing\ngeneralized views of phenomena in such diverse areas as economics and\ngeophysics. \n\n"}
{"id": "physics/0603076", "contents": "Title: Living in an Irrational Society: Wealth Distribution with Correlations\n  between Risk and Expected Profits Abstract: Different models to study the wealth distribution in an artificial society\nhave considered a transactional dynamics as the driving force. Those models\ninclude a risk aversion factor, but also a finite probability of favoring the\npoorer agent in a transaction. Here we study the case where the partners in the\ntransaction have a previous knowledge of the winning probability and adjust\ntheir risk aversion taking this information into consideration. The results\nindicate that a relatively equalitarian society is obtained when the agents\nrisk in direct proportion to their winning probabilities. However, it is the\nopposite case that delivers wealth distribution curves and Gini indices closer\nto empirical data. This indicates that, at least for this very simple model,\neither agents have no knowledge of their winning probabilities, either they\nexhibit an ``irrational'' behavior risking more than reasonable. \n\n"}
{"id": "physics/0606224", "contents": "Title: Of Songs and Men: a Model for Multiple Choice with Herding Abstract: We propose a generic model for multiple choice situations in the presence of\nherding and compare it with recent empirical results from a Web-based music\nmarket experiment. The model predicts a phase transition between a weak\nimitation phase and a strong imitation, `fashion' phase, where choices are\ndriven by peer pressure and the ranking of individual preferences is strongly\ndistorted at the aggregate level. The model can be calibrated to reproduce the\nmain experimental results of Salganik et al. (Science, 311, pp. 854-856\n(2006)); we show in particular that the value of the social influence parameter\ncan be estimated from the data. In one of the experimental situation, this\nvalue is found to be close to the critical value of the model. \n\n"}
{"id": "physics/0607258", "contents": "Title: Ideal-gas like market models with savings: quenched and annealed cases Abstract: We analyze the ideal gas like models of markets and review the different\ncases where a `savings' factor changes the nature and shape of the distribution\nof wealth. These models can produce similar distribution of wealth as observed\nacross varied economies. We present a more realistic model where the saving\nfactor can vary over time (annealed savings) and yet produces Pareto\ndistribution of wealth in certain cases. We discuss the relevance of such\nmodels in the context of wealth distribution, and address some recent issues in\nthe context of these models. \n\n"}
{"id": "physics/0607287", "contents": "Title: Response of Firm Agent Network to Exogenous Shock Abstract: This paper describes an agent-based model of interacting firms, in which\ninteracting firm agents rationally invest capital and labor in order to\nmaximize payoff. Both transactions and production are taken into account in\nthis model. First, the performance of individual firms on a real transaction\nnetwork was simulated. The simulation quantitatively reproduced the cumulative\nprobability distribution of revenue, material cost, capital, and labor. Then,\nthe response of the firms to a given exogenous shock, defined as a sudden\nchange of gross domestic product, is discussed. The longer tail in cumulative\nprobability and skewed distribution of growth rate are observed for a high\ngrowth scenario. \n\n"}
{"id": "physics/0608174", "contents": "Title: Relaxation in statistical many-agent economy models Abstract: We review some statistical many-agent models of economic and social systems\ninspired by microscopic molecular models and discuss their stochastic\ninterpretation. We apply these models to wealth exchange in economics and study\nhow the relaxation process depends on the parameters of the system, in\nparticular on the saving propensities that define and diversify the agent\nprofiles. \n\n"}
{"id": "physics/0612231", "contents": "Title: A mechanism to derive multi-power law functions: an application in the\n  econophysics framework Abstract: It is generally recognized that economical systems, and more in general\ncomplex systems, are characterized by power law distributions. Sometime, these\ndistributions show a changing of the slope in the tail so that, more\nappropriately, they show a multi-power law behavior. We present a method to\nderive analytically a two-power law distribution starting from a single power\nlaw function recently obtained, in the frameworks of the generalized\nstatistical mechanics based on the Sharma-Taneja-Mittal information measure. In\norder to test the method, we fit the cumulative distribution of personal income\nand gross domestic production of several countries, obtaining a good agreement\nfor a wide range of data. \n\n"}
{"id": "quant-ph/0505089", "contents": "Title: Quantum key distribution with trusted quantum relay Abstract: A trusted quantum relay is introduced to enable quantum key distribution\nlinks to form the basic legs in a quantum key distribution network. The idea is\nbased on the well-known intercept/resend eavesdropping. The same scheme can be\nused to make quantum key distribution between several parties. No entanglement\nis required. \n\n"}
