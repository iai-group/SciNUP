{"id": "0704.1074", "contents": "Title: Markov basis and Groebner basis of Segre-Veronese configuration for\n  testing independence in group-wise selections Abstract: We consider testing independence in group-wise selections with some\nrestrictions on combinations of choices. We present models for frequency data\nof selections for which it is easy to perform conditional tests by Markov chain\nMonte Carlo (MCMC) methods. When the restrictions on the combinations can be\ndescribed in terms of a Segre-Veronese configuration, an explicit form of a\nGr\\\"obner basis consisting of moves of degree two is readily available for\nperforming a Markov chain. We illustrate our setting with the National Center\nTest for university entrance examinations in Japan. We also apply our method to\ntesting independence hypotheses involving genotypes at more than one locus or\nhaplotypes of alleles on the same chromosome. \n\n"}
{"id": "0706.1062", "contents": "Title: Power-law distributions in empirical data Abstract: Power-law distributions occur in many situations of scientific interest and\nhave significant consequences for our understanding of natural and man-made\nphenomena. Unfortunately, the detection and characterization of power laws is\ncomplicated by the large fluctuations that occur in the tail of the\ndistribution -- the part of the distribution representing large but rare events\n-- and by the difficulty of identifying the range over which power-law behavior\nholds. Commonly used methods for analyzing power-law data, such as\nleast-squares fitting, can produce substantially inaccurate estimates of\nparameters for power-law distributions, and even in cases where such methods\nreturn accurate answers they are still unsatisfactory because they give no\nindication of whether the data obey a power law at all. Here we present a\nprincipled statistical framework for discerning and quantifying power-law\nbehavior in empirical data. Our approach combines maximum-likelihood fitting\nmethods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic\nand likelihood ratios. We evaluate the effectiveness of the approach with tests\non synthetic data and give critical comparisons to previous approaches. We also\napply the proposed methods to twenty-four real-world data sets from a range of\ndifferent disciplines, each of which has been conjectured to follow a power-law\ndistribution. In some cases we find these conjectures to be consistent with the\ndata while in others the power law is ruled out. \n\n"}
{"id": "0708.1449", "contents": "Title: Slow beams of massive molecules Abstract: Slow beams of neutral molecules are of great interest for a wide range of\napplications, from cold chemistry through precision measurements to tests of\nthe foundations of quantum mechanics. We report on the quantitative observation\nof thermal beams of perfluorinated macromolecules with masses up to 6000 amu,\nreaching velocities down to 11 m/s. Such slow, heavy and neutral molecular\nbeams are of importance for a new class of experiments in matter-wave\ninterferometry and we also discuss the requirements for further manipulation\nand cooling schemes with molecules in this unprecedented mass range. \n\n"}
{"id": "0801.4172", "contents": "Title: Computational aspects and applications of a new transform for solving\n  the complex exponentials approximation problem Abstract: Many real life problems can be reduced to the solution of a complex\nexponentials approximation problem which is usually ill posed. Recently a new\ntransform for solving this problem, formulated as a specific moments problem in\nthe plane, has been proposed in a theoretical framework. In this work some\ncomputational issues are addressed to make this new tool useful in practice. An\nalgorithm is developed and used to solve a Nuclear Magnetic Resonance\nspectrometry problem, two time series interpolation and extrapolation problems\nand a shape from moments problem. \n\n"}
{"id": "0802.3735", "contents": "Title: Energetics and Kinetics of Primary Charge Separation in Bacterial\n  Photosynthesis Abstract: We report the results of Molecular Dynamics (MD) simulations and formal\nmodeling of the free energy surfaces and reaction rates of primary charge\nseparation in the reaction center of \\textit{Rhodobacter sphaeroides}. Two\nsimulation protocols were used to produce MD trajectories. Standard force field\npotentials were employed in the first protocol. In the second protocol, the\nspecial pair was made polarizable to reproduce a high polarizability of its\nphotoexcited state observed by Stark spectroscopy. The charge distribution\nbetween covalent and charge-transfer states of the special pair was dynamically\nadjusted during the simulation run. We found from both protocols that the\nbreadth of electrostatic fluctuations of the protein/water environment far\nexceeds previous estimates resulting in about 1.6 eV reorganization energy of\nelectron transfer in the first protocol and 2.5 eV in the second protocol. Most\nof these electrostatic fluctuations become dynamically frozen on the time-scale\nof primary charge separation resulting in much smaller solvation contributions\nto the activation barrier. A non-ergodic formulation of the diffusion-reaction\nelectron transfer kinetics has allowed us to reproduce the experimental results\nfor both the temperature dependence of the rate and the non-exponential decay\nof the population of the photoexcited special pair. \n\n"}
{"id": "0804.2749", "contents": "Title: Solvent viscosity dependence for enzymatic reactions Abstract: A mechanism for relationship of solvent viscosity with reaction rate constant\nat enzyme action is suggested. It is based on fluctuations of electric field in\nenzyme active site produced by thermally equilibrium rocking (cranckshaft\nmotion) of the rigid plane (in which the dipole moment $\\approx 3.6 D$ lies) of\na favourably located and oriented peptide group (or may be a few of them). Thus\nthe rocking of the plane leads to fluctuations of the electric field of the\ndipole moment. These fluctuations can interact with the reaction coordinate\nbecause the latter in its turn has transition dipole moment due to separation\nof charges at movement of the reacting system along it. The rocking of the\nplane of the peptide group is sensitive to the microviscosity of its\nenvironment in protein interior and the latter is a function of the solvent\nviscosity. Thus we obtain an additional factor of interrelationship for these\ncharacteristics with the reaction rate constant. We argue that due to the\nproperties of the cranckshaft motion the frequency spectrum of the electric\nfield fluctuations has a sharp resonance peak at some frequency and the\ncorresponding Fourier mode can be approximated as oscillations. We employ a\nknown result from the theory of thermally activated escape with periodic\ndriving to obtain the reaction rate constant and argue that it yields reliable\ndescription of the preexponent where the dependence on solvent viscosity\nmanifests itself. The suggested mechanism is shown to grasp the main feature of\nthis dependence known from the experiment and satisfactorily yields the upper\nlimit of the fractional index of a power in it. \n\n"}
{"id": "0804.3779", "contents": "Title: On Estimation of Finite Population Proportion Abstract: In this paper, we study the classical problem of estimating the proportion of\na finite population. First, we consider a fixed sample size method and derive\nan explicit sample size formula which ensures a mixed criterion of absolute and\nrelative errors. Second, we consider an inverse sampling scheme such that the\nsampling is continue until the number of units having a certain attribute\nreaches a threshold value or the whole population is examined. We have\nestablished a simple method to determine the threshold so that a prescribed\nrelative precision is guaranteed. Finally, we develop a multistage sampling\nscheme for constructing fixed-width confidence interval for the proportion of a\nfinite population. Powerful computational techniques are introduced to make it\npossible that the fixed-width confidence interval ensures prescribed level of\ncoverage probability. \n\n"}
{"id": "0805.2366", "contents": "Title: LSST: from Science Drivers to Reference Design and Anticipated Data\n  Products Abstract: (Abridged) We describe here the most ambitious survey currently planned in\nthe optical, the Large Synoptic Survey Telescope (LSST). A vast array of\nscience will be enabled by a single wide-deep-fast sky survey, and LSST will\nhave unique survey capability in the faint time domain. The LSST design is\ndriven by four main science themes: probing dark energy and dark matter, taking\nan inventory of the Solar System, exploring the transient optical sky, and\nmapping the Milky Way. LSST will be a wide-field ground-based system sited at\nCerro Pach\\'{o}n in northern Chile. The telescope will have an 8.4 m (6.5 m\neffective) primary mirror, a 9.6 deg$^2$ field of view, and a 3.2 Gigapixel\ncamera. The standard observing sequence will consist of pairs of 15-second\nexposures in a given field, with two such visits in each pointing in a given\nnight. With these repeats, the LSST system is capable of imaging about 10,000\nsquare degrees of sky in a single filter in three nights. The typical 5$\\sigma$\npoint-source depth in a single visit in $r$ will be $\\sim 24.5$ (AB). The\nproject is in the construction phase and will begin regular survey operations\nby 2022. The survey area will be contained within 30,000 deg$^2$ with\n$\\delta<+34.5^\\circ$, and will be imaged multiple times in six bands, $ugrizy$,\ncovering the wavelength range 320--1050 nm. About 90\\% of the observing time\nwill be devoted to a deep-wide-fast survey mode which will uniformly observe a\n18,000 deg$^2$ region about 800 times (summed over all six bands) during the\nanticipated 10 years of operations, and yield a coadded map to $r\\sim27.5$. The\nremaining 10\\% of the observing time will be allocated to projects such as a\nVery Deep and Fast time domain survey. The goal is to make LSST data products,\nincluding a relational database of about 32 trillion observations of 40 billion\nobjects, available to the public and scientists around the world. \n\n"}
{"id": "0805.2741", "contents": "Title: Environment-Assisted Quantum Walks in Photosynthetic Energy Transfer Abstract: Energy transfer within photosynthetic systems can display quantum effects\nsuch as delocalized excitonic transport. Recently, direct evidence of\nlong-lived coherence has been experimentally demonstrated for the dynamics of\nthe Fenna-Matthews-Olson (FMO) protein complex [Engel et al., Nature 446, 782\n(2007)]. However, the relevance of quantum dynamical processes to the exciton\ntransfer efficiency is to a large extent unknown. Here, we develop a\ntheoretical framework for studying the role of quantum interference effects in\nenergy transfer dynamics of molecular arrays interacting with a thermal bath\nwithin the Lindblad formalism. To this end, we generalize continuous-time\nquantum walks to non-unitary and temperature-dependent dynamics in Liouville\nspace derived from a microscopic Hamiltonian. Different physical effects of\ncoherence and decoherence processes are explored via a universal measure for\nthe energy transfer efficiency and its susceptibility. In particular, we\ndemonstrate that for the FMO complex an effective interplay between free\nHamiltonian and thermal fluctuations in the environment leads to a substantial\nincrease in energy transfer efficiency from about 70% to 99%. \n\n"}
{"id": "0806.2305", "contents": "Title: Universality in movie rating distributions Abstract: In this paper histograms of user ratings for movies (1,...,10) are analysed.\nThe evolving stabilised shapes of histograms follow the rule that all are\neither double- or triple-peaked. Moreover, at most one peak can be on the\ncentral bins 2,...,9 and the distribution in these bins looks smooth\n`Gaussian-like' while changes at the extremes (1 and 10) often look abrupt. It\nis shown that this is well approximated under the assumption that histograms\nare confined and discretised probability density functions of L\\'evy skew\nalpha-stable distributions. These distributions are the only stable\ndistributions which could emerge due to a generalized central limit theorem\nfrom averaging of various independent random avriables as which one can see the\ninitial opinions of users. Averaging is also an appropriate assumption about\nthe social process which underlies the process of continuous opinion formation.\nSurprisingly, not the normal distribution achieves the best fit over histograms\nobseved on the web, but distributions with fat tails which decay as power-laws\nwith exponent -(1+alpha) (alpha=4/3). The scale and skewness parameters of the\nLevy skew alpha-stable distributions seem to depend on the deviation from an\naverage movie (with mean about 7.6). The histogram of such an average movie has\nno skewness and is the most narrow one. If a movie deviates from average the\ndistribution gets broader and skew. The skewness pronounces the deviation. This\nis used to construct a one parameter fit which gives some evidence of\nuniversality in processes of continuous opinion dynamics about taste. \n\n"}
{"id": "0807.0786", "contents": "Title: Ion-induced electron production in tissue-like media and DNA damage\n  mechanisms Abstract: We propose an inclusive approach for calculating characteristics of secondary\nelectrons produced by ions/protons in tissue-like media. This approach is based\non an analysis of the projectile's interaction with the medium on the\nmicroscopic level. It allows us to obtain the energy spectrum and abundance of\nsecondary electrons as functions of the projectile kinetic energy. The physical\ninformation obtained in this analysis is related to biological processes\nresponsible for the irrepearable DNA damage induced by the projectile. In\nparticular, we consider double strand breaks of DNA caused by secondary\nelectrons and free radicals, and local heating in the ion's track. The heating\nmay enhance the biological effectiveness of electron/free radical interactions\nwith the DNA and may even be considered as an independent mechanism of DNA\ndamage. Numerical estimates are performed for the case of carbon-ion beams. The\nobtained dose-depth curves are compared with results of the MCHIT model based\non the GEANT4 toolkit. \n\n"}
{"id": "0807.4500", "contents": "Title: Dynamical transition, hydrophobic interface, and the temperature\n  dependence of electrostatic fluctuations in proteins Abstract: Molecular dynamics simulations have revealed a dramatic increase, with\nincreasing temperature, of the amplitude of electrostatic fluctuations caused\nby water at the active site of metalloprotein plastocyanin. The increased\nbreadth of electrostatic fluctuations, expressed in terms of the reorganization\nenergy of changing the redox state of the protein, is related to the formation\nof the hydrophobic protein/water interface allowing large-amplitude collective\nfluctuations of the water density in the protein's first solvation shell. On\nthe top of the monotonic increase of the reorganization energy with increasing\ntemperature, we have observed a spike at 220 K also accompanied by a\nsignificant slowing of the exponential collective Stokes shift dynamics. In\ncontrast to the local density fluctuations of the hydration-shell waters, these\nspikes might be related to the global property of the water solvent crossing\nthe Widom line. \n\n"}
{"id": "0808.1409", "contents": "Title: New Tests of Spatial Segregation Based on Nearest Neighbor Contingency\n  Tables Abstract: The spatial clustering of points from two or more classes (or species) has\nimportant implications in many fields and may cause the spatial patterns of\nsegregation and association, which are two major types of spatial interaction\nbetween the classes. The null patterns we consider are random labeling (RL) and\ncomplete spatial randomness (CSR) of points from two or more classes, which is\ncalled CSR independence. The segregation and association patterns can be\nstudied using a nearest neighbor contingency table (NNCT) which is constructed\nusing the frequencies of nearest neighbor (NN) types in a contingency table.\nAmong NNCT-tests Pielou's test is liberal the null pattern but Dixon's test has\nthe desired significance level under the RL pattern. We propose three new\nmultivariate clustering tests based on NNCTs. We compare the finite sample\nperformance of these new tests with Pielou's and Dixon's tests and Cuzick &\nEdward's k-NN tests in terms of empirical size under the null cases and\nempirical power under various segregation and association alternatives and\nprovide guidelines for using the tests in practice. We demonstrate that the\nnewly proposed NNCT-tests perform relatively well compared to their competitors\nand illustrate the tests using three example data sets. Furthermore, we compare\nthe NNCT-tests with the second-order methods using these examples. \n\n"}
{"id": "0809.0492", "contents": "Title: From Data to the p-Adic or Ultrametric Model Abstract: We model anomaly and change in data by embedding the data in an ultrametric\nspace. Taking our initial data as cross-tabulation counts (or other input data\nformats), Correspondence Analysis allows us to endow the information space with\na Euclidean metric. We then model anomaly or change by an induced ultrametric.\nThe induced ultrametric that we are particularly interested in takes a\nsequential - e.g. temporal - ordering of the data into account. We apply this\nwork to the flow of narrative expressed in the film script of the Casablanca\nmovie; and to the evolution between 1988 and 2004 of the Colombian social\nconflict and violence. \n\n"}
{"id": "0809.0939", "contents": "Title: Applications of Bayesian Probability Theory in Astrophysics Abstract: Bayesian Inference is a powerful approach to data analysis that is based\nalmost entirely on probability theory. In this approach, probabilities model\n{\\it uncertainty} rather than randomness or variability. This thesis is\ncomposed of a series of papers that have been published in various astronomical\njournals during the years 2005-2008. The unifying thread running through the\npapers is the use of Bayesian Inference to solve underdetermined inverse\nproblems in astrophysics. Firstly, a methodology is developed to solve a\nquestion in gravitational lens inversion - using the observed images of\ngravitational lens systems to reconstruct the undistorted source profile and\nthe mass profile of the lensing galaxy. A similar technique is also applied to\nthe task of inferring the number and frequency of modes of oscillation of a\nstar from the time series observations that are used in the field of\nasteroseismology. For these complex problems, many of the required calculations\ncannot be done analytically, and so Markov Chain Monte Carlo algorithms have\nbeen used. Finally, probabilistic reasoning is applied to a controversial\nquestion in astrobiology: does the fact that life formed quite soon after the\nEarth constitute evidence that the formation of life is quite probable, given\nthe right macroscopic conditions? \n\n"}
{"id": "0810.0561", "contents": "Title: Algebraic Methods for Inferring Biochemical Networks: a Maximum\n  Likelihood Approach Abstract: We present a novel method for identifying a biochemical reaction network\nbased on multiple sets of estimated reaction rates in the corresponding\nreaction rate equations arriving from various (possibly different) experiments.\nThe current method, unlike some of the graphical approaches proposed in the\nliterature, uses the values of the experimental measurements only relative to\nthe geometry of the biochemical reactions under the assumption that the\nunderlying reaction network is the same for all the experiments.\n  The proposed approach utilizes algebraic statistical methods in order to\nparametrize the set of possible reactions so as to identify the most likely\nnetwork structure, and is easily scalable to very complicated biochemical\nsystems involving a large number of species and reactions. The method is\nillustrated with a numerical example of a hypothetical network arising form a\n\"mass transfer\"-type model. \n\n"}
{"id": "0810.4990", "contents": "Title: Osmotically driven flows in microchannels separated by a semipermeable\n  membrane Abstract: We perform experimental investigations of osmotically driven flows in\nartificial microchannels by studying the dynamics and structure of the front of\na sugar solution traveling in 200 um wide and 50-200 um deep microchannels. We\nfind that the sugar front travels with constant speed, and that this speed is\nproportional to the concentration of the sugar solution and inversely\nproportional to the depth of the channel. We propose a theoretical model,\nwhich, in the limit of low axial flow resistance, predicts that the sugar front\nindeed should travel with a constant velocity. The model also predicts an\ninverse relationship between the depth of the channel and the speed and a\nlinear relation between the sugar concentration and the speed. We thus find\ngood agreement between the experimental results and the predictions of the\nmodel. Our motivation for studying osmotically driven flows is that they are\nbelieved to be responsible for the translocation of sugar in plants through the\nphloem sieve element cells. Also, we suggest that osmotic elements can act as\nintegrated pumps with no movable parts in lab-on-a-chip systems. \n\n"}
{"id": "0811.0637", "contents": "Title: Optimality of Myopic Sensing in Multi-Channel Opportunistic Access Abstract: We consider opportunistic communications over multiple channels where the\nstate (\"good\" or \"bad\") of each channel evolves as independent and identically\ndistributed Markov processes. A user, with limited sensing and access\ncapability, chooses one channel to sense and subsequently access (based on the\nsensed channel state) in each time slot. A reward is obtained when the user\nsenses and accesses a \"good\" channel. The objective is to design the optimal\nchannel selection policy that maximizes the expected reward accrued over time.\nThis problem can be generally cast as a Partially Observable Markov Decision\nProcess (POMDP) or a restless multi-armed bandit process, to which optimal\nsolutions are often intractable. We show in this paper that the myopic policy,\nwith a simple and robust structure, achieves optimality under certain\nconditions. This result finds applications in opportunistic communications in\nfading environment, cognitive radio networks for spectrum overlay, and\nresource-constrained jamming and anti-jamming. \n\n"}
{"id": "0811.3644", "contents": "Title: Markov switching multinomial logit model: an application to accident\n  injury severities Abstract: In this study, two-state Markov switching multinomial logit models are\nproposed for statistical modeling of accident injury severities. These models\nassume Markov switching in time between two unobserved states of roadway\nsafety. The states are distinct, in the sense that in different states accident\nseverity outcomes are generated by separate multinomial logit processes. To\ndemonstrate the applicability of the approach presented herein, two-state\nMarkov switching multinomial logit models are estimated for severity outcomes\nof accidents occurring on Indiana roads over a four-year time interval.\nBayesian inference methods and Markov Chain Monte Carlo (MCMC) simulations are\nused for model estimation. The estimated Markov switching models result in a\nsuperior statistical fit relative to the standard (single-state) multinomial\nlogit models. It is found that the more frequent state of roadway safety is\ncorrelated with better weather conditions. The less frequent state is found to\nbe correlated with adverse weather conditions. \n\n"}
{"id": "0812.0972", "contents": "Title: Network Protection Codes: Providing Self-healing in Autonomic Networks\n  Using Network Coding Abstract: Agile recovery from link failures in autonomic communication networks is\nessential to increase robustness, accessibility, and reliability of data\ntransmission. However, this must be done with the least amount of protection\nresources, while using simple management plane functionality. Recently, network\ncoding has been proposed as a solution to provide agile and cost efficient\nnetwork self-healing against link failures, in a manner that does not require\ndata rerouting, packet retransmission, or failure localization, hence leading\nto simple control and management planes. To achieve this, separate paths have\nto be provisioned to carry encoded packets, hence requiring either the addition\nof extra links, or reserving some of the resources for this purpose.\n  In this paper we introduce autonomic self-healing strategies for autonomic\nnetworks in order to protect against link failures. The strategies are based on\nnetwork coding and reduced capacity, which is a technique that we call network\nprotection codes (NPC). In these strategies, an autonomic network is able to\nprovide self-healing from various network failures affecting network operation.\nThe techniques improve service and enhance reliability of autonomic\ncommunication.\n  Network protection codes are extended to provide self-healing from multiple\nlink failures in autonomic networks. We provide implementation aspects of the\nproposed strategies. We present bounds and network protection code\nconstructions. Finally, we study the construction of such codes over the binary\nfield. The paper also develops an Integer Linear Program formulation to\nevaluate the cost of provisioning connections using the proposed strategies. \n\n"}
{"id": "0812.1625", "contents": "Title: Simultaneous confidence intervals for the population cell means, for\n  two-by-two factorial data, that utilize uncertain prior information Abstract: Consider a two-by-two factorial experiment with more than 1 replicate.\nSuppose that we have uncertain prior information that the two-factor\ninteraction is zero. We describe new simultaneous frequentist confidence\nintervals for the 4 population cell means, with simultaneous confidence\ncoefficient 1-alpha, that utilize this prior information in the following\nsense. These simultaneous confidence intervals define a cube with expected\nvolume that (a) is relatively small when the two-factor interaction is zero and\n(b) has maximum value that is not too large. Also, these intervals coincide\nwith the standard simultaneous confidence intervals obtained by Tukey's method,\nwith simultaneous confidence coefficient 1-alpha, when the data strongly\ncontradict the prior information that the two-factor interaction is zero. We\nillustrate the application of these new simultaneous confidence intervals to a\nreal data set. \n\n"}
{"id": "0812.1690", "contents": "Title: Estimating limits from Poisson counting data using Dempster--Shafer\n  analysis Abstract: We present a Dempster--Shafer (DS) approach to estimating limits from Poisson\ncounting data with nuisance parameters. Dempster--Shafer is a statistical\nframework that generalizes Bayesian statistics. DS calculus augments\ntraditional probability by allowing mass to be distributed over power sets of\nthe event space. This eliminates the Bayesian dependence on prior distributions\nwhile allowing the incorporation of prior information when it is available. We\nuse the Poisson Dempster--Shafer model (DSM) to derive a posterior DSM for the\n``Banff upper limits challenge'' three-Poisson model. The results compare\nfavorably with other approaches, demonstrating the utility of the approach. We\nargue that the reduced dependence on priors afforded by the Dempster--Shafer\nframework is both practically and theoretically desirable. \n\n"}
{"id": "0812.3671", "contents": "Title: Regularized Multivariate Regression for Identifying Master Predictors\n  with Application to Integrative Genomics Study of Breast Cancer Abstract: In this paper, we propose a new method remMap -- REgularized Multivariate\nregression for identifying MAster Predictors -- for fitting multivariate\nresponse regression models under the high-dimension-low-sample-size setting.\nremMap is motivated by investigating the regulatory relationships among\ndifferent biological molecules based on multiple types of high dimensional\ngenomic data. Particularly, we are interested in studying the influence of DNA\ncopy number alterations on RNA transcript levels. For this purpose, we model\nthe dependence of the RNA expression levels on DNA copy numbers through\nmultivariate linear regressions and utilize proper regularizations to deal with\nthe high dimensionality as well as to incorporate desired network structures.\nCriteria for selecting the tuning parameters are also discussed. The\nperformance of the proposed method is illustrated through extensive simulation\nstudies. Finally, remMap is applied to a breast cancer study, in which genome\nwide RNA transcript levels and DNA copy numbers were measured for 172 tumor\nsamples. We identify a tran-hub region in cytoband 17q12-q21, whose\namplification influences the RNA expression levels of more than 30 unlinked\ngenes. These findings may lead to a better understanding of breast cancer\npathology. \n\n"}
{"id": "0901.2333", "contents": "Title: Q-CSMA: Queue-Length Based CSMA/CA Algorithms for Achieving Maximum\n  Throughput and Low Delay in Wireless Networks Abstract: Recently, it has been shown that CSMA-type random access algorithms can\nachieve the maximum possible throughput in ad hoc wireless networks. However,\nthese algorithms assume an idealized continuous-time CSMA protocol where\ncollisions can never occur. In addition, simulation results indicate that the\ndelay performance of these algorithms can be quite bad. On the other hand,\nalthough some simple heuristics (such as distributed approximations of greedy\nmaximal scheduling) can yield much better delay performance for a large set of\narrival rates, they may only achieve a fraction of the capacity region in\ngeneral. In this paper, we propose a discrete-time version of the CSMA\nalgorithm. Central to our results is a discrete-time distributed randomized\nalgorithm which is based on a generalization of the so-called Glauber dynamics\nfrom statistical physics, where multiple links are allowed to update their\nstates in a single time slot. The algorithm generates collision-free\ntransmission schedules while explicitly taking collisions into account during\nthe control phase of the protocol, thus relaxing the perfect CSMA assumption.\nMore importantly, the algorithm allows us to incorporate mechanisms which lead\nto very good delay performance while retaining the throughput-optimality\nproperty. It also resolves the hidden and exposed terminal problems associated\nwith wireless networks. \n\n"}
{"id": "0902.0751", "contents": "Title: Gene ranking and biomarker discovery under correlation Abstract: Biomarker discovery and gene ranking is a standard task in genomic high\nthroughput analysis. Typically, the ordering of markers is based on a\nstabilized variant of the t-score, such as the moderated t or the SAM\nstatistic. However, these procedures ignore gene-gene correlations, which may\nhave a profound impact on the gene orderings and on the power of the subsequent\ntests.\n  We propose a simple procedure that adjusts gene-wise t-statistics to take\naccount of correlations among genes. The resulting correlation-adjusted\nt-scores (\"cat\" scores) are derived from a predictive perspective, i.e. as a\nscore for variable selection to discriminate group membership in two-class\nlinear discriminant analysis. In the absence of correlation the cat score\nreduces to the standard t-score. Moreover, using the cat score it is\nstraightforward to evaluate groups of features (i.e. gene sets). For\ncomputation of the cat score from small sample data we propose a shrinkage\nprocedure. In a comparative study comprising six different synthetic and\nempirical correlation structures we show that the cat score improves estimation\nof gene orderings and leads to higher power for fixed true discovery rate, and\nvice versa. Finally, we also illustrate the cat score by analyzing metabolomic\ndata.\n  The shrinkage cat score is implemented in the R package \"st\" available from\nURL http://cran.r-project.org/web/packages/st/ \n\n"}
{"id": "0902.2808", "contents": "Title: Ultrametric Wavelet Regression of Multivariate Time Series: Application\n  to Colombian Conflict Analysis Abstract: We first pursue the study of how hierarchy provides a well-adapted tool for\nthe analysis of change. Then, using a time sequence-constrained hierarchical\nclustering, we develop the practical aspects of a new approach to wavelet\nregression. This provides a new way to link hierarchical relationships in a\nmultivariate time series data set with external signals. Violence data from the\nColombian conflict in the years 1990 to 2004 is used throughout. We conclude\nwith some proposals for further study on the relationship between social\nviolence and market forces, viz. between the Colombian conflict and the US\nnarcotics market. \n\n"}
{"id": "0902.3210", "contents": "Title: Coverage in Multi-Antenna Two-Tier Networks Abstract: In two-tier networks -- comprising a conventional cellular network overlaid\nwith shorter range hotspots (e.g. femtocells, distributed antennas, or wired\nrelays) -- with universal frequency reuse, the near-far effect from cross-tier\ninterference creates dead spots where reliable coverage cannot be guaranteed to\nusers in either tier. Equipping the macrocell and femtocells with multiple\nantennas enhances robustness against the near-far problem. This work derives\nthe maximum number of simultaneously transmitting multiple antenna femtocells\nmeeting a per-tier outage probability constraint. Coverage dead zones are\npresented wherein cross-tier interference bottlenecks cellular and hotspot\ncoverage. Two operating regimes are shown namely 1) a cellular-limited regime\nin which femtocell users experience unacceptable cross-tier interference and 2)\na hotspot-limited regime wherein both femtocell users and cellular users are\nlimited by hotspot interference. Our analysis accounts for the per-tier\ntransmit powers, the number of transmit antennas (single antenna transmission\nbeing a special case) and terrestrial propagation such as the Rayleigh fading\nand the path loss exponents. Single-user (SU) multiple antenna transmission at\neach tier is shown to provide significantly superior coverage and spatial reuse\nrelative to multiuser (MU) transmission. We propose a decentralized\ncarrier-sensing approach to regulate femtocell transmission powers based on\ntheir location. Considering a worst-case cell-edge location, simulations using\ntypical path loss scenarios show that our interference management strategy\nprovides reliable cellular coverage with about 60 femtocells per cellsite. \n\n"}
{"id": "0902.3619", "contents": "Title: Context tree selection and linguistic rhythm retrieval from written\n  texts Abstract: The starting point of this article is the question \"How to retrieve\nfingerprints of rhythm in written texts?\" We address this problem in the case\nof Brazilian and European Portuguese. These two dialects of Modern Portuguese\nshare the same lexicon and most of the sentences they produce are superficially\nidentical. Yet they are conjectured, on linguistic grounds, to implement\ndifferent rhythms. We show that this linguistic question can be formulated as a\nproblem of model selection in the class of variable length Markov chains. To\ncarry on this approach, we compare texts from European and Brazilian\nPortuguese. These texts are previously encoded according to some basic rhythmic\nfeatures of the sentences which can be automatically retrieved. This is an\nentirely new approach from the linguistic point of view. Our statistical\ncontribution is the introduction of the smallest maximizer criterion which is a\nconstant free procedure for model selection. As a by-product, this provides a\nsolution for the problem of optimal choice of the penalty constant when using\nthe BIC to select a variable length Markov chain. Besides proving the\nconsistency of the smallest maximizer criterion when the sample size diverges,\nwe also make a simulation study comparing our approach with both the standard\nBIC selection and the Peres-Shields order estimation. Applied to the linguistic\nsample constituted for our case study, the smallest maximizer criterion assigns\ndifferent context-tree models to the two dialects of Portuguese. The features\nof the selected models are compatible with current conjectures discussed in the\nlinguistic literature. \n\n"}
{"id": "0902.3907", "contents": "Title: Gaussian Process Modelling of Asteroseismic Data Abstract: The measured properties of stellar oscillations can provide powerful\nconstraints on the internal structure and composition of stars. To begin this\nprocess, oscillation frequencies must be extracted from the observational data,\ntypically time series of the star's brightness or radial velocity. In this\npaper, a probabilistic model is introduced for inferring the frequencies and\namplitudes of stellar oscillation modes from data, assuming that there is some\nperiodic character to the oscillations, but that they may not be exactly\nsinusoidal. Effectively we fit damped oscillations to the time series, and\nhence the mode lifetime is also recovered. While this approach is\ncomputationally demanding for large time series (> 1500 points), it should at\nleast allow improved analysis of observations of solar-like oscillations in\nsubgiant and red giant stars, as well as sparse observations of semiregular\nstars, where the number of points in the time series is often low. The method\nis demonstrated on simulated data and then applied to radial velocity\nmeasurements of the red giant star xi Hydrae, yielding a mode lifetime between\n0.41 and 2.65 days with 95% posterior probability. The large frequency\nseparation between modes is ambiguous, however we argue that the most plausible\nvalue is 6.3 microHz, based on the radial velocity data and the star's position\nin the HR diagram. \n\n"}
{"id": "0902.4111", "contents": "Title: Bivariate Instantaneous Frequency and Bandwidth Abstract: The generalizations of instantaneous frequency and instantaneous bandwidth to\na bivariate signal are derived. These are uniquely defined whether the signal\nis represented as a pair of real-valued signals, or as one analytic and one\nanti-analytic signal. A nonstationary but oscillatory bivariate signal has a\nnatural representation as an ellipse whose properties evolve in time, and this\nrepresentation provides a simple geometric interpretation for the bivariate\ninstantaneous moments. The bivariate bandwidth is shown to consist of three\nterms measuring the degree of instability of the time-varying ellipse:\namplitude modulation with fixed eccentricity, eccentricity modulation, and\norientation modulation or precession. An application to the analysis of data\nfrom a free-drifting oceanographic float is presented and discussed. \n\n"}
{"id": "0903.3623", "contents": "Title: Matrix plots of reordered bistochastized transaction flow tables: A\n  United States intercounty migration example Abstract: We present a number of variously rearranged matrix plots of the $3, 107\n\\times 3, 107$ 1995-2000 (asymmetric) intercounty migration table for the\nUnited States, principally in its bistochasticized form (all 3,107 row and\ncolumn sums iteratively proportionally fitted to equal 1). In one set of plots,\nthe counties are seriated on the bases of the subdominant (left and right)\neigenvectors of the bistochastic matrix. In another set, we use the ordering of\ncounties in the dendrogram generated by the associated strong component\nhierarchical clustering. Interesting, diverse features of U. S. intercounty\nmigration emerge--such as a contrast in centralized, hub-like\n(cosmopolitan/provincial) properties between cosmopolitan \"Sunbelt\" and\nprovincial \"Black Belt\" counties. The methodologies employed should also be\ninsightful for the many other diverse forms of interesting transaction\nflow-type data--interjournal citations being an obvious, much-studied example,\nwhere one might expect that the journals Science, Nature and PNAS would display\n\"cosmopolitan\" characteristics. \n\n"}
{"id": "0904.1730", "contents": "Title: Feedback-based online network coding Abstract: Current approaches to the practical implementation of network coding are\nbatch-based, and often do not use feedback, except possibly to signal\ncompletion of a file download. In this paper, the various benefits of using\nfeedback in a network coded system are studied. It is shown that network coding\ncan be performed in a completely online manner, without the need for batches or\ngenerations, and that such online operation does not affect the throughput.\nAlthough these ideas are presented in a single-hop packet erasure broadcast\nsetting, they naturally extend to more general lossy networks which employ\nnetwork coding in the presence of feedback. The impact of feedback on queue\nsize at the sender and decoding delay at the receivers is studied. Strategies\nfor adaptive coding based on feedback are presented, with the goal of\nminimizing the queue size and delay. The asymptotic behavior of these metrics\nis characterized, in the limit of the traffic load approaching capacity.\nDifferent notions of decoding delay are considered, including an\norder-sensitive notion which assumes that packets are useful only when\ndelivered in order. Our work may be viewed as a natural extension of Automatic\nRepeat reQuest (ARQ) schemes to coded networks. \n\n"}
{"id": "0904.2207", "contents": "Title: Delayed rejection schemes for efficient Markov-Chain Monte-Carlo\n  sampling of multimodal distributions Abstract: A number of problems in a variety of fields are characterised by target\ndistributions with a multimodal structure in which the presence of several\nisolated local maxima dramatically reduces the efficiency of Markov Chain Monte\nCarlo sampling algorithms. Several solutions, such as simulated tempering or\nthe use of parallel chains, have been proposed to facilitate the exploration of\nthe relevant parameter space. They provide effective strategies in the cases in\nwhich the dimension of the parameter space is small and/or the computational\ncosts are not a limiting factor. These approaches fail however in the case of\nhigh-dimensional spaces where the multimodal structure is induced by\ndegeneracies between regions of the parameter space. In this paper we present a\nfully Markovian way to efficiently sample this kind of distribution based on\nthe general Delayed Rejection scheme with an arbitrary number of steps, and\nprovide details for an efficient numerical implementation of the algorithm. \n\n"}
{"id": "0905.2979", "contents": "Title: Extreme deconvolution: Inferring complete distribution functions from\n  noisy, heterogeneous and incomplete observations Abstract: We generalize the well-known mixtures of Gaussians approach to density\nestimation and the accompanying Expectation--Maximization technique for finding\nthe maximum likelihood parameters of the mixture to the case where each data\npoint carries an individual $d$-dimensional uncertainty covariance and has\nunique missing data properties. This algorithm reconstructs the\nerror-deconvolved or \"underlying\" distribution function common to all samples,\neven when the individual data points are samples from different distributions,\nobtained by convolving the underlying distribution with the heteroskedastic\nuncertainty distribution of the data point and projecting out the missing data\ndirections. We show how this basic algorithm can be extended with conjugate\npriors on all of the model parameters and a \"split-and-merge\" procedure\ndesigned to avoid local maxima of the likelihood. We demonstrate the full\nmethod by applying it to the problem of inferring the three-dimensional\nvelocity distribution of stars near the Sun from noisy two-dimensional,\ntransverse velocity measurements from the Hipparcos satellite. \n\n"}
{"id": "0905.3789", "contents": "Title: Nonequilibrium phase transition in a mesoscopic biochemical system: From\n  stochastic to nonlinear dynamics and beyond Abstract: A rigorous mathematical framework for analyzing the chemical master equation\n(CME) with bistability, based on the theory of large deviation, is proposed.\nUsing a simple phosphorylation-dephosphorylation cycle with feedback as an\nexample, we show that a nonequilibrium steady-state (NESS) phase transition\noccurs in the system which has all the characteristics of classic equilibrium\nphase transition: Maxwell construction, discontinuous fraction of\nphosphorylation as a function of the kinase activity, and Lee-Yang's zero for\nthe generating function. The cusp in nonlinear bifurcation theory matches the\ntricritical point of the phase transition. The mathematical analysis suggests\nthree distinct time scales, and related mathematical descriptions, of (i)\nmolecular signaling, (ii) biochemical network dynamics, and (iii) cellular\nevolution. The (i) and (iii) are stochastic while (ii) is deterministic. \n\n"}
{"id": "0906.5254", "contents": "Title: Quantum Measurement Theory Explains the Deuteration Effect in\n  Radical-Ion-Pair Reactions Abstract: It has been recently shown that radical-ion pairs and their reactions are a\nparadigm biological system manifesting non-trivial quantum effects, so far\ninvisible due to the phenomenological description of radical-ion-pair reactions\nused until now. We here use the quantum-mechanically consistent master equation\ndescribing magnetic-sensitive radical-ion-pair reactions to explain\nexperimental data [C. R. Timmel and K. B. Henbest, Phil. Trans. R. Soc. Lond. A\n{\\bf 362}, 2573 (2004); C. T. Rodgers, S. A. Norman, K. B. Henbest, C. R.\nTimmel and P. J. Hore, J. Am. Chem. Soc. {\\bf 129} 6746 (2007)] on the effect\nof deuteration on the reaction yields. Anomalous behavior of radical-ion-pair\nreactions after deuteration, i.e. data inconsistent with the predictions of the\nphenomenological theory used so far, has been observed since the 70's and has\nremained unexplained until now. \n\n"}
{"id": "0907.2393", "contents": "Title: Multiscale Network Reduction Methodologies: Bistochastic and Disparity\n  Filtering of Human Migration Flows between 3,000+ U. S. Counties Abstract: To control for multiscale effects in networks, one can transform the matrix\nof (in general) weighted, directed internodal flows to bistochastic\n(doubly-stochastic) form, using the iterative proportional fitting\n(Sinkhorn-Knopp) procedure, which alternatively scales row and column sums to\nall equal 1. The dominant entries in the bistochasticized table can then be\nemployed for network reduction, using strong component hierarchical clustering.\nWe illustrate various facets of this well-established, widely-applied two-stage\nalgorithm with the 3, 107 x 3, 107 (asymmetric) 1995-2000 intercounty migration\nflow table for the United States. We compare the results obtained with ones\nusing the disparity filter, for \"extracting the \"multiscale backbone of complex\nweighted networks\", recently put forth by Serrano, Boguna and Vespignani (SBV)\n(Proc. Natl. Acad. Sci. 106 [2009], 6483), upon which we have briefly commented\n(Proc. Natl. Acad. Sci. 106 [2009], E66). The performance of the bistochastic\nfilter appears to be superior-at least in this specific case-in two respects:\n(1) it requires far fewer links to complete a stongly-connected network\nbackbone; and (2) it \"belittles\" small flows and nodes less-a principal\ndesideratum of SBV-in the sense that the correlations of the nonzero raw flows\nare considerably weaker with the corresponding bistochastized links than with\nthe significance levels yielded by the disparity filter. Additional comparative\nstudies--as called for by SBV-of these two filtering procedures, in particular\nas regards their topological properties, should be of considerable interest.\nRelatedly, in its many geographic applications, the two-stage procedure\nhas--with rare exceptions-clustered contiguous areas, often reconstructing\ntraditional regions (islands, for example), even though no contiguity\nconstraints, at all, are imposed beforehand. \n\n"}
{"id": "0907.3166", "contents": "Title: Checking election outcome accuracy Post-election audit sampling Abstract: This article\n  * provides an overview of post-election audit sampling research and compares\nvarious approaches to calculating post-election audit sample sizes, focusing on\nrisklimiting audits,\n  * discusses fundamental concepts common to all risk-limiting post-election\naudits, presenting new margin error bounds, sampling weights and sampling\nprobabilities that improve upon existing approaches and work for any size audit\nunit and for single or multi-winner election contests,\n  * provides two new simple formulas for estimating post-election audit sample\nsizes in cases when detailed data, expertise, or tools are not available,\n  * summarizes four improved methods for calculating risk-limiting election\naudit sample sizes, showing how to apply precise margin error bounds to improve\nthe accuracy and efficacy of existing methods, and\n  * discusses sampling mistakes that reduce post-election audit effectiveness. \n\n"}
{"id": "0910.1847", "contents": "Title: Limits of quantum speedup in photosynthetic light harvesting Abstract: It has been suggested that excitation transport in photosynthetic light\nharvesting complexes features speedups analogous to those found in quantum\nalgorithms. Here we compare the dynamics in these light harvesting systems to\nthe dynamics of quantum walks, in order to elucidate the limits of such quantum\nspeedups. For the Fenna-Matthews-Olson (FMO) complex of green sulfur bacteria,\nwe show that while there is indeed speedup at short times, this is short lived\n(70 fs) despite longer lived (ps) quantum coherence. Remarkably, this time\nscale is independent of the details of the decoherence model. More generally,\nwe show that the distinguishing features of light-harvesting complexes not only\nlimit the extent of quantum speedup but also reduce rates of diffusive\ntransport. These results suggest that quantum coherent effects in biological\nsystems are optimized for efficiency or robustness rather than the more elusive\ngoal of quantum speedup. \n\n"}
{"id": "0911.2716", "contents": "Title: Non-parametric Deprojection of Surface Brightness Profiles of Galaxies\n  in Generalised Geometries Abstract: We present a new Bayesian non-parametric deprojection algorithm DOPING\n(Deprojection of Observed Photometry using and INverse Gambit), that is\ndesigned to extract 3-D luminosity density distributions $\\rho$ from observed\nsurface brightness maps $I$, in generalised geometries, while taking into\naccount changes in intrinsic shape with radius, using a penalised likelihood\napproach and an MCMC optimiser. We provide the most likely solution to the\nintegral equation that represents deprojection of the measured $I$ to $\\rho$.\nIn order to keep the solution modular, we choose to express $\\rho$ as a\nfunction of the line-of-sight (LOS) coordinate $z$. We calculate the extent of\nthe system along the ${\\bf z}$-axis, for a given point on the image that lies\nwithin an identified isophotal annulus. The extent along the LOS is binned and\ndensity is held a constant over each such $z$-bin. The code begins with a seed\ndensity and at the beginning of an iterative step, the trial $\\rho$ is updated.\nComparison of the projection of the current choice of $\\rho$ and the observed\n$I$ defines the likelihood function (which is supplemented by Laplacian\nregularisation), the maximal region of which is sought by the optimiser\n(Metropolis Hastings). The algorithm is successfully tested on a set of test\ngalaxies, the morphology of which ranges from an elliptical galaxy with varying\neccentricity to an infinitesimally thin disk galaxy marked by an abruptly\nvarying eccentricity profile. Applications are made to faint dwarf elliptical\ngalaxy Ic~3019 and another dwarf elliptical that is characterised by a central\nspheroidal nuclear component superimposed upon a more extended flattened\ncomponent. The result of deprojection of the X-ray image of triaxial cluster\nA1413 is also presented. \n\n"}
{"id": "0911.5427", "contents": "Title: Environmental correlation effects on excitation energy transfer in\n  photosynthetic light harvesting Abstract: Several recent studies of energy transfer in photosynthetic light harvesting\ncomplexes have revealed a subtle interplay between coherent and decoherent\ndynamic contributions to the overall transfer efficiency in these open quantum\nsystems. In this work we systematically investigate the impact of temporal and\nspatial correlations in environmental fluctuations on excitation transport in\nthe Fenna-Matthews-Olson photosynthetic complex. We demonstrate that the exact\nnature of the correlations can have a large impact on the efficiency of light\nharvesting. In particular, we find that (i) spatial correlations can enhance\ncoherences in the site basis while at the same time slowing transport, and (ii)\nthe overall efficiency of transport is optimized at a finite temporal\ncorrelation that produces maximum overlap between the environmental power\nspectrum and the excitonic energy differences, which in turn results in\nenhanced driving of transitions between excitonic states. \n\n"}
{"id": "0912.0201", "contents": "Title: LSST Science Book, Version 2.0 Abstract: A survey that can cover the sky in optical bands over wide fields to faint\nmagnitudes with a fast cadence will enable many of the exciting science\nopportunities of the next decade. The Large Synoptic Survey Telescope (LSST)\nwill have an effective aperture of 6.7 meters and an imaging camera with field\nof view of 9.6 deg^2, and will be devoted to a ten-year imaging survey over\n20,000 deg^2 south of +15 deg. Each pointing will be imaged 2000 times with\nfifteen second exposures in six broad bands from 0.35 to 1.1 microns, to a\ntotal point-source depth of r~27.5. The LSST Science Book describes the basic\nparameters of the LSST hardware, software, and observing plans. The book\ndiscusses educational and outreach opportunities, then goes on to describe a\nbroad range of science that LSST will revolutionize: mapping the inner and\nouter Solar System, stellar populations in the Milky Way and nearby galaxies,\nthe structure of the Milky Way disk and halo and other objects in the Local\nVolume, transient and variable objects both at low and high redshift, and the\nproperties of normal and active galaxies at low and high redshift. It then\nturns to far-field cosmological topics, exploring properties of supernovae to\nz~1, strong and weak lensing, the large-scale distribution of galaxies and\nbaryon oscillations, and how these different probes may be combined to\nconstrain cosmological models and the physics of dark energy. \n\n"}
{"id": "0912.4554", "contents": "Title: Lambert W random variables - a new family of generalized skewed\n  distributions with applications to risk estimation Abstract: Originating from a system theory and an input/output point of view, I\nintroduce a new class of generalized distributions. A parametric nonlinear\ntransformation converts a random variable $X$ into a so-called Lambert $W$\nrandom variable $Y$, which allows a very flexible approach to model skewed\ndata. Its shape depends on the shape of $X$ and a skewness parameter $\\gamma$.\nIn particular, for symmetric $X$ and nonzero $\\gamma$ the output $Y$ is skewed.\nIts distribution and density function are particular variants of their input\ncounterparts. Maximum likelihood and method of moments estimators are\npresented, and simulations show that in the symmetric case additional\nestimation of $\\gamma$ does not affect the quality of other parameter\nestimates. Applications in finance and biomedicine show the relevance of this\nclass of distributions, which is particularly useful for slightly skewed data.\nA practical by-result of the Lambert $W$ framework: data can be \"unskewed.\" The\n$R$ package http://cran.r-project.org/web/packages/LambertWLambertW developed\nby the author is publicly available (http://cran.r-project.orgCRAN). \n\n"}
{"id": "0912.5112", "contents": "Title: Identifying the quantum correlations in light-harvesting complexes Abstract: One of the major efforts in the quantum biological program is to subject\nbiological systems to standard tests or measures of quantumness. These tests\nand measures should elucidate if non-trivial quantum effects may be present in\nbiological systems. Two such measures of quantum correlations are the quantum\ndiscord and the relative entropy of entanglement. Here, we show that the\nrelative entropy of entanglement admits a simple analytic form when dynamics\nand accessible degrees of freedom are restricted to a zero- and\nsingle-excitation subspace. We also simulate and calculate the amount of\nquantum discord that is present in the Fenna-Matthews-Olson protein complex\nduring the transfer of an excitation from a chlorosome antenna to a reaction\ncenter. We find that the single-excitation quantum discord and relative entropy\nof entanglement are equal for all of our numerical simulations, but a proof of\ntheir general equality for this setting evades us for now. Also, some of our\nsimulations demonstrate that the relative entropy of entanglement without the\nsingle-excitation restriction is much lower than the quantum discord. The first\npicosecond of dynamics is the relevant timescale for the transfer of the\nexcitation, according to some sources in the literature. Our simulation results\nindicate that quantum correlations contribute a significant fraction of the\ntotal correlation during this first picosecond in many cases, at both cryogenic\nand physiological temperature. \n\n"}
{"id": "1001.0279", "contents": "Title: Regularization for Matrix Completion Abstract: We consider the problem of reconstructing a low rank matrix from noisy\nobservations of a subset of its entries. This task has applications in\nstatistical learning, computer vision, and signal processing. In these\ncontexts, \"noise\" generically refers to any contribution to the data that is\nnot captured by the low-rank model. In most applications, the noise level is\nlarge compared to the underlying signal and it is important to avoid\noverfitting. In order to tackle this problem, we define a regularized cost\nfunction well suited for spectral reconstruction methods. Within a random noise\nmodel, and in the large system limit, we prove that the resulting accuracy\nundergoes a phase transition depending on the noise level and on the fraction\nof observed entries. The cost function can be minimized using OPTSPACE (a\nmanifold gradient descent algorithm). Numerical simulations show that this\napproach is competitive with state-of-the-art alternatives. \n\n"}
{"id": "1001.3006", "contents": "Title: Asymptotic equivalence and sufficiency for volatility estimation under\n  microstructure noise Abstract: The basic model for high-frequency data in finance is considered, where an\nefficient price process is observed under microstructure noise. It is shown\nthat this nonparametric model is in Le Cam's sense asymptotically equivalent to\na Gaussian shift experiment in terms of the square root of the volatility\nfunction $\\sigma$. As an application, simple rate-optimal estimators of the\nvolatility and efficient estimators of the integrated volatility are\nconstructed. \n\n"}
{"id": "1001.4639", "contents": "Title: The scaling relation between richness and mass of galaxy clusters: a\n  Bayesian approach Abstract: We use a sample of 53 galaxy clusters at 0.03 < z < 0.1 with available masses\nderived from the caustic technique and with velocity dispersions computed using\n208 galaxies on average per cluster, in order to investigate the scaling\nbetween richness, mass and velocity dispersion. A tight scaling between\nrichness and mass is found, with an intrinsic scatter of only 0.19 dex in mass\nand with a slope one, i.e. clusters which have twice as many galaxies are twice\nas massive. When richness is measured without any knowledge of the cluster mass\nor linked parameters (such as r200), it can predict mass with an uncertainty of\n0.29+/-0.01 dex. As a mass proxy, richness competes favourably with both direct\nmeasurements of mass given by the caustic method, which has typically 0.14 dex\nerrors (vs 0.29) and X-ray luminosity, which offers a similar 0.30 dex\nuncertainty. The similar performances of X-ray luminosity and richness in\npredicting cluster masses has been confirmed using cluster masses derived from\nvelocity dispersion fixed by numerical simulations. These results suggest that\ncluster masses can be reliably estimated from simple galaxy counts, at least at\nthe redshift and masses explored in this work. This has important applications\nin the estimation of cosmological parameters from optical cluster surveys,\nbecause in current surveys clusters detected in the optical range outnumber, by\nat least one order of magnitude, those detected in X-ray. Our analysis is\nrobust from astrophysical and statistical perspectives. The data and code used\nfor the stochastic computation is distributed with the paper. [Abridged] \n\n"}
{"id": "1002.0496", "contents": "Title: Dimerization-assisted energy transport in light-harvesting complexes Abstract: We study the role of the dimer structure of light-harvesting complex II (LH2)\nin excitation transfer from the LH2 (without a reaction center (RC)) to the LH1\n(surrounding the RC), or from the LH2 to another LH2. The excited and\nun-excited states of a bacteriochlorophyll (BChl) are modeled by a quasi-spin.\nIn the framework of quantum open system theory, we represent the excitation\ntransfer as the total leakage of the LH2 system and then calculate the transfer\nefficiency and average transfer time. For different initial states with various\nquantum superposition properties, we study how the dimerization of the B850\nBChl ring can enhance the transfer efficiency and shorten the average transfer\ntime. \n\n"}
{"id": "1002.1312", "contents": "Title: Adaptive LASSO-type estimation for ergodic diffusion processes Abstract: The LASSO is a widely used statistical methodology for simultaneous\nestimation and variable selection. In the last years, many authors analyzed\nthis technique from a theoretical and applied point of view. We introduce and\nstudy the adaptive LASSO problem for discretely observed ergodic diffusion\nprocesses. We prove oracle properties also deriving the asymptotic distribution\nof the LASSO estimator. Our theoretical framework is based on the random field\napproach and it applied to more general families of regular statistical\nexperiments in the sense of Ibragimov-Hasminskii (1981). Furthermore, we\nperform a simulation and real data analysis to provide some evidence on the\napplicability of this method. \n\n"}
{"id": "1002.2426", "contents": "Title: The Sensitivity of Respondent-driven Sampling Method Abstract: Researchers in many scientific fields make inferences from individuals to\nlarger groups. For many groups however, there is no list of members from which\nto take a random sample. Respondent-driven sampling (RDS) is a relatively new\nsampling methodology that circumvents this difficulty by using the social\nnetworks of the groups under study. The RDS method has been shown to provide\nunbiased estimates of population proportions given certain conditions. The\nmethod is now widely used in the study of HIV-related high-risk populations\nglobally. In this paper, we test the RDS methodology by simulating RDS studies\non the social networks of a large LGBT web community. The robustness of the RDS\nmethod is tested by violating, one by one, the conditions under which the\nmethod provides unbiased estimates. Results reveal that the risk of bias is\nlarge if networks are directed, or respondents choose to invite persons based\non characteristics that are correlated with the study outcomes. If these two\nproblems are absent, the RDS method shows strong resistance to low response\nrates and certain errors in the participants' reporting of their network sizes.\nOther issues that might affect the RDS estimates, such as the method for\nchoosing initial participants, the maximum number of recruitments per\nparticipant, sampling with or without replacement and variations in network\nstructures, are also simulated and discussed. \n\n"}
{"id": "1002.3878", "contents": "Title: The Three Doors Problem...-s Abstract: I argue that we must distinguish between:\n  (0) the Three-Doors-Problem Problem [sic], which is to make sense of some\nreal world question of a real person.\n  (1) a large number of solutions to this meta-problem, i.e., many specific\nThree-Doors-Problem problems, which are competing mathematizations of the\nmeta-problem (0).\n  Each of the solutions at level (1) can well have a number of different\nsolutions: nice ones and ugly ones; correct ones and incorrect ones. I discuss\nthree level (1) solutions, i.e., three different Monty Hall problems; and try\nto give three short correct and attractive solutions. These are: an\nunconditional probability question; a conditional probability question; and a\ngame-theory question.\n  The meta-message of the article is that applied statisticians should beware\nof solution-driven science. \n\n"}
{"id": "1003.1442", "contents": "Title: Entropy Considerations in Spin-Selective Radical-Ion-Pair Reactions Abstract: Radical-ion-pair reactions were recently shown to manifest a host of\nnon-trivial quantum effects accounted for by quantum measurement theory. An\nalternative approach purporting to describe the fundamental quantum dynamics of\nspin-selective radical-ion pair reactions was introduced most recently,\nbringing to three the competing theories, including the one traditionally used\nin spin chemistry. We here consider entropy as a fundamental concept enabling a\ncomparison of the predictions of these theories against what is physically\nacceptable on quite general grounds. \n\n"}
{"id": "1003.2469", "contents": "Title: The Directed Closure Process in Hybrid Social-Information Networks, with\n  an Analysis of Link Formation on Twitter Abstract: It has often been taken as a working assumption that directed links in\ninformation networks are frequently formed by \"short-cutting\" a two-step path\nbetween the source and the destination -- a kind of implicit \"link copying\"\nanalogous to the process of triadic closure in social networks. Despite the\nrole of this assumption in theoretical models such as preferential attachment,\nit has received very little direct empirical investigation. Here we develop a\nformalization and methodology for studying this type of directed closure\nprocess, and we provide evidence for its important role in the formation of\nlinks on Twitter. We then analyze a sequence of models designed to capture the\nstructural phenomena related to directed closure that we observe in the Twitter\ndata. \n\n"}
{"id": "1003.3857", "contents": "Title: Quantum coherent biomolecular energy transfer with spatially correlated\n  fluctuations Abstract: We show that the quantum coherent transfer of excitations between\nbiomolecular chromophores is strongly influenced by spatial correlations of the\nenvironmental fluctuations. The latter are due either to propagating\nenvironmental modes or to local fluctuations with a finite localization length.\nA simple toy model of a single donor-acceptor pair with spatially separated\nchromophore sites allows to investigate the influence of these spatial\ncorrelations on the quantum coherent excitation transfer. The sound velocity of\nthe solvent determines the wave lengths of the environmental modes, which, in\nturn, has to be compared to the spatial distance of the chromophore sites. When\nthe wave length exceeds the distance between donor and acceptor site, we find\nstrong suppression of decoherence. In addition, we consider two spatially\nseparated donor-acceptor pairs under the influence of propagating environmental\nmodes. Depending on their wave lengths fixed by the sound velocity of the\nsolvent material, the spatial range of correlations may extend over typical\ninterpair distances, which can lead to an increase of the decohering influence\nof the solvent. Surprisingly, this effect is counteracted by increasing\ntemperature. \n\n"}
{"id": "1004.2785", "contents": "Title: The stellar mass fraction and baryon content of galaxy clusters and\n  groups Abstract: [Abridged] The analysis of a sample of 52 clusters with precise and\nhypothesis-parsimonious measurements of mass shows that low mass clusters and\ngroups are not simple scaled-down versions of their massive cousins in terms of\nstellar content: lighter clusters have more stars per unit cluster mass. The\nsame analysis also shows that the stellar content of clusters and groups\ndisplays an intrinsic spread at a given cluster mass, i.e. clusters are not\nsimilar each other in the amount of stars they contain, not even at a fixed\ncluster mass. The stellar mass fraction depends on halo mass with (logarithmic)\nslope -0.55+/-0.08 and with 0.15+/-0.02 dex of intrinsic scatter at a fixed\ncluster mass. The intrinsic scatter at a fixed cluster mass we determine for\ngas mass fractions is smaller, 0.06+/-0.01 dex. The intrinsic scatter in both\nthe stellar and gas mass fractions is a distinctive signature that the regions\nfrom which clusters and groups collected matter, a few tens of Mpc, are yet not\nrepresentative, in terms of gas and baryon content, of the mean matter content\nof the Universe. The observed stellar mass fraction values are in marked\ndisagreement with gasdynamics simulations with cooling and star formation of\nclusters and groups. We found the the baryon (gas+stellar) fraction is fairly\nconstant for clusters and groups with 13.7<lg(mass)<15.0 solar masses and it is\noffset from the WMAP-derived value by about 6 sigmas. The offset could be\nrelated to the possible non universality of the baryon fraction pointed out by\nour measurements of the intrinsic scatter. Our analysis is the first that does\nnot assume that clusters are identically equal at a given halo mass and it is\nalso more accurate in many aspects. The data and code used for the stochastic\ncomputation are distributed with the paper. \n\n"}
{"id": "1004.4704", "contents": "Title: Homophily and Contagion Are Generically Confounded in Observational\n  Social Network Studies Abstract: We consider processes on social networks that can potentially involve three\nfactors: homophily, or the formation of social ties due to matching individual\ntraits; social contagion, also known as social influence; and the causal effect\nof an individual's covariates on their behavior or other measurable responses.\nWe show that, generically, all of these are confounded with each other.\nDistinguishing them from one another requires strong assumptions on the\nparametrization of the social process or on the adequacy of the covariates used\n(or both). In particular we demonstrate, with simple examples, that asymmetries\nin regression coefficients cannot identify causal effects, and that very simple\nmodels of imitation (a form of social contagion) can produce substantial\ncorrelations between an individual's enduring traits and their choices, even\nwhen there is no intrinsic affinity between them. We also suggest some possible\nconstructive responses to these results. \n\n"}
{"id": "1005.0698", "contents": "Title: Analysis of Brownian Dynamics Simulations of Reversible Bimolecular\n  Reactions Abstract: A class of Brownian dynamics algorithms for stochastic reaction-diffusion\nmodels which include reversible bimolecular reactions is presented and\nanalyzed. The method is a generalization of the $\\lambda$--$\\newrho$ model for\nirreversible bimolecular reactions which was introduced in [arXiv:0903.1298].\nThe formulae relating the experimentally measurable quantities (reaction rate\nconstants and diffusion constants) with the algorithm parameters are derived.\nThe probability of geminate recombination is also investigated. \n\n"}
{"id": "1005.2857", "contents": "Title: Molecular Dynamics simulations of concentrated aqueous electrolyte\n  solutions Abstract: Transport properties of concentrated electrolytes have been analyzed using\nclassical molecular dynamics simulations with the algorithms and parameters\ntypical of simulations describing complex electrokinetic phenomena. The\nelectrical conductivity and transport numbers of electrolytes containing\nmonovalent (KCl), divalent (MgCl$_2$), a mixture of both (KCl + MgCl$_2$), and\ntrivalent (LaCl$_3$) cations have been obtained from simulations of the\nelectrolytes in electric fields of different magnitude. The results obtained\nfor different simulation parameters have been discussed and compared with\nexperimental measurements of our own and from the literature. The\nelectroosmotic flow of water molecules induced by the ionic current in the\ndifferent cases has been calculated and interpreted with the help of the\nhydration properties extracted from the simulations. \n\n"}
{"id": "1006.0158", "contents": "Title: Statistics of voltage drop in radial distribution circuits: a dynamic\n  programming approach Abstract: We analyze a power distribution line with high penetration of distributed\ngeneration and strong variations of power consumption and generation levels. In\nthe presence of uncertainty the statistical description of the system is\nrequired to assess the risks of power outages. In order to find the probability\nof exceeding the constraints for voltage levels we introduce the probability\ndistribution of maximal voltage drop and propose an algorithm for finding this\ndistribution. The algorithm is based on the assumption of random but\nstatistically independent distribution of loads on buses. Linear complexity in\nthe number of buses is achieved through the dynamic programming technique. We\nillustrate the performance of the algorithm by analyzing a simple 4-bus system\nwith high variations of load levels. \n\n"}
{"id": "1007.3424", "contents": "Title: Bacterial Community Reconstruction Using A Single Sequencing Reaction Abstract: Bacteria are the unseen majority on our planet, with millions of species and\ncomprising most of the living protoplasm. While current methods enable in-depth\nstudy of a small number of communities, a simple tool for breadth studies of\nbacterial population composition in a large number of samples is lacking. We\npropose a novel approach for reconstruction of the composition of an unknown\nmixture of bacteria using a single Sanger-sequencing reaction of the mixture.\nThis method is based on compressive sensing theory, which deals with\nreconstruction of a sparse signal using a small number of measurements.\nUtilizing the fact that in many cases each bacterial community is comprised of\na small subset of the known bacterial species, we show the feasibility of this\napproach for determining the composition of a bacterial mixture. Using\nsimulations, we show that sequencing a few hundred base-pairs of the 16S rRNA\ngene sequence may provide enough information for reconstruction of mixtures\ncontaining tens of species, out of tens of thousands, even in the presence of\nrealistic measurement noise. Finally, we show initial promising results when\napplying our method for the reconstruction of a toy experimental mixture with\nfive species. Our approach may have a potential for a practical and efficient\nway for identifying bacterial species compositions in biological samples. \n\n"}
{"id": "1008.0149", "contents": "Title: Bayesian Cointegrated Vector Autoregression models incorporating\n  Alpha-stable noise for inter-day price movements via Approximate Bayesian\n  Computation Abstract: We consider a statistical model for pairs of traded assets, based on a\nCointegrated Vector Auto Regression (CVAR) Model. We extend standard CVAR\nmodels to incorporate estimation of model parameters in the presence of price\nseries level shifts which are not accurately modeled in the standard Gaussian\nerror correction model (ECM) framework. This involves developing a novel matrix\nvariate Bayesian CVAR mixture model comprised of Gaussian errors intra-day and\nAlpha-stable errors inter-day in the ECM framework. To achieve this we derive a\nnovel conjugate posterior model for the Scaled Mixtures of Normals (SMiN CVAR)\nrepresentation of Alpha-stable inter-day innovations. These results are\ngeneralized to asymmetric models for the innovation noise at inter-day\nboundaries allowing for skewed Alpha-stable models.\n  Our proposed model and sampling methodology is general, incorporating the\ncurrent literature on Gaussian models as a special subclass and also allowing\nfor price series level shifts either at random estimated time points or known a\npriori time points. We focus analysis on regularly observed non-Gaussian level\nshifts that can have significant effect on estimation performance in\nstatistical models failing to account for such level shifts, such as at the\nclose and open of markets. We compare the estimation accuracy of our model and\nestimation approach to standard frequentist and Bayesian procedures for CVAR\nmodels when non-Gaussian price series level shifts are present in the\nindividual series, such as inter-day boundaries. We fit a bi-variate\nAlpha-stable model to the inter-day jumps and model the effect of such jumps on\nestimation of matrix-variate CVAR model parameters using the likelihood based\nJohansen procedure and a Bayesian estimation. We illustrate our model and the\ncorresponding estimation procedures we develop on both synthetic and actual\ndata. \n\n"}
{"id": "1008.3705", "contents": "Title: Techniques for Enhanced Physical-Layer Security Abstract: Information-theoretic security--widely accepted as the strictest notion of\nsecurity--relies on channel coding techniques that exploit the inherent\nrandomness of propagation channels to strengthen the security of communications\nsystems. Within this paradigm, we explore strategies to improve secure\nconnectivity in a wireless network. We first consider the intrinsically secure\ncommunications graph (iS-graph), a convenient representation of the links that\ncan be established with information-theoretic security on a large-scale\nnetwork. We then propose and characterize two techniques--sectorized\ntransmission and eavesdropper neutralization--which are shown to dramatically\nenhance the connectivity of the iS-graph. \n\n"}
{"id": "1009.2958", "contents": "Title: Equilibrium Sampling in Biomolecular Simulation Abstract: Equilibrium sampling of biomolecules remains an unmet challenge after more\nthan 30 years of atomistic simulation. Efforts to enhance sampling capability,\nwhich are reviewed here, range from the development of new algorithms to\nparallelization to novel uses of hardware. Special focus is placed on\nclassifying algorithms -- most of which are underpinned by a few key ideas --\nin order to understand their fundamental strengths and limitations. Although\nalgorithms have proliferated, progress resulting from novel hardware use\nappears to be more clear-cut than from algorithms alone, partly due to the lack\nof widely used sampling measures. \n\n"}
{"id": "1009.3601", "contents": "Title: Pair-Wise Cluster Analysis Abstract: This paper studies the problem of learning clusters which are consistently\npresent in different (continuously valued) representations of observed data.\nOur setup differs slightly from the standard approach of (co-) clustering as we\nuse the fact that some form of `labeling' becomes available in this setup: a\ncluster is only interesting if it has a counterpart in the alternative\nrepresentation. The contribution of this paper is twofold: (i) the problem\nsetting is explored and an analysis in terms of the PAC-Bayesian theorem is\npresented, (ii) a practical kernel-based algorithm is derived exploiting the\ninherent relation to Canonical Correlation Analysis (CCA), as well as its\nextension to multiple views. A content based information retrieval (CBIR) case\nstudy is presented on the multi-lingual aligned Europal document dataset which\nsupports the above findings. \n\n"}
{"id": "1009.5173", "contents": "Title: Gains in Power from Structured Two-Sample Tests of Means on Graphs Abstract: We consider multivariate two-sample tests of means, where the location shift\nbetween the two populations is expected to be related to a known graph\nstructure. An important application of such tests is the detection of\ndifferentially expressed genes between two patient populations, as shifts in\nexpression levels are expected to be coherent with the structure of graphs\nreflecting gene properties such as biological process, molecular function,\nregulation, or metabolism. For a fixed graph of interest, we demonstrate that\naccounting for graph structure can yield more powerful tests under the\nassumption of smooth distribution shift on the graph. We also investigate the\nidentification of non-homogeneous subgraphs of a given large graph, which poses\nboth computational and multiple testing problems. The relevance and benefits of\nthe proposed approach are illustrated on synthetic data and on breast cancer\ngene expression data analyzed in context of KEGG pathways. \n\n"}
{"id": "1010.0124", "contents": "Title: A model selection approach to genome wide association studies Abstract: For the vast majority of genome wide association studies (GWAS) published so\nfar, statistical analysis was performed by testing markers individually. In\nthis article we present some elementary statistical considerations which\nclearly show that in case of complex traits the approach based on multiple\nregression or generalized linear models is preferable to multiple testing. We\nintroduce a model selection approach to GWAS based on modifications of Bayesian\nInformation Criterion (BIC) and develop some simple search strategies to deal\nwith the huge number of potential models. Comprehensive simulations based on\nreal SNP data confirm that model selection has larger power than multiple\ntesting to detect causal SNPs in complex models. On the other hand multiple\ntesting has substantial problems with proper ranking of causal SNPs and tends\nto detect a certain number of false positive SNPs, which are not linked to any\nof the causal mutations. We show that this behavior is typical in GWAS for\ncomplex traits and can be explained by an aggregated influence of many small\nrandom sample correlations between genotypes of a SNP under investigation and\nother causal SNPs. We believe that our findings at least partially explain\nproblems with low power and nonreplicability of results in many real data GWAS.\nFinally, we discuss the advantages of our model selection approach in the\ncontext of real data analysis, where we consider publicly available gene\nexpression data as traits for individuals from the HapMap project. \n\n"}
{"id": "1010.3888", "contents": "Title: The Jones-Hore theory of radical-ion-pair reactions is not\n  self-consistent Abstract: It is shown that the master equation introduced by Jones & Hore and purported\nto describe radical-ion-pair reactions is not self-consistent. \n\n"}
{"id": "1011.1798", "contents": "Title: Sparse regulatory networks Abstract: In many organisms the expression levels of each gene are controlled by the\nactivation levels of known \"Transcription Factors\" (TF). A problem of\nconsiderable interest is that of estimating the \"Transcription Regulation\nNetworks\" (TRN) relating the TFs and genes. While the expression levels of\ngenes can be observed, the activation levels of the corresponding TFs are\nusually unknown, greatly increasing the difficulty of the problem. Based on\nprevious experimental work, it is often the case that partial information about\nthe TRN is available. For example, certain TFs may be known to regulate a given\ngene or in other cases a connection may be predicted with a certain\nprobability. In general, the biology of the problem indicates there will be\nvery few connections between TFs and genes. Several methods have been proposed\nfor estimating TRNs. However, they all suffer from problems such as unrealistic\nassumptions about prior knowledge of the network structure or computational\nlimitations. We propose a new approach that can directly utilize prior\ninformation about the network structure in conjunction with observed gene\nexpression data to estimate the TRN. Our approach uses $L_1$ penalties on the\nnetwork to ensure a sparse structure. This has the advantage of being\ncomputationally efficient as well as making many fewer assumptions about the\nnetwork structure. We use our methodology to construct the TRN for E. coli and\nshow that the estimate is biologically sensible and compares favorably with\nprevious estimates. \n\n"}
{"id": "1012.1007", "contents": "Title: Neighbor Discovery for Wireless Networks via Compressed Sensing Abstract: This paper studies the problem of neighbor discovery in wireless networks,\nnamely, each node wishes to discover and identify the network interface\naddresses (NIAs) of those nodes within a single hop. A novel paradigm, called\ncompressed neighbor discovery is proposed, which enables all nodes to\nsimultaneously discover their respective neighborhoods with a single frame of\ntransmission, which is typically of a few thousand symbol epochs. The key\ntechnique is to assign each node a unique on-off signature and let all nodes\nsimultaneously transmit their signatures. Despite that the radios are\nhalf-duplex, each node observes a superposition of its neighbors' signatures\n(partially) through its own off-slots. To identify its neighbors out of a large\nnetwork address space, each node solves a compressed sensing (or sparse\nrecovery) problem.\n  Two practical schemes are studied. The first employs random on-off\nsignatures, and each node discovers its neighbors using a noncoherent detection\nalgorithm based on group testing. The second scheme uses on-off signatures\nbased on a deterministic second-order Reed-Muller code, and applies a chirp\ndecoding algorithm. The second scheme needs much lower signal-to-noise ratio\n(SNR) to achieve the same error performance. The complexity of the chirp\ndecoding algorithm is sub-linear, so that it is in principle scalable to\nnetworks with billions of nodes with 48-bit IEEE 802.11 MAC addresses. The\ncompressed neighbor discovery schemes are much more efficient than conventional\nrandom-access discovery, where nodes have to retransmit over many frames with\nrandom delays to be successfully discovered. \n\n"}
{"id": "1012.3424", "contents": "Title: Fluctuations in the Ensemble of Reaction Pathways Abstract: The dominant reaction pathway (DRP) is a rigorous framework to\nmicroscopically compute the most probable trajectories, in non-equilibrium\ntransitions. In the low-temperature regime, such dominant pathways encode the\ninformation about the reaction mechanism and can be used to estimate\nnon-equilibrium averages of arbitrary observables. On the other hand, at\nsufficiently high temperatures, the stochastic fluctuations around the dominant\npaths become important and have to be taken into account. In this work, we\ndevelop a technique to systematically include the effects of such stochastic\nfluctuations, to order k_B T. This method is used to compute the probability\nfor a transition to take place through a specific reaction channel and to\nevaluate the reaction rate. \n\n"}
{"id": "1012.5726", "contents": "Title: Enhanced Sampling Algorithms Abstract: In biomolecular systems (especially all-atom models) with many degrees of\nfreedom such as proteins and nucleic acids, there exist an astronomically large\nnumber of local-minimum-energy states. Conventional simulations in the\ncanonical ensemble are of little use, because they tend to get trapped in\nstates of these energy local minima. Enhanced conformational sampling\ntechniques are thus in great demand. A simulation in generalized ensemble\nperforms a random walk in potential energy space and can overcome this\ndifficulty. From only one simulation run, one can obtain canonical-ensemble\naverages of physical quantities as functions of temperature by the\nsingle-histogram and/or multiple-histogram reweighting techniques. In this\narticle we review uses of the generalized-ensemble algorithms in biomolecular\nsystems. Three well-known methods, namely, multicanonical algorithm, simulated\ntempering, and replica-exchange method, are described first. Both Monte Carlo\nand molecular dynamics versions of the algorithms are given. We then present\nvarious extensions of these three generalized-ensemble algorithms. The\neffectiveness of the methods is tested with short peptide and protein systems. \n\n"}
{"id": "1101.1164", "contents": "Title: Modeling the dynamics of biomarkers during primary HIV infection taking\n  into account the uncertainty of infection date Abstract: During primary HIV infection, the kinetics of plasma virus concentrations and\nCD4+ cell counts is very complex. Parametric and nonparametric models have been\nsuggested for fitting repeated measurements of these markers. Alternatively,\nmechanistic approaches based on ordinary differential equations have also been\nproposed. These latter models are constructed according to biological knowledge\nand take into account the complex nonlinear interactions between viruses and\ncells. However, estimating the parameters of these models is difficult. A main\ndifficulty in the context of primary HIV infection is that the date of\ninfection is generally unknown. For some patients, the date of last negative\nHIV test is available in addition to the date of first positive HIV test\n(seroconverters). In this paper we propose a likelihood-based method for\nestimating the parameters of dynamical models using a population approach and\ntaking into account the uncertainty of the infection date. We applied this\nmethod to a sample of 761 HIV-infected patients from the Concerted Action on\nSeroConversion to AIDS and Death in Europe (CASCADE). \n\n"}
{"id": "1101.4577", "contents": "Title: Bayesian Variable Selection for Probit Mixed Models Applied to Gene\n  Selection Abstract: In computational biology, gene expression datasets are characterized by very\nfew individual samples compared to a large number of measurements per sample.\nThus, it is appealing to merge these datasets in order to increase the number\nof observations and diversify the data, allowing a more reliable selection of\ngenes relevant to the biological problem. Besides, the increased size of a\nmerged dataset facilitates its re-splitting into training and validation sets.\nThis necessitates the introduction of the dataset as a random effect. In this\ncontext, extending a work of Lee et al. (2003), a method is proposed to select\nrelevant variables among tens of thousands in a probit mixed regression model,\nconsidered as part of a larger hierarchical Bayesian model. Latent variables\nare used to identify subsets of selected variables and the grouping (or\nblocking) technique of Liu (1994) is combined with a Metropolis-within-Gibbs\nalgorithm (Robert and Casella 2004). The method is applied to a merged dataset\nmade of three individual gene expression datasets, in which tens of thousands\nof measurements are available for each of several hundred human breast cancer\nsamples. Even for this large dataset comprised of around 20000 predictors, the\nmethod is shown to be efficient and feasible. As an illustration, it is used to\nselect the most important genes that characterize the estrogen receptor status\nof patients with breast cancer. \n\n"}
{"id": "1101.5084", "contents": "Title: Joint Detection and Estimation: Optimum Tests and Applications Abstract: We consider a well defined joint detection and parameter estimation problem.\nBy combining the Baysian formulation of the estimation subproblem with suitable\nconstraints on the detection subproblem we develop optimum one- and two-step\ntest for the joint detection/estimation case. The proposed combined strategies\nhave the very desirable characteristic to allow for the trade-off between\ndetection power and estimation efficiency. Our theoretical developments are\nthen applied to the problems of retrospective changepoint detection and MIMO\nradar. In the former case we are interested in detecting a change in the\nstatistics of a set of available data and provide an estimate for the time of\nchange, while in the latter in detecting a target and estimating its location.\nIntense simulations demonstrate that by using the jointly optimum schemes, we\ncan experience significant improvement in estimation quality with small\nsacrifice in detection power. \n\n"}
{"id": "1101.5091", "contents": "Title: Why approximate Bayesian computational (ABC) methods cannot handle model\n  choice problems Abstract: Approximate Bayesian computation (ABC), also known as likelihood-free\nmethods, have become a favourite tool for the analysis of complex stochastic\nmodels, primarily in population genetics but also in financial analyses. We\nadvocated in Grelaud et al. (2009) the use of ABC for Bayesian model choice in\nthe specific case of Gibbs random fields (GRF), relying on a sufficiency\nproperty mainly enjoyed by GRFs to show that the approach was legitimate.\nDespite having previously suggested the use of ABC for model choice in a wider\nrange of models in the DIY ABC software (Cornuet et al., 2008), we present\ntheoretical evidence that the general use of ABC for model choice is fraught\nwith danger in the sense that no amount of computation, however large, can\nguarantee a proper approximation of the posterior probabilities of the models\nunder comparison. \n\n"}
{"id": "1101.5502", "contents": "Title: How does the first water shell fold proteins so fast ? Abstract: First shells of hydration and bulk solvent plays a crucial role in the\nfolding of proteins. Here, the role of water in the dynamics of proteins has\nbeen investigated using a theoretical protein-solvent model and a statistical\nphysics approach. We formulate a hydration model where the hydrogen bonds\nbetween water molecules pertaining to the first shell of the protein\nconformation may be either mainly formed or broken. At thermal equilibrium,\nhydrogen bonds are formed at low temperature and are broken at high\ntemperature. To explore the solvent effect, we follow the folding of a large\nsampling of protein chains, using a master-equation evolution. The dynamics\nshows a clear mechanism. Above the glass-transition temperature, a large ratio\nof chains fold very rapidly into the native structure irrespective of the\ntemperature, following pathways of high transition rates through structures\nsurrounded by the solvent with broken hydrogen bonds. Although these states\nhave an infinitesimal probability, they act as strong dynamical attractors and\nfast folding proceeds along these routes rather than pathways with small\ntransition rates between configurations of much higher equilibrium\nprobabilities. At a given low temperature, a broad jump in the folding times is\nobserved. Below this glass temperature, the pathways where hydrogen bonds are\nmainly formed become those of highest rates although with conformational\nchanges of huge relaxation times. The present results reveal that folding obeys\na double-funnel mechanism. \n\n"}
{"id": "1102.4210", "contents": "Title: A dynamic nonstationary spatio-temporal model for short term prediction\n  of precipitation Abstract: Precipitation is a complex physical process that varies in space and time.\nPredictions and interpolations at unobserved times and/or locations help to\nsolve important problems in many areas. In this paper, we present a\nhierarchical Bayesian model for spatio-temporal data and apply it to obtain\nshort term predictions of rainfall. The model incorporates physical knowledge\nabout the underlying processes that determine rainfall, such as advection,\ndiffusion and convection. It is based on a temporal autoregressive convolution\nwith spatially colored and temporally white innovations. By linking the\nadvection parameter of the convolution kernel to an external wind vector, the\nmodel is temporally nonstationary. Further, it allows for nonseparable and\nanisotropic covariance structures. With the help of the Voronoi tessellation,\nwe construct a natural parametrization, that is, space as well as time\nresolution consistent, for data lying on irregular grid points. In the\napplication, the statistical model combines forecasts of three other\nmeteorological variables obtained from a numerical weather prediction model\nwith past precipitation observations. The model is then used to predict\nthree-hourly precipitation over 24 hours. It performs better than a separable,\nstationary and isotropic version, and it performs comparably to a deterministic\nnumerical weather prediction model for precipitation and has the advantage that\nit quantifies prediction uncertainty. \n\n"}
{"id": "1102.5094", "contents": "Title: Reassessing The Fundamentals: New Constraints on the Evolution, Ages and\n  Masses of Neutron Stars Abstract: The ages and masses of neutron stars (NSs) are two fundamental threads that\nmake pulsars accessible to other sub-disciplines of astronomy and physics. A\nrealistic and accurate determination of these two derived parameters play an\nimportant role in understanding of advanced stages of stellar evolution and the\nphysics that govern relevant processes. Here I summarize new constraints on the\nages and masses of NSs with an evolutionary perspective. I show that the\nobserved P-Pdot demographics is more diverse than what is theoretically\npredicted for the standard evolutionary channel. In particular, standard\nrecycling followed by dipole spin-down fails to reproduce the population of\nmillisecond pulsars with higher magnetic fields (B > 4 x 10^{8} G) at rates\ndeduced from observations. A proper inclusion of constraints arising from\nbinary evolution and mass accretion offers a more realistic insight into the\nage distribution. By analytically implementing these constraints, I propose a\n\"modified\" spin-down age for millisecond pulsars that gives estimates closer to\nthe true age. Finally, I independently analyze the peak, skewness and cutoff\nvalues of the underlying mass distribution from a comprehensive list of radio\npulsars for which secure mass measurements are available. The inferred mass\ndistribution shows clear peaks at 1.35 Msun and 1.50 Msun for NSs in double\nneutron star (DNS) and neutron star-white dwarf (NS-WD) systems respectively. I\nfind a mass cutoff at 2 Msun for NSs with WD companions, which establishes a\nfirm lower bound for the maximum mass of NSs. \n\n"}
{"id": "1103.2944", "contents": "Title: The optimization topography of exciton transport Abstract: Stunningly large exciton transfer rates in the light harvesting complex of\nphotosynthesis, together with recent experimental 2D spectroscopic data, have\nspurred a vivid debate on the possible quantum origin of such efficiency. Here\nwe show that configurations of a random molecular network that optimize\nconstructive quantum interference from input to output site yield\nsystematically shorter transfer times than classical transport induced by\nambient dephasing noise. \n\n"}
{"id": "1103.6034", "contents": "Title: Semi-supervised Learning for Photometric Supernova Classification Abstract: We present a semi-supervised method for photometric supernova typing. Our\napproach is to first use the nonlinear dimension reduction technique diffusion\nmap to detect structure in a database of supernova light curves and\nsubsequently employ random forest classification on a spectroscopically\nconfirmed training set to learn a model that can predict the type of each newly\nobserved supernova. We demonstrate that this is an effective method for\nsupernova typing. As supernova numbers increase, our semi-supervised method\nefficiently utilizes this information to improve classification, a property not\nenjoyed by template based methods. Applied to supernova data simulated by\nKessler et al. (2010b) to mimic those of the Dark Energy Survey, our methods\nachieve (cross-validated) 95% Type Ia purity and 87% Type Ia efficiency on the\nspectroscopic sample, but only 50% Type Ia purity and 50% efficiency on the\nphotometric sample due to their spectroscopic follow-up strategy. To improve\nthe performance on the photometric sample, we search for better spectroscopic\nfollow-up procedures by studying the sensitivity of our machine learned\nsupernova classification on the specific strategy used to obtain training sets.\nWith a fixed amount of spectroscopic follow-up time, we find that deeper\nmagnitude-limited spectroscopic surveys are better for producing training sets.\nFor supernova Ia (II-P) typing, we obtain a 44% (1%) increase in purity to 72%\n(87%) and 30% (162%) increase in efficiency to 65% (84%) of the sample using a\n25th (24.5th) magnitude-limited survey instead of the shallower spectroscopic\nsample used in the original simulations. When redshift information is\navailable, we incorporate it into our analysis using a novel method of altering\nthe diffusion map representation of the supernovae. Incorporating host\nredshifts leads to a 5% improvement in Type Ia purity and 13% improvement in\nType Ia efficiency. \n\n"}
{"id": "1104.0475", "contents": "Title: On a recent development in stochastic inversion with applications to\n  hydrogeology Abstract: We comment on a recent approach to spatial stochastic inversion, which\ncenters on a concept known as \"anchors\" and conducts nonparametric estimation\nof the likelihood of the anchors (along with other model parameters) with\nrespect to data obtained from field processes. The approach is called \"anchored\ninversion\" or, less desirably, \"method of anchored distribution\". Conceptual\nand technical observations are made regarding the development, interpretation,\nand use of this approach. We also point out that this approach has been\nsuperseded by new developments. \n\n"}
{"id": "1104.2790", "contents": "Title: On Intrinsic Geometric Stability of Controller Abstract: This work explores the role of the intrinsic fluctuations in finite parameter\ncontroller configurations characterizing an ensemble of arbitrary irregular\nfilter circuits. Our analysis illustrates that the parametric intrinsic\ngeometric description exhibits a set of exact pair correction functions and\nglobal correlation volume with and without the variation of the mismatch\nfactor. The present consideration shows that the canonical fluctuations can\nprecisely be depicted without any approximation. The intrinsic geometric notion\noffers a clear picture of the fluctuating controllers, which as the limit of\nthe ensemble averaging reduce to the specified controller. For the constant\nmismatch factor controllers, the Gaussian fluctuations over equilibrium basis\naccomplish a well-defined, non-degenerate, flat regular intrinsic Riemannian\nsurface. An explicit computation further demonstrates that the underlying power\ncorrelations involve ordinary summations, even if we consider the variable\nmismatch factor controllers. Our intrinsic geometric framework describes a\ndefinite character to the canonical power fluctuations of the controllers and\nconstitutes a stable design strategy for the parameters. \n\n"}
{"id": "1104.2943", "contents": "Title: Atomistic study of the long-lived quantum coherences in the\n  Fenna-Matthews-Olson complex Abstract: A remarkable amount of theoretical research has been carried out to elucidate\nthe physical origins of the recently observed long-lived quantum coherence in\nthe electronic energy transfer process in biological photosynthetic systems.\nAlthough successful in many respects, several widely used descriptions only\ninclude an effective treatment of the protein-chromophore interactions. In this\nwork, by combining an all-atom molecular dynamics simulation, time-dependent\ndensity functional theory, and open quantum system approaches, we successfully\nsimulate the dynamics of the electronic energy transfer of the\nFenna-Matthews-Olson pigment-protein complex. The resulting characteristic\nbeating of populations and quantum coherences is in good agreement with the\nexperimental results and the hierarchy equation of motion approach. The\nexperimental absorption, linear and circular dichroism spectra and dephasing\nrates are recovered at two different temperatures. In addition, we provide an\nextension of our method to include zero-point fluctuations of the vibrational\nenvironment. This work thus presents one of the first steps to explain the role\nof excitonic quantum coherence in photosynthetic light-harvesting complexes\nbased on their atomistic and molecular description. \n\n"}
{"id": "1104.5674", "contents": "Title: Using causal models to distinguish between neurogenesis-dependent and\n  -independent effects on behaviour Abstract: There has been a substantial amount of research on the relationship between\nhippocampal neurogenesis and behaviour over the past fifteen years, but the\ncausal role that new neurons have on cognitive and affective behavioural tasks\nis still far from clear. This is partly due to the difficulty of manipulating\nlevels of neurogenesis without inducing off-target effects, which might also\ninfluence behaviour. In addition, the analytical methods typically used do not\ndirectly test whether neurogenesis mediates the effect of an intervention on\nbehaviour. Previous studies may have incorrectly attributed changes in\nbehavioural performance to neurogenesis because the role of known (or unknown)\nneurogenesis-independent mechanisms were not formally taken into consideration\nduring the analysis. Causal models can tease apart complex causal relationships\nand were used to demonstrate that the effect of exercise on pattern separation\nis via neurogenesis-independent mechanisms. Many studies in the neurogenesis\nliterature would benefit from the use of statistical methods that can separate\nneurogenesis-dependent from neurogenesis-independent effects on behaviour. \n\n"}
{"id": "1105.0828", "contents": "Title: MissForest - nonparametric missing value imputation for mixed-type data Abstract: Modern data acquisition based on high-throughput technology is often facing\nthe problem of missing data. Algorithms commonly used in the analysis of such\nlarge-scale data often depend on a complete set. Missing value imputation\noffers a solution to this problem. However, the majority of available\nimputation methods are restricted to one type of variable only: continuous or\ncategorical. For mixed-type data the different types are usually handled\nseparately. Therefore, these methods ignore possible relations between variable\ntypes. We propose a nonparametric method which can cope with different types of\nvariables simultaneously. We compare several state of the art methods for the\nimputation of missing values. We propose and evaluate an iterative imputation\nmethod (missForest) based on a random forest. By averaging over many unpruned\nclassification or regression trees random forest intrinsically constitutes a\nmultiple imputation scheme. Using the built-in out-of-bag error estimates of\nrandom forest we are able to estimate the imputation error without the need of\na test set. Evaluation is performed on multiple data sets coming from a diverse\nselection of biological fields with artificially introduced missing values\nranging from 10% to 30%. We show that missForest can successfully handle\nmissing values, particularly in data sets including different types of\nvariables. In our comparative study missForest outperforms other methods of\nimputation especially in data settings where complex interactions and nonlinear\nrelations are suspected. The out-of-bag imputation error estimates of\nmissForest prove to be adequate in all settings. Additionally, missForest\nexhibits attractive computational efficiency and can cope with high-dimensional\ndata. \n\n"}
{"id": "1105.3416", "contents": "Title: Implementation of Physical-layer Network Coding Abstract: This paper presents the first implementation of a two-way relay network based\non the principle of physical-layer network coding. To date, only a simplified\nversion of physical-layer network coding (PNC) method, called analog network\ncoding (ANC), has been successfully implemented. The advantage of ANC is that\nit is simple to implement; the disadvantage, on the other hand, is that the\nrelay amplifies the noise along with the signal before forwarding the signal.\nPNC systems in which the relay performs XOR or other denoising PNC mappings of\nthe received signal have the potential for significantly better performance.\nHowever, the implementation of such PNC systems poses many challenges. For\nexample, the relay must be able to deal with symbol and carrier-phase\nasynchronies of the simultaneous signals received from the two end nodes, and\nthe relay must perform channel estimation before detecting the signals. We\ninvestigate a PNC implementation in the frequency domain, referred to as FPNC,\nto tackle these challenges. FPNC is based on OFDM. In FPNC, XOR mapping is\nperformed on the OFDM samples in each subcarrier rather than on the samples in\nthe time domain. We implement FPNC on the universal soft radio peripheral\n(USRP) platform. Our implementation requires only moderate modifications of the\npacket preamble design of 802.11a/g OFDM PHY. With the help of the cyclic\nprefix (CP) in OFDM, symbol asynchrony and the multi-path fading effects can be\ndealt with in a similar fashion. Our experimental results show that\nsymbol-synchronous and symbol-asynchronous FPNC have essentially the same BER\nperformance, for both channel-coded and unchannel-coded FPNC. \n\n"}
{"id": "1105.5651", "contents": "Title: Towards a Queueing-Based Framework for In-Network Function Computation Abstract: We seek to develop network algorithms for function computation in sensor\nnetworks. Specifically, we want dynamic joint aggregation, routing, and\nscheduling algorithms that have analytically provable performance benefits due\nto in-network computation as compared to simple data forwarding. To this end,\nwe define a class of functions, the Fully-Multiplexible functions, which\nincludes several functions such as parity, MAX, and k th -order statistics. For\nsuch functions we exactly characterize the maximum achievable refresh rate of\nthe network in terms of an underlying graph primitive, the min-mincut. In\nacyclic wireline networks, we show that the maximum refresh rate is achievable\nby a simple algorithm that is dynamic, distributed, and only dependent on local\ninformation. In the case of wireless networks, we provide a MaxWeight-like\nalgorithm with dynamic flow splitting, which is shown to be throughput-optimal. \n\n"}
{"id": "1106.1275", "contents": "Title: Liquid-solid interaction at nanoscale and its application in vegetal\n  biology Abstract: The water ascent in tall trees is subject to controversy: the vegetal\nbiologists debate on the validity of the cohesion-tension theory which\nconsiders strong negative pressures in microtubes of xylem carrying the crude\nsap. This article aims to point out that liquids are submitted at the walls to\nintermolecular forces inferring density gradients making heterogeneous liquid\nlayers and therefore disqualifying the Navier-Stokes equations for nanofilms.\nThe crude sap motion takes the disjoining pressure gradient into account and\nthe sap flow dramatically increases such that the watering of nanolayers may be\nanalogous to a microscopic flow. Application to microtubes of xylem avoids the\nproblem of cavitation and enables us to understand why the ascent of sap is\npossible for very high trees. \n\n"}
{"id": "1106.2229", "contents": "Title: Fast, Linear Time Hierarchical Clustering using the Baire Metric Abstract: The Baire metric induces an ultrametric on a dataset and is of linear\ncomputational complexity, contrasted with the standard quadratic time\nagglomerative hierarchical clustering algorithm. In this work we evaluate\nempirically this new approach to hierarchical clustering. We compare\nhierarchical clustering based on the Baire metric with (i) agglomerative\nhierarchical clustering, in terms of algorithm properties; (ii) generalized\nultrametrics, in terms of definition; and (iii) fast clustering through k-means\npartititioning, in terms of quality of results. For the latter, we carry out an\nin depth astronomical study. We apply the Baire distance to spectrometric and\nphotometric redshifts from the Sloan Digital Sky Survey using, in this work,\nabout half a million astronomical objects. We want to know how well the (more\ncostly to determine) spectrometric redshifts can predict the (more easily\nobtained) photometric redshifts, i.e. we seek to regress the spectrometric on\nthe photometric redshifts, and we use clusterwise regression for this. \n\n"}
{"id": "1106.2832", "contents": "Title: Active Learning to Overcome Sample Selection Bias: Application to\n  Photometric Variable Star Classification Abstract: Despite the great promise of machine-learning algorithms to classify and\npredict astrophysical parameters for the vast numbers of astrophysical sources\nand transients observed in large-scale surveys, the peculiarities of the\ntraining data often manifest as strongly biased predictions on the data of\ninterest. Typically, training sets are derived from historical surveys of\nbrighter, more nearby objects than those from more extensive, deeper surveys\n(testing data). This sample selection bias can cause catastrophic errors in\npredictions on the testing data because a) standard assumptions for\nmachine-learned model selection procedures break down and b) dense regions of\ntesting space might be completely devoid of training data. We explore possible\nremedies to sample selection bias, including importance weighting (IW),\nco-training (CT), and active learning (AL). We argue that AL---where the data\nwhose inclusion in the training set would most improve predictions on the\ntesting set are queried for manual follow-up---is an effective approach and is\nappropriate for many astronomical applications. For a variable star\nclassification problem on a well-studied set of stars from Hipparcos and OGLE,\nAL is the optimal method in terms of error rate on the testing data, beating\nthe off-the-shelf classifier by 3.4% and the other proposed methods by at least\n3.0%. To aid with manual labeling of variable stars, we developed a web\ninterface which allows for easy light curve visualization and querying of\nexternal databases. Finally, we apply active learning to classify variable\nstars in the ASAS survey, finding dramatic improvement in our agreement with\nthe ACVS catalog, from 65.5% to 79.5%, and a significant increase in the\nclassifier's average confidence for the testing set, from 14.6% to 42.9%, after\na few AL iterations. \n\n"}
{"id": "1106.2911", "contents": "Title: Spatial propagation of excitonic coherence enables ratcheted energy\n  transfer Abstract: Experimental evidence shows that a variety of photosynthetic systems can\npreserve quantum beats in the process of electronic energy transfer, even at\nroom temperature. However, whether this quantum coherence arises in vivo and\nwhether it has any biological function have remained unclear. Here we present a\ntheoretical model that suggests that the creation and recreation of coherence\nunder natural conditions is ubiquitous. Our model allows us to theoretically\ndemonstrate a mechanism for a ratchet effect enabled by quantum coherence, in a\ndesign inspired by an energy transfer pathway in the Fenna-Matthews-Olson\ncomplex of the green sulfur bacteria. This suggests a possible biological role\nfor coherent oscillations in spatially directing energy transfer. Our results\nemphasize the importance of analyzing long-range energy transfer in terms of\ntransfer between inter-complex coupling (ICC) states rather than between site\nor exciton states. \n\n"}
{"id": "1106.3016", "contents": "Title: Goodness-of-Fit tests with Dependent Observations Abstract: We revisit the Kolmogorov-Smirnov and Cram\\'er-von Mises goodness-of-fit\n(GoF) tests and propose a generalisation to identically distributed, but\ndependent univariate random variables. We show that the dependence leads to a\nreduction of the \"effective\" number of independent observations. The\ngeneralised GoF tests are not distribution-free but rather depend on all the\nlagged bivariate copulas. These objects, that we call \"self-copulas\", encode\nall the non-linear temporal dependences. We introduce a specific, log-normal\nmodel for these self-copulas, for which a number of analytical results are\nderived. An application to financial time series is provided. As is well known,\nthe dependence is to be long-ranged in this case, a finding that we confirm\nusing self-copulas. As a consequence, the acceptance rates for GoF tests are\nsubstantially higher than if the returns were iid random variables. \n\n"}
{"id": "1106.5919", "contents": "Title: Monte Carlo algorithms for model assessment via conflicting summaries Abstract: The development of statistical methods and numerical algorithms for model\nchoice is vital to many real-world applications. In practice, the ABC approach\ncan be instrumental for sequential model design; however, the theoretical basis\nof its use has been questioned. We present a measure-theoretic framework for\nusing the ABC error towards model choice and describe how easily existing\nrejection, Metropolis-Hastings and sequential importance sampling ABC\nalgorithms are extended for the purpose of model checking. Considering a panel\nof applications from evolutionary biology to dynamic systems, we discuss the\nchoice of summaries which differs from standard ABC approaches. The methods and\nalgorithms presented here may provide the workhorse machinery for an\nexploratory approach to ABC model choice, particularly as the application of\nstandard Bayesian tools can prove impossible. \n\n"}
{"id": "1107.1900", "contents": "Title: Behavior patterns of online users and the effect on information\n  filtering Abstract: Understanding the structure and evolution of web-based user-object bipartite\nnetworks is an important task since they play a fundamental role in online\ninformation filtering. In this paper, we focus on investigating the patterns of\nonline users' behavior and the effect on recommendation process. Empirical\nanalysis on the e-commercial systems show that users have significant taste\ndiversity and their interests for niche items highly overlap. Additionally,\nrecommendation process are investigated on both the real networks and the\nreshuffled networks in which real users' behavior patterns can be gradually\ndestroyed. Our results shows that the performance of personalized\nrecommendation methods is strongly related to the real network structure.\nDetail study on each item shows that recommendation accuracy for hot items is\nalmost maximum and quite robust to the reshuffling process. However, niche\nitems cannot be accurately recommended after removing users' behavior patterns.\nOur work also is meaningful in practical sense since it reveals an effective\ndirection to improve the accuracy and the robustness of the existing\nrecommender systems. \n\n"}
{"id": "1107.6043", "contents": "Title: Measurement and Application of Entropy Production Rate in Human Subject\n  Social Interaction Systems Abstract: This paper illustrates the measurement and the applications of the\nobservable, entropy production rate (EPR), in human subject social interaction\nsystems. To this end, we show (1) how to test the minimax randomization model\nwith experimental economics' 2$\\times$2 games data and with the Wimbledon\nTennis data; (2) how to identify the Edgeworth price cycle in experimental\nmarket data; and (3) the relationship within EPR and motion in data. As a\nresult, in human subject social interaction systems, EPR can be measured\npractically and can be employed to test models and to search for facts\nefficiently. \n\n"}
{"id": "1108.2999", "contents": "Title: Dual $\\phi$-divergences estimation in normal models Abstract: A class of robust estimators which are obtained from dual representation of\n$\\phi$-divergences, are studied empirically for the normal location model.\nMembers of this class of estimators are compared, and it is found that they are\nefficient at the true model and offer an attractive alternative to the maximum\nlikelihood, in term of robustness . \n\n"}
{"id": "1108.3180", "contents": "Title: An adaptively weighted statistic for detecting differential gene\n  expression when combining multiple transcriptomic studies Abstract: Global expression analyses using microarray technologies are becoming more\ncommon in genomic research, therefore, new statistical challenges associated\nwith combining information from multiple studies must be addressed. In this\npaper we will describe our proposal for an adaptively weighted (AW) statistic\nto combine multiple genomic studies for detecting differentially expressed\ngenes. We will also present our results from comparisons of our proposed AW\nstatistic to Fisher's equally weighted (EW), Tippett's minimum p-value (minP)\nand Pearson's (PR) statistics. Due to the absence of a uniformly powerful test,\nwe used a simplified Gaussian scenario to compare the four methods. Our AW\nstatistic consistently produced the best or near-best power for a range of\nalternative hypotheses. AW-obtained weights also have the additional advantage\nof filtering discordant biomarkers and providing natural detected gene\ncategories for further biological investigation. Here we will demonstrate the\nsuperior performance of our proposed AW statistic based on a mix of power\nanalyses, simulations and applications using data sets for multi-tissue energy\nmetabolism mouse, multi-lab prostate cancer and lung cancer. \n\n"}
{"id": "1108.4226", "contents": "Title: Research on Wireless Multi-hop Networks: Current State and Challenges Abstract: Wireless multi-hop networks, in various forms and under various names, are\nbeing increasingly used in military and civilian applications. Studying\nconnectivity and capacity of these networks is an important problem. The\nscaling behavior of connectivity and capacity when the network becomes\nsufficiently large is of particular interest. In this position paper, we\nbriefly overview recent development and discuss research challenges and\nopportunities in the area, with a focus on the network connectivity. \n\n"}
{"id": "1108.5052", "contents": "Title: On the Quality of Wireless Network Connectivity Abstract: Despite intensive research in the area of network connectivity, there is an\nimportant category of problems that remain unsolved: how to measure the quality\nof connectivity of a wireless multi-hop network which has a realistic number of\nnodes, not necessarily large enough to warrant the use of asymptotic analysis,\nand has unreliable connections, reflecting the inherent unreliable\ncharacteristics of wireless communications? The quality of connectivity\nmeasures how easily and reliably a packet sent by a node can reach another\nnode. It complements the use of \\emph{capacity} to measure the quality of a\nnetwork in saturated traffic scenarios and provides a native measure of the\nquality of (end-to-end) network connections. In this paper, we explore the use\nof probabilistic connectivity matrix as a possible tool to measure the quality\nof network connectivity. Some interesting properties of the probabilistic\nconnectivity matrix and their connections to the quality of connectivity are\ndemonstrated. We argue that the largest eigenvalue of the probabilistic\nconnectivity matrix can serve as a good measure of the quality of network\nconnectivity. \n\n"}
{"id": "1109.0152", "contents": "Title: Stable Graphical Model Estimation with Random Forests for Discrete,\n  Continuous, and Mixed Variables Abstract: A conditional independence graph is a concise representation of pairwise\nconditional independence among many variables. Graphical Random Forests (GRaFo)\nare a novel method for estimating pairwise conditional independence\nrelationships among mixed-type, i.e. continuous and discrete, variables. The\nnumber of edges is a tuning parameter in any graphical model estimator and\nthere is no obvious number that constitutes a good choice. Stability Selection\nhelps choosing this parameter with respect to a bound on the expected number of\nfalse positives (error control).\n  The performance of GRaFo is evaluated and compared with various other methods\nfor p = 50, 100, and 200 possibly mixed-type variables while sample size is n =\n100 (n = 500 for maximum likelihood). Furthermore, GRaFo is applied to data\nfrom the Swiss Health Survey in order to evaluate how well it can reproduce the\ninterconnection of functional health components, personal, and environmental\nfactors, as hypothesized by the World Health Organization's International\nClassification of Functioning, Disability and Health (ICF). Finally, GRaFo is\nused to identify risk factors which may be associated with adverse\nneurodevelopment of children who suffer from trisomy 21 and experienced\nopen-heart surgery.\n  GRaFo performs well with mixed data and thanks to Stability Selection it\nprovides an error control mechanism for false positive selection. \n\n"}
{"id": "1109.2044", "contents": "Title: Information-sharing and aggregation models for interacting minds Abstract: We study mathematical models of the collaborative solving of a two-choice\ndiscrimination task. We estimate the difference between the shared performance\nfor a group of n observers over a single person performance. Our paper is a\ntheoretical extension of the recent work of Bahrami et al. (2010) from a dyad\n(a pair) to a group of n interacting minds. We analyze several models of\ncommunication, decision-making and hierarchical information-aggregation.\n  The maximal slope of psychometric function (closely related to the percentage\nof right answers vs. easiness of the task) is a convenient parameter\ncharacterizing performance. For every model we investigated, the group\nperformance turns out to be a product of two numbers: a scaling factor\ndepending of the group size and an average performance. The scaling factor is a\npower function of the group size (with the exponent ranging from 0 to 1),\nwhereas the average is arithmetic mean, quadratic mean, or maximum of the\nindividual slopes. Moreover, voting can be almost as efficient as more\nelaborate communication models, given the participants have similar individual\nperformances. \n\n"}
{"id": "1109.2522", "contents": "Title: Ion-sensitive phase transitions driven by Debye-H\\\"uckel non-ideality Abstract: We find that the Debye-H\\\"uckel nonideality of dilute aqueous electrolytes is\nsufficient to drive volume phase transitions and criticality, even in the\nabsence of a self-attracting or elastic network. Our result follows from a\nLandau mean-field theory for a system of confined ions in an external solution\nof mixed-valence counterions, where the ratio of squared monovalent to divalent\nion concentration provides a temperature-like variable for the phase\ntransition. Our analysis was motivated by long-studied volume phase transitions\nvia ion exchange in ionic gels, but our findings agree with existing theory for\nvolume-temperature phase transitions in charged hard-sphere models and other\nsystems by Fisher and Levin, and McGahay and Tomozawa. Our mean-field model\npredicts a continuous line of gas-liquid-type critical points connecting a\npurely monovalent, divalent-sensitive critical point at one extreme with a\ndivalent, monovalent-sensitive critical point at the other; an alternative\nrepresentation of the Landau functional handles this second limit. It follows\nthat critical sensitivity to ion valence is tunable to any desired valence\nratio. The critical or discontinuous dependent variable can be the confinement\nvolume; alternatively the internal electrical potential may be more convenient\nin applications. Our simplified conditions for ionic phase transitions to\noccur, together with our relatively simple theory to describe them, may\nfacilitate exploration of tunable critical sensitivity in areas such as ion\ndetection technology, biological switches and osmotic control. \n\n"}
{"id": "1109.3416", "contents": "Title: Efficient energy transfer in light-harvesting systems, III: The\n  influence of the eighth bacteriochlorophyll on the dynamics and efficiency in\n  FMO Abstract: The most recent crystal structure of the Fenna-Matthews-Olson (FMO) protein\ncomplex indicates that each subunit contains an additional eighth chromophore.\nIt has been proposed that this extra site functions as a link between the\nchlorosome antenna complex and the remaining seven chromophores in FMO [Schmidt\nam Busch et al, J. Phys. Chem. Lett., {\\bf 2}, 93 (2011)]. Here, we investigate\nthe implications of this scenario through numerical calculations with the\ngeneralized Bloch-Redfield (GBR) equation and the non-interacting blip\napproximation (NIBA). Three key insights into the population dynamics and\nenergy transfer efficiency in FMO are provided. First, it is shown that the\noscillations that are often observed in the population relaxation of the dimer\ncomposed of sites one and two may be completely suppressed in the eight site\nmodel. The presence of the coherent oscillations is shown to depend upon the\nparticular initial preparation of the dimer state. Secondly it is demonstrated\nthat while the presence of the eighth chromophore does not cause a dramatic\nchange in the energy transfer efficiency, it does however lead to a dominant\nenergy transfer pathway which can be characterized by an effective three site\nsystem arranged in an equally spaced downhill configuration. Such a\nconfiguration leads to an optimal value of the site energy of the eighth\nchromophore which is shown to be near to its suggested value. Finally we\nconfirm that the energy transfer process in the eight site FMO complex remains\nefficient and robust. The optimal values of the bath parameters are computed\nand shown to be closer to the experimentally fitted values than those\ncalculated previously for the seven site system. \n\n"}
{"id": "1109.4179", "contents": "Title: FemtoCaching: Wireless Video Content Delivery through Distributed\n  Caching Helpers Abstract: Video on-demand streaming from Internet-based servers is becoming one of the\nmost important services offered by wireless networks today. In order to improve\nthe area spectral efficiency of video transmission in cellular systems, small\ncells heterogeneous architectures (e.g., femtocells, WiFi off-loading) are\nbeing proposed, such that video traffic to nomadic users can be handled by\nshort-range links to the nearest small cell access points (referred to as\n\"helpers\"). As the helper deployment density increases, the backhaul capacity\nbecomes the system bottleneck. In order to alleviate such bottleneck we propose\na system where helpers with low-rate backhaul but high storage capacity cache\npopular video files. Files not available from helpers are transmitted by the\ncellular base station. We analyze the optimum way of assigning files to the\nhelpers, in order to minimize the expected downloading time for files. We\ndistinguish between the uncoded case (where only complete files are stored) and\nthe coded case, where segments of Fountain-encoded versions of the video files\nare stored at helpers. We show that the uncoded optimum file assignment is\nNP-hard, and develop a greedy strategy that is provably within a factor 2 of\nthe optimum. Further, for a special case we provide an efficient algorithm\nachieving a provably better approximation ratio of $1-(1-1/d)^d$, where $d$ is\nthe maximum number of helpers a user can be connected to. We also show that the\ncoded optimum cache assignment problem is convex that can be further reduced to\na linear program. We present numerical results comparing the proposed schemes. \n\n"}
{"id": "1109.5485", "contents": "Title: A new estimator for the tail-dependence coefficient Abstract: Recently, the concept of tail dependence has been discussed in financial\napplications related to market or credit risk. The multivariate extreme value\ntheory is a proper tool to measure and model dependence, for example, of large\nloss events. A common measure of tail dependence is given by the so-called\ntail-dependence coefficient. We present a simple estimator of this latter that\navoids the drawbacks of the estimation procedure that has been used so far. We\nprove strong consistency and asymptotic normality and analyze the finite sample\nbehavior through simulation. We illustrate with an application to financial\ndata. \n\n"}
{"id": "1109.6191", "contents": "Title: Likelihood Consensus-Based Distributed Particle Filtering with\n  Distributed Proposal Density Adaptation Abstract: We present a consensus-based distributed particle filter (PF) for wireless\nsensor networks. Each sensor runs a local PF to compute a global state estimate\nthat takes into account the measurements of all sensors. The local PFs use the\njoint (all-sensors) likelihood function, which is calculated in a distributed\nway by a novel generalization of the likelihood consensus scheme. A performance\nimprovement (or a reduction of the required number of particles) is achieved by\na novel distributed, consensus-based method for adapting the proposal densities\nof the local PFs. The performance of the proposed distributed PF is\ndemonstrated for a target tracking problem. \n\n"}
{"id": "1110.1338", "contents": "Title: Robustness and Conditional Independence Ideals Abstract: We study notions of robustness of Markov kernels and probability distribution\nof a system that is described by $n$ input random variables and one output\nrandom variable. Markov kernels can be expanded in a series of potentials that\nallow to describe the system's behaviour after knockouts. Robustness imposes\nstructural constraints on these potentials. Robustness of probability\ndistributions is defined via conditional independence statements. These\nstatements can be studied algebraically. The corresponding conditional\nindependence ideals are related to binary edge ideals. The set of robust\nprobability distributions lies on an algebraic variety. We compute a Gr\\\"obner\nbasis of this ideal and study the irreducible decomposition of the variety.\nThese algebraic results allow to parametrize the set of all robust probability\ndistributions. \n\n"}
{"id": "1110.5002", "contents": "Title: Testing the approximations described in \"Asymptotic formulae for\n  likelihood-based tests of new physics\" Abstract: \"Asymptotic formulae for likelihood-based tests of new physics\" presents a\nmathematical formalism for a new approximation for hypothesis testing in high\nenergy physics. The approximations are designed to greatly reduce the\ncomputational burden for such problems. We seek to test the conditions under\nwhich the approximations described remain valid. To do so, we perform parallel\ncalculations for a range of scenarios and compare the full calculation to the\napproximations to determine the limits and robustness of the approximation. We\ncompare this approximation against values calculated with the Collie framework,\nwhich for our analysis we assume produces true values. \n\n"}
{"id": "1111.0911", "contents": "Title: Exploiting Non-Linear Structure in Astronomical Data for Improved\n  Statistical Inference Abstract: Many estimation problems in astrophysics are highly complex, with\nhigh-dimensional, non-standard data objects (e.g., images, spectra, entire\ndistributions, etc.) that are not amenable to formal statistical analysis. To\nutilize such data and make accurate inferences, it is crucial to transform the\ndata into a simpler, reduced form. Spectral kernel methods are non-linear data\ntransformation methods that efficiently reveal the underlying geometry of\nobservable data. Here we focus on one particular technique: diffusion maps or\nmore generally spectral connectivity analysis (SCA). We give examples of\napplications in astronomy; e.g., photometric redshift estimation, prototype\nselection for estimation of star formation history, and supernova light curve\nclassification. We outline some computational and statistical challenges that\nremain, and we discuss some promising future directions for astronomy and data\nmining. \n\n"}
{"id": "1111.1678", "contents": "Title: Topology and energy transport in networks of interacting photosynthetic\n  complexes Abstract: We address the role of topology in the energy transport process that occurs\nin networks of photosynthetic complexes. We take inspiration from light\nharvesting networks present in purple bacteria and simulate an incoherent\ndissipative energy transport process on more general and abstract networks,\nconsidering both regular structures (Cayley trees and hyperbranched fractals)\nand randomly-generated ones. We focus on the the two primary light harvesting\ncomplexes of purple bacteria, i.e., the LH1 and LH2, and we use\nnetwork-theoretical centrality measures in order to select different LH1\narrangements. We show that different choices cause significant differences in\nthe transport efficiencies, and that for regular networks centrality measures\nallow to identify arrangements that ensure transport efficiencies which are\nbetter than those obtained with a random disposition of the complexes. The\noptimal arrangements strongly depend on the dissipative nature of the dynamics\nand on the topological properties of the networks considered, and depending on\nthe latter they are achieved by using global vs. local centrality measures. For\nrandomly-generated networks a random arrangement of the complexes already\nprovides efficient transport, and this suggests the process is strong with\nrespect to limited amount of control in the structure design and to the\ndisorder inherent in the construction of randomly-assembled structures.\nFinally, we compare the networks considered with the real biological networks\nand find that the latter have in general better performances, due to their\nhigher connectivity, but the former with optimal arrangements can mimic the\nreal networks' behaviour for a specific range of transport parameters. These\nresults show that the use of network-theoretical concepts can be crucial for\nthe characterization and design of efficient artificial energy transport\nnetworks. \n\n"}
{"id": "1111.5993", "contents": "Title: Estimating within-household contact networks from egocentric data Abstract: Acute respiratory diseases are transmitted over networks of social contacts.\nLarge-scale simulation models are used to predict epidemic dynamics and\nevaluate the impact of various interventions, but the contact behavior in these\nmodels is based on simplistic and strong assumptions which are not informed by\nsurvey data. These assumptions are also used for estimating transmission\nmeasures such as the basic reproductive number and secondary attack rates.\nDevelopment of methodology to infer contact networks from survey data could\nimprove these models and estimation methods. We contribute to this area by\ndeveloping a model of within-household social contacts and using it to analyze\nthe Belgian POLYMOD data set, which contains detailed diaries of social\ncontacts in a 24-hour period. We model dependency in contact behavior through a\nlatent variable indicating which household members are at home. We estimate\nage-specific probabilities of being at home and age-specific probabilities of\ncontact conditional on two members being at home. Our results differ from the\nstandard random mixing assumption. In addition, we find that the probability\nthat all members contact each other on a given day is fairly low: 0.49 for\nhouseholds with two 0--5 year olds and two 19--35 year olds, and 0.36 for\nhouseholds with two 12--18 year olds and two 36+ year olds. We find higher\ncontact rates in households with 2--3 members, helping explain the higher\ninfluenza secondary attack rates found in households of this size. \n\n"}
{"id": "1111.6828", "contents": "Title: Bayesian Estimation of a Gaussian source in Middleton's Class-A\n  Impulsive Noise Abstract: The paper focuses on minimum mean square error (MMSE) Bayesian estimation for\na Gaussian source impaired by additive Middleton's Class-A impulsive noise. In\naddition to the optimal Bayesian estimator, the paper considers also the\nsoft-limiter and the blanker, which are two popular suboptimal estimators\ncharacterized by very low complexity. The MMSE-optimum thresholds for such\nsuboptimal estimators are obtained by practical iterative algorithms with fast\nconvergence. The paper derives also the optimal thresholds according to a\nmaximum-SNR (MSNR) criterion, and establishes connections with the MMSE\ncriterion. Furthermore, closed form analytic expressions are derived for the\nMSE and the SNR of all the suboptimal estimators, which perfectly match\nsimulation results. Noteworthy, these results can be applied to characterize\nthe receiving performance of any multicarrier system impaired by a\nGaussian-mixture noise, such as asymmetric digital subscriber lines (ADSL) and\npower-line communications (PLC). \n\n"}
{"id": "1112.1023", "contents": "Title: Weighted KS Statistics for Inference on Conditional Moment Inequalities Abstract: This paper proposes confidence regions for the identified set in conditional\nmoment inequality models using Kolmogorov-Smirnov statistics with a truncated\ninverse variance weighting with increasing truncation points. The new weighting\ndiffers from those proposed in the literature in two important ways. First,\nconfidence regions based on KS tests with the weighting function I propose\nconverge to the identified set at a faster rate than existing procedures based\non bounded weight functions in a broad class of models. This provides a\ntheoretical justification for inverse variance weighting in this context, and\ncontrasts with analogous results for conditional moment equalities in which\noptimal weighting only affects the asymptotic variance. Second, the new\nweighting changes the asymptotic behavior, including the rate of convergence,\nof the KS statistic itself, requiring a new asymptotic theory in choosing the\ncritical value, which I provide. To make these comparisons, I derive rates of\nconvergence for the confidence regions I propose along with new results for\nrates of convergence of existing estimators under a general set of conditions.\nA series of examples illustrates the broad applicability of the conditions. A\nmonte carlo study examines the finite sample behavior of the confidence\nregions. \n\n"}
{"id": "1112.1024", "contents": "Title: Asymptotically Exact Inference in Conditional Moment Inequality Models Abstract: This paper derives the rate of convergence and asymptotic distribution for a\nclass of Kolmogorov-Smirnov style test statistics for conditional moment\ninequality models for parameters on the boundary of the identified set under\ngeneral conditions. In contrast to other moment inequality settings, the rate\nof convergence is faster than root-$n$, and the asymptotic distribution depends\nentirely on nonbinding moments. The results require the development of new\ntechniques that draw a connection between moment selection, irregular\nidentification, bandwidth selection and nonstandard M-estimation. Using these\nresults, I propose tests that are more powerful than existing approaches for\nchoosing critical values for this test statistic. I quantify the power\nimprovement by showing that the new tests can detect alternatives that converge\nto points on the identified set at a faster rate than those detected by\nexisting approaches. A monte carlo study confirms that the tests and the\nasymptotic approximations they use perform well in finite samples. In an\napplication to a regression of prescription drug expenditures on income with\ninterval data from the Health and Retirement Study, confidence regions based on\nthe new tests are substantially tighter than those based on existing methods. \n\n"}
{"id": "1201.0153", "contents": "Title: Empirical Bayes estimation of posterior probabilities of enrichment Abstract: To interpret differentially expressed genes or other discovered features,\nresearchers conduct hypothesis tests to determine which biological categories\nsuch as those of the Gene Ontology (GO) are enriched in the sense of having\ndifferential representation among the discovered features. We study application\nof better estimators of the local false discovery rate (LFDR), a probability\nthat the biological category has equivalent representation among the\npreselected features.\n  We identified three promising estimators of the LFDR for detecting\ndifferential representation: a semiparametric estimator (SPE), a normalized\nmaximum likelihood estimator (NMLE), and a maximum likelihood estimator (MLE).\nWe found that the MLE performs at least as well as the SPE for on the order of\n100 of GO categories even when the ideal number of components in its underlying\nmixture model is unknown. However, the MLE is unreliable when the number of GO\ncategories is small compared to the number of PMM components. Thus, if the\nnumber of categories is on the order of 10, the SPE is a more reliable LFDR\nestimator. The NMLE depends not only on the data but also on a specified value\nof the prior probability of differential representation. It is therefore an\nappropriate LFDR estimator only when the number of GO categories is too small\nfor application of the other methods.\n  For enrichment detection, we recommend estimating the LFDR by the MLE given\nat least a medium number (~100) of GO categories, by the SPE given a small\nnumber of GO categories (~10), and by the NMLE given a very small number (~1)\nof GO categories. \n\n"}
{"id": "1201.4013", "contents": "Title: Connectivity of Confined Dense Networks: Boundary Effects and Scaling\n  Laws Abstract: In this paper, we study the probability that a dense network confined within\na given geometry is fully connected. We employ a cluster expansion approach\noften used in statistical physics to analyze the effects that the boundaries of\nthe geometry have on connectivity. To maximize practicality and applicability,\nwe adopt four important point-to-point link models based on outage probability\nin our analysis: single-input single-output (SISO), single-input\nmultiple-output (SIMO), multiple-input single-output (MISO), and multiple-input\nmultiple-output (MIMO). Furthermore, we derive diversity and power scaling laws\nthat dictate how boundary effects can be mitigated (to leading order) in\nconfined dense networks for each of these models. Finally, in order to\ndemonstrate the versatility of our theory, we analyze boundary effects for\ndense networks comprising MIMO point-to-point links confined within a right\nprism, a polyhedron that accurately models many geometries that can be found in\npractice. We provide numerical results for this example, which verify our\nanalytical results. \n\n"}
{"id": "1201.5968", "contents": "Title: A New Poisson Noise Filter based on Weights Optimization Abstract: We propose a new image denoising algorithm when the data is contaminated by a\nPoisson noise. As in the Non-Local Means filter, the proposed algorithm is\nbased on a weighted linear combination of the bserved image. But in contract to\nthe latter where the weights are defined by a Gaussian kernel, we propose to\nchoose them in an optimal way. First some \"oracle\" weights are defined by\nminimizing a very tight upper bound of the Mean Square Error. For a practical\napplication the weights are estimated from the observed image. We prove that\nthe proposed filter converges at the usual optimal rate to the true image.\nSimulation results are presented to compare the performance of the presented\nfilter with conventional filtering methods. \n\n"}
{"id": "1201.6325", "contents": "Title: Origin of Long Lived Coherences in Light-Harvesting Complexes Abstract: A vibronic exciton model is developed to investigate the origin of long lived\ncoherences in light-harvesting complexes. Using experimentally determined\nparameters and uncorrelated site energy fluctuations, the model predicts\noscillations in the nonlinear spectra of the Fenna-Matthews-Olson (FMO) complex\nwith a dephasing time of 1.3 ps at 77 K. These oscillations correspond to the\ncoherent superposition of vibronic exciton states with dominant contributions\nfrom vibrational excitations on the same pigment. Purely electronic coherences\nare found to decay on a 200 fs timescale. \n\n"}
{"id": "1202.1916", "contents": "Title: Homogenization of the Poisson-Nernst-Planck Equations for Ion Transport\n  in Charged Porous Media Abstract: Effective Poisson-Nernst-Planck (PNP) equations are derived for macroscopic\nion transport in charged porous media under periodic fluid flow by an\nasymptotic multi-scale expansion with drift. The microscopic setting is a\ntwo-component periodic composite consisting of a dilute electrolyte continuum\n(described by standard PNP equations) and a continuous dielectric matrix, which\nis impermeable to the ions and carries a given surface charge. Four new\nfeatures arise in the upscaled equations: (i) the effective ionic diffusivities\nand mobilities become tensors, related to the microstructure; (ii) the\neffective permittivity is also a tensor, depending on the electrolyte/matrix\npermittivity ratio and the ratio of the Debye screening length to the\nmacroscopic length of the porous medium; (iii) the microscopic fluidic\nconvection is replaced by a diffusion-dispersion correction in the effective\ndiffusion tensor; and (iv) the surface charge per volume appears as a\ncontinuous \"background charge density\", as in classical membrane models. The\ncoefficient tensors in the upscaled PNP equations can be calculated from\nperiodic reference cell problems. For an insulating solid matrix, all gradients\nare corrected by the same tensor, and the Einstein relation holds at the\nmacroscopic scale, which is not generally the case for a polarizable matrix,\nunless the permittivity and electric field are suitably defined. In the limit\nof thin double layers, Poisson's equation is replaced by macroscopic\nelectroneutrality (balancing ionic and surface charges). The general form of\nthe macroscopic PNP equations may also hold for concentrated solution theories,\nbased on the local-density and mean-field approximations. These results have\nbroad applicability to ion transport in porous electrodes, separators,\nmembranes, ion-exchange resins, soils, porous rocks, and biological tissues. \n\n"}
{"id": "1202.3797", "contents": "Title: Nonlinear, electrocatalytic swimming in the presence of salt Abstract: A small, bimetallic particle in a hydrogen peroxide solution can propel\nitself by means of an electrocatalytic reaction. The swimming is driven by a\nflux of ions around the particle. We model this process for the presence of a\nmonovalent salt, where reaction-driven proton currents induce salt ion\ncurrents. A theory for thin diffuse layers is employed, which yields nonlinear,\ncoupled transport equations. The boundary conditions include a compact Stern\nlayer of adsorbed ions. Electrochemical processes on the particle surface are\nmodeled with a first order reaction of the Butler-Volmer type. The equations\nare solved numerically for the swimming speed. An analytical approximation is\nderived under the assumption that the decomposition of hydrogen peroxide occurs\nmainly without inducing an electric current. We find that the swimming speed\nincreases linearly with hydrogen peroxide concentration for small\nconcentrations. The influence of ion diffusion on the reaction rate can lead to\na concave shape of the function of speed vs. hydrogen peroxide concentration.\nThe compact layer of ions on the particle diminishes the reaction rate and\nconsequently reduces the speed. Our results are consistent with published\nexperimental data. \n\n"}
{"id": "1202.6418", "contents": "Title: An Information-geometric Approach to Sensor Management Abstract: An information-geometric approach to sensor management is introduced that is\nbased on following geodesic curves in a manifold of possible sensor\nconfigurations. This perspective arises by observing that, given a parameter\nestimation problem to be addressed through management of sensor assets, any\nparticular sensor configuration corresponds to a Riemannian metric on the\nparameter manifold. With this perspective, managing sensors involves navigation\non the space of all Riemannian metrics on the parameter manifold, which is\nitself a Riemannian manifold. Existing work assumes the metric on the parameter\nmanifold is one that, in statistical terms, corresponds to a Jeffreys prior on\nthe parameter to be estimated. It is observed that informative priors, as arise\nin sensor management, can also be accommodated. Given an initial sensor\nconfiguration, the trajectory along which to move in sensor configuration space\nto gather most information is seen to be locally defined by the geodesic\nstructure of this manifold. Further, divergences based on Fisher and Shannon\ninformation lead to the same Riemannian metric and geodesics. \n\n"}
{"id": "1203.0052", "contents": "Title: Excitation energy transfer in light-harvesting system: Effect of initial\n  state Abstract: The light-harvesting is a problem of long interest. It becomes active again\nin recent years stimulated by suggestions of quantum effects in energy\ntransport. Recent experiments found evidence that BChla 1 and BChla 6 are the\nfirst to be excited in the Fenna-Matthews-Olson(FMO) protein, theoretical\nstudies, however, are mostly restricted to consider the exciton in BChla 1\ninitially. In this paper, we study the energy transport in the FMO complex by\ntaking different initial states into account. Optimizations are performed for\nthe decoherence rates as to maximal transport efficiency. Dependence of the\nenergy transfer efficiency on the initial states is given and discussed.\nEffects of fluctuations in the site energies and couplings are also examined. \n\n"}
{"id": "1203.1485", "contents": "Title: Long-Lived Electronic Coherence in Dissipative Exciton-Dynamics of\n  Light-Harvesting Complexes Abstract: The observed prevalence of oscillatory signals in the spectroscopy of\nbiological light-harvesting complexes at ambient temperatures has led to a\nsearch for mechanisms supporting coherent transport through larger molecules in\nnoisy environments. We demonstrate a generic mechanism supporting long-lasting\nelectronic coherence up to 0.3 ps at a temperature of 277 K. The mechanism\nrelies on two properties of the spectral density: (i) a large dissipative\ncoupling to a continuum of higher-frequency vibrations required for efficient\ntransport and (ii) a small slope of the spectral density at zero frequency. \n\n"}
{"id": "1203.3896", "contents": "Title: Fast and Adaptive Sparse Precision Matrix Estimation in High Dimensions Abstract: This paper proposes a new method for estimating sparse precision matrices in\nthe high dimensional setting. It has been popular to study fast computation and\nadaptive procedures for this problem. We propose a novel approach, called\nSparse Column-wise Inverse Operator, to address these two issues. We analyze an\nadaptive procedure based on cross validation, and establish its convergence\nrate under the Frobenius norm. The convergence rates under other matrix norms\nare also established. This method also enjoys the advantage of fast computation\nfor large-scale problems, via a coordinate descent algorithm. Numerical merits\nare illustrated using both simulated and real datasets. In particular, it\nperforms favorably on an HIV brain tissue dataset and an ADHD resting-state\nfMRI dataset. \n\n"}
{"id": "1203.3978", "contents": "Title: Computational Methodologies and Physical Insights into Electronic Energy\n  Transfer in Photosynthetic Light-Harvesting Complexes Abstract: We examine computational techniques and methodologies currently in use to\nexplore electronic excitation energy transfer in the context of\nlight-harvesting complexes in photosynthetic antenna systems, and comment on\nsome new insights into the underlying physics. Advantages and pitfalls of these\nmethodologies are discussed, as are some physical insights into the\nphotosynthetic dynamics. By combining results from molecular modelling of the\ncomplexes (structural description) with an effective non-equilibrium\nstatistical description (time evolution), we identify some general features,\nregardless of the particular distribution in the protein scaffold, that are\ncentral to light-harvesting dynamics and, that could ultimately be related to\nthe high efficiency of the overall process. Based on these general common\nfeatures, some possible new directions in the field are discussed. \n\n"}
{"id": "1203.5056", "contents": "Title: The fundamental role of quantized vibrations in coherent light\n  harvesting by cryptophyte algae Abstract: The influence of fast vibrations on energy transfer and conversion in natural\nmolecular aggregates is an issue of central interest. This article shows the\nimportant role of high-energy quantized vibrations and their non-equilibrium\ndynamics for energy transfer in photosynthetic systems with highly localized\nexcitonic states. We consider the cryptophyte antennae protein phycoerythrin\n545 and show that coupling to quantized vibrations which are quasi-resonant\nwith excitonic transitions is fundamental for biological function as it\ngenerates non-cascaded transport with rapid and wider spatial distribution of\nexcitation energy. Our work also indicates that the non-equilibrium dynamics of\nsuch vibrations can manifest itself in ultrafast beating of both excitonic\npopulations and coherences at room temperature, with time scales in agreement\nwith those reported in experiments. Moreover, we show that mechanisms\nsupporting coherent excitonic dynamics assist coupling to selected modes that\nchannel energy to preferential sites in the complex. We therefore argue that,\nin the presence of strong coupling between electronic excitations and quantized\nvibrations, a concrete and important advantage of quantum coherent dynamics is\nprecisely to tune resonances that promote fast and effective energy\ndistribution. \n\n"}
{"id": "1203.5072", "contents": "Title: Coherence and Decoherence in Biological Systems: Principles of Noise\n  Assisted Transport and the Origin of Long-lived Coherences Abstract: The quantum dynamics of transport networks in the presence of noisy\nenvironments have recently received renewed attention with the discovery of\nlong-lived coherences in different photosynthetic complexes. This experimental\nevidence has raised two fundamental questions: Firstly, what are the mechanisms\nsupporting long-lived coherences and secondly, how can we assess the possible\nfunctional role that the interplay of noise and quantum coherence might play in\nthe seemingly optimal operation of biological systems under natural conditions?\nHere we review recent results, illuminate them at the hand of two paradigmatic\nsystems, the Fenna-Matthew-Olson (FMO) complex and the light harvesting complex\nLHII, and present new progress on both questions. In particular we introduce\nthe concept of the phonon antennae and discuss the possible microscopic origin\nor long-lived electronic coherences. \n\n"}
{"id": "1203.5362", "contents": "Title: Throughput Optimal Scheduling with Dynamic Channel Feedback Abstract: It is well known that opportunistic scheduling algorithms are throughput\noptimal under full knowledge of channel and network conditions. However, these\nalgorithms achieve a hypothetical achievable rate region which does not take\ninto account the overhead associated with channel probing and feedback required\nto obtain the full channel state information at every slot. We adopt a channel\nprobing model where $\\beta$ fraction of time slot is consumed for acquiring the\nchannel state information (CSI) of a single channel. In this work, we design a\njoint scheduling and channel probing algorithm named SDF by considering the\noverhead of obtaining the channel state information. We first analytically\nprove SDF algorithm can support $1+\\epsilon$ fraction of of the full rate\nregion achieved when all users are probed where $\\epsilon$ depends on the\nexpected number of users which are not probed. Then, for homogenous channel, we\nshow that when the number of users in the network is greater than 3, $\\epsilon\n> 0$, i.e., we guarantee to expand the rate region. In addition, for\nheterogenous channels, we prove the conditions under which SDF guarantees to\nincrease the rate region. We also demonstrate numerically in a realistic\nsimulation setting that this rate region can be achieved by probing only less\nthan 50% of all channels in a CDMA based cellular network utilizing high data\nrate protocol under normal channel conditions. \n\n"}
{"id": "1203.6674", "contents": "Title: Path integral Monte Carlo with importance sampling for excitons\n  interacting with an arbitrary phonon bath Abstract: The reduced density matrix of excitons coupled to a phonon bath at a finite\ntemperature is studied using the path integral Monte Carlo method. Appropriate\nchoices of estimators and importance sampling schemes are crucial to the\nperformance of the Monte Carlo simulation. We show that by choosing the\npopulation-normalized estimator for the reduced density matrix, an efficient\nand physically-meaningful sampling function can be obtained. In addition, the\nnonadiabatic phonon probability density is obtained as a byproduct during the\nsampling procedure. For importance sampling, we adopted the Metropolis-adjusted\nLangevin algorithm. The analytic expression for the gradient of the target\nprobability density function associated with the population-normalized\nestimator cannot be obtained in closed form without a matrix power series. An\napproximated gradient that can be efficiently calculated is explored to achieve\nbetter computational scaling and efficiency. Application to a simple\none-dimensional model system from the previous literature confirms the\ncorrectness of the method developed in this manuscript. The displaced harmonic\nmodel system within the single exciton manifold shows the numerically exact\ntemperature dependence of the coherence and population of the excitonic system.\nThe sampling scheme can be applied to an arbitrary anharmonic environment, such\nas multichromophoric systems embedded in the protein complex. The result of\nthis study is expected to stimulate further development of real time\npropagation methods that satisfy the detailed balance condition for exciton\npopulations. \n\n"}
{"id": "1204.0151", "contents": "Title: Distribution of Maximal Luminosity of Galaxies in the Sloan Digital Sky\n  Survey Abstract: Extreme value statistics (EVS) is applied to the distribution of galaxy\nluminosities in the Sloan Digital Sky Survey (SDSS). We analyze the DR8 Main\nGalaxy Sample (MGS), as well as the Luminous Red Galaxies (LRG). Maximal\nluminosities are sampled from batches consisting of elongated pencil beams in\nthe radial direction of sight. For the MGS, results suggest a small and\npositive tail index $\\xi$, effectively ruling out the possibility of having a\nfinite maximum cutoff luminosity, and implying that the luminosity distribution\nfunction may decay as a power law at the high luminosity end. Assuming,\nhowever, $\\xi=0$, a non-parametric comparison of the maximal luminosities with\nthe Fisher-Tippett-Gumbel distribution (limit distribution for variables\ndistributed by the Schechter fit) indicates a good agreement provided\nuncertainties arising both from the finite batch size and from the batch size\ndistribution are accounted for. For a volume limited sample of LRGs, results\nshow that they can be described as being the extremes of a luminosity\ndistribution with an exponentially decaying tail, provided the uncertainties\nrelated to batch-size distribution are taken care of. \n\n"}
{"id": "1204.1400", "contents": "Title: Connectivity of Large Wireless Networks under A Generic Connection Model Abstract: This paper provides a necessary and sufficient condition for a random network\nwith nodes Poissonly distributed on a unit square and a pair of nodes directly\nconnected following a generic random connection model to be asymptotically\nalmost surely connected. The results established in this paper expand recent\nresults obtained for connectivity of random geometric graphs from the unit disk\nmodel and the fewer results from the log-normal model to the more generic and\nmore practical random connection model. \n\n"}
{"id": "1204.1595", "contents": "Title: Femtocaching and Device-to-Device Collaboration: A New Architecture for\n  Wireless Video Distribution Abstract: We present a new architecture to handle the ongoing explosive increase in the\ndemand for video content in wireless networks. It is based on distributed\ncaching of the content in femto-basestations with small or non-existing\nbackhaul capacity but with considerable storage space, called helper nodes. We\nalso consider using the mobile terminals themselves as caching helpers, which\ncan distribute video through device-to-device communications. This approach\nallows an improvement in the video throughput without deployment of any\nadditional infrastructure. The new architecture can improve video throughput by\none to two orders-of-magnitude. \n\n"}
{"id": "1204.3941", "contents": "Title: A Log-Linear Graphical Model for Inferring Genetic Networks from\n  High-Throughput Sequencing Data Abstract: Gaussian graphical models are often used to infer gene networks based on\nmicroarray expression data. Many scientists, however, have begun using\nhigh-throughput sequencing technologies to measure gene expression. As the\nresulting high-dimensional count data consists of counts of sequencing reads\nfor each gene, Gaussian graphical models are not optimal for modeling gene\nnetworks based on this discrete data. We develop a novel method for estimating\nhigh-dimensional Poisson graphical models, the Log-Linear Graphical Model,\nallowing us to infer networks based on high-throughput sequencing data. Our\nmodel assumes a pair-wise Markov property: conditional on all other variables,\neach variable is Poisson. We estimate our model locally via neighborhood\nselection by fitting 1-norm penalized log-linear models. Additionally, we\ndevelop a fast parallel algorithm, an approach we call the Poisson Graphical\nLasso, permitting us to fit our graphical model to high-dimensional genomic\ndata sets. In simulations, we illustrate the effectiveness of our methods for\nrecovering network structure from count data. A case study on breast cancer\nmicroRNAs, a novel application of graphical models, finds known regulators of\nbreast cancer genes and discovers novel microRNA clusters and hubs that are\ntargets for future research. \n\n"}
{"id": "1204.4094", "contents": "Title: The nanofluidics can explain ascent of water in tallest trees Abstract: In Amazing numbers in biology, Flindt reports a giant, 128 meter-tall\neucalyptus, and a 135 meter-tall sequoia. However, the explanation of the\nmaximum altitude of the crude sap ascent and consequently the main reason of\nthe maximum size that trees can reach is not well understood. According to tree\nspecies, the crude sap is driven in xylem microtubes with diameters ranging\nbetween 50 and 400 micrometers. The sap contains diluted salts but its physical\nproperties are roughly those of water; consequently, hydrodynamic, capillarity\nand osmotic pressure yield a crude sap ascent of a few tens of meters only.\nToday, we can propound a new understanding of the ascent of sap to the top of\nvery tall trees thanks to a new comparison between experiments associated with\nthe cohesion-tension theory and the disjoining pressure concept. Here we show\nthat the pressure in the water-storing tracheids of leaves can be strongly\nnegative whereas the pressure in the xylem microtubes of stems may remain\npositive when, at high level, inhomogeneous liquid nanolayers wet the xylem\nwalls of microtubes. The nanofluidic model of crude sap in tall trees discloses\na stable sap layer up to an altitude where the pancake layer thickness coexists\nwith the dry xylem wall and corresponds to the maximum size of tallest trees.\nIn very thin layers, sap flows are widely more significant than those obtained\nwith classical Navier-Stokes models and consequently are able to refill\nstomatic cells when phloem embolisms supervene. These results drop an inkling\nthat the disjoining pressure is an efficient tool to study biological liquids\nin contact with substrates at a nanoscale range. \n\n"}
{"id": "1204.5262", "contents": "Title: Population and Coherence Dynamics in Light Harvesting Complex II (LH2) Abstract: The electronic excitation population and coherence dynamics in the\nchromophores of the photosynthetic light harvesting complex 2 (LH2) B850 ring\nfrom purple bacteria (Rhodopseudomonas acidophila) have been studied\ntheoretically at both physiological and cryogenic temperatures. Similar to the\nwell-studied Fenna-Matthews-Olson (FMO) protein, oscillations of the excitation\npopulation and coherence in the site basis are observed in LH2 by using a\nscaled hierarchical equation of motion (HEOM) approach. However, this\noscillation time (300 fs) is much shorter compared to the FMO protein (650 fs)\nat cryogenic temperature. Both environment and high temperature are found to\nenhance the propagation speed of the exciton wave packet yet they shorten the\ncoherence time and suppress the oscillation amplitude of coherence and the\npopulation. Our calculations show that a long-lived coherence between\nchromophore electronic excited states can exist in such a noisy biological\nenvironment. \n\n"}
{"id": "1205.0883", "contents": "Title: Functional quantum biology in photosynthesis and magnetoreception Abstract: Is there a functional role for quantum mechanics or coherent quantum effects\nin biological processes? While this question is as old as quantum theory, only\nrecently have measurements on biological systems on ultra-fast time-scales shed\nlight on a possible answer. In this review we give an overview of the two main\ncandidates for biological systems which may harness such functional quantum\neffects: photosynthesis and magnetoreception. We discuss some of the latest\nevidence both for and against room temperature quantum coherence, and consider\nwhether there is truly a functional role for coherence in these biological\nmechanisms. Finally, we give a brief overview of some more speculative examples\nof functional quantum biology including the sense of smell, long-range quantum\ntunneling in proteins, biological photoreceptors, and the flow of ions across a\ncell membrane. \n\n"}
{"id": "1205.1931", "contents": "Title: First Passage Times for a Tracer Particle in Single File Diffusion and\n  Fractional Brownian Motion Abstract: We investigate the full functional form of the first passage time density\n(FPTD) of a tracer particle in a single-file diffusion (SFD) system whose\npopulation is: (i) homogeneous, i.e., all particles having the same diffusion\nconstant and (ii) heterogeneous, with diffusion constants drawn from a\nheavy-tailed power-law distribution. In parallel, the full FPTD for fractional\nBrownian motion [fBm - defined by the Hurst parameter, 0<H<1] is studied, of\ninterest here as fBm and SFD systems belong to the same universality class.\n  Extensive stochastic (non-Markovian) SFD and fBm simulations are performed\nand compared to two analytical Markovian techniques: the Method of Images\napproximation (MIA) and the Willemski-Fixman approximation (WFA). We find that\nthe MIA cannot approximate well any temporal scale of the SFD FPTD. Our exact\ninversion of the Willemski-Fixman integral equation captures the long-time\npower-law exponent, when H >= 1/3, as predicted by Molchan [1999] for fBm. When\nH<1/3, which includes homogeneous SFD (H=1/4), and heterogeneous SFD (H<1/4),\nthe WFA fails to agree with any temporal scale of the simulations and Molchan's\nlong-time result.\n  SFD systems are compared to their fBm counter parts; and in the homogeneous\nsystem both scaled FPTDs agree on all temporal scales including also, the\nresult by Molchan, thus affirming that SFD and fBm dynamics belong to the same\nuniversality class. In the heterogeneous case SFD and fBm results for\nheterogeneity-averaged FPTDs agree in the asymptotic time limit. The\nnon-averaged heterogeneous SFD systems display a lack of self-averaging. An\nexponential with a power-law argument, multiplied by a power-law pre-factor is\nshown to describe well the FPTD for all times for homogeneous SFD and\nsub-diffusive fBm systems. \n\n"}
{"id": "1205.2501", "contents": "Title: Tobit Bayesian Model Averaging and the Determinants of Foreign Direct\n  Investment Abstract: We develop a fully Bayesian, computationally efficient framework for\nincorporating model uncertainty into Type II Tobit models and apply this to the\ninvestigation of the determinants of Foreign Direct Investment (FDI). While\ndirect evaluation of modelprobabilities is intractable in this setting, we show\nthat by using conditional Bayes Factors, which nest model moves inside a Gibbs\nsampler, we are able to incorporate model uncertainty in a straight-forward\nfashion. We conclude with a study of global FDI flows between 1988-2000. \n\n"}
{"id": "1205.4097", "contents": "Title: On efficient estimators of the proportion of true null hypotheses in a\n  multiple testing setup Abstract: We consider the problem of estimating the proportion $\\theta$ of true null\nhypotheses in a multiple testing context. The setup is classically modeled\nthrough a semiparametric mixture with two components: a uniform distribution on\ninterval $[0,1]$ with prior probability $\\theta$ and a nonparametric density\n$f$. We discuss asymptotic efficiency results and establish that two different\ncases occur whether $f$ vanishes on a set with non null Lebesgue measure or\nnot. In the first case, we exhibit estimators converging at parametric rate,\ncompute the optimal asymptotic variance and conjecture that no estimator is\nasymptotically efficient (i.e. attains the optimal asymptotic variance). In the\nsecond case, we prove that the quadratic risk of any estimator does not\nconverge at parametric rate. We illustrate those results on simulated data. \n\n"}
{"id": "1205.4476", "contents": "Title: Soft Rule Ensembles for Statistical Learning Abstract: In this article supervised learning problems are solved using soft rule\nensembles. We first review the importance sampling learning ensembles (ISLE)\napproach that is useful for generating hard rules. The soft rules are then\nobtained with logistic regression from the corresponding hard rules. In order\nto deal with the perfect separation problem related to the logistic regression,\nFirth's bias corrected likelihood is used. Various examples and simulation\nresults show that soft rule ensembles can improve predictive performance over\nhard rule ensembles. \n\n"}
{"id": "1206.4032", "contents": "Title: Rank-based model selection for multiple ions quantum tomography Abstract: The statistical analysis of measurement data has become a key component of\nmany quantum engineering experiments. As standard full state tomography becomes\nunfeasible for large dimensional quantum systems, one needs to exploit prior\ninformation and the \"sparsity\" properties of the experimental state in order to\nreduce the dimensionality of the estimation problem. In this paper we propose\nmodel selection as a general principle for finding the simplest, or most\nparsimonious explanation of the data, by fitting different models and choosing\nthe estimator with the best trade-off between likelihood fit and model\ncomplexity. We apply two well established model selection methods -- the Akaike\ninformation criterion (AIC) and the Bayesian information criterion (BIC) -- to\nmodels consising of states of fixed rank and datasets such as are currently\nproduced in multiple ions experiments. We test the performance of AIC and BIC\non randomly chosen low rank states of 4 ions, and study the dependence of the\nselected rank with the number of measurement repetitions for one ion states. We\nthen apply the methods to real data from a 4 ions experiment aimed at creating\na Smolin state of rank 4. The two methods indicate that the optimal model for\ndescribing the data lies between ranks 6 and 9, and the Pearson $\\chi^{2}$ test\nis applied to validate this conclusion. Additionally we find that the mean\nsquare error of the maximum likelihood estimator for pure states is close to\nthat of the optimal over all possible measurements. \n\n"}
{"id": "1206.5232", "contents": "Title: Extending Monte Carlo Methods to Factor Graphs with Negative and Complex\n  Factors Abstract: The partition function of a factor graph can sometimes be accurately\nestimated by Monte Carlo methods. In this paper, such methods are extended to\nfactor graphs with negative and complex factors. \n\n"}
{"id": "1207.0225", "contents": "Title: Spectral rate theory for projected two-state kinetics Abstract: Classical rate theories often fail in cases where the observable(s) or order\nparameter(s) used are poor reaction coordinates or the observed signal is\ndeteriorated by noise, such that no clear separation between reactants and\nproducts is possible. Here, we present a general spectral two-state rate theory\nfor ergodic dynamical systems in thermal equilibrium that explicitly takes into\naccount how the system is observed. The theory allows the systematic estimation\nerrors made by standard rate theories to be understood and quantified. We also\nelucidate the connection of spectral rate theory with the popular Markov state\nmodeling (MSM) approach for molecular simulation studies. An optimal rate\nestimator is formulated that gives robust and unbiased results even for poor\nreaction coordinates and can be applied to both computer simulations and\nsingle-molecule experiments. No definition of a dividing surface is required.\nAnother result of the theory is a model-free definition of the reaction\ncoordinate quality (RCQ). The RCQ can be bounded from below by the directly\ncomputable observation quality (OQ), thus providing a measure allowing the RCQ\nto be optimized by tuning the experimental setup. Additionally, the respective\npartial probability distributions can be obtained for the reactant and product\nstates along the observed order parameter, even when these strongly overlap.\nThe effects of both filtering (averaging) and uncorrelated noise are also\nexamined. The approach is demonstrated on numerical examples and experimental\nsingle-molecule force probe data of the p5ab RNA hairpin and the apo-myoglobin\nprotein at low pH, here focusing on the case of two-state kinetics. \n\n"}
{"id": "1207.0752", "contents": "Title: Test MaxEnt in Social Strategy Transitions with Experimental Two-Person\n  Constant Sum 2$\\times$2 Games Abstract: By using laboratory experimental data, we test the uncertainty of social\nstrategy transitions in various competing environments of fixed paired\ntwo-person constant sum $2 \\times 2$ games. It firstly shows that, the\ndistributions of social strategy transitions are not erratic but obey the\nprinciple of the maximum entropy (MaxEnt). This finding indicates that human\nsubject social systems and natural systems could have wider common backgrounds. \n\n"}
{"id": "1207.1758", "contents": "Title: The Social Contagion Hypothesis: Comment on \"Social Contagion Theory:\n  Examining Dynamic Social Networks and Human Behavior\" Abstract: I reflect on the statistical methods of the Christakis-Fowler studies on\nnetwork-based contagion of traits by checking the sensitivity of these kinds of\nresults to various alternate specifications and generative mechanisms. Despite\nthe honest efforts of all involved, I remain pessimistic about establishing\nwhether binary health outcomes or product adoptions are contagious if the\nevidence comes from simultaneously observed data. \n\n"}
{"id": "1207.1888", "contents": "Title: Keeping greed good: sparse regression under design uncertainty with\n  application to biomass characterization Abstract: In this paper, we consider the classic measurement error regression scenario\nin which our independent, or design, variables are observed with several\nsources of additive noise. We will show that our motivating example's\nreplicated measurements on both the design and dependent variables may be\nleveraged to enhance a sparse regression algorithm. Specifically, we estimate\nthe variance and use it to scale our design variables. We demonstrate the\nefficacy of scaling from several points of view and validate it empirically\nwith a biomass characterization data set using two of the most widely used\nsparse algorithms: least angle regression (LARS) and the Dantzig selector (DS). \n\n"}
{"id": "1207.6430", "contents": "Title: Optimal Data Collection For Informative Rankings Expose Well-Connected\n  Graphs Abstract: Given a graph where vertices represent alternatives and arcs represent\npairwise comparison data, the statistical ranking problem is to find a\npotential function, defined on the vertices, such that the gradient of the\npotential function agrees with the pairwise comparisons. Our goal in this paper\nis to develop a method for collecting data for which the least squares\nestimator for the ranking problem has maximal Fisher information. Our approach,\nbased on experimental design, is to view data collection as a bi-level\noptimization problem where the inner problem is the ranking problem and the\nouter problem is to identify data which maximizes the informativeness of the\nranking. Under certain assumptions, the data collection problem decouples,\nreducing to a problem of finding multigraphs with large algebraic connectivity.\nThis reduction of the data collection problem to graph-theoretic questions is\none of the primary contributions of this work. As an application, we study the\nYahoo! Movie user rating dataset and demonstrate that the addition of a small\nnumber of well-chosen pairwise comparisons can significantly increase the\nFisher informativeness of the ranking. As another application, we study the\n2011-12 NCAA football schedule and propose schedules with the same number of\ngames which are significantly more informative. Using spectral clustering\nmethods to identify highly-connected communities within the division, we argue\nthat the NCAA could improve its notoriously poor rankings by simply scheduling\nmore out-of-conference games. \n\n"}
{"id": "1207.6504", "contents": "Title: Space-time correlations in urban population flows Abstract: Evidences are presented concerning tantalizing regularities in cities'\npopulation-flows in what regards to space and time correlations. The former\nexhibit a distance-behavior (for large distances) compatible with the inverse\nsquare law, following an overall Lorentzian dependence with an scale-parameter\nof $74\\pm6$ km. The later decay exponentially with a characteristic time of\n$17.2\\pm1.3$ years. These features can be explained by a dynamical model for\ncities' population-growth of a Lagevinian nature. Numerical simulations based\non the model confirm its applicability. The model also allows for the\nidentification of collective normal modes of city-growth dynamics that can be\nempirically identified. \n\n"}
{"id": "1207.7298", "contents": "Title: Throughput of Rateless Codes over Broadcast Erasure Channels Abstract: In this paper, we characterize the throughput of a broadcast network with n\nreceivers using rateless codes with block size K. We assume that the underlying\nchannel is a Markov modulated erasure channel that is i.i.d. across users, but\ncan be correlated in time. We characterize the system throughput asymptotically\nin n. Specifically, we explicitly show how the throughput behaves for different\nvalues of the coding block size K as a function of n, as n approaches infinity.\nFor finite values of K and n, under the more restrictive assumption of\nGilbert-Elliott channels, we are able to provide a lower bound on the maximum\nachievable throughput. Using simulations we show the tightness of the bound\nwith respect to system parameters n and K, and find that its performance is\nsignificantly better than the previously known lower bounds. \n\n"}
{"id": "1208.0177", "contents": "Title: Gouy-Stodola Theorem as a variational principle for open systems Abstract: The recent researches in non equilibrium and far from equilibrium systems\nhave been proved to be useful for their applications in different disciplines\nand many subjects. A general principle to approach all these phenomena with a\nunique method of analysis is required in science and engineering: a variational\nprinciple would have this fundamental role. Here, the Gouy-Stodola theorem is\nproposed to be this general variational principle, both proving that it\nsatisfies the above requirements and relating it to a statistical results on\nentropy production. \n\n"}
{"id": "1208.1188", "contents": "Title: Relations between allometric scalings and fluctuations in complex\n  systems: The case of Japanese firms Abstract: To elucidate allometric scaling in complex systems, we investigated the\nunderlying scaling relationships between typical three-scale indicators for\napproximately 500,000 Japanese firms; namely, annual sales, number of\nemployees, and number of business partners. First, new scaling relations\nincluding the distributions of fluctuations were discovered by systematically\nanalyzing conditional statistics. Second, we introduced simple probabilistic\nmodels that reproduce all these scaling relations, and we derived relations\nbetween scaling exponents and the magnitude of fluctuations. \n\n"}
{"id": "1208.3036", "contents": "Title: Bayesian astrostatistics: a backward look to the future Abstract: This perspective chapter briefly surveys: (1) past growth in the use of\nBayesian methods in astrophysics; (2) current misconceptions about both\nfrequentist and Bayesian statistical inference that hinder wider adoption of\nBayesian methods by astronomers; and (3) multilevel (hierarchical) Bayesian\nmodeling as a major future direction for research in Bayesian astrostatistics,\nexemplified in part by presentations at the first ISI invited session on\nastrostatistics, commemorated in this volume. It closes with an intentionally\nprovocative recommendation for astronomical survey data reporting, motivated by\nthe multilevel Bayesian perspective on modeling cosmic populations: that\nastronomers cease producing catalogs of estimated fluxes and other source\nproperties from surveys. Instead, summaries of likelihood functions (or\nmarginal likelihood functions) for source properties should be reported (not\nposterior probability density functions), including nontrivial summaries (not\nsimply upper limits) for candidate objects that do not pass traditional\ndetection thresholds. \n\n"}
{"id": "1208.3428", "contents": "Title: Comparative Bi-stochastizations and Associated\n  Clusterings/Regionalizations of the 1995-2000 U. S. Intercounty Migration\n  Network Abstract: Wang, Li and Konig have recently compared the cluster-theoretic properties of\nbi-stochasticized symmetric data similarity (e. g. kernel) matrices, produced\nby minimizing two different forms of Bregman divergences. We extend their\ninvestigation to non-symmetric matrices, specifically studying the 1995-2000 U.\nS. 3,107 x 3,107 intercounty migration matrix. A particular bi-stochastized\nform of it had been obtained (arXiv:1207.0437), using the well-established\nSinkhorn-Knopp (SK) (biproportional) algorithm--which minimizes the\nKullback-Leibler form of the divergence. This matrix has but a single entry\nequal to (the maximal possible value of) 1. Highly contrastingly, the\nbi-stochastic matrix obtained here, implementing the Wang-Li-Konig-algorithm\nfor the minimum of the alternative, squared-norm form of the divergence, has\n2,707 such unit entries. The corresponding 3,107-vertex, 2,707-link directed\ngraph has 2,352 strong components. These consist of 1,659 single/isolated\ncounties, 654 doublets (thirty-one interstate in nature), 22 triplets (one\nbeing interstate), 13 quartets (one being interstate), three quintets and one\nseptet. Not manifest in these graph-theoretic results, however, are the\nfive-county states of Hawaii and Rhode Island and the eight-county state of\nConnecticut. These--among other regional configurations--appealingly emerged as\nwell-defined entities in the SK-based strong-component hierarchical clustering. \n\n"}
{"id": "1208.3524", "contents": "Title: Power-law distributions in binned empirical data Abstract: Many man-made and natural phenomena, including the intensity of earthquakes,\npopulation of cities and size of international wars, are believed to follow\npower-law distributions. The accurate identification of power-law patterns has\nsignificant consequences for correctly understanding and modeling complex\nsystems. However, statistical evidence for or against the power-law hypothesis\nis complicated by large fluctuations in the empirical distribution's tail, and\nthese are worsened when information is lost from binning the data. We adapt the\nstatistically principled framework for testing the power-law hypothesis,\ndeveloped by Clauset, Shalizi and Newman, to the case of binned data. This\napproach includes maximum-likelihood fitting, a hypothesis test based on the\nKolmogorov--Smirnov goodness-of-fit statistic and likelihood ratio tests for\ncomparing against alternative explanations. We evaluate the effectiveness of\nthese methods on synthetic binned data with known structure, quantify the loss\nof statistical power due to binning, and apply the methods to twelve real-world\nbinned data sets with heavy-tailed patterns. \n\n"}
{"id": "1208.4043", "contents": "Title: Dynamic Anomalography: Tracking Network Anomalies via Sparsity and Low\n  Rank Abstract: In the backbone of large-scale networks, origin-to-destination (OD) traffic\nflows experience abrupt unusual changes known as traffic volume anomalies,\nwhich can result in congestion and limit the extent to which end-user quality\nof service requirements are met. As a means of maintaining seamless end-user\nexperience in dynamic environments, as well as for ensuring network security,\nthis paper deals with a crucial network monitoring task termed dynamic\nanomalography. Given link traffic measurements (noisy superpositions of\nunobserved OD flows) periodically acquired by backbone routers, the goal is to\nconstruct an estimated map of anomalies in real time, and thus summarize the\nnetwork `health state' along both the flow and time dimensions. Leveraging the\nlow intrinsic-dimensionality of OD flows and the sparse nature of anomalies, a\nnovel online estimator is proposed based on an exponentially-weighted\nleast-squares criterion regularized with the sparsity-promoting $\\ell_1$-norm\nof the anomalies, and the nuclear norm of the nominal traffic matrix. After\nrecasting the non-separable nuclear norm into a form amenable to online\noptimization, a real-time algorithm for dynamic anomalography is developed and\nits convergence established under simplifying technical assumptions. For\noperational conditions where computational complexity reductions are at a\npremium, a lightweight stochastic gradient algorithm based on Nesterov's\nacceleration technique is developed as well. Comprehensive numerical tests with\nboth synthetic and real network data corroborate the effectiveness of the\nproposed online algorithms and their tracking capabilities, and demonstrate\nthat they outperform state-of-the-art approaches developed to diagnose traffic\nanomalies. \n\n"}
{"id": "1208.4989", "contents": "Title: Penalized estimation in high-dimensional hidden Markov models with\n  state-specific graphical models Abstract: We consider penalized estimation in hidden Markov models (HMMs) with\nmultivariate Normal observations. In the moderate-to-large dimensional setting,\nestimation for HMMs remains challenging in practice, due to several concerns\narising from the hidden nature of the states. We address these concerns by\n$\\ell_1$-penalization of state-specific inverse covariance matrices. Penalized\nestimation leads to sparse inverse covariance matrices which can be interpreted\nas state-specific conditional independence graphs. Penalization is nontrivial\nin this latent variable setting; we propose a penalty that automatically adapts\nto the number of states $K$ and the state-specific sample sizes and can cope\nwith scaling issues arising from the unknown states. The methodology is\nadaptive and very general, applying in particular to both low- and\nhigh-dimensional settings without requiring hand tuning. Furthermore, our\napproach facilitates exploration of the number of states $K$ by coupling\nestimation for successive candidate values $K$. Empirical results on simulated\nexamples demonstrate the effectiveness of the proposed approach. In a\nchallenging real data example from genome biology, we demonstrate the ability\nof our approach to yield gains in predictive power and to deliver richer\nestimates than existing methods. \n\n"}
{"id": "1209.0089", "contents": "Title: Estimating the historical and future probabilities of large terrorist\n  events Abstract: Quantities with right-skewed distributions are ubiquitous in complex social\nsystems, including political conflict, economics and social networks, and these\nsystems sometimes produce extremely large events. For instance, the 9/11\nterrorist events produced nearly 3000 fatalities, nearly six times more than\nthe next largest event. But, was this enormous loss of life statistically\nunlikely given modern terrorism's historical record? Accurately estimating the\nprobability of such an event is complicated by the large fluctuations in the\nempirical distribution's upper tail. We present a generic statistical algorithm\nfor making such estimates, which combines semi-parametric models of tail\nbehavior and a nonparametric bootstrap. Applied to a global database of\nterrorist events, we estimate the worldwide historical probability of observing\nat least one 9/11-sized or larger event since 1968 to be 11-35%. These results\nare robust to conditioning on global variations in economic development,\ndomestic versus international events, the type of weapon used and a truncated\nhistory that stops at 1998. We then use this procedure to make a data-driven\nstatistical forecast of at least one similar event over the next decade. \n\n"}
{"id": "1209.0565", "contents": "Title: The enrichment history of the intracluster medium: a Bayesian approach Abstract: This work measures the evolution of the iron content in galaxy clusters by a\nrigorous analysis of the data of 130 clusters at 0.1<z<1.3. This task is made\ndifficult by a) the low signal-to-noise ratio of abundance measurements and the\nupper limits, b) possible selection effects, c) boundaries in the parameter\nspace, d) non-Gaussian errors, e) the intrinsic variety of the objects studied,\nand f) abundance systematics. We introduce a Bayesian model to address all\nthese issues at the same time, thus allowing cross-talk (covariance). On\nsimulated data, the Bayesian fit recovers the input enrichment history, unlike\nin standard analysis. After accounting for a possible dependence on X-ray\ntemperature, for metal abundance systematics, and for the intrinsic variety of\nstudied objects, we found that the present-day metal content is not reached\neither at high or at low redshifts, but gradually over time: iron abundance\nincreases by a factor 1.5 in the 7 Gyr sampled by the data. Therefore, feedback\nin metal abundance does not end at high redshift. Evolution is established with\na moderate amount of evidence, 19 to 1 odds against faster or slower metal\nenrichment histories. We quantify, for the first time, the intrinsic spread in\nmetal abundance, 18+/-3 %, after correcting for the effect of evolution, X-ray\ntemperature, and metal abundance systematics. Finally, we also present an\nanalytic approximation of the X-ray temperature and metal abundance likelihood\nfunctions, which are useful for other regression fitting involving these\nparameters. The data for the 130 clusters and code used for the stochastic\ncomputation are provided with the paper. \n\n"}
{"id": "1209.1270", "contents": "Title: A practical recipe to fit discrete power-law distributions Abstract: Power laws pervade statistical physics and complex systems, but,\ntraditionally, researchers in these fields have paid little attention to\nproperly fit these distributions. Who has not seen (or even shown) a log-log\nplot of a completely curved line pretending to be a power law? Recently,\nClauset et al. have proposed a method to decide if a set of values of a\nvariable has a distribution whose tail is a power law. The key of their\nprocedure is the identification of the minimum value of the variable for which\nthe fit holds, which is selected as the value for which the Kolmogorov-Smirnov\ndistance between the empirical distribution and its maximum-likelihood fit is\nminimum. However, it has been shown that this method can reject the power-law\nhypothesis even in the case of power-law simulated data. Here we propose a\nsimpler selection criterion, which is illustrated with the more involving case\nof discrete power-law distributions. \n\n"}
{"id": "1209.2072", "contents": "Title: On the impossibility of constructing good population mean estimators in\n  a realistic Respondent Driven Sampling model Abstract: Current methods for population mean estimation from data collected by\nRespondent Driven Sampling (RDS) are based on the Horvitz-Thompson estimator\ntogether with a set of assumptions on the sampling model under which the\ninclusion probabilities can be determined from the information contained in the\ndata. In this paper, we argue that such set of assumptions are too simplistic\nto be realistic and that under realistic sampling models, the situation is far\nmore complicated. Specifically, we study a realistic RDS sampling model that is\nmotivated by a real world RDS dataset. We show that, for this model, the\ninclusion probabilities, which are necessary for the application of the\nHorvitz-Thompson estimator, can not be determined by the information in the\nsample alone. An implication is that, unless additional information about the\nunderlying population network is obtained, it is hopeless to conceive of a\ngeneral theory of population mean estimation from current RDS data. \n\n"}
{"id": "1209.3522", "contents": "Title: Evidence for Conservatism in LHC SUSY Searches Abstract: The standard in the high energy physics community for claiming discovery of\nnew physics is a $5\\sigma$ excess in the observed signal over the estimated\nbackground. While a $3\\sigma$ excess is not enough to claim discovery, it is\ncertainly enough to pique the interest of both experimentalists and theorists.\nHowever, with a large number of searches performed by both the ATLAS and CMS\ncollaborations at the LHC, one expects a nonzero number of multi-$\\sigma$\nresults simply due to statistical fluctuations in the no-signal scenario. Our\nanalysis examines the distribution of p-values for CMS and ATLAS supersymmetry\n(SUSY) searches using the full 2011 data set to determine if the collaborations\nare being overly conservative in their analyses. We find that there is a\nstatistically significant excess of `medium' $\\sigma$ values at the level of\n$p=0.005$, indicating over-conservativism in the estimation of uncertainties. \n\n"}
{"id": "1209.3775", "contents": "Title: Using Machine Learning for Discovery in Synoptic Survey Imaging Abstract: Modern time-domain surveys continuously monitor large swaths of the sky to\nlook for astronomical variability. Astrophysical discovery in such data sets is\ncomplicated by the fact that detections of real transient and variable sources\nare highly outnumbered by bogus detections caused by imperfect subtractions,\natmospheric effects and detector artefacts. In this work we present a machine\nlearning (ML) framework for discovery of variability in time-domain imaging\nsurveys. Our ML methods provide probabilistic statements, in near real time,\nabout the degree to which each newly observed source is astrophysically\nrelevant source of variable brightness. We provide details about each of the\nanalysis steps involved, including compilation of the training and testing\nsets, construction of descriptive image-based and contextual features, and\noptimization of the feature subset and model tuning parameters. Using a\nvalidation set of nearly 30,000 objects from the Palomar Transient Factory, we\ndemonstrate a missed detection rate of at most 7.7% at our chosen\nfalse-positive rate of 1% for an optimized ML classifier of 23 features,\nselected to avoid feature correlation and over-fitting from an initial library\nof 42 attributes. Importantly, we show that our classification methodology is\ninsensitive to mis-labelled training data up to a contamination of nearly 10%,\nmaking it easier to compile sufficient training sets for accurate performance\nin future surveys. This ML framework, if so adopted, should enable the\nmaximization of scientific gain from future synoptic survey and enable fast\nfollow-up decisions on the vast amounts of streaming data produced by such\nexperiments. \n\n"}
{"id": "1209.5026", "contents": "Title: Estimating Player Contribution in Hockey with Regularized Logistic\n  Regression Abstract: We present a regularized logistic regression model for evaluating player\ncontributions in hockey. The traditional metric for this purpose is the\nplus-minus statistic, which allocates a single unit of credit (for or against)\nto each player on the ice for a goal. However, plus-minus scores measure only\nthe marginal effect of players, do not account for sample size, and provide a\nvery noisy estimate of performance. We investigate a related regression\nproblem: what does each player on the ice contribute, beyond aggregate team\nperformance and other factors, to the odds that a given goal was scored by\ntheir team? Due to the large-p (number of players) and imbalanced design\nsetting of hockey analysis, a major part of our contribution is a careful\ntreatment of prior shrinkage in model estimation. We showcase two recently\ndeveloped techniques -- for posterior maximization or simulation -- that make\nsuch analysis feasible. Each approach is accompanied with publicly available\nsoftware and we include the simple commands used in our analysis. Our results\nshow that most players do not stand out as measurably strong (positive or\nnegative) contributors. This allows the stars to really shine, reveals diamonds\nin the rough overlooked by earlier analyses, and argues that some of the\nhighest paid players in the league are not making contributions worth their\nexpense. \n\n"}
{"id": "1209.6463", "contents": "Title: Clustering and Classification via Cluster-Weighted Factor Analyzers Abstract: In model-based clustering and classification, the cluster-weighted model\nconstitutes a convenient approach when the random vector of interest\nconstitutes a response variable Y and a set p of explanatory variables X.\nHowever, its applicability may be limited when p is high. To overcome this\nproblem, this paper assumes a latent factor structure for X in each mixture\ncomponent. This leads to the cluster-weighted factor analyzers (CWFA) model. By\nimposing constraints on the variance of Y and the covariance matrix of X, a\nnovel family of sixteen CWFA models is introduced for model-based clustering\nand classification. The alternating expectation-conditional maximization\nalgorithm, for maximum likelihood estimation of the parameters of all the\nmodels in the family, is described; to initialize the algorithm, a 5-step\nhierarchical procedure is proposed, which uses the nested structures of the\nmodels within the family and thus guarantees the natural ranking among the\nsixteen likelihoods. Artificial and real data show that these models have very\ngood clustering and classification performance and that the algorithm is able\nto recover the parameters very well. \n\n"}
{"id": "1210.3555", "contents": "Title: Task-Based Core-Periphery Organisation of Human Brain Dynamics Abstract: As a person learns a new skill, distinct synapses, brain regions, and\ncircuits are engaged and change over time. In this paper, we develop methods to\nexamine patterns of correlated activity across a large set of brain regions.\nOur goal is to identify properties that enable robust learning of a motor\nskill. We measure brain activity during motor sequencing and characterize\nnetwork properties based on coherent activity between brain regions. Using\nrecently developed algorithms to detect time-evolving communities, we find that\nthe complex reconfiguration patterns of the brain's putative functional modules\nthat control learning can be described parsimoniously by the combined presence\nof a relatively stiff temporal core that is composed primarily of sensorimotor\nand visual regions whose connectivity changes little in time and a flexible\ntemporal periphery that is composed primarily of multimodal association regions\nwhose connectivity changes frequently. The separation between temporal core and\nperiphery changes over the course of training and, importantly, is a good\npredictor of individual differences in learning success. The core of\ndynamically stiff regions exhibits dense connectivity, which is consistent with\nnotions of core-periphery organization established previously in social\nnetworks. Our results demonstrate that core-periphery organization provides an\ninsightful way to understand how putative functional modules are linked. This,\nin turn, enables the prediction of fundamental human capacities, including the\nproduction of complex goal-directed behavior. \n\n"}
{"id": "1210.5022", "contents": "Title: Does coherence enhance transport in photosynthesis? Abstract: Recent observations of coherence in photosynthetic complexes have led to the\nquestion of whether quantum effects can occur in vivo, not under femtosecond\nlaser pulses but in incoherent sunlight and at steady state, and, if so,\nwhether the coherence explains the high exciton transfer efficiency. We\ndistinguish several types of coherence and show that although some\nphotosynthetic pathways are partially coherent processes, photosynthesis in\nnature proceeds through stationary states. This distinction allows us to rule\nout several mechanisms of transport enhancement in sunlight. In particular,\nalthough they are crucial for understanding exciton transport, neither wavelike\nmotion nor microscopic coherence, on their own, enhance the efficiency. By\ncontrast, two partially coherent mechanisms---ENAQT and supertransfer---can\nenhance transport even in sunlight and thus constitute motifs for the\noptimisation of artificial sunlight harvesting. Finally, we clarify the\nimportance of ultrafast spectroscopy in understanding incoherent processes. \n\n"}
{"id": "1211.0313", "contents": "Title: Multiple Antenna Cyclostationary Spectrum Sensing Based on the Cyclic\n  Correlation Significance Test Abstract: In this paper, we propose and analyze a spectrum sensing method based on\ncyclostationarity specifically targeted for receivers with multiple antennas.\nThis detection method is used for determining the presence or absence of\nprimary users in cognitive radio networks based on the eigenvalues of the\ncyclic covariance matrix of received signals. In particular, the cyclic\ncorrelation significance test is used to detect a specific signal-of-interest\nby exploiting knowledge of its cyclic frequencies. Analytical expressions for\nthe probability of detection and probability of false-alarm under both\nspatially uncorrelated or spatially correlated noise are derived and verified\nby simulation. The detection performance in a Rayleigh flat-fading environment\nis found and verified through simulations. One of the advantages of the\nproposed method is that the detection threshold is shown to be independent of\nboth the number of samples and the noise covariance, effectively eliminating\nthe dependence on accurate noise estimation. The proposed method is also shown\nto provide higher detection probability and better robustness to noise\nuncertainty than existing multiple-antenna cyclostationary-based spectrum\nsensing algorithms under both AWGN as well as a quasi-static Rayleigh fading\nchannel. \n\n"}
{"id": "1211.0757", "contents": "Title: Efficient Point-to-Subspace Query in $\\ell^1$: Theory and Applications\n  in Computer Vision Abstract: Motivated by vision tasks such as robust face and object recognition, we\nconsider the following general problem: given a collection of low-dimensional\nlinear subspaces in a high-dimensional ambient (image) space and a query point\n(image), efficiently determine the nearest subspace to the query in $\\ell^1$\ndistance. We show in theory that Cauchy random embedding of the objects into\nsignificantly-lower-dimensional spaces helps preserve the identity of the\nnearest subspace with constant probability. This offers the possibility of\nefficiently selecting several candidates for accurate search. We sketch\npreliminary experiments on robust face and digit recognition to corroborate our\ntheory. \n\n"}
{"id": "1211.3210", "contents": "Title: Fast estimation of the ICL criterion for change-point detection problems\n  with applications to Next-Generation Sequencing data Abstract: In this paper, we consider the Integrated Completed Likelihood (ICL) as a\nuseful criterion for estimating the number of changes in the underlying\ndistribution of data in problems where detecting the precise location of these\nchanges is the main goal. The exact computation of the ICL requires O(Kn2)\noperations (with K the number of segments and n the number of data-points)\nwhich is prohibitive in many practical situations with large sequences of data.\nWe describe a framework to estimate the ICL with O(Kn) complexity. Our approach\nis general in the sense that it can accommodate any given model distribution.\nWe checked the run-time and validity of our approach on simulated data and\ndemonstrate its good performance when analyzing real Next-Generation Sequencing\n(NGS) data using a negative binomial model. \n\n"}
{"id": "1211.3706", "contents": "Title: Bayesian Sparse Factor Analysis of Genetic Covariance Matrices Abstract: Quantitative genetic studies that model complex, multivariate phenotypes are\nimportant for both evolutionary prediction and artificial selection. For\nexample, changes in gene expression can provide insight into developmental and\nphysiological mechanisms that link genotype and phenotype. However, classical\nanalytical techniques are poorly suited to quantitative genetic studies of gene\nexpression where the number of traits assayed per individual can reach many\nthousand. Here, we derive a Bayesian genetic sparse factor model for estimating\nthe genetic covariance matrix (G-matrix) of high-dimensional traits, such as\ngene expression, in a mixed effects model. The key idea of our model is that we\nneed only consider G-matrices that are biologically plausible. An organism's\nentire phenotype is the result of processes that are modular and have limited\ncomplexity. This implies that the G-matrix will be highly structured. In\nparticular, we assume that a limited number of intermediate traits (or factors,\ne.g., variations in development or physiology) control the variation in the\nhigh-dimensional phenotype, and that each of these intermediate traits is\nsparse -- affecting only a few observed traits. The advantages of this approach\nare two-fold. First, sparse factors are interpretable and provide biological\ninsight into mechanisms underlying the genetic architecture. Second, enforcing\nsparsity helps prevent sampling errors from swamping out the true signal in\nhigh-dimensional data. We demonstrate the advantages of our model on simulated\ndata and in an analysis of a published Drosophila melanogaster gene expression\ndata set. \n\n"}
{"id": "1211.3967", "contents": "Title: Quick overview of inference methods in PLoM: combining fast and exact\n  plug-and-play algorithms to achieve flexibility, precision and efficiency Abstract: Overview of inference methods in PLoM. \n\n"}
{"id": "1211.6988", "contents": "Title: Simultaneous Distributed Sensor Self-Localization and Target Tracking\n  Using Belief Propagation and Likelihood Consensus Abstract: We introduce the framework of cooperative simultaneous localization and\ntracking (CoSLAT), which provides a consistent combination of cooperative\nself-localization (CSL) and distributed target tracking (DTT) in sensor\nnetworks without a fusion center. CoSLAT extends simultaneous localization and\ntracking (SLAT) in that it uses also intersensor measurements. Starting from a\nfactor graph formulation of the CoSLAT problem, we develop a particle-based,\ndistributed message passing algorithm for CoSLAT that combines nonparametric\nbelief propagation with the likelihood consensus scheme. The proposed CoSLAT\nalgorithm improves on state-of-the-art CSL and DTT algorithms by exchanging\nprobabilistic information between CSL and DTT. Simulation results demonstrate\nsubstantial improvements in both self-localization and tracking performance. \n\n"}
{"id": "1212.0181", "contents": "Title: Stochastic Volatility Regression for Functional Data Dynamics Abstract: Although there are many methods for functional data analysis (FDA), little\nemphasis is put on characterizing variability among volatilities of individual\nfunctions. In particular, certain individuals exhibit erratic swings in their\ntrajectory while other individuals have more stable trajectories. There is\nevidence of such volatility heterogeneity in blood pressure trajectories during\npregnancy, for example, and reason to suspect that volatility is a biologically\nimportant feature. Most FDA models implicitly assume similar or identical\nsmoothness of the individual functions, and hence can lead to misleading\ninferences on volatility and an inadequate representation of the functions. We\npropose a novel class of FDA models characterized using hierarchical stochastic\ndifferential equations. We model the derivatives of a mean function and\ndeviation functions using Gaussian processes, while also allowing covariate\ndependence including on the volatilities of the deviation functions. Following\na Bayesian approach to inference, a Markov chain Monte Carlo algorithm is used\nfor posterior computation. The methods are tested on simulated data and applied\nto blood pressure trajectories during pregnancy. \n\n"}
{"id": "1212.1223", "contents": "Title: Throughput Analysis of Primary and Secondary Networks in a Shared IEEE\n  802.11 System Abstract: In this paper, we analyze the coexistence of a primary and a secondary\n(cognitive) network when both networks use the IEEE 802.11 based distributed\ncoordination function for medium access control. Specifically, we consider the\nproblem of channel capture by a secondary network that uses spectrum sensing to\ndetermine the availability of the channel, and its impact on the primary\nthroughput. We integrate the notion of transmission slots in Bianchi's Markov\nmodel with the physical time slots, to derive the transmission probability of\nthe secondary network as a function of its scan duration. This is used to\nobtain analytical expressions for the throughput achievable by the primary and\nsecondary networks. Our analysis considers both saturated and unsaturated\nnetworks. By performing a numerical search, the secondary network parameters\nare selected to maximize its throughput for a given level of protection of the\nprimary network throughput. The theoretical expressions are validated using\nextensive simulations carried out in the Network Simulator 2. Our results\nprovide critical insights into the performance and robustness of different\nschemes for medium access by the secondary network. In particular, we find that\nthe channel captures by the secondary network does not significantly impact the\nprimary throughput, and that simply increasing the secondary contention window\nsize is only marginally inferior to silent-period based methods in terms of its\nthroughput performance. \n\n"}
{"id": "1212.1779", "contents": "Title: Evaluation of Gaussian approximations for data assimilation in reservoir\n  models Abstract: In this paper we propose to numerically assess the performance of standard\nGaussian approximations to probe the posterior distribution that arises from\nBayesian data assimilation in petroleum reservoirs. In particular we assess the\nperformance of (i) the linearization around the maximum a posterior estimate,\n(ii) the randomized maximum likelihood and (iii) standard ensemble Kalman\nfilter-type methods. In order to fully resolve the posterior distribution we\nimplement a state-of-the art MCMC method that scales well with respect to the\ndimension of the parameter space. Our implementation of the MCMC method\nprovides the gold standard against which to assess the aforementioned Gaussian\napproximations. We present numerical synthetic experiments where we quantify\nthe capability of each of the {\\em ad hoc} Gaussian approximation in\nreproducing the mean and the variance of the posterior distribution\n(characterized via MCMC) associated to a data assimilation problem. The main\nobjective of our controlled experiments is to exhibit the substantial\ndiscrepancies of the approximation properties of standard {\\em ad hoc} Gaussian\napproximations. Numerical investigations of the type we present here will lead\nto greater understanding of the cost-efficient, but {\\em ad hoc}, Bayesian\ntechniques used for data assimilation in petroleum reservoirs, and hence\nultimately to improved techniques with more accurate uncertainty\nquantification. \n\n"}
{"id": "1212.2617", "contents": "Title: Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on\n  support vector machine classification of RT-QuIC data Abstract: In this work we study numerical construction of optimal clinical diagnostic\ntests for detecting sporadic Creutzfeldt-Jakob disease (sCJD). A cerebrospinal\nfluid sample (CSF) from a suspected sCJD patient is subjected to a process\nwhich initiates the aggregation of a protein present only in cases of sCJD.\nThis aggregation is indirectly observed in real-time at regular intervals, so\nthat a longitudinal set of data is constructed that is then analysed for\nevidence of this aggregation. The best existing test is based solely on the\nfinal value of this set of data, which is compared against a threshold to\nconclude whether or not aggregation, and thus sCJD, is present. This test\ncriterion was decided upon by analysing data from a total of 108 sCJD and\nnon-sCJD samples, but this was done subjectively and there is no supporting\nmathematical analysis declaring this criterion to be exploiting the available\ndata optimally. This paper addresses this deficiency, seeking to validate or\nimprove the test primarily via support vector machine (SVM) classification.\nBesides this, we address a number of additional issues such as i) early\nstopping of the measurement process, ii) the possibility of detecting the\nparticular type of sCJD and iii) the incorporation of additional patient data\nsuch as age, sex, disease duration and timing of CSF sampling into the\nconstruction of the test. \n\n"}
{"id": "1212.3215", "contents": "Title: Simulating rare events using a Weighted Ensemble-based string method Abstract: We introduce an extension to the Weighted Ensemble (WE) path sampling method\nto restrict sampling to a one dimensional path through a high dimensional phase\nspace. Our method, which is based on the finite-temperature string method,\npermits efficient sampling of both equilibrium and non-equilibrium systems.\nSampling obtained from the WE method guides the adaptive refinement of a\nVoronoi tessellation of order parameter space, whose generating points, upon\nconvergence, coincide with the principle reaction pathway. We demonstrate the\napplication of this method to several simple, two-dimensional models of driven\nBrownian motion and to the conformational change of the nitrogen regulatory\nprotein C receiver domain using an elastic network model. The simplicity of the\ntwo-dimensional models allows us to directly compare the efficiency of the WE\nmethod to conventional brute force simulations and other path sampling\nalgorithms, while the example of protein conformational change demonstrates how\nthe method can be used to efficiently study transitions in the space of many\ncollective variables. \n\n"}
{"id": "1212.4786", "contents": "Title: A statistical framework for joint eQTL analysis in multiple tissues Abstract: Mapping expression Quantitative Trait Loci (eQTLs) represents a powerful and\nwidely-adopted approach to identifying putative regulatory variants and linking\nthem to specific genes. Up to now eQTL studies have been conducted in a\nrelatively narrow range of tissues or cell types. However, understanding the\nbiology of organismal phenotypes will involve understanding regulation in\nmultiple tissues, and ongoing studies are collecting eQTL data in dozens of\ncell types. Here we present a statistical framework for powerfully detecting\neQTLs in multiple tissues or cell types (or, more generally, multiple\nsubgroups). The framework explicitly models the potential for each eQTL to be\nactive in some tissues and inactive in others. By modeling the sharing of\nactive eQTLs among tissues this framework increases power to detect eQTLs that\nare present in more than one tissue compared with \"tissue-by-tissue\" analyses\nthat examine each tissue separately. Conversely, by modeling the inactivity of\neQTLs in some tissues, the framework allows the proportion of eQTLs shared\nacross different tissues to be formally estimated as parameters of a model,\naddressing the difficulties of accounting for incomplete power when comparing\noverlaps of eQTLs identified by tissue-by-tissue analyses. Applying our\nframework to re-analyze data from transformed B cells, T cells and fibroblasts\nwe find that it substantially increases power compared with tissue-by-tissue\nanalysis, identifying 63% more genes with eQTLs (at FDR=0.05). Further the\nresults suggest that, in contrast to previous analyses of the same data, the\nmajority of eQTLs detectable in these data are shared among all three tissues. \n\n"}
{"id": "1212.5032", "contents": "Title: Distributed Rate Allocation in Inter-Session Network Coding Abstract: In this work, we propose a distributed rate allocation algorithm that\nminimizes the average decoding delay for multimedia clients in inter-session\nnetwork coding systems. We consider a scenario where the users are organized in\na mesh network and each user requests the content of one of the available\nsources. We propose a novel distributed algorithm where network users determine\nthe coding operations and the packet rates to be requested from the parent\nnodes, such that the decoding delay is minimized for all the clients. A rate\nallocation problem is solved by every user, which seeks the rates that minimize\nthe average decoding delay for its children and for itself. Since the\noptimization problem is a priori non-convex, we introduce the concept of\nequivalent packet flows, which permits to estimate the expected number of\npackets that every user needs to collect for decoding. We then decompose our\noriginal rate allocation problem into a set of convex subproblems, which are\neventually combined to obtain an effective approximate solution to the delay\nminimization problem. The results demonstrate that the proposed scheme\neliminates the bottlenecks and reduces the decoding delay experienced by users\nwith limited bandwidth resources. We validate the performance of our\ndistributed rate allocation algorithm in different video streaming scenarios\nusing the NS-3 network simulator. We show that our system is able to take\nbenefit of inter-session network coding for simultaneous delivery of video\nsessions in networks with path diversity. \n\n"}
{"id": "1212.6416", "contents": "Title: Coherent Phase Control in Closed and Open Quantum Systems: A General\n  Master Equation Approach Abstract: The underlying mechanisms for one photon phase control are revealed through a\nmaster equation approach. Specifically, two mechanisms are identified, one\noperating on the laser time scale and the other on the time scale of the\nsystem-bath interaction. The effect of the secular and non-secular Markovian\napproximations are carefully examined. \n\n"}
{"id": "1212.6757", "contents": "Title: Testing Regression Monotonicity in Econometric Models Abstract: Monotonicity is a key qualitative prediction of a wide array of economic\nmodels derived via robust comparative statics. It is therefore important to\ndesign effective and practical econometric methods for testing this prediction\nin empirical analysis. This paper develops a general nonparametric framework\nfor testing monotonicity of a regression function. Using this framework, a\nbroad class of new tests is introduced, which gives an empirical researcher a\nlot of flexibility to incorporate ex ante information she might have. The paper\nalso develops new methods for simulating critical values, which are based on\nthe combination of a bootstrap procedure and new selection algorithms. These\nmethods yield tests that have correct asymptotic size and are asymptotically\nnonconservative. It is also shown how to obtain an adaptive rate optimal test\nthat has the best attainable rate of uniform consistency against models whose\nregression function has Lipschitz-continuous first-order derivatives and that\nautomatically adapts to the unknown smoothness of the regression function.\nSimulations show that the power of the new tests in many cases significantly\nexceeds that of some prior tests, e.g. that of Ghosal, Sen, and Van der Vaart\n(2000). An application of the developed procedures to the dataset of Ellison\nand Ellison (2011) shows that there is some evidence of strategic entry\ndeterrence in pharmaceutical industry where incumbents may use strategic\ninvestment to prevent generic entries when their patents expire. \n\n"}
{"id": "1212.6804", "contents": "Title: Geometrical effects on energy transfer in disordered open quantum\n  systems Abstract: We explore various design principles for efficient excitation energy\ntransport in complex quantum systems. We investigate energy transfer efficiency\nin randomly disordered geometries consisting of up to 20 chromophores to\nexplore spatial and spectral properties of small natural/artificial\nLight-Harvesting Complexes (LHC). We find significant statistical correlations\namong highly efficient random structures with respect to ground state\nproperties, excitonic energy gaps, multichromophoric spatial connectivity, and\npath strengths. These correlations can even exist beyond the optimal regime of\nenvironment-assisted quantum transport. For random configurations embedded in\nspatial dimensions of 30 A and 50 A, we observe that the transport efficiency\nsaturates to its maximum value if the systems contain 7 and 14 chromophores\nrespectively. Remarkably, these optimum values coincide with the number of\nchlorophylls in (Fenna-Matthews-Olson) FMO protein complex and LHC II monomers,\nrespectively, suggesting a potential natural optimization with respect to\nchromophoric density. \n\n"}
{"id": "1301.0264", "contents": "Title: Validation of Soft Classification Models using Partial Class\n  Memberships: An Extended Concept of Sensitivity & Co. applied to the Grading\n  of Astrocytoma Tissues Abstract: We use partial class memberships in soft classification to model uncertain\nlabelling and mixtures of classes. Partial class memberships are not restricted\nto predictions, but may also occur in reference labels (ground truth, gold\nstandard diagnosis) for training and validation data.\n  Classifier performance is usually expressed as fractions of the confusion\nmatrix, such as sensitivity, specificity, negative and positive predictive\nvalues. We extend this concept to soft classification and discuss the bias and\nvariance properties of the extended performance measures. Ambiguity in\nreference labels translates to differences between best-case, expected and\nworst-case performance. We show a second set of measures comparing expected and\nideal performance which is closely related to regression performance, namely\nthe root mean squared error RMSE and the mean absolute error MAE.\n  All calculations apply to classical crisp classification as well as to soft\nclassification (partial class memberships and/or one-class classifiers). The\nproposed performance measures allow to test classifiers with actual borderline\ncases. In addition, hardening of e.g. posterior probabilities into class labels\nis not necessary, avoiding the corresponding information loss and increase in\nvariance.\n  We implement the proposed performance measures in the R package\n\"softclassval\", which is available from CRAN and at\nhttp://softclassval.r-forge.r-project.org.\n  Our reasoning as well as the importance of partial memberships for\nchemometric classification is illustrated by a real-word application:\nastrocytoma brain tumor tissue grading (80 patients, 37000 spectra) for finding\nsurgical excision borders. As borderline cases are the actual target of the\nanalytical technique, samples which are diagnosed to be borderline cases must\nbe included in the validation. \n\n"}
{"id": "1301.2954", "contents": "Title: The ranking lasso and its application to sport tournaments Abstract: Ranking a vector of alternatives on the basis of a series of paired\ncomparisons is a relevant topic in many instances. A popular example is ranking\ncontestants in sport tournaments. To this purpose, paired comparison models\nsuch as the Bradley-Terry model are often used. This paper suggests fitting\npaired comparison models with a lasso-type procedure that forces contestants\nwith similar abilities to be classified into the same group. Benefits of the\nproposed method are easier interpretation of rankings and a significant\nimprovement of the quality of predictions with respect to the standard maximum\nlikelihood fitting. Numerical aspects of the proposed method are discussed in\ndetail. The methodology is illustrated through ranking of the teams of the\nNational Football League 2010-2011 and the American College Hockey Men's\nDivision I 2009-2010. \n\n"}
{"id": "1301.6491", "contents": "Title: SINR-based k-coverage probability in cellular networks with arbitrary\n  shadowing Abstract: We give numerically tractable, explicit integral expressions for the\ndistribution of the signal-to-interference-and-noise-ratio (SINR) experienced\nby a typical user in the down-link channel from the k-th strongest base\nstations of a cellular network modelled by Poisson point process on the plane.\nOur signal propagation-loss model comprises of a power-law path-loss function\nwith arbitrarily distributed shadowing, independent across all base stations,\nwith and without Rayleigh fading. Our results are valid in the whole domain of\nSINR, in particular for SINR<1, where one observes multiple coverage. In this\nlatter aspect our paper complements previous studies reported in [Dhillon et\nal. JSAC 2012]. \n\n"}
{"id": "1302.2525", "contents": "Title: Foundations of Descriptive and Inferential Statistics Abstract: These lecture notes were written with the aim to provide an accessible though\ntechnically solid introduction to the logic of systematical analyses of\nstatistical data to both undergraduate and postgraduate students, in particular\nin the Social Sciences, Economics, and the Financial Services. They may also\nserve as a general reference for the application of quantitative--empirical\nresearch methods. In an attempt to encourage the adoption of an\ninterdisciplinary perspective on quantitative problems arising in practice, the\nnotes cover the four broad topics (i) descriptive statistical processing of raw\ndata, (ii) elementary probability theory, (iii) the operationalisation of\none-dimensional latent statistical variables according to Likert's widely used\nscaling approach, and (iv) null hypothesis significance testing within the\nfrequentist approach to probability theory concerning (a) distributional\ndifferences of variables between subgroups of a target population, and (b)\nstatistical associations between two variables. The relevance of effect sizes\nfor making inferences is emphasised. These lecture notes are fully hyperlinked,\nthus providing a direct route to original scientific papers as well as to\ninteresting biographical information. They also list many commands for running\nstatistical functions and data analysis routines in the software packages R,\nSPSS, EXCEL and OpenOffice. The immediate involvement in actual data analysis\npractices is strongly recommended. \n\n"}
{"id": "1302.4404", "contents": "Title: Analysis of Forensic DNA Mixtures with Artefacts Abstract: DNA is now routinely used in criminal investigations and court cases,\nalthough DNA samples taken at crime scenes are of varying quality and therefore\npresent challenging problems for their interpretation. We present a statistical\nmodel for the quantitative peak information obtained from an electropherogram\n(EPG) of a forensic DNA sample and illustrate its potential use for the\nanalysis of criminal cases. In contrast to most previously used methods, we\ndirectly model the peak height information and incorporates important artefacts\nassociated with the production of the EPG. Our model has a number of unknown\nparameters, and we show that these can be estimated by the method of maximum\nlikelihood in the presence of multiple unknown contributors, and their\napproximate standard errors calculated; the computations exploit a Bayesian\nnetwork representation of the model. A case example from a UK trial, as\nreported in the literature, is used to illustrate the efficacy and use of the\nmodel, both in finding likelihood ratios to quantify the strength of evidence,\nand in the deconvolution of mixtures for the purpose of finding likely profiles\nof one or more unknown contributors to a DNA sample. Our model is readily\nextended to simultaneous analysis of more than one mixture as illustrated in a\ncase example. We show that combination of evidence from several samples may\ngive an evidential strength close to that of a single source trace and thus\nmodelling of peak height information provides for a potentially very efficient\nmixture analysis. \n\n"}
{"id": "1302.4793", "contents": "Title: Opportunistic Wireless Energy Harvesting in Cognitive Radio Networks Abstract: Wireless networks can be self-sustaining by harvesting energy from ambient\nradio-frequency (RF) signals. Recently, researchers have made progress on\ndesigning efficient circuits and devices for RF energy harvesting suitable for\nlow-power wireless applications. Motivated by this and building upon the\nclassic cognitive radio (CR) network model, this paper proposes a novel method\nfor wireless networks coexisting where low-power mobiles in a secondary\nnetwork, called secondary transmitters (STs), harvest ambient RF energy from\ntransmissions by nearby active transmitters in a primary network, called\nprimary transmitters (PTs), while opportunistically accessing the spectrum\nlicensed to the primary network. We consider a stochastic-geometry model in\nwhich PTs and STs are distributed as independent homogeneous Poisson point\nprocesses (HPPPs) and communicate with their intended receivers at fixed\ndistances. Each PT is associated with a guard zone to protect its intended\nreceiver from ST's interference, and at the same time delivers RF energy to STs\nlocated in its harvesting zone. Based on the proposed model, we analyze the\ntransmission probability of STs and the resulting spatial throughput of the\nsecondary network. The optimal transmission power and density of STs are\nderived for maximizing the secondary network throughput under the given\noutage-probability constraints in the two coexisting networks, which reveal key\ninsights to the optimal network design. Finally, we show that our analytical\nresult can be generally applied to a non-CR setup, where distributed wireless\npower chargers are deployed to power coexisting wireless transmitters in a\nsensor network. \n\n"}
{"id": "1302.5624", "contents": "Title: Semi-automatic selection of summary statistics for ABC model choice Abstract: A central statistical goal is to choose between alternative explanatory\nmodels of data. In many modern applications, such as population genetics, it is\nnot possible to apply standard methods based on evaluating the likelihood\nfunctions of the models, as these are numerically intractable. Approximate\nBayesian computation (ABC) is a commonly used alternative for such situations.\nABC simulates data x for many parameter values under each model, which is\ncompared to the observed data xobs. More weight is placed on models under which\nS(x) is close to S(xobs), where S maps data to a vector of summary statistics.\nPrevious work has shown the choice of S is crucial to the efficiency and\naccuracy of ABC. This paper provides a method to select good summary statistics\nfor model choice. It uses a preliminary step, simulating many x values from all\nmodels and fitting regressions to this with the model as response. The\nresulting model weight estimators are used as S in an ABC analysis. Theoretical\nresults are given to justify this as approximating low dimensional sufficient\nstatistics. A substantive application is presented: choosing between competing\ncoalescent models of demographic growth for Campylobacter jejuni in New Zealand\nusing multi-locus sequence typing data. \n\n"}
{"id": "1302.5945", "contents": "Title: Queue-Based Random-Access Algorithms: Fluid Limits and Stability Issues Abstract: We use fluid limits to explore the (in)stability properties of wireless\nnetworks with queue-based random-access algorithms. Queue-based random-access\nschemes are simple and inherently distributed in nature, yet provide the\ncapability to match the optimal throughput performance of centralized\nscheduling mechanisms in a wide range of scenarios. Unfortunately, the type of\nactivation rules for which throughput optimality has been established, may\nresult in excessive queue lengths and delays. The use of more\naggressive/persistent access schemes can improve the delay performance, but\ndoes not offer any universal maximum-stability guarantees. In order to gain\nqualitative insight and investigate the (in)stability properties of more\naggressive/persistent activation rules, we examine fluid limits where the\ndynamics are scaled in space and time. In some situations, the fluid limits\nhave smooth deterministic features and maximum stability is maintained, while\nin other scenarios they exhibit random oscillatory characteristics, giving rise\nto major technical challenges. In the latter regime, more aggressive access\nschemes continue to provide maximum stability in some networks, but may cause\ninstability in others. Simulation experiments are conducted to illustrate and\nvalidate the analytical results. \n\n"}
{"id": "1302.7088", "contents": "Title: Continuous-time Infinite Dynamic Topic Models Abstract: Topic models are probabilistic models for discovering topical themes in\ncollections of documents. In real world applications, these models provide us\nwith the means of organizing what would otherwise be unstructured collections.\nThey can help us cluster a huge collection into different topics or find a\nsubset of the collection that resembles the topical theme found in an article\nat hand.\n  The first wave of topic models developed were able to discover the prevailing\ntopics in a big collection of documents spanning a period of time. It was later\nrealized that these time-invariant models were not capable of modeling 1) the\ntime varying number of topics they discover and 2) the time changing structure\nof these topics. Few models were developed to address this two deficiencies.\nThe online-hierarchical Dirichlet process models the documents with a time\nvarying number of topics. It varies the structure of the topics over time as\nwell. However, it relies on document order, not timestamps to evolve the model\nover time. The continuous-time dynamic topic model evolves topic structure in\ncontinuous-time. However, it uses a fixed number of topics over time.\n  In this dissertation, I present a model, the continuous-time infinite dynamic\ntopic model, that combines the advantages of these two models 1) the\nonline-hierarchical Dirichlet process, and 2) the continuous-time dynamic topic\nmodel. More specifically, the model I present is a probabilistic topic model\nthat does the following: 1) it changes the number of topics over continuous\ntime, and 2) it changes the topic structure over continuous-time.\n  I compared the model I developed with the two other models with different\nsetting values. The results obtained were favorable to my model and showed the\nneed for having a model that has a continuous-time varying number of topics and\ntopic structure. \n\n"}
{"id": "1303.2074", "contents": "Title: An analytical continuation approach for evaluating emission lineshapes\n  of molecular aggregates and the adequacy of multichromophoric F\\\"orster\n  theory Abstract: In large photosynthetic chromophore-protein complexes not all chromophores\nare coupled strongly, and thus the situation is well described by formation of\ndelocalized states in certain domains of strongly coupled chromophores. In\norder to describe excitation energy transfer among different domains without\nperforming extensive numerical calculations,one of the most popular techniques\nis a generalization of Forster theory to multichromophoric aggregates\n(generalized Forster theory) proposed by Sumi [J.Phys.Chem.B,103,252(1999)] and\nScholes and Fleming [J.Phys.Chem.B 104,1854(2000)]. The aim of this paper is\ntwofold. In the first place, by means of analytic continuation and a time\nconvolutionless quantum master equation approach, a theory of emission\nlineshape of multichromophoric systems or molecular aggregates is proposed. In\nthe second place,a comprehensive framework that allows for a clear,compact and\neffective study of the multichromophoric approach in the full general version\nproposed by Jang, Newton and Silbey [Phys. Rev. Lett.,92,218301,(2004)] is\ndeveloped. We apply the present theory to simple paradigmatic systems and we\nshow: the effectiveness of time-convolutionless techniques in deriving\nlineshape operators; how the multichromophoric approach can give significant\nimprovements in the determination of energy transfer rates in particular when\nthe systems under study are not the purely Forster regime. The presented scheme\nallows for an effective implementation of the multichromophoric Forster\napproach which may be of use for simulating energy transfer dynamics in large\nphotosynthetic aggregates, for which massive computational resources are\nusually required. Furthermore,our method allows for a systematic comparison of\nmultichromophoric Foster and generalized Forster theories and for a clear\nunderstanding of their respective limits of validity. \n\n"}
{"id": "1303.3823", "contents": "Title: Purely helical absolute equilibria and chirality of (magneto)fluid\n  turbulence Abstract: Left- and right-handed helical modes' statistical absolute equilibria appear\n\\textit{separately}. If both chiral sectors present, one can dominate around\nits positive pole, which is relevant to the nearly maximally helical (force\nfree for magnetic field) states of turbulence. Pure magnetodynamics (PMD, or\nelectron magnetohydrodynamics --- EMHD), pure hydrodynamics (PHD), and,\nsingle-fluid and two-fluid MHDs are studied. Relevant documented data and\nissues of cascade properties, and, helical and nonhelical dynamos are\nrevisited. We also discuss new scenarios, such as PMD inverse magnetic helicity\nand forward energy cascades, and, the continuous transition from\ncompletely-inverse to partly-inverse-and-partly-forward and to\ncompletely-forward energy transfers in PHD. \n\n"}
{"id": "1303.5195", "contents": "Title: Adding a systematic uncertainty to the signal estimation in the\n  on/off-zone measurements Abstract: The measurements with the background estimation from an off-zone are widely\nused in astrophysics, accelerator physics and other areas. Usually, the\nexpected number of the background events in the off-zone and in the on-zone is\nknown with a limited precision. This fact should be included as a systematic\nuncertainty. In this note an overview of the statistical methods which estimate\nthe range and the significance of the measured signal is done. The method which\nincludes a systematic uncertainty is developed for the on/off-zone measurements\nand compared with other existing methods. \n\n"}
{"id": "1303.5986", "contents": "Title: Efficient Stochastic Simulation of Chemical Kinetics Networks using a\n  Weighted Ensemble of Trajectories Abstract: We apply the \"weighted ensemble\" (WE) simulation strategy, previously\nemployed in the context of molecular dynamics simulations, to a series of\nsystems-biology models that range in complexity from one-dimensional to a\nsystem with 354 species and 3680 reactions. WE is relatively easy to implement,\ndoes not require extensive hand-tuning of parameters, does not depend on the\ndetails of the simulation algorithm, and can facilitate the simulation of\nextremely rare events.\n  For the coupled stochastic reaction systems we study, WE is able to produce\naccurate and efficient approximations of the joint probability distribution for\nall chemical species for all time t. WE is also able to efficiently extract\nmean first passage times for the systems, via the construction of a\nsteady-state condition with feedback. In all cases studied here, WE results\nagree with independent calculations, but significantly enhance the precision\nwith which rare or slow processes can be characterized. Speedups over\n\"brute-force\" in sampling rare events via the Gillespie direct Stochastic\nSimulation Algorithm range from ~10^12 to ~10^20 for rare states in a\ndistribution, and ~10^2 to ~10^4 for finding mean first passage times. \n\n"}
{"id": "1303.6597", "contents": "Title: Direct Numerical Test of the Statistical Mechanical Theory of\n  Hydrophobic Interactions Abstract: This work tests the statistical mechanical theory of hydrophobic\ninteractions, isolates consequences of excluded volume interactions, and\nobtains B2 for those purposes. Cavity methods that are particularly appropriate\nfor study of hydrophobic interactions between atomic-size hard spheres in\nliquid water are developed and applied to test aspects of the Pratt-Chandler\n(PC) theory that have not been tested. Contact hydrophobic interactions between\nAr-size hard-spheres in water are significantly more attractive than predicted\nby the PC theory. The corresponding results for the osmotic second virial\ncoefficient are attractive (B2 <0), and more attractive with increasing\ntemperature (Delta B2/Delta T < 0) in the temperature range 300K < T < 360K.\nThis information has not been available previously, but is essential for\ndevelopment of the molecular-scale statistical mechanical theory of hydrophobic\ninteractions, particularly for better definition of the role of attractive\nintermolecular interactions associated with the solutes. \n\n"}
{"id": "1303.6765", "contents": "Title: Zero Intelligence Models of the Continuous Double Auction: Econometrics,\n  Empirical Evidence and Generalization Abstract: In the paper, a statistical procedure for estimating the parameters of zero\nintelligence models by means of tick-by-tick quote (L1) data is proposed. A\nlarge class of existing zero intelligence models is reviewed. It is shown that\nall those models fail to describe the actual behavior of limit order books\nclose to the ask price. A generalized model, accommodating the discrepancies\nfound, is proposed and shown to give significant results for L1 data from three\nUS electronic markets. It is also demonstrated that the generalized model\npreforms significantly better than the reviewed models. \n\n"}
{"id": "1303.6784", "contents": "Title: Measuring the likelihood of models for network evolution Abstract: Many researchers have hypothesised models which explain the evolution of the\ntopology of a target network. The framework described in this paper gives the\nlikelihood that the target network arose from the hypothesised model. This\nallows rival hypothesised models to be compared for their ability to explain\nthe target network. A null model (of random evolution) is proposed as a\nbaseline for comparison. The framework also considers models made from linear\ncombinations of model components. A method is given for the automatic\noptimisation of component weights. The framework is tested on simulated\nnetworks with known parameters and also on real data. \n\n"}
{"id": "1303.6992", "contents": "Title: Parameter tuning for a multi-fidelity dynamical model of the\n  magnetosphere Abstract: Geomagnetic storms play a critical role in space weather physics with the\npotential for far reaching economic impacts including power grid outages, air\ntraffic rerouting, satellite damage and GPS disruption. The LFM-MIX is a\nstate-of-the-art coupled magnetospheric-ionospheric model capable of simulating\ngeomagnetic storms. Imbedded in this model are physical equations for turning\nthe magnetohydrodynamic state parameters into energy and flux of electrons\nentering the ionosphere, involving a set of input parameters. The exact values\nof these input parameters in the model are unknown, and we seek to quantify the\nuncertainty about these parameters when model output is compared to\nobservations. The model is available at different fidelities: a lower fidelity\nwhich is faster to run, and a higher fidelity but more computationally intense\nversion. Model output and observational data are large spatiotemporal systems;\nthe traditional design and analysis of computer experiments is unable to cope\nwith such large data sets that involve multiple fidelities of model output. We\ndevelop an approach to this inverse problem for large spatiotemporal data sets\nthat incorporates two different versions of the physical model. After an\ninitial design, we propose a sequential design based on expected improvement.\nFor the LFM-MIX, the additional run suggested by expected improvement\ndiminishes posterior uncertainty by ruling out a posterior mode and shrinking\nthe width of the posterior distribution. We also illustrate our approach using\nthe Lorenz `96 system of equations for a simplified atmosphere, using known\ninput parameters. For the Lorenz `96 system, after performing sequential runs\nbased on expected improvement, the posterior mode converges to the true value\nand the posterior variability is reduced. \n\n"}
{"id": "1303.7050", "contents": "Title: Quantile Models with Endogeneity Abstract: In this article, we review quantile models with endogeneity. We focus on\nmodels that achieve identification through the use of instrumental variables\nand discuss conditions under which partial and point identification are\nobtained. We discuss key conditions, which include monotonicity and\nfull-rank-type conditions, in detail. In providing this review, we update the\nidentification results of Chernozhukov and Hansen (2005, Econometrica). We\nillustrate the modeling assumptions through economically motivated examples. We\nalso briefly review the literature on estimation and inference.\n  Key Words: identification, treatment effects, structural models, instrumental\nvariables \n\n"}
{"id": "1304.1936", "contents": "Title: A van der Waals density functional mapping of attraction in DNA dimers Abstract: The dispersion interaction between a pair of parallel DNA double-helix\nstructures is investigated by means of the van der Waals density functional\n(vdW-DF) method. Each double-helix structure consists of an infinite repetition\nof one B-DNA coil with 10 base pairs. This parameter-free density functional\ntheory (DFT) study illustrates the initial step in a proposed vdW-DF\ncomputational strategy for large biomolecular problems. The strategy is to\nfirst perform a survey of interaction geometries, based on the evaluation of\nthe van der Waals (vdW) attraction, and then limit the evaluation of the\nremaining DFT parts (specifically the expensive study of the kinetic-energy\nrepulsion) to the thus identified interesting geometries. Possibilities for\naccelerating this second step is detailed in a separate study. For the B-DNA\ndimer, the variation in van der Waals attraction is explored at relatively\nshort distances (although beyond the region of density overlap) for a 360\ndegrees rotation. This study highlights the role of the structural motifs, like\nthe grooves, in enhancing or reducing the vdW interaction strength. We find\nthat to a first approximation, it is possible to compare the DNA double strand\nat large wall-to-wall separations to the cylindrical shape of a carbon nanotube\n(which is almost isotropic under rotation). We compare our first-principles\nresults with the atom-based dispersive interaction predicted by DFT-D2 [J.\nComp. Chem. 27, 1787 (2006)] and find agreement in the asymptotic region.\nHowever, we also find that the differences in the enhancement that occur at\nshorter distances reveal characteristic features that result from the fact that\nthe vdW-DF method is an electron-based (as opposed to atom-based) description. \n\n"}
{"id": "1304.4143", "contents": "Title: Chemical compass model for avian magnetoreception as a quantum coherent\n  device Abstract: It is known that more than 50 species use the Earth's magnetic field for\norientation and navigation. Intensive studies particularly behavior experiments\nwith birds, provide support for a chemical compass based on magnetically\nsensitive free radical reactions as a source of this sense. However, the\nfundamental question of how quantum coherence plays an essential role in such a\nchemical compass model of avian magnetoreception yet remains controversial.\nHere, we show that the essence of the chemical compass model can be understood\nin analogy to a quantum interferometer exploiting global quantum coherence\nrather than any subsystem coherence. Within the framework of quantum metrology,\nwe quantify global quantum coherence and correlate it with the function of\nchemical magnetoreception. Our results allow us to understand and predict how\nvarious factors can affect the performance of a chemical compass from the\nunique perspective of quantum coherence assisted metrology. This represents a\ncrucial step to affirm a direct connection between quantum coherence and the\nfunction of a chemical compass. \n\n"}
{"id": "1304.4929", "contents": "Title: A new method to obtain risk neutral probability, without stochastic\n  calculus and price modeling, confirms the universal validity of\n  Black-Scholes-Merton formula and volatility's role Abstract: A new method is proposed to obtain the risk neutral probability of share\nprices without stochastic calculus and price modeling, via an embedding of the\nprice return modeling problem in Le Cam's statistical experiments framework.\nStrategies-probabilities $P_{t_0,n}$ and $P_{T,n}$ are thus determined and\nused, respectively,for the trader selling the share's European call option at\ntime $t_0$ and for the buyer who may exercise it in the future, at $T; \\ n$\nincreases with the number of share's transactions in $[t_0,T].$ When the\ntransaction times are dense in $[t_0,T]$ it is shown, with mild conditions,\nthat under each of these probabilities $\\log \\frac{S_T}{S_{t_0}}$ has\ninfinitely divisible distribution and in particular normal distribution for\n\"calm\" share; $S_t$ is the share's price at time $t.$ The price of the share's\ncall is the limit of the expected values of the call's payoff under the\ntranslated $P_{t_0,n}.$ It coincides for \"calm\" share prices with the\nBlack-Scholes-Merton formula with variance not necessarily proportional to\n$(T-t_0),$ thus confirming formula's universal validity without model\nassumptions. Additional results clarify volatility's role in the transaction\nand the behaviors of the trader and the buyer. Traders may use the pricing\nformulae after estimation of the unknown parameters. \n\n"}
{"id": "1304.5642", "contents": "Title: Spatio-Temporal Low Count Processes with Application to Violent Crime\n  Events Abstract: There is significant interest in being able to predict where crimes will\nhappen, for example to aid in the efficient tasking of police and other\nprotective measures. We aim to model both the temporal and spatial dependencies\noften exhibited by violent crimes in order to make such predictions. The\ntemporal variation of crimes typically follows patterns familiar in time series\nanalysis, but the spatial patterns are irregular and do not vary smoothly\nacross the area. Instead we find that spatially disjoint regions exhibit\ncorrelated crime patterns. It is this indeterminate inter-region correlation\nstructure along with the low-count, discrete nature of counts of serious crimes\nthat motivates our proposed forecasting tool. In particular, we propose to\nmodel the crime counts in each region using an integer-valued first order\nautoregressive process. We take a Bayesian nonparametric approach to flexibly\ndiscover a clustering of these region-specific time series. We then describe\nhow to account for covariates within this framework. Both approaches adjust for\nseasonality. We demonstrate our approach through an analysis of weekly reported\nviolent crimes in Washington, D.C. between 2001-2008. Our forecasts outperform\nstandard methods while additionally providing useful tools such as prediction\nintervals. \n\n"}
{"id": "1304.6777", "contents": "Title: A Bayesian approach for predicting the popularity of tweets Abstract: We predict the popularity of short messages called tweets created in the\nmicro-blogging site known as Twitter. We measure the popularity of a tweet by\nthe time-series path of its retweets, which is when people forward the tweet to\nothers. We develop a probabilistic model for the evolution of the retweets\nusing a Bayesian approach, and form predictions using only observations on the\nretweet times and the local network or \"graph\" structure of the retweeters. We\nobtain good step ahead forecasts and predictions of the final total number of\nretweets even when only a small fraction (i.e., less than one tenth) of the\nretweet path is observed. This translates to good predictions within a few\nminutes of a tweet being posted, and has potential implications for\nunderstanding the spread of broader ideas, memes, or trends in social networks. \n\n"}
{"id": "1304.8083", "contents": "Title: Adaptive Video Streaming for Wireless Networks with Multiple Users and\n  Helpers Abstract: We consider the optimal design of a scheduling policy for adaptive video\nstreaming in a wireless network formed by several users and helpers. A feature\nof such networks is that any user is typically in the range of multiple\nhelpers. Hence, in order to cope with user-helper association, load balancing\nand inter-cell interference, an efficient streaming policy should allow the\nusers to dynamically select the helper node to download from, and determine\nadaptively the video quality level of the download. In order to obtain a\ntractable formulation, we follow a \"divide and conquer\" approach: i) Assuming\nthat each video packet (chunk) is delivered within its playback delay (\"smooth\nstreaming regime\"), the problem is formulated as a network utility maximization\n(NUM), subject to queue stability, where the network utility function is a\nconcave and componentwise non-decreasing function of the users' video quality\nmeasure. ii) We solve the NUM problem by using a Lyapunov Drift Plus Penalty\napproach, obtaining a scheme that naturally decomposes into two sub-policies\nreferred to as \"congestion control\" (adaptive video quality and helper station\nselection) and \"transmission scheduling\" (dynamic allocation of the helper-user\nphysical layer transmission rates).Our solution is provably optimal with\nrespect to the proposed NUM problem, in a strong per-sample path sense. iii)\nFinally, we propose a method to adaptively estimate the maximum queuing delays,\nsuch that each user can calculate its pre-buffering and re-buffering time in\norder to cope with the fluctuations of the queuing delays. Through simulations,\nwe evaluate the performance of the proposed algorithm under realistic\nassumptions of a network with densely deployed helper nodes, and demonstrate\nthe per-sample path optimality of the proposed solution by considering a\nnon-stationary non-ergodic scenario with user mobility, VBR video coding. \n\n"}
{"id": "1305.0060", "contents": "Title: Complexity penalized hydraulic fracture localization and moment tensor\n  estimation under limited model information Abstract: In this paper we present a novel technique for micro-seismic localization\nusing a group sparse penalization that is robust to the focal mechanism of the\nsource and requires only a velocity model of the stratigraphy rather than a\nfull Green's function model of the earth's response. In this technique we\nconstruct a set of perfect delta detector responses, one for each detector in\nthe array, to a seismic event at a given location and impose a group sparsity\nacross the array. This scheme is independent of the moment tensor and exploits\nthe time compactness of the incident seismic signal. Furthermore we present a\nmethod for improving the inversion of the moment tensor and Green's function\nwhen the geometry of seismic array is limited. In particular we demonstrate\nthat both Tikhonov regularization and truncated SVD can improve the recovery of\nthe moment tensor and be robust to noise. We evaluate our algorithm on\nsynthetic data and present error bounds for both estimation of the moment\ntensor as well as localization. Furthermore we discuss the estimated moment\ntensor accuracy as a function of both array geometry and fault orientation. \n\n"}
{"id": "1305.2026", "contents": "Title: Comparison of nonhomogeneous regression models for probabilistic wind\n  speed forecasting Abstract: In weather forecasting, nonhomogeneous regression is used to statistically\npostprocess forecast ensembles in order to obtain calibrated predictive\ndistributions. For wind speed forecasts, the regression model is given by a\ntruncated normal distribution where location and spread are derived from the\nensemble. This paper proposes two alternative approaches which utilize the\ngeneralized extreme value (GEV) distribution. A direct alternative to the\ntruncated normal regression is to apply a predictive distribution from the GEV\nfamily, while a regime switching approach based on the median of the forecast\nensemble incorporates both distributions. In a case study on daily maximum wind\nspeed over Germany with the forecast ensemble from the European Centre for\nMedium-Range Weather Forecasts, all three approaches provide calibrated and\nsharp predictive distributions with the regime switching approach showing the\nhighest skill in the upper tail. \n\n"}
{"id": "1305.5355", "contents": "Title: Variable selection for sparse Dirichlet-multinomial regression with an\n  application to microbiome data analysis Abstract: With the development of next generation sequencing technology, researchers\nhave now been able to study the microbiome composition using direct sequencing,\nwhose output are bacterial taxa counts for each microbiome sample. One goal of\nmicrobiome study is to associate the microbiome composition with environmental\ncovariates. We propose to model the taxa counts using a Dirichlet-multinomial\n(DM) regression model in order to account for overdispersion of observed\ncounts. The DM regression model can be used for testing the association between\ntaxa composition and covariates using the likelihood ratio test. However, when\nthe number of covariates is large, multiple testing can lead to loss of power.\nTo address the high dimensionality of the problem, we develop a penalized\nlikelihood approach to estimate the regression parameters and to select the\nvariables by imposing a sparse group $\\ell_1$ penalty to encourage both\ngroup-level and within-group sparsity. Such a variable selection procedure can\nlead to selection of the relevant covariates and their associated bacterial\ntaxa. An efficient block-coordinate descent algorithm is developed to solve the\noptimization problem. We present extensive simulations to demonstrate that the\nsparse DM regression can result in better identification of the\nmicrobiome-associated covariates than models that ignore overdispersion or only\nconsider the proportions. We demonstrate the power of our method in an analysis\nof a data set evaluating the effects of nutrient intake on human gut microbiome\ncomposition. Our results have clearly shown that the nutrient intake is\nstrongly associated with the human gut microbiome. \n\n"}
{"id": "1306.0267", "contents": "Title: Locality statistics for anomaly detection in time series of graphs Abstract: The ability to detect change-points in a dynamic network or a time series of\ngraphs is an increasingly important task in many applications of the emerging\ndiscipline of graph signal processing. This paper formulates change-point\ndetection as a hypothesis testing problem in terms of a generative latent\nposition model, focusing on the special case of the Stochastic Block Model time\nseries. We analyze two classes of scan statistics, based on distinct underlying\nlocality statistics presented in the literature. Our main contribution is the\nderivation of the limiting distributions and power characteristics of the\ncompeting scan statistics. Performance is compared theoretically, on synthetic\ndata, and on the Enron email corpus. We demonstrate that both statistics are\nadmissible in one simple setting, while one of the statistics is inadmissible a\nsecond setting. \n\n"}
{"id": "1306.0772", "contents": "Title: Equivalence and comparison of heterogeneous cellular networks Abstract: We consider a general heterogeneous network in which, besides general\npropagation effects (shadowing and/or fading), individual base stations can\nhave different emitting powers and be subject to different parameters of\nHata-like path-loss models (path-loss exponent and constant) due to, for\nexample, varying antenna heights. We assume also that the stations may have\nvarying parameters of, for example, the link layer performance (SINR threshold,\netc). By studying the propagation processes of signals received by the typical\nuser from all antennas marked by the corresponding antenna parameters, we show\nthat seemingly different heterogeneous networks based on Poisson point\nprocesses can be equivalent from the point of view a typical user. These\nneworks can be replaced with a model where all the previously varying\npropagation parameters (including path-loss exponents) are set to constants\nwhile the only trade-off being the introduction of an isotropic base station\ndensity. This allows one to perform analytic comparisons of different network\nmodels via their isotropic representations. In the case of a constant path-loss\nexponent, the isotropic representation simplifies to a homogeneous modification\nof the constant intensity of the original network, thus generalizing a previous\nresult showing that the propagation processes only depend on one moment of the\nemitted power and propagation effects. We give examples and applications to\nmotivate these results and highlight an interesting observation regarding\nrandom path-loss exponents. \n\n"}
{"id": "1306.1693", "contents": "Title: Dynamic Coherence in Excitonic Molecular Complexes under Various\n  Excitation Conditions Abstract: We investigate the relevance of dynamic quantum coherence in the energy\ntransfer efficiency of molecular aggregates. We contrast the dynamics after\nexcitation of a quantum mechanical system with that of a classical system. We\ndemonstrate how a classical description of an ensemble average can be\nsatisfactorily interpreted either as a single system driven by a continuous\nforce or as an ensemble of systems each driven by an impulsive force. We derive\nthe time evolution of the density matrix for an open quantum system excited by\nlight or by a neighboring antenna. We argue that unlike in the classical case,\nthe quantum description does not allow for a formal decomposition of the\ndynamics into sudden jumps in the quantum mechanical state. Rather, there is a\nnatural finite time-scale associated with the excitation process. We propose a\nsimple experiment how to test the influence of this time scale on the yield of\nphotosynthesis. Because photosynthesis is intrinsically an average process, the\nefficiency of photosynthesis can be assessed from the quantum mechanical\nexpectation value calculated from the second-order response theory, which has\nthe same validity as the perturbative description of ultrafast experiments. We\ndemonstrate using typical parameters of the currently most studied\nphotosynthetic antenna, the Fenna-Matthews-Olson (FMO) complex, and a typical\nenergy transfer rate from the chlorosome baseplate, that dynamic coherences are\naveraged out in the complex despite excitation proceeding through a coherent\nsuperposition of its eigenstates. The dynamic coherence averages out even when\nthe FMO model is completely free of all dissipation and dephasing. We conclude\nthat under natural excitation conditions coherent dynamics cannot be\nresponsible for the remarkable efficiency of the photosynthesis even when\nconsidering the dynamics at a single molecular level. \n\n"}
{"id": "1306.1904", "contents": "Title: Network Inference Using Steady State Data and Goldbeter-Koshland\n  Kinetics Abstract: Network inference approaches are widely used to shed light on regulatory\ninterplay between molecular players such as genes and proteins. Biochemical\nprocesses underlying networks of interest (e.g. gene regulatory or protein\nsignalling networks) are generally nonlinear. In many settings, knowledge is\navailable concerning relevant chemical kinetics. However, existing network\ninference methods for continuous, steady-state data are typically rooted in\nstatistical formulations, which do not exploit chemical kinetics to guide\ninference. Herein, we present an approach to network inference for steady-state\ndata that is rooted in non-linear descriptions of biochemical mechanism. We use\nequilibrium analysis of chemical kinetics to obtain functional forms that are\nin turn used to infer networks using steady-state data. The approach we propose\nis directly applicable to conventional steady-state gene expression or\nproteomic data and does not require knowledge of either network topology or any\nkinetic parameters. We illustrate the approach in the context of protein\nphosphorylation networks, using data simulated from a recent mechanistic model\nand proteomic data from cancer cell lines. In the former, the true network is\nknown and used for assessment, whereas in the latter, results are compared\nagainst known biochemistry. We find that the proposed methodology is more\neffective at estimating network topology than methods based on linear models. \n\n"}
{"id": "1306.3110", "contents": "Title: Some applications of first-passage ideas to finance Abstract: Many problems in finance are related to first passage times. Among all of\nthem, we chose three on which we contributed personally. Our first example\nrelates Kolmogorov-Smirnov like goodness-of-fit tests, modified in such a way\nthat tail events and core events contribute equally to the test (in the\nstandard Kolmogorov-Smirnov, the tails contribute very little to the measure of\ngoodness-of-fit). We show that this problem can be mapped onto that of a random\nwalk inside moving walls. The second example is the optimal time to sell an\nasset (modelled as a random walk with drift) such that the sell time is as\nclose as possible to the time at which the asset reaches its maximum value. The\nlast example concerns optimal trading in the presence of transaction costs. In\nthis case, the optimal strategy is to wait until the predictor reaches (plus or\nminus) a threshold value before buying or selling. The value of this threshold\nis found by mapping the problem onto that of a random walk between two walls. \n\n"}
{"id": "1306.4942", "contents": "Title: Disentangling electronic and vibronic coherences in two-dimensional echo\n  spectra Abstract: The prevalence of long-lasting oscillatory signals in the 2d\necho-spectroscopy of light-harvesting complexes has led to a search for\npossible mechanisms. We investigate how two causes of oscillatory signals are\nintertwined: (i) electronic coherences supporting delocalized wave-like motion,\nand (ii) narrow bands in the vibronic spectral density. To disentangle the\nvibronic and electronic contributions we introduce a time-windowed Fourier\ntransform of the signal amplitude. We find that 2d spectra can be dominated by\nexcitations of pathways which are absent in excitonic energy transport. This\nleads to an underestimation of the life-time of electronic coherences by 2d\nspectra. \n\n"}
{"id": "1306.6281", "contents": "Title: Compressive Coded Aperture Keyed Exposure Imaging with Optical Flow\n  Reconstruction Abstract: This paper describes a coded aperture and keyed exposure approach to\ncompressive video measurement which admits a small physical platform, high\nphoton efficiency, high temporal resolution, and fast reconstruction\nalgorithms. The proposed projections satisfy the Restricted Isometry Property\n(RIP), and hence compressed sensing theory provides theoretical guarantees on\nthe video reconstruction quality. Moreover, the projections can be easily\nimplemented using existing optical elements such as spatial light modulators\n(SLMs). We extend these coded mask designs to novel dual-scale masks (DSMs)\nwhich enable the recovery of a coarse-resolution estimate of the scene with\nnegligible computational cost. We develop fast numerical algorithms which\nutilize both temporal correlations and optical flow in the video sequence as\nwell as the innovative structure of the projections. Our numerical experiments\ndemonstrate the efficacy of the proposed approach on short-wave infrared data. \n\n"}
{"id": "1306.6510", "contents": "Title: Multi-Structural Signal Recovery for Biomedical Compressive Sensing Abstract: Compressive sensing has shown significant promise in biomedical fields. It\nreconstructs a signal from sub-Nyquist random linear measurements. Classical\nmethods only exploit the sparsity in one domain. A lot of biomedical signals\nhave additional structures, such as multi-sparsity in different domains,\npiecewise smoothness, low rank, etc. We propose a framework to exploit all the\navailable structure information. A new convex programming problem is generated\nwith multiple convex structure-inducing constraints and the linear measurement\nfitting constraint. With additional a priori information for solving the\nunderdetermined system, the signal recovery performance can be improved. In\nnumerical experiments, we compare the proposed method with classical methods.\nBoth simulated data and real-life biomedical data are used. Results show that\nthe newly proposed method achieves better reconstruction accuracy performance\nin term of both L1 and L2 errors. \n\n"}
{"id": "1306.6650", "contents": "Title: Vibration-assisted resonance in photosynthetic excitation energy\n  transfer Abstract: Understanding how the effectiveness of natural photosynthetic energy\nharvesting systems arises from the interplay between quantum coherence and\nenvironmental noise represents a significant challenge for quantum theory.\nRecently it has begun to be appreciated that discrete molecular vibrational\nmodes may play an important role in the dynamics of such systems. As an\nalternative to computationally demanding numerical approaches, we present a\nmicroscopic mechanism by which intramolecular vibrations contribute to the\nefficiency and directionality of energy transfer. Excited vibrational states\ncreate resonant pathways through the system, supporting fast and efficient\nenergy transport. Vibrational damping together with the natural downhill\narrangement of molecular energy levels gives intrinsic directionality to the\nenergy flow. Analytical and numerical results demonstrate a significant\nenhancement of the efficiency and directionality of energy transport that can\nbe directly related to the existence of resonances between vibrational and\nexcitonic levels. \n\n"}
{"id": "1307.0886", "contents": "Title: Atomistic study of energy funneling in the light-harvesting complex of\n  green sulfur bacteria Abstract: Phototrophic organisms such as plants, photosynthetic bacteria and algae use\nmicroscopic complexes of pigment molecules to absorb sunlight. Within the\nlight-harvesting complexes, which frequently have several functional and\nstructural subunits, the energy is transferred in the form of molecular\nexcitations with very high efficiency. Green sulfur bacteria are considered to\nbe amongst the most efficient light-harvesting organisms. Despite multiple\nexperimental and theoretical studies of these bacteria the physical origin of\nthe efficient and robust energy transfer in their light-harvesting complexes is\nnot well understood. To study excitation dynamics at the systems level we\nintroduce an atomistic model that mimics a complete light-harvesting apparatus\nof green sulfur bacteria. The model contains approximately 4000 pigment\nmolecules and comprises a double wall roll for the chlorosome, a baseplate and\nsix Fenna-Matthews-Olson trimer complexes. We show that the fast relaxation\nwithin functional subunits combined with the transfer between collective\nexcited states of pigments can result in robust energy funneling. Energy\ntransfer is robust on the initial excitation conditions and temperature\nchanges. Moreover, the same mechanism describes the coexistence of multiple\ntimescales of excitation dynamics frequently observed in ultrafast optical\nexperiments. While our findings support the hypothesis of supertransfer, the\nmodel reveals energy transport through multiple channels on different length\nscales. \n\n"}
{"id": "1307.0898", "contents": "Title: Entropy of a Zipfian Distributed Lexicon Abstract: This article presents the calculation of the entropy of a system with Zipfian\ndistribution and shows that a communication system tends to present an exponent\nvalue close to one, but still greater than one, so that it might maximize\nentropy and hold a feasible lexicon with an increasing size. This result is in\nagreement with what is observed in natural languages and with the balance\nbetween the speaker and listener communication efforts. On the other hand, the\nentropy of the communicating source is very sensitive to the exponent value as\nwell as the length of the observable data, making it a poor parameter to\ncharacterize the communication process. \n\n"}
{"id": "1307.1164", "contents": "Title: Statistical Inference for Stochastic Differential Equations with Memory Abstract: In this paper we construct a framework for doing statistical inference for\ndiscretely observed stochastic differential equations (SDEs) where the driving\nnoise has 'memory'. Classical SDE models for inference assume the driving noise\nto be Brownian motion, or \"white noise\", thus implying a Markov assumption. We\nfocus on the case when the driving noise is a fractional Brownian motion, which\nis a common continuous-time modeling device for capturing long-range memory.\nSince the likelihood is intractable, we proceed via data augmentation, adapting\na familiar discretization and missing data approach developed for the white\nnoise case. In addition to the other SDE parameters, we take the Hurst index to\nbe unknown and estimate it from the data. Posterior sampling is performed via a\nHybrid Monte Carlo algorithm on both the parameters and the missing data\nsimultaneously so as to improve mixing. We point out that, due to the\nlong-range correlations of the driving noise, careful discretization of the\nunderlying SDE is necessary for valid inference. Our approach can be adapted to\nother types of rough-path driving processes such as Gaussian \"colored\" noise.\nThe methodology is used to estimate the evolution of the memory parameter in US\nshort-term interest rates. \n\n"}
{"id": "1307.1524", "contents": "Title: Fundamentals of Heterogeneous Cellular Networks with Energy Harvesting Abstract: We develop a new tractable model for K-tier heterogeneous cellular networks\n(HetNets), where each base station (BS) is powered solely by a self-contained\nenergy harvesting module. The BSs across tiers differ in terms of the energy\nharvesting rate, energy storage capacity, transmit power and deployment\ndensity. Since a BS may not always have enough energy, it may need to be kept\nOFF and allowed to recharge while nearby users are served by neighboring BSs\nthat are ON. We show that the fraction of time a k^{th} tier BS can be kept ON,\ntermed availability \\rho_k, is a fundamental metric of interest. Using tools\nfrom random walk theory, fixed point analysis and stochastic geometry, we\ncharacterize the set of K-tuples (\\rho_1, \\rho_2, ... \\rho_K), termed the\navailability region, that is achievable by general uncoordinated operational\nstrategies, where the decision to toggle the current ON/OFF state of a BS is\ntaken independently of the other BSs. If the availability vector corresponding\nto the optimal system performance, e.g., in terms of rate, lies in this\navailability region, there is no performance loss due to the presence of\nunreliable energy sources. As a part of our analysis, we model the temporal\ndynamics of the energy level at each BS as a birth-death process, derive the\nenergy utilization rate, and use hitting/stopping time analysis to prove that\nthere exists a fundamental limit on \\rho_k that cannot be surpassed by any\nuncoordinated strategy. \n\n"}
{"id": "1307.2579", "contents": "Title: Tuned Models of Peer Assessment in MOOCs Abstract: In massive open online courses (MOOCs), peer grading serves as a critical\ntool for scaling the grading of complex, open-ended assignments to courses with\ntens or hundreds of thousands of students. But despite promising initial\ntrials, it does not always deliver accurate results compared to human experts.\nIn this paper, we develop algorithms for estimating and correcting for grader\nbiases and reliabilities, showing significant improvement in peer grading\naccuracy on real data with 63,199 peer grades from Coursera's HCI course\nofferings --- the largest peer grading networks analysed to date. We relate\ngrader biases and reliabilities to other student factors such as student\nengagement, performance as well as commenting style. We also show that our\nmodel can lead to more intelligent assignment of graders to gradees. \n\n"}
{"id": "1307.3530", "contents": "Title: Vibrations, Quanta and Biology Abstract: Quantum biology is an emerging field of research that concerns itself with\nthe experimental and theoretical exploration of non-trivial quantum phenomena\nin biological systems. In this tutorial overview we aim to bring out\nfundamental assumptions and questions in the field, identify basic design\nprinciples and develop a key underlying theme -- the dynamics of quantum\ndynamical networks in the presence of an environment and the fruitful interplay\nthat the two may enter. At the hand of three biological phenomena whose\nunderstanding is held to require quantum mechanical processes, namely\nexcitation and charge transfer in photosynthetic complexes, magneto-reception\nin birds and the olfactory sense, we demonstrate that this underlying theme\nencompasses them all, thus suggesting its wider relevance as an archetypical\nframework for quantum biology. \n\n"}
{"id": "1307.5130", "contents": "Title: Perspective: Coulomb fluids -- weak coupling, strong coupling, in\n  between and beyond Abstract: We present a personal view on the current state of statistical mechanics of\nCoulomb fluids with special emphasis on the interactions between macromolecular\nsurfaces, concentrating on the weak and the strong coupling limits. Both are\nintroduced for a (primitive) counterion-only system in the presence of\nmacroscopic, uniformly charged boundaries, where they can be derived\nsystematically. Later we show how this formalism can be generalized to the\ncases with additional characteristic length scales that introduce new coupling\nparameters into the problem. These cases most notably include asymmetric ionic\nmixtures with mono- and multivalent ions that couple differently to charged\nsurfaces, ions with internal charge (multipolar) structure and finite static\npolarizability, where weak and strong coupling limits can be constructed by\nanalogy with the counterion-only case and lead to important new insights into\ntheir properties that can not be derived by any other means. \n\n"}
{"id": "1307.5558", "contents": "Title: Mixtures of Common Skew-t Factor Analyzers Abstract: A mixture of common skew-t factor analyzers model is introduced for\nmodel-based clustering of high-dimensional data. By assuming common component\nfactor loadings, this model allows clustering to be performed in the presence\nof a large number of mixture components or when the number of dimensions is too\nlarge to be well-modelled by the mixtures of factor analyzers model or a\nvariant thereof. Furthermore, assuming that the component densities follow a\nskew-t distribution allows robust clustering of skewed data. The alternating\nexpectation-conditional maximization algorithm is employed for parameter\nestimation. We demonstrate excellent clustering performance when our model is\napplied to real and simulated data.This paper marks the first time that skewed\ncommon factors have been used. \n\n"}
{"id": "1307.6539", "contents": "Title: Quantifying playmaking ability in hockey Abstract: It is often said that a sign of a great player is that he makes the players\naround him better. The player may or may not score much himself, but his\nteammates perform better when he plays. One way a hockey player can improve his\nor her teammates' performance is to create goal scoring opportunities.\nUnfortunately, in hockey goal scoring is relatively infrequent, and statistics\nlike assists can be unreliable as a measure of a player's playmaking ability.\nAssists also depend on playing time, power play usage, the strength of a\nplayer's linemates, and other factors. In this paper we develop a metric for\nquantifying playmaking ability that addresses these issues. Our playmaking\nmetric has two benefits over assists for which we can provide statistical\nevidence: it is more consistent than assists, and it is better than assists at\npredicting future assists. Quantifying player contributions using this measure\ncan assist decision-makers in identifying, acquiring, and integrating\nsuccessful playmakers into their lineups. \n\n"}
{"id": "1307.7309", "contents": "Title: Optimal Rate Sampling in 802.11 Systems Abstract: In 802.11 systems, Rate Adaptation (RA) is a fundamental mechanism allowing\ntransmitters to adapt the coding and modulation scheme as well as the MIMO\ntransmission mode to the radio channel conditions, and in turn, to learn and\ntrack the (mode, rate) pair providing the highest throughput. So far, the\ndesign of RA mechanisms has been mainly driven by heuristics. In contrast, in\nthis paper, we rigorously formulate such design as an online stochastic\noptimisation problem. We solve this problem and present ORS (Optimal Rate\nSampling), a family of (mode, rate) pair adaptation algorithms that provably\nlearn as fast as it is possible the best pair for transmission. We study the\nperformance of ORS algorithms in both stationary radio environments where the\nsuccessful packet transmission probabilities at the various (mode, rate) pairs\ndo not vary over time, and in non-stationary environments where these\nprobabilities evolve. We show that under ORS algorithms, the throughput loss\ndue to the need to explore sub-optimal (mode, rate) pairs does not depend on\nthe number of available pairs, which is a crucial advantage as evolving 802.11\nstandards offer an increasingly large number of (mode, rate) pairs. We\nillustrate the efficiency of ORS algorithms (compared to the state-of-the-art\nalgorithms) using simulations and traces extracted from 802.11 test-beds. \n\n"}
{"id": "1307.8046", "contents": "Title: Joint estimation of causal effects from observational and intervention\n  gene expression data Abstract: Background: Inference of gene regulatory networks from transcriptomic data\nhas been a wide research area in recent years. Proposed methods are mainly\nbased on the use of graphical Gaussian models for observational wild-type data\nand provide undirected graphs that are not able to accurately highlight the\ncausal relationships among genes. In the present work, we seek to improve\nestimation of causal effects among genes by jointly modeling observational\ntranscriptomic data with intervention data obtained by performing knock-outs or\nknock-downs on a subset of genes. By examining the impact of such expression\nperturbations on other genes, a more accurate reflection of regulatory\nrelationships may be obtained than through the use of wild-type data alone.\nResults: Using the framework of Gaussian Bayesian networks, we propose a Markov\nchain Monte Carlo algorithm with a Mallows model and an analytical likelihood\nmaximization to sample from the posterior distribution of causal node\norderings, and in turn, to estimate causal effects. The main advantage of the\nproposed algorithm over previously proposed methods is that it has the\nflexibility to accommodate any kind of intervention design, including partial\nor multiple knock-out experiments. Methods were compared on simulated data as\nwell as data from the DREAM 2007 challenge. Conclusions: The simulation study\nconfirmed the impossibility of estimating causal orderings of genes with\nobservation data only. The proposed algorithm was found, in most cases, to\nperform better than the previously proposed methods in terms of accuracy for\nthe estimation of causal effects. In addition, multiple knock-outs proved to\nbring valuable additional information compared to single knock-outs. The choice\nof optimal intervention design therefore appears to be a crucial aspect for\ncausal inference and an interesting challenge for future research. \n\n"}
{"id": "1308.0642", "contents": "Title: Nonlinear Time Series Modeling: A Unified Perspective, Algorithm, and\n  Application Abstract: A new comprehensive approach to nonlinear time series analysis and modeling\nis developed in the present paper. We introduce novel data-specific\nmid-distribution based Legendre Polynomial (LP) like nonlinear transformations\nof the original time series Y(t) that enables us to adapt all the existing\nstationary linear Gaussian time series modeling strategy and made it applicable\nfor non-Gaussian and nonlinear processes in a robust fashion. The emphasis of\nthe present paper is on empirical time series modeling via the algorithm\nLPTime. We demonstrate the effectiveness of our theoretical framework using\ndaily S&P 500 return data between Jan/2/1963 - Dec/31/2009. Our proposed LPTime\nalgorithm systematically discovers all the `stylized facts' of the financial\ntime series automatically all at once, which were previously noted by many\nresearchers one at a time. \n\n"}
{"id": "1308.1764", "contents": "Title: Dynamics of a two-level system under the simultaneous influence of a\n  spin bath and a boson bath Abstract: We study dynamics of a two-level system coupled simultaneously to a pair of\ndissimilar reservoirs, namely, a spin bath and a boson bath, which are\nconnected via finite interbath coupling. It is found that the steady-state\nenergy transfer in the two-level system increases with its coupling to the spin\nbath while optimal transfer occurs at intermediate coupling in the transient\nprocess. If the two-level system is strongly coupled to the spin bath, the\npopulation transfer is unidirectional barring minor population oscillations of\nminute amplitudes. If the spin bath is viewed as an atomic ensemble, robust\ngeneration of macroscopic superposition states exists against parameter\nvariations of the two-level system and the boson bath. \n\n"}
{"id": "1308.2454", "contents": "Title: Understanding the Benefits of Open Access in Femtocell Networks:\n  Stochastic Geometric Analysis in the Uplink Abstract: We introduce a comprehensive analytical framework to compare between open\naccess and closed access in two-tier femtocell networks, with regard to uplink\ninterference and outage. Interference at both the macrocell and femtocell\nlevels is considered. A stochastic geometric approach is employed as the basis\nfor our analysis. We further derive sufficient conditions for open access and\nclosed access to outperform each other in terms of the outage probability,\nleading to closed-form expressions to upper and lower bound the difference in\nthe targeted received power between the two access modes. Simulations are\nconducted to validate the accuracy of the analytical model and the correctness\nof the bounds. \n\n"}
{"id": "1308.3548", "contents": "Title: Distributed Ranging and Localization for Wireless Networks via\n  Compressed Sensing Abstract: Location-based services in a wireless network require nodes to know their\nlocations accurately. Conventional solutions rely on contention-based medium\naccess, where only one node can successfully transmit at any time in any\nneighborhood. In this paper, a novel, complete, distributed ranging and\nlocalization solution is proposed, which let all nodes in the network broadcast\ntheir location estimates and measure distances to all neighbors simultaneously.\nAn on-off signaling is designed to overcome the physical half-duplex\nconstraint. In each iteration, all nodes transmit simultaneously, each\nbroadcasting codewords describing the current location estimate. From the\nsuperposed signals from all neighbors, each node decodes their neighbors'\nlocations and also estimates their distances using the signal strengths. The\nnode then broadcasts its improved location estimates in the subsequent\niteration. Simulations demonstrate accurate localization throughout a large\nnetwork over a few thousand symbol intervals, suggesting much higher efficiency\nthan conventional schemes based on ALOHA or CSMA. \n\n"}
{"id": "1308.3925", "contents": "Title: Distance Correlation Methods for Discovering Associations in Large\n  Astrophysical Databases Abstract: High-dimensional, large-sample astrophysical databases of galaxy clusters,\nsuch as the Chandra Deep Field South COMBO-17 database, provide measurements on\nmany variables for thousands of galaxies and a range of redshifts. Current\nunderstanding of galaxy formation and evolution rests sensitively on\nrelationships between different astrophysical variables; hence an ability to\ndetect and verify associations or correlations between variables is important\nin astrophysical research. In this paper, we apply a recently defined\nstatistical measure called the distance correlation coefficient which can be\nused to identify new associations and correlations between astrophysical\nvariables. The distance correlation coefficient applies to variables of any\ndimension; it can be used to determine smaller sets of variables that provide\nequivalent astrophysical information; it is zero only when variables are\nindependent; and it is capable of detecting nonlinear associations that are\nundetectable by the classical Pearson correlation coefficient. Hence, the\ndistance correlation coefficient provides more information than the Pearson\ncoefficient. We analyze numerous pairs of variables in the COMBO-17 database\nwith the distance correlation method and with the maximal information\ncoefficient. We show that the Pearson coefficient can be estimated with higher\naccuracy from the corresponding distance correlation coefficient than from the\nmaximal information coefficient. For given values of the Pearson coefficient,\nthe distance correlation method has a greater ability than the maximal\ninformation coefficient to resolve astrophysical data into highly concentrated\nV-shapes, which enhances classification and pattern identification. These\nresults are observed over a range of redshifts beyond the local universe and\nfor galaxies from elliptical to spiral. \n\n"}
{"id": "1308.4288", "contents": "Title: Efficient hierarchical analysis of the stability of a network through\n  dimensional reduction of its influence topology Abstract: The connection between network topology and stability remains unclear.\nGeneral approaches that clarify this relationship and allow for more efficient\nstability analysis would be desirable. Inspired by chemical reaction networks,\nI demonstrate the utility of expressing the governing equations of arbitrary\nfirst-order dynamical systems (interaction networks) in terms of sums of real\nfunctions (generalized reactions) multiplied by real scalars (generalized\nstoichiometries). Specifically, I examine the mathematical notion of influence\ntopology, which is based on the reaction stoichiometries and the first\nderivatives of the reactions with respect to each species at the steady state\nsolution(s). It is naturally represented as a signed directed bipartite graph\nwith arrows or blunt arrows connecting a species node to a reaction node\n(positive/negative derivative) or a reaction node to a species node\n(positive/negative stoichiometry). The set of all such graphs is denumerable. A\nsignificant reduction in dimensionality is possible through stoichiometric\nscaling, cycle compaction, and temporal scaling. All cycles in a network can be\nread directly from the graph of its influence topology, enabling efficient and\nintuitive computation of the principal minors (sums of products of\nnon-overlapping bipartite cycles) and the Hurwitz determinants (sums of\nproducts of either the principal minors or the bipartite cycles) for testing\nsteady state stability. The stability of a given network is shown to have a\nhierarchical dependence first on its influence topology and then, more\nspecifically, on algebraic conditions (exact functional form of the reactions).\nThe utility of this hierarchical approach to bifurcation analysis is\ndemonstrated on classical networks from control theory, biology, chemistry,\nphysics, and electronics. \n\n"}
{"id": "1308.5623", "contents": "Title: One-step estimator paths for concave regularization Abstract: The statistics literature of the past 15 years has established many favorable\nproperties for sparse diminishing-bias regularization: techniques which can\nroughly be understood as providing estimation under penalty functions spanning\nthe range of concavity between $L_0$ and $L_1$ norms. However, lasso\n$L_1$-regularized estimation remains the standard tool for industrial `Big\nData' applications because of its minimal computational cost and the presence\nof easy-to-apply rules for penalty selection. In response, this article\nproposes a simple new algorithm framework that requires no more computation\nthan a lasso path: the path of one-step estimators (POSE) does $L_1$ penalized\nregression estimation on a grid of decreasing penalties, but adapts\ncoefficient-specific weights to decrease as a function of the coefficient\nestimated in the previous path step. This provides sparse diminishing-bias\nregularization at no extra cost over the fastest lasso algorithms. Moreover,\nour `gamma lasso' implementation of POSE is accompanied by a reliable heuristic\nfor the fit degrees of freedom, so that standard information criteria can be\napplied in penalty selection. We also provide novel results on the distance\nbetween weighted-$L_1$ and $L_0$ penalized predictors; this allows us to build\nintuition about POSE and other diminishing-bias regularization schemes. The\nmethods and results are illustrated in extensive simulations and in application\nof logistic regression to evaluating the performance of hockey players. \n\n"}
{"id": "1309.1187", "contents": "Title: Understanding shape entropy through local dense packing Abstract: Entropy drives the phase behavior of colloids ranging from dense suspensions\nof hard spheres or rods to dilute suspensions of hard spheres and depletants.\nEntropic ordering of anisotropic shapes into complex crystals, liquid crystals,\nand even quasicrystals has been demonstrated recently in computer simulations\nand experiments. The ordering of shapes appears to arise from the emergence of\ndirectional entropic forces (DEFs) that align neighboring particles, but these\nforces have been neither rigorously defined nor quantified in generic systems.\nHere, we show quantitatively that shape drives the phase behavior of systems of\nanisotropic particles upon crowding through DEFs. We define DEFs in generic\nsystems, and compute them for several hard particle systems. We show that they\nare on the order of a few kT at the onset of ordering, placing DEFs on par with\ntraditional depletion, van der Waals, and other intrinsic interactions. In\nexperimental systems with these other interactions, we provide direct\nquantitative evidence that entropic effects of shape also contribute to\nself-assembly. We use DEFs to draw a distinction between self-assembly and\npacking behavior. We show that the mechanism that generates directional\nentropic forces is the maximization of entropy by optimizing local particle\npacking. We show that this mechanism occurs in a wide class of systems, and we\ntreat, in a unified way, the entropy-driven phase behavior of arbitrary shapes\nincorporating the well-known works of Kirkwood, Onsager, and Asakura and\nOosawa. \n\n"}
{"id": "1309.4151", "contents": "Title: A Non-Local Means Filter for Removing the Poisson Noise Abstract: A new image denoising algorithm to deal with the Poisson noise model is\ngiven, which is based on the idea of Non-Local Mean. By using the \"Oracle\"\nconcept, we establish a theorem to show that the Non-Local Means Filter can\neffectively deal with Poisson noise with some modification. Under the\ntheoretical result, we construct our new algorithm called Non-Local Means\nPoisson Filter and demonstrate in theory that the filter converges at the usual\noptimal rate. The filter is as simple as the classic Non-Local Means and the\nsimulation results show that our filter is very competitive. \n\n"}
{"id": "1309.4513", "contents": "Title: Distributed Detection in Tree Topologies with Byzantines Abstract: In this paper, we consider the problem of distributed detection in tree\ntopologies in the presence of Byzantines. The expression for minimum attacking\npower required by the Byzantines to blind the fusion center (FC) is obtained.\nMore specifically, we show that when more than a certain fraction of individual\nnode decisions are falsified, the decision fusion scheme becomes completely\nincapable. We obtain closed form expressions for the optimal attacking\nstrategies that minimize the detection error exponent at the FC. We also look\nat the possible counter-measures from the FC's perspective to protect the\nnetwork from these Byzantines. We formulate the robust topology design problem\nas a bi-level program and provide an efficient algorithm to solve it. We also\nprovide some numerical results to gain insights into the solution. \n\n"}
{"id": "1309.4975", "contents": "Title: Gaussian Approximation of Perturbed Chi-Square Risks Abstract: In this paper we show that the conditional distribution of perturbed\nchi-quare risks can be approximated by certain distributions including the\nGaussian ones. Our results are of interest for conditional extreme value models\nand multivariate extremes as shown in three applications. \n\n"}
{"id": "1309.4978", "contents": "Title: An Analytical Model of Packet Collisions in IEEE 802.15.4 Wireless\n  Networks Abstract: Numerous studies showed that concurrent transmissions can boost wireless\nnetwork performance despite collisions. While these works provide empirical\nevidence that concurrent transmissions may be received reliably, existing\nsignal capture models only partially explain the root causes of this\nphenomenon. We present a comprehensive mathematical model that reveals the\nreasons and provides insights on the key parameters affecting the performance\nof MSK-modulated transmissions. A major contribution is a closed-form\nderivation of the receiver bit decision variable for arbitrary numbers of\ncolliding signals and constellations of power ratios, timing offsets, and\ncarrier phase offsets. We systematically explore the root causes for successful\npacket delivery under concurrent transmissions across the whole parameter space\nof the model. We confirm the capture threshold behavior observed in previous\nstudies but also reveal new insights relevant for the design of optimal\nprotocols: We identify capture zones depending not only on the signal power\nratio but also on time and phase offsets. \n\n"}
{"id": "1309.5073", "contents": "Title: Non-linear dependences in finance Abstract: The thesis is composed of three parts. Part I introduces the mathematical and\nstatistical tools that are relevant for the study of dependences, as well as\nstatistical tests of Goodness-of-fit for empirical probability distributions. I\npropose two extensions of usual tests when dependence is present in the sample\ndata and when observations have a fat-tailed distribution. The financial\ncontent of the thesis starts in Part II. I present there my studies regarding\nthe \"cross-sectional\" dependences among the time series of daily stock returns,\ni.e. the instantaneous forces that link several stocks together and make them\nbehave somewhat collectively rather than purely independently. A calibration of\na new factor model is presented here, together with a comparison to\nmeasurements on real data. Finally, Part III investigates the temporal\ndependences of single time series, using the same tools and measures of\ncorrelation. I propose two contributions to the study of the origin and\ndescription of \"volatility clustering\": one is a generalization of the\nARCH-like feedback construction where the returns are self-exciting, and the\nother one is a more original description of self-dependences in terms of\ncopulas. The latter can be formulated model-free and is not specific to\nfinancial time series. In fact, I also show here how concepts like recurrences,\nrecords, aftershocks and waiting times, that characterize the dynamics in a\ntime series can be written in the unifying framework of the copula. \n\n"}
{"id": "1309.6178", "contents": "Title: Spot volatility estimation for high-frequency data: adaptive estimation\n  in practice Abstract: We develop further the spot volatility estimator introduced in Hoffmann, Munk\nand Schmidt-Hieber (2012) from a practical point of view and make it useful for\nthe analysis of high-frequency financial data. In a first part, we adjust the\nestimator substantially in order to achieve good finite sample performance and\nto overcome difficulties arising from violations of the additive microstructure\nnoise model (e.g. jumps, rounding errors). These modifications are justified by\nsimulations. The second part is devoted to investigate the behavior of\nvolatility in response to macroeconomic events. We give evidence that the spot\nvolatility of Euro-BUND futures is considerably higher during press conferences\nof the European Central Bank. As an outlook, we present an estimator for the\nspot covolatility of two different prices. \n\n"}
{"id": "1310.0364", "contents": "Title: Segregation Indices for Disease Clustering Abstract: Spatial clustering has important implications in various fields. In\nparticular, disease clustering is of major public concern in epidemiology. In\nthis article, we propose the use of two distance-based segregation indices to\ntest the significance of disease clustering among subjects whose locations are\nfrom a homogeneous or an inhomogeneous population. We derive their asymptotic\ndistributions and compare them with other distance-based disease clustering\ntests in terms of empirical size and power by extensive Monte Carlo\nsimulations. The null pattern we consider is the random labeling (RL) of cases\nand controls to the given locations. Along this line, we investigate the\nsensitivity of the size of these tests to the underlying background pattern\n(e.g., clustered or homogenous) on which the RL is applied, the level of\nclustering and number of clusters, or differences in relative abundances of the\nclasses. We demonstrate that differences in relative abundance has the highest\nimpact on the empirical sizes of the tests. We also propose various non-RL\npatterns as alternatives to the RL pattern and assess the empirical power\nperformance of the tests under these alternatives. We illustrate the methods on\ntwo real-life examples from epidemiology. \n\n"}
{"id": "1310.0677", "contents": "Title: DVB-S2 Spectrum Efficiency Improvement with Hierarchical Modulation Abstract: We study the design of a DVB-S2 system in order to maximise spectrum\nefficiency. This task is usually challenging due to channel variability. Modern\nsatellite communications systems such as DVB-SH and DVB-S2 rely mainly on a\ntime sharing strategy to optimise the spectrum efficiency. Recently, we showed\nthat combining time sharing with hierarchical modulation can provide\nsignificant gains (in terms of spectrum efficiency) compared to the best time\nsharing strategy. However, our previous design does not improve the DVB-S2\nperformance when all the receivers experience low or large signal-to-noise\nratios. In this article, we introduce and study a hierarchical QPSK and a\nhierarchical 32-APSK to overcome the previous limitations.We show in a\nrealistic case based on DVB-S2 that the hierarchical QPSK provides an\nimprovement when the receivers experience poor channel condition, while the\n32-APSK increases the spectrum efficiency when the receivers experience good\nchannel condition. \n\n"}
{"id": "1310.2873", "contents": "Title: Regional variance for multi-object filtering Abstract: Recent progress in multi-object filtering has led to algorithms that compute\nthe first-order moment of multi-object distributions based on sensor\nmeasurements. The number of targets in arbitrarily selected regions can be\nestimated using the first-order moment. In this work, we introduce explicit\nformulae for the computation of the second-order statistic on the target\nnumber. The proposed concept of regional variance quantifies the level of\nconfidence on target number estimates in arbitrary regions and facilitates\ninformation-based decisions. We provide algorithms for its computation for the\nProbability Hypothesis Density (PHD) and the Cardinalized Probability\nHypothesis Density (CPHD) filters. We demonstrate the behaviour of the regional\nstatistics through simulation examples. \n\n"}
{"id": "1310.3223", "contents": "Title: Sparse Median Graphs Estimation in a High Dimensional Semiparametric\n  Model Abstract: In this manuscript a unified framework for conducting inference on complex\naggregated data in high dimensional settings is proposed. The data are assumed\nto be a collection of multiple non-Gaussian realizations with underlying\nundirected graphical structures. Utilizing the concept of median graphs in\nsummarizing the commonality across these graphical structures, a novel\nsemiparametric approach to modeling such complex aggregated data is provided\nalong with robust estimation of the median graph, which is assumed to be\nsparse. The estimator is proved to be consistent in graph recovery and an upper\nbound on the rate of convergence is given. Experiments on both synthetic and\nreal datasets are conducted to illustrate the empirical usefulness of the\nproposed models and methods. \n\n"}
{"id": "1310.3970", "contents": "Title: Green Communication via Power-optimized HARQ Protocols Abstract: Recently, efficient use of energy has become an essential research topic for\ngreen communication. This paper studies the effect of optimal power controllers\non the performance of delay-sensitive communication setups utilizing hybrid\nautomatic repeat request (HARQ). The results are obtained for repetition time\ndiversity (RTD) and incremental redundancy (INR) HARQ protocols. In all cases,\nthe optimal power allocation, minimizing the outage-limited average\ntransmission power, is obtained under both continuous and bursting\ncommunication models. Also, we investigate the system throughput in different\nconditions. The results indicate that the power efficiency is increased\nsubstantially, if adaptive power allocation is utilized. For instance, assume\nRayleigh-fading channel, a maximum of two (re)transmission rounds with rates\n$\\{1,\\frac{1}{2}\\}$ nats-per-channel-use and an outage probability constraint\n${10}^{-3}$. Then, compared to uniform power allocation, optimal power\nallocation in RTD reduces the average power by 9 and 11 dB in the bursting and\ncontinuous communication models, respectively. In INR, these values are\nobtained to be 8 and 9 dB, respectively. \n\n"}
{"id": "1310.6316", "contents": "Title: Modelling excitonic-energy transfer in light-harvesting complexes Abstract: The theoretical and experimental study of energy transfer in photosynthesis\nhas revealed an interesting transport regime, which lies at the borderline\nbetween classical transport dynamics and quantum-mechanical interference\neffects. Dissipation is caused by the coupling of electronic degrees of freedom\nto vibrational modes and leads to a directional energy transfer from the\nantenna complex to the target reaction-center. The dissipative driving is\nrobust and does not rely on fine-tuning of specific vibrational modes. For the\nparameter regime encountered in the biological systems new theoretical tools\nare required to directly compare theoretical results with experimental\nspectroscopy data. The calculations require to utilize massively parallel\ngraphics processor units (GPUs) for efficient and exact computations. \n\n"}
{"id": "1310.6443", "contents": "Title: Leveraging Physical Layer Capabilites: Distributed Scheduling in\n  Interference Networks with Local Views Abstract: In most wireless networks, nodes have only limited local information about\nthe state of the network, which includes connectivity and channel state\ninformation. With limited local information about the network, each node's\nknowledge is mismatched; therefore, they must make distributed decisions. In\nthis paper, we pose the following question - if every node has network state\ninformation only about a small neighborhood, how and when should nodes choose\nto transmit? While link scheduling answers the above question for\npoint-to-point physical layers which are designed for an interference-avoidance\nparadigm, we look for answers in cases when interference can be embraced by\nadvanced PHY layer design, as suggested by results in network information\ntheory.\n  To make progress on this challenging problem, we propose a constructive\ndistributed algorithm that achieves rates higher than link scheduling based on\ninterference avoidance, especially if each node knows more than one hop of\nnetwork state information. We compare our new aggressive algorithm to a\nconservative algorithm we have presented in [1]. Both algorithms schedule\nsub-networks such that each sub-network can employ advanced\ninterference-embracing coding schemes to achieve higher rates. Our innovation\nis in the identification, selection and scheduling of sub-networks, especially\nwhen sub-networks are larger than a single link. \n\n"}
{"id": "1310.7761", "contents": "Title: Natural Light Harvesting Systems: Unraveling the quantum puzzles Abstract: In natural light harvesting systems, the sequential quantum events of photon\nabsorption by specialized biological antenna complexes, charge separation,\nexciton formation and energy transfer to localized reaction centers culminates\nin the conversion of solar to chemical energy. A notable feature in these\nprocesses is the exceptionally high efficiencies ($>$ 95\\%) at which excitation\nis transferred from the illuminated protein complex site to the reaction\ncenters. The high speeds of excitation propagation within a system of\ninterwoven biomolecular network structures, is yet to be replicated in\nartificial light harvesting complexes. A clue to unraveling the quantum puzzles\nof nature may lie in the observations of long lived coherences lasting several\npicoseconds in the electronic spectra of photosynthetic complexes which occurs\neven in noisy environmental baths. The exact nature of the association between\nthe high energy propagation rates and strength of quantum coherences remains\nlargely unsolved. This review presents recent developments in quantum theories,\nand links information-theoretic aspects with photosynthetic light-harvesting\nprocesses in biomolecular systems. There is examination of various attempts to\npinpoint the processes that underpin coherence features arising from the light\nharvesting activities of biomolecular systems, with particular emphasis on the\neffects that factors such non-Markovianity, zeno mechanisms, teleportation,\nquantum predictability and the role of multipartite states have on the quantum\ndynamics of biomolecular systems. A discussion of how quantum thermodynamical\nprinciples and agent-based modeling and simulation approaches can improve our\nunderstanding of natural photosynthetic systems is included. \n\n"}
{"id": "1310.7850", "contents": "Title: Fundamental Limits of Nonintrusive Load Monitoring Abstract: Provided an arbitrary nonintrusive load monitoring (NILM) algorithm, we seek\nbounds on the probability of distinguishing between scenarios, given an\naggregate power consumption signal. We introduce a framework for studying a\ngeneral NILM algorithm, and analyze the theory in the general case. Then, we\nspecialize to the case where the error is Gaussian. In both cases, we are able\nto derive upper bounds on the probability of distinguishing scenarios. Finally,\nwe apply the results to real data to derive bounds on the probability of\ndistinguishing between scenarios as a function of the measurement noise, the\nsampling rate, and the device usage. \n\n"}
{"id": "1311.0376", "contents": "Title: On the Bootstrap for Persistence Diagrams and Landscapes Abstract: Persistent homology probes topological properties from point clouds and\nfunctions. By looking at multiple scales simultaneously, one can record the\nbirths and deaths of topological features as the scale varies. In this paper we\nuse a statistical technique, the empirical bootstrap, to separate topological\nsignal from topological noise. In particular, we derive confidence sets for\npersistence diagrams and confidence bands for persistence landscapes. \n\n"}
{"id": "1311.0644", "contents": "Title: Forecasting multivariate longitudinal binary data with marginal and\n  marginally specified models Abstract: Forecasting with longitudinal data has been rarely studied. Most of the\navailable studies are for continuous response and all of them are for\nunivariate response. In this study, we consider forecasting multivariate\nlongitudinal binary data. Five different models including simple ones,\nunivariate and multivariate marginal models, and complex ones, marginally\nspecified models, are studied to forecast such data. Model forecasting\nabilities are illustrated via a real life data set and a simulation study. The\nsimulation study includes a model independent data generation to provide a fair\nenvironment for model competitions. Independent variables are forecast as well\nas the dependent ones to mimic the real life cases best. Several accuracy\nmeasures are considered to compare model forecasting abilities. Results show\nthat complex models yield better forecasts. \n\n"}
{"id": "1311.6039", "contents": "Title: Variable density sampling with continuous trajectories. Application to\n  MRI Abstract: Reducing acquisition time is a crucial challenge for many imaging techniques.\nCompressed Sensing (CS) theory offers an appealing framework to address this\nissue since it provides theoretical guarantees on the reconstruction of sparse\nsignals by projection on a low dimensional linear subspace. In this paper, we\nfocus on a setting where the imaging device allows to sense a fixed set of\nmeasurements. We first discuss the choice of an optimal sampling subspace\n(smallest subset) allowing perfect reconstruction of sparse signals. Its\nstandard design relies on the random drawing of independent measurements. We\ndiscuss how to select the drawing distribution and show that a mixed strategy\ninvolving partial deterministic sampling and independent drawings can help\nbreaking the so-called \"coherence barrier\". Unfortunately, independent random\nsampling is irrelevant for many acquisition devices owing to acquisition\nconstraints. To overcome this limitation, the notion of Variable Density\nSamplers (VDS) is introduced and defined as a stochastic process with a\nprescribed limit empirical measure. It encompasses samplers based on\nindependent measurements or continuous curves. The latter are crucial to extend\nCS results to actual applications. Our main contribution lies in two original\ncontinuous VDS. The first one relies on random walks over the acquisition space\nwhereas the second one is heuristically driven and rests on the approximate\nsolution of a Traveling Salesman Problem. Theoretical analysis and\nretrospective CS simulations in magnetic resonance imaging highlight that the\nTSP-based solution provides improved reconstructed images in terms of\nsignal-to-noise ratio compared to standard sampling schemes (spiral, radial, 3D\niid...). \n\n"}
{"id": "1312.0825", "contents": "Title: FRANTIC: A Fast Reference-based Algorithm for Network Tomography via\n  Compressive Sensing Abstract: We study the problem of link and node delay estimation in undirected networks\nwhen at most k out of n links or nodes in the network are congested. Our\napproach relies on end-to-end measurements of path delays across pre-specified\npaths in the network. We present a class of algorithms that we call FRANTIC.\nThe FRANTIC algorithms are motivated by compressive sensing; however, unlike\ntraditional compressive sensing, the measurement design here is constrained by\nthe network topology and the matrix entries are constrained to be positive\nintegers. A key component of our design is a new compressive sensing algorithm\nSHO-FA-INT that is related to the prior SHO-FA algorithm for compressive\nsensing, but unlike SHO-FA, the matrix entries here are drawn from the set of\nintegers {0, 1, ..., M}. We show that O(k log n /log M) measurements suffice\nboth for SHO-FA-INT and FRANTIC. Further, we show that the computational\ncomplexity of decoding is also O(k log n/log M) for each of these algorithms.\nFinally, we look at efficient constructions of the measurement operations\nthrough Steiner Trees. \n\n"}
{"id": "1312.0919", "contents": "Title: Hierarchical Reverberation Mapping Abstract: Reverberation mapping (RM) is an important technique in studies of active\ngalactic nuclei (AGN). The key idea of RM is to measure the time lag $\\tau$\nbetween variations in the continuum emission from the accretion disc and\nsubsequent response of the broad line region (BLR). The measurement of $\\tau$\nis typically used to estimate the physical size of the BLR and is combined with\nother measurements to estimate the black hole mass $M_{\\rm BH}$. A major\ndifficulty with RM campaigns is the large amount of data needed to measure\n$\\tau$. Recently, Fine et al (2012) introduced a new approach to RM where the\nBLR light curve is sparsely sampled, but this is counteracted by observing a\nlarge sample of AGN, rather than a single system. The results are combined to\ninfer properties of the sample of AGN. In this letter we implement this method\nusing a hierarchical Bayesian model and contrast this with the results from the\nprevious stacked cross-correlation technique. We find that our inferences are\nmore precise and allow for more straightforward interpretation than the stacked\ncross-correlation results. \n\n"}
{"id": "1312.2923", "contents": "Title: Lagrangian Time Series Models for Ocean Surface Drifter Trajectories Abstract: This paper proposes stochastic models for the analysis of ocean surface\ntrajectories obtained from freely-drifting satellite-tracked instruments. The\nproposed time series models are used to summarise large multivariate datasets\nand infer important physical parameters of inertial oscillations and other\nocean processes. Nonstationary time series methods are employed to account for\nthe spatiotemporal variability of each trajectory. Because the datasets are\nlarge, we construct computationally efficient methods through the use of\nfrequency-domain modelling and estimation, with the data expressed as\ncomplex-valued time series. We detail how practical issues related to sampling\nand model misspecification may be addressed using semi-parametric techniques\nfor time series, and we demonstrate the effectiveness of our stochastic models\nthrough application to both real-world data and to numerical model output. \n\n"}
{"id": "1312.6443", "contents": "Title: Global inequality in energy consumption from 1980 to 2010 Abstract: We study the global probability distribution of energy consumption per capita\naround the world using data from the U.S. Energy Information Administration\n(EIA) for 1980-2010. We find that the Lorenz curves have moved up during this\ntime period, and the Gini coefficient G has decreased from 0.66 in 1980 to 0.55\nin 2010, indicating a decrease in inequality. The global probability\ndistribution of energy consumption per capita in 2010 is close to the\nexponential distribution with G=0.5. We attribute this result to the\nglobalization of the world economy, which mixes the world and brings it closer\nto the state of maximal entropy. We argue that global energy production is a\nlimited resource that is partitioned among the world population. The most\nprobable partition is the one that maximizes entropy, thus resulting in the\nexponential distribution function. A consequence of the latter is the law of\n1/3: the top 1/3 of the world population consumes 2/3 of produced energy. We\nalso find similar results for the global probability distribution of CO2\nemissions per capita. \n\n"}
{"id": "1312.7614", "contents": "Title: Inference on causal and structural parameters using many moment\n  inequalities Abstract: This paper considers the problem of testing many moment inequalities where\nthe number of moment inequalities, denoted by $p$, is possibly much larger than\nthe sample size $n$. There is a variety of economic applications where solving\nthis problem allows to carry out inference on causal and structural parameters,\na notable example is the market structure model of Ciliberto and Tamer (2009)\nwhere $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter\nthe market. We consider the test statistic given by the maximum of $p$\nStudentized (or $t$-type) inequality-specific statistics, and analyze various\nways to compute critical values for the test statistic. Specifically, we\nconsider critical values based upon (i) the union bound combined with a\nmoderate deviation inequality for self-normalized sums, (ii) the multiplier and\nempirical bootstraps, and (iii) two-step and three-step variants of (i) and\n(ii) by incorporating the selection of uninformative inequalities that are far\nfrom being binding and a novel selection of weakly informative inequalities\nthat are potentially binding but do not provide first order information. We\nprove validity of these methods, showing that under mild conditions, they lead\nto tests with the error in size decreasing polynomially in $n$ while allowing\nfor $p$ being much larger than $n$, indeed $p$ can be of order $\\exp (n^{c})$\nfor some $c > 0$. Importantly, all these results hold without any restriction\non the correlation structure between $p$ Studentized statistics, and also hold\nuniformly with respect to suitably large classes of underlying distributions.\nMoreover, in the online supplement, we show validity of a test based on the\nblock multiplier bootstrap in the case of dependent data under some general\nmixing conditions. \n\n"}
{"id": "1401.1671", "contents": "Title: Distributed Energy Efficient Channel Allocation Abstract: Design of energy efficient protocols for modern wireless systems has become\nan important area of research. In this paper, we propose a distributed\noptimization algorithm for the channel assignment problem for multiple\ninterfering transceiver pairs that cannot communicate with each other. We first\nmodify the auction algorithm for maximal energy efficiency and show that the\nproblem can be solved without explicit message passing using the carrier sense\nmultiple access (CSMA) protocols. We then develop a novel scheme by converting\nthe channel assignment problem into perfect matchings on bipartite graphs. The\nproposed scheme improves the energy efficiency and does not require any\nexplicit message passing or a shared memory between the users. We derive bounds\non the convergence rate and show that the proposed algorithm converges faster\nthan the distributed auction algorithm and achieves near-optimal performance\nunder Rayleigh fading channels. We also present an asymptotic performance\nanalysis of the fast matching algorithm for energy efficient resource\nallocation and prove the optimality for large enough number of users and number\nof channels. Finally, we provide numerical assessments that confirm the energy\nefficiency gains compared to the state of the art. \n\n"}
{"id": "1401.2293", "contents": "Title: Discussion of \"Estimating the historical and future probabilities of\n  large terrorist events\" by Aaron Clauset and Ryan Woodard Abstract: Discussion of \"Estimating the historical and future probabilities of large\nterrorist events\" by Aaron Clauset and Ryan Woodard [arXiv:1209.0089]. \n\n"}
{"id": "1401.2728", "contents": "Title: A semiparametric approach to mixed outcome latent variable models:\n  Estimating the association between cognition and regional brain volumes Abstract: Multivariate data that combine binary, categorical, count and continuous\noutcomes are common in the social and health sciences. We propose a\nsemiparametric Bayesian latent variable model for multivariate data of\narbitrary type that does not require specification of conditional\ndistributions. Drawing on the extended rank likelihood method by Hoff [Ann.\nAppl. Stat. 1 (2007) 265-283], we develop a semiparametric approach for latent\nvariable modeling with mixed outcomes and propose associated Markov chain Monte\nCarlo estimation methods. Motivated by cognitive testing data, we focus on\nbifactor models, a special case of factor analysis. We employ our\nsemiparametric Bayesian latent variable model to investigate the association\nbetween cognitive outcomes and MRI-measured regional brain volumes. \n\n"}
{"id": "1401.2756", "contents": "Title: Multi-way blockmodels for analyzing coordinated high-dimensional\n  responses Abstract: We consider the problem of quantifying temporal coordination between multiple\nhigh-dimensional responses. We introduce a family of multi-way stochastic\nblockmodels suited for this problem, which avoids preprocessing steps such as\nbinning and thresholding commonly adopted for this type of data, in biology. We\ndevelop two inference procedures based on collapsed Gibbs sampling and\nvariational methods. We provide a thorough evaluation of the proposed methods\non simulated data, in terms of membership and blockmodel estimation,\npredictions out-of-sample and run-time. We also quantify the effects of\ncensoring procedures such as binning and thresholding on the estimation tasks.\nWe use these models to carry out an empirical analysis of the functional\nmechanisms driving the coordination between gene expression and metabolite\nconcentrations during carbon and nitrogen starvation, in S. cerevisiae. \n\n"}
{"id": "1401.3211", "contents": "Title: Modeling Light Curves for Improved Classification Abstract: Many synoptic surveys are observing large parts of the sky multiple times.\nThe resulting lightcurves provide a wonderful window to the dynamic nature of\nthe universe. However, there are many significant challenges in analyzing these\nlight curves. These include heterogeneity of the data, irregularly sampled\ndata, missing data, censored data, known but variable measurement errors, and\nmost importantly, the need to classify in astronomical objects in real time\nusing these imperfect light curves. We describe a modeling-based approach using\nGaussian process regression for generating critical measures representing\nfeatures for the classification of such lightcurves. We demonstrate that our\napproach performs better by comparing it with past methods. Finally, we provide\nfuture directions for use in sky-surveys that are getting even bigger by the\nday. \n\n"}
{"id": "1401.6683", "contents": "Title: Resource Allocation Under Channel Uncertainties for Relay-Aided\n  Device-to-Device Communication Underlaying LTE-A Cellular Networks Abstract: Device-to-device (D2D) communication in cellular networks allows direct\ntransmission between two cellular devices with local communication needs. Due\nto the increasing number of autonomous heterogeneous devices in future mobile\nnetworks, an efficient resource allocation scheme is required to maximize\nnetwork throughput and achieve higher spectral efficiency. In this paper,\nperformance of network-integrated D2D communication under channel uncertainties\nis investigated where D2D traffic is carried through relay nodes. Considering a\nmulti-user and multi-relay network, we propose a robust distributed solution\nfor resource allocation with a view to maximizing network sum-rate when the\ninterference from other relay nodes and the link gains are uncertain. An\noptimization problem is formulated for allocating radio resources at the relays\nto maximize end-to-end rate as well as satisfy the quality-of-service (QoS)\nrequirements for cellular and D2D user equipments under total power constraint.\nEach of the uncertain parameters is modeled by a bounded distance between its\nestimated and bounded values. We show that the robust problem is convex and a\ngradient-aided dual decomposition algorithm is applied to allocate radio\nresources in a distributed manner. Finally, to reduce the cost of robustness\ndefined as the reduction of achievable sum-rate, we utilize the \\textit{chance\nconstraint approach} to achieve a trade-off between robustness and optimality.\nThe numerical results show that there is a distance threshold beyond which\nrelay-aided D2D communication significantly improves network performance when\ncompared to direct communication between D2D peers. \n\n"}
{"id": "1402.1801", "contents": "Title: Efficient Low Dose X-ray CT Reconstruction through Sparsity-Based MAP\n  Modeling Abstract: Ultra low radiation dose in X-ray Computed Tomography (CT) is an important\nclinical objective in order to minimize the risk of carcinogenesis. Compressed\nSensing (CS) enables significant reductions in radiation dose to be achieved by\nproducing diagnostic images from a limited number of CT projections. However,\nthe excessive computation time that conventional CS-based CT reconstruction\ntypically requires has limited clinical implementation. In this paper, we first\ndemonstrate that a thorough analysis of CT reconstruction through a Maximum a\nPosteriori objective function results in a weighted compressive sensing\nproblem. This analysis enables us to formulate a low dose fan beam and helical\ncone beam CT reconstruction. Subsequently, we provide an efficient solution to\nthe formulated CS problem based on a Fast Composite Splitting Algorithm-Latent\nExpected Maximization (FCSA-LEM) algorithm. In the proposed method we use\npseudo polar Fourier transform as the measurement matrix in order to decrease\nthe computational complexity; and rebinning of the projections to parallel rays\nin order to extend its application to fan beam and helical cone beam scans. The\nweight involved in the proposed weighted CS model, denoted by Error Adaptation\nWeight (EAW), is calculated based on the statistical characteristics of CT\nreconstruction and is a function of Poisson measurement noise and rebinning\ninterpolation error. Simulation results show that low computational complexity\nof the proposed method made the fast recovery of the CT images possible and\nusing EAW reduces the reconstruction error by one order of magnitude. Recovery\nof a high quality 512$\\times$ 512 image was achieved in less than 20 sec on a\ndesktop computer without numerical optimizations. \n\n"}
{"id": "1402.3890", "contents": "Title: Power laws in citation distributions: Evidence from Scopus Abstract: Modeling distributions of citations to scientific papers is crucial for\nunderstanding how science develops. However, there is a considerable empirical\ncontroversy on which statistical model fits the citation distributions best.\nThis paper is concerned with rigorous empirical detection of power-law\nbehaviour in the distribution of citations received by the most highly cited\nscientific papers. We have used a large, novel data set on citations to\nscientific papers published between 1998 and 2002 drawn from Scopus. The\npower-law model is compared with a number of alternative models using a\nlikelihood ratio test. We have found that the power-law hypothesis is rejected\nfor around half of the Scopus fields of science. For these fields of science,\nthe Yule, power-law with exponential cut-off and log-normal distributions seem\nto fit the data better than the pure power-law model. On the other hand, when\nthe power-law hypothesis is not rejected, it is usually empirically\nindistinguishable from most of the alternative models. The pure power-law model\nseems to be the best model only for the most highly cited papers in \"Physics\nand Astronomy\". Overall, our results seem to support theories implying that the\nmost highly cited scientific papers follow the Yule, power-law with exponential\ncut-off or log-normal distribution. Our findings suggest also that power laws\nin citation distributions, when present, account only for a very small fraction\nof the published papers (less than 1% for most of science fields) and that the\npower-law scaling parameter (exponent) is substantially higher (from around 3.2\nto around 4.7) than found in the older literature. \n\n"}
{"id": "1402.4393", "contents": "Title: Viruses and Fullerenes - Symmetry as a Common Thread? Abstract: We apply here the principle of affine symmetry to the nested fullerene cages\n(carbon onions) that arise in the context of carbon chemistry. Previous work on\naffine extensions of the icosahedral group has revealed a new organisational\nprinciple in virus structure and assembly. We adapt this group theoretic\nframework here to the physical requirements dictated by carbon chemistry, and\nshow that we can derive mathematical models for carbon onions within this\naffine symmetry approach. This suggests the applicability of affine symmetry in\na wider context in Nature, as well as offering a novel perspective on the\ngeometric principles underpinning carbon chemistry. \n\n"}
{"id": "1402.5196", "contents": "Title: Synchronization-Free Delay Tomography Based on Compressed Sensing Abstract: Delay tomography has so far burdened source and receiver measurement nodes in\na network with two requirements such as path establishment and clock\nsynchronization between them. In this letter, we focus on the clock\nsynchronization problem in delay tomography and propose a synchronization-free\ndelay tomography scheme. The proposed scheme selects a path between source and\nreceiver measurement nodes as a reference path, which results in a loss of\nequation in a conventional delay tomography problem. However, by utilizing\ncompressed sensing, the proposed scheme becomes robust to the loss. Simulation\nexperiments confirm that the proposed scheme works comparable to a conventional\ndelay tomography scheme in networks with no clock synchronization between\nsource and receiver measurement nodes. \n\n"}
{"id": "1402.5257", "contents": "Title: Conditional Multilevel Monte Carlo Simulation of Groundwater Flow in the\n  Culebra Dolomite at the Waste Isolation Pilot Plant (WIPP) Site Abstract: We extended the multilevel Monte of Carlo (MLMC) approach to simulation of\ngroundwater flow in porous media by incorporating direct measurements of medium\nproperties. Numerical simulations of Waste Isolation Pilot Plant (WIPP)\nrepository in southeastern New Mexico are performed to test the performance of\nthe conditional MLMC technique. The log-transmissivity of WIPP site is modeled\nas the conditional random fields which honor exact field values at a few\nlocations. The conditional random fields are generated through the modified\ncirculant embedding methods in (Dietrich and Newsam, 1996). We also study\neffects of a combination of the conditional MLMC accompanied by antithetic\nvariates. The main quantity of interest is the time of radionuclides travelling\nfrom the center of repository to the site boundary. Numerical examples are\npresented to demonstrate the cost-effectiveness of the multilevel approach in\ncomparison to the standard Monte Carlo (MC) simulation. \n\n"}
{"id": "1402.6951", "contents": "Title: Modeling the Complex Dynamics and Changing Correlations of Epileptic\n  Events Abstract: Patients with epilepsy can manifest short, sub-clinical epileptic \"bursts\" in\naddition to full-blown clinical seizures. We believe the relationship between\nthese two classes of events---something not previously studied\nquantitatively---could yield important insights into the nature and intrinsic\ndynamics of seizures. A goal of our work is to parse these complex epileptic\nevents into distinct dynamic regimes. A challenge posed by the intracranial EEG\n(iEEG) data we study is the fact that the number and placement of electrodes\ncan vary between patients. We develop a Bayesian nonparametric Markov switching\nprocess that allows for (i) shared dynamic regimes between a variable number of\nchannels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary\nof dynamic regimes. We encode a sparse and changing set of dependencies between\nthe channels using a Markov-switching Gaussian graphical model for the\ninnovations process driving the channel dynamics and demonstrate the importance\nof this model in parsing and out-of-sample predictions of iEEG data. We show\nthat our model produces intuitive state assignments that can help automate\nclinical analysis of seizures and enable the comparison of sub-clinical bursts\nand full clinical seizures. \n\n"}
{"id": "1402.7031", "contents": "Title: Energy transfer efficiency in the FMO complex strongly coupled to a\n  vibronic mode Abstract: Using methods of condensed matter and statistical physics, we examine the\ntransport of excitons through the Fenna-Matthews-Olson (FMO) complex from a\nreceiving antenna to a reaction center. Writing the equations of motion for the\nexciton creation/annihilation operators, we are able to describe the exciton\ndynamics, even in the regime when the reorganization energy is of the order of\nthe intra-system couplings. In particular, we obtain the well-known quantum\noscillations of the site populations. We determine the exciton transfer\nefficiency in the presence of a quenching field and protein environment. While\nthe majority of the protein vibronic modes are treated as a heat bath, we\naddress the situation when specific modes are strongly coupled to excitons and\nexamine the effects of these modes on the quantum oscillations and the energy\ntransfer efficiency. We find that, for the vibronic frequencies below 16 meV,\nthe exciton transfer is drastically suppressed. We attribute this effect to the\nformation of \"polaronic states\" where the exciton is transferred back and forth\nbetween the two pigments with the absorption/emission of the vibronic quanta,\ninstead of proceeding to the reaction center. The same effect suppresses the\nquantum beating at the vibronic frequency of 25 meV. We also show that the\nefficiency of the energy transfer can be enhanced when the vibronic mode\nstrongly couples to the third pigment only, instead of coupling to the entire\nsystem. \n\n"}
{"id": "1402.7079", "contents": "Title: Type Ia Supernova Colors and Ejecta Velocities: Hierarchical Bayesian\n  Regression with Non-Gaussian Distributions Abstract: We investigate the statistical dependence of the peak intrinsic colors of\nType Ia supernovae (SN Ia) on their expansion velocities at maximum light,\nmeasured from the Si II 6355 spectral feature. We construct a new hierarchical\nBayesian regression model, accounting for the random effects of intrinsic\nscatter, measurement error, and reddening by host galaxy dust, and implement a\nGibbs sampler and deviance information criteria to estimate the correlation.\nThe method is applied to the apparent colors from BVRI light curves and Si II\nvelocity data for 79 nearby SNe Ia. The apparent color distributions of high\n(HV) and normal velocity (NV) supernovae exhibit significant discrepancies for\nB-V and B-R, but not other colors. Hence, they are likely due to intrinsic\ncolor differences originating in the B-band, rather than dust reddening. The\nmean intrinsic B-V and B-R color differences between HV and NV groups are 0.06\n+/- 0.02 and 0.09 +/- 0.02 mag, respectively. A linear model finds significant\nslopes of -0.021 +/- 0.006 and -0.030 +/- 0.009 mag/(1000 km/s) for intrinsic\nB-V and B-R colors versus velocity, respectively. Since the ejecta velocity\ndistribution is skewed towards high velocities, these effects imply\nnon-Gaussian intrinsic color distributions with skewness up to +0.3. Accounting\nfor the intrinsic color-velocity correlation results in corrections to A_V\nextinction estimates as large as -0.12 mag for HV SNe Ia and +0.06 mag for NV\nevents. Velocity measurements from SN Ia spectra have potential to diminish\nsystematic errors from the confounding of intrinsic colors and dust reddening\naffecting supernova distances. \n\n"}
{"id": "1403.0173", "contents": "Title: Coordinated Direct and Relay Schemes for Two-Hop Communication in VANETS Abstract: In order to accommodate increasing need and offer communication with high\nperformance, both vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V)\ncommunications are exploited. The advantages of static nodes and vehicular\nnodes are combined to achieve an optimal routing scheme. In this paper, we\nconsider the communications between a static node and the vehicular nodes\nmoving in an adjacent area of it. The adjacent area is defined as the zone\nwhere a vehicular can communicate with the static node within maximum two hops.\nWe only consider single-hop and two-hop transmissions because these\ntransmissions can be considered as building blocks to construct transmissions\nwith a higher number of hops. Different cases in which an uplink or a downlink\nfor the two-hop user combined with an uplink or a downlink for the single-hop\nuser correspond to different CDR schemes. Using side information to\nintentionally cancel the interference, Network Coding (NC), CDR, overhearing\nand multi-way schemes aggregate communications flows in order to increase the\nperformance of the network. We apply the mentioned schemes to a V2I network and\npropose novel schemes to optimally arrange and combine the transmissions. \n\n"}
{"id": "1403.0289", "contents": "Title: Blind and fully constrained unmixing of hyperspectral images Abstract: This paper addresses the problem of blind and fully constrained unmixing of\nhyperspectral images. Unmixing is performed without the use of any dictionary,\nand assumes that the number of constituent materials in the scene and their\nspectral signatures are unknown. The estimated abundances satisfy the desired\nsum-to-one and nonnegativity constraints. Two models with increasing complexity\nare developed to achieve this challenging task, depending on how noise\ninteracts with hyperspectral data. The first one leads to a convex optimization\nproblem, and is solved with the Alternating Direction Method of Multipliers.\nThe second one accounts for signal-dependent noise, and is addressed with a\nReweighted Least Squares algorithm. Experiments on synthetic and real data\ndemonstrate the effectiveness of our approach. \n\n"}
{"id": "1403.1057", "contents": "Title: Use of spatial cross correlation function to study formation mechanism\n  of massive elliptical galaxies Abstract: Spatial clustering nature of galaxies have been studied previously through\nauto correlation function. The same type of cross correlation function has been\nused to investigate parametric clustering nature of galaxies e.g. with respect\nto masses and sizes of galaxies.\n  Here formation and evolution of several components of nearby massive early\ntype galaxies have been envisaged through cross correlations, in the mass-size\nparametric plane, with high redshift early type galaxies (hereafter ETG).It is\nfound that the inner most components of nearby ETGs have significant\ncorrelation with ETGs in the highest redshift range called red nuggets whereas\nintermediate components are highly correlated with ETGs in the redshift range\nwith z value greater than 0.5 and less than 0.75. The outer most part has no\ncorrelation in any range, suggesting a scenario through in situ accretion. The\nabove formation scenario is consistent with the previous results obtained for\nNGC5128 (Chattopadhyay et al. (2009), Chattopadhyay et al. (2013)) and to some\nextent for nearby elliptical galaxies (Huang et al. (2013)) after considering a\nsample of ETGs at high redshift with stellar masses greater than or equal to\n108.73 M-Sun. So the present work indicates a three phase formation of massive\nnearby elliptical galaxies instead of two as discussed in previous works. \n\n"}
{"id": "1403.3363", "contents": "Title: Control of chemical wave propagation Abstract: Using the Schl\\\"{o}gl model as a paradigmatic example of a bistable\nreaction-diffusion system, we discuss some physically feasible options of open\nand closed loop spatio-temporal control of chemical wave propagation. \n\n"}
{"id": "1403.5539", "contents": "Title: Pick and freeze estimation of sensitivity indices for models with\n  dependent and dynamic input processes Abstract: This paper addresses sensitivity analysis for dynamic models, linking\ndependent inputs to observed outputs. The usual method to estimate Sobol\nindices are based on the independence of input variables. We present a method\nto overpass this constraint when inputs are Gaussian processes of high\ndimension in a time related framework. Our proposition leads to a\ngeneralization of Sobol indices when inputs are both dependant and dynamic. The\nmethod of estimation is a modification of the Pick and Freeze simulation\nscheme. First we study the general Gaussian cases and secondly we detail the\ncase of stationary models. We then apply the results to an example of heat\nexchanges inside a building. \n\n"}
{"id": "1403.5977", "contents": "Title: On the convergence of Maronna's $M$-estimators of scatter Abstract: In this paper, {we propose an alternative proof for the uniqueness} of\nMaronna's $M$-estimator of scatter (Maronna, 1976) for $N$ vector observations\n$\\mathbf y_1,...,\\mathbf y_N\\in\\mathbb R^m$ under a mild constraint of linear\nindependence of any subset of $m$ of these vectors. This entails in particular\nalmost sure uniqueness for random vectors $\\mathbf y_i$ with a density as long\nas $N>m$. {This approach allows to establish further relations that demonstrate\nthat a properly normalized Tyler's $M$-estimator of scatter (Tyler, 1987) can\nbe considered as a limit of Maronna's $M$-estimator. More precisely, the\ncontribution is to show that each $M$-estimator converges towards a particular\nTyler's $M$-estimator.} These results find important implications in recent\nworks on the large dimensional (random matrix) regime of robust $M$-estimation. \n\n"}
{"id": "1403.5997", "contents": "Title: Bayesian calibration for forensic evidence reporting Abstract: We introduce a Bayesian solution for the problem in forensic speaker\nrecognition, where there may be very little background material for estimating\nscore calibration parameters. We work within the Bayesian paradigm of evidence\nreporting and develop a principled probabilistic treatment of the problem,\nwhich results in a Bayesian likelihood-ratio as the vehicle for reporting\nweight of evidence. We show in contrast, that reporting a likelihood-ratio\ndistribution does not solve this problem. Our solution is experimentally\nexercised on a simulated forensic scenario, using NIST SRE'12 scores, which\ndemonstrates a clear advantage for the proposed method compared to the\ntraditional plugin calibration recipe. \n\n"}
{"id": "1403.7274", "contents": "Title: Bias Correction in Species Distribution Models: Pooling Survey and\n  Collection Data for Multiple Species Abstract: Presence-only records may provide data on the distributions of rare species,\nbut commonly suffer from large, unknown biases due to their typically haphazard\ncollection schemes. Presence-absence or count data collected in systematic,\nplanned surveys are more reliable but typically less abundant.\n  We proposed a probabilistic model to allow for joint analysis of\npresence-only and survey data to exploit their complementary strengths. Our\nmethod pools presence-only and presence-absence data for many species and\nmaximizes a joint likelihood, simultaneously estimating and adjusting for the\nsampling bias affecting the presence-only data. By assuming that the sampling\nbias is the same for all species, we can borrow strength across species to\nefficiently estimate the bias and improve our inference from presence-only\ndata.\n  We evaluate our model's performance on data for 36 eucalypt species in\nsoutheastern Australia. We find that presence-only records exhibit a strong\nsampling bias toward the coast and toward Sydney, the largest city. Our\ndata-pooling technique substantially improves the out-of-sample predictive\nperformance of our model when the amount of available presence-absence data for\na given species is scarce.\n  If we have only presence-only data and no presence-absence data for a given\nspecies, but both types of data for several other species that suffer from the\nsame spatial sampling bias, then our method can obtain an unbiased estimate of\nthe first species' geographic range. \n\n"}
{"id": "1403.7548", "contents": "Title: Functional Data Analysis of Aging Curves in Sports Abstract: It is well known that athletic and physical condition is affected by age.\nPlotting an individual athlete's performance against age creates a graph\ncommonly called the player's aging curve. Despite the obvious interest to\ncoaches and managers, the analysis of aging curves so far has used fairly\nrudimentary techniques. In this paper, we introduce functional data analysis\n(FDA) to the study of aging curves in sports and argue that it is both more\ngeneral and more flexible compared to the methods that have previously been\nused. We also illustrate the rich analysis that is possible by analyzing data\nfor NBA and MLB players. In the analysis of MLB data, we use functional\nprincipal components analysis (fPCA) to perform functional hypothesis testing\nand show differences in aging curves between potential power hitters and\npotential non-power hitters. The analysis of aging curves in NBA players\nillustrates the use of the PACE method. We show that there are three distinct\naging patterns among NBA players and that player scoring ability differs across\nthe patterns. We also show that aging pattern is independent of position. \n\n"}
{"id": "1403.7642", "contents": "Title: The Sensitivity of College Football Rankings to Several Modeling Choices Abstract: This paper proposes a multiple-membership generalized linear mixed model for\nranking college football teams using only their win/loss records. The model\nresults in an intractable, high-dimensional integral due to the random effects\nstructure and nonlinear link function. We use recent data sets to explore the\neffect of the choice of integral approximation and other modeling assumptions\non the rankings. Varying the modeling assumptions sometimes leads to changes in\nthe team rankings that could affect bowl assignments. \n\n"}
{"id": "1404.3878", "contents": "Title: NILMTK: An Open Source Toolkit for Non-intrusive Load Monitoring Abstract: Non-intrusive load monitoring, or energy disaggregation, aims to separate\nhousehold energy consumption data collected from a single point of measurement\ninto appliance-level consumption data. In recent years, the field has rapidly\nexpanded due to increased interest as national deployments of smart meters have\nbegun in many countries. However, empirically comparing disaggregation\nalgorithms is currently virtually impossible. This is due to the different data\nsets used, the lack of reference implementations of these algorithms and the\nvariety of accuracy metrics employed. To address this challenge, we present the\nNon-intrusive Load Monitoring Toolkit (NILMTK); an open source toolkit designed\nspecifically to enable the comparison of energy disaggregation algorithms in a\nreproducible manner. This work is the first research to compare multiple\ndisaggregation approaches across multiple publicly available data sets. Our\ntoolkit includes parsers for a range of existing data sets, a collection of\npreprocessing algorithms, a set of statistics for describing data sets, two\nreference benchmark disaggregation algorithms and a suite of accuracy metrics.\nWe demonstrate the range of reproducible analyses which are made possible by\nour toolkit, including the analysis of six publicly available data sets and the\nevaluation of both benchmark disaggregation algorithms across such data sets. \n\n"}
{"id": "1404.6473", "contents": "Title: Quantifying Uncertainty in Random Forests via Confidence Intervals and\n  Hypothesis Tests Abstract: This work develops formal statistical inference procedures for machine\nlearning ensemble methods. Ensemble methods based on bootstrapping, such as\nbagging and random forests, have improved the predictive accuracy of individual\ntrees, but fail to provide a framework in which distributional results can be\neasily determined. Instead of aggregating full bootstrap samples, we consider\npredicting by averaging over trees built on subsamples of the training set and\ndemonstrate that the resulting estimator takes the form of a U-statistic. As\nsuch, predictions for individual feature vectors are asymptotically normal,\nallowing for confidence intervals to accompany predictions. In practice, a\nsubset of subsamples is used for computational speed; here our estimators take\nthe form of incomplete U-statistics and equivalent results are derived. We\nfurther demonstrate that this setup provides a framework for testing the\nsignificance of features. Moreover, the internal estimation method we develop\nallows us to estimate the variance parameters and perform these inference\nprocedures at no additional computational cost. Simulations and illustrations\non a real dataset are provided. \n\n"}
{"id": "1405.0089", "contents": "Title: Performance Modeling of Next-Generation Wireless Networks Abstract: The industry is satisfying the increasing demand for wireless bandwidth by\ndensely deploying a large number of access points which are centrally managed,\ne.g. enterprise WiFi networks deployed in university campuses, companies,\nairports etc. This small cell architecture is gaining traction in the cellular\nworld as well, as witnessed by the direction in which 4G+ and 5G\nstandardization is moving. Prior academic work in analyzing such large-scale\nwireless networks either uses oversimplified models for the physical layer, or\nignores other important, real-world aspects of the problem, like MAC layer\nconsiderations, topology characteristics, and protocol overhead. On the other\nhand, the industry is using for deployment purposes on-site surveys and\nsimulation tools which do not scale, cannot efficiently optimize the design of\nsuch a network, and do not explain why one design choice is better than\nanother. In this paper we introduce a simple yet accurate analytical model\nwhich combines the realism and practicality of industrial simulation tools with\nthe ability to scale, analyze the effect of various design parameters, and\noptimize the performance of real- world deployments. The model takes into\naccount all central system parameters, including channelization, power\nallocation, user scheduling, load balancing, MAC, advanced PHY techniques\n(single and multi user MIMO as well as cooperative transmission from multiple\naccess points), topological characteristics and protocol overhead. The accuracy\nof the model is verified via extensive simulations and the model is used to\nstudy a wide range of real world scenarios, providing design guidelines on the\neffect of various design parameters on performance. \n\n"}
{"id": "1405.0203", "contents": "Title: Binary Fading Interference Channel with No CSIT Abstract: We study the capacity region of the two-user Binary Fading (or Erasure)\nInterference Channel where the transmitters have no knowledge of the channel\nstate information. We develop new inner-bounds and outer-bounds for this\nproblem. We identify three regimes based on the channel parameters: weak,\nmoderate, and strong interference regimes. Interestingly, this is similar to\nthe generalized degrees of freedom of the two-user Gaussian interference\nchannel where transmitters have perfect channel knowledge. We show that for the\nweak interference regime, treating interference as erasure is optimal while for\nthe strong interference regime, decoding interference is optimal. For the\nmoderate interference regime, we provide new inner and outer bounds. The\ninner-bound is based on a modification of the Han-Kobayashi scheme for the\nerasure channel, enhanced by time-sharing. We study the gap between our\ninner-bound and our outer-bounds for the moderate interference regime and\ncompare our results to that of the Gaussian interference channel. Deriving our\nnew outer-bounds has three main steps. We first create a contracted channel\nthat has fewer states compared to the original channel, in order to make the\nanalysis tractable. We then prove the Correlation Lemma that shows an\nouter-bound on the capacity region of the contracted channel also serves as an\nouter-bound for the original channel. Finally using the Conditional Entropy\nLeakage Lemma, we derive our outer-bound on the capacity region of the\ncontracted channel. \n\n"}
{"id": "1405.1963", "contents": "Title: Distributed Interference-Aware Energy-Efficient Resource Allocation for\n  Device-to-Device Communications Underlaying Cellular Networks Abstract: The introduction of device-to-device (D2D) into cellular networks poses many\nnew challenges in the resource allocation design due to the co-channel\ninterference caused by spectrum reuse and limited battery life of user\nequipments (UEs). In this paper, we propose a distributed interference-aware\nenergy-efficient resource allocation algorithm to maximize each UE's energy\nefficiency (EE) subject to its specific quality of service (QoS) and maximum\ntransmission power constraints. We model the resource allocation problem as a\nnoncooperative game, in which each player is self-interested and wants to\nmaximize its own EE. The formulated EE maximization problem is a non-convex\nproblem and is transformed into a convex optimization problem by exploiting the\nproperties of the nonlinear fractional programming. An iterative optimization\nalgorithm is proposed and verified through computer simulations. \n\n"}
{"id": "1405.2492", "contents": "Title: Unified and Distributed QoS-Driven Cell Association Algorithms in\n  Heterogeneous Networks Abstract: This paper addresses the cell association problem in the downlink of a\nmulti-tier heterogeneous network (HetNet), where base stations (BSs) have\nfinite number of resource blocks (RBs) available to distribute among their\nassociated users. Two problems are defined and treated in this paper: sum\nutility of long term rate maximization with long term rate quality of service\n(QoS) constraints, and global outage probability minimization with outage QoS\nconstraints. The first problem is well-suited for low mobility environments,\nwhile the second problem provides a framework to deal with environments with\nfast fading. The defined optimization problems in this paper are solved in two\nphases: cell association phase followed by the optional RB distribution phase.\nWe show that the cell association phase of both problems have the same\nstructure. Based on this similarity, we propose a unified distributed algorithm\nwith low levels of message passing to for the cell association phase. This\ndistributed algorithm is derived by relaxing the association constraints and\nusing Lagrange dual decomposition method. In the RB distribution phase, the\nremaining RBs after the cell association phase are distributed among the users.\nSimulation results show the superiority of our distributed cell association\nscheme compared to schemes that are based on maximum signal to interference\nplus noise ratio (SINR). \n\n"}
{"id": "1405.2566", "contents": "Title: Learning modular structures from network data and node variables Abstract: A standard technique for understanding underlying dependency structures among\na set of variables posits a shared conditional probability distribution for the\nvariables measured on individuals within a group. This approach is often\nreferred to as module networks, where individuals are represented by nodes in a\nnetwork, groups are termed modules, and the focus is on estimating the network\nstructure among modules. However, estimation solely from node-specific\nvariables can lead to spurious dependencies, and unverifiable structural\nassumptions are often used for regularization. Here, we propose an extended\nmodel that leverages direct observations about the network in addition to\nnode-specific variables. By integrating complementary data types, we avoid the\nneed for structural assumptions. We illustrate theoretical and practical\nsignificance of the model and develop a reversible-jump MCMC learning procedure\nfor learning modules and model parameters. We demonstrate the method accuracy\nin predicting modular structures from synthetic data and capability to learn\ninfluence structures in twitter data and regulatory modules in the\nMycobacterium tuberculosis gene regulatory network. \n\n"}
{"id": "1405.2623", "contents": "Title: Numerical Evidence for Robustness of Environment-Assisted Quantum\n  Transport Abstract: Recent theoretical studies show that decoherence process can enhance\ntransport efficiency in quantum systems. This effect is known as\nenvironment-assisted quantum transport (ENAQT). The role of ENAQT in optimal\nquantum transport is well investigated, however, it is less known how robust\nENAQT is with respect to variations in the system or its environment\ncharacteristic. Toward answering this question, we simulated excitonic energy\ntransfer in Fenna-Matthews-Olson (FMO) photosynthetic complex. We found that\nENAQT is robust with respect to many relevant parameters of environmental\ninteractions and Frenkel-exciton Hamiltonian including reorganization energy,\nbath frequency cutoff, temperature, and initial excitations, dissipation rate,\ntrapping rate, disorders, and dipole moments orientations. Our study suggests\nthat the ENAQT phenomenon can be exploited in robust design of highly efficient\nquantum transport systems. \n\n"}
{"id": "1405.2673", "contents": "Title: Particle MCMC for Bayesian Microwave Control Abstract: We consider the problem of local radioelectric property estimation from\nglobal electromagnetic scattering measurements. This challenging ill-posed high\ndimensional inverse problem can be explored by intensive computations of a\nparallel Maxwell solver on a petaflopic supercomputer. Then, it is shown how\nBayesian inference can be perfomed with a Particle Marginal Metropolis-Hastings\n(PMMH) approach, which includes a Rao-Blackwellised Sequential Monte Carlo\nalgorithm with interacting Kalman filters. Material properties, including a\nmultiple components \"Debye relaxation\"/\"Lorenzian resonant\" material model, are\nestimated; it is illustrated on synthetic data. Eventually, we propose\ndifferent ways to deal with higher dimensional problems, from parallelization\nto the original introduction of efficient sequential data assimilation\ntechniques, widely used in weather forecasting, oceanography, geophysics, etc. \n\n"}
{"id": "1405.2876", "contents": "Title: Location-Aware Cross-Tier Coordinated Multipoint Transmission in\n  Two-Tier Cellular Networks Abstract: Multi-tier cellular networks are considered as an effective solution to\nenhance the coverage and data rate offered by cellular systems. In a multi-tier\nnetwork, high power base stations (BSs) such as macro BSs are overlaid by lower\npower small cells such as femtocells and/or picocells. However, co-channel\ndeployment of multiple tiers of BSs gives rise to the problem of cross-tier\ninterference that significantly impacts the performance of wireless networks.\nMulticell cooperation techniques, such as coordinated multipoint (CoMP)\ntransmission, have been proposed as a promising solution to mitigate the impact\nof the cross-tier interference in multi-tier networks. In this paper, we\npropose a novel scheme for Location-Aware Cross-Tier Cooperation (LA-CTC)\nbetween BSs in different tiers for downlink CoMP transmission in two-tier\ncellular networks. On one hand, the proposed scheme only uses CoMP transmission\nto enhance the performance of the users who suffer from high cross-tier\ninterference due to the co-channel deployment of small cells such as picocells.\nOn the other hand, users with good signal-to-interference-plus-noise ratio\n(${\\rm SINR}$) conditions are served directly by a single BS from any of the\ntwo tiers. Thus, the data exchange between the cooperating BSs over the\nbackhaul network can be reduced when compared to the traditional CoMP\ntransmission scheme. We use tools from stochastic geometry to quantify the\nperformance gains obtained by using the proposed scheme in terms of outage\nprobability, achievable data rate, and load per BS. We compare the performance\nof the proposed scheme with that of other schemes in the literature such as the\nschemes which use cooperation to serve all users and schemes that use range\nexpansion to offload users to the small cell tier. \n\n"}
{"id": "1405.3241", "contents": "Title: Spectral gene set enrichment (SGSE) Abstract: Motivation: Gene set testing is typically performed in a supervised context\nto quantify the association between groups of genes and a clinical phenotype.\nIn many cases, however, a gene set-based interpretation of genomic data is\ndesired in the absence of a phenotype variable. Although methods exist for\nunsupervised gene set testing, they predominantly compute enrichment relative\nto clusters of the genomic variables with performance strongly dependent on the\nclustering algorithm and number of clusters. Results: We propose a novel\nmethod, spectral gene set enrichment (SGSE), for unsupervised competitive\ntesting of the association between gene sets and empirical data sources. SGSE\nfirst computes the statistical association between gene sets and principal\ncomponents (PCs) using our principal component gene set enrichment (PCGSE)\nmethod. The overall statistical association between each gene set and the\nspectral structure of the data is then computed by combining the PC-level\np-values using the weighted Z-method with weights set to the PC variance scaled\nby Tracey-Widom test p-values. Using simulated data, we show that the SGSE\nalgorithm can accurately recover spectral features from noisy data. To\nillustrate the utility of our method on real data, we demonstrate the superior\nperformance of the SGSE method relative to standard cluster-based techniques\nfor testing the association between MSigDB gene sets and the variance structure\nof microarray gene expression data. Availability:\nhttp://cran.r-project.org/web/packages/PCGSE/index.html Contact:\nrob.frost@dartmouth.edu or jason.h.moore@dartmouth.edu \n\n"}
{"id": "1405.4001", "contents": "Title: Thermodynamics of statistical inference by cells Abstract: The deep connection between thermodynamics, computation, and information is\nnow well established both theoretically and experimentally. Here, we extend\nthese ideas to show that thermodynamics also places fundamental constraints on\nstatistical estimation and learning. To do so, we investigate the constraints\nplaced by (nonequilibrium) thermodynamics on the ability of biochemical\nsignaling networks within cells to estimate the concentration of an external\nsignal. We show that accuracy is limited by energy consumption, suggesting that\nthere are fundamental thermodynamic constraints on statistical inference. \n\n"}
{"id": "1405.4081", "contents": "Title: Sequential Monte Carlo with Highly Informative Observations Abstract: We propose sequential Monte Carlo (SMC) methods for sampling the posterior\ndistribution of state-space models under highly informative observation\nregimes, a situation in which standard SMC methods can perform poorly. A\nspecial case is simulating bridges between given initial and final values. The\nbasic idea is to introduce a schedule of intermediate weighting and resampling\ntimes between observation times, which guide particles towards the final state.\nThis can always be done for continuous-time models, and may be done for\ndiscrete-time models under sparse observation regimes; our main focus is on\ncontinuous-time diffusion processes. The methods are broadly applicable in that\nthey support multivariate models with partial observation, do not require\nsimulation of the backward transition (which is often unavailable), and, where\npossible, avoid pointwise evaluation of the forward transition. When simulating\nbridges, the last cannot be avoided entirely without concessions, and we\nsuggest an epsilon-ball approach (reminiscent of Approximate Bayesian\nComputation) as a workaround. Compared to the bootstrap particle filter, the\nnew methods deliver substantially reduced mean squared error in normalising\nconstant estimates, even after accounting for execution time. The methods are\ndemonstrated for state estimation with two toy examples, and for parameter\nestimation (within a particle marginal Metropolis--Hastings sampler) with three\napplied examples in econometrics, epidemiology and marine biogeochemistry. \n\n"}
{"id": "1405.4321", "contents": "Title: Shaping wave patterns in reaction-diffusion systems Abstract: We present a method to control the two-dimensional shape of traveling wave\nsolutions to reaction-diffusion systems, as e.g. interfaces and excitation\npulses. Control signals that realize a pre-given wave shape are determined\nanalytically from nonlinear evolution equation for isoconcentration lines as\nthe perturbed nonlinear phase diffusion equation or the perturbed linear\neikonal equation. While the control enforces a desired wave shape perpendicular\nto the local propagation direction, the wave profile along the propagation\ndirection itself remains almost unaffected. Provided that the one-dimensional\nwave profile and its propagation velocity can be measured experimentally, and\nthe diffusion coefficients of the reacting species are given, the new approach\ncan be applied even if the underlying nonlinear reaction kinetics are unknown. \n\n"}
{"id": "1405.5974", "contents": "Title: Living on the Edge: The Role of Proactive Caching in 5G Wireless\n  Networks Abstract: This article explores one of the key enablers of beyond $4$G wireless\nnetworks leveraging small cell network deployments, namely proactive caching.\nEndowed with predictive capabilities and harnessing recent developments in\nstorage, context-awareness and social networks, peak traffic demands can be\nsubstantially reduced by proactively serving predictable user demands, via\ncaching at base stations and users' devices. In order to show the effectiveness\nof proactive caching, we examine two case studies which exploit the spatial and\nsocial structure of the network, where proactive caching plays a crucial role.\nFirstly, in order to alleviate backhaul congestion, we propose a mechanism\nwhereby files are proactively cached during off-peak demands based on file\npopularity and correlations among users and files patterns. Secondly,\nleveraging social networks and device-to-device (D2D) communications, we\npropose a procedure that exploits the social structure of the network by\npredicting the set of influential users to (proactively) cache strategic\ncontents and disseminate them to their social ties via D2D communications.\nExploiting this proactive caching paradigm, numerical results show that\nimportant gains can be obtained for each case study, with backhaul savings and\na higher ratio of satisfied users of up to $22\\%$ and $26\\%$, respectively.\nHigher gains can be further obtained by increasing the storage capability at\nthe network edge. \n\n"}
{"id": "1406.0189", "contents": "Title: Convex Total Least Squares Abstract: We study the total least squares (TLS) problem that generalizes least squares\nregression by allowing measurement errors in both dependent and independent\nvariables. TLS is widely used in applied fields including computer vision,\nsystem identification and econometrics. The special case when all dependent and\nindependent variables have the same level of uncorrelated Gaussian noise, known\nas ordinary TLS, can be solved by singular value decomposition (SVD). However,\nSVD cannot solve many important practical TLS problems with realistic noise\nstructure, such as having varying measurement noise, known structure on the\nerrors, or large outliers requiring robust error-norms. To solve such problems,\nwe develop convex relaxation approaches for a general class of structured TLS\n(STLS). We show both theoretically and experimentally, that while the plain\nnuclear norm relaxation incurs large approximation errors for STLS, the\nre-weighted nuclear norm approach is very effective, and achieves better\naccuracy on challenging STLS problems than popular non-convex solvers. We\ndescribe a fast solution based on augmented Lagrangian formulation, and apply\nour approach to an important class of biological problems that use population\naverage measurements to infer cell-type and physiological-state specific\nexpression levels that are very hard to measure directly. \n\n"}
{"id": "1406.1651", "contents": "Title: The insignificant evolution of the richness-mass relation of galaxy\n  clusters Abstract: We analysed the richness--mass scaling of 23 very massive clusters at\n$0.15<z<0.55$ with homogenously measured weak-lensing masses and richnesses\nwithin a fixed aperture of $0.5$ Mpc radius. We found that the richness--mass\nscaling is very tight (the scatter is $<0.09$ dex with 90 \\% probability) and\nindependent of cluster evolutionary status and morphology. This implies a close\nassociation between infall and evolution of dark matter and galaxies in the\ncentral region of clusters. We also found that the evolution of the\nrichness-mass intercept is minor at most, and, given the minor mass evolution\nacross the studied redshift range, the richness evolution of individual massive\nclusters also turns out to be very small. Finally, it was paramount to account\nfor the cluster mass function and the selection function. Ignoring them would\nled to biases larger than the (otherwise quoted) errors. Our study benefits\nfrom: a) weak-lensing masses instead of proxy-based masses thereby removing the\nambiguity between a real trend and one induced by an accounted evolution of the\nused mass proxy; b) the use of projected masses that simplify the statistical\nanalysis thereby not requiring consideration of the unknown covariance induced\nby the cluster orientation/triaxiality; c) the use of aperture masses as they\nare free of the pseudo-evolution of mass definitions anchored to the evolving\ndensity of the Universe; d) a proper accounting of the sample selection\nfunction and of the Malmquist-like effect induced by the cluster mass function;\ne) cosmological simulations for the computation of the cluster mass function,\nits evolution, and the mass growth of each individual cluster. \n\n"}
{"id": "1406.1901", "contents": "Title: Subsampling Methods for Persistent Homology Abstract: Persistent homology is a multiscale method for analyzing the shape of sets\nand functions from point cloud data arising from an unknown distribution\nsupported on those sets. When the size of the sample is large, direct\ncomputation of the persistent homology is prohibitive due to the combinatorial\nnature of the existing algorithms. We propose to compute the persistent\nhomology of several subsamples of the data and then combine the resulting\nestimates. We study the risk of two estimators and we prove that the\nsubsampling approach carries stable topological information while achieving a\ngreat reduction in computational complexity. \n\n"}
{"id": "1406.2255", "contents": "Title: Energy-Efficient Cooperative Cognitive Relaying Schemes for Cognitive\n  Radio Networks Abstract: We investigate a cognitive radio network in which a primary user (PU) may\ncooperate with a cognitive radio user (i.e., a secondary user (SU)) for\ntransmissions of its data packets. The PU is assumed to be a buffered node\noperating in a time-slotted fashion where the time is partitioned into\nequal-length slots. We develop two schemes which involve cooperation between\nprimary and secondary users. To satisfy certain quality of service (QoS)\nrequirements, users share time slot duration and channel frequency bandwidth.\nMoreover, the SU may leverage the primary feedback message to further increase\nboth its data rate and satisfy the PU QoS requirements. The proposed\ncooperative schemes are designed such that the SU data rate is maximized under\nthe constraint that the PU average queueing delay is maintained less than the\naverage queueing delay in case of non-cooperative PU. In addition, the proposed\nschemes guarantee the stability of the PU queue and maintain the average energy\nemitted by the SU below a certain value. The proposed schemes also provide more\nrobust and potentially continuous service for SUs compared to the conventional\npractice in cognitive networks where SUs transmit in the spectrum holes and\nsilence sessions of the PUs. We include primary source burstiness, sensing\nerrors, and feedback decoding errors to the analysis of our proposed\ncooperative schemes. The optimization problems are solved offline and require a\nsimple 2-dimensional grid-based search over the optimization variables.\nNumerical results show the beneficial gains of the cooperative schemes in terms\nof SU data rate and PU throughput, average PU queueing delay, and average PU\nenergy savings. \n\n"}
{"id": "1406.3402", "contents": "Title: Relieving and Readjusting Pythagoras Abstract: Bill James invented the Pythagorean expectation in the late 70's to predict a\nbaseball team's winning percentage knowing just their runs scored and allowed.\nHis original formula estimates a winning percentage of ${\\rm RS}^2/({\\rm\nRS}^2+{\\rm RA}^2)$, where ${\\rm RS}$ stands for runs scored and ${\\rm RA}$ for\nruns allowed; later versions found better agreement with data by replacing the\nexponent 2 with numbers near 1.83. Miller and his colleagues provided a\ntheoretical justification by modeling runs scored and allowed by independent\nWeibull distributions. They showed that a single Weibull distribution did a\nvery good job of describing runs scored and allowed, and led to a predicted\nwon-loss percentage of $({\\rm RS_{\\rm obs}}-1/2)^\\gamma / (({\\rm RS_{\\rm\nobs}}-1/2)^\\gamma + ({\\rm RA_{\\rm obs}}-1/2)^\\gamma)$, where ${\\rm RS_{\\rm\nobs}}$ and ${\\rm RA_{\\rm obs}}$ are the observed runs scored and allowed and\n$\\gamma$ is the shape parameter of the Weibull (typically close to 1.8). We\nshow a linear combination of Weibulls more accurately determines a team's run\nproduction and increases the prediction accuracy of a team's winning percentage\nby an average of about 25% (thus while the currently used variants of the\noriginal predictor are accurate to about four games a season, the new\ncombination is accurate to about three). The new formula is more involved\ncomputationally; however, it can be easily computed on a laptop in a matter of\nminutes from publicly available season data. It performs as well (or slightly\nbetter) than the related Pythagorean formulas in use, and has the additional\nadvantage of having a theoretical justification for its parameter values (and\nnot just an optimization of parameters to minimize prediction error). \n\n"}
{"id": "1406.4736", "contents": "Title: Seek and Decode: Random Access with Physical-Layer Network Coding and\n  Multiuser Detection Abstract: We present a novel cross layer approach to random access (RA) that combines\nphysical-layer network coding (PLNC) with multiuser detection (MUD). PLNC and\nMUD are applied jointly at the physical level in order to extract any linear\ncombination of messages experiencing a collision. The set of combinations\nextracted from a whole frame is then processed by the receiver to recover the\noriginal packets. A simple pre-coding stage at the transmitting terminals\nallows the receiver to further increase system diversity. We derive an\nanalytical bound on the system throughput and present simulation results for\nthe decoding at the physical level as well as several performance measures at\nframe level in block fading channels, namely throughput, packet loss rate and\nenergy efficiency. The results we present are promising and suggest that a\ncross layer approach leveraging on the joint use of PLNC and MUD can\nsignificantly improve the performance of RA systems. \n\n"}
{"id": "1406.5231", "contents": "Title: Reducing Basis Mismatch in Harmonic Signal Recovery via Alternating\n  Convex Search Abstract: The theory behind compressive sampling pre-supposes that a given sequence of\nobservations may be exactly represented by a linear combination of a small\nnumber of basis vectors. In practice, however, even small deviations from an\nexact signal model can result in dramatic increases in estimation error; this\nis the so-called \"basis mismatch\" problem. This work provides one possible\nsolution to this problem in the form of an iterative, biconvex search\nalgorithm. The approach uses standard $\\ell_1$-minimization to find the signal\nmodel coefficients followed by a maximum likelihood estimate of the signal\nmodel. The algorithm is illustrated on harmonic signals of varying sparsity and\noutperforms the current state-of-the-art. \n\n"}
{"id": "1406.5854", "contents": "Title: Load Forecasting of Supermarket Refrigeration Abstract: This paper presents a study of models for forecasting the electrical load for\nsupermarket refrigeration. The data used for building the models consists of\nload measurements, local climate measurements and weather forecasts. The load\nmeasurements are from a supermarket located in a village in Denmark. Every hour\nthe hourly electrical load for refrigeration is forecasted for the following 42\nhours. The forecast models are adaptive linear time series models. The model\nhas two regimes; one for opening hours and one for closing hours, this is\nmodelled by a regime switching model and two different methods for predicting\nthe regimes are tested. The dynamic relation between the weather and the load\nis modelled by simple transfer functions and the non-linearities are described\nusing spline functions. The results are thoroughly evaluated and it is shown\nthat the spline functions are suitable for handling the non-linear relations\nand that after applying an auto-regressive noise model the one-step ahead\nresiduals do not contain further significant information. \n\n"}
{"id": "1406.6470", "contents": "Title: Wireless Networks with RF Energy Harvesting: A Contemporary Survey Abstract: Radio frequency (RF) energy transfer and harvesting techniques have recently\nbecome alternative methods to power the next generation wireless networks. As\nthis emerging technology enables proactive energy replenishment of wireless\ndevices, it is advantageous in supporting applications with quality of service\n(QoS) requirement. In this paper, we present an extensive literature review on\nthe research progresses in wireless networks with RF energy harvesting\ncapability, referred to as RF energy harvesting networks (RF-EHNs). First, we\npresent an overview of the RF-EHNs including system architecture, RF energy\nharvesting techniques and existing applications. Then, we present the\nbackground in circuit design as well as the state-of-the-art circuitry\nimplementations, and review the communication protocols specially designed for\nRF-EHNs. We also explore various key design issues in the development of\nRF-EHNs according to the network types, i.e., single-hop network, multi-antenna\nnetwork, relay network and cognitive radio network. Finally, we envision some\nopen research directions. \n\n"}
{"id": "1407.0474", "contents": "Title: Recent Advances in Joint Wireless Energy and Information Transfer Abstract: In this paper, we provide an overview of the recent advances in\nmicrowave-enabled wireless energy transfer (WET) technologies and their\napplications in wireless communications. Specifically, we divide our\ndiscussions into three parts. First, we introduce the state-of-the-art WET\ntechnologies and the signal processing techniques to maximize the energy\ntransfer efficiency. Then, we discuss an interesting paradigm named\nsimultaneous wireless information and power transfer (SWIPT), where energy and\ninformation are jointly transmitted using the same radio waveform. At last, we\nreview the recent progress in wireless powered communication networks (WPCN),\nwhere wireless devices communicate using the power harvested by means of WET.\nExtensions and future directions are also discussed in each of these areas. \n\n"}
{"id": "1407.0536", "contents": "Title: Analysis of the Decoupled Access for Downlink and Uplink in Wireless\n  Heterogeneous Networks Abstract: Wireless cellular networks evolve towards a heterogeneous infrastructure,\nfeaturing multiple types of Base Stations (BSs), such as Femto BSs (FBSs) and\nMacro BSs (MBSs). A wireless device observes multiple points (BSs) through\nwhich it can access the infrastructure and it may choose to receive the\ndownlink (DL) traffic from one BS and send uplink (UL) traffic through another\nBS. Such a situation is referred to as decoupled DL/UL access. Using the\nframework of stochastic geometry, we derive the association probability for\nDL/UL. In order to maximize the average received power, as the relative density\nof FBSs initially increases, a large fraction of devices chooses decoupled\naccess, i.e. receive from a MBS in DL and transmit through a FBS in UL. We\nanalyze the impact that this type of association has on the average throughput\nin the system. \n\n"}
{"id": "1407.3322", "contents": "Title: Distribution System Load and Forecast Model Abstract: This short document provides experimental evidence for the set of assumptions\non the mean load and forecast errors made in \\cite{Sevlian2014A_Outage} and\n\\cite{Sevlian2014B_Outage}. We show that the mean load at any given node is\ndistributed normally, where we compute the mean and variance. We then present\nan aggregation-error curve for a single day ahead forecaster. Residual analysis\nshows that beyond 500 customers, gaussian residuals is a reasonable model. We\nthen show the forecaster has uncorrelated errors. \n\n"}
{"id": "1407.4694", "contents": "Title: Distributed Pricing-Based User Association for Downlink Heterogeneous\n  Cellular Networks Abstract: This paper considers the optimization of the user and base-station (BS)\nassociation in a wireless downlink heterogeneous cellular network under the\nproportional fairness criterion. We first consider the case where each BS has a\nsingle antenna and transmits at fixed power, and propose a distributed price\nupdate strategy for a pricing-based user association scheme, in which the users\nare assigned to the BS based on the value of a utility function minus a price.\nThe proposed price update algorithm is based on a coordinate descent method for\nsolving the dual of the network utility maximization problem, and it has a\nrigorous performance guarantee. The main advantage of the proposed algorithm as\ncompared to the existing subgradient method for price update is that the\nproposed algorithm is independent of parameter choices and can be implemented\nasynchronously. Further, this paper considers the joint user association and BS\npower control problem, and proposes an iterative dual coordinate descent and\nthe power optimization algorithm that significantly outperforms existing\napproaches. Finally, this paper considers the joint user association and BS\nbeamforming problem for the case where the BSs are equipped with multiple\nantennas and spatially multiplex multiple users. We incorporate dual coordinate\ndescent with the weighted minimum mean-squared error (WMMSE) algorithm, and\nshow that it achieves nearly the same performance as a computationally more\ncomplex benchmark algorithm (which applies the WMMSE algorithm on the entire\nnetwork for BS association), while avoiding excessive BS handover. \n\n"}
{"id": "1407.6242", "contents": "Title: Frequency behaviour for multinomial counts of fisheries discards via a\n  nested wavelet zero and N inflated binomial model Abstract: In this paper we identify the changing frequency behaviour of multinomial\ncounts of fish species discarded by vessels in the Irish Sea. We use a Bayesian\nhierarchical model which captures dynamic frequency changes via a shrinkage\nmodel applied to wavelet basis functions. Wavelets are known for capturing data\nfeatures at different temporal scales; we use a recently-proposed shrinkage\nprior from the factor analysis literature so that features at the finest levels\nof detail exhibit the greatest shrinkage. Rather than using a multinomial\ndistribution for monitoring the changes in discards over time, which can be\nslow to fit and inflexible, we use a nested zero-and-N inflated (ZaNI) binomial\ndistribution which enables much faster computation with no obvious\ndeterioration in model flexibility. Our results show that seasonal behaviour in\nthese data are not regular and occur at different frequencies. We also show\nthat the nested ZaNI binomial distribution is a good fit to multinomial count\ndata of this sort when an informative nested structure is applied. \n\n"}
{"id": "1407.6895", "contents": "Title: Bayesian Exponential Random Graph Models with Nodal Random Effects Abstract: We extend the well-known and widely used Exponential Random Graph Model\n(ERGM) by including nodal random effects to compensate for heterogeneity in the\nnodes of a network. The Bayesian framework for ERGMs proposed by Caimo and\nFriel (2011) yields the basis of our modelling algorithm. A central question in\nnetwork models is the question of model selection and following the Bayesian\nparadigm we focus on estimating Bayes factors. To do so we develop an\napproximate but feasible calculation of the Bayes factor which allows one to\npursue model selection. Two data examples and a small simulation study\nillustrate our mixed model approach and the corresponding model selection. \n\n"}
{"id": "1407.7118", "contents": "Title: Estimation of the Hawkes Process With Renewal Immigration Using the EM\n  Algorithm Abstract: We introduce the Hawkes process with renewal immigration and make its\nstatistical estimation possible with two Expectation Maximization (EM)\nalgorithms. The standard Hawkes process introduces immigrant points via a\nPoisson process, and each immigrant has a subsequent cluster of associated\noffspring of multiple generations. We generalize the immigration to come from a\nRenewal process; introducing dependence between neighbouring clusters, and\nallowing for over/under dispersion in cluster locations. This complicates\nevaluation of the likelihood since one needs to know which subset of the\nobserved points are immigrants. Two EM algorithms enable estimation here: The\nfirst is an extension of an existing algorithm that treats the entire branching\nstructure - which points are immigrants, and which point is the parent of each\noffspring - as missing data. The second considers only if a point is an\nimmigrant or not as missing data and can be implemented with linear time\ncomplexity. Both algorithms are found to be consistent in simulation studies.\nFurther, we show that misspecifying the immigration process introduces\nsignficant bias into model estimation-- especially the branching ratio, which\nquantifies the strength of self excitation. Thus, this extended model provides\na valuable alternative model in practice. \n\n"}
{"id": "1407.7267", "contents": "Title: On Spectrum Sharing Between Energy Harvesting Cognitive Radio Users and\n  Primary Users Abstract: This paper investigates the maximum secondary throughput for a rechargeable\nsecondary user (SU) sharing the spectrum with a primary user (PU) plugged to a\nreliable power supply. The SU maintains a finite energy queue and harvests\nenergy from natural resources and primary radio frequency (RF) transmissions.\nWe propose a power allocation policy at the PU and analyze its effect on the\nthroughput of both the PU and SU. Furthermore, we study the impact of the\nbursty arrivals at the PU on the energy harvested by the SU from RF\ntransmissions. Moreover, we investigate the impact of the rate of energy\nharvesting from natural resources on the SU throughput. We assume fading\nchannels and compute exact closed-form expressions for the energy harvested by\nthe SU under fading. Results reveal that the proposed power allocation policy\nalong with the implemented RF energy harvesting at the SU enhance the\nthroughput of both primary and secondary links. \n\n"}
{"id": "1407.7757", "contents": "Title: Retrodictive derivation of the radical-ion-pair master equation and\n  Monte-Carlo simulation with single-molecule quantum trajectories Abstract: Radical-ion-pair reactions, central in photosynthesis and the avian magnetic\ncompass mechanism, have recently shown to be a paradigm system for applying\nquantum information science in a biochemical setting. The fundamental quantum\nmaster equation describing radical-ion-pair reactions is still under debate. We\nhere use quantum retrodiction to produce a rigorous refinement of the theory\nput forward in Phys. Rev. E {\\bf 83}, 056118 (2011). We also provide a rigorous\nanalysis of the measure of singlet-triplet coherence required for deriving the\nradical-pair master equation. A Monte-Carlo simulation with single-molecule\nquantum trajectories supports the self-consistency of our approach. \n\n"}
{"id": "1407.8083", "contents": "Title: Variational cross-validation of slow dynamical modes in molecular\n  kinetics Abstract: Markov state models (MSMs) are a widely used method for approximating the\neigenspectrum of the molecular dynamics propagator, yielding insight into the\nlong-timescale statistical kinetics and slow dynamical modes of biomolecular\nsystems. However, the lack of a unified theoretical framework for choosing\nbetween alternative models has hampered progress, especially for non-experts\napplying these methods to novel biological systems. Here, we consider\ncross-validation with a new objective function for estimators of these slow\ndynamical modes, a generalized matrix Rayleigh quotient (GMRQ), which measures\nthe ability of a rank-$m$ projection operator to capture the slow subspace of\nthe system. It is shown that a variational theorem bounds the GMRQ from above\nby the sum of the first $m$ eigenvalues of the system's propagator, but that\nthis bound can be violated when the requisite matrix elements are estimated\nsubject to statistical uncertainty. This overfitting can be detected and\navoided through cross-validation. These result make it possible to construct\nMarkov state models for protein dynamics in a way that appropriately captures\nthe tradeoff between systematic and statistical errors. \n\n"}
{"id": "1408.1239", "contents": "Title: The Minimum S-Divergence Estimator under Continuous Models: The\n  Basu-Lindsay Approach Abstract: Robust inference based on the minimization of statistical divergences has\nproved to be a useful alternative to the classical maximum likelihood based\ntechniques. Recently Ghosh et al. (2013) proposed a general class of divergence\nmeasures for robust statistical inference, named the S-Divergence Family. Ghosh\n(2014) discussed its asymptotic properties for the discrete model of densities.\nIn the present paper, we develop the asymptotic properties of the proposed\nminimum S-Divergence estimators under continuous models. Here we use the\nBasu-Lindsay approach (1994) of smoothing the model densities that, unlike\nprevious approaches, avoids much of the complications of the kernel bandwidth\nselection. Illustrations are presented to support the performance of the\nresulting estimators both in terms of efficiency and robustness through\nextensive simulation studies and real data examples. \n\n"}
{"id": "1408.2335", "contents": "Title: Wireless Powered Communication: Opportunities and Challenges Abstract: The performance of wireless communication is fundamentally constrained by the\nlimited battery life of wireless devices, whose operations are frequently\ndisrupted due to the need of manual battery replacement/recharging. The recent\nadvance in radio frequency (RF) enabled wireless energy transfer (WET)\ntechnology provides an attractive solution named wireless powered communication\n(WPC), where the wireless devices are powered by dedicated wireless power\ntransmitters to provide continuous and stable microwave energy over the air. As\na key enabling technology for truly perpetual communications, WPC opens up the\npotential to build a network with larger throughput, higher robustness, and\nincreased flexibility compared to its battery-powered counterpart. However, the\ncombination of wireless energy and information transmissions also raises many\nnew research problems and implementation issues to be addressed. In this\narticle, we provide an overview of state-of-the-art RF-enabled WET technologies\nand their applications to wireless communications, with highlights on the key\ndesign challenges, solutions, and opportunities ahead. \n\n"}
{"id": "1408.2726", "contents": "Title: Bloch-Redfield equations for modeling light-harvesting complexes Abstract: We challenge the misconception that Bloch-Redfield equations are a less\npowerful tool than phenomenological Lindblad equations for modeling exciton\ntransport in photosynthetic complexes. This view predominantly originates from\nan indiscriminate use of the secular approximation. We provide a detailed\ndescription of how to model both coherent oscillations and several types of\nnoise, giving explicit examples. All issues with non-positivity are overcome by\na consistent straightforward physical noise model. Herein also lies the\nstrength of the Bloch-Redfield approach because it facilitates the analysis of\nnoise-effects by linking them back to physical parameters of the noise\nenvironment. This includes temporal and spatial correlations and the strength\nand type of interaction between the noise and the system of interest. Finally\nwe analyze a prototypical dimer system as well as a 7-site Fenna-Matthews-Olson\n(FMO) complex in regards to spatial correlation length of the noise, noise\nstrength, temperature and their connection to the transfer time and transfer. \n\n"}
{"id": "1408.2779", "contents": "Title: A Probabilistic MAC for Cognitive Radio Systems with Energy Harvesting\n  Nodes Abstract: In this paper, we consider a cognitive radio (CR) system where the secondary\nuser (SU) harvests energy from both the nature resources and the primary user\n(PU) radio frequency(RF) signal. We propose an energy-based probabilistic\naccess scheme in which SU probabilistically accesses and senses the primary\nchannel. The decision is based on the available energy and the PU's activity.\nWe investigate the problem of maximizing the SU's success rate provided that\nthe PU average quality of service (QoS) constraint is satisfied. We also assume\nmulti-packet reception (MPR) capability and sensing errors under a Rayleigh\nfading channel. Numerical results show the effectiveness of the proposed\nprobabilistic access scheme. \n\n"}
{"id": "1408.3556", "contents": "Title: Probing the Role of the Eighth Bacteriochlorophyll in holo-FMO Complex\n  by Simulated Two-Dimensional Electronic Spectroscopy Abstract: The Fenna-Matthews-Olson (FMO) protein-pigment complex acts as a molecular\nwire between the outer antenna system and the reaction center (RC); it is an\nimportant model system to study the excitonic energy transfer. Recent\ncrystallographic studies report the existence of an additional (eighth)\nbacteriochlorophyll a (BChl a). To understand the functionality of this eighth\nBChl, we simulated the two-dimensional electronic spectra of both the 7-site\n(apo form) and the 8-site (holo form) variant of the FMO complex from green\nsulfur bacteria, Prosthecochloris aestuarii. By comparing the difference\nbetween the spectrum, it was found that the eighth BChl can affect two\ndifferent excitonic energy transfer pathways, these being: (1) directly involve\nin the first pathway 6 $\\rightarrow$ 3 $\\rightarrow$ 1 of the apo form model by\npassing the excitonic energy to exciton 6; and (2) increase the excitonic wave\nfunction overlap between excitons 4 and 5 in the second pathway (7\n$\\rightarrow$ 4,5 $\\rightarrow$ 2 $\\rightarrow$ 1) and thus increase the\npossible downward sampling routes across the BChls. \n\n"}
{"id": "1409.1606", "contents": "Title: Power Optimal Non-contiguous Spectrum Access in Multi Front End Radio\n  Enabled Point-to-Point Link Abstract: Non-contiguous spectrum chunks allow wireless links to flexibly access a wide\namount of bandwidth. Multi- Channel Multi-Radio (MC-MR) and Non-Contiguous\nOrthogonal Frequency Division Multiplexing (NC-OFDM) are the two commercially\nviable strategies to access non-contiguous spectrum chunks. MC-MR accesses\nmultiple non-contiguous chunks by activating multiple front ends which, in\nturn, increases the circuit power consumption of each of the activated front\nends. NC-OFDM accesses non-contiguous spectrum chunks with a single front end\nby nulling remaining subchannels but increases spectrum span which, in turn,\nincreases the power consumption of ADC and DAC. This work focuses on a\npoint-to-point link where transmitter and receiver have multiple front ends and\ncan employ NC-OFDM technology. We investigate optimal spectrum fragmentation in\neach front end from a system power (summation of transmit power and circuit\npower) perspective. We formulate a mixed integer non-linear program (MINLP) to\nperform power control and scheduling, and minimize system power by providing a\ngreedy algorithm (O(M^3 I)) where M and I denote the number of channels and\nradio front ends respectively. \n\n"}
{"id": "1409.1798", "contents": "Title: Using Regression Kernels to Forecast A Failure to Appear in Court Abstract: Forecasts of prospective criminal behavior have long been an important\nfeature of many criminal justice decisions. There is now substantial evidence\nthat machine learning procedures will classify and forecast at least as well,\nand typically better, than logistic regression, which has to date dominated\nconventional practice. However, machine learning procedures are adaptive. They\n\"learn\" inductively from training data. As a result, they typically perform\nbest with very large datasets. There is a need, therefore, for forecasting\nprocedures with the promise of machine learning that will perform well with\nsmall to moderately-sized datasets. Kernel methods provide precisely that\npromise. In this paper, we offer an overview of kernel methods in regression\nsettings and compare such a method, regularized with principle components, to\nstepwise logistic regression. We apply both to a timely and important criminal\njustice concern: a failure to appear (FTA) at court proceedings following an\narraignment. A forecast of an FTA can be an important factor is a judge's\ndecision to release a defendant while awaiting trial and can influence the\nconditions imposed on that release. Forecasting accuracy matters, and our\nkernel approach forecasts far more accurately than stepwise logistic\nregression. The methods developed here are implemented in the R package kernReg\ncurrently available on CRAN. \n\n"}
{"id": "1409.3246", "contents": "Title: Wideband Sensing and Optimization for Cognitive Radio Networks with\n  Noise Variance Uncertainty Abstract: This paper considers wide-band spectrum sensing and optimization for\ncognitive radio (CR) networks with noise variance uncertainty. It is assumed\nthat the considered wide-band contains one or more white sub-bands. Under this\nassumption, we consider throughput maximization of the CR network while\nappropriately protecting the primary network. We address this problem as\nfollows. First, we propose novel ratio based test statistics for detecting the\nedges of each sub-band. Second, we employ simple energy comparison approach to\nchoose one reference white sub-band. Third, we propose novel generalized energy\ndetector (GED) for examining each of the remaining sub-bands by exploiting the\nnoise information of the reference white sub-band. Finally, we optimize the\nsensing time ($T_o$) to maximize the CR network throughput using the detection\nand false alarm probabilities of the GED. The proposed GED does not suffer from\nsignal to noise ratio (SNR) wall and outperforms the existing signal detectors.\nMoreover, the relationship between the proposed GED and conventional energy\ndetector (CED) is quantified analytically. We show that the optimal $T_o$\ndepends on the noise variance information. In particular, with $10$TV bands,\nSNR=$-20$dB and $2$s frame duration, we found that the optimal $T_o$ is\n$28.5$ms ($50.6$ms) with perfect (imperfect) noise variance scenario. \n\n"}
{"id": "1409.6001", "contents": "Title: Reconfigurable Wireless Networks Abstract: Driven by the advent of sophisticated and ubiquitous applications, and the\never-growing need for information, wireless networks are without a doubt\nsteadily evolving into profoundly more complex and dynamic systems. The user\ndemands are progressively rampant, while application requirements continue to\nexpand in both range and diversity. Future wireless networks, therefore, must\nbe equipped with the ability to handle numerous, albeit challenging\nrequirements. Network reconfiguration, considered as a prominent network\nparadigm, is envisioned to play a key role in leveraging future network\nperformance and considerably advancing current user experiences. This paper\npresents a comprehensive overview of reconfigurable wireless networks and an\nin-depth analysis of reconfiguration at all layers of the protocol stack. Such\nnetworks characteristically possess the ability to reconfigure and adapt their\nhardware and software components and architectures, thus enabling flexible\ndelivery of broad services, as well as sustaining robust operation under highly\ndynamic conditions. The paper offers a unifying framework for research in\nreconfigurable wireless networks. This should provide the reader with a\nholistic view of concepts, methods, and strategies in reconfigurable wireless\nnetworks. Focus is given to reconfigurable systems in relatively new and\nemerging research areas such as cognitive radio networks, cross-layer\nreconfiguration and software-defined networks. In addition, modern networks\nhave to be intelligent and capable of self-organization. Thus, this paper\ndiscusses the concept of network intelligence as a means to enable\nreconfiguration in highly complex and dynamic networks. Finally, the paper is\nsupported with several examples and case studies showing the tremendous impact\nof reconfiguration on wireless networks. \n\n"}
{"id": "1409.7715", "contents": "Title: Parameter inference and model selection in deterministic and stochastic\n  dynamical models via approximate Bayesian computation: modeling a wildlife\n  epidemic Abstract: We consider the problem of selecting deterministic or stochastic models for a\nbiological, ecological, or environmental dynamical process. In most cases, one\nprefers either deterministic or stochastic models as candidate models based on\nexperience or subjective judgment. Due to the complex or intractable likelihood\nin most dynamical models, likelihood-based approaches for model selection are\nnot suitable. We use approximate Bayesian computation for parameter estimation\nand model selection to gain further understanding of the dynamics of two\nepidemics of chronic wasting disease in mule deer. The main novel contribution\nof this work is that under a hierarchical model framework we compare three\ntypes of dynamical models: ordinary differential equation, continuous time\nMarkov chain, and stochastic differential equation models. To our knowledge\nmodel selection between these types of models has not appeared previously.\nSince the practice of incorporating dynamical models into data models is\nbecoming more common, the proposed approach may be very useful in a variety of\napplications. \n\n"}
{"id": "1409.7994", "contents": "Title: Randomness is valid at large numbers Abstract: Randomness is a central concept to statistics and physics. Here, a\nstatistical analysis shows experimental evidence that tossing coins and finding\nlast digits of prime numbers are identical regarding statistics for equally\nlikely outcomes. This analysis explains why randomness in equally likely\noutcomes can be valid only at large numbers. \n\n"}
{"id": "1409.8502", "contents": "Title: Combining Particle MCMC with Rao-Blackwellized Monte Carlo Data\n  Association for Parameter Estimation in Multiple Target Tracking Abstract: We consider state and parameter estimation in multiple target tracking\nproblems with data association uncertainties and unknown number of targets. We\nshow how the problem can be recast into a conditionally linear Gaussian\nstate-space model with unknown parameters and present an algorithm for\ncomputationally efficient inference on the resulting model. The proposed\nalgorithm is based on combining the Rao-Blackwellized Monte Carlo data\nassociation algorithm with particle Markov chain Monte Carlo algorithms to\njointly estimate both parameters and data associations. Both particle marginal\nMetropolis-Hastings and particle Gibbs variants of particle MCMC are\nconsidered. We demonstrate the performance of the method both using simulated\ndata and in a real-data case study of using multiple target tracking to\nestimate the brown bear population in Finland. \n\n"}
{"id": "1410.1013", "contents": "Title: Assess Sleep Stage by Modern Signal Processing Techniques Abstract: In this paper, two modern adaptive signal processing techniques, Empirical\nIntrinsic Geometry and Synchrosqueezing transform, are applied to quantify\ndifferent dynamical features of the respiratory and electroencephalographic\nsignals. We show that the proposed features are theoretically rigorously\nsupported, as well as capture the sleep information hidden inside the signals.\nThe features are used as input to multiclass support vector machines with the\nradial basis function to automatically classify sleep stages. The effectiveness\nof the classification based on the proposed features is shown to be comparable\nto human expert classification -- the proposed classification of awake, REM,\nN1, N2 and N3 sleeping stages based on the respiratory signal (resp.\nrespiratory and EEG signals) has the overall accuracy $81.7\\%$ (resp. $89.3\\%$)\nin the relatively normal subject group. In addition, by examining the\ncombination of the respiratory signal with the electroencephalographic signal,\nwe conclude that the respiratory signal consists of ample sleep information,\nwhich supplements to the information stored in the electroencephalographic\nsignal. \n\n"}
{"id": "1410.1101", "contents": "Title: Sequential Monte Carlo Samplers for capital allocation under\n  copula-dependent risk models Abstract: In this paper we assume a multivariate risk model has been developed for a\nportfolio and its capital derived as a homogeneous risk measure. The Euler (or\ngradient) principle, then, states that the capital to be allocated to each\ncomponent of the portfolio has to be calculated as an expectation conditional\nto a rare event, which can be challenging to evaluate in practice. We exploit\nthe copula-dependence within the portfolio risks to design a Sequential Monte\nCarlo Samplers based estimate to the marginal conditional expectations involved\nin the problem, showing its efficiency through a series of computational\nexamples. \n\n"}
{"id": "1410.2466", "contents": "Title: Quantification and visualization of variation in anatomical trees Abstract: This paper presents two approaches to quantifying and visualizing variation\nin datasets of trees. The first approach localizes subtrees in which\nsignificant population differences are found through hypothesis testing and\nsparse classifiers on subtree features. The second approach visualizes the\nglobal metric structure of datasets through low-distortion embedding into\nhyperbolic planes in the style of multidimensional scaling. A case study is\nmade on a dataset of airway trees in relation to Chronic Obstructive Pulmonary\nDisease. \n\n"}
{"id": "1410.6059", "contents": "Title: Integer percentages as electoral falsification fingerprints Abstract: We hypothesize that if election results are manipulated or forged, then, due\nto the well-known human attraction to round numbers, the frequency of reported\nround percentages can be increased. To test this hypothesis, we analyzed raw\ndata from seven federal elections held in the Russian Federation during the\nperiod from 2000 to 2012 and found that in all elections since 2004 the number\nof polling stations reporting turnout and/or leader's result expressed by an\ninteger percentage (as opposed to a fractional value) was much higher than\nexpected by pure chance. Monte Carlo simulations confirmed high statistical\nsignificance of the observed phenomenon, thereby suggesting its man-made\nnature. Geographical analysis showed that these anomalies were concentrated in\na specific subset of Russian regions which strongly suggests its orchestrated\norigin. Unlike previously proposed statistical indicators of alleged electoral\nfalsifications, our observations can hardly be explained differently but by a\nwidespread election fraud. \n\n"}
{"id": "1410.7056", "contents": "Title: Smoothing, Clustering, and Benchmarking for Small Area Estimation Abstract: We develop constrained Bayesian estimation methods for small area problems:\nthose requiring smoothness with respect to similarity across areas, such as\ngeographic proximity or clustering by covariates; and benchmarking constraints,\nrequiring (weighted) means of estimates to agree across levels of aggregation.\nWe develop methods for constrained estimation decision-theoretically and\ndiscuss their geometric interpretation. Our constrained estimators are the\nsolutions to tractable optimization problems and have closed-form solutions.\nMean squared errors of the constrained estimators are calculated via\nbootstrapping. Our techniques are free of distributional assumptions and apply\nwhether the estimator is linear or non-linear, univariate or multivariate. We\nillustrate our methods using data from the U.S. Census's Small Area Income and\nPoverty Estimates program. \n\n"}
{"id": "1410.7270", "contents": "Title: Capacity Analysis of Decoupled Downlink and Uplink Access in 5G\n  Heterogeneous Systems Abstract: Our traditional notion of a cell is changing dramatically given the\nincreasing degree of heterogeneity in 4G and emerging 5G systems. Rather than\nbelonging to a specific cell, a device would choose the most suitable\nconnection from the plethora of connections available. In such a setting, given\nthe transmission powers differ significantly between downlink (DL) and uplink\n(UL), a wireless device that sees multiple Base Stations (BSs) may access the\ninfrastructure in a way that it receives the downlink (DL) traffic from one BS\nand sends uplink (UL) traffic through another BS. This situation is referred to\nas Downlink and Uplink Decoupling (DUDe). In this paper, the capacity and\nthroughput gains brought by decoupling are rigorously derived using stochastic\ngeometry. Theoretical findings are then corroborated by means of simulation\nresults. A further constituent of this paper is the verification of the\ntheoretically derived results by means of a real-world system simulation\nplatform. Despite theoretical assumptions differing from the very complete\nsystem simulator, the trends in the association probabilities and capacity\ngains are similar. Based on the promising results, we then outline\narchitectural changes needed to facilitate the decoupling of DL and UL. \n\n"}
{"id": "1410.7716", "contents": "Title: Forecasting the 2013--2014 Influenza Season using Wikipedia Abstract: Infectious diseases are one of the leading causes of morbidity and mortality\naround the world; thus, forecasting their impact is crucial for planning an\neffective response strategy. According to the Centers for Disease Control and\nPrevention (CDC), seasonal influenza affects between 5% to 20% of the U.S.\npopulation and causes major economic impacts resulting from hospitalization and\nabsenteeism. Understanding influenza dynamics and forecasting its impact is\nfundamental for developing prevention and mitigation strategies.\n  We combine modern data assimilation methods with Wikipedia access logs and\nCDC influenza like illness (ILI) reports to create a weekly forecast for\nseasonal influenza. The methods are applied to the 2013--2014 influenza season\nbut are sufficiently general to forecast any disease outbreak, given incidence\nor case count data. We adjust the initialization and parametrization of a\ndisease model and show that this allows us to determine systematic model bias.\nIn addition, we provide a way to determine where the model diverges from\nobservation and evaluate forecast accuracy.\n  Wikipedia article access logs are shown to be highly correlated with\nhistorical ILI records and allow for accurate prediction of ILI data several\nweeks before it becomes available. The results show that prior to the peak of\nthe flu season, our forecasting method projected the actual outcome with a high\nprobability. However, since our model does not account for re-infection or\nmultiple strains of influenza, the tail of the epidemic is not predicted well\nafter the peak of flu season has past. \n\n"}
{"id": "1411.0622", "contents": "Title: A Subspace Method for Array Covariance Matrix Estimation Abstract: This paper introduces a subspace method for the estimation of an array\ncovariance matrix. It is shown that when the received signals are uncorrelated,\nthe true array covariance matrices lie in a specific subspace whose dimension\nis typically much smaller than the dimension of the full space. Based on this\nidea, a subspace based covariance matrix estimator is proposed. The estimator\nis obtained as a solution to a semi-definite convex optimization problem. While\nthe optimization problem has no closed-form solution, a nearly optimal\nclosed-form solution is proposed making it easy to implement. In comparison to\nthe conventional approaches, the proposed method yields higher estimation\naccuracy because it eliminates the estimation error which does not lie in the\nsubspace of the true covariance matrices. The numerical examples indicate that\nthe proposed covariance matrix estimator can significantly improve the\nestimation quality of the covariance matrix. \n\n"}
{"id": "1411.3013", "contents": "Title: Bayesian Evidence and Model Selection Abstract: In this paper we review the concepts of Bayesian evidence and Bayes factors,\nalso known as log odds ratios, and their application to model selection. The\ntheory is presented along with a discussion of analytic, approximate and\nnumerical techniques. Specific attention is paid to the Laplace approximation,\nvariational Bayes, importance sampling, thermodynamic integration, and nested\nsampling and its recent variants. Analogies to statistical physics, from which\nmany of these techniques originate, are discussed in order to provide readers\nwith deeper insights that may lead to new techniques. The utility of Bayesian\nmodel testing in the domain sciences is demonstrated by presenting four\nspecific practical examples considered within the context of signal processing\nin the areas of signal detection, sensor characterization, scientific model\nselection and molecular force characterization. \n\n"}
{"id": "1411.4867", "contents": "Title: Respondent-driven sampling and an unusual epidemic Abstract: Respondent-driven sampling (RDS) is frequently used when sampling\nhard-to-reach and/or stigmatized communities. RDS utilizes a peer-driven\nrecruitment mechanism where sampled individuals pass on participation coupons\nto at most $c$ of their acquaintances in the community ($c=3$ being a common\nchoice), who then in turn pass on to their acquaintances if they choose to\nparticipate, and so on. This process of distributing coupons is shown to behave\nlike a new Reed-Frost type network epidemic model, in which becoming infected\ncorresponds to receiving a coupon. The difference from existing network\nepidemic models is that an infected individual can not infect (i.e.\\ sample)\nall of its contacts, but only at most $c$ of them. We calculate $R_0$, the\nprobability of a major \"outbreak\", and the relative size of a major outbreak in\nthe limit of infinite population size and evaluate their adequacy in finite\npopulations. We study the effect of varying $c$ and compare RDS to the\ncorresponding usual epidemic models, i.e.\\ the case of $c=\\infty$. Our results\nsuggest that the number of coupons has a large effect on RDS recruitment.\nAdditionally, we use our findings to explain previous empirical observations. \n\n"}
{"id": "1411.7571", "contents": "Title: A Bayesian Semiparametric Approach to Learning About Gene-Gene\n  Interactions in Case-Control Studies Abstract: Gene-gene interactions are often regarded as playing significant roles in\ninfluencing variabilities of complex traits. Although much research has been\ndevoted to this area, to date a comprehensive statistical model that addresses\nthe various sources of uncertainties, seem to be lacking. In this paper, we\npropose and develop a novel Bayesian semiparametric approach composed of finite\nmixtures based on Dirichlet processes and a hierarchical matrix-normal\ndistribution that can comprehensively account for the unknown number of\nsub-populations and gene-gene interactions. Then, by formulating novel and\nsuitable Bayesian tests of hypotheses we attempt to single out the roles of the\ngenes, individually, and in interaction with other genes, in case-control\nstudies. We also attempt to identify the significant loci associated with the\ndisease. Our model facilitates a highly efficient parallel computing\nmethodology, combining Gibbs sampling and Transformation based MCMC (TMCMC).\nApplication of our ideas to biologically realistic data sets revealed quite\nencouraging performance. We also applied our ideas to a real, myocardial\ninfarction dataset, and obtained interesting results that partly agree with,\nand also complement, the existing works in this area, to reveal the importance\nof sophisticated and realistic modeling of gene-gene interactions. \n\n"}
{"id": "1412.0176", "contents": "Title: Differential geometry-based solvation and electrolyte transport models\n  for biomolecular modeling: a review Abstract: This chapter reviews the differential geometry-based solvation and\nelectrolyte transport for biomolecular solvation that have been developed over\nthe past decade. A key component of these methods is the differential geometry\nof surfaces theory, as applied to the solvent-solute boundary. In these\napproaches, the solvent-solute boundary is determined by a variational\nprinciple that determines the major physical observables of interest, for\nexample, biomolecular surface area, enclosed volume, electrostatic potential,\nion density, electron density, etc. Recently, differential geometry theory has\nbeen used to define the surfaces that separate the microscopic (solute) domains\nfor biomolecules from the macroscopic (solvent) domains. In these approaches,\nthe microscopic domains are modeled with atomistic or quantum mechanical\ndescriptions, while continuum mechanics models (including fluid mechanics,\nelastic mechanics, and continuum electrostatics) are applied to the macroscopic\ndomains. This multiphysics description is integrated through an energy\nfunctional formalism and the resulting Euler-Lagrange equation is employed to\nderive a variety of governing partial differential equations for different\nsolvation and transport processes; e.g., the Laplace-Beltrami equation for the\nsolvent-solute interface, Poisson or Poisson-Boltzmann equations for\nelectrostatic potentials, the Nernst-Planck equation for ion densities, and the\nKohn-Sham equation for solute electron density. Extensive validation of these\nmodels has been carried out over hundreds of molecules, including proteins and\nion channels, and the experimental data have been compared in terms of\nsolvation energies, voltage-current curves, and density distributions. We also\npropose a new quantum model for electrolyte transport. \n\n"}
{"id": "1412.1977", "contents": "Title: Quantum metrology with non-equilibrium steady states of quantum spin\n  chains Abstract: We consider parameter estimations with probes being the boundary\ndriven/dissipated non- equilibrium steady states of XXZ spin 1/2 chains. The\nparameters to be estimated are the dissipation coupling and the anisotropy of\nthe spin-spin interaction. In the weak coupling regime we compute the scaling\nof the Fisher information, i.e. the inverse best sensitivity among all\nestimators, with the number of spins. We find superlinear scalings and\ntransitions between the distinct, isotropic and anisotropic, phases. We also\nlook at the best relative error which decreases with the number of particles\nfaster than the shot-noise only for the estimation of anisotropy. \n\n"}
{"id": "1412.2345", "contents": "Title: Group Symmetric Robust Covariance Estimation Abstract: In this paper we consider Tyler's robust covariance M-estimator under group\nsymmetry constraints. We assume that the covariance matrix is invariant to the\nconjugation action of a unitary matrix group, referred to as group symmetry.\nExamples of group symmetric structures include circulant, perHermitian and\nproper quaternion matrices. We introduce a group symmetric version of Tyler's\nestimator (STyler) and provide an iterative fixed point algorithm to compute\nit. The classical results claim that at least n=p+1 sample points in general\nposition are necessary to ensure the existence and uniqueness of Tyler's\nestimator, where p is the ambient dimension. We show that the STyler requires\nsignificantly less samples. In some groups even two samples are enough to\nguarantee its existence and uniqueness. In addition, in the case of elliptical\npopulations, we provide high probability bounds on the error of the STyler.\nThese too, quantify the advantage of exploiting the symmetry structure.\nFinally, these theoretical results are supported by numerical simulations.ted\nby numerical simulations. \n\n"}
{"id": "1412.5000", "contents": "Title: Randomization Inference for Treatment Effect Variation Abstract: Applied researchers are increasingly interested in whether and how treatment\neffects vary in randomized evaluations, especially variation not explained by\nobserved covariates. We propose a model-free approach for testing for the\npresence of such unexplained variation. To use this randomization-based\napproach, we must address the fact that the average treatment effect, generally\nthe object of interest in randomized experiments, actually acts as a nuisance\nparameter in this setting. We explore potential solutions and advocate for a\nmethod that guarantees valid tests in finite samples despite this nuisance. We\nalso show how this method readily extends to testing for heterogeneity beyond a\ngiven model, which can be useful for assessing the sufficiency of a given\nscientific theory. We finally apply our method to the National Head Start\nImpact Study, a large-scale randomized evaluation of a Federal preschool\nprogram, finding that there is indeed significant unexplained treatment effect\nvariation. \n\n"}
{"id": "1412.5530", "contents": "Title: Electron production by sensitizing gold nanoparticles irradiated by fast\n  ions Abstract: The yield of electrons generated by gold nanoparticles due to irradiation by\nfast charged projectiles is estimated. The results of calculations are compared\nto those obtained for pure water medium. It is demonstrated that a significant\nincrease in the number of emitted electrons arises from collective electron\nexcitations in the nanoparticle. The dominating enhancement mechanisms are\nrelated to the formation of (i) plasmons excited in a whole nanoparticle, and\n(ii) atomic giant resonances due to excitation of d electrons in individual\natoms. Decay of the collective electron excitations in a nanoparticle embedded\nin a biological medium thus represents an important mechanism of the low-energy\nelectron production. Parameters of the utilized model approach are justified\nthrough the calculation of the photoabsorption spectra of several gold\nnanoparticles, performed by means of time-dependent density-functional theory. \n\n"}
{"id": "1412.5568", "contents": "Title: Vibration-assisted coherent excitation energy transfer in a detuning\n  system Abstract: The roles of the vibration motions played in the excitation energy transfer\nprocess are studied. It is found that a strong coherent transfer in the hybrid\nsystem emerges when the detuning between the donor and the acceptor equals the\nintrinsic frequency of the vibrational mode, and as a result the energy can be\ntransferred into the acceptor much effectively. Three cases of the donor and\nthe acceptor coupling with vibrational modes are investigated respectively. We\nfind that the quantum interference between the two different transfer channels\nvia the vibrational modes can affects the dynamics of the system significantly. \n\n"}
{"id": "1412.5656", "contents": "Title: A Note on Minimax Testing and Confidence Intervals in Moment Inequality\n  Models Abstract: This note uses a simple example to show how moment inequality models used in\nthe empirical economics literature lead to general minimax relative efficiency\ncomparisons. The main point is that such models involve inference on a low\ndimensional parameter, which leads naturally to a definition of \"distance\"\nthat, in full generality, would be arbitrary in minimax testing problems. This\ndefinition of distance is justified by the fact that it leads to a duality\nbetween minimaxity of confidence intervals and tests, which does not hold for\nother definitions of distance. Thus, the use of moment inequalities for\ninference in a low dimensional parametric model places additional structure on\nthe testing problem, which leads to stronger conclusions regarding minimax\nrelative efficiency than would otherwise be possible. \n\n"}
{"id": "1412.5731", "contents": "Title: Optimizing User Association and Spectrum Allocation in HetNets: A\n  Utility Perspective Abstract: The joint user association and spectrum allocation problem is studied for\nmulti-tier heterogeneous networks (HetNets) in both downlink and uplink in the\ninterference-limited regime. Users are associated with base-stations (BSs)\nbased on the biased downlink received power. Spectrum is either shared or\northogonally partitioned among the tiers. This paper models the placement of\nBSs in different tiers as spatial point processes and adopts stochastic\ngeometry to derive the theoretical mean proportionally fair utility of the\nnetwork based on the coverage rate. By formulating and solving the network\nutility maximization problem, the optimal user association bias factors and\nspectrum partition ratios are analytically obtained for the multi-tier network.\nThe resulting analysis reveals that the downlink and uplink user associations\ndo not have to be symmetric. For uplink under spectrum sharing, if all tiers\nhave the same target signal-to-interference ratio (SIR), distance-based user\nassociation is shown to be optimal under a variety of path loss and power\ncontrol settings. For both downlink and uplink, under orthogonal spectrum\npartition, it is shown that the optimal proportion of spectrum allocated to\neach tier should match the proportion of users associated with that tier.\nSimulations validate the analytical results. Under typical system parameters,\nsimulation results suggest that spectrum partition performs better for downlink\nin terms of utility, while spectrum sharing performs better for uplink with\npower control. \n\n"}
{"id": "1412.6799", "contents": "Title: Strong coupling electrostatics for randomly charged surfaces:\n  Antifragility and effective interactions Abstract: We study the effective interaction mediated by strongly coupled Coulomb\nfluids between dielectric surfaces carrying quenched, random monopolar charges\nwith equal mean and variance, both when the Coulomb fluid consists only of\nmobile multivalent counterions and when it consists of an asymmetric ionic\nmixture containing multivalent and monovalent (salt) ions in equilibrium with\nan aqueous bulk reservoir. We analyze the consequences that follow from the\ninterplay between surface charge disorder, dielectric and salt image effects,\nand the strong electrostatic coupling that results from multivalent counterions\non the distribution of these ions and the effective interaction pressure they\nmediate between the surfaces. In a dielectrically homogeneous system, we show\nthat the multivalent counterions are attracted towards the surfaces with a\nsingular, disorder-induced potential that diverges logarithmically on approach\nto the surfaces, creating a singular counterion density profile with an\nalgebraic divergence at the surfaces. This effect drives the system towards a\nstate of lower thermal \"disorder\", one that can be described by a renormalized\ntemperature, exhibiting thus a remarkable antifragility. The interaction\npressure acting on the surfaces displays in general a highly non-monotonic\nbehavior as a function of the inter-surface separation with a prominent regime\nof attraction at small to intermediate separations. This attraction is caused\ndirectly by the combined effects from charge disorder and strong coupling\nelectrostatics of multivalent counterions, which can be quite significant even\nwith a small degree of surface charge disorder relative to the mean surface\ncharge. The strong coupling, disorder-induced attraction is typically far more\nstronger than the van der Waals interaction between the surfaces, especially\nwithin a range of several nanometers for the inter-surface separation. \n\n"}
{"id": "1412.8416", "contents": "Title: Joint Optimization of Radio and Computational Resources for Multicell\n  Mobile-Edge Computing Abstract: Migrating computational intensive tasks from mobile devices to more\nresourceful cloud servers is a promising technique to increase the\ncomputational capacity of mobile devices while saving their battery energy. In\nthis paper, we consider a MIMO multicell system where multiple mobile users\n(MUs) ask for computation offloading to a common cloud server. We formulate the\noffloading problem as the joint optimization of the radio resources-the\ntransmit precoding matrices of the MUs-and the computational resources-the CPU\ncycles/second assigned by the cloud to each MU-in order to minimize the overall\nusers' energy consumption, while meeting latency constraints. The resulting\noptimization problem is nonconvex (in the objective function and constraints).\nNevertheless, in the single-user case, we are able to express the global\noptimal solution in closed form. In the more challenging multiuser scenario, we\npropose an iterative algorithm, based on a novel successive convex\napproximation technique, converging to a local optimal solution of the original\nnonconvex problem. Then, we reformulate the algorithm in a distributed and\nparallel implementation across the radio access points, requiring only a\nlimited coordination/signaling with the cloud. Numerical results show that the\nproposed schemes outperform disjoint optimization algorithms. \n\n"}
{"id": "1412.8708", "contents": "Title: Full Duplex Operation for Small Cells Abstract: Full duplex (FD) communications has the potential to double the capacity of a\nhalf duplex (HD) system at the link level. However, FD operation increases the\naggregate interference on each communication link, which limits the capacity\nimprovement. In this paper, we investigate how much of the potential doubling\ncan be practically achieved in the resource-managed, small multi-cellular\nsystem, similar to the TDD variant of LTE, both in indoor and outdoor\nenvironments, assuming FD base stations (BSs) and HD user equipment (UEs). We\nfocus on low-powered small cellular systems, because they are more suitable for\nFD operation given practical self-interference cancellation limits. A joint UE\nselection and power allocation method for a multi-cell scenario is presented,\nwhere a hybrid scheduling policy assigns FD timeslots when it provides a\nthroughput advantage by pairing UEs with appropriate power levels to mitigate\nthe mutual interference, but otherwise defaults to HD operation. Due to the\ncomplexity of finding the globally optimum solution of the proposed algorithm,\na sub-optimal method based on a heuristic greedy algorithm for UE selection,\nand a novel solution using geometric programming for power allocation, is\nproposed. With practical self-interference cancellation, antennas and circuits,\nit is shown that the proposed hybrid FD system achieves as much as 94%\nthroughput improvement in the downlink, and 92% in the uplink, compared to a HD\nsystem in an indoor multi-cell scenario and 54% in downlink and 61% in uplink\nin an outdoor multi-cell scenario. Further, we also compare the energy\nefficiency of FD operation. \n\n"}
{"id": "1501.00078", "contents": "Title: Joint Downlink Cell Association and Bandwidth Allocation for Wireless\n  Backhauling in Two-Tier HetNets with Large-Scale Antenna Arrays Abstract: The problem of joint downlink cell association (CA) and wireless backhaul\nbandwidth allocation (WBBA) in two-tier cellular heterogeneous networks\n(HetNets) is considered. Large-scale antenna array is implemented at the macro\nbase station (BS), while the small cells within the macro cell range are\nsingle-antenna BSs and they rely on over-the-air links to the macro BS for\nbackhauling. A sum logarithmic user rate maximization problem is investigated\nconsidering wireless backhauling constraints. A duplex and spectrum sharing\nscheme based on co-channel reverse time-division duplex (TDD) and dynamic soft\nfrequency reuse (SFR) is proposed for interference management in two-tier\nHetNets with large-scale antenna arrays at the macro BS and wireless\nbackhauling for small cells. Two in-band WBBA scenarios, namely, unified\nbandwidth allocation and per-small-cell bandwidth allocation scenarios, are\ninvestigated for joint CA-WBBA in the HetNet. A two-level hierarchical\ndecomposition method for relaxed optimization is employed to solve the\nmixed-integer nonlinear program (MINLP). Solutions based on the General\nAlgorithm Modeling System (GAMS) optimization solver and fast heuristics are\nalso proposed for cell association in the per-small-cell WBBA scenario. It is\nshown that when all small cells have to use in-band wireless backhaul, the\nsystem load has more impact on both the sum log-rate and per-user rate\nperformance than the number of small cells deployed within the macro cell\nrange. The proposed joint CA-WBBA algorithms have an optimal load approximately\nequal to the size of the large-scale antenna array at the macro BS. The cell\nrange expansion (CRE) strategy, which is an efficient cell association scheme\nfor HetNets with perfect backhauling, is shown to be inefficient when in-band\nwireless backhauling for small cells comes into play. \n\n"}
{"id": "1501.00592", "contents": "Title: Robust Classification of High Dimension Low Sample Size Data Abstract: The robustification of pattern recognition techniques has been the subject of\nintense research in recent years. Despite the multiplicity of papers on the\nsubject, very few articles have deeply explored the topic of robust\nclassification in the high dimension low sample size context. In this work, we\nexplore and compare the predictive performances of robust classification\ntechniques with a special concentration on robust discriminant analysis and\nrobust PCA applied to a wide variety of large $p$ small $n$ data sets. We also\nexplore the performance of random forest by way of comparing and contrasting\nthe differences single model methods and ensemble methods in this context. Our\nwork reveals that Random Forest, although not inherently designed to be robust\nto outliers, substantially outperforms the existing techniques specifically\ndesigned to achieve robustness. Indeed, random forest emerges as the best\npredictively on both real life and simulated data. \n\n"}
{"id": "1501.01668", "contents": "Title: Handoff Rate and Coverage Analysis in Multi-tier Heterogeneous Networks Abstract: This paper analyzes the impact of user mobility in multi-tier heterogeneous\nnetworks. We begin by obtaining the handoff rate for a mobile user in an\nirregular cellular network with the access point locations modeled as a\nhomogeneous Poisson point process. The received signal-to-interference-ratio\n(SIR) distribution along with a chosen SIR threshold is then used to obtain the\nprobability of coverage. To capture potential connection failures due to\nmobility, we assume that a fraction of handoffs result in such failures.\nConsidering a multi-tier network with orthogonal spectrum allocation among\ntiers and the maximum biased average received power as the tier association\nmetric, we derive the probability of coverage for two cases: 1) the user is\nstationary (i.e., handoffs do not occur, or the system is not sensitive to\nhandoffs); 2) the user is mobile, and the system is sensitive to handoffs. We\nderive the optimal bias factors to maximize the coverage. We show that when the\nuser is mobile, and the network is sensitive to handoffs, both the optimum tier\nassociation and the probability of coverage depend on the user's speed; a\nspeed-dependent bias factor can then adjust the tier association to effectively\nimprove the coverage, and hence system performance, in a fully-loaded network. \n\n"}
{"id": "1501.02467", "contents": "Title: Fast and optimal nonparametric sequential design for astronomical\n  observations Abstract: The spectral energy distribution (SED) is a relatively easy way for\nastronomers to distinguish between different astronomical objects such as\ngalaxies, black holes, and stellar objects. By comparing the observations from\na source at different frequencies with template models, astronomers are able to\ninfer the type of this observed object. In this paper, we take a Bayesian model\naveraging perspective to learn astronomical objects, employing a Bayesian\nnonparametric approach to accommodate the deviation from convex combinations of\nknown log-SEDs. To effectively use telescope time for observations, we then\nstudy Bayesian nonparametric sequential experimental design without conjugacy,\nin which we use sequential Monte Carlo as an efficient tool to maximize the\nvolume of information stored in the posterior distribution of the parameters of\ninterest. A new technique for performing inferences in log-Gaussian Cox\nprocesses called the Poisson log-normal approximation is also proposed.\nSimulations show the speed, accuracy, and usefulness of our method. While the\nstrategy we propose in this paper is brand new in the astronomy literature, the\ninferential techniques developed apply to more general nonparametric sequential\nexperimental design problems. \n\n"}
{"id": "1501.03185", "contents": "Title: Post-Selection and Post-Regularization Inference in Linear Models with\n  Many Controls and Instruments Abstract: In this note, we offer an approach to estimating causal/structural parameters\nin the presence of many instruments and controls based on methods for\nestimating sparse high-dimensional models. We use these high-dimensional\nmethods to select both which instruments and which control variables to use.\nThe approach we take extends BCCH2012, which covers selection of instruments\nfor IV models with a small number of controls, and extends BCH2014, which\ncovers selection of controls in models where the variable of interest is\nexogenous conditional on observables, to accommodate both a large number of\ncontrols and a large number of instruments. We illustrate the approach with a\nsimulation and an empirical example. Technical supporting material is available\nin a supplementary online appendix. \n\n"}
{"id": "1501.06674", "contents": "Title: Long-range coherent energy transport in Photosystem II Abstract: We simulate the long-range inter-complex electronic energy transfer in\nPhotosystem II -- from the antenna complex, via a core complex, to the reaction\ncenter -- using a non-Markovian (ZOFE) quantum master equation description that\nallows us to quantify the electronic coherence involved in the energy transfer.\nWe identify the pathways of the energy transfer in the network of coupled\nchromophores, using a description based on excitation probability currents. We\ninvestigate how the energy transfer depends on the initial excitation --\nlocalized, coherent initial excitation versus delocalized, incoherent initial\nexcitation -- and find that the energy transfer is remarkably robust with\nrespect to such strong variations of the initial condition. To explore the\nimportance of vibrationally enhanced transfer and to address the question of\noptimization in the system parameters, we vary the strength of the coupling\nbetween the electronic and the vibrational degrees of freedom. We find that the\noriginal parameters lie in a (broad) region that enables optimal transfer\nefficiency, and that the energy transfer appears to be very robust with respect\nto variations in the vibronic coupling. Nevertheless, vibrationally enhanced\ntransfer appears to be crucial to obtain a high transfer efficiency. We compare\nour quantum simulation to a \"classical\" rate equation based on a\nmodified-Redfield/generalized-F\\\"orster description that was previously used to\nsimulate energy transfer dynamics in the entire Photosystem II complex, and\nfind very good agreement between quantum and rate-equation simulation of the\noverall energy transfer dynamics. \n\n"}
{"id": "1502.00680", "contents": "Title: Quasi-Centralized Limit Order Books Abstract: A quasi-centralized limit order book (QCLOB) is a limit order book (LOB) in\nwhich financial institutions can only access the trading opportunities offered\nby counterparties with whom they possess sufficient bilateral credit. We\nperform an empirical analysis of a recent, high-quality data set from a large\nelectronic trading platform that utilizes QCLOBs to facilitate trade. We find\nmany significant differences between our results and those widely reported for\nother LOBs. We also uncover a remarkable empirical universality: although the\ndistributions describing order flow and market state vary considerably across\ndays, a simple, linear rescaling causes them to collapse onto a single curve.\nMotivated by this finding, we propose a semi-parametric model of order flow and\nmarket state in a QCLOB on a single trading day. Our model provides similar\nperformance to that of parametric curve-fitting techniques, while being simpler\nto compute and faster to implement. \n\n"}
{"id": "1502.00757", "contents": "Title: The use of covariates and random effects in evaluating predictive\n  biomarkers under a potential outcome framework Abstract: Predictive or treatment selection biomarkers are usually evaluated in a\nsubgroup or regression analysis with focus on the treatment-by-marker\ninteraction. Under a potential outcome framework (Huang, Gilbert and Janes\n[Biometrics 68 (2012) 687-696]), a predictive biomarker is considered a\npredictor for a desirable treatment benefit (defined by comparing potential\noutcomes for different treatments) and evaluated using familiar concepts in\nprediction and classification. However, the desired treatment benefit is\nunobservable because each patient can receive only one treatment in a typical\nstudy. Huang et al. overcome this problem by assuming monotonicity of potential\noutcomes, with one treatment dominating the other in all patients. Motivated by\nan HIV example that appears to violate the monotonicity assumption, we propose\na different approach based on covariates and random effects for evaluating\npredictive biomarkers under the potential outcome framework. Under the proposed\napproach, the parameters of interest can be identified by assuming conditional\nindependence of potential outcomes given observed covariates, and a sensitivity\nanalysis can be performed by incorporating an unobserved random effect that\naccounts for any residual dependence. Application of this approach to the\nmotivating example shows that baseline viral load and CD4 cell count are both\nuseful as predictive biomarkers for choosing antiretroviral drugs for\ntreatment-naive patients. \n\n"}
{"id": "1502.01717", "contents": "Title: Vibronic origin of long-lived coherence in an artificial molecular light\n  harvester Abstract: Natural and artificial light harvesting processes have recently gained new\ninterest. Signatures of long lasting coherence in spectroscopic signals of\nbiological systems have been repeatedly observed, albeit their origin is a\nmatter of ongoing debate, as it is unclear how the loss of coherence due to\ninteraction with the noisy environments in such systems is averted. Here we\nreport experimental and theoretical verification of coherent\nexciton-vibrational (vibronic) coupling as the origin of long-lasting coherence\nin an artificial light harvester, a molecular J-aggregate. In this\nmacroscopically aligned tubular system, polarization controlled 2D spectroscopy\ndelivers an uncongested and specific optical response as an ideal foundation\nfor an in-depth theoretical description. We derive analytical expressions that\nshow under which general conditions vibronic coupling leads to prolonged\nexcited-state coherence. \n\n"}
{"id": "1502.01780", "contents": "Title: Sequential Channel State Tracking & SpatioTemporal Channel Prediction in\n  Mobile Wireless Sensor Networks Abstract: We propose a nonlinear filtering framework for approaching the problems of\nchannel state tracking and spatiotemporal channel gain prediction in mobile\nwireless sensor networks, in a Bayesian setting. We assume that the wireless\nchannel constitutes an observable (by the sensors/network nodes),\nspatiotemporal, conditionally Gaussian stochastic process, which is\nstatistically dependent on a set of hidden channel parameters, called the\nchannel state. The channel state evolves in time according to a known, non\nstationary, nonlinear and/or non Gaussian Markov stochastic kernel. This\nformulation results in a partially observable system, with a temporally varying\nglobal state and spatiotemporally varying observations. Recognizing the\nintractability of general nonlinear state estimation, we advocate the use of\ngrid based approximate filters as an effective and robust means for recursive\ntracking of the channel state. We also propose a sequential spatiotemporal\npredictor for tracking the channel gains at any point in time and space,\nproviding real time sequential estimates for the respective channel gain map,\nfor each sensor in the network. Additionally, we show that both estimators\nconverge towards the true respective MMSE optimal estimators, in a common,\nrelatively strong sense. Numerical simulations corroborate the practical\neffectiveness of the proposed approach. \n\n"}
{"id": "1502.02312", "contents": "Title: Bayesian and empirical Bayesian forests Abstract: We derive ensembles of decision trees through a nonparametric Bayesian model,\nallowing us to view random forests as samples from a posterior distribution.\nThis insight provides large gains in interpretability, and motivates a class of\nBayesian forest (BF) algorithms that yield small but reliable performance\ngains. Based on the BF framework, we are able to show that high-level tree\nhierarchy is stable in large samples. This leads to an empirical Bayesian\nforest (EBF) algorithm for building approximate BFs on massive distributed\ndatasets and we show that EBFs outperform sub-sampling based alternatives by a\nlarge margin. \n\n"}
{"id": "1502.02363", "contents": "Title: Quantum Process Tomography by 2D Fluorescence Spectroscopy Abstract: Reconstruction of the dynamics (quantum process tomography) of the\nsingle-exciton manifold in energy transfer systems is proposed here on the\nbasis of two-dimensional fluorescence spectroscopy (2D-FS) with\nphase-modulation. The quantum-process-tomography protocol introduced here\nbenefits from, e.g., the sensitivity enhancement ascribed to 2D-FS. Although\nthe isotropically averaged spectroscopic signals depend on the quantum yield\nparameter $\\Gamma$ of the doubly-excited-exciton manifold, it is shown that the\nreconstruction of the dynamics is insensitive to this parameter. Applications\nto foundational and applied problems, as well as further extensions, are\ndiscussed. \n\n"}
{"id": "1502.02657", "contents": "Title: Efficiency of energy funneling in the photosystem II supercomplex of\n  higher plants Abstract: The investigation of energy transfer properties in photosynthetic\nmulti-protein networks gives insight into their underlying design\nprinciples.Here, we discuss excitonic energy transfer mechanisms of the\nphotosystem II (PS-II) C$_2$S$_2$M$_2$ supercomplex, which is the largest\nisolated functional unit of the photosynthetic apparatus of higher\nplants.Despite the lack of a decisive energy gradient in C$_2$S$_2$M$_2$, we\nshow that the energy transfer is directed by relaxation to low energy states.\nC$_2$S$_2$M$_2$ is not organized to form pathways with strict energetic\ndownhill transfer, which has direct consequences on the transfer efficiency,\ntransfer pathways and transfer limiting steps. The exciton dynamics is\nsensitive to small structural changes, which, for instance, are induced by the\nreorganization of vibrational coordinates. In order to incorporate the\nreorganization process in our numerical simulations, we go beyond rate\nequations and use the hierarchically coupled equation of motion approach\n(HEOM). While transfer from the peripherical antenna to the proteins in\nproximity to the reaction center occurs on a faster time scale, the final step\nof the energy transfer to the RC core is rather slow, and thus the limiting\nstep in the transfer chain. Our findings suggest that the structure of the\nPS-II supercomplex guarantees photoprotection rather than optimized efficiency. \n\n"}
{"id": "1502.02792", "contents": "Title: A Continued Fraction Resummation Form of Bath Relaxation Effect in the\n  Spin-Boson Model Abstract: In the spin-boson model, a continued fraction form is proposed to\nsystematically resum high-order quantum kinetic expansion (QKE) rate kernels,\naccounting for the bath relaxation effect beyond the second-order perturbation.\nIn particular, the analytical expression of the sixth-order QKE rate kernel is\nderived for resummation. With higher-order correction terms systematically\nextracted from higher-order rate kernels, the resummed quantum kinetic\nexpansion (RQKE) approach in the continued fraction form extends the Pade\napproximation and can fully recover the exact quantum dynamics as the expansion\norder increases. \n\n"}
{"id": "1502.03035", "contents": "Title: Sharp Threshold Detection Based on Sup-norm Error rates in\n  High-dimensional Models Abstract: We propose a new estimator, the thresholded scaled Lasso, in high dimensional\nthreshold regressions. First, we establish an upper bound on the $\\ell_\\infty$\nestimation error of the scaled Lasso estimator of Lee et al. (2012). This is a\nnon-trivial task as the literature on high-dimensional models has focused\nalmost exclusively on $\\ell_1$ and $\\ell_2$ estimation errors. We show that\nthis sup-norm bound can be used to distinguish between zero and non-zero\ncoefficients at a much finer scale than would have been possible using\nclassical oracle inequalities. Thus, our sup-norm bound is tailored to\nconsistent variable selection via thresholding.\n  Our simulations show that thresholding the scaled Lasso yields substantial\nimprovements in terms of variable selection. Finally, we use our estimator to\nshed further empirical light on the long running debate on the relationship\nbetween the level of debt (public and private) and GDP growth. \n\n"}
{"id": "1502.03494", "contents": "Title: A semiparametric spatio-temporal model for solar irradiance data Abstract: Design and operation of a utility scale photovoltaic (PV) power plant depends\non accurate modeling of the power generated, which is highly correlated with\naggregate solar irradiance on the plant's PV modules. At present, aggregate\nsolar irradiance over the area of a typical PV power plant cannot be measured\ndirectly. Rather, irradiance measurements are typically available from a few,\nrelatively small sensors and thus aggregate solar irradiance must be estimated\nfrom these data. As a step towards finding more accurate methods for estimating\naggregate irradiance from avaialble measurements, we evaluate semiparametric\nspatio-temporal models for global horizontal irradiance. Using data from a 1.2\nMW PV plant located in Lanai, Hawaii, we show that a semiparametric model can\nbe more accurate than simple intepolation between sensor locations. We\ninvestigate spatio-temporal models with separable and nonseparable covariance\nstructures and find no evidence to support assuming a separable covariance\nstructure. \n\n"}
{"id": "1502.04083", "contents": "Title: Impact of model choice on LR assessment in case of rare haplotype match\n  (frequentist approach) Abstract: The likelihood ratio (LR) measures the relative weight of forensic data\nregarding two hypotheses. Several levels of uncertainty arise if frequentist\nmethods are chosen for its assessment: the assumed population model only\napproximates the true one and its parameters are estimated through a database.\nMoreover, it may be wise to discard part of data, especially that only\nindirectly related to the hypotheses. Different reductions define different\nLRs. Therefore, it is more sensible to talk about \"a\" LR instead of \"the\" LR,\nand the error involved in the estimation should be quantified. Two frequentist\nmethods are proposed in the light of these points for the `rare type match\nproblem', that is when a match between the perpetrator's and the suspect's DNA\nprofile, never observed before in the database of reference, is to be\nevaluated. \n\n"}
{"id": "1502.06197", "contents": "Title: On Online Control of False Discovery Rate Abstract: Multiple hypotheses testing is a core problem in statistical inference and\narises in almost every scientific field. Given a sequence of null hypotheses\n$\\mathcal{H}(n) = (H_1,..., H_n)$, Benjamini and Hochberg\n\\cite{benjamini1995controlling} introduced the false discovery rate (FDR)\ncriterion, which is the expected proportion of false positives among rejected\nnull hypotheses, and proposed a testing procedure that controls FDR below a\npre-assigned significance level. They also proposed a different criterion,\ncalled mFDR, which does not control a property of the realized set of tests;\nrather it controls the ratio of expected number of false discoveries to the\nexpected number of discoveries.\n  In this paper, we propose two procedures for multiple hypotheses testing that\nwe will call \"LOND\" and \"LORD\". These procedures control FDR and mFDR in an\n\\emph{online manner}. Concretely, we consider an ordered --possibly infinite--\nsequence of null hypotheses $\\mathcal{H} = (H_1,H_2,H_3,...)$ where, at each\nstep $i$, the statistician must decide whether to reject hypothesis $H_i$\nhaving access only to the previous decisions. To the best of our knowledge, our\nwork is the first that controls FDR in this setting. This model was introduced\nby Foster and Stine \\cite{alpha-investing} whose alpha-investing rule only\ncontrols mFDR in online manner.\n  In order to compare different procedures, we develop lower bounds on the\ntotal discovery rate under the mixture model and prove that both LOND and LORD\nhave nearly linear number of discoveries. We further propose adjustment to LOND\nto address arbitrary correlation among the $p$-values. Finally, we evaluate the\nperformance of our procedures on both synthetic and real data comparing them\nwith alpha-investing rule, Benjamin-Hochberg method and a Bonferroni procedure. \n\n"}
{"id": "1502.06880", "contents": "Title: Quantum Criticality at the Origin of Life Abstract: Why life persists at the edge of chaos is a question at the very heart of\nevolution. Here we show that molecules taking part in biochemical processes\nfrom small molecules to proteins are critical quantum mechanically. Electronic\nHamiltonians of biomolecules are tuned exactly to the critical point of the\nmetal-insulator transition separating the Anderson localized insulator phase\nfrom the conducting disordered metal phase. Using tools from Random Matrix\nTheory we confirm that the energy level statistics of these biomolecules show\nthe universal transitional distribution of the metal-insulator critical point\nand the wave functions are multifractals in accordance with the theory of\nAnderson transitions. The findings point to the existence of a universal\nmechanism of charge transport in living matter. The revealed bio-conductor\nmaterial is neither a metal nor an insulator but a new quantum critical\nmaterial which can exist only in highly evolved systems and has unique material\nproperties. \n\n"}
{"id": "1503.00226", "contents": "Title: Adaptive estimation of the baseline hazard function in the Cox model by\n  model selection, with high-dimensional covariates Abstract: The purpose of this article is to provide an adaptive estimator of the\nbaseline function in the Cox model with high-dimensional covariates. We\nconsider a two-step procedure : first, we estimate the regression parameter of\nthe Cox model via a Lasso procedure based on the partial log-likelihood,\nsecondly, we plug this Lasso estimator into a least-squares type criterion and\nthen perform a model selection procedure to obtain an adaptive penalized\ncontrast estimator of the baseline function. Using non-asymptotic estimation\nresults stated for the Lasso estimator of the regression parameter, we\nestablish a non-asymptotic oracle inequality for this penalized contrast\nestimator of the baseline function, which highlights the discrepancy of the\nrate of convergence when the dimension of the covariates increases. \n\n"}
{"id": "1503.00339", "contents": "Title: Variation of word frequencies in Russian literary texts Abstract: We study the variation of word frequencies in Russian literary texts. Our\nfindings indicate that the standard deviation of a word's frequency across\ntexts depends on its average frequency according to a power law with exponent\n$0.62,$ showing that the rarer words have a relatively larger degree of\nfrequency volatility (i.e., \"burstiness\").\n  Several latent factors models have been estimated to investigate the\nstructure of the word frequency distribution. The dependence of a word's\nfrequency volatility on its average frequency can be explained by the asymmetry\nin the distribution of latent factors. \n\n"}
{"id": "1503.01271", "contents": "Title: Performance analysis of an improved MUSIC DoA estimator Abstract: This paper adresses the statistical performance of subspace DoA estimation\nusing a sensor array, in the asymptotic regime where the number of samples and\nsensors both converge to infinity at the same rate. Improved subspace DoA\nestimators were derived (termed as G-MUSIC) in previous works, and were shown\nto be consistent and asymptotically Gaussian distributed in the case where the\nnumber of sources and their DoA remain fixed. In this case, which models widely\nspaced DoA scenarios, it is proved in the present paper that the traditional\nMUSIC method also provides DoA consistent estimates having the same asymptotic\nvariances as the G-MUSIC estimates. The case of DoA that are spaced of the\norder of a beamwidth, which models closely spaced sources, is also considered.\nIt is shown that G-MUSIC estimates are still able to consistently separate the\nsources, while it is no longer the case for the MUSIC ones. The asymptotic\nvariances of G-MUSIC estimates are also evaluated. \n\n"}
{"id": "1503.03355", "contents": "Title: Automatic Unsupervised Tensor Mining with Quality Assessment Abstract: A popular tool for unsupervised modelling and mining multi-aspect data is\ntensor decomposition. In an exploratory setting, where and no labels or ground\ntruth are available how can we automatically decide how many components to\nextract? How can we assess the quality of our results, so that a domain expert\ncan factor this quality measure in the interpretation of our results? In this\npaper, we introduce AutoTen, a novel automatic unsupervised tensor mining\nalgorithm with minimal user intervention, which leverages and improves upon\nheuristics that assess the result quality. We extensively evaluate AutoTen's\nperformance on synthetic data, outperforming existing baselines on this very\nhard problem. Finally, we apply AutoTen on a variety of real datasets,\nproviding insights and discoveries. We view this work as a step towards a fully\nautomated, unsupervised tensor mining tool that can be easily adopted by\npractitioners in academia and industry. \n\n"}
{"id": "1503.05486", "contents": "Title: Molecular recognition by van der Waals interaction between polymers with\n  sequence-specific polarizabilities Abstract: We analyze van der Waals interactions between two rigid polymers with\nsequence-specific, anisotropic polarizabilities along the polymer backbones, so\nthat the dipole moments fluctuate parallel to the polymer backbones. Assuming\nthat each polymer has a quenched-in polarizability sequence which reflects, for\nexample, the polynucleotide sequence of a double-stranded DNA molecule, we\nstudy the van der Waals interaction energy between a pair of such polymers with\nrod-like structure for the cases where their respective polarizability\nsequences are (i) distinct and (ii) identical, with both zero and non-zero\ncorrelation length of the polarizability correlator along the polymer backbones\nin the latter case. For identical polymers, we find a novel $r^{-5}$ scaling\nbehavior of the van der Waals interaction energy for small inter-polymer\nseparation $r$, in contradistinction to the $r^{-4}$ scaling behavior of\ndistinct polymers, with furthermore a pronounced angular dependence favoring\nattraction between sufficiently aligned identical polymers. Such behavior can\nassist the molecular recognition between polymers. \n\n"}
{"id": "1503.06266", "contents": "Title: Moments of the log non-central chi-square distribution Abstract: The cumulants and moments of the log of the non-central chi-square\ndistribution are derived. For example, the expected log of a chi-square random\nvariable with v degrees of freedom is log(2) + psi(v/2). Applications to\nmodeling probability distributions are discussed. \n\n"}
{"id": "1503.06575", "contents": "Title: Unveiling Spatial Epidemiology of HIV with Mobile Phone Data Abstract: An increasing amount of geo-referenced mobile phone data enables the\nidentification of behavioral patterns, habits and movements of people. With\nthis data, we can extract the knowledge potentially useful for many\napplications including the one tackled in this study - understanding spatial\nvariation of epidemics. We explored the datasets collected by a cell phone\nservice provider and linked them to spatial HIV prevalence rates estimated from\npublicly available surveys. For that purpose, 224 features were extracted from\nmobility and connectivity traces and related to the level of HIV epidemic in 50\nIvory Coast departments. By means of regression models, we evaluated predictive\nability of extracted features. Several models predicted HIV prevalence that are\nhighly correlated (>0.7) with actual values. Through contribution analysis we\nidentified key elements that impact the rate of infections. Our findings\nindicate that night connectivity and activity, spatial area covered by users\nand overall migrations are strongly linked to HIV. By visualizing the\ncommunication and mobility flows, we strived to explain the spatial structure\nof epidemics. We discovered that strong ties and hubs in communication and\nmobility align with HIV hot spots. \n\n"}
{"id": "1503.07441", "contents": "Title: Neuroreceptor Activation by Vibration-Assisted Tunneling Abstract: G protein-coupled receptors (GPCRs) constitute a large family of receptor\nproteins that sense molecular signals on the exterior of a cell and activate\nsignal transduction pathways within the cell. Modeling how an agonist activates\nsuch a receptor is fundamental for an understanding of a wide variety of\nphysiological processes and it is of tremendous value for pharmacology and drug\ndesign. Inelastic electron tunneling spectroscopy (IETS) has been proposed as a\nmodel for the mechanism by which olfactory GPCRs are activated by a bound\nagonist. We apply this hypothesis to GPCRs within the mammalian nervous system\nusing quantum chemical modeling. We found that non-endogenous agonists of the\nserotonin receptor share a particular IET spectral aspect both amongst each\nother and with the serotonin molecule: a peak whose intensity scales with the\nknown agonist potencies. We propose an experiential validation of this model by\nutilizing lysergic acid dimethylamide (DAM-57), an ergot derivative, and its\ndeuterated isotopologues; we also provide theoretical predictions for\ncomparison to experiment. If validated our theory may provide new avenues for\nguided drug design and elevate methods of in silico potency/activity\nprediction. \n\n"}
{"id": "1503.07711", "contents": "Title: Ideological and Temporal Components of Network Polarization in Online\n  Political Participatory Media Abstract: Political polarization is traditionally analyzed through the ideological\nstances of groups and parties, but it also has a behavioral component that\nmanifests in the interactions between individuals. We present an empirical\nanalysis of the digital traces of politicians in politnetz.ch, a Swiss online\nplatform focused on political activity, in which politicians interact by\ncreating support links, comments, and likes. We analyze network polarization as\nthe level of intra- party cohesion with respect to inter-party connectivity,\nfinding that supports show a very strongly polarized structure with respect to\nparty alignment. The analysis of this multiplex network shows that each layer\nof interaction contains relevant information, where comment groups follow\ntopics related to Swiss politics. Our analysis reveals that polarization in the\nlayer of likes evolves in time, increasing close to the federal elections of\n2011. Furthermore, we analyze the internal social network of each party through\nmetrics related to hierarchical structures, information efficiency, and social\nresilience. Our results suggest that the online social structure of a party is\nrelated to its ideology, and reveal that the degree of connectivity across two\nparties increases when they are close in the ideological space of a multi-party\nsystem. \n\n"}
{"id": "1504.01281", "contents": "Title: Random matrix ensembles involving Gaussian Wigner and Wishart matrices,\n  and biorthogonal structure Abstract: We consider four nontrivial ensembles involving Gaussian Wigner and Wishart\nmatrices. These are relevant to problems ranging from multiantenna\ncommunication to random supergravity. We derive the matrix probability density,\nas well as the eigenvalue densities for these ensembles. In all cases the joint\neigenvalue density exhibits a biorthogonal structure. A determinantal\nrepresentation, based on a generalization of Andr\\'{e}ief's integration\nformula, is used to compactly express the $r$-point correlation function of\neigenvalues. This representation circumvents the complications encountered in\nthe usual approaches, and the answer is obtained immediately by examining the\njoint density of eigenvalues. We validate our analytical results using Monte\nCarlo simulations. \n\n"}
{"id": "1504.01452", "contents": "Title: The Performance Analysis of Coded Cache in Wireless Fading Channel Abstract: The rapid growth of data volume and the accompanying congestion problems over\nthe wireless networks have been critical issues to content providers. A novel\ntechnique, termed as coded cache, is proposed to relieve the burden. Through\ncreating coded-multicasting opportunities, the coded-cache scheme can provide\nextra performance gain over the conventional push technique that simply\npre-stores contents at local caches during the network idle period. But\nexisting works on the coded caching scheme assumed the availability of an\nerror-free shared channel accessible by each user. This paper considers the\nmore realistic scenario where each user may experience different link quality.\nIn this case, the system performance would be restricted by the user with the\nworst channel condition. And the corresponding resource allocation schemes\naimed at breaking this obstacles are developed. Specifically, we employ the\ncoded caching scheme in time division and frequency division transmission mode\nand formulate the sub-optimal problems. Power and bandwidth are allocated\nrespectively to maximum the system throughput. The simulation results show that\nthe throughput of the technique in wireless scenario will be limited and would\ndecrease as the number of users becomes sufficiently large. \n\n"}
{"id": "1504.01873", "contents": "Title: Location, location, location: Border effects in interference limited ad\n  hoc networks Abstract: Wireless networks are fundamentally limited by the intensity of the received\nsignals and by their inherent interference. It is shown here that in finite ad\nhoc networks where node placement is modelled according to a Poisson point\nprocess and no carrier sensing is employed for medium access, the SINR received\nby nodes located at the border of the network deployment/operation region is on\naverage greater than the rest. This is primarily due to the uneven interference\nlandscape of such networks which is particularly kind to border nodes giving\nrise to all sorts of performance inhomogeneities and access unfairness. Using\ntools from stochastic geometry we quantify these spatial variations and provide\nclosed form communication-theoretic results showing why the receiver's location\nis so important. \n\n"}
{"id": "1504.02346", "contents": "Title: Optimal User Association for Massive MIMO Empowered Ultra-Dense Wireless\n  Networks Abstract: Ultra network densification and Massive MIMO are considered major 5G enablers\nsince they promise huge capacity gains by exploiting proximity, spectral and\nspatial reuse benefits. Both approaches rely on increasing the number of access\nelements per user, either through deploying more access nodes over an area or\nincreasing the number of antenna elements per access node. At the\nnetwork-level, optimal user-association for a densely and randomly deployed\nnetwork of Massive MIMO empowered access nodes must account for both channel\nand load conditions. In this paper we formulate this complex problem, report\nits computationally intractability and reformulate it to a plausible form,\namenable to acquire a global optimal solution with reasonable complexity. We\napply the proposed optimization model to typical ultra-dense outdoor small-cell\nsetups and demonstrate: (i) the significant impact of optimal user-association\nto the achieved rate levels compared to a baseline strategy, and (ii) the\noptimality of alternative network access element deployment strategies. \n\n"}
{"id": "1504.05753", "contents": "Title: Efficient Sequential Monte-Carlo Samplers for Bayesian Inference Abstract: In many problems, complex non-Gaussian and/or nonlinear models are required\nto accurately describe a physical system of interest. In such cases, Monte\nCarlo algorithms are remarkably flexible and extremely powerful approaches to\nsolve such inference problems. However, in the presence of a high-dimensional\nand/or multimodal posterior distribution, it is widely documented that standard\nMonte-Carlo techniques could lead to poor performance. In this paper, the study\nis focused on a Sequential Monte-Carlo (SMC) sampler framework, a more robust\nand efficient Monte Carlo algorithm. Although this approach presents many\nadvantages over traditional Monte-Carlo methods, the potential of this emergent\ntechnique is however largely underexploited in signal processing. In this work,\nwe aim at proposing some novel strategies that will improve the efficiency and\nfacilitate practical implementation of the SMC sampler specifically for signal\nprocessing applications. Firstly, we propose an automatic and adaptive strategy\nthat selects the sequence of distributions within the SMC sampler that\nminimizes the asymptotic variance of the estimator of the posterior\nnormalization constant. This is critical for performing model selection in\nmodelling applications in Bayesian signal processing. The second original\ncontribution we present improves the global efficiency of the SMC sampler by\nintroducing a novel correction mechanism that allows the use of the particles\ngenerated through all the iterations of the algorithm (instead of only\nparticles from the last iteration). This is a significant contribution as it\nremoves the need to discard a large portion of the samples obtained, as is\nstandard in standard SMC methods. This will improve estimation performance in\npractical settings where computational budget is important to consider. \n\n"}
{"id": "1504.05837", "contents": "Title: New Perspectives on Multiple Source Localization in Wireless Sensor\n  Networks Abstract: In this paper we address the challenging problem of multiple source\nlocalization in Wireless Sensor Networks (WSN). We develop an efficient\nstatistical algorithm, based on the novel application of Sequential Monte Carlo\n(SMC) sampler methodology, that is able to deal with an unknown number of\nsources given quantized data obtained at the fusion center from different\nsensors with imperfect wireless channels. We also derive the Posterior\nCram\\'er-Rao Bound (PCRB) of the source location estimate. The PCRB is used to\nanalyze the accuracy of the proposed SMC sampler algorithm and the impact that\nquantization has on the accuracy of location estimates of the sources.\nExtensive experiments show that the benefits of the proposed scheme in terms of\nthe accuracy of the estimation method that are required for model selection\n(i.e., the number of sources) and the estimation of the source characteristics\ncompared to the classical importance sampling method. \n\n"}
{"id": "1504.06896", "contents": "Title: False Positives and Other Statistical Errors in Standard Analyses of Eye\n  Movements in Reading Abstract: In research on eye movements in reading, it is common to analyze a number of\ncanonical dependent measures to study how the effects of a manipulation unfold\nover time. Although this gives rise to the well-known multiple comparisons\nproblem, i.e. an inflated probability that the null hypothesis is incorrectly\nrejected (Type I error), it is accepted standard practice not to apply any\ncorrection procedures. Instead, there appears to be a widespread belief that\ncorrections are not necessary because the increase in false positives is too\nsmall to matter. To our knowledge, no formal argument has ever been presented\nto justify this assumption. Here, we report a computational investigation of\nthis issue using Monte Carlo simulations. Our results show that, contrary to\nconventional wisdom, false positives are increased to unacceptable levels when\nno corrections are applied. Our simulations also show that counter-measures\nlike the Bonferroni correction keep false positives in check while reducing\nstatistical power only moderately. Hence, there is little reason why such\ncorrections should not be made a standard requirement. Further, we discuss\nthree statistical illusions that can arise when statistical power is low, and\nwe show how power can be improved to prevent these illusions. In sum, our work\nrenders a detailed picture of the various types of statistical errors than can\noccur in studies of reading behavior and we provide concrete guidance about how\nthese errors can be avoided. \n\n"}
{"id": "1505.01147", "contents": "Title: Prediction and Quantification of Individual Athletic Performance Abstract: We provide scientific foundations for athletic performance prediction on an\nindividual level, exposing the phenomenology of individual athletic running\nperformance in the form of a low-rank model dominated by an individual power\nlaw. We present, evaluate, and compare a selection of methods for prediction of\nindividual running performance, including our own, \\emph{local matrix\ncompletion} (LMC), which we show to perform best. We also show that many\ndocumented phenomena in quantitative sports science, such as the form of\nscoring tables, the success of existing prediction methods including Riegel's\nformula, the Purdy points scheme, the power law for world records performances\nand the broken power law for world record speeds may be explained on the basis\nof our findings in a unified way. \n\n"}
{"id": "1505.01920", "contents": "Title: Will the Area Spectral Efficiency Monotonically Grow as Small Cells Go\n  Dense? Abstract: In this paper, we introduce a sophisticated path loss model into the\nstochastic geometry analysis incorporating both line-of-sight (LoS) and\nnon-line-of-sight (NLoS) transmissions to study their performance impact in\nsmall cell networks (SCNs). Analytical results are obtained on the coverage\nprobability and the area spectral efficiency (ASE) assuming both a general path\nloss model and a special case of path loss model recommended by the 3rd\nGeneration Partnership Project (3GPP) standards. The performance impact of LoS\nand NLoS transmissions in SCNs in terms of the coverage probability and the ASE\nis shown to be significant both quantitatively and qualitatively, compared with\nprevious work that does not differentiate LoS and NLoS transmissions.\nParticularly, our analysis demonstrates that when the density of small cells is\nlarger than a threshold, the network coverage probability will decrease as\nsmall cells become denser, which in turn makes the ASE suffer from a slow\ngrowth or even a notable decrease. For practical regime of small cell density,\nthe performance results derived from our analysis are distinctively different\nfrom previous results, and shed new insights on the design and deployment of\nfuture dense/ultra-dense SCNs. \n\n"}
{"id": "1505.02130", "contents": "Title: Dynamics and quantumness of excitation energy transfer through a complex\n  quantum network Abstract: Understanding the mechanisms of efficient and robust energy transfer in\norganic systems provides us with new insights for the optimal design of\nartificial systems. In this paper, we explore the dynamics of excitation energy\ntransfer (EET) through a complex quantum network by a toy model consisting of\nthree sites coupled to environments. We study how the coherent evolution and\nthe noise-induced decoherence work together to reach efficient EET and\nillustrate the role of the phase factor attached to the coupling constant in\nthe EET. By comparing the differences between the Markovian and non-Markovian\ndynamics, we discuss the effect of environment and the spatial structure of\nsystem on the dynamics and the efficiency of EET. A intuitive picture is given\nto show how the exciton is transferred through the system. Employing the simple\nmodel, we show the robustness of EET efficiency under the influence of the\nenvironment and elucidate the important role of quantum coherence in EET. We go\nfurther to study the quantum feature of the EET dynamics by {\\it quantumness}\nand show the importance of quantum coherence from a new respect. We calculate\nthe energy current in the EET and its quantumness, results for different system\nparameters are presented and discussed. \n\n"}
{"id": "1505.04827", "contents": "Title: Semi-Markov Arnason-Schwarz models Abstract: We consider multi-state capture-recapture-recovery data where observed\nindividuals are recorded in a set of possible discrete states. Traditionally,\nthe Arnason-Schwarz model has been fitted to such data where the state process\nis modeled as a first-order Markov chain, though second-order models have also\nbeen proposed and fitted to data. However, low-order Markov models may not\naccurately represent the underlying biology. For example, specifying a\n(time-independent) first-order Markov process assumes that the dwell time in\neach state (i.e., the duration of a stay in a given state) has a geometric\ndistribution, and hence that the modal dwell time is one. Specifying\ntime-dependent or higher-order processes provides additional flexibility, but\nat the expense of a potentially significant number of additional model\nparameters. We extend the Arnason-Schwarz model by specifying a semi-Markov\nmodel for the state process, where the dwell-time distribution is specified\nmore generally, using for example a shifted Poisson or negative binomial\ndistribution. A state expansion technique is applied in order to represent the\nresulting semi-Markov Arnason-Schwarz model in terms of a simpler and\ncomputationally tractable hidden Markov model. Semi-Markov Arnason-Schwarz\nmodels come with only a very modest increase in the number of parameters, yet\npermit a significantly more flexible state process. Model selection can be\nperformed using standard procedures, and in particular via the use of\ninformation criteria. The semi-Markov approach allows for important biological\ninference to be drawn on the underlying state process, for example on the times\nspent in the different states. The feasibility of the approach is demonstrated\nin a simulation study, before being applied to real data corresponding to house\nfinches where the states correspond to the presence or absence of\nconjunctivitis. \n\n"}
{"id": "1505.04944", "contents": "Title: Coexisting Success Probability and Throughput of Multi-RAT Wireless\n  Networks with Unlicensed Band Access Abstract: In this letter, the coexisting success probability and throughput of a\nwireless network consisting of multiple subnetworks of different radio access\ntechnologies (RATs) is investigated. The coexisting success probability that is\ndefined as the average of all success probabilities of all subnetworks is found\nin closed-form and it will be shown to have the concavity over the number of\nchannels in the unlicensed band. The optimal deployment densities of all\ndifferent RATs access points (APs) that maximize the coexisting success\nprobability are shown to exist and can be found under the derived constraint on\nnetwork parameters. The coexisting throughput is defined as the per-channel sum\nof all spectrum efficiencies of all subnetworks and numerical results show that\nit is significantly higher than the throughput of the unlicensed band only\naccessed by WiFi APs. \n\n"}
{"id": "1505.05816", "contents": "Title: An empirical approach to demographic inference with genomic data Abstract: Inference with population genetic data usually treats the population pedigree\nas a nuisance parameter, the unobserved product of a past history of random\nmating. However, the history of genetic relationships in a given population is\na fixed, unobserved object, and so an alternative approach is to treat this\nnetwork of relationships as a complex object we wish to learn about, by\nobserving how genomes have been noisily passed down through it. This paper\nexplores this point of view, showing how to translate questions about\npopulation genetic data into calculations with a Poisson process of mutations\non all ancestral genomes. This method is applied to give a robust\ninterpretation to the $f_4$ statistic used to identify admixture, and to design\na new statistic that measures covariances in mean times to most recent common\nancestor between two pairs of sequences. The method more generally interprets\npopulation genetic statistics in terms of sums of specific functions over\nancestral genomes, thereby providing concrete, broadly interpretable\ninterpretations for these statistics. This provides a method for describing\ndemographic history without simplified demographic models. More generally, it\nbrings into focus the population pedigree, which is averaged over in\nmodel-based demographic inference. \n\n"}
{"id": "1505.06856", "contents": "Title: WiFlix: Adaptive Video Streaming in Massive MU-MIMO Wireless Networks Abstract: We consider the problem of simultaneous on-demand streaming of stored video\nto multiple users in a multi-cell wireless network where multiple unicast\nstreaming sessions are run in parallel and share the same frequency band. Each\nstreaming session is formed by the sequential transmission of video \"chunks,\"\nsuch that each chunk arrives into the corresponding user playback buffer within\nits playback deadline. We formulate the problem as a Network Utility\nMaximization (NUM) where the objective is to fairly maximize users' video\nstreaming Quality of Experience (QoE) and then derive an iterative control\npolicy using Lyapunov Optimization, which solves the NUM problem up to any\nlevel of accuracy and yields an online protocol with control actions at every\niteration decomposing into two layers interconnected by the users' request\nqueues : i) a video streaming adaptation layer reminiscent of DASH, implemented\nat each user node; ii) a transmission scheduling layer where a max-weight\nscheduler is implemented at each base station. The proposed chunk request\nscheme is a pull strategy where every user opportunistically requests video\nchunks from the neighboring base stations and dynamically adapts the quality of\nits requests based on the current size of the request queue. For the\ntransmission scheduling component, we first describe the general max-weight\nscheduler and then particularize it to a wireless network where the base\nstations have multiuser MIMO (MU-MIMO) beamforming capabilities. We exploit the\nchannel hardening effect of large-dimensional MIMO channels (massive MIMO) and\ndevise a low complexity user selection scheme to solve the underlying\ncombinatorial problem of selecting user subsets for downlink beamforming, which\ncan be easily implemented and run independently at each base station. \n\n"}
{"id": "1505.07649", "contents": "Title: A trust-region method for stochastic variational inference with\n  applications to streaming data Abstract: Stochastic variational inference allows for fast posterior inference in\ncomplex Bayesian models. However, the algorithm is prone to local optima which\ncan make the quality of the posterior approximation sensitive to the choice of\nhyperparameters and initialization. We address this problem by replacing the\nnatural gradient step of stochastic varitional inference with a trust-region\nupdate. We show that this leads to generally better results and reduced\nsensitivity to hyperparameters. We also describe a new strategy for variational\ninference on streaming data and show that here our trust-region method is\ncrucial for getting good performance. \n\n"}
{"id": "1505.07661", "contents": "Title: Reactive point processes: A new approach to predicting power failures in\n  underground electrical systems Abstract: Reactive point processes (RPPs) are a new statistical model designed for\npredicting discrete events in time based on past history. RPPs were developed\nto handle an important problem within the domain of electrical grid\nreliability: short-term prediction of electrical grid failures (\"manhole\nevents\"), including outages, fires, explosions and smoking manholes, which can\ncause threats to public safety and reliability of electrical service in cities.\nRPPs incorporate self-exciting, self-regulating and saturating components. The\nself-excitement occurs as a result of a past event, which causes a temporary\nrise in vulner ability to future events. The self-regulation occurs as a result\nof an external inspection which temporarily lowers vulnerability to future\nevents. RPPs can saturate when too many events or inspections occur close\ntogether, which ensures that the probability of an event stays within a\nrealistic range. Two of the operational challenges for power companies are (i)\nmaking continuous-time failure predictions, and (ii) cost/benefit analysis for\ndecision making and proactive maintenance. RPPs are naturally suited for\nhandling both of these challenges. We use the model to predict power-grid\nfailures in Manhattan over a short-term horizon, and to provide a cost/benefit\nanalysis of different proactive maintenance programs. \n\n"}
{"id": "1505.07752", "contents": "Title: The Impact of Estimation: A New Method for Clustering and Trajectory\n  Estimation in Patient Flow Modeling Abstract: The ability to accurately forecast and control inpatient census, and thereby\nworkloads, is a critical and longstanding problem in hospital management.\nMajority of current literature focuses on optimal scheduling of inpatients, but\nlargely ignores the process of accurate estimation of the trajectory of\npatients throughout the treatment and recovery process. The result is that\ncurrent scheduling models are optimizing based on inaccurate input data. We\ndeveloped a Clustering and Scheduling Integrated (CSI) approach to capture\npatient flows through a network of hospital services. CSI functions by\nclustering patients into groups based on similarity of trajectory using a novel\nSemi-Markov model (SMM)-based clustering scheme proposed in this paper, as\nopposed to clustering by admit type or condition as in previous literature. The\nmethodology is validated by simulation and then applied to real patient data\nfrom a partner hospital where we see it outperforms current methods. Further,\nwe demonstrate that extant optimization methods achieve significantly better\nresults on key hospital performance measures under CSI, compared with\ntraditional estimation approaches, increasing elective admissions by 97% and\nutilization by 22% compared to 30% and 8% using traditional estimation\ntechniques. From a theoretical standpoint, the SMM-clustering is a novel\napproach applicable to any temporal-spatial stochastic data that is prevalent\nin many industries and application areas. \n\n"}
{"id": "1506.00238", "contents": "Title: Measurement Matrix Design for Compressive Detection with Secrecy\n  Guarantees Abstract: In this letter, we consider the problem of detecting a high dimensional\nsignal based on compressed measurements with physical layer secrecy guarantees.\nWe assume that the network operates in the presence of an eavesdropper who\nintends to discover the state of the nature being monitored by the system. We\ndesign measurement matrices which maximize the detection performance of the\nnetwork while guaranteeing a certain level of secrecy. We solve the measurement\nmatrix design problem under three different scenarios: $a)$ signal is known,\n$b)$ signal lies in a low dimensional subspace, and $c)$ signal is sparse. It\nis shown that the security performance of the system can be improved by using\noptimized measurement matrices along with artificial noise injection based\ntechniques. \n\n"}
{"id": "1506.00633", "contents": "Title: Influences of quantum mechanically mixed electronic and vibrational\n  pigment states in 2D electronic spectra of photosynthetic systems: Strong\n  electronic coupling cases Abstract: In 2D electronic spectroscopy studies, long-lived quantum beats have recently\nbeen observed in photosynthetic systems, and it has been suggested that the\nbeats are produced by quantum mechanically mixed electronic and vibrational\nstates. Concerning the electronic-vibrational quantum mixtures, the impact of\nprotein-induced fluctuations was examined by calculating the 2D electronic\nspectra of a weakly coupled dimer with vibrational modes in the resonant\ncondition [J. Chem. Phys. 142, 212403 (2015)]. This analysis demonstrated that\nquantum mixtures of the vibronic resonance are rather robust under the\ninfluence of the fluctuations at cryogenic temperatures, whereas the mixtures\nare eradicated by the fluctuations at physiological temperatures. However, this\nconclusion cannot be generalized because the magnitude of the coupling inducing\nthe quantum mixtures is proportional to the inter-pigment coupling. In this\nstudy, we explore the impact of the fluctuations on electronic-vibrational\nquantum mixtures in a strongly coupled dimer. with an off-resonant vibrational\nmode. Toward this end, we calculate electronic energy transfer (EET) dynamics\nand 2D electronic spectra of a dimer that corresponds to the most strongly\ncoupled bacteriochlorophyll molecules in the Fenna-Matthews-Olson complex in a\nnumerically accurate manner. The quantum mixtures are found to be robust under\nthe exposure of protein-induced fluctuations at cryogenic temperatures,\nirrespective of the resonance. At 300 K, however, the quantum mixing is\ndisturbed more strongly by the fluctuations, and therefore, the beats in the 2D\nspectra become obscure even in a strongly coupled dimer with a resonant\nvibrational mode. Further, the overall behaviors of the EET dynamics are\ndemonstrated to be dominated by the environment and coupling between the 0-0\nvibronic transitions as long as the Huang-Rhys factor of the vibrational mode\nis small. \n\n"}
{"id": "1506.00728", "contents": "Title: Network assisted analysis to reveal the genetic basis of autism Abstract: While studies show that autism is highly heritable, the nature of the genetic\nbasis of this disorder remains illusive. Based on the idea that highly\ncorrelated genes are functionally interrelated and more likely to affect risk,\nwe develop a novel statistical tool to find more potentially autism risk genes\nby combining the genetic association scores with gene co-expression in specific\nbrain regions and periods of development. The gene dependence network is\nestimated using a novel partial neighborhood selection (PNS) algorithm, where\nnode specific properties are incorporated into network estimation for improved\nstatistical and computational efficiency. Then we adopt a hidden Markov random\nfield (HMRF) model to combine the estimated network and the genetic association\nscores in a systematic manner. The proposed modeling framework can be naturally\nextended to incorporate additional structural information concerning the\ndependence between genes. Using currently available genetic association data\nfrom whole exome sequencing studies and brain gene expression levels, the\nproposed algorithm successfully identified 333 genes that plausibly affect\nautism risk. \n\n"}
{"id": "1506.02539", "contents": "Title: Coarse-grained modelling of supercoiled RNA Abstract: We study the behaviour of double-stranded RNA under twist and tension using\noxRNA, a recently developed coarse-grained model of RNA. Introducing explicit\nsalt-dependence into the model allows us to directly compare our results to\ndata from recent single-molecule experiments. The model reproduces extension\ncurves as a function of twist and stretching force, including the buckling\ntransition and the behaviour of plectoneme structures. For negative\nsupercoiling, we predict denaturation bubble formation in plectoneme end-loops,\nsuggesting preferential plectoneme localisation in weak base sequences. OxRNA\nexhibits a positive twist-stretch coupling constant, in agreement with recent\nexperimental observations. \n\n"}
{"id": "1506.03920", "contents": "Title: A vine copula mixed effect model for trivariate meta-analysis of\n  diagnostic test accuracy studies accounting for disease prevalence Abstract: A bivariate copula mixed model has been recently proposed to synthesize\ndiagnostic test accuracy studies and it has been shown that is superior to the\nstandard generalized linear mixed model (GLMM) in this context. Here we call\ntrivariate vine copulas to extend the bivariate meta-analysis of diagnostic\ntest accuracy studies by accounting for disease prevalence. Our vine copula\nmixed model includes the trivariate GLMM as a special case and can also operate\non the original scale of sensitivity, specificity, and disease prevalence. Our\ngeneral methodology is illustrated by re-analysing the data of two published\nmeta-analyses. Our study suggests that there can be an improvement on\ntrivariate GLMM in fit to data and makes the argument for moving to vine copula\nrandom effects models especially because of their richness including reflection\nasymmetric tail dependence, and, computational feasibility despite their\nthree-dimensionality. \n\n"}
{"id": "1506.04125", "contents": "Title: A risk management approach to capital allocation Abstract: The European insurance sector will soon be faced with the application of\nSolvency 2 regulation norms. It will create a real change in risk management\npractices. The ORSA approach of the second pillar makes the capital allocation\nan important exercise for all insurers and specially for groups. Considering\nmulti-branches firms, capital allocation has to be based on a multivariate risk\nmodeling. Several allocation methods are present in the literature and insurers\npractices. In this paper, we present a new risk allocation method, we study its\ncoherence using an axiomatic approach, and we try to define what the best\nallocation choice for an insurance group is. \n\n"}
{"id": "1506.04928", "contents": "Title: Network inference and community detection, based on covariance matrices,\n  correlations and test statistics from arbitrary distributions Abstract: In this paper we propose methodology for inference of binary-valued adjacency\nmatrices from various measures of the strength of association between pairs of\nnetwork nodes, or more generally pairs of variables. This strength of\nassociation can be quantified by sample covariance and correlation matrices,\nand more generally by test-statistics and hypothesis test p-values from\narbitrary distributions. Community detection methods such as block modelling\ntypically require binary-valued adjacency matrices as a starting point. Hence,\na main motivation for the methodology we propose is to obtain binary-valued\nadjacency matrices from such pairwise measures of strength of association\nbetween variables. The proposed methodology is applicable to large\nhigh-dimensional data-sets and is based on computationally efficient\nalgorithms. We illustrate its utility in a range of contexts and data-sets. \n\n"}
{"id": "1506.05215", "contents": "Title: Robust Estimation of Structured Covariance Matrix for Heavy-Tailed\n  Elliptical Distributions Abstract: This paper considers the problem of robustly estimating a structured\ncovariance matrix with an elliptical underlying distribution with known mean.\nIn applications where the covariance matrix naturally possesses a certain\nstructure, taking the prior structure information into account in the\nestimation procedure is beneficial to improve the estimation accuracy. We\npropose incorporating the prior structure information into Tyler's M-estimator\nand formulate the problem as minimizing the cost function of Tyler's estimator\nunder the prior structural constraint. First, the estimation under a general\nconvex structural constraint is introduced with an efficient algorithm for\nfinding the estimator derived based on the majorization minimization (MM)\nalgorithm framework. Then, the algorithm is tailored to several special\nstructures that enjoy a wide range of applications in signal processing related\nfields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitz\nstructure. In addition, two types of non-convex structures, i.e., the Kronecker\nstructure and the spiked covariance structure, are also discussed, where it is\nshown that simple algorithms can be derived under the guidelines of MM.\nNumerical results show that the proposed estimator achieves a smaller\nestimation error than the benchmark estimators at a lower computational cost. \n\n"}
{"id": "1506.05830", "contents": "Title: Asymptotic Theory for M-Estimates in Unstable AR(p) Processes with\n  Infinite Variance Innovations Abstract: In this paper, we present the asymptotic distribution of M-estimators for\nparameters in non-stationary AR(p) processes. The innovations are assumed to be\nin the domain of attraction of a stable law with index $0<\\alpha\\le2$. In\nparticular, when the model involves repeated unit roots or conjugate complex\nunit roots, M-estimators have a higher asymptotic rate of convergence compared\nto the least square estimators and the asymptotic results can be written as\nIt\\^{o} stochastic integrals. \n\n"}
{"id": "1506.06169", "contents": "Title: A Model-Based Approach for Analog Spatio-Temporal Dynamic Forecasting Abstract: Analog forecasting has been applied in a variety of fields for predicting\nfuture states of complex nonlinear systems that require flexible forecasting\nmethods. Past analog methods have almost exclu- sively been used in an\nempirical framework without the structure of a model-based approach. We propose\na Bayesian model framework for analog forecasting, building upon previous\nanalog methods but accounting for parameter uncertainty. Thus, unlike\ntraditional analog forecasting methods, the use of Bayesian modeling allows one\nto rigorously quantify uncertainty to obtain realistic posterior predictive\ndistributions. The model is applied to the long-lead time forecasting of\nmid-May averaged soil moisture anomalies in Iowa over a high-resolution grid of\nspatial locations. Sea Surface Tem- perature (SST) is used to find past time\nperiods with similar trajectories to the current pre-forecast period. The\nanalog model is developed on projection coefficients from a basis expansion of\nthe soil moisture and SST fields. Separate models are constructed for locations\nfalling in each Iowa Crop Reporting District (CRD) and the forecasting ability\nof the proposed model is compared against a variety of alternative methods and\nmetrics. \n\n"}
{"id": "1506.06259", "contents": "Title: Kinetic distance and kinetic maps from molecular dynamics simulation Abstract: Characterizing macromolecular kinetics from molecular dynamics (MD)\nsimulations requires a distance metric that can distinguish\nslowly-interconverting states. Here we build upon diffusion map theory and\ndefine a kinetic distance for irreducible Markov processes that quantifies how\nslowly molecular conformations interconvert. The kinetic distance can be\ncomputed given a model that approximates the eigenvalues and eigenvectors\n(reaction coordinates) of the MD Markov operator. Here we employ the\ntime-lagged independent component analysis (TICA). The TICA components can be\nscaled to provide a kinetic map in which the Euclidean distance corresponds to\nthe kinetic distance. As a result, the question of how many TICA dimensions\nshould be kept in a dimensionality reduction approach becomes obsolete, and one\nparameter less needs to be specified in the kinetic model construction. We\ndemonstrate the approach using TICA and Markov state model (MSM) analyses for\nillustrative models, protein conformation dynamics in bovine pancreatic trypsin\ninhibitor and protein-inhibitor association in trypsin and benzamidine. \n\n"}
{"id": "1506.08151", "contents": "Title: Phase-dependent exciton transport and energy harvesting from thermal\n  environments Abstract: Non-Markovian effects in the evolution of open quantum systems have recently\nattracted widespread interest, particularly in the context of assessing the\nefficiency of energy and charge transfer in nanoscale biomolecular networks and\nquantum technologies. With the aid of many-body simulation methods, we uncover\nand analyse an ultrafast environmental process that causes energy relaxation in\nthe reduced system to depend explicitly on the phase relation of the initial\nstate preparation. Remarkably, for particular phases and system parameters, the\nnet energy flow is uphill, transiently violating the principle of detailed\nbalance, and implying that energy is spontaneously taken up from the\nenvironment. A theoretical analysis reveals that non-secular contributions,\nsignificant only within the environmental correlation time, underlie this\neffect. This suggests that environmental energy harvesting will be observable\nacross a wide range of coupled quantum systems. \n\n"}
{"id": "1506.08180", "contents": "Title: An Empirical Study of Stochastic Variational Algorithms for the Beta\n  Bernoulli Process Abstract: Stochastic variational inference (SVI) is emerging as the most promising\ncandidate for scaling inference in Bayesian probabilistic models to large\ndatasets. However, the performance of these methods has been assessed primarily\nin the context of Bayesian topic models, particularly latent Dirichlet\nallocation (LDA). Deriving several new algorithms, and using synthetic, image\nand genomic datasets, we investigate whether the understanding gleaned from LDA\napplies in the setting of sparse latent factor models, specifically beta\nprocess factor analysis (BPFA). We demonstrate that the big picture is\nconsistent: using Gibbs sampling within SVI to maintain certain posterior\ndependencies is extremely effective. However, we find that different posterior\ndependencies are important in BPFA relative to LDA. Particularly,\napproximations able to model intra-local variable dependence perform best. \n\n"}
{"id": "1507.00171", "contents": "Title: The Statistical Performance of Collaborative Inference Abstract: The statistical analysis of massive and complex data sets will require the\ndevelopment of algorithms that depend on distributed computing and\ncollaborative inference. Inspired by this, we propose a collaborative framework\nthat aims to estimate the unknown mean $\\theta$ of a random variable $X$. In\nthe model we present, a certain number of calculation units, distributed across\na communication network represented by a graph, participate in the estimation\nof $\\theta$ by sequentially receiving independent data from $X$ while\nexchanging messages via a stochastic matrix $A$ defined over the graph. We give\nprecise conditions on the matrix $A$ under which the statistical precision of\nthe individual units is comparable to that of a (gold standard) virtual\ncentralized estimate, even though each unit does not have access to all of the\ndata. We show in particular the fundamental role played by both the non-trivial\neigenvalues of $A$ and the Ramanujan class of expander graphs, which provide\nremarkable performance for moderate algorithmic cost. \n\n"}
{"id": "1507.00280", "contents": "Title: Network Lasso: Clustering and Optimization in Large Graphs Abstract: Convex optimization is an essential tool for modern data analysis, as it\nprovides a framework to formulate and solve many problems in machine learning\nand data mining. However, general convex optimization solvers do not scale\nwell, and scalable solvers are often specialized to only work on a narrow class\nof problems. Therefore, there is a need for simple, scalable algorithms that\ncan solve many common optimization problems. In this paper, we introduce the\n\\emph{network lasso}, a generalization of the group lasso to a network setting\nthat allows for simultaneous clustering and optimization on graphs. We develop\nan algorithm based on the Alternating Direction Method of Multipliers (ADMM) to\nsolve this problem in a distributed and scalable manner, which allows for\nguaranteed global convergence even on large graphs. We also examine a\nnon-convex extension of this approach. We then demonstrate that many types of\nproblems can be expressed in our framework. We focus on three in particular -\nbinary classification, predicting housing prices, and event detection in time\nseries data - comparing the network lasso to baseline approaches and showing\nthat it is both a fast and accurate method of solving large optimization\nproblems. \n\n"}
{"id": "1507.01397", "contents": "Title: Adaptive kernel estimation of the baseline function in the Cox model,\n  with high-dimensional covariates Abstract: The aim of this article is to propose a novel kernel estimator of the\nbaseline function in a general high-dimensional Cox model, for which we derive\nnon-asymptotic rates of convergence. To construct our estimator, we first\nestimate the regression parameter in the Cox model via a Lasso procedure. We\nthen plug this estimator into the classical kernel estimator of the baseline\nfunction, obtained by smoothing the so-called Breslow estimator of the\ncumulative baseline function. We propose and study an adaptive procedure for\nselecting the bandwidth, in the spirit of Gold-enshluger and Lepski (2011). We\nstate non-asymptotic oracle inequalities for the final estimator, which reveal\nthe reduction of the rates of convergence when the dimension of the covariates\ngrows. \n\n"}
{"id": "1507.01454", "contents": "Title: Principal Component Analysis of Persistent Homology Rank Functions with\n  case studies of Spatial Point Patterns, Sphere Packing and Colloids Abstract: Persistent homology, while ostensibly measuring changes in topology, captures\nmultiscale geometrical information. It is a natural tool for the analysis of\npoint patterns. In this paper we explore the statistical power of the\n(persistent homology) rank functions. For a point pattern $X$ we construct a\nfiltration of spaces by taking the union of balls of radius $a$ centered on\npoints in $X$, $X_a = \\cup_{x\\in X}B(x,a)$. The rank function\n${\\beta}_k(X):{\\{(a,b)\\in \\mathbb{R}^2: a\\leq b\\}} \\to \\mathbb{R}$ is then\ndefined by ${\\beta}_k(X)(a,b) = rank ( \\iota_*:H_k(X_a) \\to H_k(X_b))$ where\n$\\iota_*$ is the induced map on homology from the inclusion map on spaces. We\nconsider the rank functions as lying in a Hilbert space and show that under\nreasonable conditions the rank functions from multiple simulations or\nexperiments will lie in an affine subspace. This enables us to perform\nfunctional principal component analysis which we apply to experimental data\nfrom colloids at different effective temperatures and of sphere packings with\ndifferent volume fractions. We also investigate the potential of rank functions\nin providing a test of complete spatial randomness of 2D point patterns using\nthe distances to an empirically computed mean rank function of binomial point\npatterns in the unit square. \n\n"}
{"id": "1507.01537", "contents": "Title: Identifying Functional Thermodynamics in Autonomous Maxwellian Ratchets Abstract: We introduce a family of Maxwellian Demons for which correlations among\ninformation bearing degrees of freedom can be calculated exactly and in compact\nanalytical form. This allows one to precisely determine Demon functional\nthermodynamic operating regimes, when previous methods either misclassify or\nsimply fail due to approximations they invoke. This reveals that these Demons\nare more functional than previous candidates. They too behave either as\nengines, lifting a mass against gravity by extracting energy from a single heat\nreservoir, or as Landauer erasers, consuming external work to remove\ninformation from a sequence of binary symbols by decreasing their individual\nuncertainty. Going beyond these, our Demon exhibits a new functionality that\nerases bits not by simply decreasing individual-symbol uncertainty, but by\nincreasing inter-bit correlations (that is, by adding temporal order) while\nincreasing single-symbol uncertainty. In all cases, but especially in the new\nerasure regime, exactly accounting for informational correlations leads to\ntight bounds on Demon performance, expressed as a refined Second Law of\nThermodynamics that relies on the Kolmogorov-Sinai entropy for dynamical\nprocesses and not on changes purely in system configurational entropy, as\npreviously employed. We rigorously derive the refined Second Law under minimal\nassumptions and so it applies quite broadly---for Demons with and without\nmemory and input sequences that are correlated or not. We note that general\nMaxwellian Demons readily violate previously proposed, alternative such bounds,\nwhile the current bound still holds. \n\n"}
{"id": "1507.01933", "contents": "Title: On Joint Estimation of Gaussian Graphical Models for Spatial and\n  Temporal Data Abstract: In this paper, we first propose a Bayesian neighborhood selection method to\nestimate Gaussian Graphical Models (GGMs). We show the graph selection\nconsistency of this method in the sense that the posterior probability of the\ntrue model converges to one. When there are multiple groups of data available,\ninstead of estimating the networks independently for each group, joint\nestimation of the networks may utilize the shared information among groups and\nlead to improved estimation for each individual network. Our method is extended\nto jointly estimate GGMs in multiple groups of data with complex structures,\nincluding spatial data, temporal data and data with both spatial and temporal\nstructures. Markov random field (MRF) models are used to efficiently\nincorporate the complex data structures. We develop and implement an efficient\nalgorithm for statistical inference that enables parallel computing. Simulation\nstudies suggest that our approach achieves better accuracy in network\nestimation compared with methods not incorporating spatial and temporal\ndependencies when there are shared structures among the networks, and that it\nperforms comparably well otherwise. Finally, we illustrate our method using the\nhuman brain gene expression microarray dataset, where the expression levels of\ngenes are measured in different brain regions across multiple time periods. \n\n"}
{"id": "1507.02216", "contents": "Title: Robust Sparse Blind Source Separation Abstract: Blind Source Separation is a widely used technique to analyze multichannel\ndata. In many real-world applications, its results can be significantly\nhampered by the presence of unknown outliers. In this paper, a novel algorithm\ncoined rGMCA (robust Generalized Morphological Component Analysis) is\nintroduced to retrieve sparse sources in the presence of outliers. It\nexplicitly estimates the sources, the mixing matrix, and the outliers. It also\ntakes advantage of the estimation of the outliers to further implement a\nweighting scheme, which provides a highly robust separation procedure.\nNumerical experiments demonstrate the efficiency of rGMCA to estimate the\nmixing matrix in comparison with standard BSS techniques. \n\n"}
{"id": "1507.02822", "contents": "Title: Hawkes Processes Abstract: Hawkes processes are a particularly interesting class of stochastic process\nthat have been applied in diverse areas, from earthquake modelling to financial\nanalysis. They are point processes whose defining characteristic is that they\n'self-excite', meaning that each arrival increases the rate of future arrivals\nfor some period of time. Hawkes processes are well established, particularly\nwithin the financial literature, yet many of the treatments are inaccessible to\none not acquainted with the topic. This survey provides background, introduces\nthe field and historical developments, and touches upon all major aspects of\nHawkes processes. \n\n"}
{"id": "1507.02954", "contents": "Title: An Iterative Receiver for OFDM With Sparsity-Based Parametric Channel\n  Estimation Abstract: In this work we design a receiver that iteratively passes soft information\nbetween the channel estimation and data decoding stages. The receiver\nincorporates sparsity-based parametric channel estimation. State-of-the-art\nsparsity-based iterative receivers simplify the channel estimation problem by\nrestricting the multipath delays to a grid. Our receiver does not impose such a\nrestriction. As a result it does not suffer from the leakage effect, which\ndestroys sparsity. Communication at near capacity rates in high SNR requires a\nlarge modulation order. Due to the close proximity of modulation symbols in\nsuch systems, the grid-based approximation is of insufficient accuracy. We show\nnumerically that a state-of-the-art iterative receiver with grid-based sparse\nchannel estimation exhibits a bit-error-rate floor in the high SNR regime. On\nthe contrary, our receiver performs very close to the perfect channel state\ninformation bound for all SNR values. We also demonstrate both theoretically\nand numerically that parametric channel estimation works well in dense\nchannels, i.e., when the number of multipath components is large and each\nindividual component cannot be resolved. \n\n"}
{"id": "1507.04199", "contents": "Title: Evaluating the Causal Effect of University Grants on Student Dropout:\n  Evidence from a Regression Discontinuity Design Using Principal\n  Stratification Abstract: Regression discontinuity (RD) designs are often interpreted as local\nrandomized experiments: a RD design can be considered as a randomized\nexperiment for units with a realized value of a so-called forcing variable\nfalling around a pre-fixed threshold. Motivated by the evaluation of Italian\nuniversity grants, we consider a fuzzy RD design where the receipt of the\ntreatment is based on both eligibility criteria and a voluntary application\nstatus. Resting on the fact that grant application and grant receipt statuses\nare post-assignment (post-eligibility) intermediate variables, we use the\nprincipal stratification framework to define causal estimands within the Rubin\nCausal Model. We propose a probabilistic formulation of the assignment\nmechanism underlying RD designs, by re-formulating the Stable Unit Treatment\nValue Assumption (SUTVA) and making an explicit local overlap assumption for a\nsubpopulation around the threshold. A local randomization assumption is invoked\ninstead of more standard continuity assumptions. We also develop a model-based\nBayesian approach to select the target subpopulation(s) with adjustment for\nmultiple comparisons, and to draw inference for the target causal estimands in\nthis framework. Applying the method to the data from two Italian universities,\nwe find evidence that university grants are effective in preventing students\nfrom low-income families from dropping out of higher education. \n\n"}
{"id": "1507.04658", "contents": "Title: Tractable Resource Management in Millimeter-Wave Overlaid Ultra-Dense\n  Cellular Networks Abstract: What does millimeter-wave (mmW) seek assistance for from micro-wave ({\\mu}W)\nin a mmW overlaid 5G cellular network? This paper raises the question of\nwhether to complement downlink (DL) or uplink (UL) transmissions, and concludes\nthat {\\mu}W should aid UL more. Such dedication to UL results from the low mmW\nUL rate due to high peak-to-average power ratio (PAPR) at mobile users. The\nDL/UL allocations are tractably provided based on a novel closed-form mm-{\\mu}W\nspectral efficiency (SE) derivation via stochastic geometry. The findings\nexplicitly indicate: (i) both DL/UL mmW (or {\\mu}W) SEs coincidentally converge\non the same value in an ultra-dense cellular network (UDN) and (ii) such a mmW\n(or {\\mu}W) UDN SE is a logarithmic function of BS-to-user density ratio. The\ncorresponding mm-{\\mu}W resource management is evaluated by utilizing a three\ndimensional (3D) blockage model with real geography in Seoul, Korea. \n\n"}
{"id": "1507.05664", "contents": "Title: Distributed Learning Algorithms for Spectrum Sharing in Spatial Random\n  Access Wireless Networks Abstract: We consider distributed optimization over orthogonal collision channels in\nspatial random access networks. Users are spatially distributed and each user\nis in the interference range of a few other users. Each user is allowed to\ntransmit over a subset of the shared channels with a certain attempt\nprobability. We study both the non-cooperative and cooperative settings. In the\nformer, the goal of each user is to maximize its own rate irrespective of the\nutilities of other users. In the latter, the goal is to achieve proportionally\nfair rates among users. Simple distributed learning algorithms are developed to\nsolve these problems. The efficiencies of the proposed algorithms are\ndemonstrated via both theoretical analysis and simulation results. \n\n"}
{"id": "1507.06807", "contents": "Title: Bayesian inference for diffusion driven mixed-effects models Abstract: Stochastic differential equations (SDEs) provide a natural framework for\nmodelling intrinsic stochasticity inherent in many continuous-time physical\nprocesses. When such processes are observed in multiple individuals or\nexperimental units, SDE driven mixed-effects models allow the quantification of\nbetween (as well as within) individual variation. Performing Bayesian inference\nfor such models, using discrete time data that may be incomplete and subject to\nmeasurement error is a challenging problem and is the focus of this paper. We\nextend a recently proposed MCMC scheme to include the SDE driven mixed-effects\nframework. Fundamental to our approach is the development of a novel construct\nthat allows for efficient sampling of conditioned SDEs that may exhibit\nnonlinear dynamics between observation times. We apply the resulting scheme to\nsynthetic data generated from a simple SDE model of orange tree growth, and\nreal data consisting of observations on aphid numbers recorded under a variety\nof different treatment regimes. In addition, we provide a systematic comparison\nof our approach with an inference scheme based on a tractable approximation of\nthe SDE, that is, the linear noise approximation. \n\n"}
{"id": "1507.07024", "contents": "Title: A multiscale strategy for Bayesian inference using transport maps Abstract: In many inverse problems, model parameters cannot be precisely determined\nfrom observational data. Bayesian inference provides a mechanism for capturing\nthe resulting parameter uncertainty, but typically at a high computational\ncost. This work introduces a multiscale decomposition that exploits conditional\nindependence across scales, when present in certain classes of inverse\nproblems, to decouple Bayesian inference into two stages: (1) a computationally\ntractable coarse-scale inference problem; and (2) a mapping of the\nlow-dimensional coarse-scale posterior distribution into the original\nhigh-dimensional parameter space. This decomposition relies on a\ncharacterization of the non-Gaussian joint distribution of coarse- and\nfine-scale quantities via optimal transport maps. We demonstrate our approach\non a sequence of inverse problems arising in subsurface flow, using the\nmultiscale finite element method to discretize the steady state pressure\nequation. We compare the multiscale strategy with full-dimensional Markov chain\nMonte Carlo on a problem of moderate dimension (100 parameters) and then use it\nto infer a conductivity field described by over 10,000 parameters. \n\n"}
{"id": "1507.07536", "contents": "Title: Online Censoring for Large-Scale Regressions with Application to\n  Streaming Big Data Abstract: Linear regression is arguably the most prominent among statistical inference\nmethods, popular both for its simplicity as well as its broad applicability. On\npar with data-intensive applications, the sheer size of linear regression\nproblems creates an ever growing demand for quick and cost efficient solvers.\nFortunately, a significant percentage of the data accrued can be omitted while\nmaintaining a certain quality of statistical inference with an affordable\ncomputational budget. The present paper introduces means of identifying and\nomitting \"less informative\" observations in an online and data-adaptive\nfashion, built on principles of stochastic approximation and data censoring.\nFirst- and second-order stochastic approximation maximum likelihood-based\nalgorithms for censored observations are developed for estimating the\nregression coefficients. Online algorithms are also put forth to reduce the\noverall complexity by adaptively performing censoring along with estimation.\nThe novel algorithms entail simple closed-form updates, and have provable\n(non)asymptotic convergence guarantees. Furthermore, specific rules are\ninvestigated for tuning to desired censoring patterns and levels of\ndimensionality reduction. Simulated tests on real and synthetic datasets\ncorroborate the efficacy of the proposed data-adaptive methods compared to\ndata-agnostic random projection-based alternatives. \n\n"}
{"id": "1507.07587", "contents": "Title: The statistical analysis of acoustic phonetic data: exploring\n  differences between spoken Romance languages Abstract: The historical and geographical spread from older to more modern languages\nhas long been studied by examining textual changes and in terms of changes in\nphonetic transcriptions. However, it is more difficult to analyze language\nchange from an acoustic point of view, although this is usually the dominant\nmode of transmission. We propose a novel analysis approach for acoustic\nphonetic data, where the aim will be to statistically model the acoustic\nproperties of spoken words. We explore phonetic variation and change using a\ntime-frequency representation, namely the log-spectrograms of speech\nrecordings. We identify time and frequency covariance functions as a feature of\nthe language; in contrast, mean spectrograms depend mostly on the particular\nword that has been uttered. We build models for the mean and covariances\n(taking into account the restrictions placed on the statistical analysis of\nsuch objects) and use these to define a phonetic transformation that models how\nan individual speaker would sound in a different language, allowing the\nexploration of phonetic differences between languages. Finally, we map back\nthese transformations to the domain of sound recordings, allowing us to listen\nto the output of the statistical analysis. The proposed approach is\ndemonstrated using recordings of the words corresponding to the numbers from\n\"one\" to \"ten\" as pronounced by speakers from five different Romance languages. \n\n"}
{"id": "1507.08010", "contents": "Title: Licensed and Unlicensed Spectrum Allocation in Heterogeneous Networks Abstract: In future networks, an operator may employ a wide range of access points\nusing diverse radio access technologies (RATs) over multiple licensed and\nunlicensed frequency bands. This paper studies centralized user association and\nspectrum allocation across many access points in such a heterogeneous network\n(HetNet). Such centralized control is on a relatively slow timescale to allow\ninformation exchange and joint optimization over multiple cells. This is in\ncontrast and complementary to fast timescale distributed scheduling. A queueing\nmodel is introduced to capture the lower spectral efficiency, reliability, and\nadditional delays of data transmission over the unlicensed bands due to\ncontention and/or listen-before-talk requirements. Two optimization-based\nspectrum allocation schemes are proposed along with efficient algorithms for\ncomputing the allocations. The proposed solutions are fully aware of traffic\nloads, network topology, as well as external interference levels in the\nunlicensed bands. Packet-level simulation results show that the proposed\nschemes significantly outperform orthogonal and full-frequency-reuse\nallocations under all traffic conditions. \n\n"}
{"id": "1508.00129", "contents": "Title: A comparative review of variable selection techniques for covariate\n  dependent Dirichlet process mixture models Abstract: Dirichlet Process Mixture (DPM) models have been increasingly employed to\nspecify random partition models that take into account possible patterns within\nthe covariates. Furthermore, to deal with large numbers of covariates, methods\nfor selecting the most important covariates have been proposed. Commonly, the\ncovariates are chosen either for their importance in determining the clustering\nof the observations or for their effect on the level of a response variable\n(when a regression model is specified). Typically both strategies involve the\nspecification of latent indicators that regulate the inclusion of the\ncovariates in the model. Common examples involve the use of spike and slab\nprior distributions. In this work we review the most relevant DPM models that\ninclude covariate information in the induced partition of the observations and\nwe focus on available variable selection techniques for these models. We\nhighlight the main features of each model and demonstrate them in simulations\nand in a real data application. \n\n"}
{"id": "1508.01551", "contents": "Title: A Knowledge Gradient Policy for Sequencing Experiments to Identify the\n  Structure of RNA Molecules Using a Sparse Additive Belief Model Abstract: We present a sparse knowledge gradient (SpKG) algorithm for adaptively\nselecting the targeted regions within a large RNA molecule to identify which\nregions are most amenable to interactions with other molecules. Experimentally,\nsuch regions can be inferred from fluorescence measurements obtained by binding\na complementary probe with fluorescence markers to the targeted regions. We use\na biophysical model which shows that the fluorescence ratio under the log scale\nhas a sparse linear relationship with the coefficients describing the\naccessibility of each nucleotide, since not all sites are accessible (due to\nthe folding of the molecule). The SpKG algorithm uniquely combines the Bayesian\nranking and selection problem with the frequentist $\\ell_1$ regularized\nregression approach Lasso. We use this algorithm to identify the sparsity\npattern of the linear model as well as sequentially decide the best regions to\ntest before experimental budget is exhausted. Besides, we also develop two\nother new algorithms: batch SpKG algorithm, which generates more suggestions\nsequentially to run parallel experiments; and batch SpKG with a procedure which\nwe call length mutagenesis. It dynamically adds in new alternatives, in the\nform of types of probes, are created by inserting, deleting or mutating\nnucleotides within existing probes. In simulation, we demonstrate these\nalgorithms on the Group I intron (a mid-size RNA molecule), showing that they\nefficiently learn the correct sparsity pattern, identify the most accessible\nregion, and outperform several other policies. \n\n"}
{"id": "1508.02808", "contents": "Title: Microscopic Analysis of the Uplink Interference in FDMA Small Cell\n  Networks Abstract: In this paper, we analytically derive an upper bound on the error in\napproximating the uplink (UL) single-cell interference by a lognormal\ndistribution in frequency division multiple access (FDMA) small cell networks\n(SCNs). Such an upper bound is measured by the Kolmogorov Smirnov (KS) distance\nbetween the actual cumulative density function (CDF) and the approximate CDF.\nThe lognormal approximation is important because it allows tractable network\nperformance analysis. Our results are more general than the existing works in\nthe sense that we do not pose any requirement on (i) the shape and/or size of\ncell coverage areas, (ii) the uniformity of user equipment (UE) distribution,\nand (iii) the type of multi-path fading. Based on our results, we propose a new\nframework to directly and analytically investigate a complex network with\npractical deployment of multiple BSs placed at irregular locations, using a\npower lognormal approximation of the aggregate UL interference. The proposed\nnetwork performance analysis is particularly useful for the 5th generation (5G)\nsystems with general cell deployment and UE distribution. \n\n"}
{"id": "1508.03498", "contents": "Title: Lensless Compressive Imaging Abstract: We develop a lensless compressive imaging architecture, which consists of an\naperture assembly and a single sensor, without using any lens. An anytime\nalgorithm is proposed to reconstruct images from the compressive measurements;\nthe algorithm produces a sequence of solutions that monotonically converge to\nthe true signal (thus, anytime). The algorithm is developed based on the\nsparsity of local overlapping patches (in the transformation domain) and\nstate-of-the-art results have been obtained. Experiments on real data\ndemonstrate that encouraging results are obtained by measuring about 10% (of\nthe image pixels) compressive measurements. The reconstruction results of the\nproposed algorithm are compared with the JPEG compression (based on file sizes)\nand the reconstructed image quality is close to the JPEG compression, in\nparticular at a high compression rate. \n\n"}
{"id": "1508.04152", "contents": "Title: Exploring the relationship between the magnitudes of seismic events Abstract: The distribution of the magnitudes of seismic events is generally assumed to\nbe independent on past seismicity. However, by considering events in causal\nrelation, for example mother-daughter, it seems natural to assume that the\nmagnitude of a daughter event is conditionally dependent on the one of the\ncorresponding mother event. In order to find experimental evidence supporting\nthis hypothesis, we analyze different catalogs, both real and simulated, in two\ndifferent ways. From each catalog, we obtain the law of triggered events'\nmagnitude by kernel density. The results obtained show that the distribution\ndensity of triggered events' magnitude varies with the magnitude of their\ncorresponding mother events. As the intuition suggests, an increase of mother\nevents' magnitude induces an increase of the probability of having \"high\"\nvalues of triggered events' magnitude. In addition, we see a statistically\nsignificant increasing linear dependence of the magnitude means. \n\n"}
{"id": "1508.04217", "contents": "Title: Macroeconomic Forecasting and Variable Selection with a Very Large\n  Number of Predictors: A Penalized Regression Approach Abstract: This paper studies macroeconomic forecasting and variable selection using a\nfolded-concave penalized regression with a very large number of predictors. The\npenalized regression approach leads to sparse estimates of the regression\ncoefficients, and is applicable even if the dimensionality of the model is much\nlarger than the sample size. The first half of the paper discusses the\ntheoretical aspects of a folded-concave penalized regression when the model\nexhibits time series dependence. Specifically, we show the oracle inequality\nand the oracle property for ultrahigh-dimensional time-dependent regressors.\nThe latter half of the paper shows the validity of the penalized regression\nusing two motivating empirical applications. The first forecasts U.S. GDP with\nthe FRED-MD data using the MIDAS regression framework, where there are more\nthan 1000 covariates, while the sample size is at most 200. The second examines\nhow well the penalized regression screens the hidden portfolio with around 40\nstocks from more than 1800 potential stocks using NYSE stock price data. Both\napplications reveal that the penalized regression provides remarkable results\nin terms of forecasting performance and variable selection. \n\n"}
{"id": "1508.04688", "contents": "Title: Exploring chance in NCAA basketball Abstract: There seems to be an upper limit to predicting the outcome of matches in\n(semi-)professional sports. Recent work has proposed that this is due to chance\nand attempts have been made to simulate the distribution of win percentages to\nidentify the most likely proportion of matches decided by chance. We argue that\nthe approach that has been chosen so far makes some simplifying assumptions\nthat cause its result to be of limited practical value. Instead, we propose to\nuse clustering of statistical team profiles and observed scheduling information\nto derive limits on the predictive accuracy for particular seasons, which can\nbe used to assess the performance of predictive models on those seasons. We\nshow that the resulting simulated distributions are much closer to the observed\ndistributions and give higher assessments of chance and tighter limits on\npredictive accuracy. \n\n"}
{"id": "1508.05047", "contents": "Title: Rare Event Simulation Abstract: Rare events are events that are expected to occur infrequently, or more\ntechnically, those that have low probabilities (say, order of $10^{-3}$ or\nless) of occurring according to a probability model. In the context of\nuncertainty quantification, the rare events often correspond to failure of\nsystems designed for high reliability, meaning that the system performance\nfails to meet some design or operation specifications. As reviewed in this\nsection, computation of such rare-event probabilities is challenging.\nAnalytical solutions are usually not available for non-trivial problems and\nstandard Monte Carlo simulation is computationally inefficient. Therefore, much\nresearch effort has focused on developing advanced stochastic simulation\nmethods that are more efficient. In this section, we address the problem of\nestimating rare-event probabilities by Monte Carlo simulation, Importance\nSampling and Subset Simulation for highly reliable dynamic systems. \n\n"}
{"id": "1508.06686", "contents": "Title: Analysis of multiview legislative networks with structured matrix\n  factorization: Does Twitter influence translate to the real world? Abstract: The rise of social media platforms has fundamentally altered the public\ndiscourse by providing easy to use and ubiquitous forums for the exchange of\nideas and opinions. Elected officials often use such platforms for\ncommunication with the broader public to disseminate information and engage\nwith their constituencies and other public officials. In this work, we\ninvestigate whether Twitter conversations between legislators reveal their\nreal-world position and influence by analyzing multiple Twitter networks that\nfeature different types of link relations between the Members of Parliament\n(MPs) in the United Kingdom and an identical data set for politicians within\nIreland. We develop and apply a matrix factorization technique that allows the\nanalyst to emphasize nodes with contextual local network structures by\nspecifying network statistics that guide the factorization solution. Leveraging\nonly link relation data, we find that important politicians in Twitter networks\nare associated with real-world leadership positions, and that rankings from the\nproposed method are correlated with the number of future media headlines. \n\n"}
{"id": "1508.06901", "contents": "Title: Compressive Sensing via Low-Rank Gaussian Mixture Models Abstract: We develop a new compressive sensing (CS) inversion algorithm by utilizing\nthe Gaussian mixture model (GMM). While the compressive sensing is performed\nglobally on the entire image as implemented in our lensless camera, a low-rank\nGMM is imposed on the local image patches. This low-rank GMM is derived via\neigenvalue thresholding of the GMM trained on the projection of the measurement\ndata, thus learned {\\em in situ}. The GMM and the projection of the measurement\ndata are updated iteratively during the reconstruction. Our GMM algorithm\ndegrades to the piecewise linear estimator (PLE) if each patch is represented\nby a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also\ndeveloped for CS inversion, constituting an additional contribution of this\npaper. Extensive results on both simulation data and real data captured by the\nlensless camera demonstrate the efficacy of the proposed algorithm.\nFurthermore, we compare the CS reconstruction results using our algorithm with\nthe JPEG compression. Simulation results demonstrate that when limited\nbandwidth is available (a small number of measurements), our algorithm can\nachieve comparable results as JPEG. \n\n"}
{"id": "1508.07083", "contents": "Title: Detecting Abrupt Changes in the Spectra of High-Energy Astrophysical\n  Sources Abstract: Variable-intensity astronomical sources are the result of complex and often\nextreme physical processes. Abrupt changes in source intensity are typically\naccompanied by equally sudden spectral shifts, i.e., sudden changes in the\nwavelength distribution of the emission. This article develops a method for\nmodeling photon counts collected from observation of such sources. We embed\nchange points into a marked Poisson process, where photon wavelengths are\nregarded as marks and both the Poisson intensity parameter and the distribution\nof the marks are allowed to change. To the best of our knowledge this is the\nfirst effort to embed change points into a marked Poisson process. Between the\nchange points, the spectrum is modeled non-parametrically using a mixture of a\nsmooth radial basis expansion and a number of local deviations from the smooth\nterm representing spectral emission lines. Because the model is over\nparameterized we employ an $\\ell_1$ penalty. The tuning parameter in the\npenalty and the number of change points are determined via the minimum\ndescription length principle. Our method is validated via a series of\nsimulation studies and its practical utility is illustrated in the analysis of\nthe ultra-fast rotating yellow giant star known as FK Com. \n\n"}
{"id": "1509.01038", "contents": "Title: Multi-Source Cooperative Communication with Opportunistic Interference\n  Cancelling Relays Abstract: In this paper we present a multi-user cooperative protocol for wireless\nnetworks. Two sources transmit simultaneously their information blocks and\nrelays employ opportunistically successive interference cancellation (SIC) in\nan effort to decode them. An adaptive decode/amplify-and-forward scheme is\napplied at the relays to the decoded blocks or their sufficient statistic if\ndecoding fails. The main feature of the protocol is that SIC is exploited in a\nnetwork since more opportunities arise for each block to be decoded as the\nnumber of used relays NRU is increased. This feature leads to benefits in terms\nof diversity and multiplexing gains that are proven with the help of an\nanalytical outage model and a diversity-multiplexing tradeoff (DMT) analysis.\nThe performance improvements are achieved without any network synchronization\nand coordination. In the final part of this work the closed-form outage\nprobability model is used by a novel approach for offline pre-selection of the\nNRU relays, that have the best SIC performance, from a larger number of NR\nnodes. The analytical results are corroborated with extensive simulations,\nwhile the protocol is compared with orthogonal and multi-user protocols\nreported in the literature. \n\n"}
{"id": "1509.01537", "contents": "Title: Distinguishing the roles of energy funnelling and delocalization in\n  photosynthetic light harvesting Abstract: Photosynthetic complexes improve the transfer of excitation energy from\nperipheral antennas to reaction centers in several ways. In particular, a\ndownward energy funnel can direct excitons in the right direction, while\ncoherent excitonic delocalization can enhance transfer rates through the\ncooperative phenomenon of supertransfer. However, isolating the role of purely\ncoherent effects is difficult because any change to the delocalization also\nchanges the energy landscape. Here, we show that the relative importance of the\ntwo processes can be determined by comparing the natural light-harvesting\napparatus with counterfactual models in which the delocalization and the energy\nlandscape are altered. Applied to the example of purple bacteria, our approach\nshows that although supertransfer does enhance the rates somewhat, the\nenergetic funnelling plays the decisive role. Because delocalization has a\nminor role (and is sometimes detrimental), it is most likely not adaptive,\nbeing a side-effect of the dense chlorophyll packing that evolved to increase\nlight absorption per reaction center. \n\n"}
{"id": "1509.03000", "contents": "Title: Full-Duplex Transceiver for Future Cellular Network: A Smart Antenna\n  Approach Abstract: In this paper, we propose a transceiver architecture for full-duplex (FD)\neNodeB (eNB) and FD user equipment (UE) transceiver. For FD\ncommunication,.i.e., simultaneous in-band uplink and downlink operation, same\nsubcarriers can be allocated to UE in both uplink and downlink. Hence, contrary\nto traditional LTE, we propose using single-carrier frequency division multiple\naccesses (SC-FDMA) for downlink along with the conventional method of using it\nfor uplink. The use of multiple antennas at eNB and singular value\ndecomposition (SVD) in the downlink allows multiple users (MU) to operate on\nthe same set of ubcarriers. In the uplink, successive interference cancellation\nwith optimal ordering (SSIC-OO) algorithm is used to decouple signals of UEs\noperating in the same set of subcarriers. A smart antenna approach is adopted\nwhich prevents interference, in downlink of a UE, from uplink signals of other\nUEs sharing same subcarriers. The approach includes using multiple antennas at\nUEs to form directed beams towards eNode and nulls towards other UEs. The\nproposed architecture results in significant improvement of the overall\nspectrum efficiency per cell of the cellular network. \n\n"}
{"id": "1509.03521", "contents": "Title: Similarity-based semi-local estimation of EMOS models Abstract: Weather forecasts are typically given in the form of forecast ensembles\nobtained from multiple runs of numerical weather prediction models with varying\ninitial conditions and physics parameterizations. Such ensemble predictions\ntend to be biased and underdispersive and thus require statistical\npostprocessing. In the ensemble model output statistics (EMOS) approach, a\nprobabilistic forecast is given by a single parametric distribution with\nparameters depending on the ensemble members. This article proposes two\nsemi-local methods for estimating the EMOS coefficients where the training data\nfor a specific observation station are augmented with corresponding forecast\ncases from stations with similar characteristics. Similarities between stations\nare determined using either distance functions or clustering based on various\nfeatures of the climatology, forecast errors, ensemble predictions and\nlocations of the observation stations. In a case study on wind speed over\nEurope with forecasts from the Grand Limited Area Model Ensemble Prediction\nSystem, the proposed similarity-based semi-local models show significant\nimprovement in predictive performance compared to standard regional and local\nestimation methods. They further allow for estimating complex models without\nnumerical stability issues and are computationally more efficient than local\nparameter estimation. \n\n"}
{"id": "1509.03730", "contents": "Title: Estimating whole brain dynamics using spectral clustering Abstract: The estimation of time-varying networks for functional Magnetic Resonance\nImaging (fMRI) data sets is of increasing importance and interest. In this\nwork, we formulate the problem in a high-dimensional time series framework and\nintroduce a data-driven method, namely Network Change Points Detection (NCPD),\nwhich detects change points in the network structure of a multivariate time\nseries, with each component of the time series represented by a node in the\nnetwork. NCPD is applied to various simulated data and a resting-state fMRI\ndata set. This new methodology also allows us to identify common functional\nstates within and across subjects. Finally, NCPD promises to offer a deep\ninsight into the large-scale characterisations and dynamics of the brain \n\n"}
{"id": "1509.03770", "contents": "Title: Practical Bayesian Tomography Abstract: In recent years, Bayesian methods have been proposed as a solution to a wide\nrange of issues in quantum state and process tomography. State-of-the-art\nBayesian tomography solutions suffer from three problems: numerical\nintractability, a lack of informative prior distributions, and an inability to\ntrack time-dependent processes. Here, we address all three problems. First, we\nuse modern statistical methods, as pioneered by Husz\\'ar and Houlsby and by\nFerrie, to make Bayesian tomography numerically tractable. Our approach allows\nfor practical computation of Bayesian point and region estimators for quantum\nstates and channels. Second, we propose the first priors on quantum states and\nchannels that allow for including useful experimental insight. Finally, we\ndevelop a method that allows tracking of time-dependent states and estimates\nthe drift and diffusion processes affecting a state. We provide source code and\nanimated visual examples for our methods. \n\n"}
{"id": "1509.04447", "contents": "Title: Bayesian ensemble refinement by replica simulations and reweighting Abstract: We describe different Bayesian ensemble refinement methods, examine their\ninterrelation, and discuss their practical application. With ensemble\nrefinement, the properties of dynamic and partially disordered (bio)molecular\nstructures can be characterized by integrating a wide range of experimental\ndata, including measurements of ensemble-averaged observables. We start from a\nBayesian formulation in which the posterior is a functional that ranks\ndifferent configuration space distributions. By maximizing this posterior, we\nderive an optimal Bayesian ensemble distribution. For discrete configurations,\nthis optimal distribution is identical to that obtained by the maximum entropy\n\"ensemble refinement of SAXS\" (EROS) formulation. Bayesian replica ensemble\nrefinement enhances the sampling of relevant configurations by imposing\nrestraints on averages of observables in coupled replica molecular dynamics\nsimulations. We show that the strength of the restraint should scale linearly\nwith the number of replicas to ensure convergence to the optimal Bayesian\nresult in the limit of infinitely many replicas. In the \"Bayesian inference of\nensembles\" (BioEn) method, we combine the replica and EROS approaches to\naccelerate the convergence. An adaptive algorithm can be used to sample\ndirectly from the optimal ensemble, without replicas. We discuss the\nincorporation of single-molecule measurements and dynamic observables such as\nrelaxation parameters. The theoretical analysis of different Bayesian ensemble\nrefinement approaches provides a basis for practical applications and a\nstarting point for further investigations. \n\n"}
{"id": "1509.04486", "contents": "Title: Large-scale analysis of Zipf's law in English texts Abstract: Despite being a paradigm of quantitative linguistics, Zipf's law for words\nsuffers from three main problems: its formulation is ambiguous, its validity\nhas not been tested rigorously from a statistical point of view, and it has not\nbeen confronted to a representatively large number of texts. So, we can\nsummarize the current support of Zipf's law in texts as anecdotic.\n  We try to solve these issues by studying three different versions of Zipf's\nlaw and fitting them to all available English texts in the Project Gutenberg\ndatabase (consisting of more than 30000 texts). To do so we use state-of-the\nart tools in fitting and goodness-of-fit tests, carefully tailored to the\npeculiarities of text statistics. Remarkably, one of the three versions of\nZipf's law, consisting of a pure power-law form in the complementary cumulative\ndistribution function of word frequencies, is able to fit more than 40% of the\ntexts in the database (at the 0.05 significance level), for the whole domain of\nfrequencies (from 1 to the maximum value) and with only one free parameter (the\nexponent). \n\n"}
{"id": "1509.04492", "contents": "Title: Perpetual Codes for Network Coding Abstract: Random Linear Network Coding (RLNC) provides a theoretically efficient method\nfor coding. Some of its practical drawbacks are the complexity of decoding and\nthe overhead due to the coding vectors. For computationally weak and\nbattery-driven platforms, these challenges are particular important. In this\nwork, we consider the coding variant Perpetual codes which are sparse,\nnon-uniform and the coding vectors have a compact representation. The sparsity\nallows for fast encoding and decoding, and the non-uniform protection of\nsymbols enables recoding where the produced symbols are indistinguishable from\nthose encoded at the source. The presented results show that the approach can\nprovide a coding overhead arbitrarily close to that of RLNC, but at reduced\ncomputational load. The achieved gain over RLNC grows with the generation size,\nand both encoding and decoding throughput is approximately one order of\nmagnitude higher compared to RLNC at a generation size of 2048. Additionally,\nthe approach allows for easy adjustment between coding throughput and code\noverhead, which makes it suitable for a broad range of platforms and\napplications. \n\n"}
{"id": "1509.04828", "contents": "Title: Estimating heterogeneous graphical models for discrete data with an\n  application to roll call voting Abstract: We consider the problem of jointly estimating a collection of graphical\nmodels for discrete data, corresponding to several categories that share some\ncommon structure. An example for such a setting is voting records of\nlegislators on different issues, such as defense, energy, and healthcare. We\ndevelop a Markov graphical model to characterize the heterogeneous dependence\nstructures arising from such data. The model is fitted via a joint estimation\nmethod that preserves the underlying common graph structure, but also allows\nfor differences between the networks. The method employs a group penalty that\ntargets the common zero interaction effects across all the networks. We apply\nthe method to describe the internal networks of the U.S. Senate on several\nimportant issues. Our analysis reveals individual structure for each issue,\ndistinct from the underlying well-known bipartisan structure common to all\ncategories which we are able to extract separately. We also establish\nconsistency of the proposed method both for parameter estimation and model\nselection, and evaluate its numerical performance on a number of simulated\nexamples. \n\n"}
{"id": "1509.05121", "contents": "Title: Detecting Community Structures in Hi-C Genomic Data Abstract: Community detection (CD) algorithms are applied to Hi-C data to discover new\ncommunities of loci in the 3D conformation of human and mouse DNA. We find that\nCD has some distinct advantages over pre-existing methods: (1) it is capable of\nfinding a variable number of communities, (2) it can detect communities of DNA\nloci either adjacent or distant in the 1D sequence, and (3) it allows us to\nobtain a principled value of k, the number of communities present. Forcing k =\n2, our method recovers earlier findings of Lieberman-Aiden, et al. (2009), but\nletting k be a parameter, our method obtains as optimal value k = 6,\ndiscovering new candidate communities. In addition to discovering large\ncommunities that partition entire chromosomes, we also show that CD can detect\nsmall-scale topologically associating domains (TADs) such as those found in\nDixon, et al. (2012). CD thus provides a natural and flexible statistical\nframework for understanding the folding structure of DNA at multiple scales in\nHi-C data. \n\n"}
{"id": "1509.06443", "contents": "Title: Cosmic Web Reconstruction through Density Ridges: Catalogue Abstract: We construct a catalogue for filaments using a novel approach called SCMS\n(subspace constrained mean shift; Ozertem & Erdogmus 2011; Chen et al. 2015).\nSCMS is a gradient-based method that detects filaments through density ridges\n(smooth curves tracing high-density regions). A great advantage of SCMS is its\nuncertainty measure, which allows an evaluation of the errors for the detected\nfilaments. To detect filaments, we use data from the Sloan Digital Sky Survey,\nwhich consist of three galaxy samples: the NYU main galaxy sample (MGS), the\nLOWZ sample and the CMASS sample. Each of the three dataset covers different\nredshift regions so that the combined sample allows detection of filaments up\nto z = 0.7. Our filament catalogue consists of a sequence of two-dimensional\nfilament maps at different redshifts that provide several useful statistics on\nthe evolution cosmic web. To construct the maps, we select spectroscopically\nconfirmed galaxies within 0.050 < z < 0.700 and partition them into 130 bins.\nFor each bin, we ignore the redshift, treating the galaxy observations as a 2-D\ndata and detect filaments using SCMS. The filament catalogue consists of 130\nindividual 2-D filament maps, and each map comprises points on the detected\nfilaments that describe the filamentary structures at a particular redshift. We\nalso apply our filament catalogue to investigate galaxy luminosity and its\nrelation with distance to filament. Using a volume-limited sample, we find\nstrong evidence (6.1$\\sigma$ - 12.3$\\sigma$) that galaxies close to filaments\nare generally brighter than those at significant distance from filaments. \n\n"}
{"id": "1509.07535", "contents": "Title: Bayesian Nonparametric Graph Clustering Abstract: We present clustering methods for multivariate data exploiting the underlying\ngeometry of the graphical structure between variables. As opposed to standard\napproaches that assume known graph structures, we first estimate the edge\nstructure of the unknown graph using Bayesian neighborhood selection\napproaches, wherein we account for the uncertainty of graphical structure\nlearning through model-averaged estimates of the suitable parameters.\nSubsequently, we develop a nonparametric graph clustering model on the lower\ndimensional projections of the graph based on Laplacian embeddings using\nDirichlet process mixture models. In contrast to standard algorithmic\napproaches, this fully probabilistic approach allows incorporation of\nuncertainty in estimation and inference for both graph structure learning and\nclustering. More importantly, we formalize the arguments for Laplacian\nembeddings as suitable projections for graph clustering by providing\ntheoretical support for the consistency of the eigenspace of the estimated\ngraph Laplacians. We develop fast computational algorithms that allow our\nmethods to scale to large number of nodes. Through extensive simulations we\ncompare our clustering performance with standard clustering methods. We apply\nour methods to a novel pan-cancer proteomic data set, and evaluate protein\nnetworks and clusters across multiple different cancer types. \n\n"}
{"id": "1509.08304", "contents": "Title: Optimal Energy Allocation Policies for a High Altitude Flying Wireless\n  Access Point Abstract: Inspired by recent industrial efforts toward high altitude flying wireless\naccess points powered by renewable energy, an online resource allocation\nproblem for a mobile access point (AP) travelling at high altitude is\nformulated. The AP allocates its resources (available energy) to maximize the\ntotal utility (reward) provided to a sequentially observed set of users\ndemanding service. The problem is formulated as a 0/1 dynamic knapsack problem\nwith incremental capacity over a finite time horizon, the solution of which is\nquite open in the literature. We address the problem through deterministic and\nstochastic formulations. For the deterministic problem, several online\napproximations are proposed based on an instantaneous threshold that can adapt\nto short-time-scale dynamics. For the stochastic model, after showing the\noptimality of a threshold based solution on a dynamic programming (DP)\nformulation, an approximate threshold based policy is obtained. The\nperformances of proposed policies are compared with that of the optimal\nsolution obtained through DP. \n\n"}
{"id": "1510.00646", "contents": "Title: Bayesian modeling of networks in complex business intelligence problems Abstract: Complex network data problems are increasingly common in many fields of\napplication. Our motivation is drawn from strategic marketing studies\nmonitoring customer choices of specific products, along with co-subscription\nnetworks encoding multiple purchasing behavior. Data are available for several\nagencies within the same insurance company, and our goal is to efficiently\nexploit co-subscription networks to inform targeted advertising of cross-sell\nstrategies to currently mono-product customers. We address this goal by\ndeveloping a Bayesian hierarchical model, which clusters agencies according to\ncommon mono-product customer choices and co-subscription networks. Within each\ncluster, we efficiently model customer behavior via a cluster-dependent mixture\nof latent eigenmodels. This formulation provides key information on\nmono-product customer choices and multiple purchasing behavior within each\ncluster, informing targeted cross-sell strategies. We develop simple algorithms\nfor tractable inference, and assess performance in simulations and an\napplication to business intelligence. \n\n"}
{"id": "1510.03229", "contents": "Title: Statistically efficient tomography of low rank states with incomplete\n  measurements Abstract: The construction of physically relevant low dimensional state models, and the\ndesign of appropriate measurements are key issues in tackling quantum state\ntomography for large dimensional systems. We consider the statistical problem\nof estimating low rank states in the set-up of multiple ions tomography, and\ninvestigate how the estimation error behaves with a reduction in the number of\nmeasurement settings, compared with the standard ion tomography setup. We\npresent extensive simulation results showing that the error is robust with\nrespect to the choice of states of a given rank, the random selection of\nsettings, and that the number of settings can be significantly reduced with\nonly a negligible increase in error. We present an argument to explain these\nfindings based on a concentration inequality for the Fisher information matrix.\nIn the more general setup of random basis measurements we use this argument to\nshow that for certain rank $r$ states it suffices to measure in $O(r\\log d)$\nbases to achieve the average Fisher information over all bases. We present\nnumerical evidence for states upto 8 atoms, supporting a conjecture on a lower\nbound for the Fisher information which, if true, would imply a similar\nbehaviour in the case of Pauli bases. The relation to similar problems in\ncompressed sensing is also discussed. \n\n"}
{"id": "1510.03349", "contents": "Title: Toward a Better Understanding of Leaderboard Abstract: The leaderboard in machine learning competitions is a tool to show the\nperformance of various participants and to compare them. However, the\nleaderboard quickly becomes no longer accurate, due to hack or overfitting.\nThis article gives two pieces of advice to prevent easy hack or overfitting. By\nfollowing these advice, we reach the conclusion that something like the Ladder\nleaderboard introduced in [blum2015ladder] is inevitable. With this\nunderstanding, we naturally simplify Ladder by eliminating its redundant\ncomputation and explain how to choose the parameter and interpret it. We also\nprove that the sample complexity is cubic to the desired precision of the\nleaderboard. \n\n"}
{"id": "1510.05391", "contents": "Title: Unifying inference on brain network variations in neurological diseases:\n  The Alzheimer's case Abstract: There is growing interest in understanding how the structural\ninterconnections among brain regions change with the occurrence of neurological\ndiseases. Diffusion weighted MRI imaging has allowed researchers to\nnon-invasively estimate a network of structural cortical connections made by\nwhite matter tracts, but current statistical methods for relating such networks\nto the presence or absence of a disease cannot exploit this rich network\ninformation. Standard practice considers each edge independently or summarizes\nthe network with a few simple features. We enable dramatic gains in biological\ninsight via a novel unifying methodology for inference on brain network\nvariations associated to the occurrence of neurological diseases. The key of\nthis approach is to define a probabilistic generative mechanism directly on the\nspace of network configurations via dependent mixtures of low-rank\nfactorizations, which efficiently exploit network information and allow the\nprobability mass function for the brain network-valued random variable to vary\nflexibly across the group of patients characterized by a specific neurological\ndisease and the one comprising age-matched cognitively healthy individuals. \n\n"}
{"id": "1510.06021", "contents": "Title: Lightning Strikes and Attribution of Climatic Change Abstract: Using lightning strikes as an example, two possible schemes are discussed for\nthe attribution of changes in event frequency to climate change, and estimating\nthe cost associated with them. The schemes determine the fraction of events\nthat should be attributed to climatic change, and the fraction that should be\nattributed to natural chance. They both allow for the expected increase in\nclaims and the fluctuations about this expected value. Importantly, the\nattribution fraction proposed in the second of these schemes is necessarily\ndifferent to that found in epidemiological studies. This ensures that the\nstatistically expected fraction of attributed claims is correctly equal to the\nexpected increase in claims. The analysis of lightning data highlights two\nparticular difficulties with data-driven, as opposed to modeled, attribution\nstudies. The first is the possibility of unknown \"confounding\" variables that\ncan influence the strike frequency. This is partly accounted for here by\nconsidering the influence of temperature changes within a given month, so as to\nstandardise the data to allow for cyclical climatic influences. The second is\nthe possibility suggested by the data presented here, that climate change may\nlead to qualitatively different climate patterns, with a different relationship\nbetween e.g. strike frequency and temperature. \n\n"}
{"id": "1510.06488", "contents": "Title: When ICN Meets C-RAN for HetNets: An SDN Approach Abstract: In this paper, we contribute to novelly proposing and elaborating the\nintegration of the ICN, C-RAN and SDN for the HetNet to achieve win-win\nsituation. The vision of the proposed system is demonstrated, followed by the\nadvantages and challenges. We further present the hybrid system with a\nlarge-scale wireless heterogeneous campus network. \n\n"}
{"id": "1510.06871", "contents": "Title: mgm: Estimating Time-Varying Mixed Graphical Models in High-Dimensional\n  Data Abstract: We present the R-package mgm for the estimation of k-order Mixed Graphical\nModels (MGMs) and mixed Vector Autoregressive (mVAR) models in high-dimensional\ndata. These are a useful extensions of graphical models for only one variable\ntype, since data sets consisting of mixed types of variables (continuous,\ncount, categorical) are ubiquitous. In addition, we allow to relax the\nstationarity assumption of both models by introducing time-varying versions\nMGMs and mVAR models based on a kernel weighting approach. Time-varying models\noffer a rich description of temporally evolving systems and allow to identify\nexternal influences on the model structure such as the impact of interventions.\nWe provide the background of all implemented methods and provide fully\nreproducible examples that illustrate how to use the package. \n\n"}
{"id": "1510.07932", "contents": "Title: Downlink Power Control in Two-Tier Cellular Networks with\n  Energy-Harvesting Small Cells as Stochastic Games Abstract: Energy harvesting in cellular networks is an emerging technique to enhance\nthe sustainability of power-constrained wireless devices. This paper considers\nthe co-channel deployment of a macrocell overlaid with small cells. The small\ncell base stations (SBSs) harvest energy from environmental sources whereas the\nmacrocell base station (MBS) uses conventional power supply. Given a stochastic\nenergy arrival process for the SBSs, we derive a power control policy for the\ndownlink transmission of both MBS and SBSs such that they can achieve their\nobjectives (e.g., maintain the signal-to-interference-plus-noise ratio (SINR)\nat an acceptable level) on a given transmission channel. We consider a\ncentralized energy harvesting mechanism for SBSs, i.e., there is a central\nenergy storage (CES) where energy is harvested and then distributed to the\nSBSs. When the number of SBSs is small, the game between the CES and the MBS is\nmodeled as a single-controller stochastic game and the equilibrium policies are\nobtained as a solution of a quadratic programming problem. However, when the\nnumber of SBSs tends to infinity (i.e., a highly dense network), the\ncentralized scheme becomes infeasible, and therefore, we use a mean field\nstochastic game to obtain a distributed power control policy for each SBS. By\nsolving a system of partial differential equations, we derive the power control\npolicy of SBSs given the knowledge of mean field distribution and the available\nharvested energy levels in the batteries of the SBSs. \n\n"}
{"id": "1511.01864", "contents": "Title: Balancing Type I Error and Power in Linear Mixed Models Abstract: Linear mixed-effects models have increasingly replaced mixed-model analyses\nof variance for statistical inference in factorial psycholinguistic\nexperiments. Although LMMs have many advantages over ANOVA, like ANOVAs,\nsetting them up for data analysis also requires some care. One simple option,\nwhen numerically possible, is to fit the full variance-covariance structure of\nrandom effects (the maximal model; Barr et al. 2013), presumably to keep Type I\nerror down to the nominal alpha in the presence of random effects. Although it\nis true that fitting a model with only random intercepts may lead to higher\nType I error, fitting a maximal model also has a cost: it can lead to a\nsignificant loss of power. We demonstrate this with simulations and suggest\nthat for typical psychological and psycholinguistic data, higher power is\nachieved without inflating Type I error rate if a model selection criterion is\nused to select a random effect structure that is supported by the data. \n\n"}
{"id": "1511.03120", "contents": "Title: The cave of Shadows. Addressing the human factor with generalized\n  additive mixed models Abstract: Generalized additive mixed models are introduced as an extension of the\ngeneralized linear mixed model which makes it possible to deal with temporal\nautocorrelational structure in experimental data. This autocorrelational\nstructure is likely to be a consequence of learning, fatigue, or the ebb and\nflow of attention within an experiment (the `human factor'). Unlike molecules\nor plots of barley, subjects in psycholinguistic experiments are intelligent\nbeings that depend for their survival on constant adaptation to their\nenvironment, including the environment of an experiment. Three data sets\nillustrate that the human factor may interact with predictors of interest, both\nfactorial and metric. We also show that, especially within the framework of the\ngeneralized additive model, in the nonlinear world, fitting maximally complex\nmodels that take every possible contingency into account is ill-advised as a\nmodeling strategy. Alternative modeling strategies are discussed for both\nconfirmatory and exploratory data analysis. \n\n"}
{"id": "1511.05877", "contents": "Title: Generation of scenarios from calibrated ensemble forecasts with a dual\n  ensemble copula coupling approach Abstract: Probabilistic forecasts in the form of ensemble of scenarios are required for\ncomplex decision making processes. Ensemble forecasting systems provide such\nproducts but the spatio-temporal structures of the forecast uncertainty is lost\nwhen statistical calibration of the ensemble forecasts is applied for each lead\ntime and location independently. Non-parametric approaches allow the\nreconstruction of spatio-temporal joint probability distributions at a low\ncomputational cost. For example, the ensemble copula coupling (ECC) method\nrebuilds the multivariate aspect of the forecast from the original ensemble\nforecasts. Based on the assumption of error stationarity, parametric methods\naim to fully describe the forecast dependence structures. In this study, the\nconcept of ECC is combined with past data statistics in order to account for\nthe autocorrelation of the forecast error. The new approach, called d-ECC, is\napplied to wind forecasts from the high resolution ensemble system COSMO-DE-EPS\nrun operationally at the German weather service. Scenarios generated by ECC and\nd-ECC are compared and assessed in the form of time series by means of\nmultivariate verification tools and in a product oriented framework.\nVerification results over a 3 month period show that the innovative method\nd-ECC outperforms or performs as well as ECC in all investigated aspects. \n\n"}
{"id": "1511.06028", "contents": "Title: Optimal inference in a class of regression models Abstract: We consider the problem of constructing confidence intervals (CIs) for a\nlinear functional of a regression function, such as its value at a point, the\nregression discontinuity parameter, or a regression coefficient in a linear or\npartly linear regression. Our main assumption is that the regression function\nis known to lie in a convex function class, which covers most smoothness and/or\nshape assumptions used in econometrics. We derive finite-sample optimal CIs and\nsharp efficiency bounds under normal errors with known variance. We show that\nthese results translate to uniform (over the function class) asymptotic results\nwhen the error distribution is not known. When the function class is\ncentrosymmetric, these efficiency bounds imply that minimax CIs are close to\nefficient at smooth regression functions. This implies, in particular, that it\nis impossible to form CIs that are tighter using data-dependent tuning\nparameters, and maintain coverage over the whole function class. We specialize\nour results to inference on the regression discontinuity parameter, and\nillustrate them in simulations and an empirical application. \n\n"}
{"id": "1511.06063", "contents": "Title: A Novel Approach for Phase Identification in Smart Grids Using Graph\n  Theory and Principal Component Analysis Abstract: Consumers with low demand, like households, are generally supplied\nsingle-phase power by connecting their service mains to one of the phases of a\ndistribution transformer. The distribution companies face the problem of\nkeeping a record of consumer connectivity to a phase due to uninformed changes\nthat happen. The exact phase connectivity information is important for the\nefficient operation and control of distribution system. We propose a new data\ndriven approach to the problem based on Principal Component Analysis (PCA) and\nits Graph Theoretic interpretations, using energy measurements in equally timed\nshort intervals, generated from smart meters. We propose an algorithm for\ninferring phase connectivity from noisy measurements. The algorithm is\ndemonstrated using simulated data for phase connectivities in distribution\nnetworks. \n\n"}
{"id": "1511.06462", "contents": "Title: Joint Inverse Covariances Estimation with Mutual Linear Structure Abstract: We consider the problem of joint estimation of structured inverse covariance\nmatrices. We perform the estimation using groups of measurements with different\ncovariances of the same unknown structure. Assuming the inverse covariances to\nspan a low dimensional linear subspace in the space of symmetric matrices, our\naim is to determine this structure. It is then utilized to improve the\nestimation of the inverse covariances. We propose a novel optimization\nalgorithm discovering and exploiting the underlying structure and provide its\nefficient implementation. Numerical simulations are presented to illustrate the\nperformance benefits of the proposed algorithm. \n\n"}
{"id": "1511.08670", "contents": "Title: A Bayesian binary classification approach to pure tone audiometry Abstract: The pure tone hearing threshold is usually estimated from responses to\nstimuli at a set of standard frequencies. This paper describes a probabilistic\napproach to the estimation problem in which the hearing threshold is modelled\nas a smooth continuous function of frequency using a Gaussian process. This\nallows sampling at any frequency and reduces the number of required\nmeasurements. The Gaussian process is combined with a probabilistic response\nmodel to account for uncertainty in the responses. The resulting full model can\nbe interpreted as a two-dimensional binary classifier for stimuli, and provides\nuncertainty bands on the estimated threshold curve. The optimal next stimulus\nis determined based on an information theoretic criterion. This leads to a\nrobust adaptive estimation method that can be applied to fully automate the\nhearing threshold estimation process. \n\n"}
{"id": "1511.09416", "contents": "Title: Stochastic simulation of predictive space-time scenarios of wind speed\n  using observations and physical models Abstract: We propose a statistical space-time model for predicting atmospheric wind\nspeed based on deterministic numerical weather predictions and historical\nmeasurements. We consider a Gaussian multivariate space-time framework that\ncombines multiple sources of past physical model outputs and measurements along\nwith model predictions in order to produce a probabilistic wind speed forecast\nwithin the prediction window. We illustrate this strategy on a ground wind\nspeed forecast for several months in 2012 for a region near the Great Lakes in\nthe United States. The results show that the prediction is improved in the\nmean-squared sense relative to the numerical forecasts as well as in\nprobabilistic scores. Moreover, the samples are shown to produce realistic wind\nscenarios based on the sample spectrum. \n\n"}
{"id": "1512.00887", "contents": "Title: Single Molecule Spectroscopy of Monomeric LHCII: Experiment and Theory Abstract: We derive approximate equations of motion for excited state dynamics of a\nmultilevel open quantum system weakly interacting with light to describe\nfluorescence detected single molecule spectra. Based on the Frenkel exciton\ntheory, we construct a model for the chlorophyll part of the LHCII complex of\nhigher plants and its interaction with previously proposed excitation quencher\nin the form of the lutein molecule Lut 1. The resulting description is valid\nover a broad range of timescales relevant for single molecule spectroscopy,\ni.e. from ps to minutes. Validity of these equations is demonstrated by\ncomparing simulations of ensemble and single-molecule spectra of monomeric\nLHCII with experiments. Using a conformational change of the LHCII protein as a\nswitching mechanism, the intensity and spectral time traces of individual LHCII\ncomplexes are simulated, and the experimental statistical distributions are\nreproduced. Based on our model, it is shown that with reasonable assumptions\nabout its interaction with chlorophylls, Lut 1 can act as an efficient\nfluorescence quencher in LHCII. \n\n"}
{"id": "1512.01292", "contents": "Title: On Improving the Performance of Nonphotochemical Quenching in CP29\n  Light-Harvesting Antenna Complex Abstract: We model and simulate the performance of charge-transfer in nonphotochemical\nquenching (NPQ) in the CP29 light-harvesting antenna-complex associated with\nphotosystem II (PSII). The model consists of five discrete excitonic energy\nstates and two sinks, responsible for the potentially damaging processes and\ncharge-transfer channels, respectively. We demonstrate that by varying (i) the\nparameters of the chlorophyll-based dimer, (ii) the resonant properties of the\nprotein-solvent environment interaction, and (iii) the energy transfer rates to\nthe sinks, one can significantly improve the performance of the NPQ. Our\nanalysis suggests strategies for improving the performance of the NPQ in\nresponse to environmental changes, and may stimulate experimental verification. \n\n"}
{"id": "1512.01496", "contents": "Title: Embarrassingly Parallel Sequential Markov-chain Monte Carlo for Large\n  Sets of Time Series Abstract: Bayesian computation crucially relies on Markov chain Monte Carlo (MCMC)\nalgorithms. In the case of massive data sets, running the Metropolis-Hastings\nsampler to draw from the posterior distribution becomes prohibitive due to the\nlarge number of likelihood terms that need to be calculated at each iteration.\nIn order to perform Bayesian inference for a large set of time series, we\nconsider an algorithm that combines 'divide and conquer\" ideas previously used\nto design MCMC algorithms for big data with a sequential MCMC strategy. The\nperformance of the method is illustrated using a large set of financial data. \n\n"}
{"id": "1512.01998", "contents": "Title: Energy Efficiency of Massive MIMO: Coping with Daily Load Variation Abstract: Massive MIMO is a promising technique to meet the exponential growth of\nmobile traffic demand. However, contrary to the current systems, energy\nconsumption of next generation networks is required to be load adaptive as the\nnetwork load varies significantly throughout the day. In this paper, we propose\na load adaptive multi-cell massive MIMO system where each base station (BS)\nadapts the number of antennas to the daily load profile (DLP) in order to\nmaximize the downlink energy efficiency (EE). In order to incorporate the DLP,\nthe load at each BS is modeled as an M/G/m/m state dependent queue under the\nassumption that the network is dimensioned to serve a maximum number of users\nat the peak load. The EE maximization problem is formulated in a game theoretic\nframework where the number of antennas to be used by a BS is determined through\nbest response iteration. This load adaptive system achieves around 24% higher\nEE and saves around 40% energy compared to a baseline system where the BSs\nalways run with the fixed number of antennas that is most energy efficient at\nthe peak load and that can be switched off when there is no traffic. \n\n"}
{"id": "1512.06217", "contents": "Title: Bayesian bivariate meta-analysis of diagnostic test studies with\n  interpretable priors Abstract: In a bivariate meta-analysis the number of diagnostic studies involved is\noften very low so that frequentist methods may result in problems. Bayesian\ninference is attractive as informative priors that add small amount of\ninformation can stabilise the analysis without overwhelming the data. However,\nBayesian analysis is often computationally demanding and the selection of the\nprior for the covariance matrix of the bivariate structure is crucial with\nlittle data. The integrated nested Laplace approximations (INLA) method\nprovides an efficient solution to the computational issues by avoiding any\nsampling, but the important question of priors remain. We explore the penalised\ncomplexity (PC) prior framework for specifying informative priors for the\nvariance parameters and the correlation parameter. PC priors facilitate model\ninterpretation and hyperparameter specification as expert knowledge can be\nincorporated intuitively. We conduct a simulation study to compare the\nproperties and behaviour of differently defined PC priors to currently used\npriors in the field. The simulation study shows that the use of PC priors\nresults in more precise estimates when specified in a sensible neighbourhood\naround the truth. To investigate the usage of PC priors in practice we\nreanalyse a meta-analysis using the telomerase marker for the diagnosis of\nbladder cancer. \n\n"}
{"id": "1512.07914", "contents": "Title: The LSST Data Management System Abstract: The Large Synoptic Survey Telescope (LSST) is a large-aperture, wide-field,\nground-based survey system that will image the sky in six optical bands from\n320 to 1050 nm, uniformly covering approximately $18,000$deg$^2$ of the sky\nover 800 times. The LSST is currently under construction on Cerro Pach\\'on in\nChile, and expected to enter operations in 2022. Once operational, the LSST\nwill explore a wide range of astrophysical questions, from discovering \"killer\"\nasteroids to examining the nature of Dark Energy.\n  The LSST will generate on average 15 TB of data per night, and will require a\ncomprehensive Data Management system to reduce the raw data to scientifically\nuseful catalogs and images with minimum human intervention. These reductions\nwill result in a real-time alert stream, and eleven data releases over the\n10-year duration of LSST operations. To enable this processing, the LSST\nproject is developing a new, general-purpose, high-performance, scalable, well\ndocumented, open source data processing software stack for O/IR surveys.\nPrototypes of this stack are already capable of processing data from existing\ncameras (e.g., SDSS, DECam, MegaCam), and form the basis of the Hyper-Suprime\nCam (HSC) Survey data reduction pipeline. \n\n"}
{"id": "1512.08810", "contents": "Title: Dynamics of a Chlorophyll Dimer in Collective and Local Thermal\n  Environments Abstract: We present a theoretical analysis of exciton transfer and decoherence effects\nin a photosynthetic dimer interacting with collective (correlated) and local\n(uncorrelated) protein-solvent environments. Our approach is based on the\nframework of the spin-boson model. We derive explicitly the thermal relaxation\nand decoherence rates of the exciton transfer process, valid for arbitrary\ntemperatures and for arbitrary (in particular, large) interaction constants\nbetween the dimer and the environments. We establish a generalization of the\nMarcus formula, giving reaction rates for dimer levels possibly individually\nand asymmetrically coupled to environments. We identify rigorously parameter\nregimes for the validity of the generalized Marcus formula.\n  The existence of long living quantum coherences at ambient temperatures\nemerges naturally from our approach. \n\n"}
{"id": "1601.02522", "contents": "Title: Stationary signal processing on graphs Abstract: Graphs are a central tool in machine learning and information processing as\nthey allow to conveniently capture the structure of complex datasets. In this\ncontext, it is of high importance to develop flexible models of signals defined\nover graphs or networks. In this paper, we generalize the traditional concept\nof wide sense stationarity to signals defined over the vertices of arbitrary\nweighted undirected graphs. We show that stationarity is expressed through the\ngraph localization operator reminiscent of translation. We prove that\nstationary graph signals are characterized by a well-defined Power Spectral\nDensity that can be efficiently estimated even for large graphs. We leverage\nthis new concept to derive Wiener-type estimation procedures of noisy and\npartially observed signals and illustrate the performance of this new model for\ndenoising and regression. \n\n"}
{"id": "1601.03425", "contents": "Title: Frames and Phaseless Reconstruction Abstract: Frame design for phaseless reconstruction is now part of the broader problem\nof nonlinear reconstruction and is an emerging topic in harmonic analysis. The\nproblem of phaseless reconstruction can be simply stated as follows. Given the\nmagnitudes of the coefficients generated by a linear redundant system (frame),\nwe want to reconstruct the unknown input. This problem first occurred in X-ray\ncrystallography starting in the early 20th century. The same nonlinear\nreconstruction problem shows up in speech processing, particularly in speech\nrecognition.\n  In this lecture we shall cover existing analysis results as well as stability\nbounds for signal recovery including: necessary and sufficient conditions for\ninjectivity, Lipschitz bounds of the nonlinear map and its left inverses,\nstochastic performance bounds, and algorithms for signal recovery. \n\n"}
{"id": "1601.03461", "contents": "Title: Greedy-Knapsack Algorithm for Optimal Downlink Resource Allocation in\n  LTE Networks Abstract: The Long Term Evolution (LTE) as a mobile broadband technology supports a\nwide domain of communication services with different requirements. Therefore,\nscheduling of all flows from various applications in overload states in which\nthe requested amount of bandwidth exceeds the limited available spectrum\nresources is a challenging issue. Accordingly, in this paper, a greedy\nalgorithm is presented to evaluate user candidates which are waiting for\nscheduling and select an optimal set of the users to maximize system\nperformance, without exceeding available bandwidth capacity. The\ngreedy-knapsack algorithm is defined as an optimal solution to the resource\nallocation problem, formulated based on the fractional knapsack problem. A\ncompromise between throughput and QoS provisioning is obtained by proposing a\nclass-based ranking function, which is a combination of throughput and QoS\nrelated parameters defined for each application. The simulation results show\nthat the proposed method provides high performance in terms of throughput, loss\nand delay for different classes of QoS over the existing ones, especially under\noverload traffic. \n\n"}
{"id": "1601.04217", "contents": "Title: Harvest the potential of massive MIMO with multi-layer techniques Abstract: Massive MIMO is envisioned as a promising technology for 5G wireless networks\ndue to its high potential to improve both spectral and energy efficiency.\nAlthough the massive MIMO system is based on innovations in the physical layer,\nthe upper layer techniques also play important roles in harvesting the\nperformance gains of massive MIMO. In this article, we begin with an analysis\nof the benefits and challenges of massive MIMO systems. We then investigate the\nmulti-layer techniques for incorporating massive MIMO in several important\nnetwork deployment scenarios. We conclude this article with a discussion of\nopen and potential problems for future research. \n\n"}
{"id": "1601.04219", "contents": "Title: BOOST: Base station on-off switching strategy for energy efficient\n  massive MIMO HetNets Abstract: In this paper, we investigate the problem of optimal base station (BS) ON-OFF\nswitching and user association in a heterogeneous network (HetNet) with massive\nMIMO, with the objective to maximize the system energy efficiency (EE). The\njoint BS ON-OFF switching and user association problem is formulated as an\ninteger programming problem. We first develop a centralized scheme, in which we\nrelax the integer constraints and employ a series of Lagrangian dual methods\nthat transform the original problem into a standard linear programming (LP)\nproblem. Due to the special structure of the LP, we prove that the optimal\nsolution to the relaxed LP is also feasible and optimal to the original\nproblem. We then propose a distributed scheme by formulating a repeated bidding\ngame for users and BS's, and prove that the game converges to a Nash\nEquilibrium (NE). Simulation studies demonstrate that the proposed schemes can\nachieve considerable gains in EE over several benchmark schemes in all the\nscenarios considered. \n\n"}
{"id": "1601.05260", "contents": "Title: On the perfomance of a photosystem II reaction centre-based photocell Abstract: The photosystem II reaction centre is the photosynthetic complex responsible\nfor oxygen production on Earth. Its water splitting function is particularly\nfavoured by the formation of a stable charge separated state via a pathway that\nstarts at an accessory chlorophyll. Here we envision a photovoltaic device that\nplaces one of these complexes between electrodes and investigate how the mean\ncurrent and its fluctuations depend on the microscopic interactions underlying\ncharge separation in the pathway considered. Our results indicate that coupling\nto well resolved vibrational modes does not necessarily offer an advantage in\nterms of power output but can lead to photo-currents with suppressed noise\nlevels characterizing a multi-step ordered transport process. Besides giving\ninsight into the suitability of these complexes for molecular-scale\nphotovoltaics, our work suggests a new possible biological function for the\nvibrational environment of photosynthetic reaction centres, namely, to reduce\nthe intrinsic current noise for regulatory processes. \n\n"}
{"id": "1601.06447", "contents": "Title: Sequential Hypothesis Test with Online Usage-Constrained Sensor\n  Selection Abstract: This work investigates the sequential hypothesis testing problem with online\nsensor selection and sensor usage constraints. That is, in a sensor network,\nthe fusion center sequentially acquires samples by selecting one \"most\ninformative\" sensor at each time until a reliable decision can be made. In\nparticular, the sensor selection is carried out in the online fashion since it\ndepends on all the previous samples at each time. Our goal is to develop the\nsequential test (i.e., stopping rule and decision function) and sensor\nselection strategy that minimize the expected sample size subject to the\nconstraints on the error probabilities and sensor usages. To this end, we first\nrecast the usage-constrained formulation into a Bayesian optimal stopping\nproblem with different sampling costs for the usage-contrained sensors. The\nBayesian problem is then studied under both finite- and infinite-horizon\nsetups, based on which, the optimal solution to the original usage-constrained\nproblem can be readily established. Moreover, by capitalizing on the structures\nof the optimal solution, a lower bound is obtained for the optimal expected\nsample size. In addition, we also propose algorithms to approximately evaluate\nthe parameters in the optimal sequential test so that the sensor usage and\nerror probability constraints are satisfied. Finally, numerical experiments are\nprovided to illustrate the theoretical findings, and compare with the existing\nmethods. \n\n"}
{"id": "1601.06805", "contents": "Title: P-values: misunderstood and misused Abstract: P-values are widely used in both the social and natural sciences to quantify\nthe statistical significance of observed results. The recent surge of big data\nresearch has made the p-value an even more popular tool to test the\nsignificance of a study. However, substantial literature has been produced\ncritiquing how p-values are used and understood. In this paper we review this\nrecent critical literature, much of which is routed in the life sciences, and\nconsider its implications for social scientific research. We provide a coherent\npicture of what the main criticisms are, and draw together and disambiguate\ncommon themes. In particular, we explain how the False Discovery Rate is\ncalculated, and how this differs from a p-value. We also make explicit the\nBayesian nature of many recent criticisms, a dimension that is often\nunderplayed or ignored. We conclude by identifying practical steps to help\nremediate some of the concerns identified. We recommend that (i) far lower\nsignificance levels are used, such as $0.01$ or $0.001$, and (ii) p-values are\ninterpreted contextually, and situated within both the findings of the\nindividual study and the broader field of inquiry (through, for example,\nmeta-analyses). \n\n"}
{"id": "1602.00202", "contents": "Title: Bayesian stochastic volatility models for high-frequency data Abstract: We formulate a discrete-time Bayesian stochastic volatility model for\nhigh-frequency stock-market data that directly accounts for microstructure\nnoise, and outline a Markov chain Monte Carlo algorithm for parameter\nestimation. The methods described in this paper are designed to be coherent\nacross all sampling timescales, with the goal of estimating the latent\nlog-volatility signal from data collected at arbitrarily short sampling\nperiods. In keeping with this goal, we carefully develop a method for eliciting\npriors. The empirical results derived from both simulated and real data show\nthat directly accounting for microstructure in a state-space formulation allows\nfor well-calibrated estimates of the log-volatility process driving prices. \n\n"}
{"id": "1602.00924", "contents": "Title: (Quantum) Fractional Brownian Motion and Multifractal Processes under\n  the Loop of a Tensor Networks Abstract: We derive fractional Brownian motion and stochastic processes with\nmultifractal properties using a framework of network of Gaussian conditional\nprobabilities. This leads to the derivation of new representations of\nfractional Brownian motion. These constructions are inspired from\nrenormalization. The main result of this paper consists of constructing each\nincrement of the process from two-dimensional gaussian noise inside the\nlight-cone of each seperate increment. Not only does this allows us to derive\nfractional Brownian motion, we can introduce extensions with multifractal\nflavour. In another part of this paper, we discuss the use of the multi-scale\nentanglement renormalization ansatz (MERA), introduced in the study critical\nsystems in quantum spin lattices, as a method for sampling integrals with\nrespect to such multifractal processes. After proper calibration, a MERA\npromises the generation of a sample of size $N$ of a multifractal process in\nthe order of $O(N\\log(N))$, an improvement over the known methods, such as the\nCholesky decomposition and the circulant methods, which scale between $O(N^2)$\nand $O(N^3)$. \n\n"}
{"id": "1602.01672", "contents": "Title: The Generalised Isolation-With-Migration Model: a Maximum-Likelihood\n  Implementation for Multilocus Data Sets Abstract: Statistical inference about the speciation process has often been based on\nthe isolation-with-migration (IM) model, especially when the research aim is to\nlearn about the presence or absence of gene flow during divergence. The\ngeneralised IM model introduced in this paper extends both the standard\ntwo-population IM model and the isolation-with-initial-migration (IIM) model,\nand encompasses both these models as special cases. It can be described as a\ntwo-population IM model in which migration rates and population sizes are\nallowed to change at some point in the past. By developing a maximum-likelihood\nimplementation of this GIM model, we enable inference on both historical and\ncontemporary rates of gene flow between two closely related species. Our method\nrelies on the spectral decomposition of the coalescent generator matrix and is\napplicable to data sets consisting of the numbers of nucleotide differences\nbetween one pair of DNA sequences at each of a large number of independent\nloci. \n\n"}
{"id": "1602.02655", "contents": "Title: Optimisation of Simulations of Stochastic Processes by Removal of\n  Opposing Reactions Abstract: Models invoking the chemical master equation are used in many areas of\nscience, and, hence, their simulation is of interest to many researchers. The\ncomplexity of the problems at hand often requires considerable computational\npower, so a large number of algorithms have been developed to speed up\nsimulations. However, a drawback of many of these algorithms is that their\nimplementation is more complicated than, for instance, the Gillespie algorithm,\nwhich is widely used to simulate the chemical master equation, and can be\nimplemented with a few lines of code. Here, we present an algorithm which does\nnot modify the way in which the master equation is solved, but instead modifies\nthe transition rates, and can thus be implemented with a few lines of code. It\nworks for all models in which reversible reactions occur by replacing such\nreversible reactions with effective net reactions. Examples of such systems\ninclude reaction-diffusion systems, in which diffusion is modelled by a random\nwalk. The random movement of particles between neighbouring sites is then\nreplaced with a net random flux. Furthermore, as we modify the transition rates\nof the model, rather than its implementation on a computer, our method can be\ncombined with existing algorithms that were designed to speed up simulations of\nthe stochastic master equation. By focusing on some specific models, we show\nhow our algorithm can significantly speed up model simulations while\nmaintaining essential features of the original model. \n\n"}
{"id": "1602.03305", "contents": "Title: Coverage and capacity scaling laws in downlink ultra-dense cellular\n  networks Abstract: Driven by new types of wireless devices and the proliferation of\nbandwidth-intensive applications, data traffic and the corresponding network\nload are increasing dramatically. Network densification has been recognized as\na promising and efficient way to provide higher network capacity and enhanced\ncoverage. Most prior work on performance analysis of ultra-dense networks\n(UDNs) has focused on random spatial deployment with idealized singular path\nloss models and Rayleigh fading. In this paper, we consider a more precise and\ngeneral model, which incorporates multi-slope path loss and general fading\ndistributions. We derive the tail behavior and scaling laws for the coverage\nprobability and the capacity considering strongest base station association in\na Poisson field network. Our analytical results identify the regimes in which\nthe signal-to-interference-plus-noise ratio (SINR) either asymptotically grows,\nsaturates, or decreases with increasing network density. We establish general\nresults on when UDNs lead to worse or even zero SINR coverage and capacity, and\nwe provide crisp insights on the fundamental limits of wireless network\ndensification. \n\n"}
{"id": "1602.03551", "contents": "Title: Knowledge Transfer with Medical Language Embeddings Abstract: Identifying relationships between concepts is a key aspect of scientific\nknowledge synthesis. Finding these links often requires a researcher to\nlaboriously search through scien- tific papers and databases, as the size of\nthese resources grows ever larger. In this paper we describe how distributional\nsemantics can be used to unify structured knowledge graphs with unstructured\ntext to predict new relationships between medical concepts, using a\nprobabilistic generative model. Our approach is also designed to ameliorate\ndata sparsity and scarcity issues in the medical domain, which make language\nmodelling more challenging. Specifically, we integrate the medical relational\ndatabase (SemMedDB) with text from electronic health records (EHRs) to perform\nknowledge graph completion. We further demonstrate the ability of our model to\npredict relationships between tokens not appearing in the relational database. \n\n"}
{"id": "1602.03906", "contents": "Title: How to group wireless nodes together? Abstract: This report presents a survey on how to group together in a static way planar\nnodes, that may belong to a wireless network (ad hoc or cellular). The aim is\nto identify appropriate methods that could also be applied for Point Processes.\nSpecifically matching pairs and algorithms are initially discussed. Next,\nspecifically for Point Processes, the Nearest Neighbour and Lilypond models are\npresented. Properties and results for the two models are stated. Original\nbounds are given for the value of the so-called generation number, which is\nrelated to the size of the nearest neighbour cluster. Finally, a variation of\nthe nearest neighbour grouping is proposed and an original metric is\nintroduced, named here the ancestor number. This is used to facilitate the\nanalysis of the distribution of cluster size. Based on this certain related\nbounds are derived. The report and the analysis included show clearly the\ndifficulty of working in point processes with static clusters of size greater\nthan two, when these are defined by proximity criteria. \n\n"}
{"id": "1602.04003", "contents": "Title: Bayesian smoothing of dipoles in Magneto-/Electro-encephalography Abstract: We describe a novel method for dynamic estimation of multi-dipole states from\nMagneto/Electro-encephalography (M/EEG) time series. The new approach builds on\nthe recent development of particle filters for M/EEG; these algorithms\napproximate, with samples and weights, the posterior distribution of the neural\nsources at time t given the data up to time t. However, for off-line inference\npurposes it is preferable to work with the smoothing distribution, i.e. the\ndistribution for the neural sources at time t conditioned on the whole time\nseries. In this study, we use a Monte Carlo algorithm to approximate the\nsmoothing distribution for a time-varying set of current dipoles. We show,\nusing numerical simulations, that the estimates provided by the smoothing\ndistribution are more accurate than those provided by the filtering\ndistribution, particularly at the appearance of the source. We validate the\nproposed algorithm using an experimental dataset recorded from an epileptic\npatient. Improved localization of the source onset can be particularly relevant\nin source modeling of epileptic patients, where the source onset brings\ninformation on the epileptogenic zone. \n\n"}
{"id": "1602.04528", "contents": "Title: Hierarchical multivariate space-time methods for modeling counts with an\n  application to stroke mortality data Abstract: Geographic patterns in stroke mortality have been studied as far back as the\n1960s, when a region of the southeastern United States became known as the\n\"stroke belt\" due to its unusually high rates of stroke mortality. While stroke\nmortality rates are known to increase exponentially with age, an investigation\nof spatiotemporal trends by age group at the county-level is daunting due to\nthe preponderance of small population sizes and/or few stroke events by age\ngroup. Here, we harness the power of a complex, nonseparable multivariate\nspace-time model which borrows strength across space, time, and age group to\nobtain reliable estimates of yearly county-level mortality rates from US\ncounties between 1973 and 2013 for those aged 65+. Furthermore, we propose an\nalternative metric for measuring changes in event rates over time which\naccounts for the full trajectory of a county's event rates, as opposed to\nsimply comparing the rates at the beginning and end of the study period. In our\nanalysis of the stroke data, we identify differing spatiotemporal trends in\nmortality rates across age groups, shed light on the gains achieved in the Deep\nSouth, and provide evidence that a separable model is inappropriate for these\ndata. \n\n"}
{"id": "1602.05642", "contents": "Title: When the Filter Bubble Bursts: Collective Evaluation Dynamics in Online\n  Communities Abstract: We analyze online collective evaluation processes through positive and\nnegative votes in various social media. We find two modes of collective\nevaluations that stem from the existence of filter bubbles. Above a threshold\nof collective attention, negativity grows faster with positivity, as a sign of\nthe burst of a filter bubble when information reaches beyond the local social\ncontext of a user. We analyze how collectively evaluated content can reach\nlarge social contexts and create polarization, showing that emotions expressed\nthrough text play a key role in collective evaluation processes. \n\n"}
{"id": "1602.05652", "contents": "Title: The effect of base pair mismatch on DNA strand displacement Abstract: DNA strand displacement is a key reaction in DNA homologous recombination and\nDNA mismatch repair and is also heavily utilized in DNA-based computation and\nlocomotion. Despite its ubiquity in science and engineering, sequence-dependent\neffects of displacement kinetics have not been extensively characterized. Here,\nwe measured toehold-mediated strand displacement kinetics using single-molecule\nfluorescence in the presence of a single base pair mismatch. The apparent\ndisplacement rate varied significantly when the mismatch was introduced in the\ninvading DNA strand. The rate generally decreased as the mismatch in the\ninvader was encountered earlier in displacement. Our data indicate that a\nsingle base pair mismatch in the invader stalls branch migration, and\ndisplacement occurs via direct dissociation of the destabilized incumbent\nstrand from the substrate strand. We combined both branch migration and direct\ndissociation into a model, which we term, the concurrent displacement model,\nand used the first passage time approach to quantitatively explain the salient\nfeatures of the observed relationship. We also introduce the concept of\nsplitting probabilities to justify that the concurrent model can be simplified\ninto a three-step sequential model in the presence of an invader mismatch. We\nexpect our model to become a powerful tool to design DNA-based reaction schemes\nwith broad functionality. \n\n"}
{"id": "1602.07112", "contents": "Title: Multipath streaming: fundamental limits and efficient algorithms Abstract: We investigate streaming over multiple links. A file is split into small\nunits called chunks that may be requested on the various links according to\nsome policy, and received after some random delay. After a start-up time called\npre-buffering time, received chunks are played at a fixed speed. There is\nstarvation if the chunk to be played has not yet arrived. We provide lower\nbounds (fundamental limits) on the starvation probability of any policy. We\nfurther propose simple, order-optimal policies that require no feedback. For\ngeneral delay distributions, we provide tractable upper bounds for the\nstarvation probability of the proposed policies, allowing to select the\npre-buffering time appropriately. We specialize our results to: (i) links that\nemploy CSMA or opportunistic scheduling at the packet level, (ii) links shared\nwith a primary user (iii) links that use fair rate sharing at the flow level.\nWe consider a generic model so that our results give insight into the design\nand performance of media streaming over (a) wired networks with several paths\nbetween the source and destination, (b) wireless networks featuring spectrum\naggregation and (c) multi-homed wireless networks. \n\n"}
{"id": "1602.07754", "contents": "Title: A Compressed Sensing Based Decomposition of Electrodermal Activity\n  Signals Abstract: The measurement and analysis of Electrodermal Activity (EDA) offers\napplications in diverse areas ranging from market research, to seizure\ndetection, to human stress analysis. Unfortunately, the analysis of EDA signals\nis made difficult by the superposition of numerous components which can obscure\nthe signal information related to a user's response to a stimulus. We show how\nsimple pre-processing followed by a novel compressed sensing based\ndecomposition can mitigate the effects of the undesired noise components and\nhelp reveal the underlying physiological signal. The proposed framework allows\nfor decomposition of EDA signals with provable bounds on the recovery of user\nresponses. We test our procedure on both synthetic and real-world EDA signals\nfrom wearable sensors and demonstrate that our approach allows for more\naccurate recovery of user responses as compared to the existing techniques. \n\n"}
{"id": "1602.08066", "contents": "Title: Scalable semiparametric inference for the means of heavy-tailed\n  distributions Abstract: Heavy tailed distributions present a tough setting for inference. They are\nalso common in industrial applications, particularly with Internet transaction\ndatasets, and machine learners often analyze such data without considering the\nbiases and risks associated with the misuse of standard tools. This paper\noutlines a procedure for inference about the mean of a (possibly conditional)\nheavy tailed distribution that combines nonparametric analysis for the bulk of\nthe support with Bayesian parametric modeling -- motivated from extreme value\ntheory -- for the heavy tail. The procedure is fast and massively scalable. The\nresulting point estimators attain lowest-possible error rates and, unique among\nalternatives, we are able to provide accurate uncertainty quantification for\nthese estimators. The work should find application in settings wherever correct\ninference is important and reward tails are heavy; we illustrate the framework\nin causal inference for A/B experiments involving hundreds of millions of users\nof eBay.com. \n\n"}
{"id": "1602.08154", "contents": "Title: Efficient Bayesian Inference for Multivariate Factor Stochastic\n  Volatility Models Abstract: We discuss efficient Bayesian estimation of dynamic covariance matrices in\nmultivariate time series through a factor stochastic volatility model. In\nparticular, we propose two interweaving strategies (Yu and Meng, Journal of\nComputational and Graphical Statistics, 20(3), 531-570, 2011) to substantially\naccelerate convergence and mixing of standard MCMC approaches. Similar to\nmarginal data augmentation techniques, the proposed acceleration procedures\nexploit non-identifiability issues which frequently arise in factor models. Our\nnew interweaving strategies are easy to implement and come at almost no extra\ncomputational cost; nevertheless, they can boost estimation efficiency by\nseveral orders of magnitude as is shown in extensive simulation studies. To\nconclude, the application of our algorithm to a 26-dimensional exchange rate\ndata set illustrates the superior performance of the new approach for\nreal-world data. \n\n"}
{"id": "1602.08290", "contents": "Title: Explicit back-off rates for achieving target throughputs in CSMA/CA\n  networks Abstract: CSMA/CA networks have often been analyzed using a stylized model that is\nfully characterized by a vector of back-off rates and a conflict graph.\nFurther, for any achievable throughput vector $\\vec \\theta$ the existence of a\nunique vector $\\vec \\nu(\\vec \\theta)$ of back-off rates that achieves this\nthroughput vector was proven. Although this unique vector can in principle be\ncomputed iteratively, the required time complexity grows exponentially in the\nnetwork size, making this only feasible for small networks.\n  In this paper, we present an explicit formula for the unique vector of\nback-off rates $\\vec \\nu(\\vec \\theta)$ needed to achieve any achievable\nthroughput vector $\\vec \\theta$ provided that the network has a chordal\nconflict graph. This class of networks contains a number of special cases of\ninterest such as (inhomogeneous) line networks and networks with an acyclic\nconflict graph. Moreover, these back-off rates are such that the back-off rate\nof a node only depends on its own target throughput and the target throughput\nof its neighbors and can be determined in a distributed manner.\n  We further indicate that back-off rates of this form cannot be obtained in\ngeneral for networks with non-chordal conflict graphs. For general conflict\ngraphs we nevertheless show how to adapt the back-off rates when a node is\nadded to the network when its interfering nodes form a clique in the conflict\ngraph. Finally, we introduce a distributed chordal approximation algorithm for\ngeneral conflict graphs which is shown (using numerical examples) to be more\naccurate than the Bethe approximation. \n\n"}
{"id": "1602.08658", "contents": "Title: Interference Avoidance Algorithm (IAA) for Multi-hop Wireless Body Area\n  Network Communication Abstract: In this paper, we propose a distributed multi-hop interference avoidance\nalgorithm, namely, IAA to avoid co-channel interference inside a wireless body\narea network (WBAN). Our proposal adopts carrier sense multiple access with\ncollision avoidance (CSMA/CA) between sources and relays and a flexible time\ndivision multiple access (FTDMA) between relays and coordinator. The proposed\nscheme enables low interfering nodes to transmit their messages using base\nchannel. Depending on suitable situations, high interfering nodes double their\ncontention windows (CW) and probably use switched orthogonal channel.\nSimulation results show that proposed scheme has far better minimum SINR (12dB\nimprovement) and longer energy lifetime than other schemes (power control and\nopportunistic relaying). Additionally, we validate our proposal in a\ntheoretical analysis and also propose a probabilistic approach to prove the\noutage probability can be effectively reduced to the minimal. \n\n"}
{"id": "1602.08710", "contents": "Title: Dynamic Channel Access Scheme for Interference Mitigation in\n  Relay-assisted Intra-WBANs Abstract: This work addresses problems related to interference mitigation in a single\nwireless body area network (WBAN). In this paper, We propose a distributed\n\\textit{C}ombined carrier sense multiple access with collision avoidance\n(CSMA/CA) with \\textit{F}lexible time division multiple access (\\textit{T}DMA)\nscheme for \\textit{I}nterference \\textit{M}itigation in relay-assisted\nintra-WBAN, namely, CFTIM. In CFTIM scheme, non interfering sources\n(transmitters) use CSMA/CA to communicate with relays. Whilst, high interfering\nsources and best relays use flexible TDMA to communicate with coordinator (C)\nthrough using stable channels. Simulation results of the proposed scheme are\ncompared to other schemes and consequently CFTIM scheme outperforms in all\ncases. These results prove that the proposed scheme mitigates interference,\nextends WBAN energy lifetime and improves the throughput. To further reduce the\ninterference level, we analytically show that the outage probability can be\neffectively reduced to the minimal. \n\n"}
{"id": "1602.08861", "contents": "Title: Bayesian inference for age-structured population model of infectious\n  disease with application to varicella in Poland Abstract: Dynamics of the infectious disease transmission is often best understood\ntaking into account the structure of population with respect to specific\nfeatures, in example age or immunity level. Practical utility of such models\ndepends on the appropriate calibration with the observed data. Here, we discuss\nthe Bayesian approach to data assimilation in case of two-state age-structured\nmodel. This kind of models are frequently used to describe the disease dynamics\n(i.e. force of infection) basing on prevalence data collected at several time\npoints. We demonstrate that, in the case when the explicit solution to the\nmodel equation is known, accounting for the data collection process in the\nBayesian framework allows to obtain an unbiased posterior distribution for the\nparameters determining the force of infection. We further show analytically and\nthrough numerical tests that the posterior distribution of these parameters is\nstable with respect to cohort approximation (Escalator Boxcar Train) to the\nsolution. Finally, we apply the technique to calibrate the model based on\nobserved sero-prevalence of varicella in Poland. \n\n"}
{"id": "1602.09039", "contents": "Title: Dynamic Channel Allocation for Interference Mitigation in Relay-assisted\n  Wireless Body Networks Abstract: We focus on interference mitigation and energy conservation within a single\nwireless body area network (WBAN). We adopt two-hop communication scheme\nsupported by the the IEEE 802.15.6 standard (2012). In this paper, we propose a\ndynamic channel allocation scheme, namely DCAIM to mitigate node-level\ninterference amongst the coexisting regions of a WBAN. At the time, the sensors\nare in the radius communication of a relay, they form a relay region (RG)\ncoordinated by that relay using time division multiple access (TDMA). In the\nproposed scheme, each RG creates a table consisting of interfering sensors\nwhich it broadcasts to its neighboring sensors. This broadcast allows each pair\nof RGs to create an interference set (IS). Thus, the members of IS are assigned\northogonal sub-channels whereas other sonsors that do not belong to IS can\ntransmit using the same time slots. Experimental results show that our proposal\nmitigates node-level interference and improves node and WBAN energy savings.\nThese results are then compared to the results of other schemes. As a result,\nour scheme outperforms in all cases. Node-level signal to interference and\nnoise ratio (SINR) improved by 11dB whilst, the energy consumption decreased\nsignificantly. We further present a probabilistic method and analytically show\nthe outage probability can be effectively reduced to the minimal. \n\n"}
{"id": "1603.00074", "contents": "Title: Modeling the Infectiousness of Twitter Hashtags Abstract: This study applies dynamical and statistical modeling techniques to quantify\nthe proliferation and popularity of trending hashtags on Twitter. Using\ntime-series data reflecting actual tweets in New York City and San Francisco,\nwe present estimates for the dynamics (i.e., rates of infection and recovery)\nof several hundred trending hashtags using an epidemic modeling framework\ncoupled with Bayesian Markov Chain Monte Carlo (MCMC) methods. This\nmethodological strategy is an extension of techniques traditionally used to\nmodel the spread of infectious disease. We demonstrate that in some models,\nhashtags can be grouped by infectiousness, possibly providing a method for\nquantifying the trendiness of a topic. \n\n"}
{"id": "1603.01066", "contents": "Title: What we look at in paintings: A comparison between experienced and\n  inexperienced art viewers Abstract: How do people look at art? Are there any differences between how experienced\nand inexperienced art viewers look at a painting? We approach these questions\nby analyzing and modeling eye movement data from a cognitive art research\nexperiment, where the eye movements of twenty test subjects, ten experienced\nand ten inexperienced art viewers, were recorded while they were looking at\npaintings.\n  Eye movements consist of stops of the gaze as well as jumps between the\nstops. Hence, the observed gaze stop locations can be thought as a spatial\npoint pattern, which can be modeled by a spatio-temporal point process. We\nintroduce some statistical tools to analyze the spatio-temporal eye movement\ndata, and compare the eye movements of experienced and inexperienced art\nviewers. In addition, we develop a stochastic model, which is rather simple but\nfits quite well to the eye movement data, to further investigate the\ndifferences between the two groups through functional summary statistics. \n\n"}
{"id": "1603.03950", "contents": "Title: A Copula Model for Non-Gaussian Multivariate Spatial Data Abstract: We propose a new copula model for replicated multivariate spatial data.\nUnlike classical models that assume multivariate normality of the data, the\nproposed copula is based on the assumption that some factors exist that affect\nthe joint spatial dependence of all measurements of each variable as well as\nthe joint dependence among these variables. The model is parameterized in terms\nof a cross-covariance function that may be chosen from the many models proposed\nin the literature. In addition, there are additive factors in the model that\nallow tail dependence and reflection asymmetry of each variable measured at\ndifferent locations and of different variables to be modeled. The proposed\napproach can therefore be seen as an extension of the linear model of\ncoregionalization widely used for modeling multivariate spatial data. The\nlikelihood of the model can be obtained in a simple form and therefore the\nlikelihood estimation is quite fast. The model is not restricted to the set of\ndata locations, and using the estimated copula, spatial data can be\ninterpolated at locations where values of variables are unknown. We apply the\nproposed model to temperature and pressure data and compare its performance\nwith the performance of a popular model from multivariate geostatistics. \n\n"}
{"id": "1603.05215", "contents": "Title: Phase Retrieval from 1D Fourier Measurements: Convexity, Uniqueness, and\n  Algorithms Abstract: This paper considers phase retrieval from the magnitude of 1D over-sampled\nFourier measurements, a classical problem that has challenged researchers in\nvarious fields of science and engineering. We show that an optimal vector in a\nleast-squares sense can be found by solving a convex problem, thus establishing\na hidden convexity in Fourier phase retrieval. We also show that the standard\nsemidefinite relaxation approach yields the optimal cost function value (albeit\nnot necessarily an optimal solution) in this case. A method is then derived to\nretrieve an optimal minimum phase solution in polynomial time. Using these\nresults, a new measuring technique is proposed which guarantees uniqueness of\nthe solution, along with an efficient algorithm that can solve large-scale\nFourier phase retrieval problems with uniqueness and optimality guarantees. \n\n"}
{"id": "1603.05914", "contents": "Title: Statistically validated network of portfolio overlaps and systemic risk Abstract: Common asset holding by financial institutions, namely portfolio overlap, is\nnowadays regarded as an important channel for financial contagion with the\npotential to trigger fire sales and thus severe losses at the systemic level.\nIn this paper we propose a method to assess the statistical significance of the\noverlap between pairs of heterogeneously diversified portfolios, which then\nallows us to build a validated network of financial institutions where links\nindicate potential contagion channels due to realized portfolio overlaps. The\nmethod is implemented on a historical database of institutional holdings\nranging from 1999 to the end of 2013, but can be in general applied to any\nbipartite network where the presence of similar sets of neighbors is of\ninterest. We find that the proportion of validated network links (i.e., of\nstatistically significant overlaps) increased steadily before the 2007-2008\nglobal financial crisis and reached a maximum when the crisis occurred. We\nargue that the nature of this measure implies that systemic risk from fire\nsales liquidation was maximal at that time. After a sharp drop in 2008,\nsystemic risk resumed its growth in 2009, with a notable acceleration in 2013,\nreaching levels not seen since 2007. We finally show that market trends tend to\nbe amplified in the portfolios identified by the algorithm, such that it is\npossible to have an informative signal about financial institutions that are\nabout to suffer (enjoy) the most significant losses (gains). \n\n"}
{"id": "1603.06476", "contents": "Title: Dynamic Prediction for Multiple Repeated Measures and Event Time Data:\n  An Application to Parkinson's Disease Abstract: In many clinical trials studying neurodegenerative diseases such as\nParkinson's disease (PD), multiple longitudinal outcomes are collected to fully\nexplore the multidimensional impairment caused by this disease. If the outcomes\ndeteriorate rapidly, patients may reach a level of functional disability\nsufficient to initiate levodopa therapy for ameliorating disease symptoms. An\naccurate prediction of the time to functional disability is helpful for\nclinicians to monitor patients' disease progression and make informative\nmedical decisions. In this article, we first propose a joint model that\nconsists of a semiparametric multilevel latent trait model (MLLTM) for the\nmultiple longitudinal outcomes, and a survival model for event time. The two\nsubmodels are linked together by an underlying latent variable. We develop a\nBayesian approach for parameter estimation and a dynamic prediction framework\nfor predicting target patients' future outcome trajectories and risk of a\nsurvival event, based on their multivariate longitudinal measurements. Our\nproposed model is evaluated by simulation studies and is applied to the DATATOP\nstudy, a motivating clinical trial assessing the effect of deprenyl among\npatients with early PD. \n\n"}
{"id": "1603.07409", "contents": "Title: Joint hierarchical models for sparsely sampled high-dimensional LiDAR\n  and forest variables Abstract: Recent advancements in remote sensing technology, specifically Light\nDetection and Ranging (LiDAR) sensors, provide the data needed to quantify\nforest characteristics at a fine spatial resolution over large geographic\ndomains. From an inferential standpoint, there is interest in prediction and\ninterpolation of the often sparsely sampled and spatially misaligned LiDAR\nsignals and forest variables. We propose a fully process-based Bayesian\nhierarchical model for above ground biomass (AGB) and LiDAR signals. The\nprocess-based framework offers richness in inferential capabilities, e.g.,\ninference on the entire underlying processes instead of estimates only at\npre-specified points. Key challenges we obviate include misalignment between\nthe AGB observations and LiDAR signals and the high-dimensionality in the model\nemerging from LiDAR signals in conjunction with the large number of spatial\nlocations. We offer simulation experiments to evaluate our proposed models and\nalso apply them to a challenging dataset comprising LiDAR and spatially\ncoinciding forest inventory variables collected on the Penobscot Experimental\nForest (PEF), Maine. Our key substantive contributions include AGB data\nproducts with associated measures of uncertainty for the PEF and, more broadly,\na methodology that should find use in a variety of current and upcoming forest\nvariable mapping efforts using sparsely sampled remotely sensed\nhigh-dimensional data. \n\n"}
{"id": "1603.07433", "contents": "Title: Characterizing Honeypot-Captured Cyber Attacks: Statistical Framework\n  and Case Study Abstract: Rigorously characterizing the statistical properties of cyber attacks is an\nimportant problem. In this paper, we propose the {\\em first} statistical\nframework for rigorously analyzing honeypot-captured cyber attack data. The\nframework is built on the novel concept of {\\em stochastic cyber attack\nprocess}, a new kind of mathematical objects for describing cyber attacks. To\ndemonstrate use of the framework, we apply it to analyze a low-interaction\nhoneypot dataset, while noting that the framework can be equally applied to\nanalyze high-interaction honeypot data that contains richer information about\nthe attacks. The case study finds, for the first time, that Long-Range\nDependence (LRD) is exhibited by honeypot-captured cyber attacks. The case\nstudy confirms that by exploiting the statistical properties (LRD in this\ncase), it is feasible to predict cyber attacks (at least in terms of attack\nrate) with good accuracy. This kind of prediction capability would provide\nsufficient early-warning time for defenders to adjust their defense\nconfigurations or resource allocations. The idea of \"gray-box\" (rather than\n\"black-box\") prediction is central to the utility of the statistical framework,\nand represents a significant step towards ultimately understanding (the degree\nof) the {\\em predictability} of cyber attacks. \n\n"}
{"id": "1603.07511", "contents": "Title: Statistical modelling of individual animal movement: an overview of key\n  methods and a discussion of practical challenges Abstract: With the influx of complex and detailed tracking data gathered from\nelectronic tracking devices, the analysis of animal movement data has recently\nemerged as a cottage industry amongst biostatisticians. New approaches of ever\ngreater complexity are continue to be added to the literature. In this paper,\nwe review what we believe to be some of the most popular and most useful\nclasses of statistical models used to analyze individual animal movement data.\nSpecifically we consider discrete-time hidden Markov models, more general\nstate-space models and diffusion processes. We argue that these models should\nbe core components in the toolbox for quantitative researchers working on\nstochastic modelling of individual animal movement. The paper concludes by\noffering some general observations on the direction of statistical analysis of\nanimal movement. There is a trend in movement ecology toward what are arguably\noverly-complex modelling approaches which are inaccessible to ecologists,\nunwieldy with large data sets or not based in mainstream statistical practice.\nAdditionally, some analysis methods developed within the ecological community\nignore fundamental properties of movement data, potentially leading to\nmisleading conclusions about animal movement. Corresponding approaches, e.g.\nbased on L\\'evy walk-type models, continue to be popular despite having been\nlargely discredited. We contend that there is a need for an appropriate balance\nbetween the extremes of either being overly complex or being overly simplistic,\nwhereby the discipline relies on models of intermediate complexity that are\nusable by general ecologists, but grounded in well-developed statistical\npractice and efficient to fit to large data sets. \n\n"}
{"id": "1603.07668", "contents": "Title: Estimating Cross-validatory Predictive P-values with Integrated\n  Importance Sampling for Disease Mapping Models Abstract: An important statistical task in disease mapping problems is to identify\ndivergent regions with unusually high or low risk of disease. Leave-one-out\ncross-validatory (LOOCV) model assessment is the gold standard for estimating\npredictive p-values that can flag such divergent regions. However, actual LOOCV\nis time-consuming because one needs to rerun a Markov chain Monte Carlo\nanalysis for each posterior distribution in which an observation is held out as\na test case. This paper introduces a new method, called integrated importance\nsampling (iIS), for estimating LOOCV predictive p-values with only Markov chain\nsamples drawn from the posterior based on a full data set. The key step in iIS\nis that we integrate away the latent variables associated the test observation\nwith respect to their conditional distribution \\textit{without} reference to\nthe actual observation. By following the general theory for importance\nsampling, the formula used by iIS can be proved to be equivalent to the LOOCV\npredictive p-value. We compare iIS and other three existing methods in the\nliterature with two disease mapping datasets. Our empirical results show that\nthe predictive p-values estimated with iIS are almost identical to the\npredictive p-values estimated with actual LOOCV, and outperform those given by\nthe existing three methods, namely, the posterior predictive checking, the\nordinary importance sampling, and the ghosting method by Marshall and\nSpiegelhalter (2003). \n\n"}
{"id": "1603.07749", "contents": "Title: Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High\n  Dimensional Mediators Abstract: In many scientific studies, it becomes increasingly important to delineate\nthe causal pathways through a large number of mediators, such as genetic and\nbrain mediators. Structural equation modeling (SEM) is a popular technique to\nestimate the pathway effects, commonly expressed as products of coefficients.\nHowever, it becomes unstable to fit such models with high dimensional\nmediators, especially for a general setting where all the mediators are\ncausally dependent but the exact causal relationships between them are unknown.\nThis paper proposes a sparse mediation model using a regularized SEM approach,\nwhere sparsity here means that a small number of mediators have nonzero\nmediation effects between a treatment and an outcome. To address the model\nselection challenge, we innovate by introducing a new penalty called Pathway\nLasso. This penalty function is a convex relaxation of the non-convex product\nfunction, and it enables a computationally tractable optimization criterion to\nestimate and select many pathway effects simultaneously. We develop a fast\nADMM-type algorithm to compute the model parameters, and we show that the\niterative updates can be expressed in closed form. On both simulated data and a\nreal fMRI dataset, the proposed approach yields higher pathway selection\naccuracy and lower estimation bias than other competing methods. \n\n"}
{"id": "1603.08062", "contents": "Title: Optimal Traffic Aggregation in Multi-RAT Heterogeneous Wireless Networks Abstract: Traffic load balancing and radio resource management is key to harness the\ndense and increasingly heterogeneous deployment of next generation \"$5$G\"\nwireless infrastructure. Strategies for aggregating user traffic from across\nmultiple radio access technologies (RATs) and/or access points (APs) would be\ncrucial in such heterogeneous networks (HetNets), but are not well\ninvestigated. In this paper, we develop a low complexity solution for\nmaximizing an $\\alpha$-optimal network utility leveraging the multi-link\naggregation (simultaneous connectivity to multiple RATs/APs) capability of\nusers in the network. The network utility maximization formulation has\nmaximization of sum rate ($\\alpha=0$), maximization of minimum rate ($\\alpha\n\\to \\infty$), and proportional fair ($\\alpha=1$) as its special cases. A closed\nform is also developed for the special case where a user aggregates traffic\nfrom at most two APs/RATs, and hence can be applied to practical scenarios like\nLTE-WLAN aggregation (LWA) and LTE dual-connectivity solutions. It is shown\nthat the required objective may also be realized through a decentralized\nimplementation requiring a series of message exchanges between the users and\nnetwork. Using comprehensive system level simulations, it is shown that optimal\nleveraging of multi-link aggregation leads to substantial throughput gains over\nsingle RAT/AP selection techniques. \n\n"}
{"id": "1603.08885", "contents": "Title: On the Performance of Delay Aware Shared Access with Priorities Abstract: In this paper, we analyze a shared access network with a fixed primary node\nand randomly distributed secondary nodes whose distribution follows a Poisson\npoint process (PPP). The secondaries use a random access protocol allowing them\nto access the channel with probabilities that depend on the queue size of the\nprimary. Assuming a system with multipacket reception (MPR) receivers having\nbursty packet arrivals at the primary and saturation at the secondaries, our\nprotocol can be tuned to alleviate congestion at the primary. We study the\nthroughput of the secondary network and the primary average delay, as well as\nthe impact of the secondary node access probability and transmit power. We\nformulate an optimization problem to maximize the throughput of the secondary\nnetwork under delay constraints for the primary node, which in the case that no\ncongestion control is performed has a closed form expression providing the\noptimal access probability. Our numerical results illustrate the impact of\nnetwork operating parameters on the performance of the proposed priority-based\nshared access protocol. \n\n"}
{"id": "1604.00698", "contents": "Title: Asymptotic Theory of Rerandomization in Treatment-Control Experiments Abstract: Although complete randomization ensures covariate balance on average, the\nchance for observing significant differences between treatment and control\ncovariate distributions increases with many covariates. Rerandomization\ndiscards randomizations that do not satisfy a predetermined covariate balance\ncriterion, generally resulting in better covariate balance and more precise\nestimates of causal effects. Previous theory has derived finite sample theory\nfor rerandomization under the assumptions of equal treatment group sizes,\nGaussian covariate and outcome distributions, or additive causal effects, but\nnot for the general sampling distribution of the difference-in-means estimator\nfor the average causal effect. To supplement existing results, we develop\nasymptotic theory for rerandomization without these assumptions, which reveals\na non-Gaussian asymptotic distribution for this estimator, specifically a\nlinear combination of a Gaussian random variable and a truncated Gaussian\nrandom variable. This distribution follows because rerandomization affects only\nthe projection of potential outcomes onto the covariate space but does not\naffect the corresponding orthogonal residuals. We also demonstrate that,\ncompared to complete randomization, rerandomization reduces the asymptotic\nsampling variances and quantile ranges of the difference-in-means estimator.\nMoreover, our work allows the construction of accurate large-sample confidence\nintervals for the average causal effect, thereby revealing further advantages\nof rerandomization over complete randomization. \n\n"}
{"id": "1604.01544", "contents": "Title: Day of the week effect in paper submission/acceptance/rejection to/in/by\n  peer review journals Abstract: This paper aims at providing an introduction to the behavior of authors\nsubmitting a paper to a scientific journal. Dates of electronic submission of\npapers to the Journal of the Serbian Chemical Society have been recorded from\nthe 1st January 2013 till the 31st December 2014, thus over 2 years.\n  There is no Monday or Friday effect like in financial markets, but rather a\nTuesday-Wednesday effect occurs: papers are more often submitted on Wednesday;\nhowever, the relative number of going to be accepted papers is larger if these\nare submitted on Tuesday. On the other hand, weekend days (Saturday and Sunday)\nare not the best days to finalize and submit manuscripts. An interpretation\nbased on the type of submitted work (\"experimental chemistry\") and on the\ninfluence of (senior) coauthors is presented. A thermodynamic connection is\nproposed within an entropy context. A (new) entropic distance is defined in\norder to measure the \"opaqueness\" = disorder) of the submission process. \n\n"}
{"id": "1604.01715", "contents": "Title: A phylogenetic latent feature model for clonal deconvolution Abstract: Tumours develop in an evolutionary process, in which the accumulation of\nmutations produces subpopulations of cells with distinct mutational profiles,\ncalled clones. This process leads to the genetic heterogeneity widely observed\nin tumour sequencing data, but identifying the genotypes and frequencies of the\ndifferent clones is still a major challenge. Here, we present Cloe, a\nphylogenetic latent feature model to deconvolute tumour sequencing data into a\nset of related genotypes. Our approach extends latent feature models by placing\nthe features as nodes in a latent tree. The resulting model can capture both\nthe acquisition and the loss of mutations, as well as episodes of convergent\nevolution. We establish the validity of Cloe on synthetic data and assess its\nperformance on controlled biological data, comparing our reconstructions to\nthose of several published state-of-the-art methods. We show that our method\nprovides highly accurate reconstructions and identifies the number of clones,\ntheir genotypes and frequencies even at a modest sequencing depth. As a proof\nof concept we apply our model to clinical data from three cases with chronic\nlymphocytic leukaemia, and one case with acute myeloid leukaemia. \n\n"}
{"id": "1604.02665", "contents": "Title: Energy Efficiency Optimization of 5G Radio Frequency Chain Systems Abstract: With the massive multi-input multi-output (MIMO) antennas technology adopted\nfor the fifth generation (5G) wireless communication systems, a large number of\nradio frequency (RF) chains have to be employed for RF circuits. However, a\nlarge number of RF chains not only increase the cost of RF circuits but also\nconsume additional energy in 5G wireless communication systems. In this paper\nwe investigate energy and cost efficiency optimization solutions for 5G\nwireless communication systems with a large number of antennas and RF chains.\nAn energy efficiency optimization problem is formulated for 5G wireless\ncommunication systems using massive MIMO antennas and millimeter wave\ntechnology. Considering the nonconcave feature of the objective function, a\nsuboptimal iterative algorithm, i.e., the energy efficient hybrid precoding\n(EEHP) algorithm is developed for maximizing the energy efficiency of 5G\nwireless communication systems. To reduce the cost of RF circuits, the energy\nefficient hybrid precoding with the minimum number of RF chains (EEHP-MRFC)\nalgorithm is also proposed. Moreover, the critical number of antennas searching\n(CNAS) and user equipment number optimization (UENO) algorithms are further\ndeveloped to optimize the energy efficiency of 5G wireless communication\nsystems by the number of transmit antennas and UEs. Compared with the maximum\nenergy efficiency of conventional zero-forcing (ZF) precoding algorithm,\nnumerical results indicate that the maximum energy efficiency of the proposed\nEEHP and EEHP-MRFC algorithms are improved by 220% and 171%, respectively. \n\n"}
{"id": "1604.02986", "contents": "Title: Stronger wireless signals appear more Poisson Abstract: Keeler, Ross and Xia (2016) recently derived approximation and convergence\nresults, which imply that the point process formed from the signal strengths\nreceived by an observer in a wireless network under a general statistical\npropagation model can be modelled by an inhomogeneous Poisson point process on\nthe positive real line. The basic requirement for the results to apply is that\nthere must be a large number of transmitters with different locations and\nrandom propagation effects.The aim of this note is to apply some of the main\nresults of Keeler, Ross and Xia (2016) in a less general but more easily\napplicable form to illustrate how the results can be applied in practice. New\nresults are derived that show that it is the strongest signals, after being\nweakened by random propagation effects, that behave like a Poisson process,\nwhich supports recent experimental work. \n\n"}
{"id": "1604.03776", "contents": "Title: Detecting a Structural Change in Functional Time Series Using Local\n  Wilcoxon Statistic Abstract: Functional data analysis (FDA) is a part of modern multivariate statistics\nthat analyses data providing information about curves, surfaces or anything\nelse varying over a certain continuum. In economics and empirical finance we\noften have to deal with time series of functional data, where we cannot easily\ndecide, whether they are to be considered as homogeneous or heterogeneous. At\npresent a discussion on adequate tests of homogenity for functional data is\ncarried. We propose a novel statistic for detetecting a structural change in\nfunctional time series based on a local Wilcoxon statistic induced by a local\ndepth function proposed by Paindaveine and Van Bever (2013). \n\n"}
{"id": "1604.04527", "contents": "Title: Deep Learning for Short-Term Traffic Flow Prediction Abstract: We develop a deep learning model to predict traffic flows. The main\ncontribution is development of an architecture that combines a linear model\nthat is fitted using $\\ell_1$ regularization and a sequence of $\\tanh$ layers.\nThe challenge of predicting traffic flows are the sharp nonlinearities due to\ntransitions between free flow, breakdown, recovery and congestion. We show that\ndeep learning architectures can capture these nonlinear spatio-temporal\neffects. The first layer identifies spatio-temporal relations among predictors\nand other layers model nonlinear relations. We illustrate our methodology on\nroad sensor data from Interstate I-55 and predict traffic flows during two\nspecial events; a Chicago Bears football game and an extreme snowstorm event.\nBoth cases have sharp traffic flow regime changes, occurring very suddenly, and\nwe show how deep learning provides precise short term traffic flow predictions. \n\n"}
{"id": "1604.04640", "contents": "Title: Coverage Gains from the Static Cooperation of Mutually Nearest\n  Neighbours Abstract: Cooperation in cellular networks has been recently suggested as a promising\nscheme to improve system performance. In this work, clusters are formed based\non the Mutually Nearest Neighbour relation, which defines which stations\ncooperate in pair and which do not. When node positions follow a Poisson Point\nProcess (PPP) the performance of the original clustering model can be\napproximated by another one, formed by the superposition of two PPPs (one for\nthe singles and one for the pairs) equipped with adequate marks. This allows to\nderive exact expressions for the network coverage probability under two\nuser-cluster association rules. Numerical evaluation shows coverage gains from\ndifferent signal cooperation schemes that can reach up to 15% compared to the\nstandard non-cooperative network coverage. The analysis is general and can be\napplied to any type of cooperation or coordination between pairs of\ntransmitting nodes. \n\n"}
{"id": "1604.05224", "contents": "Title: BFDA: A Matlab Toolbox for Bayesian Functional Data Analysis Abstract: We provide a MATLAB toolbox, BFDA, that implements a Bayesian hierarchical\nmodel to smooth multiple functional data with the assumptions of the same\nunderlying Gaussian process distribution, a Gaussian process prior for the mean\nfunction, and an Inverse-Wishart process prior for the covariance function.\nThis model-based approach can borrow strength from all functional data to\nincrease the smoothing accuracy, as well as estimate the mean-covariance\nfunctions simultaneously. An option of approximating the Bayesian inference\nprocess using cubic B-spline basis functions is integrated in BFDA, which\nallows for efficiently dealing with high-dimensional functional data. Examples\nof using BFDA in various scenarios and conducting follow-up functional\nregression are provided. The advantages of BFDA include: (1) Simultaneously\nsmooths multiple functional data and estimates the mean-covariance functions in\na nonparametric way; (2) flexibly deals with sparse and high-dimensional\nfunctional data with stationary and nonstationary covariance functions, and\nwithout the requirement of common observation grids; (3) provides accurately\nsmoothed functional data for follow-up analysis. \n\n"}
{"id": "1604.05482", "contents": "Title: Geometry, supertransfer, and optimality in the light harvesting of\n  purple bacteria Abstract: The remarkable rotational symmetry of the photosynthetic antenna complexes of\npurple bacteria has long been thought to enhance their light harvesting and\nexcitation energy transport. We study the role of symmetry by modeling\nhypothetical antennas whose symmetry is broken by altering the orientations of\nthe bacteriochlorophyll pigments. We find that in both LH2 and LH1 complexes,\nsymmetry increases energy transfer rates by enabling the cooperative, coherent\nprocess of supertransfer. The enhancement is particularly pronounced in the LH1\ncomplex, whose natural geometry outperforms the average randomized geometry by\n5.5 standard deviations, the most significant coherence-related enhancement\nfound in a photosynthetic complex. \n\n"}
{"id": "1604.05622", "contents": "Title: Understanding Noise and Interference Regimes in 5G Millimeter-Wave\n  Cellular Networks Abstract: With the severe spectrum shortage in conventional cellular bands,\nmillimeter-wave (mmWave) frequencies have been attracting growing attention for\nnext-generation micro- and picocellular wireless networks. A fundamental and\nopen question is whether mmWave cellular networks are likely to be noise- or\ninterference-limited. Identifying in which regime a network is operating is\ncritical for the design of MAC and physical-layer procedures and to provide\ninsights on how transmissions across cells should be coordinated to cope with\ninterference. This work uses the latest measurement-based statistical channel\nmodels to accurately assess the Interference-to-Noise Ratio (INR) in a wide\nrange of deployment scenarios. In addition to cell density, we also study\nantenna array size and antenna patterns, whose effects are critical in the\nmmWave regime. The channel models also account for blockage, line-of-sight and\nnon-line-of-sight regimes as well as local scattering, that significantly\naffect the level of spatial isolation. \n\n"}
{"id": "1604.05623", "contents": "Title: Channel Dynamics and SNR Tracking in Millimeter Wave Cellular Systems Abstract: The millimeter wave (mmWave) frequencies are likely to play a significant\nrole in fifth-generation (5G) cellular systems. A key challenge in developing\nsystems in these bands is the potential for rapid channel dynamics: since\nmmWave signals are blocked by many materials, small changes in the position or\norientation of the handset relative to objects in the environment can cause\nlarge swings in the channel quality. This paper addresses the issue of tracking\nthe signal to noise ratio (SNR), which is an essential procedure for rate\nprediction, handover and radio link failure detection. A simple method for\nestimating the SNR from periodic synchronization signals is considered. The\nmethod is then evaluated using real experiments in common blockage scenarios\ncombined with outdoor statistical models. \n\n"}
{"id": "1604.07098", "contents": "Title: WavmatND: A MATLAB Package for Non-Decimated Wavelet Transform and its\n  Applications Abstract: A non-decimated wavelet transform (NDWT) is a popular version of wavelet\ntransforms because of its many advantages in applications. The inherent\nredundancy of this transform proved beneficial in tasks of signal denoising and\nscaling assessment. To facilitate the use of NDWT, we built a MATLAB package,\n{\\bf WavmatND}, which has three novel features: First, for signals of moderate\nsize the proposed method reduces computation time of the NDWT by replacing\nrepetitive convolutions with matrix multiplications. Second, submatrices of an\nNDWT matrix can be rescaled, which enables a straightforward inverse transform.\nFinally, the method has no constraints on a size of the input signal in one or\nin two dimensions, so signals of non-dyadic length and rectangular\ntwo-dimensional signals with non-dyadic sides can be readily transformed. We\nprovide illustrative examples and a tutorial to assist users in application of\nthis stand-alone package. \n\n"}
{"id": "1604.08744", "contents": "Title: Enabling Relaying Over Heterogeneous Backhauls in the Uplink of Wireless\n  Femtocell Networks Abstract: In this paper, we develop novel two-tier interference management strategies\nthat enable macrocell users (MUEs) to improve their performance, with the help\nof open-access femtocells. To this end, we propose a rate-splitting technique\nusing which the MUEs optimize their uplink transmissions by dividing their\nsignals into two types: a coarse message that is intended for direct\ntransmission to the macrocell base station and a fine message that is decoded\nby a neighboring femtocell and subsequently relayed over a heterogeneous\n(wireless/wired) backhaul. For deploying the proposed technique, we formulate a\nnon-cooperative game between the MUEs in which each MUE can decide on its\nrelaying femtocell while maximizing a utility function that captures both the\nachieved throughput and the expected backhaul delay. Simulation results show\nthat the proposed approach yields up to 125% rate improvement and up to 2 times\ndelay reduction with wired backhaul and, 150% rate improvement and up to 10\ntimes delay reduction with wireless backhaul, relative to classical\ninterference management approaches, with no cross-tier cooperation. \n\n"}
{"id": "1604.08758", "contents": "Title: Dynamic Clustering and Sleep Mode Strategies for Small Cell Networks Abstract: In this paper, a novel cluster-based approach for optimizing the energy\nefficiency of wireless small cell networks is proposed. A dynamic mechanism\nbased on the spectral clustering technique is proposed to dynamically form\nclusters of small cell base stations. Such clustering enables intra-cluster\ncoordination among the base stations for optimizing the downlink performance\nthrough load balancing, while satisfying users' quality-of-service\nrequirements. In the proposed approach, the clusters use an opportunistic base\nstation sleep-wake switching mechanism to strike a balance between delay and\nenergy consumption. The inter-cluster interference affects the performance of\nthe clusters and their choices of active or sleep state. Due to the lack of\ninter-cluster communications, the clusters have to compete with each other to\nmake decisions on improving the energy efficiency. This competition is\nformulated as a noncooperative game among the clusters that seek to minimize a\ncost function which captures the tradeoff between energy expenditure and load.\nTo solve this game, a distributed learning algorithm is proposed using which\nthe clusters autonomously choose their optimal transmission strategies.\nSimulation results show that the proposed approach yields significant\nperformance gains in terms of reduced energy expenditures up to 40% and reduced\nload up to 23% compared to conventional approaches. \n\n"}
{"id": "1605.00382", "contents": "Title: Hybrid Spectrum Access for mmWave Networks Abstract: While spectrum at millimeter wave (mmWave) frequencies is less scarce than at\ntraditional frequencies below 6 GHz, still it is not unlimited, in particular\nif we consider the requirements from other services using the same band and the\nneed to license mmWave bands to multiple mobile operators. Therefore, an\nefficient spectrum access scheme is critical to harvest the maximum benefit\nfrom emerging mmWave technologies. In this paper, motivated by previous results\nwhere spectrum pooling was proved to be more feasible at high mmWave\nfrequencies, we study the performance of a hybrid spectrum scheme where\nexclusive access is used at frequencies in the 20/30 GHz range while spectrum\npooling/unlicensed spectrum is used at frequencies around 70 GHz. Our\npreliminary results show that hybrid spectrum access is a promising approach\nfor mmWave networks, and motivate further studies to achieve a more complete\nunderstanding of both technical and non technical implications. \n\n"}
{"id": "1605.01526", "contents": "Title: Estimating model evidence using data assimilation Abstract: We review the field of data assimilation (DA) from a Bayesian perspective and\nshow that, in addition to its by now common application to state estimation, DA\nmay be used for model selection. An important special case of the latter is the\ndiscrimination between a factual model --- which corresponds, to the best of\nthe modeler's knowledge, to the situation in the actual world in which a\nsequence of events has occurred --- and a counterfactual model, in which a\nparticular forcing or process might be absent or just quantitatively different\nfrom the actual world. Three different ensemble-DA methods are reviewed for\nthis purpose: the ensemble Kalman filter (EnKF), the ensemble four-dimensional\nvariational smoother (En-4D-Var), and the iterative ensemble Kalman smoother\n(IEnKS). An original contextual formulation of model evidence (CME) is\nintroduced. It is shown how to apply these three methods to compute CME, using\nthe approximated time-dependent probability distribution functions (pdfs) each\nof them provide in the process of state estimation. The theoretical formulae so\nderived are applied to two simplified nonlinear and chaotic models: (i) the\nLorenz three-variable convection (L63) model, and (ii) the Lorenz 40-variable\nmid-latitude atmospheric dynamics model (L95). The numerical results of these\nthree DA-based methods and those of an integration based on importance sampling\nare compared. It is found that better CME estimates are obtained by using DA,\nand the IEnKS method appears to be best among the DA methods. Differences among\nthe performance of the three DA-based methods are discussed as a function of\nmodel properties. Finally, the methodology is implemented for parameter\nestimation and for event attribution. \n\n"}
{"id": "1605.03280", "contents": "Title: The LASSO Estimator: Distributional Properties Abstract: The least absolute shrinkage and selection operator (LASSO) is a popular\ntechnique for simultaneous estimation and model selection. There have been a\nlot of studies on the large sample asymptotic distributional properties of the\nLASSO estimator, but it is also well-known that the asymptotic results can give\na wrong picture of the LASSO estimator's actual finite-sample behavior. The\nfinite sample distribution of the LASSO estimator has been previously studied\nfor the special case of orthogonal models. The aim in this work is to\ngeneralize the finite sample distribution properties of LASSO estimator for a\nreal and linear measurement model in Gaussian noise.\n  In this work, we derive an expression for the finite sample characteristic\nfunction of the LASSO estimator, we then use the Fourier slice theorem to\nobtain an approximate expression for the marginal probability density functions\nof the one-dimensional components of a linear transformation of the LASSO\nestimator. \n\n"}
{"id": "1605.03872", "contents": "Title: Discovering Effect Modification in an Observational Study of Surgical\n  Mortality at Hospitals with Superior Nursing Abstract: There is effect modification if the magnitude or stability of a treatment\neffect varies systematically with the level of an observed covariate. A larger\nor more stable treatment effect is typically less sensitive to bias from\nunmeasured covariates, so it is important to recognize effect modification when\nit is present. We illustrate a recent proposal for conducting a sensitivity\nanalysis that empirically discovers effect modification by exploratory methods,\nbut controls the family-wise error rate in discovered groups. The example\nconcerns a study of mortality and use of the intensive care unit in 23,715\nmatched pairs of two Medicare patients, one of whom underwent surgery at a\nhospital identified for superior nursing, the other at a conventional hospital.\nThe pairs were matched exactly for 130 four-digit ICD-9 surgical procedure\ncodes and balanced 172 observed covariates. The pairs were then split into five\ngroups of pairs by CART in its effort to locate effect modification. The\nevidence of a beneficial effect of magnet hospitals on mortality is least\nsensitive to unmeasured biases in a large group of patients undergoing rather\nserious surgical procedures, but in the absence of other life-threatening\nconditions, such as a comorbidity of congestive heart failure or an emergency\nadmission leading to surgery. \n\n"}
{"id": "1605.04602", "contents": "Title: Spectrum and Infrastructure Sharing in Millimeter Wave Cellular\n  Networks: An Economic Perspective Abstract: The licensing model for millimeter wave bands has been the subject of\nconsiderable debate, with some industry players advocating for unlicensed use\nand others for traditional geographic area exclusive use licenses. Meanwhile,\nthe massive bandwidth, highly directional antennas, high penetration loss and\nsusceptibility to shadowing in these bands suggest certain advantages to\nspectrum and infrastructure sharing. However, even when sharing is technically\nbeneficial (as recent research in this area suggests that it is), it may not be\nprofitable. In this paper, both the technical and economic implications of\nresource sharing in millimeter wave networks are studied. Millimeter wave\nservice is considered in the economic framework of a network good, where\nconsumers' utility depends on the size of the network, and the strategic\ndecisions of consumers and service providers are connected to detailed network\nsimulations. The results suggest that \"open\" deployments of neutral small cells\nthat serve subscribers of any service provider encourage market entry by making\nit easier for networks to reach critical mass, more than \"open\" (unlicensed)\nspectrum would. The conditions under which competitive service providers would\nprefer to share resources or not are also described. \n\n"}
{"id": "1605.05227", "contents": "Title: Revisiting XOR-based Network Coding for Energy Efficient Broadcasting in\n  Mobile Ad Hoc Networks Abstract: Network coding is commonly used to improve the energy efficiency of\nnetwork-wide broadcasting in wireless multi-hop networks. In this work, we\nfocus on XOR-based broadcasting in mobile ad hoc networks with multiple\nsources. We make the observation that the common approach, which is to benefit\nfrom the synergy of XOR network coding with a CDS-based broadcast algorithm,\nsuffers performance breakdowns. After delving into the details of this synergy,\nwe attribute this behavior to an important mechanism of the underlying\nbroadcast algorithm, known as the \"termination criterion\". To tackle the\nproblem, we propose a termination criterion that is fully compatible with XOR\ncoding. In addition to that, we revisit the internals of XOR coding. We first\nenhance the synergy of XOR coding with the underlying broadcast algorithm by\nallowing each mechanism to benefit from information available by the other. In\nthis way, we manage to improve the pruning efficiency of the CDS-based\nalgorithm while at the same time we come up with a method for detecting coding\nopportunities that has minimal storage and processing requirements compared to\ncurrent approaches. Then, for the first time, we use XOR coding as a mechanism\nnot only for enhancing energy efficiency but also for reducing the\nend-to-end-delay. We validate the effectiveness of our proposed algorithm\nthrough extensive simulations on a diverse set of scenarios. \n\n"}
{"id": "1605.05553", "contents": "Title: Does the measurement take place when nobody observes it? Abstract: We consider {\\em non-selective} continuous measurements of a particle\ntunneling to a reservoir of finite band-width ($\\Lambda$). The particle is\ncontinuously monitored by frequent projective measurements (\"quantum\ntrajectory\"), separated by a time-interval $\\tau$. A simple analytical\nexpression for the decay rate has been obtained. For Markovian reservoirs\n($\\Lambda\\to\\infty$), no effect of the measurements is found. Otherwise for a\nfinite $\\Lambda$, the decay rate always depends on the measurement time $\\tau$.\nThis result is compared with alternative calculations, with no intermediate\nmeasurements, but when the measurement device is included in the Schr\\\"odinger\nevolution. We found that the detector affects the system by the decoherence\nrate ($\\Gamma_d$), related to the detector's signal. Although both treatments\nare different, the final results become very close for $\\tau=2/\\Gamma_d$. This\n$\\tau$ corresponds to the minimal time for which the detector's signal can be\ndistinguished by an \"observer\". This indicates a fundamental role of\ninformation in quantum motion and can be used for the extension of the quantum\ntrajectory method for non-Markovian environments. \n\n"}
{"id": "1605.06459", "contents": "Title: Two-Qubit Separability Probabilities as Joint Functions of the Bloch\n  Radii of the Qubit Subsystems Abstract: We detect a certain pattern of behavior of separability probabilities\n$p(r_A,r_B)$ for two-qubit systems endowed with Hilbert-Schmidt, and more\ngenerally, random induced measures, where $r_A$ and $r_B$ are the Bloch radii\n($0 \\leq r_A,r_B \\leq 1$) of the qubit reduced states ($A,B$). We observe a\nrelative repulsion of radii effect, that is $p(r_A,r_A) < p(r_A,1-r_A)$, except\nfor rather narrow \"crossover\" intervals $[\\tilde{r}_A,\\frac{1}{2}]$. Among the\nseven specific cases we study are, firstly, the \"toy\" seven-dimensional\n$X$-states model and, then, the fifteen-dimensional two-qubit states obtained\nby tracing over the pure states in $4 \\times K$-dimensions, for $K=3, 4, 5$,\nwith $K=4$ corresponding to Hilbert-Schmidt (flat/Euclidean) measure. We also\nexamine the real (two-rebit) $K=4$, the $X$-states $K=5$, and Bures (minimal\nmonotone)--for which no nontrivial crossover behavior is observed--instances.\nIn the two $X$-states cases, we derive analytical results, for $K=3, 4$, we\npropose formulas that well-fit our numerical results, and for the other\nscenarios, rely presently upon large numerical analyses. The separability\nprobability crossover regions found expand in length (lower $\\tilde{r}_A$) as\n$K$ increases. This report continues our efforts (arXiv:1506.08739) to extend\nthe recent work of Milz and Strunz (J. Phys. A: 48 [2015] 035306) from a\nunivariate ($r_A$) framework---in which they found separability probabilities\nto hold constant with $r_A$---to a bivariate ($r_A,r_B$) one. We also analyze\nthe two-qutrit and qubit-qutrit counterparts reported in arXiv:1512.07210 in\nthis context, and study two-qubit separability probabilities of the form\n$p(r_A,\\frac{1}{2})$. \n\n"}
{"id": "1605.06884", "contents": "Title: Mobile Cloud Computing with a UAV-Mounted Cloudlet: Optimal Bit\n  Allocation for Communication and Computation Abstract: Mobile cloud computing relieves the tension between compute-intensive mobile\napplications and battery-constrained mobile devices by enabling the offloading\nof computing tasks from mobiles to a remote processors. This paper considers a\nmobile cloud computing scenario in which the \"cloudlet\" processor that provides\noffloading opportunities to mobile devices is mounted on unmanned aerial\nvehicles (UAVs) to enhance coverage. Focusing on a slotted communication system\nwith frequency division multiplexing between mobile and UAV, the joint\noptimization of the number of input bits transmitted in the uplink by the\nmobile to the UAV, the number of input bits processed by the cloudlet at the\nUAV, and the number of output bits returned by the cloudlet to the mobile in\nthe downlink in each slot is carried out by means of dual decomposition under\nmaximum latency constraints with the aim of minimizing the mobile energy\nconsumption. Numerical results reveal the critical importance of an optimized\nbit allocation in order to enable significant energy savings as compared to\nlocal mobile execution for stringent latency constraints. \n\n"}
{"id": "1605.06898", "contents": "Title: Social and Spatial Clustering of People at Humanity's Largest Gathering Abstract: Macroscopic behavior of scientific and societal systems results from the\naggregation of microscopic behaviors of their constituent elements, but\nconnecting the macroscopic with the microscopic in human behavior has\ntraditionally been difficult. Manifestations of homophily, the notion that\nindividuals tend to interact with others who resemble them, have been observed\nin many small and intermediate size settings. However, whether this behavior\ntranslates to truly macroscopic levels, and what its consequences may be,\nremains unknown. Here, we use call detail records (CDRs) to examine the\npopulation dynamics and manifestations of social and spatial homophily at a\nmacroscopic level among the residents of 23 states of India at the Kumbh Mela,\na 3-month-long Hindu festival. We estimate that the festival was attended by 61\nmillion people, making it the largest gathering in the history of humanity.\nWhile we find strong overall evidence for both types of homophily for residents\nof different states, participants from low-representation states show\nconsiderably stronger propensity for both social and spatial homophily than\nthose from high-representation states. These manifestations of homophily are\namplified on crowded days, such as the peak day of the festival, which we\nestimate was attended by 25 million people. Our findings confirm that\nhomophily, which here likely arises from social influence, permeates all scales\nof human behavior. \n\n"}
{"id": "1605.08759", "contents": "Title: Estimating Large Correlation Matrices for International Migration Abstract: The United Nations is the major organization producing and regularly updating\nprobabilistic population projections for all countries. International migration\nis a critical component of such projections, and between-country correlations\nare important for forecasts of regional aggregates. However, there are 200\ncountries and only 12 data points, each one corresponding to a five-year time\nperiod. Thus a $200 \\times 200$ correlation matrix must be estimated on the\nbasis of 12 data points. Using Pearson correlations produces many spurious\ncorrelations. We propose a maximum a posteriori estimator for the correlation\nmatrix with an interpretable informative prior distribution. The prior serves\nto regularize the correlation matrix, shrinking a priori untrustworthy elements\ntowards zero. Our estimated correlation structure improves projections of net\nmigration for regional aggregates, producing narrower projections of migration\nfor Africa as a whole and wider projections for Europe. A simulation study\nconfirms that our estimator outperforms both the Pearson correlation matrix and\na simple shrinkage estimator when estimating a sparse correlation matrix. \n\n"}
{"id": "1605.09171", "contents": "Title: Randomization and The Pernicious Effects of Limited Budgets on Auction\n  Experiments Abstract: Buyers (e.g., advertisers) often have limited financial and processing\nresources, and so their participation in auctions is throttled. Changes to\nauctions may affect bids or throttling and any change may affect what winners\npay. This paper shows that if an A/B experiment affects only bids, then the\nobserved treatment effect is unbiased when all the bidders in an auction are\nrandomly assigned to A or B but it can be severely biased otherwise, even in\nthe absence of throttling. Experiments that affect throttling algorithms can\nalso be badly biased, but the bias can be substantially reduced if the budget\nfor each advertiser in the experiment is allocated to separate pots for the A\nand B arms of the experiment. \n\n"}
{"id": "1606.00252", "contents": "Title: Testing High Dimensional Covariance Matrices, with Application to\n  Detecting Schizophrenia Risk Genes Abstract: Scientists routinely compare gene expression levels in cases versus controls\nin part to determine genes associated with a disease. Similarly, detecting\ncase-control differences in co-expression among genes can be critical to\nunderstanding complex human diseases; however statistical methods have been\nlimited by the high dimensional nature of this problem. In this paper, we\nconstruct a sparse-Leading-Eigenvalue-Driven (sLED) test for comparing two\nhigh-dimensional covariance matrices. By focusing on the spectrum of the\ndifferential matrix, sLED provides a novel perspective that accommodates what\nwe assume to be common, namely sparse and weak signals in gene expression data,\nand it is closely related with Sparse Principal Component Analysis. We prove\nthat sLED achieves full power asymptotically under mild assumptions, and\nsimulation studies verify that it outperforms other existing procedures under\nmany biologically plausible scenarios. Applying sLED to the largest\ngene-expression dataset obtained from post-mortem brain tissue from\nSchizophrenia patients and controls, we provide a novel list of genes\nimplicated in Schizophrenia and reveal intriguing patterns in gene\nco-expression change for Schizophrenia subjects. We also illustrate that sLED\ncan be generalized to compare other gene-gene \"relationship\" matrices that are\nof practical interest, such as the weighted adjacency matrices. \n\n"}
{"id": "1606.01855", "contents": "Title: Bayesian Poisson Tucker Decomposition for Learning the Structure of\n  International Relations Abstract: We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling\ncountry--country interaction event data. These data consist of interaction\nevents of the form \"country $i$ took action $a$ toward country $j$ at time\n$t$.\" BPTD discovers overlapping country--community memberships, including the\nnumber of latent communities. In addition, it discovers directed\ncommunity--community interaction networks that are specific to \"topics\" of\naction types and temporal \"regimes.\" We show that BPTD yields an efficient MCMC\ninference algorithm and achieves better predictive performance than related\nmodels. We also demonstrate that it discovers interpretable latent structure\nthat agrees with our knowledge of international relations. \n\n"}
{"id": "1606.02074", "contents": "Title: Application of the Signature Method to Pattern Recognition in the CEQUEL\n  Clinical Trial Abstract: The classification procedure of streaming data usually requires various ad\nhoc methods or particular heuristic models. We explore a novel non-parametric\nand systematic approach to analysis of heterogeneous sequential data. We\ndemonstrate an application of this method to classification of the delays in\nresponding to the prompts, from subjects with bipolar disorder collected during\na clinical trial, using both synthetic and real examples. We show how this\nmethod can provide a natural and systematic way to extract characteristic\nfeatures from sequential data. \n\n"}
{"id": "1606.03295", "contents": "Title: Simultaneous inference for misaligned multivariate functional data Abstract: We consider inference for misaligned multivariate functional data that\nrepresents the same underlying curve, but where the functional samples have\nsystematic differences in shape. In this paper we introduce a new class of\ngenerally applicable models where warping effects are modeled through nonlinear\ntransformation of latent Gaussian variables and systematic shape differences\nare modeled by Gaussian processes. To model cross-covariance between sample\ncoordinates we introduce a class of low-dimensional cross-covariance structures\nsuitable for modeling multivariate functional data. We present a method for\ndoing maximum-likelihood estimation in the models and apply the method to three\ndata sets. The first data set is from a motion tracking system where the\nspatial positions of a large number of body-markers are tracked in\nthree-dimensions over time. The second data set consists of height and weight\nmeasurements for Danish boys. The third data set consists of three-dimensional\nspatial hand paths from a controlled obstacle-avoidance experiment. We use the\ndeveloped method to estimate the cross-covariance structure, and use a\nclassification setup to demonstrate that the method outperforms\nstate-of-the-art methods for handling misaligned curve data. \n\n"}
{"id": "1606.04153", "contents": "Title: Universal temporal features of rankings in competitive sports and games Abstract: Many complex phenomena, from the selection of traits in biological systems to\nhierarchy formation in social and economic entities, show signs of competition\nand heterogeneous performance in the temporal evolution of their components,\nwhich may eventually lead to stratified structures such as the wealth\ndistribution worldwide. However, it is still unclear whether the road to\nhierarchical complexity is determined by the particularities of each phenomena,\nor if there are universal mechanisms of stratification common to many systems.\nHuman sports and games, with their (varied but simplified) rules of competition\nand measures of performance, serve as an ideal test bed to look for universal\nfeatures of hierarchy formation. With this goal in mind, we analyse here the\nbehaviour of players and team rankings over time for several sports and games.\nEven though, for a given time, the distribution of performance ranks varies\nacross activities, we find statistical regularities in the dynamics of ranks.\nSpecifically the rank diversity, a measure of the number of elements occupying\na given rank over a length of time, has the same functional form in sports and\ngames as in languages, another system where competition is determined by the\nuse or disuse of grammatical structures. Our results support the notion that\nhierarchical phenomena may be driven by the same underlying mechanisms of rank\nformation, regardless of the nature of their components. Moreover, such\nregularities can in principle be used to predict lifetimes of rank occupancy,\nthus increasing our ability to forecast stratification in the presence of\ncompetition. \n\n"}
{"id": "1606.04896", "contents": "Title: Using instrumental variables to disentangle treatment and placebo\n  effects in blinded and unblinded randomized clinical trials influenced by\n  unmeasured confounders Abstract: Clinical trials traditionally employ blinding as a design mechanism to reduce\nthe influence of placebo effects. In practice, however, it can be difficult or\nimpossible to blind study participants and unblinded trials are common in\nmedical research. Here we show how instrumental variables can be used to\nquantify and disentangle treatment and placebo effects in randomized clinical\ntrials comparing control and active treatments in the presence of confounders.\nThe key idea is to use randomization to separately manipulate treatment\nassignment and psychological encouragement messages that increase the\nparticipants' desire for improved symptoms. The proposed approach is able to\nimprove the estimation of treatment effects in blinded studies and, most\nimportantly, opens the doors to account for placebo effects in unblinded\ntrials. \n\n"}
{"id": "1606.05067", "contents": "Title: Mortality and life expectancy forecasting for a group of populations in\n  developed countries: A multilevel functional data method Abstract: A multilevel functional data method is adapted for forecasting age-specific\nmortality for two or more populations in developed countries with high-quality\nvital registration systems. It uses multilevel functional principal component\nanalysis of aggregate and population-specific data to extract the common trend\nand population-specific residual trend among populations. If the forecasts of\npopulation-specific residual trends do not show a long-term trend, then\nconvergence in forecasts may be achieved. This method is first applied to age-\nand sex-specific data for the United Kingdom, and its forecast accuracy is then\nfurther compared with several existing methods, including independent\nfunctional data and product-ratio methods, through a multi-country comparison.\nThe proposed method is also demonstrated by age-, sex- and state-specific data\nin Australia, where the convergence in forecasts can possibly be achieved by\nsex and state. For forecasting age-specific mortality, the multilevel\nfunctional data method is more accurate than the other coherent methods\nconsidered. For forecasting female life expectancy at birth, the multilevel\nfunctional data method is outperformed by the Bayesian method of \\cite{RLG14}.\nFor forecasting male life expectancy at birth, the multilevel functional data\nmethod performs better than the Bayesian methods in terms of point forecasts,\nbut less well in terms of interval forecasts. Supplementary materials for this\narticle are available online. \n\n"}
{"id": "1606.05196", "contents": "Title: A Goldilocks principle for modeling radial velocity noise Abstract: The doppler measurements of stars are diluted and distorted by stellar\nactivity noise. Different choices of noise models and statistical methods have\nled to much controversy in the confirmation of exoplanet candidates obtained\nthrough analysing radial velocity data. To quantify the limitation of various\nmodels and methods, we compare different noise models and signal detection\ncriteria for various simulated and real data sets in the Bayesian framework.\nAccording to our analyses, the white noise model tend to interpret noise as\nsignal, leading to false positives. On the other hand, the red noise models are\nlikely to interprete signal as noise, resulting in false negatives. We find\nthat the Bayesian information criterion combined with a Bayes factor threshold\nof 150 can efficiently rule out false positives and confirm true detections. We\nfurther propose a Goldilocks principle aimed at modeling radial velocity noise\nto avoid too many false positives and too many false negatives. We propose that\nthe noise model with RHK-dependent jitter is used in combination with the\nmoving average model to detect planetary signals for M dwarfs. Our work may\nalso shed light on the noise modeling for hotter stars, and provide a valid\napproach for finding similar principles in other disciplines. \n\n"}
{"id": "1606.05771", "contents": "Title: Brief Report on Estimating Regularized Gaussian Networks from Continuous\n  and Ordinal Data Abstract: In recent literature, the Gaussian Graphical model (GGM; Lauritzen, 1996),a\nnetwork of partial correlation coefficients, has been used to capture potential\ndynamic relationships between observed variables. The GGM can be estimated\nusing regularization in combination with model selection using the extended\nBayesian Information Criterion (Foygel and Drton, 2010). I term this\nmethodology GeLasso, and asses its performance using a plausible psychological\nnetwork structure with both continuous and ordinal datasets.Simulation results\nindicate that GeLasso works well as an out-of-the-box method to estimate\nnetwork structures. \n\n"}
{"id": "1606.05825", "contents": "Title: Wireless network signals with moderately correlated shadowing still\n  appear Poisson Abstract: We consider the point process of signal strengths emitted from transmitters\nin a wireless network and observed at a fixed position. In our model,\ntransmitters are placed deterministically or randomly according to a hard core\nor Poisson point process and signals are subjected to power law path loss and\nrandom propagation effects that may be correlated between transmitters.\n  We provide bounds on the distance between the point process of signal\nstrengths and a Poisson process with the same mean measure, assuming correlated\nlog-normal shadowing. For \"strong shadowing\" and moderate correlations, we find\nthat the signal strengths are close to a Poisson process, generalizing a\nrecently shown analogous result for independent shadowing. \n\n"}
{"id": "1606.06328", "contents": "Title: Inferring Mobility Measures from GPS Traces with Missing Data Abstract: With increasing availability of smartphones with GPS capabilities,\nlarge-scale studies relating individual-level mobility patterns to a wide\nvariety of patient-centered outcomes, from mood disorders to surgical recovery,\nare becoming a reality. Similar past studies have been small in scale and have\nprovided wearable GPS devices to subjects. These devices typically collect\nmobility traces continuously without significant gaps in the data, and\nconsequently the problem of data missingness has been safely ignored.\nLeveraging subjects' own smartphones makes it possible to scale up and extend\nthe duration of these types of studies, but at the same time introduces a\nsubstantial challenge: to preserve a smartphone's battery, GPS can be active\nonly for a small portion of the time, frequently less than $10\\%$, leading to a\ntremendous missing data problem. We introduce a principled statistical\napproach, based on weighted resampling of the observed data, to impute the\nmissing mobility traces, which we then summarize using different mobility\nmeasures. We compare the strengths of our approach to linear interpolation, a\npopular approach for dealing with missing data, both analytically and through\nsimulation of missingness for empirical data. We conclude that our imputation\napproach better mirrors human mobility both theoretically and over a sample of\nGPS mobility traces from 182 individuals in the Geolife data set, where,\nrelative to linear interpolation, imputation resulted in a 10-fold reduction in\nthe error averaged across all mobility features. \n\n"}
{"id": "1606.07198", "contents": "Title: On Improving Capacity of Full-Duplex Small Cells with D2D Abstract: The recent developments in full duplex (FD) communication promise doubling\nthe capacity of cellular networks using self interference cancellation (SIC)\ntechniques. FD small cells with device-to-device (D2D) communication links\ncould achieve the expected capacity of the future cellular networks (5G). In\nthis work, we consider joint scheduling and dynamic power algorithm (DPA) for a\nsingle cell FD small cell network with D2D links (D2DLs). We formulate the\noptimal user selection and power control as a non-linear programming (NLP)\noptimization problem to get the optimal user scheduling and transmission power\nin a given TTI. Our numerical results show that using DPA gives better overall\nthroughput performance than full power transmission algorithm (FPA). Also,\nsimultaneous transmissions (combination of uplink (UL), downlink (DL), and D2D\noccur 80% of the time thereby increasing the spectral efficiency and network\ncapacity. \n\n"}
{"id": "1606.07986", "contents": "Title: Flexible discrete space models of animal movement Abstract: Movement drives the spread of infectious disease, gene flow, and other\ncritical ecological processes. To study these processes we need models for\nmovement that capture complex behavior that changes over time and space in\nresponse to biotic and abiotic factors. Penalized likelihood approaches, such\nas penalized semiparametric spline expansions and LASSO regression, allow\ninference on complex models without overfitting. Continuous-time Markov chains\n(CTMCs) have been recently introduced as a flexible discrete-space model for\nanimal movement. Modeling with CTMCs involves discretizing an animal's path to\nthe resolution of a raster grid. The resulting stochastic process model can\neasily incorporate environmental and other covariates, represented as raster\nlayers, that affect directional bias and overall movement rate. We introduce a\nweighted likelihood approach that allows for modeling movement using CTMCs,\nwith path uncertainty due to missing data modeled by imputing continuous-time\npaths between telemetry locations. The framework we introduce allows for\ninference on CTMC movement models using existing software for fitting Poisson\nregression models, including penalized versions of Poisson regression. The\nresult is a flexible, powerful, and accessible framework for modeling a wide\nrange of animal movement behavior. \n\n"}
{"id": "1606.08350", "contents": "Title: An Efficient Implementation of the Generalized Labeled Multi-Bernoulli\n  Filter Abstract: This paper proposes an efficient implementation of the generalized labeled\nmulti-Bernoulli (GLMB) filter by combining the prediction and update into a\nsingle step. In contrast to an earlier implementation that involves separate\ntruncations in the prediction and update steps, the proposed implementation\nrequires only one truncation procedure for each iteration. Furthermore, we\npropose an efficient algorithm for truncating the GLMB filtering density based\non Gibbs sampling. The resulting implementation has a linear complexity in the\nnumber of measurements and quadratic in the number of hypothesized objects. \n\n"}
{"id": "1607.02633", "contents": "Title: Bayesian inference for stochastic differential equation mixed effects\n  models of a tumor xenography study Abstract: We consider Bayesian inference for stochastic differential equation mixed\neffects models (SDEMEMs) exemplifying tumor response to treatment and regrowth\nin mice. We produce an extensive study on how a SDEMEM can be fitted using both\nexact inference based on pseudo-marginal MCMC and approximate inference via\nBayesian synthetic likelihoods (BSL). We investigate a two-compartments SDEMEM,\nthese corresponding to the fractions of tumor cells killed by and survived to a\ntreatment, respectively. Case study data considers a tumor xenography study\nwith two treatment groups and one control, each containing 5-8 mice. Results\nfrom the case study and from simulations indicate that the SDEMEM is able to\nreproduce the observed growth patterns and that BSL is a robust tool for\ninference in SDEMEMs. Finally, we compare the fit of the SDEMEM to a similar\nordinary differential equation model. Due to small sample sizes, strong prior\ninformation is needed to identify all model parameters in the SDEMEM and it\ncannot be determined which of the two models is the better in terms of\npredicting tumor growth curves. In a simulation study we find that with a\nsample of 17 mice per group BSL is able to identify all model parameters and\ndistinguish treatment groups. \n\n"}
{"id": "1607.03592", "contents": "Title: Cluster Sampling Filters for Non-Gaussian Data Assimilation Abstract: This paper presents a fully non-Gaussian version of the Hamiltonian Monte\nCarlo (HMC) sampling filter. The Gaussian prior assumption in the original HMC\nfilter is relaxed. Specifically, a clustering step is introduced after the\nforecast phase of the filter, and the prior density function is estimated by\nfitting a Gaussian Mixture Model (GMM) to the prior ensemble. Using the data\nlikelihood function, the posterior density is then formulated as a mixture\ndensity, and is sampled using a HMC approach (or any other scheme capable of\nsampling multimodal densities in high-dimensional subspaces). The main filter\ndeveloped herein is named \"cluster HMC sampling filter\" (ClHMC). A multi-chain\nversion of the ClHMC filter, namely MC-ClHMC is also proposed to guarantee that\nsamples are taken from the vicinities of all probability modes of the\nformulated posterior. The new methodologies are tested using a\nquasi-geostrophic (QG) model with double-gyre wind forcing and bi-harmonic\nfriction. Numerical results demonstrate the usefulness of using GMMs to relax\nthe Gaussian prior assumption in the HMC filtering paradigm. \n\n"}
{"id": "1607.04532", "contents": "Title: Should I stay or should I go? A latent threshold approach to large-scale\n  mixture innovation models Abstract: This paper proposes a straightforward algorithm to carry out inference in\nlarge time-varying parameter vector autoregressions (TVP-VARs) with mixture\ninnovation components for each coefficient in the system. We significantly\ndecrease the computational burden by approximating the latent indicators that\ndrive the time-variation in the coefficients with a latent threshold process\nthat depends on the absolute size of the shocks. The merits of our approach are\nillustrated with two applications. First, we forecast the US term structure of\ninterest rates and demonstrate forecast gains of the proposed mixture\ninnovation model relative to other benchmark models. Second, we apply our\napproach to US macroeconomic data and find significant evidence for\ntime-varying effects of a monetary policy tightening. \n\n"}
{"id": "1607.05376", "contents": "Title: The Problem of Infra-marginality in Outcome Tests for Discrimination Abstract: Outcome tests are a popular method for detecting bias in lending, hiring, and\npolicing decisions. These tests operate by comparing the success rate of\ndecisions across groups. For example, if loans made to minority applicants are\nobserved to be repaid more often than loans made to whites, it suggests that\nonly exceptionally qualified minorities are granted loans, indicating\ndiscrimination. Outcome tests, however, are known to suffer from the problem of\ninfra-marginality: even absent discrimination, the repayment rates for minority\nand white loan recipients might differ if the two groups have different risk\ndistributions. Thus, at least in theory, outcome tests can fail to accurately\ndetect discrimination. We develop a new statistical test of\ndiscrimination---the threshold test---that mitigates the problem of\ninfra-marginality by jointly estimating decision thresholds and risk\ndistributions via a hierarchical Bayesian latent variable model. Applying our\ntest to a dataset of 4.5 million police stops in North Carolina, we find that\nthe problem of infra-marginality is more than a theoretical possibility, and\ncan cause the outcome test to yield misleading results in practice. \n\n"}
{"id": "1607.06635", "contents": "Title: Density Estimation Trees as fast non-parametric modelling tools Abstract: Density Estimation Trees (DETs) are decision trees trained on a multivariate\ndataset to estimate its probability density function. While not competitive\nwith kernel techniques in terms of accuracy, they are incredibly fast,\nembarrassingly parallel and relatively small when stored to disk. These\nproperties make DETs appealing in the resource-expensive horizon of the LHC\ndata analysis. Possible applications may include selection optimization, fast\nsimulation and fast detector calibration. In this contribution I describe the\nalgorithm, made available to the HEP community in a RooFit implementation. A\nset of applications under discussion within the LHCb Collaboration are also\nbriefly illustrated. \n\n"}
{"id": "1607.08458", "contents": "Title: The iterative reweighted Mixed-Norm Estimate for spatio-temporal MEG/EEG\n  source reconstruction Abstract: Source imaging based on magnetoencephalography (MEG) and\nelectroencephalography (EEG) allows for the non-invasive analysis of brain\nactivity with high temporal and good spatial resolution. As the\nbioelectromagnetic inverse problem is ill-posed, constraints are required. For\nthe analysis of evoked brain activity, spatial sparsity of the neuronal\nactivation is a common assumption. It is often taken into account using convex\nconstraints based on the l1-norm. The resulting source estimates are however\nbiased in amplitude and often suboptimal in terms of source selection due to\nhigh correlations in the forward model. In this work, we demonstrate that an\ninverse solver based on a block-separable penalty with a Frobenius norm per\nblock and a l0.5-quasinorm over blocks addresses both of these issues. For\nsolving the resulting non-convex optimization problem, we propose the iterative\nreweighted Mixed Norm Estimate (irMxNE), an optimization scheme based on\niterative reweighted convex surrogate optimization problems, which are solved\nefficiently using a block coordinate descent scheme and an active set strategy.\nWe compare the proposed sparse imaging method to the dSPM and the RAP-MUSIC\napproach based on two MEG data sets. We provide empirical evidence based on\nsimulations and analysis of MEG data that the proposed method improves on the\nstandard Mixed Norm Estimate (MxNE) in terms of amplitude bias, support\nrecovery, and stability. \n\n"}
{"id": "1608.03532", "contents": "Title: QPass: a Merit-based Evaluation of Soccer Passes Abstract: Quantitative analysis of soccer players' passing ability focuses on\ndescriptive statistics without considering the players' real contribution to\nthe passing and ball possession strategy of their team. Which player is able to\nhelp the build-up of an attack, or to maintain the possession of the ball? We\nintroduce a novel methodology called QPass to answer questions like these\nquantitatively. Based on the analysis of an entire season, we rank the players\nbased on the intrinsic value of their passes using QPass. We derive an album of\npass trajectories for different gaming styles. Our methodology reveals a quite\ncounterintuitive paradigm: losing the ball possession could lead to better\nchances to win a game. \n\n"}
{"id": "1608.03787", "contents": "Title: Non-stationary Gaussian models with physical barriers Abstract: The classical tools in spatial statistics are stationary models, like the\nMat\\'ern field. However, in some applications there are boundaries, holes, or\nphysical barriers in the study area, e.g. a coastline, and stationary models\nwill inappropriately smooth over these features, requiring the use of a\nnon-stationary model.\n  We propose a new model, the Barrier model, which is different from the\nestablished methods as it is not based on the shortest distance around the\nphysical barrier, nor on boundary conditions. The Barrier model is based on\nviewing the Mat\\'ern correlation, not as a correlation function on the shortest\ndistance between two points, but as a collection of paths through a\nSimultaneous Autoregressive (SAR) model. We then manipulate these local\ndependencies to cut off paths that are crossing the physical barriers. To make\nthe new SAR well behaved, we formulate it as a stochastic partial differential\nequation (SPDE) that can be discretised to represent the Gaussian field, with a\nsparse precision matrix that is automatically positive definite.\n  The main advantage with the Barrier model is that the computational cost is\nthe same as for the stationary model. The model is easy to use, and can deal\nwith both sparse data and very complex barriers, as shown in an application in\nthe Finnish Archipelago Sea. Additionally, the Barrier model is better at\nreconstructing the modified Horseshoe test function than the standard models\nused in R-INLA. \n\n"}
{"id": "1608.05292", "contents": "Title: Efficient real-time monitoring of an emerging influenza epidemic: how\n  feasible? Abstract: A prompt public health response to a new epidemic relies on the ability to\nmonitor and predict its evolution in real time as data accumulate. The 2009\nA/H1N1 outbreak in the UK revealed pandemic data as noisy, contaminated,\npotentially biased, and originating from multiple sources. This seriously\nchallenges the capacity for real-time monitoring. Here we assess the\nfeasibility of real-time inference based on such data by constructing an\nanalytic tool combining an age-stratified SEIR transmission model with various\nobservation models describing the data generation mechanisms. As batches of\ndata become available, a sequential Monte Carlo (SMC) algorithm is developed to\nsynthesise multiple imperfect data streams, iterate epidemic inferences and\nassess model adequacy amidst a rapidly evolving epidemic environment,\nsubstantially reducing computation time in comparison to standard MCMC, to\nensure timely delivery of real-time epidemic assessments. In application to\nsimulated data designed to mimic the 2009 A/H1N1 epidemic, SMC is shown to have\nadditional benefits in terms of assessing predictive performance and coping\nwith parameter non-identifiability. \n\n"}
{"id": "1608.05498", "contents": "Title: Elicitability and backtesting: Perspectives for banking regulation Abstract: Conditional forecasts of risk measures play an important role in internal\nrisk management of financial institutions as well as in regulatory capital\ncalculations. In order to assess forecasting performance of a risk measurement\nprocedure, risk measure forecasts are compared to the realized financial losses\nover a period of time and a statistical test of correctness of the procedure is\nconducted. This process is known as backtesting. Such traditional backtests are\nconcerned with assessing some optimality property of a set of risk measure\nestimates. However, they are not suited to compare different risk estimation\nprocedures. We investigate the proposal of comparative backtests, which are\nbetter suited for method comparisons on the basis of forecasting accuracy, but\nnecessitate an elicitable risk measure. We argue that supplementing traditional\nbacktests with comparative backtests will enhance the existing trading book\nregulatory framework for banks by providing the correct incentive for accuracy\nof risk measure forecasts. In addition, the comparative backtesting framework\ncould be used by banks internally as well as by researchers to guide selection\nof forecasting methods. The discussion focuses on three risk measures,\nValue-at-Risk, expected shortfall and expectiles, and is supported by a\nsimulation study and data analysis. \n\n"}
{"id": "1608.05655", "contents": "Title: Nonstationary Spatial Prediction of Soil Organic Carbon: Implications\n  for Stock Assessment Decision Making Abstract: The Rapid Carbon Assessment (RaCA) project was conducted by the US Department\nof Agriculture's National Resources Conservation Service between 2010-2012 in\norder to provide contemporaneous measurements of soil organic carbon (SOC)\nacross the US. Despite the broad extent of the RaCA data collection effort,\ndirect observations of SOC are not available at the high spatial resolution\nneeded for studying carbon storage in soil and its implications for important\nproblems in climate science and agriculture. As a result, there is a need for\npredicting SOC at spatial locations not included as part of the RaCA project.\nIn this paper, we compare spatial prediction of SOC using a subset of the RaCA\ndata for a variety of statistical methods. We investigate the performance of\nmethods with off-the-shelf software available (both stationary and\nnonstationary) as well as a novel nonstationary approach based on partitioning\nrelevant spatially-varying covariate processes. Our new method addresses open\nquestions regarding (1) how to partition the spatial domain for\nsegmentation-based nonstationary methods, (2) incorporating partially observed\ncovariates into a spatial model, and (3) accounting for uncertainty in the\npartitioning. In applying the various statistical methods we find that there\nare minimal differences in out-of-sample criteria for this particular data set,\nhowever, there are major differences in maps of uncertainty in SOC predictions.\nWe argue that the spatially-varying measures of prediction uncertainty produced\nby our new approach are valuable to decision makers, as they can be used to\nbetter benchmark mechanistic models, identify target areas for soil restoration\nprojects, and inform carbon sequestration projects. \n\n"}
{"id": "1608.05783", "contents": "Title: Non-Orthogonal Multiple Access (NOMA) in Cellular Uplink and Downlink:\n  Challenges and Enabling Techniques Abstract: By combining the concepts of superposition coding at the transmitter(s) and\nsuccessive interference cancellation (SIC) at the receiver(s), non-orthogonal\nmultiple access (NOMA) has recently emerged as a promising multiple access\ntechnique for 5G wireless technology. In this article, we first discuss the\nfundamentals of uplink and downlink NOMA transmissions and outline their key\ndistinctions (in terms of implementation complexity, detection and decoding at\nthe SIC receiver(s), incurred intra-cell and inter-cell interferences). Later,\nfor both downlink and uplink NOMA, we theoretically derive the NOMA dominant\ncondition for each individual user in a two-user NOMA cluster. NOMA dominant\ncondition refers to the condition under which the spectral efficiency gains of\nNOMA are guaranteed compared to conventional orthogonal multiple access (OMA).\nThe derived conditions provide direct insights on selecting appropriate users\nin two-user NOMA clusters. The conditions are distinct for uplink and downlink\nas well as for each individual user. Numerical results show the significance of\nthe derived conditions for the user selection in uplink/downlink NOMA clusters\nand provide a comparison to the random user selection. A brief overview of the\nrecent research investigations is then provided to highlight the existing\nresearch gaps. Finally, we discuss the potential applications and key\nchallenges of NOMA transmissions. \n\n"}
{"id": "1608.07029", "contents": "Title: Functional time series forecasting with dynamic updating: An application\n  to intraday particulate matter concentration Abstract: Environmental data often take the form of a collection of curves observed\nsequentially over time. An example of this includes daily pollution measurement\ncurves describing the concentration of a particulate matter in ambient air.\nThese curves can be viewed as a time series of functions observed at equally\nspaced intervals over a dense grid. The nature of high-dimensional data poses\nchallenges from a statistical aspect, due to the so-called `curse of\ndimensionality', but it also poses opportunities to analyze a rich source of\ninformation to better understand dynamic changes at short time intervals.\nStatistical methods are introduced and compared for forecasting one-day-ahead\nintraday concentrations of particulate matter; as new data are sequentially\nobserved, dynamic updating methods are proposed to update point and interval\nforecasts to achieve better accuracy. These forecasting methods are validated\nthrough an empirical study of half-hourly concentrations of airborne\nparticulate matter in Graz, Austria. \n\n"}
{"id": "1608.07193", "contents": "Title: Quantile Dependence between Stock Markets and its Application in\n  Volatility Forecasting Abstract: This paper examines quantile dependence between international stock markets\nand evaluates its use for improving volatility forecasting. First, we analyze\nquantile dependence and directional predictability between the US stock market\nand stock markets in the UK, Germany, France and Japan. We use the\ncross-quantilogram, which is a correlation statistic of quantile hit processes.\nThe detailed dependence between stock markets depends on specific quantile\nranges and this dependence is generally asymmetric; the negative spillover\neffect is stronger than the positive spillover effect and there exists strong\ndirectional predictability from the US market to the UK, Germany, France and\nJapan markets. Second, we consider a simple quantile-augmented volatility model\nthat accommodates the quantile dependence and directional predictability\nbetween the US market and these other markets. The quantile-augmented\nvolatility model provides superior in-sample and out-of-sample volatility\nforecasts. \n\n"}
{"id": "1608.08291", "contents": "Title: Gaussian Process Models for Mortality Rates and Improvement Factors Abstract: We develop a Gaussian process (\"GP\") framework for modeling mortality rates\nand mortality improvement factors. GP regression is a nonparametric,\ndata-driven approach for determining the spatial dependence in mortality rates\nand jointly smoothing raw rates across dimensions, such as calendar year and\nage. The GP model quantifies uncertainty associated with smoothed historical\nexperience and generates full stochastic trajectories for out-of-sample\nforecasts. Our framework is well suited for updating projections when newly\navailable data arrives, and for dealing with \"edge\" issues where credibility is\nlower. We present a detailed analysis of Gaussian process model performance for\nUS mortality experience based on the CDC datasets. We investigate the\ninteraction between mean and residual modeling, Bayesian and non-Bayesian GP\nmethodologies, accuracy of in-sample and out-of-sample forecasting, and\nstability of model parameters. We also document the general decline, along with\nstrong age-dependency, in mortality improvement factors over the past few\nyears, contrasting our findings with the Society of Actuaries (\"SOA\") MP-2014\nand -2015 models that do not fully reflect these recent trends. \n\n"}
{"id": "1608.08313", "contents": "Title: Sub-channel Assignment, Power Allocation and User Scheduling for\n  Non-Orthogonal Multiple Access Networks Abstract: In this paper, we study the resource allocation and user scheduling problem\nfor a downlink nonorthogonal multiple access network where the base station\nallocates spectrum and power resources to a set of users. We aim to jointly\noptimize the sub-channel assignment and power allocation to maximize the\nweighted total sum-rate while taking into account user fairness. We formulate\nthe sub-channel allocation problem as equivalent to a many-to-many two-sided\nuser-subchannel matching game in which the set of users and sub-channels are\nconsidered as two sets of players pursuing their own interests. We then propose\na matching algorithm which converges to a two-side exchange stable matching\nafter a limited number of iterations. A joint solution is thus provided to\nsolve the sub-channel assignment and power allocation problems iteratively.\nSimulation results show that the proposed algorithm greatly outperforms the\northogonal multiple access scheme and a previous non-orthogonal multiple access\nscheme. \n\n"}
{"id": "1608.09010", "contents": "Title: Statistical physics of vaccination Abstract: Historically, infectious diseases caused considerable damage to human\nsocieties, and they continue to do so today. To help reduce their impact,\nmathematical models of disease transmission have been studied to help\nunderstand disease dynamics and inform prevention strategies. Vaccination - one\nof the most important preventive measures of modern times - is of great\ninterest both theoretically and empirically. And in contrast to traditional\napproaches, recent research increasingly explores the pivotal implications of\nindividual behavior and heterogeneous contact patterns in populations. Our\nreport reviews the developmental arc of theoretical epidemiology with emphasis\non vaccination, as it led from classical models assuming homogeneously mixing\n(mean-field) populations and ignoring human behavior, to recent models that\naccount for behavioral feedback and/or population spatial/social structure.\nMany of the methods used originated in statistical physics, such as lattice and\nnetwork models, and their associated analytical frameworks. Similarly, the\nfeedback loop between vaccinating behavior and disease propagation forms a\ncoupled nonlinear system with analogs in physics. We also review the new\nparadigm of digital epidemiology, wherein sources of digital data such as\nonline social media are mined for high-resolution information on\nepidemiologically relevant individual behavior. Armed with the tools and\nconcepts of statistical physics, and further assisted by new sources of digital\ndata, models that capture nonlinear interactions between behavior and disease\ndynamics offer a novel way of modeling real-world phenomena, and can help\nimprove health outcomes. We conclude the review by discussing open problems in\nthe field and promising directions for future research. \n\n"}
{"id": "1609.00160", "contents": "Title: Finite-Size Corrections to the Excitation Energy Transfer in a Massless\n  Scalar Interaction Model Abstract: We study the excitation energy transfer (EET) for a simple model in which a\nmassless scalar particle is exchanged between two molecules. We show that a\nfinite-size effect appears in EET by the interaction energy due to overlapping\nof the quantum waves in a short time interval. The effect generates finite-size\ncorrections to Fermi's golden rule and modifies EET probability from the\nstandard formula in the Forster mechanism. The correction terms come from\ntransition modes outside the resonance energy region and enhance EET\nprobability substantially. \n\n"}
{"id": "1609.02354", "contents": "Title: Generalized Autoregressive Score Models in R: The GAS Package Abstract: This paper presents the R package GAS for the analysis of time series under\nthe Generalized Autoregressive Score (GAS) framework of Creal et al. (2013) and\nHarvey (2013). The distinctive feature of the GAS approach is the use of the\nscore function as the driver of time-variation in the parameters of nonlinear\nmodels. The GAS package provides functions to simulate univariate and\nmultivariate GAS processes, estimate the GAS parameters and to make time series\nforecasts. We illustrate the use of the GAS package with a detailed case study\non estimating the time-varying conditional densities of a set of financial\nassets. \n\n"}
{"id": "1609.02411", "contents": "Title: Velocity-Aware Handover Management in Two-Tier Cellular Networks Abstract: While network densification is considered an important solution to cater the\never-increasing capacity demand, its effect on the handover (HO) rate is\noverlooked. In dense 5G networks, HO delays may neutralize or even negate the\ngains offered by network densification. Hence, user mobility imposes a\nnontrivial challenge to harvest capacity gains via network densification. In\nthis paper, we propose a velocity-aware HO management scheme for two-tier\ndownlink cellular network to mitigate the HO effect on the foreseen\ndensification throughput gains. The proposed HO scheme sacrifices the best BS\nconnectivity, by skipping HO to some BSs along the user's trajectory, to\nmaintain longer connection durations and reduce HO rates. Furthermore, the\nproposed scheme enables cooperative BS service and strongest interference\ncancellation to compensate for skipping the best connectivity. To this end, we\nconsider different HO skipping scenarios and develop a velocity-aware\nmathematical model, via stochastic geometry, to quantify the performance of the\nproposed HO scheme in terms of the coverage probability and user throughput.\nThe results highlight the HO rate problem in dense cellular environments and\nshow the importance of the proposed HO schemes. Finally, the value of BS\ncooperation along with handover skipping is quantified for different user\nmobility profiles. \n\n"}
{"id": "1609.02629", "contents": "Title: Inferring social structure from continuous-time interaction data Abstract: Relational event data, which consist of events involving pairs of actors over\ntime, are now commonly available at the finest of temporal resolutions.\nExisting continuous-time methods for modeling such data are based on point\nprocesses and directly model interaction \"contagion,\" whereby one interaction\nincreases the propensity of future interactions among actors, often as dictated\nby some latent variable structure. In this article, we present an alternative\napproach to using temporal-relational point process models for continuous-time\nevent data. We characterize interactions between a pair of actors as either\nspurious or that resulting from an underlying, persistent connection in a\nlatent social network. We argue that consistent deviations from expected\nbehavior, rather than solely high frequency counts, are crucial for identifying\nwell-established underlying social relationships. This study aims to explore\nthese latent network structures in two contexts: one comprising of college\nstudents and another involving barn swallows. \n\n"}
{"id": "1609.02700", "contents": "Title: Efficient batch-sequential Bayesian optimization with moments of\n  truncated Gaussian vectors Abstract: We deal with the efficient parallelization of Bayesian global optimization\nalgorithms, and more specifically of those based on the expected improvement\ncriterion and its variants. A closed form formula relying on multivariate\nGaussian cumulative distribution functions is established for a generalized\nversion of the multipoint expected improvement criterion. In turn, the latter\nrelies on intermediate results that could be of independent interest concerning\nmoments of truncated Gaussian vectors. The obtained expansion of the criterion\nenables studying its differentiability with respect to point batches and\ncalculating the corresponding gradient in closed form. Furthermore , we derive\nfast numerical approximations of this gradient and propose efficient batch\noptimization strategies. Numerical experiments illustrate that the proposed\napproaches enable computational savings of between one and two order of\nmagnitudes, hence enabling derivative-based batch-sequential acquisition\nfunction maximization to become a practically implementable and efficient\nstandard. \n\n"}
{"id": "1609.02938", "contents": "Title: Extract fetal ECG from single-lead abdominal ECG by de-shape short time\n  Fourier transform and nonlocal median Abstract: The multiple fundamental frequency detection problem and the source\nseparation problem from a single-channel signal containing multiple oscillatory\ncomponents and a nonstationary noise are both challenging tasks. To extract the\nfetal electrocardiogram (ECG) from a single-lead maternal abdominal ECG, we\nface both challenges. In this paper, we propose a novel method to extract the\nfetal ECG signal from the single channel maternal abdominal ECG signal, without\nany additional measurement. The algorithm is composed of three main\ningredients. First, the maternal and fetal heart rates are estimated by the\nde-shape short time Fourier transform, which is a recently proposed nonlinear\ntime-frequency analysis technique; second, the beat tracking technique is\napplied to accurately obtain the maternal and fetal R peaks; third, the\nmaternal and fetal ECG waveforms are established by the nonlocal median. The\nalgorithm is evaluated on a simulated fetal ECG signal database ({\\em fecgsyn}\ndatabase), and tested on two real databases with the annotation provided by\nexperts ({\\em adfecgdb} database and {\\em CinC2013} database). In general, the\nalgorithm could be applied to solve other detection and source separation\nproblems, and reconstruct the time-varying wave-shape function of each\noscillatory component. \n\n"}
{"id": "1609.03498", "contents": "Title: LTE in Unlicensed Bands is neither Friend nor Foe to Wi-Fi Abstract: Proponents of deploying LTE in the 5 GHz band for providing additional\ncellular network capacity have claimed that LTE would be a better neighbour to\nWi-Fi in the unlicensed band, than Wi-Fi is to itself. On the other side of the\ndebate, the Wi-Fi community has objected that LTE would be highly detrimental\nto Wi-Fi network performance. However, there is a lack of transparent and\nsystematic engineering evidence supporting the contradicting claims of the two\ncamps, which is essential for ascertaining whether regulatory intervention is\nin fact required to protect the Wi-Fi incumbent from the new LTE entrant. To\nthis end, we present a comprehensive coexistence study of Wi-Fi and\nLTE-in-unlicensed, surveying a large parameter space of coexistence mechanisms\nand a range of representative network densities and deployment scenarios. Our\nresults show that, typically, harmonious coexistence between Wi-Fi and LTE is\nensured by the large number of 5 GHz channels. For the worst-case scenario of\nforced co-channel operation, LTE is sometimes a better neighbour to Wi-Fi -\nwhen effective node density is low - but sometimes worse - when density is\nhigh. We find that distributed interference coordination is only necessary to\nprevent a \"tragedy of the commons\" in regimes where interference is very\nlikely. We also show that in practice it does not make a difference to the\nincumbent what kind of coexistence mechanism is added to LTE-in-unlicensed, as\nlong as one is in place. We therefore conclude that LTE is neither friend nor\nfoe to Wi-Fi in the unlicensed bands in general. We submit that the systematic\nengineering analysis exemplified by our case study is a best-practice approach\nfor supporting evidence-based rulemaking by the regulator. \n\n"}
{"id": "1609.04078", "contents": "Title: Bayesian survival analysis of batsmen in Test cricket Abstract: Cricketing knowledge tells us batting is more difficult early in a player's\ninnings but becomes easier as a player familiarizes themselves with the\nconditions. In this paper, we develop a Bayesian survival analysis method to\npredict the Test Match batting abilities for international cricketers. The\nmodel is applied in two stages, firstly to individual players, allowing us to\nquantify players' initial and equilibrium batting abilities, and the rate of\ntransition between the two. This is followed by implementing the model using a\nhierarchical structure, providing us with more general inference concerning a\nselected group of opening batsmen from New Zealand. The results indicate most\nplayers begin their innings playing with between only a quarter and half of\ntheir potential batting ability. Using the hierarchical structure we are able\nto make predictions for the batting abilities of the next opening batsman to\ndebut for New Zealand. Additionally, we compare and identify players who excel\nin the role of opening the batting, which has practical implications in terms\nof batting order and team selection policy. \n\n"}
{"id": "1609.04466", "contents": "Title: Phenotyping using Structured Collective Matrix Factorization of\n  Multi--source EHR Data Abstract: The increased availability of electronic health records (EHRs) have\nspearheaded the initiative for precision medicine using data driven approaches.\nEssential to this effort is the ability to identify patients with certain\nmedical conditions of interest from simple queries on EHRs, or EHR-based\nphenotypes. Existing rule--based phenotyping approaches are extremely labor\nintensive. Instead, dimensionality reduction and latent factor estimation\ntechniques from machine learning can be adapted for phenotype extraction with\nno (or minimal) human supervision.\n  We propose to identify an easily interpretable latent space shared across\nvarious sources of EHR data as potential candidates for phenotypes. By\nincorporating multiple EHR data sources (e.g., diagnosis, medications, and lab\nreports) available in heterogeneous datatypes in a generalized\n\\textit{Collective Matrix Factorization (CMF)}, our methods can generate rich\nphenotypes. Further, easy interpretability in phenotyping application requires\nsparse representations of the candidate phenotypes, for example each phenotype\nderived from patients' medication and diagnosis data should preferably be\nrepresented by handful of diagnosis and medications, ($5$--$10$ active\ncomponents). We propose a constrained formulation of CMF for estimating sparse\nphenotypes. We demonstrate the efficacy of our model through an extensive\nempirical study on EHR data from Vanderbilt University Medical Center. \n\n"}
{"id": "1609.04523", "contents": "Title: STORE: Sparse Tensor Response Regression and Neuroimaging Analysis Abstract: Motivated by applications in neuroimaging analysis, we propose a new\nregression model, Sparse TensOr REsponse regression (STORE), with a tensor\nresponse and a vector predictor. STORE embeds two key sparse structures:\nelement-wise sparsity and low-rankness. It can handle both a non-symmetric and\na symmetric tensor response, and thus is applicable to both structural and\nfunctional neuroimaging data. We formulate the parameter estimation as a\nnon-convex optimization problem, and develop an efficient alternating updating\nalgorithm. We establish a non-asymptotic estimation error bound for the actual\nestimator obtained from the proposed algorithm. This error bound reveals an\ninteresting interaction between the computational efficiency and the\nstatistical rate of convergence. When the distribution of the error tensor is\nGaussian, we further obtain a fast estimation error rate which allows the\ntensor dimension to grow exponentially with the sample size. We illustrate the\nefficacy of our model through intensive simulations and an analysis of the\nAutism spectrum disorder neuroimaging data. \n\n"}
{"id": "1609.05362", "contents": "Title: Mobile Edge Computing via a UAV-Mounted Cloudlet: Optimization of Bit\n  Allocation and Path Planning Abstract: Unmanned Aerial Vehicles (UAVs) have been recently considered as means to\nprovide enhanced coverage or relaying services to mobile users (MUs) in\nwireless systems with limited or no infrastructure. In this paper, a UAV-based\nmobile cloud computing system is studied in which a moving UAV is endowed with\ncomputing capabilities to offer computation offloading opportunities to MUs\nwith limited local processing capabilities. The system aims at minimizing the\ntotal mobile energy consumption while satisfying quality of service\nrequirements of the offloaded mobile application. Offloading is enabled by\nuplink and downlink communications between the mobile devices and the UAV that\ntake place by means of frequency division duplex (FDD) via orthogonal or\nnon-orthogonal multiple access (NOMA) schemes. The problem of jointly\noptimizing the bit allocation for uplink and downlink communication as well as\nfor computing at the UAV, along with the cloudlet's trajectory under latency\nand UAV's energy budget constraints is formulated and addressed by leveraging\nsuccessive convex approximation (SCA) strategies. Numerical results demonstrate\nthe significant energy savings that can be accrued by means of the proposed\njoint optimization of bit allocation and cloudlet's trajectory as compared to\nlocal mobile execution as well as to partial optimization approaches that\ndesign only the bit allocation or the cloudlet's trajectory. \n\n"}
{"id": "1609.08074", "contents": "Title: Orientation Statistics and Quantum Information Abstract: Motivated by the engineering applications of uncertainty quantification, in\nthis work we draw connections between the notions of random quantum states and\noperations in quantum information with probability distributions commonly\nencountered in the field of orientation statistics. This approach identifies\nnatural probability distributions that can be used in the analysis, simulation,\nand inference of quantum information systems. The theory of exponential\nfamilies on Stiefel manifolds provides the appropriate generalization to the\nclassical case, and fortunately there are many existing techniques for\ninference and sampling that exist for these distributions. Furthermore, this\nviewpoint motivates a number of additional questions into the convex geometry\nof quantum operations relative to both the differential geometry of Stiefel\nmanifolds as well as the information geometry of exponential families defined\nupon them. In particular, we draw on results from convex geometry to\ncharacterize which quantum operations can be represented as the average of a\nrandom quantum operation. \n\n"}
{"id": "1610.00124", "contents": "Title: Signatures of bifurcation on quantum correlations: Case of the quantum\n  kicked top Abstract: Quantum correlations reflect the quantumness of a system and are useful\nresources for quantum information and computational processes. The measures of\nquantum correlations do not have a classical analog and yet are influenced by\nthe classical dynamics. In this work, by modelling the quantum kicked top as a\nmulti-qubit system, the effect of classical bifurcations on the measures of\nquantum correlations such as quantum discord, geometric discord, Meyer and\nWallach $Q$ measure is studied. The quantum correlation measures change rapidly\nin the vicinity of a classical bifurcation point. If the classical system is\nlargely chaotic, time averages of the correlation measures are in good\nagreement with the values obtained by considering the appropriate random matrix\nensembles. The quantum correlations scale with the total spin of the system,\nrepresenting its semiclassical limit. In the vicinity of the trivial fixed\npoints of the kicked top, scaling function decays as a power-law. In the\nchaotic limit, for large total spin, quantum correlations saturate to a\nconstant, which we obtain analytically, based on random matrix theory, for the\n$Q$ measure. We also suggest that it can have experimental consequences. \n\n"}
{"id": "1610.00195", "contents": "Title: Penalized Ensemble Kalman Filters for High Dimensional Non-linear\n  Systems Abstract: The ensemble Kalman filter (EnKF) is a data assimilation technique that uses\nan ensemble of models, updated with data, to track the time evolution of a\nusually non-linear system. It does so by using an empirical approximation to\nthe well-known Kalman filter. However, its performance can suffer when the\nensemble size is smaller than the state space, as is often necessary for\ncomputationally burdensome models. This scenario means that the empirical\nestimate of the state covariance is not full rank and possibly quite noisy. To\nsolve this problem in this high dimensional regime, we propose a\ncomputationally fast and easy to implement algorithm called the penalized\nensemble Kalman filter (PEnKF). Under certain conditions, it can be\ntheoretically proven that the PEnKF will be accurate (the estimation error will\nconverge to zero) despite having fewer ensemble members than state dimensions.\nFurther, as contrasted to localization methods, the proposed approach learns\nthe covariance structure associated with the dynamical system. These\ntheoretical results are supported with simulations of several non-linear and\nhigh dimensional systems. \n\n"}
{"id": "1610.00580", "contents": "Title: Flint Water Crisis: Data-Driven Risk Assessment Via Residential Water\n  Testing Abstract: Recovery from the Flint Water Crisis has been hindered by uncertainty in both\nthe water testing process and the causes of contamination. In this work, we\ndevelop an ensemble of predictive models to assess the risk of lead\ncontamination in individual homes and neighborhoods. To train these models, we\nutilize a wide range of data sources, including voluntary residential water\ntests, historical records, and city infrastructure data. Additionally, we use\nour models to identify the most prominent factors that contribute to a high\nrisk of lead contamination. In this analysis, we find that lead service lines\nare not the only factor that is predictive of the risk of lead contamination of\nwater. These results could be used to guide the long-term recovery efforts in\nFlint, minimize the immediate damages, and improve resource-allocation\ndecisions for similar water infrastructure crises. \n\n"}
{"id": "1610.01339", "contents": "Title: Hybrid Spectrum Sharing in mmWave Cellular Networks Abstract: While spectrum at millimeter wave (mmWave) frequencies is less scarce than at\ntraditional frequencies below 6 GHz, still it is not unlimited, in particular\nif we consider the requirements from other services using the same band and the\nneed to license mmWave bands to multiple mobile operators. Therefore, an\nefficient spectrum access scheme is critical to harvest the maximum benefit\nfrom emerging mmWave technologies. In this paper, we introduce a new hybrid\nspectrum access scheme for mmWave networks, where data is aggregated through\ntwo mmWave carriers with different characteristics. In particular, we consider\nthe case of a hybrid spectrum scheme between a mmWave band with exclusive\naccess and a mmWave band where spectrum is pooled between multiple operators.\nTo the best of our knowledge, this is the first study proposing hybrid spectrum\naccess for mmWave networks and providing a quantitative assessment of its\nbenefits. Our results show that this approach provides major advantages with\nrespect to traditional fully licensed or fully unlicensed spectrum access\nschemes, though further work is needed to achieve a more complete understanding\nof both technical and non technical implications. \n\n"}
{"id": "1610.03514", "contents": "Title: Sparse Channel Estimation for Massive MIMO with 1-bit Feedback per\n  Dimension Abstract: In massive multiple-input multiple-output (MIMO) systems, acquisition of the\nchannel state information at the transmitter side (CSIT) is crucial. In this\npaper, a practical CSIT estimation scheme is proposed for frequency division\nduplexing (FDD) massive MIMO systems. Specifically, each received pilot symbol\nis first quantized to one bit per dimension at the receiver side and then the\nquantized bits are fed back to the transmitter. A joint one-bit compressed\nsensing algorithm is implemented at the transmitter to recover the channel\nmatrices. The algorithm leverages the hidden joint sparsity structure in the\nuser channel matrices to minimize the training and feedback overhead, which is\nconsidered to be a major challenge for FDD systems. Moreover, the one-bit\ncompressed sensing algorithm accurately recovers the channel directions for\nbeamforming. The one-bit feedback mechanism can be implemented in practical\nsystems using the uplink control channel. Simulation results show that the\nproposed scheme nearly achieves the maximum output signal-to-noise-ratio for\nbeamforming based on the estimated CSIT. \n\n"}
{"id": "1610.04196", "contents": "Title: Low-Cost Energy Meter Calibration Method for Measurement and\n  Verification Abstract: Energy meters need to be calibrated for use in Measurement and Verification\n(M&V) projects. However, calibration can be prohibitively expensive and affect\nproject feasibility negatively. This study presents a novel low-cost in-situ\nmeter data calibration technique using a relatively low accuracy commercial\nenergy meter as a calibrator. Calibration is achieved by combining two machine\nlearning tools: the SIMulation EXtrapolation (SIMEX) Measurement Error Model,\nand Bayesian regression. The model is trained or calibrated on half-hourly\nbuilding energy data for 24 hours. Measurements are then compared to the true\nvalues over the following months to verify the method. Results show that the\nhybrid method significantly improves parameter estimates and goodness of fit\nwhen compared to Ordinary Least Squares regression or standard SIMEX. This\nstudy also addresses the effect of mismeasurement in energy monitoring, and\nimplements a powerful technique for mitigating the bias that arises because of\nit. Meters calibrated by the technique presented have satisfactory accuracy for\nmost M&V applications, at a significantly lower cost. \n\n"}
{"id": "1610.04836", "contents": "Title: An Efficient Uplink Multi-Connectivity Scheme for 5G mmWave Control\n  Plane Applications Abstract: The millimeter wave (mmWave) frequencies offer the potential of orders of\nmagnitude increases in capacity for next-generation cellular systems. However,\nlinks in mmWave networks are susceptible to blockage and may suffer from rapid\nvariations in quality. Connectivity to multiple cells - at mmWave and/or\ntraditional frequencies - is considered essential for robust communication. One\nof the challenges in supporting multi-connectivity in mmWaves is the\nrequirement for the network to track the direction of each link in addition to\nits power and timing. To address this challenge, we implement a novel uplink\nmeasurement system that, with the joint help of a local coordinator operating\nin the legacy band, guarantees continuous monitoring of the channel propagation\nconditions and allows for the design of efficient control plane applications,\nincluding handover, beam tracking and initial access. We show that an\nuplink-based multi-connectivity approach enables less consuming, better\nperforming, faster and more stable cell selection and scheduling decisions with\nrespect to a traditional downlink-based standalone scheme. Moreover, we argue\nthat the presented framework guarantees (i) efficient tracking of the user in\nthe presence of the channel dynamics expected at mmWaves, and (ii) fast\nreaction to situations in which the primary propagation path is blocked or not\navailable. \n\n"}
{"id": "1610.06773", "contents": "Title: Variational Koopman models: slow collective variables and molecular\n  kinetics from short off-equilibrium simulations Abstract: Markov state models (MSMs) and Master equation models are popular approaches\nto approximate molecular kinetics, equilibria, metastable states, and reaction\ncoordinates in terms of a state space discretization usually obtained by\nclustering. Recently, a powerful generalization of MSMs has been introduced,\nthe variational approach (VA) of molecular kinetics and its special case the\ntime-lagged independent component analysis (TICA), which allow us to\napproximate slow collective variables and molecular kinetics by linear\ncombinations of smooth basis functions or order parameters. While it is known\nhow to estimate MSMs from trajectories whose starting points are not sampled\nfrom an equilibrium ensemble, this has not yet been the case for TICA and the\nVA. Previous estimates from short trajectories, have been strongly biased and\nthus not variationally optimal. Here, we employ Koopman operator theory and\nideas from dynamic mode decomposition (DMD) to extend the VA and TICA to\nnon-equilibrium data. The main insight is that the VA and TICA provide a\ncoefficient matrix that we call Koopman model, as it approximates the\nunderlying dynamical (Koopman) operator in conjunction with the basis set used.\nThis Koopman model can be used to compute a stationary vector to reweight the\ndata to equilibrium. From such a Koopman-reweighted sample, equilibrium\nexpectation values and variationally optimal reversible Koopman models can be\nconstructed even with short simulations. The Koopman model can be used to\npropagate densities, and its eigenvalue decomposition provide estimates of\nrelaxation timescales and slow collective variables for dimension reduction.\nKoopman models are generalizations of Markov state models, TICA and the linear\nVA and allow molecular kinetics to be described without a cluster\ndiscretization. \n\n"}
{"id": "1610.06953", "contents": "Title: Estimation and simulation of foraging trips in land-based marine\n  predators Abstract: The behaviour of colony-based marine predators is the focus of much research\nglobally. Large telemetry and tracking data sets have been collected for this\ngroup of animals, and are accompanied by many theoretical studies of optimal\nforaging strategies. However, relatively few studies have detailed statistical\nmethods for inferring behaviours in central place foraging trips. In this paper\nwe describe an approach based on hidden Markov models, which splits foraging\ntrips into segments labelled as \"outbound\", \"search\", \"forage\", and \"inbound\".\nBy structuring the hidden Markov model transition matrix appropriately, the\nmodel naturally handles the sequence of behaviours within a foraging trip.\nAdditionally, by structuring the model in this way, we are able to develop\nrealistic simulations from the fitted model. We demonstrate our approach on\ndata from southern elephant seals (Mirounga leonina) tagged on Kerguelen Island\nin the Southern Ocean. We discuss the differences between our 4-state model and\nthe widely used 2-state model, and the advantages and disadvantages of\nemploying a more complex model. \n\n"}
{"id": "1610.07524", "contents": "Title: Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments Abstract: Recidivism prediction instruments provide decision makers with an assessment\nof the likelihood that a criminal defendant will reoffend at a future point in\ntime. While such instruments are gaining increasing popularity across the\ncountry, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses a fairness criterion originating in the\nfield of educational and psychological testing that has recently been applied\nto assess the fairness of recidivism prediction instruments. We demonstrate how\nadherence to the criterion may lead to considerable disparate impact when\nrecidivism prevalence differs across groups. \n\n"}
{"id": "1610.07684", "contents": "Title: Exploratory Analysis of High Dimensional Time Series with Applications\n  to Multichannel Electroencephalograms Abstract: In this paper, we address the the major hurdle of high dimensionality in EEG\nanalysis by extracting the optimal lower dimensional representations. Using our\napproach, connectivity between regions in a high-dimensional brain network is\ncharacterized through the connectivity between region-specific factors. The\nproposed approach is motivated by our observation that electroencephalograms\n(EEGs) from channels within each region exhibit a high degree of\nmulticollinearity and synchrony. These observations suggest that it would be\nsensible to extract summary factors for each region. We consider the general\napproach for deriving summary factors which are solutions to the criterion of\nsquared error reconstruction. In this work, we focus on two special cases of\nlinear auto encoder and decoder. In the first approach, the factors are\ncharacterized as instantaneous linear mixing of the observed high dimensional\ntime series. In the second approach, the factors signals are linear filtered\nversions of the original signal which is more general than an instantaneous\nmixing. This exploratory analysis is the starting point to the multi-scale\nfactor analysis model where the concatenated factors from all regions are\nrepresented by vector auto-regressive model that captures the connectivity in\nhigh dimensional signals. We performed evaluations on the two approaches via\nsimulations under different conditions. The simulation results provide insights\non the performance and application scope of the methods. We also performed\nexploratory analysis of EEG recorded over several epochs during resting state.\nFinally, we implemented these exploratory methods in a Matlab toolbox XHiDiTS\navailable from https://goo.gl/uXc8ei . \n\n"}
{"id": "1610.07748", "contents": "Title: Balancing, Regression, Difference-In-Differences and Synthetic Control\n  Methods: A Synthesis Abstract: In a seminal paper Abadie, Diamond, and Hainmueller [2010] (ADH), see also\nAbadie and Gardeazabal [2003], Abadie et al. [2014], develop the synthetic\ncontrol procedure for estimating the effect of a treatment, in the presence of\na single treated unit and a number of control units, with pre-treatment\noutcomes observed for all units. The method constructs a set of weights such\nthat selected covariates and pre-treatment outcomes of the treated unit are\napproximately matched by a weighted average of control units (the synthetic\ncontrol). The weights are restricted to be nonnegative and sum to one, which is\nimportant because it allows the procedure to obtain unique weights even when\nthe number of lagged outcomes is modest relative to the number of control\nunits, a common setting in applications. In the current paper we propose a\ngeneralization that allows the weights to be negative, and their sum to differ\nfrom one, and that allows for a permanent additive difference between the\ntreated unit and the controls, similar to difference-in-difference procedures.\nThe weights directly minimize the distance between the lagged outcomes for the\ntreated and the control units, using regularization methods to deal with a\npotentially large number of possible control units. \n\n"}
{"id": "1610.09878", "contents": "Title: Blinded sample size re-estimation in three-arm trials with 'gold\n  standard' design Abstract: The sample size of a clinical trial relies on information about nuisance\nparameters such as the outcome variance. When no or only limited information is\navailable, it has been proposed to include an internal pilot study in the\ndesign of the trial. Based on the results of the internal pilot study, the\ninitially planned sample size can be adjusted. In this paper, we study blinded\nsample size re-estimation in the 'gold standard' design for normally\ndistributed outcomes. The 'gold standard' design is a three-arm clinical trial\ndesign which includes an active and a placebo control in addition to an\nexperimental treatment. We compare several sample size re-estimation procedures\nin a simulation study assessing operating characteristics including power and\ntype I error. We find that sample size re-estimation based on the popular\none-sample variance estimator results in overpowered trials. Moreover, sample\nsize re-estimation based on unbiased variance estimators such as the Xing-Ganju\nvariance estimator results in underpowered trials, as it is expected since an\noverestimation of the variance and thus the sample size is in general required\nfor the re-estimation procedure to eventually meet the target power. Moreover,\nwe propose an inflation factor for the sample size re-estimation with the\nXing-Ganju variance estimator and show that this approach results in adequately\npowered trials. Due to favorable features of Xing-Ganju variance estimator such\nas unbiasedness and a distribution independent of the group means, the\ninflation factor does not depend on the nuisance parameter and, therefore, can\nbe calculated prior to a trial. \n\n"}
{"id": "1611.00036", "contents": "Title: The DESI Experiment Part I: Science,Targeting, and Survey Design Abstract: DESI (Dark Energy Spectroscopic Instrument) is a Stage IV ground-based dark\nenergy experiment that will study baryon acoustic oscillations (BAO) and the\ngrowth of structure through redshift-space distortions with a wide-area galaxy\nand quasar redshift survey. To trace the underlying dark matter distribution,\nspectroscopic targets will be selected in four classes from imaging data. We\nwill measure luminous red galaxies up to $z=1.0$. To probe the Universe out to\neven higher redshift, DESI will target bright [O II] emission line galaxies up\nto $z=1.7$. Quasars will be targeted both as direct tracers of the underlying\ndark matter distribution and, at higher redshifts ($ 2.1 < z < 3.5$), for the\nLy-$\\alpha$ forest absorption features in their spectra, which will be used to\ntrace the distribution of neutral hydrogen. When moonlight prevents efficient\nobservations of the faint targets of the baseline survey, DESI will conduct a\nmagnitude-limited Bright Galaxy Survey comprising approximately 10 million\ngalaxies with a median $z\\approx 0.2$. In total, more than 30 million galaxy\nand quasar redshifts will be obtained to measure the BAO feature and determine\nthe matter power spectrum, including redshift space distortions. \n\n"}
{"id": "1611.01006", "contents": "Title: Bayesian Heuristics for Group Decisions Abstract: We propose a model of inference and heuristic decision-making in groups that\nis rooted in the Bayes rule but avoids the complexities of rational inference\nin partially observed environments with incomplete information, which are\ncharacteristic of group interactions. Our model is also consistent with a\ndual-process psychological theory of thinking: the group members behave\nrationally at the initiation of their interactions with each other (the slow\nand deliberative mode); however, in the ensuing decision epochs, they rely on a\nheuristic that replicates their experiences from the first stage (the fast\nautomatic mode). We specialize this model to a group decision scenario where\nprivate observations are received at the beginning, and agents aim to take the\nbest action given the aggregate observations of all group members. We study the\nimplications of the information structure together with the properties of the\nprobability distributions which determine the structure of the so-called\n\"Bayesian heuristics\" that the agents follow in our model. We also analyze the\ngroup decision outcomes in two classes of linear action updates and log-linear\nbelief updates and show that many inefficiencies arise in group decisions as a\nresult of repeated interactions between individuals, leading to overconfident\nbeliefs as well as choice-shifts toward extremes. Nevertheless, balanced\nregular structures demonstrate a measure of efficiency in terms of aggregating\nthe initial information of individuals. These results not only verify some\nwell-known insights about group decision-making but also complement these\ninsights by revealing additional mechanistic interpretations for the group\ndeclension-process, as well as psychological and cognitive intuitions about the\ngroup interaction model. \n\n"}
{"id": "1611.03015", "contents": "Title: Honest Confidence Sets in Nonparametric IV Regression and Other\n  Ill-Posed Models Abstract: This paper develops inferential methods for a very general class of ill-posed\nmodels in econometrics encompassing the nonparametric instrumental variable\nregression, various functional regressions, and the density deconvolution. We\nfocus on uniform confidence sets for the parameter of interest estimated with\nTikhonov regularization, as in Darolles, Fan, Florens, and Renault (2011).\nSince it is impossible to have inferential methods based on the central limit\ntheorem, we develop two alternative approaches relying on the concentration\ninequality and bootstrap approximations. We show that expected diameters and\ncoverage properties of resulting sets have uniform validity over a large class\nof models, i.e., constructed confidence sets are honest. Monte Carlo\nexperiments illustrate that introduced confidence sets have reasonable width\nand coverage properties. Using U.S. data, we provide uniform confidence sets\nfor Engel curves for various commodities. \n\n"}
{"id": "1611.03021", "contents": "Title: Attributing Hacks Abstract: In this paper we describe an algorithm for estimating the provenance of hacks\non websites. That is, given properties of sites and the temporal occurrence of\nattacks, we are able to attribute individual attacks to joint causes and\nvulnerabilities, as well as estimating the evolution of these vulnerabilities\nover time. Specifically, we use hazard regression with a time-varying additive\nhazard function parameterized in a generalized linear form. The activation\ncoefficients on each feature are continuous-time functions over time. We\nformulate the problem of learning these functions as a constrained variational\nmaximum likelihood estimation problem with total variation penalty and show\nthat the optimal solution is a 0th order spline (a piecewise constant function)\nwith a finite number of known knots. This allows the inference problem to be\nsolved efficiently and at scale by solving a finite dimensional optimization\nproblem. Extensive experiments on real data sets show that our method\nsignificantly outperforms Cox's proportional hazard model. We also conduct a\ncase study and verify that the fitted functions are indeed recovering\nvulnerable features and real-life events such as the release of code to exploit\nthese features in hacker blogs. \n\n"}
{"id": "1611.04748", "contents": "Title: Improved Handover Through Dual Connectivity in 5G mmWave Mobile Networks Abstract: The millimeter wave (mmWave) bands offer the possibility of orders of\nmagnitude greater throughput for fifth generation (5G) cellular systems.\nHowever, since mmWave signals are highly susceptible to blockage, channel\nquality on any one mmWave link can be extremely intermittent. This paper\nimplements a novel dual connectivity protocol that enables mobile user\nequipment (UE) devices to maintain physical layer connections to 4G and 5G\ncells simultaneously. A novel uplink control signaling system combined with a\nlocal coordinator enables rapid path switching in the event of failures on any\none link. This paper provides the first comprehensive end-to-end evaluation of\nhandover mechanisms in mmWave cellular systems. The simulation framework\nincludes detailed measurement-based channel models to realistically capture\nspatial dynamics of blocking events, as well as the full details of MAC, RLC\nand transport protocols. Compared to conventional handover mechanisms, the\nstudy reveals significant benefits of the proposed method under several\nmetrics. \n\n"}
{"id": "1611.05368", "contents": "Title: Neural Style Representations and the Large-Scale Classification of\n  Artistic Style Abstract: The artistic style of a painting is a subtle aesthetic judgment used by art\nhistorians for grouping and classifying artwork. The recently introduced\n`neural-style' algorithm substantially succeeds in merging the perceived\nartistic style of one image or set of images with the perceived content of\nanother. In light of this and other recent developments in image analysis via\nconvolutional neural networks, we investigate the effectiveness of a\n`neural-style' representation for classifying the artistic style of paintings. \n\n"}
{"id": "1611.05977", "contents": "Title: Robust and Scalable Column/Row Sampling from Corrupted Big Data Abstract: Conventional sampling techniques fall short of drawing descriptive sketches\nof the data when the data is grossly corrupted as such corruptions break the\nlow rank structure required for them to perform satisfactorily. In this paper,\nwe present new sampling algorithms which can locate the informative columns in\npresence of severe data corruptions. In addition, we develop new scalable\nrandomized designs of the proposed algorithms. The proposed approach is\nsimultaneously robust to sparse corruption and outliers and substantially\noutperforms the state-of-the-art robust sampling algorithms as demonstrated by\nexperiments conducted using both real and synthetic data. \n\n"}
{"id": "1611.06715", "contents": "Title: Pitfalls in testing with linear regression model by OLS Abstract: This is a comment on Economic Letters DOI\nhttp://dx.doi.org/10.1016/j.econlet.2015.10.015. We show that due to some\nmethodological aspects the main conclusions of the above mentioned paper should\nbe a little bit altered. \n\n"}
{"id": "1611.07237", "contents": "Title: Multivariate Intensity Estimation via Hyperbolic Wavelet Selection Abstract: We propose a new statistical procedure able in some way to overcome the curse\nof dimensionality without structural assumptions on the function to estimate.\nIt relies on a least-squares type penalized criterion and a new collection of\nmodels built from hyperbolic biorthogonal wavelet bases. We study its\nproperties in a unifying intensity estimation framework, where an oracle-type\ninequality and adaptation to mixed smoothness are shown to hold. Besides, we\ndescribe an algorithm for implementing the estimator with a quite reasonable\ncomplexity. \n\n"}
{"id": "1611.07911", "contents": "Title: An efficient surrogate model for emulation and physics extraction of\n  large eddy simulations Abstract: In the quest for advanced propulsion and power-generation systems,\nhigh-fidelity simulations are too computationally expensive to survey the\ndesired design space, and a new design methodology is needed that combines\nengineering physics, computer simulations and statistical modeling. In this\npaper, we propose a new surrogate model that provides efficient prediction and\nuncertainty quantification of turbulent flows in swirl injectors with varying\ngeometries, devices commonly used in many engineering applications. The novelty\nof the proposed method lies in the incorporation of known physical properties\nof the fluid flow as {simplifying assumptions} for the statistical model. In\nview of the massive simulation data at hand, which is on the order of hundreds\nof gigabytes, these assumptions allow for accurate flow predictions in around\nan hour of computation time. To contrast, existing flow emulators which forgo\nsuch simplications may require more computation time for training and\nprediction than is needed for conducting the simulation itself. Moreover, by\naccounting for coupling mechanisms between flow variables, the proposed model\ncan jointly reduce prediction uncertainty and extract useful flow physics,\nwhich can then be used to guide further investigations. \n\n"}
{"id": "1612.01089", "contents": "Title: A Novel Approach for Big Data Analytics in Future Grids Based on Free\n  Probability Abstract: Based on the random matrix model, we can build statistical models using\nmassive datasets across the power grid, and employ hypothesis testing for\nanomaly detection. First, the aim of this paper is to make the first attempt to\napply the recent free probability result in extracting big data analytics, in\nparticular data fusion. The nature of this work is basic in that new algorithms\nand analytics tools are proposed to pave the way for the future's research.\nSecond, using the new analytic tool, we are able to make some discovery related\nto anomaly detection that is very difficult for other approaches. To our best\nknowledge, there is no similar report in the literature. Third, both linear and\nnonlinear polynomials of large random matrices can be handled in this new\nframework. Simulations demonstrate the following: Compared with the linearity,\nnonlinearity is more flexible in problem modeling and closer to the nature of\nthe reality. In some sense, some other nonlinear matrix polynomials may be more\neffective for the power grid \n\n"}
{"id": "1612.01773", "contents": "Title: Peaks over thresholds modelling with multivariate generalized Pareto\n  distributions Abstract: When assessing the impact of extreme events, it is often not just a single\ncomponent, but the combined behaviour of several components which is important.\nStatistical modelling using multivariate generalized Pareto (GP) distributions\nconstitutes the multivariate analogue of univariate peaks over thresholds\nmodelling, which is widely used in finance and engineering. We develop general\nmethods for construction of multivariate GP distributions and use them to\ncreate a variety of new statistical models. A censored likelihood procedure is\nproposed to make inference on these models, together with a threshold selection\nprocedure, goodness-of-fit diagnostics, and a computationally tractable\nstrategy for model selection. The models are fitted to returns of stock prices\nof four UK-based banks and to rainfall data in the context of landslide risk\nestimation. Supplementary materials and codes are available online. \n\n"}
{"id": "1612.02130", "contents": "Title: Predictive Business Process Monitoring with LSTM Neural Networks Abstract: Predictive business process monitoring methods exploit logs of completed\ncases of a process in order to make predictions about running cases thereof.\nExisting methods in this space are tailor-made for specific prediction tasks.\nMoreover, their relative accuracy is highly sensitive to the dataset at hand,\nthus requiring users to engage in trial-and-error and tuning when applying them\nin a specific setting. This paper investigates Long Short-Term Memory (LSTM)\nneural networks as an approach to build consistently accurate models for a wide\nrange of predictive process monitoring tasks. First, we show that LSTMs\noutperform existing techniques to predict the next event of a running case and\nits timestamp. Next, we show how to use models for predicting the next task in\norder to predict the full continuation of a running case. Finally, we apply the\nsame approach to predict the remaining time, and show that this approach\noutperforms existing tailor-made methods. \n\n"}
{"id": "1612.02195", "contents": "Title: Generalized Exponential smoothing in prediction of hierarchical time\n  series Abstract: Shang and Hyndman (2017) proposed a grouped functional time series\nforecasting approach as a combination of individual forecasts obtained using\ngeneralized least squares method. We modify their methodology using generalized\nexponential smoothing technique for the most disaggregated functional time\nseries in order to obtain more robust predictor. We discuss some properties of\nour proposals basing on results obtained via simulation studies and analysis of\nreal data related to a prediction of a demand for electricity in Australia in\n2016. \n\n"}
{"id": "1612.04535", "contents": "Title: Is the familywise error rate in genomics controlled by methods based on\n  the effective number of independent tests? Abstract: In genome-wide association (GWA) studies the goal is to detect association\nbetween one or more genetic markers and a given phenotype. The number of\ngenetic markers in a GWA study can be in the order hundreds of thousands and\ntherefore multiple testing methods are needed. This paper presents a set of\npopular methods to be used to correct for multiple testing in GWA studies. All\nare based on the concept of estimating an effective number of independent\ntests. We compare these methods using simulated data and data from the TOP\nstudy, and show that the effective number of independent tests is not additive\nover blocks of independent genetic markers unless we assume a common value for\nthe local significance level. We also show that the reviewed methods based on\nestimating the effective number of independent tests in general do not control\nthe familywise error rate. \n\n"}
{"id": "1612.04838", "contents": "Title: A discrete modification of the Benjamini-Yekutieli procedure Abstract: The Benjamini-Yekutieli procedure is a multiple testing method that controls\nthe false discovery rate under arbitrary dependence of the $p$-values. A\nmodification of this and related procedures is proposed for the case when the\ntest statistics are discrete. It is shown that taking discreteness into account\ncan improve upon known procedures. The performance of this new procedure is\nevaluated for pharmacovigilance data and in a simulation study. \n\n"}
{"id": "1612.05198", "contents": "Title: The Statistical Face of a Region under Monsoon Rainfall in Eastern India Abstract: A region under rainfall is a contiguous spatial area receiving positive\nprecipitation at a particular time. The probabilistic behavior of such a region\nis an issue of interest in meteorological studies. A region under rainfall can\nbe viewed as a shape object of a special kind, where scale and rotational\ninvariance are not necessarily desirable attributes of a mathematical\nrepresentation. For modeling variation in objects of this type, we propose an\napproximation of the boundary that can be represented as a real valued\nfunction, and arrive at further approximation through functional principal\ncomponent analysis, after suitable adjustment for asymmetry and incompleteness\nin the data. The analysis of an open access satellite data set on monsoon\nprecipitation over Eastern India leads to explanation of most of the variation\nin shapes of the regions under rainfall through a handful of interpretable\nfunctions that can be further approximated parametrically. The most important\naspect of shape is found to be the size followed by contraction/elongation,\nmostly along two pairs of orthogonal axes. The different modes of variation are\nremarkably stable across calendar years and across different thresholds for\nminimum size of the region. \n\n"}
{"id": "1612.05560", "contents": "Title: The Pan-STARRS1 Surveys Abstract: Pan-STARRS1 has carried out a set of distinct synoptic imaging sky surveys\nincluding the $3\\pi$ Steradian Survey and the Medium Deep Survey in 5 bands\n($grizy_{P1}$). The mean 5$\\sigma$ point source limiting sensitivities in the\nstacked 3$\\pi$ Steradian Survey in $grizy_{P1}$ are (23.3, 23.2, 23.1, 22.3,\n21.4) respectively. The upper bound on the systematic uncertainty in the\nphotometric calibration across the sky is 7-12 millimag depending on the\nbandpass. The systematic uncertainty of the astrometric calibration using the\nGaia frame comes from a comparison of the results with Gaia: the standard\ndeviation of the mean and median residuals ($ \\Delta ra, \\Delta dec $) are\n(2.3, 1.7) milliarcsec, and (3.1, 4.8) milliarcsec respectively. The Pan-STARRS\nsystem and the design of the PS1 surveys are described and an overview of the\nresulting image and catalog data products and their basic characteristics are\ndescribed together with a summary of important results. The images, reduced\ndata products, and derived data products from the Pan-STARRS1 surveys are\navailable to the community from the Mikulski Archive for Space Telescopes\n(MAST) at STScI. \n\n"}
{"id": "1612.06746", "contents": "Title: Correlations and forecast of death tolls in the Syrian conflict Abstract: The Syrian civil war has been ongoing since 2011 and has already caused\nthousands of deaths. The analysis of death tolls helps to understand the\ndynamics of the conflict and to better allocate resources to the affected\nareas. In this article, we use information on the daily number of deaths to\nstudy temporal and spatial correlations in the data, and exploit this\ninformation to forecast events of deaths. We find that the number of deaths per\nday follows a log-normal distribution during the conflict. We have also\nidentified strong correlations between cities and on consecutive days, implying\nthat major deaths in one location are typically followed by major deaths in\nboth the same location and in other areas. We find that war-related deaths are\nnot random events and observing death tolls in some cities helps to better\npredict these numbers across the system. \n\n"}
{"id": "1612.09596", "contents": "Title: Counterfactual Prediction with Deep Instrumental Variables Networks Abstract: We are in the middle of a remarkable rise in the use and capability of\nartificial intelligence. Much of this growth has been fueled by the success of\ndeep learning architectures: models that map from observables to outputs via\nmultiple layers of latent representations. These deep learning algorithms are\neffective tools for unstructured prediction, and they can be combined in AI\nsystems to solve complex automated reasoning problems. This paper provides a\nrecipe for combining ML algorithms to solve for causal effects in the presence\nof instrumental variables -- sources of treatment randomization that are\nconditionally independent from the response. We show that a flexible IV\nspecification resolves into two prediction tasks that can be solved with deep\nneural nets: a first-stage network for treatment prediction and a second-stage\nnetwork whose loss function involves integration over the conditional treatment\ndistribution. This Deep IV framework imposes some specific structure on the\nstochastic gradient descent routine used for training, but it is general enough\nthat we can take advantage of off-the-shelf ML capabilities and avoid extensive\nalgorithm customization. We outline how to obtain out-of-sample causal\nvalidation in order to avoid over-fit. We also introduce schemes for both\nBayesian and frequentist inference: the former via a novel adaptation of\ndropout training, and the latter via a data splitting routine. \n\n"}
{"id": "1701.00818", "contents": "Title: An optically-selected cluster catalog at redshift 0.1<z<1.1 from the\n  Hyper Suprime-Cam Subaru Strategic Program S16A data Abstract: We present an optically-selected cluster catalog from the Hyper Suprime-Cam\n(HSC) Subaru Strategic Program. The HSC images are sufficiently deep to detect\ncluster member galaxies down to $M_*\\sim 10^{10.2}M_\\odot$ even at $z\\sim 1$,\nallowing a reliable cluster detection at such high redshifts. We apply the\nCAMIRA algorithm to the HSC Wide S16A dataset covering $\\sim 232$ deg$^2$ to\nconstruct a catalog of 1921 clusters at redshift $0.1<z<1.1$ and richness\n$\\hat{N}_{\\rm mem}>15$ that roughly corresponds to $M_{\\rm 200m}\\gtrsim\n10^{14}h^{-1}M_\\odot$. We confirm good cluster photometric redshift\nperformance, with the bias and scatter in $\\Delta z/(1+z)$ being better than\n0.005 and 0.01 over most of the redshift range, respectively. We compare our\ncluster catalog with large X-ray cluster catalogs from XXL and XMM-LSS surveys\nand find good correlation between richness and X-ray properties. We also study\nthe miscentering effect from the distribution of offsets between optical and\nX-ray cluster centers. We confirm the high ($>0.9$) completeness and purity for\nhigh mass clusters by analyzing mock galaxy catalogs. \n\n"}
{"id": "1701.02950", "contents": "Title: Convex Mixture Regression for Quantitative Risk Assessment Abstract: There is wide interest in studying how the distribution of a continuous\nresponse changes with a predictor. We are motivated by environmental\napplications in which the predictor is the dose of an exposure and the response\nis a health outcome. A main focus in these studies is inference on dose levels\nassociated with a given increase in risk relative to a baseline. Popular\nmethods either dichotomize the continuous response or focus on modeling changes\nwith the dose in the expectation of the outcome. Such choices may lead to\ninformation loss and provide inaccurate inference on dose-response\nrelationships. We instead propose a Bayesian convex mixture regression model\nthat allows the entire distribution of the health outcome to be unknown and\nchanging with the dose. To balance flexibility and parsimony, we rely on a\nmixture model for the density at the extreme doses, and express the conditional\ndensity at each intermediate dose via a convex combination of these extremal\ndensities. This representation generalizes classical dose-response models for\nquantitative outcomes, and provides a more parsimonious, but still powerful,\nformulation compared to nonparametric methods, thereby improving\ninterpretability and efficiency in inference on risk functions. A Markov chain\nMonte Carlo algorithm for posterior inference is developed, and the benefits of\nour methods are outlined in simulations, along with a study on the impact of\nDDT exposure on gestational age. \n\n"}
{"id": "1701.03161", "contents": "Title: A Wavelet-Based Approach To Monitoring Parkinson's Disease Symptoms Abstract: Parkinson's disease is a neuro-degenerative disorder affecting tens of\nmillions of people worldwide. Lately, there has been considerable interest in\nsystems for at-home monitoring of patients, using wearable devices which\ncontain inertial measurement units. We present a new wavelet-based approach for\nanalysis of data from single wrist-worn smart-watches, and show high detection\nperformance for tremor, bradykinesia, and dyskinesia, which have been the major\ntargets for monitoring in this context. We also discuss the implication of our\ncontrolled-experiment results for uncontrolled home monitoring of freely\nbehaving patients. \n\n"}
{"id": "1701.04065", "contents": "Title: On the Asymptotic Behavior of Ultra-Densification under a Bounded\n  Dual-Slope Path Loss Model Abstract: In this paper, we investigate the impact of network densification on the\nperformance in terms of downlink signal-to-interference (SIR) coverage\nprobability and network area spectral efficiency (ASE). A sophisticated bounded\ndual-slope path loss model and practical user equipment (UE) densities are\nincorporated in the analysis, which have never been jointly considered before.\nBy using stochastic geometry, we derive an integral expression along with\nclosed-form bounds of the coverage probability and ASE, validated by simulation\nresults. Through these, we provide the asymptotic behavior of\nultra-densification. The coverage probability and ASE have non-zero convergence\nin asymptotic regions unless UE density goes to infinity (full load).\nMeanwhile, the effect of UE density on the coverage probability is analyzed.\nThe coverage probability will reveal an U-shape for large UE densities due to\ninterference fall into the near-field, but it will keep increasing for low UE\ndensites. Furthermore, our results indicate that the performance is\noverestimated without applying the bounded dual-slope path loss model. The\nderived expressions and results in this work pave the way for future network\nprovisioning. \n\n"}
{"id": "1701.06619", "contents": "Title: Bayesian Inference in the Presence of Intractable Normalizing Functions Abstract: Models with intractable normalizing functions arise frequently in statistics.\nCommon examples of such models include exponential random graph models for\nsocial networks and Markov point processes for ecology and disease modeling.\nInference for these models is complicated because the normalizing functions of\ntheir probability distributions include the parameters of interest. In Bayesian\nanalysis they result in so-called doubly intractable posterior distributions\nwhich pose significant computational challenges. Several Monte Carlo methods\nhave emerged in recent years to address Bayesian inference for such models. We\nprovide a framework for understanding the algorithms and elucidate connections\namong them. Through multiple simulated and real data examples, we compare and\ncontrast the computational and statistical efficiency of these algorithms and\ndiscuss their theoretical bases. Our study provides practical recommendations\nfor practitioners along with directions for future research for MCMC\nmethodologists. \n\n"}
{"id": "1701.07153", "contents": "Title: Throughput Maximization for Wireless Powered Communications Harvesting\n  from Non-dedicated Sources Abstract: We consider the wireless powered communications where users harvest energy\nfrom non-dedicated sources. The user follows a harvest-then-transmit protocol:\nin first phase of a slot time the source node harvests energy from a nearby\nconventional Access Point, then transmit information to its destination node or\nrelay node in the second phase. We obtain the optimal\\textit{ harvesting ratio}\nto maximize the expected throughput for direct transmission (DT )and decode\nforward (DF) relay under outage constraint, respectively. Our results reveal\nthat the optimal harvest ratio for DT is dominated by the outage constraint\nwhile for DF relay, by the data causality . \n\n"}
{"id": "1701.07363", "contents": "Title: E2M2: Energy Efficient Mobility Management in Dense Small Cells with\n  Mobile Edge Computing Abstract: Merging mobile edge computing with the dense deployment of small cell base\nstations promises enormous benefits such as a real proximity, ultra-low latency\naccess to cloud functionalities. However, the envisioned integration creates\nmany new challenges and one of the most significant is mobility management,\nwhich is becoming a key bottleneck to the overall system performance. Simply\napplying existing solutions leads to poor performance due to the highly\noverlapped coverage areas of multiple base stations in the proximity of the\nuser and the co-provisioning of radio access and computing services. In this\npaper, we develop a novel user-centric mobility management scheme, leveraging\nLyapunov optimization and multi-armed bandits theories, in order to maximize\nthe edge computation performance for the user while keeping the user's\ncommunication energy consumption below a constraint. The proposed scheme\neffectively handles the uncertainties present at multiple levels in the system\nand provides both short-term and long-term performance guarantee. Simulation\nresults show that our proposed scheme can significantly improve the computation\nperformance (compared to state of the art) while satisfying the communication\nenergy constraint. \n\n"}
{"id": "1701.07910", "contents": "Title: Combining Envelope Methodology and Aster Models for Variance Reduction\n  in Life History Analyses Abstract: Precise estimation of expected Darwinian fitness, the expected lifetime\nnumber of offspring of organism, is a central component of life history\nanalysis. The aster model serves as a defensible statistical model for\ndistributions of Darwinian fitness. The aster model is equipped to incorporate\nthe major life stages an organism travels through which separately may effect\nDarwinian fitness. Envelope methodology reduces asymptotic variability by\nestablishing a link between unknown parameters of interest and the asymptotic\ncovariance matrices of their estimators. It is known both theoretically and in\napplications that incorporation of envelope methodology reduces asymptotic\nvariability. We develop an envelope framework, including a new envelope\nestimator, that is appropriate for aster analyses. The level of precision\nprovided from our methods allows researchers to draw stronger conclusions about\nthe driving forces of Darwinian fitness from their life history analyses than\nthey could with the aster model alone. Our methods are illustrated on a\nsimulated dataset and a life history analysis of \\emph{Mimulus guttatus}\nflowers is provided. Useful variance reduction is obtained in both analyses. \n\n"}
{"id": "1701.07964", "contents": "Title: On the Performance of Practical Ultra-Dense Networks: The Major and\n  Minor Factors Abstract: In this paper, we conduct performance evaluation for Ultra-Dense Networks\n(UDNs), and identify which modelling factors play major roles and minor roles.\nFrom our study, we draw the following conclusions. First, there are 3\nfactors/models that have a major impact on the performance of UDNs, and they\nshould be considered when performing theoretical analyses: i) a multi-piece\npath loss model with line-of-sight (LoS) and non-lineof- sight (NLoS)\ntransmissions; ii) a non-zero antenna height difference between base stations\n(BSs) and user equipments (UEs); iii) a finite BS/UE density. Second, there are\n4 factors/models that have a minor impact on the performance of UDNs, i.e.,\nchanging the results quantitatively but not qualitatively, and thus their\nincorporation into theoretical analyses is less urgent: i) a general multi-path\nfading model based on Rician fading; ii) a correlated shadow fading model; iii)\na BS density dependent transmission power; iv) a deterministic BS/user density.\nFinally, there are 5 factors/models for future study: i) a BS vertical antenna\npattern; ii) multi-antenna and/or multi-BS joint transmissions; iii) a\nproportional fair BS scheduler; iv) a non-uniform distribution of BSs; v) a\ndynamic time division duplex (TDD) or full duplex (FD) network. Our conclusions\ncan guide researchers to downselect the assumptions in their theoretical\nanalyses, so as to avoid unnecessarily complicated results, while still\ncapturing the fundamentals of UDNs in a meaningful way. \n\n"}
{"id": "1701.08055", "contents": "Title: Modelling Competitive Sports: Bradley-Terry-\\'{E}l\\H{o} Models for\n  Supervised and On-Line Learning of Paired Competition Outcomes Abstract: Prediction and modelling of competitive sports outcomes has received much\nrecent attention, especially from the Bayesian statistics and machine learning\ncommunities. In the real world setting of outcome prediction, the seminal\n\\'{E}l\\H{o} update still remains, after more than 50 years, a valuable baseline\nwhich is difficult to improve upon, though in its original form it is a\nheuristic and not a proper statistical \"model\". Mathematically, the \\'{E}l\\H{o}\nrating system is very closely related to the Bradley-Terry models, which are\nusually used in an explanatory fashion rather than in a predictive supervised\nor on-line learning setting.\n  Exploiting this close link between these two model classes and some newly\nobserved similarities, we propose a new supervised learning framework with\nclose similarities to logistic regression, low-rank matrix completion and\nneural networks. Building on it, we formulate a class of structured log-odds\nmodels, unifying the desirable properties found in the above: supervised\nprobabilistic prediction of scores and wins/draws/losses, batch/epoch and\non-line learning, as well as the possibility to incorporate features in the\nprediction, without having to sacrifice simplicity, parsimony of the\nBradley-Terry models, or computational efficiency of \\'{E}l\\H{o}'s original\napproach.\n  We validate the structured log-odds modelling approach in synthetic\nexperiments and English Premier League outcomes, where the added expressivity\nyields the best predictions reported in the state-of-art, close to the quality\nof contemporary betting odds. \n\n"}
{"id": "1701.08312", "contents": "Title: ClipAudit: A Simple Risk-Limiting Post-Election Audit Abstract: We propose a simple risk-limiting audit for elections, ClipAudit. To\ndetermine whether candidate A (the reported winner) actually beat candidate B\nin a plurality election, ClipAudit draws ballots at random, without\nreplacement, until either all cast ballots have been drawn, or until \\[ a - b\n\\ge \\beta \\sqrt{a+b}\n  \\] where $a$ is the number of ballots in the sample for the reported winner\nA, and $b$ is the number of ballots in the sample for opponent B, and where\n$\\beta$ is a constant determined a priori as a function of the number $n$ of\nballots cast and the risk-limit $\\alpha$. ClipAudit doesn't depend on the\nunofficial margin (as does Bravo). We show how to extend ClipAudit to contests\nwith multiple winners or losers, or to multiple contests. \n\n"}
{"id": "1702.00298", "contents": "Title: Cascading Failures in Interdependent Systems: Impact of Degree\n  Variability and Dependence Abstract: We study cascading failures in a system comprising interdependent\nnetworks/systems, in which nodes rely on other nodes both in the same system\nand in other systems to perform their function. The (inter-)dependence among\nnodes is modeled using a dependence graph, where the degree vector of a node\ndetermines the number of other nodes it can potentially cause to fail in each\nsystem through aforementioned dependency. In particular, we examine the impact\nof the variability and dependence properties of node degrees on the probability\nof cascading failures. We show that larger variability in node degrees hampers\nwidespread failures in the system, starting with random failures. Similarly,\npositive correlations in node degrees make it harder to set off an epidemic of\nfailures, thereby rendering the system more robust against random failures. \n\n"}
{"id": "1702.00564", "contents": "Title: Modelling dependency completion in sentence comprehension as a Bayesian\n  hierarchical mixture process: A case study involving Chinese relative clauses Abstract: We present a case-study demonstrating the usefulness of Bayesian hierarchical\nmixture modelling for investigating cognitive processes. In sentence\ncomprehension, it is widely assumed that the distance between linguistic\nco-dependents affects the latency of dependency resolution: the longer the\ndistance, the longer the retrieval time (the distance-based account). An\nalternative theory, direct-access, assumes that retrieval times are a mixture\nof two distributions: one distribution represents successful retrievals (these\nare independent of dependency distance) and the other represents an initial\nfailure to retrieve the correct dependent, followed by a reanalysis that leads\nto successful retrieval. We implement both models as Bayesian hierarchical\nmodels and show that the direct-access model explains Chinese relative clause\nreading time data better than the distance account. \n\n"}
{"id": "1702.00900", "contents": "Title: Scheduling and Power Allocation in Self-Backhauled Full Duplex Small\n  Cells Abstract: Full duplex (FD) communications, which increases spectral efficiency through\nsimultaneous transmission and reception on the same frequency band, is a\npromising technology to meet the demand of next generation wireless networks.\nIn this paper, we consider the application of such FD communication to\nself-backhauled small cells. We consider a FD capable small cell base station\n(BS) being wirelessly backhauled by a FD capable macro-cell BS. FD\ncommunication enables simultaneous backhaul and access transmissions at small\ncell BSs, which reduces the need to orthogonalize allocated spectrum between\naccess and backhaul. However, in such simultaneous operations, all the links\nexperience higher interference, which significantly suppresses the gains of FD\noperations. We propose an interference-aware scheduling method to maximize the\nFD gain across multiple UEs in both uplink and downlink directions, while\nmaintaining a level of fairness between all UEs. It jointly schedules the\nappropriate links and traffic based on the back-pressure algorithm, and\nallocates appropriate transmission powers to the scheduled links using\nGeometric Programming. Our simulation results show that the proposed scheduler\nnearly doubles the throughput of small cells compared to traditional\nhalf-duplex self-backhauling. \n\n"}
{"id": "1702.01995", "contents": "Title: Reducing Storage of Global Wind Ensembles with Stochastic Generators Abstract: Wind has the potential to make a significant contribution to future energy\nresources. Locating the sources of this renewable energy on a global scale is\nhowever extremely challenging, given the difficulty to store very large data\nsets generated by modern computer models. We propose a statistical model that\naims at reproducing the data-generating mechanism of an ensemble of runs via a\nStochastic Generator (SG) of global annual wind data. We introduce an\nevolutionary spectrum approach with spatially varying parameters based on\nlarge-scale geographical descriptors such as altitude to better account for\ndifferent regimes across the Earth's orography. We consider a multi-step\nconditional likelihood approach to estimate the parameters that explicitly\naccounts for nonstationary features while also balancing memory storage and\ndistributed computation. We apply the proposed model to more than 18 million\npoints of yearly global wind speed. The proposed SG requires orders of\nmagnitude less storage for generating surrogate ensemble members from wind than\ndoes creating additional wind fields from the climate model, even if an\neffective lossy data compression algorithm is applied to the simulation output. \n\n"}
{"id": "1702.03762", "contents": "Title: An integrate-and-fire model to generate spike trains with long-range\n  dependence Abstract: Long-range dependence (LRD) has been observed in a variety of phenomena in\nnature, and for several years also in the spiking activity of neurons. Often,\nthis is interpreted as originating from a non-Markovian system. Here we show\nthat a purely Markovian integrate-and-fire (IF) model, with a noisy slow\nadaptation term, can generate interspike intervals (ISIs) that appear as having\nLRD. However a proper analysis shows that this is not the case asymptotically.\nFor comparison, we also consider a new model of individual IF neuron with\nfractional (non-Markovian) noise. The correlations of its spike trains are\nstudied and proven to have LRD, unlike classical IF models. On the other hand,\nto correctly measure long-range dependence, it is usually necessary to know if\nthe data are stationary. Thus, a methodology to evaluate stationarity of the\nISIs is presented and applied to the various IF models. We explain that\nMarkovian IF models may seem to have LRD because of non-stationarities. \n\n"}
{"id": "1702.04239", "contents": "Title: Production of Entanglement Entropy by Decoherence Abstract: We examine the dynamics of entanglement entropy of all parts in an open\nsystem consisting of a two-level dimer interacting with an environment of\noscillators. The dimer-environment interaction is almost energy conserving. We\nfind the precise link between decoherence and production of entanglement\nentropy. We show that not all environment oscillators carry significant\nentanglement entropy and we identify the oscillator frequency regions which\ncontribute to the production of entanglement entropy. Our results hold for\narbitrary strengths of the dimer-environment interaction, and they are\nmathematically rigorous. \n\n"}
{"id": "1702.04936", "contents": "Title: Performance Analysis of Dense Small Cell Networks with Generalized\n  Fading Abstract: In this paper, we propose a unified framework to analyze the performance of\ndense small cell networks (SCNs) in terms of the coverage probability and the\narea spectral efficiency (ASE). In our analysis, we consider a practical path\nloss model that accounts for both non-line-of-sight (NLOS) and line-of-sight\n(LOS) transmissions. Furthermore, we adopt a generalized fading model, in which\nRayleigh fading, Rician fading and Nakagami-m fading can be treated in a\nunified framework. The analytical results of the coverage probability and the\nASE are derived, using a generalized stochastic geometry analysis. Different\nfrom existing work that does not differentiate NLOS and LOS transmissions, our\nresults show that NLOS and LOS transmissions have a significant impact on the\ncoverage probability and the ASE performance, particularly when the SCNs grow\ndense. Furthermore, our results establish for the first time that the\nperformance of the SCNs can be divided into four regimes, according to the\nintensity (aka density) of BSs, where in each regime the performance is\ndominated by different factors. \n\n"}
{"id": "1702.05197", "contents": "Title: Throughput-Optimal Broadcast in Wireless Networks with\n  Point-to-Multipoint Transmissions Abstract: We consider the problem of efficient packet dissemination in wireless\nnetworks with point-to-multi-point wireless broadcast channels. We propose a\ndynamic policy, which achieves the broadcast capacity of the network. This\npolicy is obtained by first transforming the original multi-hop network into a\nprecedence-relaxed virtual single-hop network and then finding an optimal\nbroadcast policy for the relaxed network. The resulting policy is shown to be\nthroughput-optimal for the original wireless network using a sample-path\nargument. We also prove the NP-completeness of the finite-horizon broadcast\nproblem, which is in contrast with the polynomial time solvability of the\nproblem with point-to-point channels. Illustrative simulation results\ndemonstrate the efficacy of the proposed broadcast policy in achieving the full\nbroadcast capacity with low delay. \n\n"}
{"id": "1702.05732", "contents": "Title: Low-dose cryo electron ptychography via non-convex Bayesian optimization Abstract: Electron ptychography has seen a recent surge of interest for phase sensitive\nimaging at atomic or near-atomic resolution. However, applications are so far\nmainly limited to radiation-hard samples because the required doses are too\nhigh for imaging biological samples at high resolution. We propose the use of\nnon-convex, Bayesian optimization to overcome this problem and reduce the dose\nrequired for successful reconstruction by two orders of magnitude compared to\nprevious experiments. We suggest to use this method for imaging single\nbiological macromolecules at cryogenic temperatures and demonstrate 2D\nsingle-particle reconstructions from simulated data with a resolution of 7.9\n\\AA$\\,$ at a dose of 20 $e^- / \\AA^2$. When averaging over only 15 low-dose\ndatasets, a resolution of 4 \\AA$\\,$ is possible for large macromolecular\ncomplexes. With its independence from microscope transfer function, direct\nrecovery of phase contrast and better scaling of signal-to-noise ratio,\ncryo-electron ptychography may become a promising alternative to Zernike\nphase-contrast microscopy. \n\n"}
{"id": "1702.06512", "contents": "Title: Semiparametric panel data models using neural networks Abstract: This paper presents an estimator for semiparametric models that uses a\nfeed-forward neural network to fit the nonparametric component. Unlike many\nmethodologies from the machine learning literature, this approach is suitable\nfor longitudinal/panel data. It provides unbiased estimation of the parametric\ncomponent of the model, with associated confidence intervals that have\nnear-nominal coverage rates. Simulations demonstrate (1) efficiency, (2) that\nparametric estimates are unbiased, and (3) coverage properties of estimated\nintervals. An application section demonstrates the method by predicting\ncounty-level corn yield using daily weather data from the period 1981-2015,\nalong with parametric time trends representing technological change. The method\nis shown to out-perform linear methods such as OLS and ridge/lasso, as well as\nrandom forest. The procedures described in this paper are implemented in the R\npackage panelNNET. \n\n"}
{"id": "1702.06772", "contents": "Title: Efficient CSMA using Regional Free Energy Approximations Abstract: CSMA (Carrier Sense Multiple Access) algorithms based on Gibbs sampling can\nachieve throughput optimality if certain parameters called the fugacities are\nappropriately chosen. However, the problem of computing these fugacities is\nNP-hard. In this work, we derive estimates of the fugacities by using a\nframework called the regional free energy approximations. In particular, we\nderive explicit expressions for approximate fugacities corresponding to any\nfeasible service rate vector. We further prove that our approximate fugacities\nare exact for the class of chordal graphs. A distinguishing feature of our work\nis that the regional approximations that we propose are tailored to conflict\ngraphs with small cycles, which is a typical characteristic of wireless\nnetworks. Numerical results indicate that the fugacities obtained by the\nproposed method are quite accurate and significantly outperform the existing\nBethe approximation based techniques. \n\n"}
{"id": "1702.07981", "contents": "Title: BayCount: A Bayesian Decomposition Method for Inferring Tumor\n  Heterogeneity using RNA-Seq Counts Abstract: Tumor is heterogeneous - a tumor sample usually consists of a set of\nsubclones with distinct transcriptional profiles and potentially different\ndegrees of aggressiveness and responses to drugs. Understanding tumor\nheterogeneity is therefore critical to precise cancer prognosis and treatment.\nIn this paper, we introduce BayCount, a Bayesian decomposition method to infer\ntumor heterogeneity with highly over-dispersed RNA sequencing count data. Using\nnegative binomial factor analysis, BayCount takes into account both the\nbetween-sample and gene-specific random effects on raw counts of sequencing\nreads mapped to each gene. For posterior inference, we develop an efficient\ncompound Poisson based blocked Gibbs sampler. Through extensive simulation\nstudies and analysis of The Cancer Genome Atlas lung cancer and kidney cancer\nRNA sequencing count data, we show that BayCount is able to accurately estimate\nthe number of subclones, the proportions of these subclones in each tumor\nsample, and the gene expression profiles in each subclone. Our method\nrepresents the first effort in characterizing tumor heterogeneity using RNA\nsequencing count data that simultaneously removes the need of normalizing the\ncounts, achieves statistical robustness, and obtains biologically and\nclinically meaningful insights. \n\n"}
{"id": "1702.08449", "contents": "Title: First Data Release of the Hyper Suprime-Cam Subaru Strategic Program Abstract: The Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP) is a three-layered\nimaging survey aimed at addressing some of the most outstanding questions in\nastronomy today, including the nature of dark matter and dark energy. The\nsurvey has been awarded 300 nights of observing time at the Subaru Telescope\nand it started in March 2014. This paper presents the first public data release\nof HSC-SSP. This release includes data taken in the first 1.7 years of\nobservations (61.5 nights) and each of the Wide, Deep, and UltraDeep layers\ncovers about 108, 26, and 4 square degrees down to depths of i~26.4, ~26.5, and\n~27.0 mag, respectively (5sigma for point sources). All the layers are observed\nin five broad bands (grizy), and the Deep and UltraDeep layers are observed in\nnarrow bands as well. We achieve an impressive image quality of 0.6 arcsec in\nthe i-band in the Wide layer. We show that we achieve 1-2 per cent PSF\nphotometry (rms) both internally and externally (against Pan-STARRS1), and ~10\nmas and 40 mas internal and external astrometric accuracy, respectively. Both\nthe calibrated images and catalogs are made available to the community through\ndedicated user interfaces and database servers. In addition to the pipeline\nproducts, we also provide value-added products such as photometric redshifts\nand a collection of public spectroscopic redshifts. Detailed descriptions of\nall the data can be found online. The data release website is\nhttps://hsc-release.mtk.nao.ac.jp/. \n\n"}
{"id": "1703.00056", "contents": "Title: Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments Abstract: Recidivism prediction instruments (RPI's) provide decision makers with an\nassessment of the likelihood that a criminal defendant will reoffend at a\nfuture point in time. While such instruments are gaining increasing popularity\nacross the country, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses several fairness criteria that have recently\nbeen applied to assess the fairness of recidivism prediction instruments. We\ndemonstrate that the criteria cannot all be simultaneously satisfied when\nrecidivism prevalence differs across groups. We then show how disparate impact\ncan arise when a recidivism prediction instrument fails to satisfy the\ncriterion of error rate balance. \n\n"}
{"id": "1703.00154", "contents": "Title: Inertial Odometry on Handheld Smartphones Abstract: Building a complete inertial navigation system using the limited quality data\nprovided by current smartphones has been regarded challenging, if not\nimpossible. This paper shows that by careful crafting and accounting for the\nweak information in the sensor samples, smartphones are capable of pure\ninertial navigation. We present a probabilistic approach for orientation and\nuse-case free inertial odometry, which is based on double-integrating rotated\naccelerations. The strength of the model is in learning additive and\nmultiplicative IMU biases online. We are able to track the phone position,\nvelocity, and pose in real-time and in a computationally lightweight fashion by\nsolving the inference with an extended Kalman filter. The information fusion is\ncompleted with zero-velocity updates (if the phone remains stationary),\naltitude correction from barometric pressure readings (if available), and\npseudo-updates constraining the momentary speed. We demonstrate our approach\nusing an iPad and iPhone in several indoor dead-reckoning applications and in a\nmeasurement tool setup. \n\n"}
{"id": "1703.00428", "contents": "Title: Repair Strategies for Storage on Mobile Clouds Abstract: We study the data reliability problem for a community of devices forming a\nmobile cloud storage system. We consider the application of regenerating codes\nfor file maintenance within a geographically-limited area. Such codes require\nlower bandwidth to regenerate lost data fragments compared to file replication\nor reconstruction. We investigate threshold-based repair strategies where data\nrepair is initiated after a threshold number of data fragments have been lost\ndue to node mobility. We show that at a low departure-to-repair rate regime, a\nlazy repair strategy in which repairs are initiated after several nodes have\nleft the system outperforms eager repair in which repairs are initiated after a\nsingle departure. This optimality is reversed when nodes are highly mobile. We\nfurther compare distributed and centralized repair strategies and derive the\noptimal repair threshold for minimizing the average repair cost per unit of\ntime, as a function of underlying code parameters. In addition, we examine\ncooperative repair strategies and show performance improvements compared to\nnon-cooperative codes. We investigate several models for the time needed for\nnode repair including a simple fixed time model that allows for the computation\nof closed-form expressions and a more realistic model that takes into account\nthe number of repaired nodes. We derive the conditions under which the former\nmodel approximates the latter. Finally, an extended model where additional\nfailures are allowed during the repair process is investigated. Overall, our\nresults establish the joint effect of code design and repair algorithms on the\nmaintenance cost of distributed storage systems. \n\n"}
{"id": "1703.00654", "contents": "Title: Nonparametric estimation of galaxy cluster's emissivity and point source\n  detection in astrophysics with two lasso penalties Abstract: Astrophysicists are interested in recovering the 3D gas emissivity of a\ngalaxy cluster from a 2D image taken by a telescope. A blurring phenomenon and\npresence of point sources make this inverse problem even harder to solve. The\ncurrent state-of-the-art technique is two step: first identify the location of\npotential point sources, then mask these locations and deproject the data.\n  We instead model the data as a Poisson generalized linear model (involving\nblurring, Abel and wavelets operators) regularized by two lasso penalties to\ninduce sparse wavelet representation and sparse point sources. The amount of\nsparsity is controlled by two quantile universal thresholds. As a result, our\nmethod outperforms the existing one. \n\n"}
{"id": "1703.01038", "contents": "Title: Ultra-Dense Edge Caching under Spatio-Temporal Demand and Network\n  Dynamics Abstract: This paper investigates a cellular edge caching design under an extremely\nlarge number of small base stations (SBSs) and users. In this ultra-dense edge\ncaching network (UDCN), SBS-user distances shrink, and each user can request a\ncached content from multiple SBSs. Unfortunately, the complexity of existing\ncaching controls' mechanisms increases with the number of SBSs, making them\ninapplicable for solving the fundamental caching problem: How to maximize local\ncaching gain while minimizing the replicated content caching? Furthermore,\nspatial dynamics of interference is no longer negligible in UDCNs due to the\nsurge in interference. In addition, the caching control should consider\ntemporal dynamics of user demands. To overcome such difficulties, we propose a\nnovel caching algorithm weaving together notions of mean-field game theory and\nstochastic geometry. These enable our caching algorithm to become independent\nof the number of SBSs and users, while incorporating spatial interference\ndynamics as well as temporal dynamics of content popularity and storage\nconstraints. Numerical evaluation validates the fact that the proposed\nalgorithm reduces not only the long run average cost by at least 24% but also\nthe number of replicated content by 56% compared to a popularity-based\nalgorithm. \n\n"}
{"id": "1703.02441", "contents": "Title: Statistical Analysis of the Ricker Model Abstract: The Ricker model was introduced in the context of managing fishing stocks. It\nis a discrete non-linear iterative model given by $N(t+1)=rN(t)\\exp(-N(t))$\nwhere $N(t)$ is the population at time $t$. The model treated in this paper\nincludes a random component $N(t+1)=rN(t)\\exp(-N(t)+\\varepsilon(t+1))$ and what\nis observed at time $t$ is a Poisson random variable with parameter $\\varphi\nN(t)$. Such a model has been analysed using `synthetic likelihood' and ABC\n(Approximate Bayesian Computation). In contrast this paper takes a\nnon-likelihood approach and treats the model in a consistent manner as an\napproximation. The goal is to specify those parameter values if any which are\nconsistent with the data. \n\n"}
{"id": "1703.02870", "contents": "Title: Statistical Inference in Political Networks Research Abstract: Researchers interested in statistically modeling network data have a\nwell-established and quickly growing set of approaches from which to choose.\nSeveral of these methods have been regularly applied in research on political\nnetworks, while others have yet to permeate the field. Here, we review the most\nprominent methods of inferential network analysis---both for cross-sectionally\nand longitudinally observed networks including (temporal) exponential random\ngraph models, latent space models, the quadratic assignment procedure, and\nstochastic actor oriented models. For each method, we summarize its analytic\nform, identify prominent published applications in political science and\ndiscuss computational considerations. We conclude with a set of guidelines for\nselecting a method for a given application. \n\n"}
{"id": "1703.02991", "contents": "Title: The third data release of the Kilo-Degree Survey and associated data\n  products Abstract: The Kilo-Degree Survey (KiDS) is an ongoing optical wide-field imaging survey\nwith the OmegaCAM camera at the VLT Survey Telescope. It aims to image 1500\nsquare degrees in four filters (ugri). The core science driver is mapping the\nlarge-scale matter distribution in the Universe, using weak lensing shear and\nphotometric redshift measurements. Further science cases include galaxy\nevolution, Milky Way structure, detection of high-redshift clusters, and\nfinding rare sources such as strong lenses and quasars. Here we present the\nthird public data release (DR3) and several associated data products, adding\nfurther area, homogenized photometric calibration, photometric redshifts and\nweak lensing shear measurements to the first two releases. A dedicated pipeline\nembedded in the Astro-WISE information system is used for the production of the\nmain release. Modifications with respect to earlier releases are described in\ndetail. Photometric redshifts have been derived using both Bayesian template\nfitting, and machine-learning techniques. For the weak lensing measurements,\noptimized procedures based on the THELI data reduction and lensfit shear\nmeasurement packages are used. In DR3 stacked ugri images, weight maps, masks,\nand source lists for 292 new survey tiles (~300 sq.deg) are made available. The\nmulti-band catalogue, including homogenized photometry and photometric\nredshifts, covers the combined DR1, DR2 and DR3 footprint of 440 survey tiles\n(447 sq.deg). Limiting magnitudes are typically 24.3, 25.1, 24.9, 23.8 (5 sigma\nin a 2 arcsec aperture) in ugri, respectively, and the typical r-band PSF size\nis less than 0.7 arcsec. The photometric homogenization scheme ensures accurate\ncolors and an absolute calibration stable to ~2% for gri and ~3% in u.\nSeparately released are a weak lensing shear catalogue and photometric\nredshifts based on two different machine-learning techniques. \n\n"}
{"id": "1703.03853", "contents": "Title: TreeClone: Reconstruction of Tumor Subclone Phylogeny Based on Mutation\n  Pairs using Next Generation Sequencing Data Abstract: We present TreeClone, a latent feature allocation model to reconstruct tumor\nsubclones subject to phylogenetic evolution that mimics tumor evolution.\nSimilar to most current methods, we consider data from next-generation\nsequencing of tumor DNA. Unlike most methods that use information in short\nreads mapped to single nucleotide variants (SNVs), we consider subclone\nphylogeny reconstruction using pairs of two proximal SNVs that can be mapped by\nthe same short reads. As part of the Bayesian inference model, we construct a\nphylogenetic tree prior. The use of the tree structure in the prior greatly\nstrengthens inference. Only subclones that can be explained by a phylogenetic\ntree are assigned non-negligible probabilities. The proposed Bayesian framework\nimplies posterior distributions on the number of subclones, their genotypes,\ncellular proportions, and the phylogenetic tree spanned by the inferred\nsubclones. The proposed method is validated against different sets of simulated\nand real-world data using single and multiple tumor samples. An open source\nsoftware package is available at http://www.compgenome.org/treeclone. \n\n"}
{"id": "1703.04642", "contents": "Title: Robust Morphometric Analysis based on Landmarks. Applications Abstract: Procrustes Analysis is a Morphometric method based on Configurations of\nLandmarks that estimates the superimposition parameters by least-squares; for\nthis reason, the procedure is very sensitive to outliers. In the first part of\nthe paper we robustify this technique to classify individuals from a\ndescriptive point of view. In the literature there are also classical results,\nbased on the normality of the observations, to test whether there are\nsignificant differences between individuals. In the second part of the paper we\ndetermine a Von Mises plus Saddlepoint approximation for the tail probability\nof the Procrustes Statistic when the observations come from a model close to\nthe normal. We conclude the paper with some applications using the Geographical\nInformation System QGIS. \n\n"}
{"id": "1703.06946", "contents": "Title: SCALPEL: Extracting Neurons from Calcium Imaging Data Abstract: In the past few years, new technologies in the field of neuroscience have\nmade it possible to simultaneously image activity in large populations of\nneurons at cellular resolution in behaving animals. In mid-2016, a huge\nrepository of this so-called \"calcium imaging\" data was made\npublicly-available. The availability of this large-scale data resource opens\nthe door to a host of scientific questions, for which new statistical methods\nmust be developed.\n  In this paper, we consider the first step in the analysis of calcium imaging\ndata: namely, identifying the neurons in a calcium imaging video. We propose a\ndictionary learning approach for this task. First, we perform image\nsegmentation to develop a dictionary containing a huge number of candidate\nneurons. Next, we refine the dictionary using clustering. Finally, we apply the\ndictionary in order to select neurons and estimate their corresponding activity\nover time, using a sparse group lasso optimization problem. We apply our\nproposal to three calcium imaging data sets.\n  Our proposed approach is implemented in the R package scalpel, which is\navailable on CRAN. \n\n"}
{"id": "1703.06957", "contents": "Title: Sample size re-estimation incorporating prior information on a nuisance\n  parameter Abstract: Prior information is often incorporated informally when planning a clinical\ntrial. Here, we present an approach on how to incorporate prior information,\nsuch as data from historical clinical trials, into the nuisance parameter based\nsample size re-estimation in a design with an internal pilot study. We focus on\ntrials with continuous endpoints in which the outcome variance is the nuisance\nparameter. For planning and analyzing the trial frequentist methods are\nconsidered. Moreover, the external information on the variance is summarized by\nthe Bayesian meta-analytic-predictive (MAP) approach. To incorporate external\ninformation into the sample size re-estimation, we propose to update the MAP\nprior based on the results of the internal pilot study and to re-estimate the\nsample size using an estimator from the posterior. By means of a simulation\nstudy, we compare the operating characteristics such as power and sample size\ndistribution of the proposed procedure with the traditional sample size\nre-estimation approach which uses the pooled variance estimator. The simulation\nstudy shows that, if no prior-data conflict is present, incorporating external\ninformation into the sample size re-estimation improves the operating\ncharacteristics compared to the traditional approach. In the case of a\nprior-data conflict, that is when the variance of the ongoing clinical trial is\nunequal to the prior location, the performance of the traditional sample size\nre-estimation procedure is in general superior, even when the prior information\nis robustified. When considering to include prior information in sample size\nre-estimation, the potential gains should be balanced against the risks. \n\n"}
{"id": "1703.07137", "contents": "Title: MRI-based Surgical Planning for Lumbar Spinal Stenosis Abstract: The most common reason for spinal surgery in elderly patients is lumbar\nspinal stenosis(LSS). For LSS, treatment decisions based on clinical and\nradiological information as well as personal experience of the surgeon shows\nlarge variance. Thus a standardized support system is of high value for a more\nobjective and reproducible decision. In this work, we develop an automated\nalgorithm to localize the stenosis causing the symptoms of the patient in\nmagnetic resonance imaging (MRI). With 22 MRI features of each of five spinal\nlevels of 321 patients, we show it is possible to predict the location of\nlesion triggering the symptoms. To support this hypothesis, we conduct an\nautomated analysis of labeled and unlabeled MRI scans extracted from 788\npatients. We confirm quantitatively the importance of radiological information\nand provide an algorithmic pipeline for working with raw MRI scans. \n\n"}
{"id": "1703.07267", "contents": "Title: Open System Perspective on Incoherent Excitation of Light Harvesting\n  Systems Abstract: The nature of excited states of open quantum systems produced by incoherent\nnatural thermal light is analyzed based on a description of the generator of\nthe dynamics. Natural thermal light is shown to generate long-lasting coherent\ndynamics because of (i) the super-Ohmic character of the radiation, and (ii)\nthe absence of pure dephasing dynamics. In the presence of an environment, the\nlong-lasting coherences induced by suddenly turned-on incoherent light\ndissipate and stationary coherences are established. As a particular\napplication, dynamics in a subunit of the PC-645 light-harvesting complex is\nconsidered where it is further shown that aspects of the energy pathways\nlandscape depend on the nature of the exciting light and number of chromophores\nexcited. Specifically, pulsed laser and natural broadband incoherent excitation\ninduce significantly different energy transfer pathways. In addition, we\ndiscuss differences in perspective associated with the eigenstate vs site\nbasis, and note an important difference in the phase of system coherences when\ncoupled to blackbody radiation or when coupled to a phonon background. Finally,\nan Appendix contains an open systems example of the loss of coherence as the\nturn on time of the light assumes natural time scales. \n\n"}
{"id": "1703.08487", "contents": "Title: Multiscale Granger causality Abstract: In the study of complex physical and biological systems represented by\nmultivariate stochastic processes, an issue of great relevance is the\ndescription of the system dynamics spanning multiple temporal scales. While\nmethods to assess the dynamic complexity of individual processes at different\ntime scales are well-established, multiscale analysis of directed interactions\nhas never been formalized theoretically, and empirical evaluations are\ncomplicated by practical issues such as filtering and downsampling. Here we\nextend the very popular measure of Granger causality (GC), a prominent tool for\nassessing directed lagged interactions between joint processes, to quantify\ninformation transfer across multiple time scales. We show that the multiscale\nprocessing of a vector autoregressive (AR) process introduces a moving average\n(MA) component, and describe how to represent the resulting ARMA process using\nstate space (SS) models and to combine the SS model parameters for computing\nexact GC values at arbitrarily large time scales. We exploit the theoretical\nformulation to identify peculiar features of multiscale GC in basic AR\nprocesses, and demonstrate with numerical simulations the much larger\nestimation accuracy of the SS approach compared with pure AR modeling of\nfiltered and downsampled data. The improved computational reliability is\nexploited to disclose meaningful multiscale patterns of information transfer\nbetween global temperature and carbon dioxide concentration time series, both\nin paleoclimate and in recent years. \n\n"}
{"id": "1703.08985", "contents": "Title: TCP in 5G mmWave Networks: Link Level Retransmissions and MP-TCP Abstract: MmWave communications, one of the cornerstones of future 5G mobile networks,\nare characterized at the same time by a potential multi-gigabit capacity and by\na very dynamic channel, sensitive to blockage, wide fluctuations in the\nreceived signal quality, and possibly also sudden link disruption. While the\nperformance of physical and MAC layer schemes that address these issues has\nbeen thoroughly investigated in the literature, the complex interactions\nbetween mmWave links and transport layer protocols such as TCP are still\nrelatively unexplored. This paper uses the ns-3 mmWave module, with its channel\nmodel based on real measurements in New York City, to analyze the performance\nof the Linux TCP/IP stack (i) with and without link-layer retransmissions,\nshowing that they are fundamental to reach a high TCP throughput on mmWave\nlinks and (ii) with Multipath TCP (MP-TCP) over multiple LTE and mmWave links,\nillustrating which are the throughput-optimal combinations of secondary paths\nand congestion control algorithms in different conditions. \n\n"}
{"id": "1703.09701", "contents": "Title: Sampling Errors in Nested Sampling Parameter Estimation Abstract: Sampling errors in nested sampling parameter estimation differ from those in\nBayesian evidence calculation, but have been little studied in the literature.\nThis paper provides the first explanation of the two main sources of sampling\nerrors in nested sampling parameter estimation, and presents a new diagrammatic\nrepresentation for the process. We find no current method can accurately\nmeasure the parameter estimation errors of a single nested sampling run, and\npropose a method for doing so using a new algorithm for dividing nested\nsampling runs. We empirically verify our conclusions and the accuracy of our\nnew method. \n\n"}
{"id": "1703.09710", "contents": "Title: Fast and scalable Gaussian process modeling with applications to\n  astronomical time series Abstract: The growing field of large-scale time domain astronomy requires methods for\nprobabilistic data analysis that are computationally tractable, even with large\ndatasets. Gaussian Processes are a popular class of models used for this\npurpose but, since the computational cost scales, in general, as the cube of\nthe number of data points, their application has been limited to small\ndatasets. In this paper, we present a novel method for Gaussian Process\nmodeling in one-dimension where the computational requirements scale linearly\nwith the size of the dataset. We demonstrate the method by applying it to\nsimulated and real astronomical time series datasets. These demonstrations are\nexamples of probabilistic inference of stellar rotation periods, asteroseismic\noscillation spectra, and transiting planet parameters. The method exploits\nstructure in the problem when the covariance function is expressed as a mixture\nof complex exponentials, without requiring evenly spaced observations or\nuniform noise. This form of covariance arises naturally when the process is a\nmixture of stochastically-driven damped harmonic oscillators -- providing a\nphysical motivation for and interpretation of this choice -- but we also\ndemonstrate that it can be a useful effective model in some other cases. We\npresent a mathematical description of the method and compare it to existing\nscalable Gaussian Process methods. The method is fast and interpretable, with a\nrange of potential applications within astronomical data analysis and beyond.\nWe provide well-tested and documented open-source implementations of this\nmethod in C++, Python, and Julia. \n\n"}
{"id": "1703.10500", "contents": "Title: Free Energy Approximations for CSMA networks Abstract: In this paper we study how to estimate the back-off rates in an idealized\nCSMA network consisting of $n$ links to achieve a given throughput vector using\nfree energy approximations. More specifically, we introduce the class of\nregion-based free energy approximations with clique belief and present a closed\nform expression for the back-off rates based on the zero gradient points of the\nfree energy approximation (in terms of the conflict graph, target throughput\nvector and counting numbers). Next we introduce the size $k_{max}$ clique free\nenergy approximation as a special case and derive an explicit expression for\nthe counting numbers, as well as a recursion to compute the back-off rates. We\nsubsequently show that the size $k_{max}$ clique approximation coincides with a\nKikuchi free energy approximation and prove that it is exact on chordal\nconflict graphs when $k_{max} = n$. As a by-product these results provide us\nwith an explicit expression of a fixed point of the inverse generalized belief\npropagation algorithm for CSMA networks. Using numerical experiments we compare\nthe accuracy of the novel approximation method with existing methods. \n\n"}
{"id": "1704.00399", "contents": "Title: Ultra-Dense Networks: Is There a Limit to Spatial Spectrum Reuse? Abstract: The aggressive spatial spectrum reuse (SSR) by network densification using\nsmaller cells has successfully driven the wireless communication industry\nonward in the past decades. In our future journey toward ultra-dense networks\n(UDNs), a fundamental question needs to be answered. Is there a limit to SSR?\nIn other words, when we deploy thousands or millions of small cell base\nstations (BSs) per square kilometer, is activating all BSs on the same\ntime/frequency resource the best strategy? In this paper, we present\ntheoretical analyses to answer such question. In particular, we find that both\nthe signal and interference powers become bounded in practical UDNs with a\nnon-zero BS-to-UE antenna height difference and a finite UE density, which\nleads to a constant capacity scaling law. As a result, there exists an optimal\nSSR density that can maximize the network capacity. Hence, the limit to SSR\nshould be considered in the operation of future UDNs. \n\n"}
{"id": "1704.01207", "contents": "Title: Bayesian Model Averaging for the X-Chromosome Inactivation Dilemma in\n  Genetic Association Study Abstract: X-chromosome is often excluded from the so called `whole-genome' association\nstudies due to its intrinsic difference between males and females. One\nparticular analytical challenge is the unknown status of X-inactivation, where\none of the two X-chromosome variants in females may be randomly selected to be\nsilenced. In the absence of biological evidence in favour of one specific\nmodel, we consider a Bayesian model averaging framework that offers a\nprincipled way to account for the inherent model uncertainty, providing model\naveraging-based posterior density intervals and Bayes factors. We examine the\ninferential properties of the proposed methods via extensive simulation\nstudies, and we apply the methods to a genetic association study of an\nintestinal disease occurring in about twenty percent of Cystic Fibrosis\npatients. Compared with the results previously reported assuming the presence\nof inactivation, we show that the proposed Bayesian methods provide more\nfeature-rich quantities that are useful in practice. \n\n"}
{"id": "1704.01795", "contents": "Title: Structure-Dynamics Relation in Physically-Plausible Multi-Chromophore\n  Systems Abstract: We study a large number of physically-plausible arrangements of chromophores,\ngenerated via a computational method involving stochastic real-space\ntransformations of a naturally occurring `reference' structure, illustrating\nour methodology using the well-studied Fenna-Matthews-Olson complex (FMO). To\nexplore the idea that the natural structure has been tuned for efficient energy\ntransport we use an atomic transition charge method to calculate the excitonic\ncouplings of each generated structure and a Lindblad master equation to study\nthe quantum transport of an exciton from a `source' to a `drain' chromophore.\nWe find significant correlations between structure and transport efficiency:\nHigh-performing structures tend to be more compact and, among those, the best\nstructures display a certain orientation of the chromophores, particularly the\nchromophore closest to the source-to-drain vector. We conclude that, subject to\nreasonable, physically-motivated constraints, the FMO complex is highly attuned\nto the purpose of energy transport, partly by exploiting these structural\nmotifs. \n\n"}
{"id": "1704.03360", "contents": "Title: Redistricting: Drawing the Line Abstract: We develop methods to evaluate whether a political districting accurately\nrepresents the will of the people. To explore and showcase our ideas, we\nconcentrate on the congressional districts for the U.S. House of\nrepresentatives and use the state of North Carolina and its redistrictings\nsince the 2010 census. Using a Monte Carlo algorithm, we randomly generate over\n24,000 redistrictings that are non-partisan and adhere to criteria from\nproposed legislation. Applying historical voting data to these random\nredistrictings, we find that the number of democratic and republican\nrepresentatives elected varies drastically depending on how districts are\ndrawn. Some results are more common, and we gain a clear range of expected\nelection outcomes. Using the statistics of our generated redistrictings, we\ncritique the particular congressional districtings used in the 2012 and 2016 NC\nelections as well as a districting proposed by a bipartisan redistricting\ncommission. We find that the 2012 and 2016 districtings are highly atypical and\nnot representative of the will of the people. On the other hand, our results\nindicate that a plan produced by a bipartisan panel of retired judges is highly\ntypical and representative. Since our analyses are based on an ensemble of\nreasonable redistrictings of North Carolina, they provide a baseline for a\ngiven election which incorporates the geometry of the state's population\ndistribution. \n\n"}
{"id": "1704.04465", "contents": "Title: A Low-Complexity Approach to Distributed Cooperative Caching with\n  Geographic Constraints Abstract: We consider caching in cellular networks in which each base station is\nequipped with a cache that can store a limited number of files. The popularity\nof the files is known and the goal is to place files in the caches such that\nthe probability that a user at an arbitrary location in the plane will find the\nfile that she requires in one of the covering caches is maximized.\n  We develop distributed asynchronous algorithms for deciding which contents to\nstore in which cache. Such cooperative algorithms require communication only\nbetween caches with overlapping coverage areas and can operate in asynchronous\nmanner. The development of the algorithms is principally based on an\nobservation that the problem can be viewed as a potential game. Our basic\nalgorithm is derived from the best response dynamics. We demonstrate that the\ncomplexity of each best response step is independent of the number of files,\nlinear in the cache capacity and linear in the maximum number of base stations\nthat cover a certain area. Then, we show that the overall algorithm complexity\nfor a discrete cache placement is polynomial in both network size and catalog\nsize. In practical examples, the algorithm converges in just a few iterations.\nAlso, in most cases of interest, the basic algorithm finds the best Nash\nequilibrium corresponding to the global optimum. We provide two extensions of\nour basic algorithm based on stochastic and deterministic simulated annealing\nwhich find the global optimum.\n  Finally, we demonstrate the hit probability evolution on real and synthetic\nnetworks numerically and show that our distributed caching algorithm performs\nsignificantly better than storing the most popular content, probabilistic\ncontent placement policy and Multi-LRU caching policies. \n\n"}
{"id": "1704.05125", "contents": "Title: Performance Impact of Base Station Antenna Heights in Dense Cellular\n  Networks Abstract: In this paper, we present a new and significant theoretical discovery. If the\nabsolute height difference between base station (BS) antenna and user equipment\n(UE) antenna is larger than zero, then the network performance in terms of both\nthe coverage probability and the area spectral efficiency (ASE) will\ncontinuously decrease toward zero as the BS density increases for ultra-dense\n(UD) small cell networks (SCNs). Such findings are completely different from\nthe conclusions in existing works, both quantitatively and qualitatively. In\nparticular, this performance behavior has a tremendous impact on the deployment\nof UD SCNs in the 5th-generation (5G) era. Network operators may invest large\namounts of money in deploying more network infrastructure to only obtain an\neven less network capacity. Our study results reveal that one way to address\nthis issue is to lower the SCN BS antenna height to the UE antenna height.\nHowever, this requires a revolutionized approach of BS architecture and\ndeployment, which is explored in this paper too. \n\n"}
{"id": "1704.05858", "contents": "Title: The Hyper Suprime-Cam SSP Survey: Overview and Survey Design Abstract: Hyper Suprime-Cam (HSC) is a wide-field imaging camera on the prime focus of\nthe 8.2m Subaru telescope on the summit of Maunakea in Hawaii. A team of\nscientists from Japan, Taiwan and Princeton University is using HSC to carry\nout a 300-night multi-band imaging survey of the high-latitude sky. The survey\nincludes three layers: the Wide layer will cover 1400 deg$^2$ in five broad\nbands ($grizy$), with a $5\\,\\sigma$ point-source depth of $r \\approx 26$. The\nDeep layer covers a total of 26~deg$^2$ in four fields, going roughly a\nmagnitude fainter, while the UltraDeep layer goes almost a magnitude fainter\nstill in two pointings of HSC (a total of 3.5 deg$^2$). Here we describe the\ninstrument, the science goals of the survey, and the survey strategy and data\nprocessing. This paper serves as an introduction to a special issue of the\nPublications of the Astronomical Society of Japan, which includes a large\nnumber of technical and scientific papers describing results from the early\nphases of this survey. \n\n"}
{"id": "1704.05988", "contents": "Title: Photometric Redshifts for Hyper Suprime-Cam Subaru Strategic Program\n  Data Release 1 Abstract: Photometric redshifts are a key component of many science objectives in the\nHyper Suprime-Cam Subaru Strategic Program (HSC-SSP). In this paper, we\ndescribe and compare the codes used to compute photometric redshifts for\nHSC-SSP, how we calibrate them, and the typical accuracy we achieve with the\nHSC five-band photometry (grizy). We introduce a new point estimator based on\nan improved loss function and demonstrate that it works better than other\ncommonly used estimators. We find that our photo-z's are most accurate at\n0.2<~zphot<~1.5, where we can straddle the 4000A break. We achieve\nsigma(d_zphot/(1+zphot))~0.05 and an outlier rate of about 15% for galaxies\ndown to i=25 within this redshift range. If we limit to a brighter sample of\ni<24, we achieve sigma~0.04 and ~8% outliers. Our photo-z's should thus enable\nmany science cases for HSC-SSP. We also characterize the accuracy of our\nredshift probability distribution function (PDF) and discover that some codes\nover/under-estimate the redshift uncertainties, which have implications for\nN(z) reconstruction. Our photo-z products for the entire area in the Public\nData Release 1 are publicly available, and both our catalog products (such as\npoint estimates) and full PDFs can be retrieved from the data release site,\nhttps://hsc-release.mtk.nao.ac.jp/. \n\n"}
{"id": "1704.08248", "contents": "Title: Modeling and replicating statistical topology, and evidence for CMB\n  non-homogeneity Abstract: Under the banner of `Big Data', the detection and classification of structure\nin extremely large, high dimensional, data sets, is, one of the central\nstatistical challenges of our times. Among the most intriguing approaches to\nthis challenge is `TDA', or `Topological Data Analysis', one of the primary\naims of which is providing non-metric, but topologically informative,\npre-analyses of data sets which make later, more quantitative analyses\nfeasible. While TDA rests on strong mathematical foundations from Topology, in\napplications it has faced challenges due to an inability to handle issues of\nstatistical reliability and robustness and, most importantly, in an inability\nto make scientific claims with verifiable levels of statistical confidence. We\npropose a methodology for the parametric representation, estimation, and\nreplication of persistence diagrams, the main diagnostic tool of TDA. The power\nof the methodology lies in the fact that even if only one persistence diagram\nis available for analysis -- the typical case for big data applications --\nreplications can be generated to allow for conventional statistical hypothesis\ntesting. The methodology is conceptually simple and computationally practical,\nand provides a broadly effective statistical procedure for persistence diagram\nTDA analysis. We demonstrate the basic ideas on a toy example, and the power of\nthe approach in a novel and revealing analysis of CMB non-homogeneity. \n\n"}
{"id": "1704.08615", "contents": "Title: Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics Abstract: Dozens of new models on fixation prediction are published every year and\ncompared on open benchmarks such as MIT300 and LSUN. However, progress in the\nfield can be difficult to judge because models are compared using a variety of\ninconsistent metrics. Here we show that no single saliency map can perform well\nunder all metrics. Instead, we propose a principled approach to solve the\nbenchmarking problem by separating the notions of saliency models, maps and\nmetrics. Inspired by Bayesian decision theory, we define a saliency model to be\na probabilistic model of fixation density prediction and a saliency map to be a\nmetric-specific prediction derived from the model density which maximizes the\nexpected performance on that metric given the model density. We derive these\noptimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,\nNSS, CC, SIM, KL-Div) and show that they can be computed analytically or\napproximated with high precision. We show that this leads to consistent\nrankings in all metrics and avoids the penalties of using one saliency map for\nall metrics. Our method allows researchers to have their model compete on many\ndifferent metrics with state-of-the-art in those metrics: \"good\" models will\nperform well in all metrics. \n\n"}
{"id": "1705.01599", "contents": "Title: Characterization and Photometric Performance of the Hyper Suprime-Cam\n  Software Pipeline Abstract: The Subaru Strategic Program (SSP) is an ambitious multi-band survey using\nthe Hyper Suprime-Cam (HSC) on the Subaru telescope. The Wide layer of the SSP\nis both wide and deep, reaching a detection limit of i~26.0 mag. At these\ndepths, it is challenging to achieve accurate, unbiased, and consistent\nphotometry across all five bands. The HSC data are reduced using a pipeline\nthat builds on the prototype pipeline for the Large Synoptic Survey Telescope.\nWe have developed a Python-based, flexible framework to inject synthetic\ngalaxies into real HSC images called SynPipe. Here we explain the design and\nimplementation of SynPipe and generate a sample of synthetic galaxies to\nexamine the photometric performance of the HSC pipeline. For stars, we achieve\n1% photometric precision at i~19.0 mag and 6% precision at i~25.0 in the\ni-band. For synthetic galaxies with single-Sersic profiles, forced CModel\nphotometry achieves 13% photometric precision at i~20.0 mag and 18% precision\nat i~25.0 in the i-band. We show that both forced PSF and CModel photometry\nyield unbiased color estimates that are robust to seeing conditions. We\nidentify several caveats that apply to the version of HSC pipeline used for the\nfirst public HSC data release (DR1) that need to be taking into consideration.\nFirst, the degree to which an object is blended with other objects impacts the\noverall photometric performance. This is especially true for point sources.\nHighly blended objects tend to have larger photometric uncertainties,\nsystematically underestimated fluxes and slightly biased colors. Second, >20%\nof stars at 22.5< i < 25.0 mag can be misclassified as extended objects. Third,\nthe current CModel algorithm tends to strongly underestimate the half-light\nradius and ellipticity of galaxy with i>21.5 mag. \n\n"}
{"id": "1705.01727", "contents": "Title: Comparison of hidden Markov chain models and hidden Markov random field\n  models in estimation of computed tomography images Abstract: There is an interest to replace computed tomography (CT) images with magnetic\nresonance (MR) images for a number of diagnostic and therapeutic workflows. In\nthis article, predicting CT images from a number of magnetic resonance imaging\n(MRI) sequences using regression approach is explored. Two principal areas of\napplication for estimated CT images are dose calculations in MRI-based\nradiotherapy treatment planning and attenuation correction for positron\nemission tomography (PET)/MRI. The main purpose of this work is to investigate\nthe performance of hidden Markov (chain) models (HMMs) in comparison to hidden\nMarkov random field (HMRF) models when predicting CT images of head. Our study\nshows that HMMs have clear advantages over HMRF models in this particular\napplication. Obtained results suggest that HMMs deserve a further study for\ninvestigating their potential in modelling applications where the most natural\ntheoretical choice would be the class of HMRF models. \n\n"}
{"id": "1705.02441", "contents": "Title: Comments on `High-dimensional simultaneous inference with the bootstrap' Abstract: We provide comments on the article \"High-dimensional simultaneous inference\nwith the bootstrap\" by Ruben Dezeure, Peter Buhlmann and Cun-Hui Zhang. \n\n"}
{"id": "1705.03799", "contents": "Title: Model-based Computed Tomography Image Estimation: Partitioning Approach Abstract: There is a growing interest to get a fully MR based radiotherapy. The most\nimportant development needed is to obtain improved bone tissue estimation. The\nexisting model-based methods perform poorly on bone tissues. This paper was\naimed at obtaining improved bone tissue estimation. Skew Gaussian mixture model\nand Gaussian mixture model were proposed to investigate CT image estimation\nfrom MR images by partitioning the data into two major tissue types. The\nperformance of the proposed models was evaluated using leave-one-out\ncross-validation method on real data. In comparison with the existing\nmodel-based approaches, the model-based partitioning approach outperformed in\nbone tissue estimation, especially in dense bone tissue estimation. \n\n"}
{"id": "1705.05391", "contents": "Title: Optimal Rates and Tradeoffs in Multiple Testing Abstract: Multiple hypothesis testing is a central topic in statistics, but despite\nabundant work on the false discovery rate (FDR) and the corresponding Type-II\nerror concept known as the false non-discovery rate (FNR), a fine-grained\nunderstanding of the fundamental limits of multiple testing has not been\ndeveloped. Our main contribution is to derive a precise non-asymptotic tradeoff\nbetween FNR and FDR for a variant of the generalized Gaussian sequence model.\nOur analysis is flexible enough to permit analyses of settings where the\nproblem parameters vary with the number of hypotheses $n$, including various\nsparse and dense regimes (with $o(n)$ and $\\mathcal{O}(n)$ signals). Moreover,\nwe prove that the Benjamini-Hochberg algorithm as well as the Barber-Cand\\`{e}s\nalgorithm are both rate-optimal up to constants across these regimes. \n\n"}
{"id": "1705.05752", "contents": "Title: Limitations of design-based causal inference and A/B testing under\n  arbitrary and network interference Abstract: Randomized experiments on a network often involve interference between\nconnected units; i.e., a situation in which an individual's treatment can\naffect the response of another individual. Current approaches to deal with\ninterference, in theory and in practice, often make restrictive assumptions on\nits structure---for instance, assuming that interference is local---even when\nusing otherwise nonparametric inference strategies. This reliance on explicit\nrestrictions on the interference mechanism suggests a shared intuition that\ninference is impossible without any assumptions on the interference structure.\nIn this paper, we begin by formalizing this intuition in the context of a\nclassical nonparametric approach to inference, referred to as design-based\ninference of causal effects. Next, we show how, always in the context of\ndesign-based inference, even parametric structural assumptions that allow the\nexistence of unbiased estimators, cannot guarantee a decreasing variance even\nin the large sample limit. This lack of concentration in large samples is often\nobserved empirically, in randomized experiments in which interference of some\nform is expected to be present. This result has direct consequences for the\ndesign and analysis of large experiments---for instance, in online social\nplatforms---where the belief is that large sample sizes automatically guarantee\nsmall variance. More broadly, our results suggest that although strategies for\ncausal inference in the presence of interference borrow their formalism and\nmain concepts from the traditional causal inference literature, much of the\nintuition from the no-interference case do not easily transfer to the\ninterference setting. \n\n"}
{"id": "1705.06721", "contents": "Title: Examining collusion and voting biases between countries during the\n  Eurovision song contest since 1957 Abstract: The Eurovision Song Contest (ESC) is an annual event which attracts millions\nof viewers. It is an interesting activity to examine since the participants of\nthe competition represent a particular country's musical performance that will\nbe awarded a set of scores from other participating countries based upon a\nquality assessment of a performance. There is a question of whether the\ncountries will vote exclusively according to the artistic merit of the song, or\nif the vote will be a public signal of national support for another country.\nSince the competition aims to bring people together, any consistent biases in\nthe awarding of scores would defeat the purpose of the celebration of\nexpression and this has attracted researchers to investigate the supporting\nevidence for biases. This paper builds upon an approach which produces a set of\nrandom samples from an unbiased distribution of score allocation, and extends\nthe methodology to use the full set of years of the competition's life span\nwhich has seen fundamental changes to the voting schemes adopted.\n  By building up networks from statistically significant edge sets of vote\nallocations during a set of years, the results display a plausible network for\nthe origins of the culture anchors for the preferences of the awarded votes.\nWith 60 years of data, the results support the hypothesis of regional collusion\nand biases arising from proximity, culture and other irrelevant factors in\nregards to the music which that alone is intended to affect the judgment of the\ncontest. \n\n"}
{"id": "1705.06745", "contents": "Title: The first-year shear catalog of the Subaru Hyper Suprime-Cam SSP Survey Abstract: We present and characterize the catalog of galaxy shape measurements that\nwill be used for cosmological weak lensing measurements in the Wide layer of\nthe first year of the Hyper Suprime-Cam (HSC) survey. The catalog covers an\narea of 136.9 deg$^2$ split into six fields, with a mean $i$-band seeing of\n$0.58$ arcsec and $5\\sigma$ point-source depth of $i\\sim 26$. Given\nconservative galaxy selection criteria for first year science, the depth and\nexcellent image quality results in unweighted and weighted source number\ndensities of 24.6 and 21.8 arcmin$^{-2}$, respectively. We define the\nrequirements for cosmological weak lensing science with this catalog, then\nfocus on characterizing potential systematics in the catalog using a series of\ninternal null tests for problems with point-spread function (PSF) modeling,\nshear estimation, and other aspects of the image processing. We find that the\nPSF models narrowly meet requirements for weak lensing science with this\ncatalog, with fractional PSF model size residuals of approximately $0.003$\n(requirement: 0.004) and the PSF model shape correlation function\n$\\rho_1<3\\times 10^{-7}$ (requirement: $4\\times 10^{-7}$) at 0.5$^\\circ$\nscales. A variety of galaxy shape-related null tests are statistically\nconsistent with zero, but star-galaxy shape correlations reveal additive\nsystematics on $>1^\\circ$ scales that are sufficiently large as to require\nmitigation in cosmic shear measurements. Finally, we discuss the dominant\nsystematics and the planned algorithmic changes to reduce them in future data\nreductions. \n\n"}
{"id": "1705.06792", "contents": "Title: Two- and three-dimensional wide-field weak lensing mass maps from the\n  Hyper Suprime-Cam Subaru Strategic Program S16A data Abstract: We present wide-field (167 deg$^2$) weak lensing mass maps from the Hyper\nSupreme-Cam Subaru Strategic Program (HSC-SSP). We compare these weak lensing\nbased dark matter maps with maps of the distribution of the stellar mass\nassociated with luminous red galaxies. We find a strong correlation between\nthese two maps with a correlation coefficient of $\\rho=0.54\\pm0.03$ (for a\nsmoothing size of $8'$). This correlation is detected even with a smaller\nsmoothing scale of $2'$ ($\\rho=0.34\\pm 0.01$). This detection is made uniquely\npossible because of the high source density of the HSC-SSP weak lensing survey\n($\\bar{n}\\sim 25$ arcmin$^{-2}$). We also present a variety of tests to\ndemonstrate that our maps are not significantly affected by systematic effects.\nBy using the photometric redshift information associated with source galaxies,\nwe reconstruct a three-dimensional mass map. This three-dimensional mass map is\nalso found to correlate with the three-dimensional galaxy mass map.\nCross-correlation tests presented in this paper demonstrate that the HSC-SSP\nweak lensing mass maps are ready for further science analyses. \n\n"}
{"id": "1705.06960", "contents": "Title: Technical Report - MillimeterWave Communication in Vehicular Networks:\n  Coverage and Connectivity Analysis Abstract: In this technical report (TR), we describe the mathematical model we\ndeveloped to carry out a preliminary coverage and connectivity analysis in an\nautomotive communication scenario based on mmWave links. The purpose is to\nexemplify some of the complex and interesting tradeoffs that have to be\nconsidered when designing solutions for mmWave automotive scenarios. \n\n"}
{"id": "1705.09874", "contents": "Title: Targeted Learning with Daily EHR Data Abstract: Electronic health records (EHR) data provide a cost and time-effective\nopportunity to conduct cohort studies of the effects of multiple time-point\ninterventions in the diverse patient population found in real-world clinical\nsettings. Because the computational cost of analyzing EHR data at daily (or\nmore granular) scale can be quite high, a pragmatic approach has been to\npartition the follow-up into coarser intervals of pre-specified length. Current\nguidelines suggest employing a 'small' interval, but the feasibility and\npractical impact of this recommendation has not been evaluated and no formal\nmethodology to inform this choice has been developed. We start filling these\ngaps by leveraging large-scale EHR data from a diabetes study to develop and\nillustrate a fast and scalable targeted learning approach that allows to follow\nthe current recommendation and study its practical impact on inference. More\nspecifically, we map daily EHR data into four analytic datasets using 90, 30,\n15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation\napproach, the longitudinal TMLE, to estimate the causal effects of four dynamic\ntreatment rules with each dataset, and compare the resulting inferences. To\novercome the computational challenges presented by the size of these data, we\npropose a novel TMLE implementation, the 'long-format TMLE', and rely on the\nlatest advances in scalable data-adaptive machine-learning software, xgboost\nand h2o, for estimation of the TMLE nuisance parameters. \n\n"}
{"id": "1705.09976", "contents": "Title: Insert \"Price\" to Coxian Phase-Type Models: An Application to Hospital\n  Charge and Length of Stay Data Abstract: In this paper, we discuss the connection between the RGRST models (Gardiner\net al 2002, Polverejan et al 2003) and the Coxian Phase-Type (CPH) models\n(Marshall et al 2007, Tang 2012) through a construction that converts a special\nsub-class of RGRST models to CPH models. Both of the two models are widely used\nto characterize the distribution of hospital charge and length of stay (LOS),\nbut the lack of connections between them makes the two models rarely used\ntogether. We claim that our construction can make up this gap and make it\npossible to take advantage of the two different models simultaneously. As a\nconsequence, we derive a measure of the \"price\" of staying in each medical\nstage (identified with phases of a CPH model), which can't be approached\nwithout considering the RGRST and CPH models together.A two-stage algorithm is\nprovided to generate consistent estimation of model parameters. Applying the\nalgorithm to a sample drawn from the New York State's Statewide Planning and\nResearch Cooperative System 2013 (SPARCS 2013), we estimate the prices in a\nfour-phase CPH model and discuss the implications. \n\n"}
{"id": "1705.10220", "contents": "Title: Permutation-based Causal Inference Algorithms with Interventions Abstract: Learning directed acyclic graphs using both observational and interventional\ndata is now a fundamentally important problem due to recent technological\ndevelopments in genomics that generate such single-cell gene expression data at\na very large scale. In order to utilize this data for learning gene regulatory\nnetworks, efficient and reliable causal inference algorithms are needed that\ncan make use of both observational and interventional data. In this paper, we\npresent two algorithms of this type and prove that both are consistent under\nthe faithfulness assumption. These algorithms are interventional adaptations of\nthe Greedy SP algorithm and are the first algorithms using both observational\nand interventional data with consistency guarantees. Moreover, these algorithms\nhave the advantage that they are nonparametric, which makes them useful also\nfor analyzing non-Gaussian data. In this paper, we present these two algorithms\nand their consistency guarantees, and we analyze their performance on simulated\ndata, protein signaling data, and single-cell gene expression data. \n\n"}
{"id": "1705.10354", "contents": "Title: Sparsity enforcing priors in inverse problems via Normal variance\n  mixtures: model selection, algorithms and applications Abstract: The sparse structure of the solution for an inverse problem can be modelled\nusing different sparsity enforcing priors when the Bayesian approach is\nconsidered. Analytical expression for the unknowns of the model can be obtained\nby building hierarchical models based on sparsity enforcing distributions\nexpressed via conjugate priors. We consider heavy tailed distributions with\nthis property: the Student-t distribution, which is expressed as a Normal scale\nmixture, with the mixing distribution the Inverse Gamma distribution, the\nLaplace distribution, which can also be expressed as a Normal scale mixture,\nwith the mixing distribution the Exponential distribution or can be expressed\nas a Normal inverse scale mixture, with the mixing distribution the Inverse\nGamma distribution, the Hyperbolic distribution, the Variance-Gamma\ndistribution, the Normal-Inverse Gaussian distribution, all three expressed via\nconjugate distributions using the Generalized Hyperbolic distribution. For all\ndistributions iterative algorithms are derived based on hierarchical models\nthat account for the uncertainties of the forward model. For estimation,\nMaximum A Posterior (MAP) and Posterior Mean (PM) via variational Bayesian\napproximation (VBA) are used. The performances of resulting algorithm are\ncompared in applications in 3D computed tomography (3D-CT) and chronobiology.\nFinally, a theoretical study is developed for comparison between sparsity\nenforcing algorithms obtained via the Bayesian approach and the sparsity\nenforcing algorithms issued from regularization techniques, like LASSO and some\nothers. \n\n"}
{"id": "1705.10374", "contents": "Title: Extensions of the Burr Type XII distribution and Applications Abstract: The Burr type XII (BXII) distribution has been largely used in different\nfields due to its great flexibility for fitting data. These applications have\ntypically involved data showing heavy-tailed behaviors. In order to give more\nflexibility to the BXII distribution, in this paper, modifications to this\ndistribution through the use of parametric functions are introduced. For\ninstance, members of this new family of distributions allow the analysis not\nonly of data containing extreme values as the BXII distribution, but also of\nlight-tailed data. We refer to this new family of distributions as the extended\nBurr Type XII distribution (EBXIID) family. Statistical properties of members\nof the EBXIID family are discussed. The maximum likelihood method is proposed\nfor estimating model parameters. The performance of the new family of\ndistributions is studied using simulations. Applications of the new models to\nreal data sets coming from different domains show that models of the EBXIID\nfamily are an alternative to other known distributions. \n\n"}
{"id": "1706.00566", "contents": "Title: Deep Optical Imaging of the COSMOS Field with Hyper Suprime-Cam Using\n  Data from the Subaru Strategic Program and the University of Hawaii Abstract: We present the deepest optical images of the COSMOS field based on a joint\ndataset taken with Hyper Suprime-Cam (HSC) by the HSC Subaru Strategic Program\n(SSP) team and the University of Hawaii (UH). The COSMOS field is one of the\nkey extragalactic fields with a wealth of deep, multi-wavelength data. However,\nthe current optical data are not sufficiently deep to match with, e.g., the\nUltraVista data in the near-infrared. The SSP team and UH have joined forces to\nproduce very deep optical images of the COSMOS field by combining data from\nboth teams. The coadd images reach depths of g=27.8, r=27.7, i=27.6, z=26.8,\nand y=26.2 mag at 5 sigma for point sources based on flux uncertainties quoted\nby the pipeline and they cover essentially the entire COSMOS 2 square degree\nfield. The seeing is between 0.6 and 0.9 arcsec on the coadds. We perform\nseveral quality checks and confirm that the data are of science quality; ~2%\nphotometry and 30 mas astrometry. This accuracy is identical to the Public Data\nRelease 1 from HSC-SSP. We make the joint dataset including fully calibrated\ncatalogs of detected objects available to the community at\nhttps://hsc-release.mtk.nao.ac.jp/. \n\n"}
{"id": "1706.00819", "contents": "Title: Simplified Threshold Phenomena in Hypo- and Hyper-coagulation Abstract: We discuss two threshold phenomena in blood coagulation dynamics using a\nsimplified model. This perspective of the underlying complex phenomena is\nexpected to aid the understanding and characterization of many blood\ncoagulation pathologies with altered protein dynamics. \n\n"}
{"id": "1706.00904", "contents": "Title: X-TCP: A Cross Layer Approach for TCP Uplink Flows in mmWave Networks Abstract: Millimeter wave frequencies will likely be part of the fifth generation of\nmobile networks and of the 3GPP New Radio (NR) standard. MmWave communication\nindeed provides a very large bandwidth, thus an increased cell throughput, but\nhow to exploit these resources at the higher layers is still an open research\nquestion. A very relevant issue is the high variability of the channel, caused\nby the blockage from obstacles and the human body. This affects the design of\ncongestion control mechanisms at the transport layer, and state-of-the-art TCP\nschemes such as TCP CUBIC present suboptimal performance. In this paper, we\npresent a cross layer approach for uplink flows that adjusts the congestion\nwindow of TCP at the mobile equipment side using an estimation of the available\ndata rate at the mmWave physical layer, based on the actual resource allocation\nand on the Signal to Interference plus Noise Ratio. We show that this approach\nreduces the latency, avoiding to fill the buffers in the cellular stack, and\nhas a quicker recovery time after RTO events than several other TCP congestion\ncontrol algorithms. \n\n"}
{"id": "1706.01242", "contents": "Title: Bayesian LSTMs in medicine Abstract: The medical field stands to see significant benefits from the recent advances\nin deep learning. Knowing the uncertainty in the decision made by any machine\nlearning algorithm is of utmost importance for medical practitioners. This\nstudy demonstrates the utility of using Bayesian LSTMs for classification of\nmedical time series. Four medical time series datasets are used to show the\naccuracy improvement Bayesian LSTMs provide over standard LSTMs. Moreover, we\nshow cherry-picked examples of confident and uncertain classifications of the\nmedical time series. With simple modifications of the common practice for deep\nlearning, significant improvements can be made for the medical practitioner and\npatient. \n\n"}
{"id": "1706.02353", "contents": "Title: Sparse Wavelet Estimation in Quantile Regression with Multiple\n  Functional Predictors Abstract: In this manuscript, we study quantile regression in partial functional linear\nmodel where response is scalar and predictors include both scalars and multiple\nfunctions. Wavelet basis are adopted to better approximate functional slopes\nwhile effectively detect local features. The sparse group lasso penalty is\nimposed to select important functional predictors while capture shared\ninformation among them. The estimation problem can be reformulated into a\nstandard second-order cone program and then solved by an interior point method.\nWe also give a novel algorithm by using alternating direction method of\nmultipliers (ADMM) which was recently employed by many researchers in solving\npenalized quantile regression problems. The asymptotic properties such as the\nconvergence rate and prediction error bound have been established. Simulations\nand a real data from ADHD-200 fMRI data are investigated to show the\nsuperiority of our proposed method. \n\n"}
{"id": "1706.03671", "contents": "Title: Fully-Automatic Multiresolution Idealization for Filtered Ion Channel\n  Recordings: Flickering Event Detection Abstract: We propose a new model-free segmentation method, JULES, which combines recent\nstatistical multiresolution techniques with local deconvolution for\nidealization of ion channel recordings. The multiresolution criterion takes\ninto account scales down to the sampling rate enabling the detection of\nflickering events, i.e., events on small temporal scales, even below the filter\nfrequency. For such small scales the deconvolution step allows for a precise\ndetermination of dwell times and, in particular, of amplitude levels, a task\nwhich is not possible with common thresholding methods. This is confirmed\ntheoretically and in a comprehensive simulation study. In addition, JULES can\nbe applied as a preprocessing method for a refined hidden Markov analysis. Our\nnew methodolodgy allows us to show that gramicidin A flickering events have the\nsame amplitude as the slow gating events. JULES is available as an R function\njules in the package clampSeg. \n\n"}
{"id": "1706.03860", "contents": "Title: Subspace Clustering via Optimal Direction Search Abstract: This letter presents a new spectral-clustering-based approach to the subspace\nclustering problem. Underpinning the proposed method is a convex program for\noptimal direction search, which for each data point d finds an optimal\ndirection in the span of the data that has minimum projection on the other data\npoints and non-vanishing projection on d. The obtained directions are\nsubsequently leveraged to identify a neighborhood set for each data point. An\nalternating direction method of multipliers framework is provided to\nefficiently solve for the optimal directions. The proposed method is shown to\nnotably outperform the existing subspace clustering methods, particularly for\nunwieldy scenarios involving high levels of noise and close subspaces, and\nyields the state-of-the-art results for the problem of face clustering using\nsubspace segmentation. \n\n"}
{"id": "1706.04152", "contents": "Title: Learning to Detect Sepsis with a Multitask Gaussian Process RNN\n  Classifier Abstract: We present a scalable end-to-end classifier that uses streaming physiological\nand medication data to accurately predict the onset of sepsis, a\nlife-threatening complication from infections that has high mortality and\nmorbidity. Our proposed framework models the multivariate trajectories of\ncontinuous-valued physiological time series using multitask Gaussian processes,\nseamlessly accounting for the high uncertainty, frequent missingness, and\nirregular sampling rates typically associated with real clinical data. The\nGaussian process is directly connected to a black-box classifier that predicts\nwhether a patient will become septic, chosen in our case to be a recurrent\nneural network to account for the extreme variability in the length of patient\nencounters. We show how to scale the computations associated with the Gaussian\nprocess in a manner so that the entire system can be discriminatively trained\nend-to-end using backpropagation. In a large cohort of heterogeneous inpatient\nencounters at our university health system we find that it outperforms several\nbaselines at predicting sepsis, and yields 19.4% and 55.5% improved areas under\nthe Receiver Operating Characteristic and Precision Recall curves as compared\nto the NEWS score currently used by our hospital. \n\n"}
{"id": "1706.04394", "contents": "Title: Spatio-Temporal Forecasting by Coupled Stochastic Differential\n  Equations: Applications to Solar Power Abstract: Spatio-temporal problems exist in many areas of knowledge and disciplines\nranging from biology to engineering and physics. However, solution strategies\nbased on classical statistical techniques often fall short due to the large\nnumber of parameters that are to be estimated and the huge amount of data that\nneed to be handled. In this paper we apply known techniques in a novel way to\nprovide a framework for spatio-temporal modeling which is both computationally\nefficient and has a low dimensional parameter space. We present a\nmicro-to-macro approach whereby the local dynamics are first modeled and\nsubsequently combined to capture the global system behavior. The proposed\nmethodology relies on coupled stochastic differential equations and is applied\nto produce spatio-temporal forecasts for a solar power plant for very short\nhorizons, which essentially implies tracking clouds moving across the field of\nsolar power inverters. We outperform simple and complex benchmarks while\nproviding forecasts for 70 spatial dimensions and 24 lead times (i.e., for a\ntotal number of random variables equal to 1680). The resulting model can\nprovide all sorts of forecast products, ranging from point forecasts and\nco-variances to predictive densities, multi-horizon forecasts, and space-time\ntrajectories. \n\n"}
{"id": "1706.04943", "contents": "Title: Plus-Minus Player Ratings for Soccer Abstract: The paper presents a plus-minus rating for use in association football\n(soccer). We first describe the standard plus-minus methodology as used in\nbasketball and ice-hockey and then adapt it for use in soccer. The usual\ngoal-differential plus-minus is considered before two variations are proposed.\nFor the first variation, we present a methodology to calculate an expected\ngoals plus-minus rating. The second variation makes use of in-play\nprobabilities of match outcome to evaluate an expected points plus-minus\nrating. We use the ratings to examine who are the best players in European\nfootball, and demonstrate how the players' ratings evolve over time. Finally,\nwe shed light on the debate regarding which is the strongest league. The model\nsuggests the English Premier League is the strongest, with the German\nBundesliga a close runner-up. \n\n"}
{"id": "1706.05029", "contents": "Title: Distance weighted discrimination of face images for gender\n  classification Abstract: We illustrate the advantages of distance weighted discrimination for\nclassification and feature extraction in a High Dimension Low Sample Size\n(HDLSS) situation. The HDLSS context is a gender classification problem of face\nimages in which the dimension of the data is several orders of magnitude larger\nthan the sample size. We compare distance weighted discrimination with Fisher's\nlinear discriminant, support vector machines, and principal component analysis\nby exploring their classification interpretation through insightful\nvisuanimations and by examining the classifiers' discriminant errors. This\nanalysis enables us to make new contributions to the understanding of the\ndrivers of human discrimination between males and females. \n\n"}
{"id": "1706.05446", "contents": "Title: Adversarial Variational Bayes Methods for Tweedie Compound Poisson Mixed\n  Models Abstract: The Tweedie Compound Poisson-Gamma model is routinely used for modeling\nnon-negative continuous data with a discrete probability mass at zero. Mixed\nmodels with random effects account for the covariance structure related to the\ngrouping hierarchy in the data. An important application of Tweedie mixed\nmodels is pricing the insurance policies, e.g. car insurance. However, the\nintractable likelihood function, the unknown variance function, and the\nhierarchical structure of mixed effects have presented considerable challenges\nfor drawing inferences on Tweedie. In this study, we tackle the Bayesian\nTweedie mixed-effects models via variational inference approaches. In\nparticular, we empower the posterior approximation by implicit models trained\nin an adversarial setting. To reduce the variance of gradients, we\nreparameterize random effects, and integrate out one local latent variable of\nTweedie. We also employ a flexible hyper prior to ensure the richness of the\napproximation. Our method is evaluated on both simulated and real-world data.\nResults show that the proposed method has smaller estimation bias on the random\neffects compared to traditional inference methods including MCMC; it also\nachieves a state-of-the-art predictive performance, meanwhile offering a richer\nestimation of the variance function. \n\n"}
{"id": "1706.05678", "contents": "Title: A large-scale analysis of racial disparities in police stops across the\n  United States Abstract: To assess racial disparities in police interactions with the public, we\ncompiled and analyzed a dataset detailing over 60 million state patrol stops\nconducted in 20 U.S. states between 2011 and 2015. We find that black drivers\nare stopped more often than white drivers relative to their share of the\ndriving-age population, but that Hispanic drivers are stopped less often than\nwhites. Among stopped drivers -- and after controlling for age, gender, time,\nand location -- blacks and Hispanics are more likely to be ticketed, searched,\nand arrested than white drivers. These disparities may reflect differences in\ndriving behavior, and are not necessarily the result of bias. In the case of\nsearch decisions, we explicitly test for discrimination by examining both the\nrate at which drivers are searched and the likelihood searches turn up\ncontraband. We find evidence that the bar for searching black and Hispanic\ndrivers is lower than for searching whites. Finally, we find that legalizing\nrecreational marijuana in Washington and Colorado reduced the total number of\nsearches and misdemeanors for all race groups, though a race gap still\npersists. We conclude by offering recommendations for improving data\ncollection, analysis, and reporting by law enforcement agencies. \n\n"}
{"id": "1706.06976", "contents": "Title: The effect of the spatial domain in FANOVA models with ARH(1) error term Abstract: Functional Analysis of Variance (FANOVA) from Hilbert-valued correlated data\nwith spatial rectangular or circular supports is analyzed, when Dirichlet\nconditions are assumed on the boundary. Specifically, a Hilbert-valued fixed\neffect model with error term defined from an Autoregressive Hilbertian process\nof order one (ARH(1) process) is considered, extending the formulation given in\nRuiz-Medina (2016). A new statistical test is also derived to contrast the\nsignificance of the functional fixed effect parameters. The Dirichlet\nconditions established at the boundary affect the dependence range of the\ncorrelated error term. While the rate of convergence to zero of the eigenvalues\nof the covariance kernels, characterizing the Gaussian functional error\ncomponents, directly affects the stability of the generalized least-squares\nparameter estimation problem. A simulation study and a real-data application\nrelated to fMRI analysis are undertaken to illustrate the performance of the\nparameter estimator and statistical test derived. \n\n"}
{"id": "1706.07094", "contents": "Title: Constrained Bayesian Optimization with Noisy Experiments Abstract: Randomized experiments are the gold standard for evaluating the effects of\nchanges to real-world systems. Data in these tests may be difficult to collect\nand outcomes may have high variance, resulting in potentially large measurement\nerror. Bayesian optimization is a promising technique for efficiently\noptimizing multiple continuous parameters, but existing approaches degrade in\nperformance when the noise level is high, limiting its applicability to many\nrandomized experiments. We derive an expression for expected improvement under\ngreedy batch optimization with noisy observations and noisy constraints, and\ndevelop a quasi-Monte Carlo approximation that allows it to be efficiently\noptimized. Simulations with synthetic functions show that optimization\nperformance on noisy, constrained problems outperforms existing methods. We\nfurther demonstrate the effectiveness of the method with two real-world\nexperiments conducted at Facebook: optimizing a ranking system, and optimizing\nserver compiler flags. \n\n"}
{"id": "1706.07136", "contents": "Title: Multiscale Information Decomposition: Exact Computation for Multivariate\n  Gaussian Processes Abstract: Exploiting the theory of state space models, we derive the exact expressions\nof the information transfer, as well as redundant and synergistic transfer, for\ncoupled Gaussian processes observed at multiple temporal scales. All of the\nterms, constituting the frameworks known as interaction information\ndecomposition and partial information decomposition, can thus be analytically\nobtained for different time scales from the parameters of the VAR model that\nfits the processes. We report the application of the proposed methodology\nfirstly to benchmark Gaussian systems, showing that this class of systems may\ngenerate patterns of information decomposition characterized by mainly\nredundant or synergistic information transfer persisting across multiple time\nscales or even by the alternating prevalence of redundant and synergistic\nsource interaction depending on the time scale. Then, we apply our method to an\nimportant topic in neuroscience, i.e., the detection of causal interactions in\nhuman epilepsy networks, for which we show the relevance of partial information\ndecomposition to the detection of multiscale information transfer spreading\nfrom the seizure onset zone. \n\n"}
{"id": "1706.07355", "contents": "Title: Three-dimensional Cardiovascular Imaging-Genetics: A Mass Univariate\n  Framework Abstract: MOTIVATION: Left ventricular (LV) hypertrophy is a strong predictor of\ncardiovascular outcomes, but its genetic regulation remains largely\nunexplained. Conventional phenotyping relies on manual calculation of LV mass\nand wall thickness, but advanced cardiac image analysis presents an opportunity\nfor high-throughput mapping of genotype-phenotype associations in three\ndimensions (3D). RESULTS: High-resolution cardiac magnetic resonance images\nwere automatically segmented in 1,124 healthy volunteers to create a 3D shape\nmodel of the heart. Mass univariate regression was used to plot a 3D\neffect-size map for the association between wall thickness and a set of\npredictors at each vertex in the mesh. The vertices where a significant effect\nexists were determined by applying threshold-free cluster enhancement to boost\nareas of signal with spatial contiguity. Experiments on simulated phenotypic\nsignals and SNP replication show that this approach offers a substantial gain\nin statistical power for cardiac genotype-phenotype associations while\nproviding good control of the false discovery rate. This framework models the\neffects of genetic variation throughout the heart and can be automatically\napplied to large population cohorts. AVAILABILITY: The proposed approach has\nbeen coded in an R package freely available at\nhttps://doi.org/10.5281/zenodo.834610 together with the clinical data used in\nthis work. \n\n"}
{"id": "1706.08171", "contents": "Title: Faster independent component analysis by preconditioning with Hessian\n  approximations Abstract: Independent Component Analysis (ICA) is a technique for unsupervised\nexploration of multi-channel data that is widely used in observational\nsciences. In its classic form, ICA relies on modeling the data as linear\nmixtures of non-Gaussian independent sources. The maximization of the\ncorresponding likelihood is a challenging problem if it has to be completed\nquickly and accurately on large sets of real data. We introduce the\nPreconditioned ICA for Real Data (Picard) algorithm, which is a relative L-BFGS\nalgorithm preconditioned with sparse Hessian approximations. Extensive\nnumerical comparisons to several algorithms of the same class demonstrate the\nsuperior performance of the proposed technique, especially on real data, for\nwhich the ICA model does not necessarily hold. \n\n"}
{"id": "1706.08320", "contents": "Title: Unemployment estimation: Spatial point referenced methods and models Abstract: Portuguese Labor force survey, from 4th quarter of 2014 onwards, started\ngeo-referencing the sampling units, namely the dwellings in which the surveys\nare carried. This opens new possibilities in analysing and estimating\nunemployment and its spatial distribution across any region. The labor force\nsurvey choose, according to an preestablished sampling criteria, a certain\nnumber of dwellings across the nation and survey the number of unemployed in\nthese dwellings. Based on this survey, the National Statistical Institute of\nPortugal presently uses direct estimation methods to estimate the national\nunemployment figures. Recently, there has been increased interest in estimating\nthese figures in smaller areas. Direct estimation methods, due to reduced\nsampling sizes in small areas, tend to produce fairly large sampling variations\ntherefore model based methods, which tend to \"borrow strength\" from area to\narea by making use of the areal dependence, should be favored. These model\nbased methods tend use areal counting processes as models and typically\nintroduce spatial dependence through the model parameters by a latent random\neffect. In this paper, we suggest modeling the spatial distribution of\nresidential buildings across Portugal by a Log Gaussian Cox process and the\nnumber of unemployed per residential unit as a mark attached to these random\npoints. Thus the main focus of the study is to model the spatial intensity\nfunction of this marked point process. Number of unemployed in any region can\nthen be estimated using a proper functional of this marked point process. The\nprincipal objective of this point referenced method for unemployment estimation\nis to get reliable estimates at higher spatial resolutions and at the same time\nincorporate in the model the auxiliary information available at residential\nunits such as average income or education level of individuals surveyed in\nthese units. \n\n"}
{"id": "1706.09574", "contents": "Title: On the analysis of personalized medication response and classification\n  of case vs control patients in mobile health studies: the mPower case study Abstract: In this work we provide a couple of contributions to the analysis of\nlongitudinal data collected by smartphones in mobile health applications.\nFirst, we propose a novel statistical approach to disentangle personalized\ntreatment and \"time-of-the-day\" effects in observational studies. Under the\nassumption of no unmeasured confounders, we show how to use conditional\nindependence relations in the data in order to determine if a difference in\nperformance between activity tasks performed before and after the participant\nhas taken medication, are potentially due to an effect of the medication or to\na \"time-of-the-day\" effect (or still to both). Second, we show that smartphone\ndata collected from a given study participant can represent a \"digital\nfingerprint\" of the participant, and that classifiers of case/control labels,\nconstructed using longitudinal data, can show artificially improved performance\nwhen data from each participant is included in both training and test sets. We\nillustrate our contributions using data collected during the first 6 months of\nthe mPower study. \n\n"}
{"id": "1706.09796", "contents": "Title: Selective inference after likelihood- or test-based model selection in\n  linear models Abstract: Statistical inference after model selection requires an inference framework\nthat takes the selection into account in order to be valid. Following recent\nwork on selective inference, we derive analytical expressions for inference\nafter likelihood- or test-based model selection for linear models. \n\n"}
{"id": "1707.00046", "contents": "Title: Fairer and more accurate, but for whom? Abstract: Complex statistical machine learning models are increasingly being used or\nconsidered for use in high-stakes decision-making pipelines in domains such as\nfinancial services, health care, criminal justice and human services. These\nmodels are often investigated as possible improvements over more classical\ntools such as regression models or human judgement. While the modeling approach\nmay be new, the practice of using some form of risk assessment to inform\ndecisions is not. When determining whether a new model should be adopted, it is\ntherefore essential to be able to compare the proposed model to the existing\napproach across a range of task-relevant accuracy and fairness metrics. Looking\nat overall performance metrics, however, may be misleading. Even when two\nmodels have comparable overall performance, they may nevertheless disagree in\ntheir classifications on a considerable fraction of cases. In this paper we\nintroduce a model comparison framework for automatically identifying subgroups\nin which the differences between models are most pronounced. Our primary focus\nis on identifying subgroups where the models differ in terms of\nfairness-related quantities such as racial or gender disparities. We present\nexperimental results from a recidivism prediction task and a hypothetical\nlending example. \n\n"}
{"id": "1707.01287", "contents": "Title: A Matern based multivariate Gaussian random process for a consistent\n  model of the horizontal wind components and related variables Abstract: The integration of physical relationships into stochastic models is of major\ninterest e.g. in data assimilation. Here, a multivariate Gaussian random field\nformulation is introduced, which represents the differential relations of the\ntwo-dimensional wind field and related variables such as streamfunction,\nvelocity potential, vorticity and divergence. The covariance model is based on\na flexible bivariate Mat\\'ern covariance function for streamfunction and\nvelocity potential. It allows for different variances in the potentials,\nnon-zero correlations between them, anisotropy and a flexible smoothness\nparameter. The joint covariance function of the related variables is derived\nanalytically. Further, it is shown that a consistent model with non-zero\ncorrelations between the potentials and positive definite covariance function\nis possible. The statistical model is fitted to forecasts of the horizontal\nwind fields of a mesoscale numerical weather prediction system. Parameter\nuncertainty is assessed by a parametric bootstrap method. The estimates reveal\nonly physically negligible correlations between the potentials. In contrast to\nthe numerical estimator, the statistical estimator of the ratio between the\nvariances of the rotational and divergent wind components is unbiased. \n\n"}
{"id": "1707.02032", "contents": "Title: Matrix-Based Characterization of the Motion and Wrench Uncertainties in\n  Robotic Manipulators Abstract: Characterization of the uncertainty in robotic manipulators is the focus of\nthis paper. Based on the random matrix theory (RMT), we propose uncertainty\ncharacterization schemes in which the uncertainty is modeled at the macro\n(system) level. This is different from the traditional approaches that model\nthe uncertainty in the parametric space of micro (state) level. We show that\nperturbing the system matrices rather than the state of the system provides\nunique advantages especially for robotic manipulators. First, it requires only\nlimited statistical information that becomes effective when dealing with\ncomplex systems where detailed information on their variability is not\navailable. Second, the RMT-based models are aware of the system state and\nconfiguration that are significant factors affecting the level of uncertainty\nin system behavior. In this study, in addition to the motion uncertainty\nanalysis that was first proposed in our earlier work, we also develop an\nRMT-based model for the quantification of the static wrench uncertainty in\nmulti-agent cooperative systems. This model is aimed to be an alternative to\nthe elaborate parametric formulation when only rough bounds are available on\nthe system parameters. We discuss that how RMT-based model becomes advantageous\nwhen the complexity of the system increases. We perform experimental studies on\na KUKA youBot arm to demonstrate the superiority of the RMT-based motion\nuncertainty models. We show that how these models outperform the traditional\nmodels built upon Gaussianity assumption in capturing real-system uncertainty\nand providing accurate bounds on the state estimation errors. In addition, to\nexperimentally support our wrench uncertainty quantification model, we study\nthe behavior of a cooperative system of mobile robots. It is shown that one can\nrely on less demanding RMT-based formulation and yet meets the acceptable\naccuracy. \n\n"}
{"id": "1707.03510", "contents": "Title: Association of Networked Flying Platforms with Small Cells for Network\n  Centric 5G+ C-RAN Abstract: 5G+ systems expect enhancement in data rate and coverage area under limited\npower constraint. Such requirements can be fulfilled by the densification of\nsmall cells (SCs). However, a major challenge is the management of fronthaul\nlinks connected with an ultra dense network of SCs. A cost effective and\nscalable idea of using network flying platforms (NFPs) is employed here, where\nthe NFPs are used as fronthaul hubs that connect the SCs to the core network.\nThe association problem of NFPs and SCs is formulated considering a number of\npractical constraints such as backhaul data rate limit, maximum supported links\nand bandwidth by NFPs and quality of service requirement of the system. The\nnetwork centric case of the system is considered that aims to maximize the\nnumber of associated SCs without any biasing, i.e., no preference for high\npriority SCs. Then, two new efficient greedy algorithms are designed to solve\nthe presented association problem. Numerical results show a favorable\nperformance of our proposed methods in comparison to exhaustive search. \n\n"}
{"id": "1707.03916", "contents": "Title: Large Scale Variable Fidelity Surrogate Modeling Abstract: Engineers widely use Gaussian process regression framework to construct\nsurrogate models aimed to replace computationally expensive physical models\nwhile exploring design space. Thanks to Gaussian process properties we can use\nboth samples generated by a high fidelity function (an expensive and accurate\nrepresentation of a physical phenomenon) and a low fidelity function (a cheap\nand coarse approximation of the same physical phenomenon) while constructing a\nsurrogate model. However, if samples sizes are more than few thousands of\npoints, computational costs of the Gaussian process regression become\nprohibitive both in case of learning and in case of prediction calculation. We\npropose two approaches to circumvent this computational burden: one approach is\nbased on the Nystr\\\"om approximation of sample covariance matrices and another\nis based on an intelligent usage of a blackbox that can evaluate a~low fidelity\nfunction on the fly at any point of a design space. We examine performance of\nthe proposed approaches using a number of artificial and real problems,\nincluding engineering optimization of a rotating disk shape. \n\n"}
{"id": "1707.04179", "contents": "Title: Cost-Effective Cache Deployment in Mobile Heterogeneous Networks Abstract: This paper investigates one of the fundamental issues in cache-enabled\nheterogeneous networks (HetNets): how many cache instances should be deployed\nat different base stations, in order to provide guaranteed service in a\ncost-effective manner. Specifically, we consider two-tier HetNets with\nhierarchical caching, where the most popular files are cached at small cell\nbase stations (SBSs) while the less popular ones are cached at macro base\nstations (MBSs). For a given network cache deployment budget, the cache sizes\nfor MBSs and SBSs are optimized to maximize network capacity while satisfying\nthe file transmission rate requirements. As cache sizes of MBSs and SBSs affect\nthe traffic load distribution, inter-tier traffic steering is also employed for\nload balancing. Based on stochastic geometry analysis, the optimal cache sizes\nfor MBSs and SBSs are obtained, which are threshold-based with respect to cache\nbudget in the networks constrained by SBS backhauls. Simulation results are\nprovided to evaluate the proposed schemes and demonstrate the applications in\ncost-effective network deployment. \n\n"}
{"id": "1707.05232", "contents": "Title: On Lasso refitting strategies Abstract: A well-know drawback of l_1-penalized estimators is the systematic shrinkage\nof the large coefficients towards zero. A simple remedy is to treat Lasso as a\nmodel-selection procedure and to perform a second refitting step on the\nselected support. In this work we formalize the notion of refitting and provide\noracle bounds for arbitrary refitting procedures of the Lasso solution. One of\nthe most widely used refitting techniques which is based on Least-Squares may\nbring a problem of interpretability, since the signs of the refitted estimator\nmight be flipped with respect to the original estimator. This problem arises\nfrom the fact that the Least-Squares refitting considers only the support of\nthe Lasso solution, avoiding any information about signs or amplitudes. To this\nend we define a sign consistent refitting as an arbitrary refitting procedure,\npreserving the signs of the first step Lasso solution and provide Oracle\ninequalities for such estimators. Finally, we consider special refitting\nstrategies: Bregman Lasso and Boosted Lasso. Bregman Lasso has a fruitful\nproperty to converge to the Sign-Least-Squares refitting (Least-Squares with\nsign constraints), which provides with greater interpretability. We\nadditionally study the Bregman Lasso refitting in the case of orthogonal\ndesign, providing with simple intuition behind the proposed method. Boosted\nLasso, in contrast, considers information about magnitudes of the first Lasso\nstep and allows to develop better oracle rates for prediction. Finally, we\nconduct an extensive numerical study to show advantages of one approach over\nothers in different synthetic and semi-real scenarios. \n\n"}
{"id": "1707.06107", "contents": "Title: Bayesian Probabilistic Numerical Methods in Time-Dependent State\n  Estimation for Industrial Hydrocyclone Equipment Abstract: The use of high-power industrial equipment, such as large-scale mixing\nequipment or a hydrocyclone for separation of particles in liquid suspension,\ndemands careful monitoring to ensure correct operation. The fundamental task of\nstate-estimation for the liquid suspension can be posed as a time-evolving\ninverse problem and solved with Bayesian statistical methods. In this paper, we\nextend Bayesian methods to incorporate statistical models for the error that is\nincurred in the numerical solution of the physical governing equations. This\nenables full uncertainty quantification within a principled\ncomputation-precision trade-off, in contrast to the over-confident inferences\nthat are obtained when all sources of numerical error are ignored. The method\nis cast within a sequential Monte Carlo framework and an optimised\nimplementation is provided in Python. \n\n"}
{"id": "1707.08354", "contents": "Title: A hierarchical Bayesian model for predicting ecological interactions\n  using scaled evolutionary relationships Abstract: Identifying undocumented or potential future interactions among species is a\nchallenge facing modern ecologists. Recent link prediction methods rely on\ntrait data, however large species interaction databases are typically sparse\nand covariates are limited to only a fraction of species. On the other hand,\nevolutionary relationships, encoded as phylogenetic trees, can act as proxies\nfor underlying traits and historical patterns of parasite sharing among hosts.\nWe show that using a network-based conditional model, phylogenetic information\nprovides strong predictive power in a recently published global database of\nhost-parasite interactions. By scaling the phylogeny using an evolutionary\nmodel, our method allows for biological interpretation often missing from\nlatent variable models. To further improve on the phylogeny-only model, we\ncombine a hierarchical Bayesian latent score framework for bipartite graphs\nthat accounts for the number of interactions per species with the host\ndependence informed by phylogeny. Combining the two information sources yields\nsignificant improvement in predictive accuracy over each of the submodels\nalone. As many interaction networks are constructed from presence-only data, we\nextend the model by integrating a correction mechanism for missing\ninteractions, which proves valuable in reducing uncertainty in unobserved\ninteractions. \n\n"}
{"id": "1707.08681", "contents": "Title: Gerrymandering and the net number of US House seats won due to\n  vote-distribution asymmetries Abstract: Using the recently introduced declination function, we estimate the net\nnumber of seats won in the US House of Representatives due to asymmetries in\nvote distributions. Such asymmetries can arise from combinations of partisan\ngerrymandering and inherent geographic advantage. Our estimates show\nsignificant biases in favor of the Democrats prior to the mid 1990s and\nsignificant biases in favor of Republicans since then. We find net differences\nof 28, 20 and 25 seats in favor of the Republicans in the years 2012, 2014 and\n2016, respectively. The validity of our results is supported by the technique\nof simulated packing and cracking. We also use this technique to show that the\npresidential-vote logistic regression model is insensitive to the packing and\ncracking by which partisan gerrymanders are achieved. \n\n"}
{"id": "1707.09308", "contents": "Title: The Role of Mastery Learning in Intelligent Tutoring Systems: Principal\n  Stratification on a Latent Variable Abstract: Students in Algebra I classrooms typically learn at different rates and\nstruggle at different points in the curriculum---a common challenge for math\nteachers. Cognitive Tutor Algebra I (CTA1), educational computer program,\naddresses such student heterogeneity via what they term \"mastery learning,\"\nwhere students progress from one section of the curriculum to the next by\ndemonstrating appropriate \"mastery\" at each stage. However, when students are\nunable to master a section's skills even after trying many problems, they are\nautomatically promoted to the next section anyway. Does promotion without\nmastery impair the program's effectiveness?\n  At least in certain domains, CTA1 was recently shown to improve student\nlearning on average in a randomized effectiveness study. This paper uses\nstudent log data from that study in a continuous principal stratification model\nto estimate the relationship between students' potential mastery and the CTA1\ntreatment effect. In contrast to extant principal stratification applications,\na student's propensity to master worked sections here is never directly\nobserved. Consequently we embed an item-response model, which measures\nstudents' potential mastery, within the larger principal stratification model.\nWe find that the tutor may, in fact, be more effective for students who are\nmore frequently promoted (despite unsuccessfully completing sections of the\nmaterial). However, since these students are distinctive in their educational\nstrength (as well as in other respects), it remains unclear whether this\nenhanced effectiveness can be directly attributed to aspects of the mastery\nlearning program. \n\n"}
{"id": "1708.02486", "contents": "Title: Data-driven modelling and validation of aircraft inbound-stream at some\n  major European airports Abstract: This paper presents an exhaustive study on the arrivals process at eight\nimportant European airports. Using inbound traffic data, we define, compare,\nand contrast a data-driven Poisson and PSRA point process. Although, there is\nsufficient evidence that the interarrivals might follow an exponential\ndistribution, this finding does not directly translate to evidence that the\narrivals stream is Poisson. The main reason is that finite-capacity constraints\nimpose a correlation structure to the arrivals stream, which a Poisson model\ncannot capture. We show the weaknesses and somehow the difficulties of using a\nPoisson process to model with good approximation the arrivals stream. On the\nother hand, our innovative non-parametric, data-driven PSRA model, predicts\nquite well and captures important properties of the typical arrivals stream. \n\n"}
{"id": "1708.03229", "contents": "Title: Automatic Selection of t-SNE Perplexity Abstract: t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely\nused dimensionality reduction methods for data visualization, but it has a\nperplexity hyperparameter that requires manual selection. In practice, proper\ntuning of t-SNE perplexity requires users to understand the inner working of\nthe method as well as to have hands-on experience. We propose a model selection\nobjective for t-SNE perplexity that requires negligible extra computation\nbeyond that of the t-SNE itself. We empirically validate that the perplexity\nsettings found by our approach are consistent with preferences elicited from\nhuman experts across a number of datasets. The similarities of our approach to\nBayesian information criteria (BIC) and minimum description length (MDL) are\nalso analyzed. \n\n"}
{"id": "1708.03579", "contents": "Title: Self-exciting point processes with spatial covariates: modeling the\n  dynamics of crime Abstract: Crime has both varying patterns in space, related to features of the\nenvironment, economy, and policing, and patterns in time arising from criminal\nbehavior, such as retaliation. Serious crimes may also be presaged by minor\ncrimes of disorder. We demonstrate that these spatial and temporal patterns are\ngenerally confounded, requiring analyses to take both into account, and propose\na spatio-temporal self-exciting point process model which incorporates spatial\nfeatures, near-repeat and retaliation effects, and triggering. We develop\ninference methods and diagnostic tools, such as residual maps, for this model,\nand through extensive simulation and crime data obtained from Pittsburgh,\nPennsylvania, demonstrate its properties and usefulness. \n\n"}
{"id": "1708.03624", "contents": "Title: Energy transfer in $N$-component nanosystems enhanced by pulse-driven\n  vibronic many-body entanglement Abstract: The processing of energy by transfer and redistribution plays a key role in\nthe evolution of dynamical systems. At the ultrasmall and ultrafast scale of\nnanosystems, quantum coherence could in principle also play a role and has been\nreported in many pulse-driven nanosystems (e.g. quantum dots and even the\nmicroscopic Light-Harvesting Complex II (LHC-II) aggregate). Typical\ntheoretical analyses cannot easily be scaled to describe these general\n$N$-component nanosystems; they do not treat the pulse dynamically; and they\napproximate memory effects. Here our aim is to shed light on what new physics\nmight arise beyond these approximations. We adopt a purposely minimal model\nsuch that the time-dependence of the pulse is included explicitly in the\nHamiltonian. This simple model generates complex dynamics: specifically, pulses\nof intermediate duration generate highly entangled vibronic (i.e.\nelectronic-vibrational) states that spread multiple excitons -- and hence\nenergy -- maximally within the system. Subsequent pulses can then act on such\nentangled states to efficiently channel subsequent energy capture. The\nunderlying pulse-generated vibronic entanglement increases in strength and\nrobustness as $N$ increases. \n\n"}
{"id": "1708.04490", "contents": "Title: Sparse Inverse Covariance Estimation for High-throughput microRNA\n  Sequencing Data in the Poisson Log-Normal Graphical Model Abstract: We introduce the Poisson Log-Normal Graphical Model for count data, and\npresent a normality transformation for data arising from this distribution. The\nmodel and transformation are feasible for high-throughput microRNA (miRNA)\nsequencing data and directly account for known overdispersion relationships\npresent in this data set. The model allows for network dependencies to be\nmodeled, and we provide an algorithm which utilizes a one-step EM based result\nin order to allow for a provable increase in performance in determining the\nnetwork structure. The model is shown to provide an increase in performance in\nsimulation settings over a range of network structures. The model is applied to\nhigh-throughput miRNA sequencing data from patients with breast cancer from The\nCancer Genome Atlas (TCGA). By selecting the most highly connected miRNA\nmolecules in the fitted network we find that nearly all of them are known to be\ninvolved in the regulation of breast cancer. \n\n"}
{"id": "1708.04935", "contents": "Title: Spatio-Temporal Big Data Analysis for Smart Grids Based on Random Matrix\n  Theory: A Comprehensive Study Abstract: A cornerstone of the smart grid is the advanced monitorability on its assets\nand operations. Increasingly pervasive installation of the phasor measurement\nunits (PMUs) allows the so-called synchrophasor measurements to be taken\nroughly 100 times faster than the legacy supervisory control and data\nacquisition (SCADA) measurements, time-stamped using the global positioning\nsystem (GPS) signals to capture the grid dynamics. On the other hand, the\navailability of low-latency two-way communication networks will pave the way to\nhigh-precision real-time grid state estimation and detection, remedial actions\nupon network instability, and accurate risk analysis and post-event assessment\nfor failure prevention.\n  In this chapter, we firstly modelling spatio-temporal PMU data in large scale\ngrids as random matrix sequences. Secondly, some basic principles of random\nmatrix theory (RMT), such as asymptotic spectrum laws, transforms, convergence\nrate and free probability, are introduced briefly in order to the better\nunderstanding and application of RMT technologies. Lastly, the case studies\nbased on synthetic data and real data are developed to evaluate the performance\nof the RMT-based schemes in different application scenarios (i.e., state\nevaluation and situation awareness). \n\n"}
{"id": "1708.04941", "contents": "Title: Minimax estimation of qubit states with Bures risk Abstract: The central problem of quantum statistics is to devise measurement schemes\nfor the estimation of an unknown state, given an ensemble of $n$ independent\nidentically prepared systems. For locally quadratic loss functions, the risk of\nstandard procedures has the usual scaling of $1/n$. However, it has been\nnoticed that for fidelity based metrics such as the Bures distance, the risk of\nconventional (non-adaptive) qubit tomography schemes scales as $1/\\sqrt{n}$ for\nstates close to the boundary of the Bloch sphere. Several proposed estimators\nappear to improve this scaling, and our goal is to analyse the problem from the\nperspective of the maximum risk over all states.\n  We propose qubit estimation strategies based on separate and adaptive\nmeasurements, that achieve $1/n$ scalings for the maximum Bures risk. The\nestimator involving local measurements uses a fixed fraction of the available\nresource $n$ to estimate the Bloch vector direction; the length of the Bloch\nvector is then estimated from the remaining copies by measuring in the\nestimator eigenbasis. The estimator based on collective measurements uses local\nasymptotic normality techniques which allows us derive upper and lower bounds\nto its maximum Bures risk. We also discuss how to construct a minimax optimal\nestimator in this setup. Finally, we consider quantum relative entropy and show\nthat the risk of the estimator based on collective measurements achieves a rate\n$O(n^{-1}\\log n)$ under this loss function. Furthermore, we show that no\nestimator can achieve faster rates, in particular the `standard' rate $1/n$. \n\n"}
{"id": "1708.05047", "contents": "Title: Bayesian Network Regularized Regression for Modeling Urban Crime\n  Occurrences Abstract: Analyses of occurrences of residential burglary in urban areas have shown\nthat crime rates are not spatially homogeneous: rates vary across the network\nof city streets, resulting in some areas being far more susceptible to crime\nthan others. The explanation for why a certain segment of the city experiences\nhigh crime may be different than why a neighboring area experiences high crime.\nMotivated by the importance of understanding spatial patterns such as these, we\nconsider a statistical model of burglary defined on the street network of\nBoston, Massachusetts. Leveraging ideas from functional data analysis, our\nproposed solution consists of a generalized linear model with vertex-indexed\ncovariates, allowing for an interpretation of the covariate effects at the\nstreet level. We employ a regularization procedure cast as a prior distribution\non the regression coefficients under a Bayesian setup, so that the predicted\nresponses vary smoothly according to the connectivity of the city. We introduce\na novel variable selection procedure, examine computationally efficient methods\nfor sampling from the posterior distribution of the model parameters, and\ndemonstrate the flexibility of our proposed modeling structure. The resulting\nmodel and interpretations provide insight into the spatial network patterns and\ndynamics of residential burglary in Boston. \n\n"}
{"id": "1708.07213", "contents": "Title: The duration of load effect in lumber as stochastic degradation Abstract: This paper proposes a gamma process for modelling the damage that accumulates\nover time in the lumber used in structural engineering applications when stress\nis applied. The model separates the stochastic processes representing features\ninternal to the piece of lumber on the one hand, from those representing\nexternal forces due to applied dead and live loads. The model applies those\nexternal forces through a time-varying population level function designed for\ntime-varying loads. The application of this type of model, which is standard in\nreliability analysis, is novel in this context, which has been dominated by\naccumulated damage models (ADMs) over more than half a century. The proposed\nmodel is compared with one of the traditional ADMs. Our statistical results\nbased on a Bayesian analysis of experimental data highlight the limitations of\nusing accelerated testing data to assess long-term reliability, as seen in the\nwide posterior intervals. This suggests the need for more comprehensive testing\nin future applications, or to encode appropriate expert knowledge in the priors\nused for Bayesian analysis. \n\n"}
{"id": "1708.07451", "contents": "Title: Recovering Structured Data From Superimposed Non-Linear Measurements Abstract: This work deals with the problem of distributed data acquisition under\nnon-linear communication constraints. More specifically, we consider a model\nsetup where $M$ distributed nodes take individual measurements of an unknown\nstructured source vector $x_0 \\in \\mathbb{R}^n$, communicating their readings\nsimultaneously to a central receiver. Since this procedure involves collisions\nand is usually imperfect, the receiver measures a superposition of non-linearly\ndistorted signals. In a first step, we will show that an $s$-sparse vector\n$x_0$ can be successfully recovered from $O(s \\cdot\\log(2n/s))$ of such\nsuperimposed measurements, using a traditional Lasso estimator that does not\nrely on any knowledge about the non-linear corruptions. This direct method\nhowever fails to work for several \"uncalibrated\" system configurations. These\nblind reconstruction tasks can be easily handled with the\n$\\ell^{1,2}$-Group-Lasso, but coming along with an increased sampling rate of\n$O(s\\cdot \\max\\{M, \\log(2n/s) \\})$ observations - in fact, the purpose of this\nlifting strategy is to extend a certain class of bilinear inverse problems to\nnon-linear acquisition. Our two algorithmic approaches are a special instance\nof a more abstract framework which includes sub-Gaussian measurement designs as\nwell as general (convex) structural constraints. These results are of\nindependent interest for various recovery and learning tasks, as they apply to\narbitrary non-linear observation models. Finally, to illustrate the practical\nscope of our theoretical findings, an application to wireless sensor networks\nis discussed, which actually serves as the prototypical example of our\nmethodology. \n\n"}
{"id": "1708.07961", "contents": "Title: Ultra-Dense Networks: A New Look at the Proportional Fair Scheduler Abstract: In this paper, we theoretically study the proportional fair (PF) scheduler in\nthe context of ultra-dense networks (UDNs). Analytical results are obtained for\nthe coverage probability and the area spectral efficiency (ASE) performance of\ndense small cell networks (SCNs) with the PF scheduler employed at base\nstations (BSs). The key point of our analysis is that the typical user is no\nlonger a random user as assumed in most studies in the literature. Instead, a\nuser with the maximum PF metric is chosen by its serving BS as the typical\nuser. By comparing the previous results of the round-robin (RR) scheduler with\nour new results of the PF scheduler, we quantify the loss of the multi-user\ndiversity of the PF scheduler with the network densification, which casts a new\nlook at the role of the PF scheduler in UDNs. Our conclusion is that the RR\nscheduler should be used in UDNs to simplify the radio resource management\n(RRM). \n\n"}
{"id": "1708.08120", "contents": "Title: MSM lag time cannot be used for variational model selection Abstract: The variational principle for conformational dynamics has enabled the\nsystematic construction of Markov state models through the optimization of\nhyperparameters by approximating the transfer operator. In this note we discuss\nwhy lag time of the operator being approximated must be held constant in the\nvariational approach. \n\n"}
{"id": "1708.08522", "contents": "Title: Causal Inference Under Network Interference: A Framework for Experiments\n  on Social Networks Abstract: No man is an island, as individuals interact and influence one another daily\nin our society. When social influence takes place in experiments on a\npopulation of interconnected individuals, the treatment on a unit may affect\nthe outcomes of other units, a phenomenon known as interference. This thesis\ndevelops a causal framework and inference methodology for experiments where\ninterference takes place on a network of influence (i.e. network interference).\nIn this framework, the network potential outcomes serve as the key quantity and\nflexible building blocks for causal estimands that represent a variety of\nprimary, peer, and total treatment effects. These causal estimands are\nestimated via principled Bayesian imputation of missing outcomes. The theory on\nthe unconfoundedness assumptions leading to simplified imputation highlights\nthe importance of including relevant network covariates in the potential\noutcome model. Additionally, experimental designs that result in balanced\ncovariates and sizes across treatment exposure groups further improve the\ncausal estimate, especially by mitigating potential outcome model\nmis-specification. The true potential outcome model is not typically known in\nreal-world experiments, so the best practice is to account for interference and\nconfounding network covariates through both balanced designs and model-based\nimputation. A full factorial simulated experiment is formulated to demonstrate\nthis principle by comparing performance across different randomization schemes\nduring the design phase and estimators during the analysis phase, under varying\nnetwork topology and true potential outcome models. Overall, this thesis\nasserts that interference is not just a nuisance for analysis but rather an\nopportunity for quantifying and leveraging peer effects in real-world\nexperiments. \n\n"}
{"id": "1708.09021", "contents": "Title: A Connectedness Constraint for Learning Sparse Graphs Abstract: Graphs are naturally sparse objects that are used to study many problems\ninvolving networks, for example, distributed learning and graph signal\nprocessing. In some cases, the graph is not given, but must be learned from the\nproblem and available data. Often it is desirable to learn sparse graphs.\nHowever, making a graph highly sparse can split the graph into several\ndisconnected components, leading to several separate networks. The main\ndifficulty is that connectedness is often treated as a combinatorial property,\nmaking it hard to enforce in e.g. convex optimization problems. In this\narticle, we show how connectedness of undirected graphs can be formulated as an\nanalytical property and can be enforced as a convex constraint. We especially\nshow how the constraint relates to the distributed consensus problem and graph\nLaplacian learning. Using simulated and real data, we perform experiments to\nlearn sparse and connected graphs from data. \n\n"}
{"id": "1709.00957", "contents": "Title: Edge Caching in Dense Heterogeneous Cellular Networks with Massive MIMO\n  Aided Self-backhaul Abstract: This paper focuses on edge caching in dense heterogeneous cellular networks\n(HetNets), in which small base stations (SBSs) with limited cache size store\nthe popular contents, and massive multiple-input multiple-output (MIMO) aided\nmacro base stations provide wireless self-backhaul when SBSs require the\nnon-cached contents. Our aim is to address the effects of cell load and hit\nprobability on the successful content delivery (SCD), and present the minimum\nrequired base station density for avoiding the access overload in an arbitrary\nsmall cell and backhaul overload in an arbitrary macrocell. The massive MIMO\nbackhaul achievable rate without downlink channel estimation is derived to\ncalculate the backhaul time, and the latency is also evaluated in such\nnetworks. The analytical results confirm that hit probability needs to be\nappropriately selected, in order to achieve SCD. The interplay between cache\nsize and SCD is explicitly quantified. It is theoretically demonstrated that\nwhen non-cached contents are requested, the average delay of the non-cached\ncontent delivery could be comparable to the cached content delivery with the\nhelp of massive MIMO aided self-backhaul, if the average access rate of cached\ncontent delivery is lower than that of self-backhauled content delivery.\nSimulation results are presented to validate our analysis. \n\n"}
{"id": "1709.01282", "contents": "Title: Implicit Cooperative Positioning in Vehicular Networks Abstract: Absolute positioning of vehicles is based on Global Navigation Satellite\nSystems (GNSS) combined with on-board sensors and high-resolution maps. In\nCooperative Intelligent Transportation Systems (C-ITS), the positioning\nperformance can be augmented by means of vehicular networks that enable\nvehicles to share location-related information. This paper presents an Implicit\nCooperative Positioning (ICP) algorithm that exploits the Vehicle-to-Vehicle\n(V2V) connectivity in an innovative manner, avoiding the use of explicit V2V\nmeasurements such as ranging. In the ICP approach, vehicles jointly localize\nnon-cooperative physical features (such as people, traffic lights or inactive\ncars) in the surrounding areas, and use them as common noisy reference points\nto refine their location estimates. Information on sensed features are fused\nthrough V2V links by a consensus procedure, nested within a message passing\nalgorithm, to enhance the vehicle localization accuracy. As positioning does\nnot rely on explicit ranging information between vehicles, the proposed ICP\nmethod is amenable to implementation with off-the-shelf vehicular communication\nhardware. The localization algorithm is validated in different traffic\nscenarios, including a crossroad area with heterogeneous conditions in terms of\nfeature density and V2V connectivity, as well as a real urban area by using\nSimulation of Urban MObility (SUMO) for traffic data generation. Performance\nresults show that the proposed ICP method can significantly improve the vehicle\nlocation accuracy compared to the stand-alone GNSS, especially in harsh\nenvironments, such as in urban canyons, where the GNSS signal is highly\ndegraded or denied. \n\n"}
{"id": "1709.01413", "contents": "Title: The Calculus of M-estimation in R with geex Abstract: M-estimation, or estimating equation, methods are widely applicable for point\nestimation and asymptotic inference. In this paper, we present an R package\nthat can find roots and compute the empirical sandwich variance estimator for\nany set of user-specified, unbiased estimating equations. Examples from the\nM-estimation primer by Stefanski and Boos (2002) demonstrate use of the\nsoftware. The package also includes a framework for finite sample variance\ncorrections and a website with an extensive collection of tutorials. \n\n"}
{"id": "1709.01596", "contents": "Title: Evaluating Partisan Gerrymandering in Wisconsin Abstract: We examine the extent of gerrymandering for the 2010 General Assembly\ndistrict map of Wisconsin. We find that there is substantial variability in the\nelection outcome depending on what maps are used. We also found robust evidence\nthat the district maps are highly gerrymandered and that this gerrymandering\nlikely altered the partisan make up of the Wisconsin General Assembly in some\nelections. Compared to the distribution of possible redistricting plans for the\nGeneral Assembly, Wisconsin's chosen plan is an outlier in that it yields\nresults that are highly skewed to the Republicans when the statewide proportion\nof Democratic votes comprises more than 50-52% of the overall vote (with the\nprecise threshold depending on the election considered). Wisconsin's plan acts\nto preserve the Republican majority by providing extra Republican seats even\nwhen the Democratic vote increases into the range when the balance of power\nwould shift for the vast majority of redistricting plans. \n\n"}
{"id": "1709.02357", "contents": "Title: Learning from lions: inferring the utility of agents from their\n  trajectories Abstract: We build a model using Gaussian processes to infer a spatio-temporal vector\nfield from observed agent trajectories. Significant landmarks or influence\npoints in agent surroundings are jointly derived through vector calculus\noperations that indicate presence of sources and sinks. We evaluate these\ninfluence points by using the Kullback-Leibler divergence between the posterior\nand prior Laplacian of the inferred spatio-temporal vector field. Through\nlocating significant features that influence trajectories, our model aims to\ngive greater insight into underlying causal utility functions that determine\nagent decision-making. A key feature of our model is that it infers a joint\nGaussian process over the observed trajectories, the time-varying vector field\nof utility and canonical vector calculus operators. We apply our model to both\nsynthetic data and lion GPS data collected at the Bubye Valley Conservancy in\nsouthern Zimbabwe. \n\n"}
{"id": "1709.04196", "contents": "Title: Particle Filters and Data Assimilation Abstract: State-space models can be used to incorporate subject knowledge on the\nunderlying dynamics of a time series by the introduction of a latent Markov\nstate-process. A user can specify the dynamics of this process together with\nhow the state relates to partial and noisy observations that have been made.\nInference and prediction then involves solving a challenging inverse problem:\ncalculating the conditional distribution of quantities of interest given the\nobservations. This article reviews Monte Carlo algorithms for solving this\ninverse problem, covering methods based on the particle filter and the ensemble\nKalman filter. We discuss the challenges posed by models with high-dimensional\nstates, joint estimation of parameters and the state, and inference for the\nhistory of the state process. We also point out some potential new developments\nwhich will be important for tackling cutting-edge filtering applications. \n\n"}
{"id": "1709.06928", "contents": "Title: Level-Triggered Harvest-then-Consume Protocol with Two Bits or Less\n  Energy State Information Abstract: We propose a variation of harvest-then-consume protocol with low complexity\nwhere the harvest and consume phases change when the battery energy level\nreaches certain thresholds. The proposed protocol allows us to control the\npossible energy outage during consumption phase. Assuming that the battery is\nperfect and that the energy arrival is a renewal process, we analyze the duty\ncycle and the operating cycle speed of the protocol. The proposed protocol also\nallows for limited battery energy state information. The cases when the system\nhas two-bits, one-bit, and zero-bit of battery energy state information are\nstudied in detail. Numerical simulations verify the obtained formulas. \n\n"}
{"id": "1709.07032", "contents": "Title: Data-Driven Model Predictive Control of Autonomous Mobility-on-Demand\n  Systems Abstract: The goal of this paper is to present an end-to-end, data-driven framework to\ncontrol Autonomous Mobility-on-Demand systems (AMoD, i.e. fleets of\nself-driving vehicles). We first model the AMoD system using a time-expanded\nnetwork, and present a formulation that computes the optimal rebalancing\nstrategy (i.e., preemptive repositioning) and the minimum feasible fleet size\nfor a given travel demand. Then, we adapt this formulation to devise a Model\nPredictive Control (MPC) algorithm that leverages short-term demand forecasts\nbased on historical data to compute rebalancing strategies. We test the\nend-to-end performance of this controller with a state-of-the-art LSTM neural\nnetwork to predict customer demand and real customer data from DiDi Chuxing: we\nshow that this approach scales very well for large systems (indeed, the\ncomputational complexity of the MPC algorithm does not depend on the number of\ncustomers and of vehicles in the system) and outperforms state-of-the-art\nrebalancing strategies by reducing the mean customer wait time by up to to\n89.6%. \n\n"}
{"id": "1709.07716", "contents": "Title: Testing first-order intensity model in non-homogeneous Poisson point\n  processes with covariates Abstract: Modelling the first-order intensity function is one of the main aims in point\nprocess theory, and it has been approached so far from different perspectives.\nOne appealing model describes the intensity as a function of a spatial\ncovariate. In the recent literature, estimation theory and several applications\nhave been developed assuming this model, but without formally checking this\nassumption. In this paper we address this problem for a non-homogeneous Poisson\npoint process, by proposing a new test based on an $L^2$-distance. We also\nprove the asymptotic normality of the statistic and we suggest a bootstrap\nprocedure to accomplish the calibration. Two applications with real data are\npresented and a simulation study to better understand the performance of our\nproposals is accomplished. Finally some possible extensions of the present work\nto non-Poisson processes and to a multi-dimensional covariate context are\ndetailed. \n\n"}
{"id": "1709.08238", "contents": "Title: Counterparty Credit Limits: The Impact of a Risk-Mitigation Measure on\n  Everyday Trading Abstract: A counterparty credit limit (CCL) is a limit that is imposed by a financial\ninstitution to cap its maximum possible exposure to a specified counterparty.\nCCLs help institutions to mitigate counterparty credit risk via selective\ndiversification of their exposures. In this paper, we analyze how CCLs impact\nthe prices that institutions pay for their trades during everyday trading. We\nstudy a high-quality data set from a large electronic trading platform in the\nforeign exchange spot market, which enables institutions to apply CCLs. We find\nempirically that CCLs had little impact on the vast majority of trades in this\ndata. We also study the impact of CCLs using a new model of trading. By\nsimulating our model with different underlying CCL networks, we highlight that\nCCLs can have a major impact in some situations. \n\n"}
{"id": "1709.08626", "contents": "Title: A general framework for data-driven uncertainty quantification under\n  complex input dependencies using vine copulas Abstract: Systems subject to uncertain inputs produce uncertain responses. Uncertainty\nquantification (UQ) deals with the estimation of statistics of the system\nresponse, given a computational model of the system and a probabilistic model\nof its inputs. In engineering applications it is common to assume that the\ninputs are mutually independent or coupled by a Gaussian or elliptical\ndependence structure (copula). In this paper we overcome such limitations by\nmodelling the dependence structure of multivariate inputs as vine copulas. Vine\ncopulas are models of multivariate dependence built from simpler pair-copulas.\nThe vine representation is flexible enough to capture complex dependencies.\nThis paper formalises the framework needed to build vine copula models of\nmultivariate inputs and to combine them with virtually any UQ method. The\nframework allows for a fully automated, data-driven inference of the\nprobabilistic input model on available input data. The procedure is exemplified\non two finite element models of truss structures, both subject to inputs with\nnon-Gaussian dependence structures. For each case, we analyse the moments of\nthe model response (using polynomial chaos expansions), and perform a\nstructural reliability analysis to calculate the probability of failure of the\nsystem (using the first order reliability method and importance sampling).\nReference solutions are obtained by Monte Carlo simulation. The results show\nthat, while the Gaussian assumption yields biased statistics, the vine copula\nrepresentation achieves significantly more precise estimates, even when its\nstructure needs to be fully inferred from a limited amount of observations. \n\n"}
{"id": "1709.10298", "contents": "Title: Structure estimation of binary graphical models on stratified data:\n  application to the description of injury tables for victims of road accidents Abstract: Graphical models are used in many applications such as medical diagnostic,\ncomputer security, etc. More and more often, the estimation of such models has\nto be performed on several predefined strata of the whole population. For\ninstance, in epidemiology and clinical research, strata are often defined\naccording to age, gender, treatment or disease type, etc. In this article, we\npropose new approaches aimed at estimating binary graphical models on such\nstrata. Our approaches are obtained by combining well-known methods when\nestimating one single binary graphical model, with penalties encouraging\nstructured sparsity, and which have recently been shown appropriate when\ndealing with stratified data. Empirical comparions on synthetic data highlight\nthat our approaches generally outperform the competitors we considered. An\napplication is provided where we study associations among injuries suffered by\nvictims of road accidents according to road user type. \n\n"}
{"id": "1710.00173", "contents": "Title: Multi-Scale Pipeline for the Search of String-Induced CMB Anisotropies Abstract: We propose a multi-scale edge-detection algorithm to search for the\nGott-Kaiser-Stebbins imprints of a cosmic string (CS) network on the Cosmic\nMicrowave Background (CMB) anisotropies. Curvelet decomposition and extended\nCanny algorithm are used to enhance the string detectability. Various\nstatistical tools are then applied to quantify the deviation of CMB maps having\na cosmic string contribution with respect to pure Gaussian anisotropies of\ninflationary origin. These statistical measures include the one-point\nprobability density function, the weighted two-point correlation function\n(TPCF) of the anisotropies, the unweighted TPCF of the peaks and of the\nup-crossing map, as well as their cross-correlation. We use this algorithm on a\nhundred of simulated Nambu-Goto CMB flat sky maps, covering approximately\n$10\\%$ of the sky, and for different string tensions $G\\mu$. On noiseless sky\nmaps with an angular resolution of $0.9'$, we show that our pipeline detects\nCSs with $G\\mu$ as low as $G\\mu\\gtrsim 4.3\\times 10^{-10}$. At the same\nresolution, but with a noise level typical to a CMB-S4 phase II experiment, the\ndetection threshold would be to $G\\mu\\gtrsim 1.2 \\times 10^{-7}$. \n\n"}
{"id": "1710.00862", "contents": "Title: Testing for Global Network Structure Using Small Subgraph Statistics Abstract: We study the problem of testing for community structure in networks using\nrelations between the observed frequencies of small subgraphs. We propose a\nsimple test for the existence of communities based only on the frequencies of\nthree-node subgraphs. The test statistic is shown to be asymptotically normal\nunder a null assumption of no community structure, and to have power\napproaching one under a composite alternative hypothesis of a degree-corrected\nstochastic block model. We also derive a version of the test that applies to\nmultivariate Gaussian data. Our approach achieves near-optimal detection rates\nfor the presence of community structure, in regimes where the signal-to-noise\nis too weak to explicitly estimate the communities themselves, using existing\ncomputationally efficient algorithms. We demonstrate how the method can be\neffective for detecting structure in social networks, citation networks for\nscientific articles, and correlations of stock returns between companies on the\nS\\&P 500. \n\n"}
{"id": "1710.00875", "contents": "Title: Local likelihood estimation of complex tail dependence structures,\n  applied to U.S. precipitation extremes Abstract: To disentangle the complex non-stationary dependence structure of\nprecipitation extremes over the entire contiguous U.S., we propose a flexible\nlocal approach based on factor copula models. Our sub-asymptotic spatial\nmodeling framework yields non-trivial tail dependence structures, with a\nweakening dependence strength as events become more extreme, a feature commonly\nobserved with precipitation data but not accounted for in classical asymptotic\nextreme-value models. To estimate the local extremal behavior, we fit the\nproposed model in small regional neighborhoods to high threshold exceedances,\nunder the assumption of local stationarity, which allows us to gain in\nflexibility. Adopting a local censored likelihood approach, inference is made\non a fine spatial grid, and local estimation is performed by taking advantage\nof distributed computing resources and the embarrassingly parallel nature of\nthis estimation procedure. The local model is efficiently fitted at all grid\npoints, and uncertainty is measured using a block bootstrap procedure. An\nextensive simulation study shows that our approach can adequately capture\ncomplex, non-stationary dependencies, while our study of U.S. winter\nprecipitation data reveals interesting differences in local tail structures\nover space, which has important implications on regional risk assessment of\nextreme precipitation events. \n\n"}
{"id": "1710.01054", "contents": "Title: Parameter estimation of platelets deposition: Approximate Bayesian\n  computation with high performance computing Abstract: Recent studies show the existing clinical tests to detect\nCardio/cerebrovascular diseases (CVD) are ineffectual as they do not consider\ndifferent stages of platelet activation or the molecular dynamics involved in\nplatelet interactions. Further they are also incapable to consider\ninter-individual variability. A physical description of platelets deposition\nwas introduced recently in Chopard et. al. [2017], by integrating fundamental\nunderstandings of how platelets interact in a numerical model, parameterized by\nfive parameters. These parameters specify the deposition process and are\nrelevant for a biomedical understanding of the phenomena. One of the main\nintuition is that these parameters are precisely the information needed for a\npathological test identifying CVD captured and that they capture the\ninter-individual variability. Following this intuition, here we devise a\nBayesian inferential scheme for estimation of these parameters. As the\nlikelihood function of the numerical model is intractable due to the complex\nstochastic nature of the model, we use a likelihood-free inference scheme\napproximate Bayesian computation (ABC) to calibrate the parameters in a\ndata-driven manner. As ABC requires the generation of many pseudo-data by\nexpensive simulation runs, we use a high performance computing (HPC) framework\nfor ABC to make the inference possible for this model. We illustrate that our\nmean posterior prediction of platelet deposition pattern matches the\nexperimental dataset closely with a tight posterior prediction error margin for\na collective dataset of 7 volunteers. The present approach can be used to build\na new generation of personalized platelet functionality tests for CVD\ndetection, using numerical modeling of platelet deposition, Bayesian\nuncertainty quantification and High performance computing. \n\n"}
{"id": "1710.02976", "contents": "Title: Quantifying uncertainty in thermal properties of walls by means of\n  Bayesian inversion Abstract: We introduce a computational framework to statistically infer thermophysical\nproperties of any given wall from in-situ measurements of air temperature and\nsurface heat fluxes. The proposed framework uses these measurements, within a\nBayesian calibration approach, to sequentially infer input parameters of a\none-dimensional heat diffusion model that describes the thermal performance of\nthe wall. These inputs include spatially-variable functions that characterise\nthe thermal conductivity and the volumetric heat capacity of the wall. We\nencode our computational framework in an algorithm that sequentially updates\nour probabilistic knowledge of the thermophysical properties as new\nmeasurements become available, and thus enables an on-the-fly uncertainty\nquantification of these properties. In addition, the proposed algorithm enables\nus to investigate the effect of the discretisation of the underlying heat\ndiffusion model on the accuracy of estimates of thermophysical properties and\nthe corresponding predictive distributions of heat flux. By means of\nvirtual/synthetic and real experiments we show the capabilities of the proposed\napproach to (i) characterise heterogenous thermophysical properties associated\nwith, for example, unknown cavities and insulators; (ii) obtain rapid and\naccurate uncertainty estimates of effective thermal properties (e.g. thermal\ntransmittance); and (iii) accurately compute an statistical description of the\nthermal performance of the wall which is, in turn, crucial in evaluating\npossible retrofit measures. \n\n"}
{"id": "1710.03296", "contents": "Title: Testing for Network and Spatial Autocorrelation Abstract: Testing for dependence has been a well-established component of spatial\nstatistical analyses for decades. In particular, several popular test\nstatistics have desirable properties for testing for the presence of spatial\nautocorrelation in continuous variables. In this paper we propose two\ncontributions to the literature on tests for autocorrelation. First, we propose\na new test for autocorrelation in categorical variables. While some methods\ncurrently exist for assessing spatial autocorrelation in categorical variables,\nthe most popular method is unwieldy, somewhat ad hoc, and fails to provide\ngrounds for a single omnibus test. Second, we discuss the importance of testing\nfor autocorrelation in data sampled from the nodes of a network, motivated by\nsocial network applications. We demonstrate that our proposed statistic for\ncategorical variables can both be used in the spatial and network setting. \n\n"}
{"id": "1710.04749", "contents": "Title: Explaining Aviation Safety Incidents Using Deep Temporal Multiple\n  Instance Learning Abstract: Although aviation accidents are rare, safety incidents occur more frequently\nand require a careful analysis to detect and mitigate risks in a timely manner.\nAnalyzing safety incidents using operational data and producing event-based\nexplanations is invaluable to airline companies as well as to governing\norganizations such as the Federal Aviation Administration (FAA) in the United\nStates. However, this task is challenging because of the complexity involved in\nmining multi-dimensional heterogeneous time series data, the lack of\ntime-step-wise annotation of events in a flight, and the lack of scalable tools\nto perform analysis over a large number of events. In this work, we propose a\nprecursor mining algorithm that identifies events in the multidimensional time\nseries that are correlated with the safety incident. Precursors are valuable to\nsystems health and safety monitoring and in explaining and forecasting safety\nincidents. Current methods suffer from poor scalability to high dimensional\ntime series data and are inefficient in capturing temporal behavior. We propose\nan approach by combining multiple-instance learning (MIL) and deep recurrent\nneural networks (DRNN) to take advantage of MIL's ability to learn using weakly\nsupervised data and DRNN's ability to model temporal behavior. We describe the\nalgorithm, the data, the intuition behind taking a MIL approach, and a\ncomparative analysis of the proposed algorithm with baseline models. We also\ndiscuss the application to a real-world aviation safety problem using data from\na commercial airline company and discuss the model's abilities and\nshortcomings, with some final remarks about possible deployment directions. \n\n"}
{"id": "1710.05008", "contents": "Title: Automatic Detection and Uncertainty Quantification of Landmarks on\n  Elastic Curves Abstract: A population quantity of interest in statistical shape analysis is the\nlocation of landmarks, which are points that aid in reconstructing and\nrepresenting shapes of objects. We provide an automated, model-based approach\nto inferring landmarks given a sample of shape data. The model is formulated\nbased on a linear reconstruction of the shape, passing through the specified\npoints, and a Bayesian inferential approach is described for estimating unknown\nlandmark locations. The question of how many landmarks to select is addressed\nin two different ways: (1) by defining a criterion-based approach, and (2)\njoint estimation of the number of landmarks along with their locations.\nEfficient methods for posterior sampling are also discussed. We motivate our\napproach using several simulated examples, as well as data obtained from\napplications in computer vision and biology; additionally, we explore\nplacements and associated uncertainty in landmarks for various substructures\nextracted from magnetic resonance image slices. \n\n"}
{"id": "1710.05284", "contents": "Title: Multivariate Generalized Linear Mixed Models for Joint Estimation of\n  Sporting Outcomes Abstract: This paper explores improvements in prediction accuracy and inference\ncapability when allowing for potential correlation in team-level random effects\nacross multiple game-level responses from different assumed distributions.\nFirst-order and fully exponential Laplace approximations are used to fit\nnormal-binary and Poisson-binary multivariate generalized linear mixed models\nwith non-nested random effects structures. We have built these models into the\nR package mvglmmRank, which is used to explore several seasons of American\ncollege football and basketball data. \n\n"}
{"id": "1710.06012", "contents": "Title: VAMPnets: Deep learning of molecular kinetics Abstract: There is an increasing demand for computing the relevant structures,\nequilibria and long-timescale kinetics of biomolecular processes, such as\nprotein-drug binding, from high-throughput molecular dynamics simulations.\nCurrent methods employ transformation of simulated coordinates into structural\nfeatures, dimension reduction, clustering the dimension-reduced data, and\nestimation of a Markov state model or related model of the interconversion\nrates between molecular structures. This handcrafted approach demands a\nsubstantial amount of modeling expertise, as poor decisions at any step will\nlead to large modeling errors. Here we employ the variational approach for\nMarkov processes (VAMP) to develop a deep learning framework for molecular\nkinetics using neural networks, dubbed VAMPnets. A VAMPnet encodes the entire\nmapping from molecular coordinates to Markov states, thus combining the whole\ndata processing pipeline in a single end-to-end framework. Our method performs\nequally or better than state-of-the art Markov modeling methods and provides\neasily interpretable few-state kinetic models. \n\n"}
{"id": "1710.06907", "contents": "Title: Particle-based membrane model for mesoscopic simulation of cellular\n  dynamics Abstract: We present a simple and computationally efficient coarse-grained and\nsolvent-free model for simulating lipid bilayer membranes. In order to be used\nin concert with particle-based reaction-diffusion simulations, the model is\npurely based on interacting and reacting particles, each representing a coarse\npatch of a lipid monolayer. Particle interactions include nearest-neighbor\nbond-stretching and angle-bending, and are parameterized so as to reproduce the\nlocal membrane mechanics given by the Helfrich energy density over a range of\nrelevant curvatures. In-plane fluidity is implemented with Monte Carlo\nbond-flipping moves. The physical accuracy of the model is verified by five\ntests: (i) Power spectrum analysis of equilibrium thermal undulations is used\nto verify that the particle-based representation correctly captures the\ndynamics predicted by the continuum model of fluid membranes. (ii) It is\nverified that the input bending stiffness, against which the potential\nparameters are optimized, is accurately recovered. (iii) Isothermal area\ncompressibility modulus of the membrane is calculated and is shown to be\ntunable to reproduce available values for different lipid bilayers, independent\nof the bending rigidity. (iv) Simulation of two-dimensional shear flow under a\ngravity force is employed to measure the effective in-plane viscosity of the\nmembrane model, and show the possibility of modeling membranes with specified\nviscosities. (v) Interaction of the bilayer membrane with a spherical\nnanoparticle is modeled as a test case for large membrane deformations and\nbudding involved in cellular processes such as endocytosis... \n\n"}
{"id": "1710.07716", "contents": "Title: A Statistical Characterization of Localization Performance in Wireless\n  Networks Abstract: Localization performance in wireless networks has been traditionally\nbenchmarked using the Cramer-Rao lower bound (CRLB), given a fixed geometry of\nanchor nodes and a target. However, by endowing the target and anchor locations\nwith distributions, this paper recasts this traditional, scalar benchmark as a\nrandom variable. The goal of this work is to derive an analytical expression\nfor the distribution of this now random CRLB, in the context of\nTime-of-Arrival-based positioning.\n  To derive this distribution, this work first analyzes how the CRLB is\naffected by the order statistics of the angles between consecutive\nparticipating anchors (i.e., internodal angles). This analysis reveals an\nintimate connection between the second largest internodal angle and the CRLB,\nwhich leads to an accurate approximation of the CRLB. Using this approximation,\na closed-form expression for the distribution of the CRLB, conditioned on the\nnumber of participating anchors, is obtained.\n  Next, this conditioning is eliminated to derive an analytical expression for\nthe marginal CRLB distribution. Since this marginal distribution accounts for\nall target and anchor positions, across all numbers of participating anchors,\nit therefore statistically characterizes localization error throughout an\nentire wireless network. This paper concludes with a comprehensive analysis of\nthis new network-wide-CRLB paradigm. \n\n"}
{"id": "1710.08076", "contents": "Title: 3D ab initio modeling in cryo-EM by autocorrelation analysis Abstract: Single-Particle Reconstruction (SPR) in Cryo-Electron Microscopy (cryo-EM) is\nthe task of estimating the 3D structure of a molecule from a set of noisy 2D\nprojections, taken from unknown viewing directions. Many algorithms for SPR\nstart from an initial reference molecule, and alternate between refining the\nestimated viewing angles given the molecule, and refining the molecule given\nthe viewing angles. This scheme is called iterative refinement. Reliance on an\ninitial, user-chosen reference introduces model bias, and poor initialization\ncan lead to slow convergence. Furthermore, since no ground truth is available\nfor an unsolved molecule, it is difficult to validate the obtained results.\nThis creates the need for high quality ab initio models that can be quickly\nobtained from experimental data with minimal priors, and which can also be used\nfor validation. We propose a procedure to obtain such an ab initio model\ndirectly from raw data using Kam's autocorrelation method. Kam's method has\nbeen known since 1980, but it leads to an underdetermined system, with missing\northogonal matrices. Until now, this system has been solved only for special\ncases, such as highly symmetric molecules or molecules for which a homologous\nstructure was already available. In this paper, we show that knowledge of just\ntwo clean projections is sufficient to guarantee a unique solution to the\nsystem. This system is solved by an optimization-based heuristic. For the first\ntime, we are then able to obtain a low-resolution ab initio model of an\nasymmetric molecule directly from raw data, without 2D class averaging and\nwithout tilting. Numerical results are presented on both synthetic and\nexperimental data. \n\n"}
{"id": "1710.08112", "contents": "Title: Modeling rainfalls using a seasonal hidden markov model Abstract: In order to reach the supply/demand balance, electricity providers need to\npredict the demand and production of electricity at different time scales. This\nimplies the need of modeling weather variables such as temperature, wind speed,\nsolar radiation and precipitation. This work is dedicated to a new daily\nrainfall generator at a single site. It is based on a seasonal hidden Markov\nmodel with mixtures of exponential distributions as emission laws. The\nparameters of the exponential distributions include a periodic component in\norder to account for the seasonal behaviour of rainfall. We show that under\nmild assumptions , the maximum likelihood estimator is strongly consistent,\nwhich is a new result for such models. The model is able to produce arbitrarily\nlong daily rainfall simulations that reproduce closely different features of\nobserved time series, including seasonality, rainfall occurrence , daily\ndistributions of rainfall, dry and rainy spells. The model was fitted and\nvalidated on data from several weather stations across Germany. We show that it\nis possible to give a physical interpretation to the estimated states. \n\n"}
{"id": "1710.08747", "contents": "Title: A hierarchical Bayesian perspective on majorization-minimization for\n  non-convex sparse regression: application to M/EEG source imaging Abstract: Majorization-minimization (MM) is a standard iterative optimization technique\nwhich consists in minimizing a sequence of convex surrogate functionals. MM\napproaches have been particularly successful to tackle inverse problems and\nstatistical machine learning problems where the regularization term is a\nsparsity-promoting concave function. However, due to non-convexity, the\nsolution found by MM depends on its initialization. Uniform initialization is\nthe most natural and often employed strategy as it boils down to penalizing all\ncoefficients equally in the first MM iteration. Yet, this arbitrary choice can\nlead to unsatisfactory results in severely under-determined inverse problems\nsuch as source imaging with magneto- and electro-encephalography (M/EEG). The\nframework of hierarchical Bayesian modeling (HBM) is an alternative approach to\nencode sparsity. This work shows that for certain hierarchical models, a simple\nalternating scheme to compute fully Bayesian maximum a posteriori (MAP)\nestimates leads to the exact same sequence of updates as a standard MM strategy\n(cf. the Adaptive Lasso). With this parallel outlined, we show how to improve\nupon these MM techniques by probing the multimodal posterior density using\nMarkov Chain Monte-Carlo (MCMC) techniques. Firstly, we show that these samples\ncan provide well-informed initializations that help MM schemes to reach better\nlocal minima. Secondly, we demonstrate how it can reveal the different modes of\nthe posterior distribution in order to explore and quantify the inherent\nuncertainty and ambiguity of such ill-posed inference procedure. In the context\nof M/EEG, each mode corresponds to a plausible configuration of neural sources,\nwhich is crucial for data interpretation, especially in clinical contexts.\nResults on both simulations and real datasets show how the number or the type\nof sensors affect the uncertainties on the estimates. \n\n"}
{"id": "1710.10351", "contents": "Title: Bayesian Spatial Binary Regression for Label Fusion in Structural\n  Neuroimaging Abstract: Alzheimer's disease is a neurodegenerative condition that accelerates\ncognitive decline relative to normal aging. It is of critical scientific\nimportance to gain a better understanding of early disease mechanisms in the\nbrain to facilitate effective, targeted therapies. The volume of the\nhippocampus is often used in diagnosis and monitoring of the disease. Measuring\nthis volume via neuroimaging is difficult since each hippocampus must either be\nmanually identified or automatically delineated, a task referred to as\nsegmentation. Automatic hippocampal segmentation often involves mapping a\npreviously manually segmented image to a new brain image and propagating the\nlabels to obtain an estimate of where each hippocampus is located in the new\nimage. A more recent approach to this problem is to propagate labels from\nmultiple manually segmented atlases and combine the results using a process\nknown as label fusion. To date, most label fusion algorithms employ voting\nprocedures with voting weights assigned directly or estimated via optimization.\nWe propose using a fully Bayesian spatial regression model for label fusion\nthat facilitates direct incorporation of covariate information while making\naccessible the entire posterior distribution. Our results suggest that\nincorporating tissue classification (e.g, gray matter) into the label fusion\nprocedure can greatly improve segmentation when relatively homogeneous, healthy\nbrains are used as atlases for diseased brains. The fully Bayesian approach\nalso produces meaningful uncertainty measures about hippocampal volumes,\ninformation which can be leveraged to detect significant, scientifically\nmeaningful differences between healthy and diseased populations, improving the\npotential for early detection and tracking of the disease. \n\n"}
{"id": "1710.11239", "contents": "Title: Time-lagged autoencoders: Deep learning of slow collective variables for\n  molecular kinetics Abstract: Inspired by the success of deep learning techniques in the physical and\nchemical sciences, we apply a modification of an autoencoder type deep neural\nnetwork to the task of dimension reduction of molecular dynamics data. We can\nshow that our time-lagged autoencoder reliably finds low-dimensional embeddings\nfor high-dimensional feature spaces which capture the slow dynamics of the\nunderlying stochastic processes - beyond the capabilities of linear dimension\nreduction techniques. \n\n"}
{"id": "1710.11404", "contents": "Title: Reshaping Cellular Networks for the Sky: Major Factors and Feasibility Abstract: This paper studies the feasibility of supporting drone operations using\nexistent cellular infrastructure. We propose an analytical framework that\nincludes the effects of base station (BS) height and antenna radiation pattern,\ndrone antenna directivity and various propagation environments. With this\nframework, we derive an exact expression for the coverage probability of ground\nand drone users through a practical cell association strategy. Our results show\nthat a carefully designed network can control the radiated interference that is\nreceived by the drones, and therefore guarantees a satisfactory quality of\nservice. Moreover, as the network density grows the increasing level of\ninterference can be partially managed by lowering the drone flying altitude.\nHowever, even at optimal conditions the drone coverage performance converges to\nzero considerably fast, suggesting that ultra-dense networks might be poor\ncandidates for serving aerial users. \n\n"}
{"id": "1711.00484", "contents": "Title: Spatial Statistical Downscaling for Constructing High-Resolution Nature\n  Runs in Global Observing System Simulation Experiments Abstract: Observing system simulation experiments (OSSEs) have been widely used as a\nrigorous and cost-effective way to guide development of new observing systems,\nand to evaluate the performance of new data assimilation algorithms. Nature\nruns (NRs), which are outputs from deterministic models, play an essential role\nin building OSSE systems for global atmospheric processes because they are used\nboth to create synthetic observations at high spatial resolution, and to\nrepresent the \"true\" atmosphere against which the forecasts are verified.\nHowever, most NRs are generated at resolutions coarser than actual\nobservations. Here, we propose a principled statistical downscaling framework\nto construct high-resolution NRs via conditional simulation from\ncoarse-resolution numerical model output. We use nonstationary spatial\ncovariance function models that have basis function representations. This\napproach not only explicitly addresses the change-of-support problem, but also\nallows fast computation with large volumes of numerical model output. We also\npropose a data-driven algorithm to select the required basis functions\nadaptively, in order to increase the flexibility of our nonstationary\ncovariance function models. In this article we demonstrate these techniques by\ndownscaling a coarse-resolution physical NR at a native resolution of\n$1^{\\circ} \\text{ latitude} \\times 1.25^{\\circ} \\text{ longitude}$ of global\nsurface $\\text{CO}_2$ concentrations to 655,362 equal-area hexagons. \n\n"}
{"id": "1711.00564", "contents": "Title: Sophisticated and small versus simple and sizeable: When does it pay off\n  to introduce drifting coefficients in Bayesian VARs? Abstract: We assess the relationship between model size and complexity in the\ntime-varying parameter VAR framework via thorough predictive exercises for the\nEuro Area, the United Kingdom and the United States. It turns out that\nsophisticated dynamics through drifting coefficients are important in small\ndata sets, while simpler models tend to perform better in sizeable data sets.\nTo combine the best of both worlds, novel shrinkage priors help to mitigate the\ncurse of dimensionality, resulting in competitive forecasts for all scenarios\nconsidered. Furthermore, we discuss dynamic model selection to improve upon the\nbest performing individual model for each point in time. \n\n"}
{"id": "1711.00708", "contents": "Title: On Game-Theoretic Risk Management (Part Three) - Modeling and\n  Applications Abstract: The game-theoretic risk management framework put forth in the precursor\nreports \"Towards a Theory of Games with Payoffs that are\nProbability-Distributions\" (arXiv:1506.07368 [q-fin.EC]) and \"Algorithms to\nCompute Nash-Equilibria in Games with Distributions as Payoffs\"\n(arXiv:1511.08591v1 [q-fin.EC]) is herein concluded by discussing how to\nintegrate the previously developed theory into risk management processes. To\nthis end, we discuss how loss models (primarily but not exclusively\nnon-parametric) can be constructed from data. Furthermore, hints are given on\nhow a meaningful game theoretic model can be set up, and how it can be used in\nvarious stages of the ISO 27000 risk management process. Examples related to\nadvanced persistent threats and social engineering are given. We conclude by a\ndiscussion on the meaning and practical use of (mixed) Nash equilibria\nequilibria for risk management. \n\n"}
{"id": "1711.01351", "contents": "Title: Uplink Performance Analysis of a Drone Cell in a Random Field of Ground\n  Interferers Abstract: Aerial base stations are a promising technology to increase the capabilities\nof the existing communication networks. However, the existing analytical\nframeworks do not sufficiently characterize the impact of ground interferers on\nthe aerial base stations. In order to address this issue, we model the effect\nof interference coming from the coexisting ground networks on the aerial link,\nwhich could be the uplink of an aerial cell served by a drone base station. By\nconsidering a Poisson field of ground interferers, we characterize the\naggregate interference experienced by the drone. This result includes the\neffect of the drone antenna pattern, the height-dependent shadowing, and\nvarious types of environment. We show that the benefits that a drone obtains\nfrom a better line-of-sight (LoS) at high altitudes is counteracted by a high\nvulnerability to the interference coming from the ground. However, by deriving\nthe link coverage probability and transmission rate we show that a drone base\nstation is still a promising technology if the overall system is properly\ndimensioned according to the given density and transmission power of the\ninterferers. Particularly, our results illustrate how the benefits of such\nnetwork is maximized by defining the optimal drone altitude and signal-to-\ninterference (SIR) requirement. \n\n"}
{"id": "1711.01570", "contents": "Title: Modeling of Persistent Homology Abstract: Topological Data Analysis (TDA) is a novel statistical technique,\nparticularly powerful for the analysis of large and high dimensional data sets.\nMuch of TDA is based on the tool of persistent homology, represented visually\nvia persistence diagrams. In an earlier paper we proposed a parametric\nrepresentation for the probability distributions of persistence diagrams, and\nbased on it provided a method for their replication. Since the typical\nsituation for big data is that only one persistence diagram is available, these\nreplications allow for conventional statistical inference, which, by its very\nnature, requires some form of replication. In the current paper we continue\nthis analysis, and further develop its practical statistical methodology, by\ninvestigating a wider class of examples than treated previously. \n\n"}
{"id": "1711.04057", "contents": "Title: Survival analysis of DNA mutation motifs with penalized proportional\n  hazards Abstract: Antibodies, an essential part of our immune system, develop through an\nintricate process to bind a wide array of pathogens. This process involves\nrandomly mutating DNA sequences encoding these antibodies to find variants with\nimproved binding, though mutations are not distributed uniformly across\nsequence sites. Immunologists observe this nonuniformity to be consistent with\n\"mutation motifs\", which are short DNA subsequences that affect how likely a\ngiven site is to experience a mutation. Quantifying the effect of motifs on\nmutation rates is challenging: a large number of possible motifs makes this\nstatistical problem high dimensional, while the unobserved history of the\nmutation process leads to a nontrivial missing data problem. We introduce an\n$\\ell_1$-penalized proportional hazards model to infer mutation motifs and\ntheir effects. In order to estimate model parameters, our method uses a Monte\nCarlo EM algorithm to marginalize over the unknown ordering of mutations. We\nshow that our method performs better on simulated data compared to current\nmethods and leads to more parsimonious models. The application of proportional\nhazards to mutation processes is, to our knowledge, novel and formalizes the\ncurrent methods in a statistical framework that can be easily extended to\nanalyze the effect of other biological features on mutation rates. \n\n"}
{"id": "1711.04801", "contents": "Title: Quantum information in the Posner model of quantum cognition Abstract: Matthew Fisher recently postulated a mechanism by which quantum phenomena\ncould influence cognition: Phosphorus nuclear spins may resist decoherence for\nlong times, especially when in Posner molecules. The spins would serve as\nbiological qubits. We imagine that Fisher postulates correctly. How adroitly\ncould biological systems process quantum information (QI)? We establish a\nframework for answering. Additionally, we construct applications of biological\nqubits to quantum error correction, quantum communication, and quantum\ncomputation. First, we posit how the QI encoded by the spins transforms as\nPosner molecules form. The transformation points to a natural computational\nbasis for qubits in Posner molecules. From the basis, we construct a quantum\ncode that detects arbitrary single-qubit errors. Each molecule encodes one\nqutrit. Shifting from information storage to computation, we define the model\nof Posner quantum computation. To illustrate the model's quantum-communication\nability, we show how it can teleport information incoherently: A state's\nweights are teleported. Dephasing results from the entangling operation's\nsimulation of a coarse-grained Bell measurement. Whether Posner quantum\ncomputation is universal remains an open question. However, the model's\noperations can efficiently prepare a Posner state usable as a resource in\nuniversal measurement-based quantum computation. The state results from\ndeforming the Affleck-Kennedy-Lieb-Tasaki (AKLT) state and is a projected\nentangled-pair state (PEPS). Finally, we show that entanglement can affect\nmolecular-binding rates, boosting a binding probability from 33.6% to 100% in\nan example. This work opens the door for the QI-theoretic analysis of\nbiological qubits and Posner molecules. \n\n"}
{"id": "1711.06154", "contents": "Title: Reliable Video Streaming over mmWave with Multi Connectivity and Network\n  Coding Abstract: The next generation of multimedia applications will require the\ntelecommunication networks to support a higher bitrate than today, in order to\ndeliver virtual reality and ultra-high quality video content to the users. Most\nof the video content will be accessed from mobile devices, prompting the\nprovision of very high data rates by next generation (5G) cellular networks. A\npossible enabler in this regard is communication at mmWave frequencies, given\nthe vast amount of available spectrum that can be allocated to mobile users;\nhowever, the harsh propagation environment at such high frequencies makes it\nhard to provide a reliable service. This paper presents a reliable video\nstreaming architecture for mmWave networks, based on multi connectivity and\nnetwork coding, and evaluates its performance using a novel combination of the\nns-3 mmWave module, real video traces and the network coding library Kodo. The\nresults show that it is indeed possible to reliably stream video over cellular\nmmWave links, while the combination of multi connectivity and network coding\ncan support high video quality with low latency. \n\n"}
{"id": "1711.07629", "contents": "Title: On statistical approaches to generate Level 3 products from satellite\n  remote sensing retrievals Abstract: Satellite remote sensing of trace gases such as carbon dioxide (CO$_2$) has\nincreased our ability to observe and understand Earth's climate. However, these\nremote sensing data, specifically~Level 2 retrievals, tend to be irregular in\nspace and time, and hence, spatio-temporal prediction is required to infer\nvalues at any location and time point. Such inferences are not only required to\nanswer important questions about our climate, but they are also needed for\nvalidating the satellite instrument, since Level 2 retrievals are generally not\nco-located with ground-based remote sensing instruments. Here, we discuss\nstatistical approaches to construct Level 3 products from Level 2 retrievals,\nplacing particular emphasis on the strengths and potential pitfalls when using\nstatistical prediction in this context. Following this discussion, we use a\nspatio-temporal statistical modelling framework known as fixed rank kriging\n(FRK) to obtain global predictions and prediction standard errors of\ncolumn-averaged carbon dioxide based on Version 7r and Version 8r retrievals\nfrom the Orbiting Carbon Observatory-2 (OCO-2) satellite. The FRK predictions\nallow us to validate statistically the Level 2 retrievals globally even though\nthe data are at locations and at time points that do not coincide with\nvalidation data. Importantly, the validation takes into account the prediction\nuncertainty, which is dependent both on the temporally-varying density of\nobservations around the ground-based measurement sites and on the\nspatio-temporal high-frequency components of the trace gas field that are not\nexplicitly modelled. Here, for validation of remotely-sensed CO$_2$ data, we\nuse observations from the Total Carbon Column Observing Network. We demonstrate\nthat the resulting FRK product based on Version 8r compares better with TCCON\ndata than that based on Version 7r. \n\n"}
{"id": "1711.08208", "contents": "Title: Post-hoc labeling of arbitrary EEG recordings for data-efficient\n  evaluation of neural decoding methods Abstract: Many cognitive, sensory and motor processes have correlates in oscillatory\nneural sources, which are embedded as a subspace into the recorded brain\nsignals. Decoding such processes from noisy\nmagnetoencephalogram/electroencephalogram (M/EEG) signals usually requires the\nuse of data-driven analysis methods. The objective evaluation of such decoding\nalgorithms on experimental raw signals, however, is a challenge: the amount of\navailable M/EEG data typically is limited, labels can be unreliable, and raw\nsignals often are contaminated with artifacts. The latter is specifically\nproblematic, if the artifacts stem from behavioral confounds of the oscillatory\nneural processes of interest.\n  To overcome some of these problems, simulation frameworks have been\nintroduced for benchmarking decoding methods. Generating artificial brain\nsignals, however, most simulation frameworks make strong and partially\nunrealistic assumptions about brain activity, which limits the generalization\nof obtained results to real-world conditions.\n  In the present contribution, we thrive to remove many shortcomings of current\nsimulation frameworks and propose a versatile alternative, that allows for\nobjective evaluation and benchmarking of novel data-driven decoding methods for\nneural signals. Its central idea is to utilize post-hoc labelings of arbitrary\nM/EEG recordings. This strategy makes it paradigm-agnostic and allows to\ngenerate comparatively large datasets with noiseless labels. Source code and\ndata of the novel simulation approach are made available for facilitating its\nadoption. \n\n"}
{"id": "1711.08576", "contents": "Title: Variational Encoding of Complex Dynamics Abstract: Often the analysis of time-dependent chemical and biophysical systems\nproduces high-dimensional time-series data for which it can be difficult to\ninterpret which individual features are most salient. While recent work from\nour group and others has demonstrated the utility of time-lagged co-variate\nmodels to study such systems, linearity assumptions can limit the compression\nof inherently nonlinear dynamics into just a few characteristic components.\nRecent work in the field of deep learning has led to the development of\nvariational autoencoders (VAE), which are able to compress complex datasets\ninto simpler manifolds. We present the use of a time-lagged VAE, or variational\ndynamics encoder (VDE), to reduce complex, nonlinear processes to a single\nembedding with high fidelity to the underlying dynamics. We demonstrate how the\nVDE is able to capture nontrivial dynamics in a variety of examples, including\nBrownian dynamics and atomistic protein folding. Additionally, we demonstrate a\nmethod for analyzing the VDE model, inspired by saliency mapping, to determine\nwhat features are selected by the VDE model to describe dynamics. The VDE\npresents an important step in applying techniques from deep learning to more\naccurately model and interpret complex biophysics. \n\n"}
{"id": "1711.08970", "contents": "Title: Sparse and Low-Rank Matrix Decomposition for Automatic Target Detection\n  in Hyperspectral Imagery Abstract: Given a target prior information, our goal is to propose a method for\nautomatically separating targets of interests from the background in\nhyperspectral imagery. More precisely, we regard the given hyperspectral image\n(HSI) as being made up of the sum of low-rank background HSI and a sparse\ntarget HSI that contains the targets based on a pre-learned target dictionary\nconstructed from some online spectral libraries. Based on the proposed method,\ntwo strategies are briefly outlined and evaluated to realize the target\ndetection on both synthetic and real experiments. \n\n"}
{"id": "1711.09365", "contents": "Title: Ensemble-marginalized Kalman filter for linear time-dependent PDEs with\n  noisy boundary conditions: Application to heat transfer in building walls Abstract: In this work, we present the ensemble-marginalized Kalman filter (EnMKF), a\nsequential algorithm analogous to our previously proposed approach [1,2], for\nestimating the state and parameters of linear parabolic partial differential\nequations in initial-boundary value problems when the boundary data are noisy.\nWe apply EnMKF to infer the thermal properties of building walls and to\nestimate the corresponding heat flux from real and synthetic data. Compared\nwith a modified Ensemble Kalman Filter (EnKF) that is not marginalized, EnMKF\nreduces the bias error, avoids the collapse of the ensemble without needing to\nadd inflation, and converges to the mean field posterior using $50\\%$ or less\nof the ensemble size required by EnKF. According to our results, the\nmarginalization technique in EnMKF is key to performance improvement with\nsmaller ensembles at any fixed time. \n\n"}
{"id": "1711.10937", "contents": "Title: Forest-based methods and ensemble model output statistics for rainfall\n  ensemble forecasting Abstract: Rainfall ensemble forecasts have to be skillful for both low precipitation\nand extreme events. We present statistical post-processing methods based on\nQuantile Regression Forests (QRF) and Gradient Forests (GF) with a parametric\nextension for heavy-tailed distributions. Our goal is to improve ensemble\nquality for all types of precipitation events, heavy-tailed included, subject\nto a good overall performance. Our hybrid proposed methods are applied to daily\n51-h forecasts of 6-h accumulated precipitation from 2012 to 2015 over France\nusing the M{\\'e}t{\\'e}o-France ensemble prediction system called PEARP. They\nprovide calibrated pre-dictive distributions and compete favourably with\nstate-of-the-art methods like Analogs method or Ensemble Model Output\nStatistics. In particular, hybrid forest-based procedures appear to bring an\nadded value to the forecast of heavy rainfall. \n\n"}
{"id": "1711.11057", "contents": "Title: On the use of bootstrap with variational inference: Theory,\n  interpretation, and a two-sample test example Abstract: Variational inference is a general approach for approximating complex density\nfunctions, such as those arising in latent variable models, popular in machine\nlearning. It has been applied to approximate the maximum likelihood estimator\nand to carry out Bayesian inference, however, quantification of uncertainty\nwith variational inference remains challenging from both theoretical and\npractical perspectives. This paper is concerned with developing uncertainty\nmeasures for variational inference by using bootstrap procedures. We first\ndevelop two general bootstrap approaches for assessing the uncertainty of a\nvariational estimate and the study the underlying bootstrap theory in both\nfixed- and increasing-dimension settings. We then use the bootstrap approach\nand our theoretical results in the context of mixed membership modeling with\nmultivariate binary data on functional disability from the National Long Term\nCare Survey. We carry out a two-sample approach to test for changes in the\nrepeated measures of functional disability for the subset of individuals\npresent in 1989 and 1994 waves. \n\n"}
{"id": "1712.00203", "contents": "Title: Closed-loop field development with multipoint geostatistics and\n  statistical performance assessment Abstract: Closed-loop field development (CLFD) optimization is a comprehensive\nframework for optimal development of subsurface resources. CLFD involves three\nmajor steps: 1) optimization of full development plan based on current set of\nmodels, 2) drilling new wells and collecting new spatial and temporal\n(production) data, 3) model calibration based on all data. This process is\nrepeated until the optimal number of wells is drilled. This work introduces an\nefficient CLFD implementation for complex systems described by multipoint\ngeostatistics (MPS). Model calibration is accomplished in two steps:\nconditioning to spatial data by a geostatistical simulation method, and\nconditioning to production data by optimization-based PCA. A statistical\nprocedure is presented to assess the performance of CLFD. Methodology is\napplied to an oil reservoir example for 25 different true-model cases.\nApplication of a single-step of CLFD, improved the true NPV in 64%--80% of\ncases. The full CLFD procedure (with three steps) improved the true NPV in 96%\nof cases, with an average improvement of 37%. \n\n"}
{"id": "1712.01795", "contents": "Title: Optimizing aerodynamic lenses for single-particle imaging Abstract: A numerical simulation infrastructure capable of calculating the flow of gas\nand the trajectories of particles through an aerodynamic lens injector is\npresented. The simulations increase the fundamental understanding and predict\noptimized injection geometries and parameters. Our simulation results were\ncompared to previous reports and also validated against experimental data for\n500 nm polystyrene spheres from an aerosol-beam- characterization setup. The\nsimulations yielded a detailed understanding of the radial phase-space\ndistribution and highlighted weaknesses of current aerosol injectors for\nsingle-particle diffractive imaging. With the aid of these simulations we\ndeveloped new experimental implementations to overcome current limitations. \n\n"}
{"id": "1712.02700", "contents": "Title: milliProxy: a TCP Proxy Architecture for 5G mmWave Cellular Systems Abstract: TCP is the most widely used transport protocol in the internet. However, it\noffers suboptimal performance when operating over high bandwidth mmWave links.\nThe main issues introduced by communications at such high frequencies are (i)\nthe sensitivity to blockage and (ii) the high bandwidth fluctuations due to\nLine of Sight (LOS) to Non Line of Sight (NLOS) transitions and vice versa. In\nparticular, TCP has an abstract view of the end-to-end connection, which does\nnot properly capture the dynamics of the wireless mmWave link. The consequence\nis a suboptimal utilization of the available resources. In this paper we\npropose a TCP proxy architecture that improves the performance of TCP flows\nwithout any modification at the remote sender side. The proxy is installed in\nthe Radio Access Network, and exploits information available at the gNB in\norder to maximize throughput and minimize latency. \n\n"}
{"id": "1712.03834", "contents": "Title: Crime prediction through urban metrics and statistical learning Abstract: Understanding the causes of crime is a longstanding issue in researcher's\nagenda. While it is a hard task to extract causality from data, several linear\nmodels have been proposed to predict crime through the existing correlations\nbetween crime and urban metrics. However, because of non-Gaussian distributions\nand multicollinearity in urban indicators, it is common to find controversial\nconclusions about the influence of some urban indicators on crime. Machine\nlearning ensemble-based algorithms can handle well such problems. Here, we use\na random forest regressor to predict crime and quantify the influence of urban\nindicators on homicides. Our approach can have up to 97% of accuracy on crime\nprediction, and the importance of urban indicators is ranked and clustered in\ngroups of equal influence, which are robust under slightly changes in the data\nsample analyzed. Our results determine the rank of importance of urban\nindicators to predict crime, unveiling that unemployment and illiteracy are the\nmost important variables for describing homicides in Brazilian cities. We\nfurther believe that our approach helps in producing more robust conclusions\nregarding the effects of urban indicators on crime, having potential\napplications for guiding public policies for crime control. \n\n"}
{"id": "1712.04723", "contents": "Title: Bayesian graphical compositional regression for microbiome data Abstract: An important task in microbiome studies is to test the existence of and give\ncharacterization to differences in the microbiome composition across groups of\nsamples. Important challenges of this problem include the large within-group\nheterogeneities among samples and the existence of potential confounding\nvariables that, when ignored, increase the chance of false discoveries and\nreduce the power for identifying true differences. We propose a probabilistic\nframework to overcome these issues by combining three ideas: (i) a phylogenetic\ntree-based decomposition of the cross-group comparison problem into a series of\nlocal tests, (ii) a graphical model that links the local tests to allow\ninformation sharing across taxa, and (iii) a Bayesian testing strategy that\nincorporates covariates and integrates out the within-group variation, avoiding\npotentially unstable point estimates. We derive an efficient inference\nalgorithm based on numerical integration and junction-tree message passing,\nconduct extensive simulation studies to investigate the performance of our\napproach, and compare it to state-of-the-art methods in a number of\nrepresentative settings. We then apply our method to the American Gut data to\nanalyze the association of dietary habits and human's gut microbiome\ncomposition in the presence of covariates, and illustrate the importance of\nincorporating covariates in microbiome cross-group comparison. \n\n"}
{"id": "1712.05879", "contents": "Title: Hierarchical Bayesian Bradley-Terry for Applications in Major League\n  Baseball Abstract: A common problem faced in statistical inference is drawing conclusions from\npaired comparisons, in which two objects compete and one is declared the\nvictor. A probabilistic approach to such a problem is the Bradley-Terry model,\nfirst studied by Zermelo in 1929 and rediscovered by Bradley and Terry in 1952.\nOne obvious area of application for such a model is sporting events, and in\nparticular Major League Baseball. With this in mind, we describe a hierarchical\nBayesian version of Bradley-Terry suitable for use in ranking and prediction\nproblems, and compare results from these application domains to standard\nmaximum likelihood approaches. Our Bayesian methods outperform the MLE-based\nanalogues, while being simple to construct, implement, and interpret. \n\n"}
{"id": "1712.08238", "contents": "Title: Interventions over Predictions: Reframing the Ethical Debate for\n  Actuarial Risk Assessment Abstract: Actuarial risk assessments might be unduly perceived as a neutral way to\ncounteract implicit bias and increase the fairness of decisions made at almost\nevery juncture of the criminal justice system, from pretrial release to\nsentencing, parole and probation. In recent times these assessments have come\nunder increased scrutiny, as critics claim that the statistical techniques\nunderlying them might reproduce existing patterns of discrimination and\nhistorical biases that are reflected in the data. Much of this debate is\ncentered around competing notions of fairness and predictive accuracy, resting\non the contested use of variables that act as \"proxies\" for characteristics\nlegally protected against discrimination, such as race and gender. We argue\nthat a core ethical debate surrounding the use of regression in risk\nassessments is not simply one of bias or accuracy. Rather, it's one of purpose.\nIf machine learning is operationalized merely in the service of predicting\nindividual future crime, then it becomes difficult to break cycles of\ncriminalization that are driven by the iatrogenic effects of the criminal\njustice system itself. We posit that machine learning should not be used for\nprediction, but rather to surface covariates that are fed into a causal model\nfor understanding the social, structural and psychological drivers of crime. We\npropose an alternative application of machine learning and causal inference\naway from predicting risk scores to risk mitigation. \n\n"}
{"id": "1712.08894", "contents": "Title: EXONEST: The Bayesian Exoplanetary Explorer Abstract: The fields of astronomy and astrophysics are currently engaged in an\nunprecedented era of discovery as recent missions have revealed thousands of\nexoplanets orbiting other stars. While the Kepler Space Telescope mission has\nenabled most of these exoplanets to be detected by identifying transiting\nevents, exoplanets often exhibit additional photometric effects that can be\nused to improve the characterization of exoplanets. The EXONEST Exoplanetary\nExplorer is a Bayesian exoplanet inference engine based on nested sampling and\noriginally designed to analyze archived Kepler Space Telescope and CoRoT\n(Convection Rotation et Transits plan\\'etaires) exoplanet mission data. We\ndiscuss the EXONEST software package and describe how it accommodates\nplug-and-play models of exoplanet-associated photometric effects for the\npurpose of exoplanet detection, characterization and scientific hypothesis\ntesting. The current suite of models allows for both circular and eccentric\norbits in conjunction with photometric effects, such as the primary transit and\nsecondary eclipse, reflected light, thermal emissions, ellipsoidal variations,\nDoppler beaming and superrotation. We discuss our new efforts to expand the\ncapabilities of the software to include more subtle photometric effects\ninvolving reflected and refracted light. We discuss the EXONEST inference\nengine design and introduce our plans to port the current MATLAB-based EXONEST\nsoftware package over to the next generation Exoplanetary Explorer, which will\nbe a Python-based open source project with the capability to employ third-party\nplug-and-play models of exoplanet-related photometric effects. \n\n"}
{"id": "1712.09149", "contents": "Title: Reduced Bias for respondent driven sampling: accounting for non-uniform\n  edge sampling probabilities in people who inject drugs in Mauritius Abstract: People who inject drugs are an important population to study in order to\nreduce transmission of blood-borne illnesses including HIV and Hepatitis. In\nthis paper we estimate the HIV and Hepatitis C prevalence among people who\ninject drugs, as well as the proportion of people who inject drugs who are\nfemale in Mauritius. Respondent driven sampling (RDS), a widely adopted\nlink-tracing sampling design used to collect samples from hard-to-reach human\npopulations, was used to collect this sample. The random walk approximation\nunderlying many common RDS estimators assumes that each social relation (edge)\nin the underlying social network has an equal probability of being traced in\nthe collection of the sample. This assumption does not hold in practice. We\nshow that certain RDS estimators are sensitive to the violation of this\nassumption. In order to address this limitation in current methodology, and the\nimpact it may have on prevalence estimates, we present a new method for\nimproving RDS prevalence estimators using estimated edge inclusion\nprobabilities, and apply this to data from Mauritius. \n\n"}
{"id": "1712.09816", "contents": "Title: Extremal Behavior of Aggregated Data with an Application to Downscaling Abstract: The distribution of spatially aggregated data from a stochastic process $X$\nmay exhibit a different tail behavior than its marginal distributions. For a\nlarge class of aggregating functionals $\\ell$ we introduce the $\\ell$-extremal\ncoefficient that quantifies this difference as a function of the extremal\nspatial dependence in $X$. We also obtain the joint extremal dependence for\nmultiple aggregation functionals applied to the same process. Explicit formulas\nfor the $\\ell$-extremal coefficients and multivariate dependence structures are\nderived in important special cases. The results provide a theoretical link\nbetween the extremal distribution of the aggregated data and the corresponding\nunderlying process, which we exploit to develop a method for statistical\ndownscaling. We apply our framework to downscale daily temperature maxima in\nthe south of France from a gridded data set and use our model to generate high\nresolution maps of the warmest day during the 2003 heatwave. \n\n"}
{"id": "1801.00594", "contents": "Title: Dynamic Channel Bonding in Spatially Distributed High-Density WLANs Abstract: In this paper, we discuss the effects on throughput and fairness of dynamic\nchannel bonding (DCB) in spatially distributed high-density wireless local area\nnetworks (WLANs). First, we present an analytical framework based on\ncontinuous-time Markov networks (CTMNs) for depicting the behavior of different\nDCB policies in spatially distributed scenarios, where nodes are not required\nto be within the carrier sense range of each other. Then, we assess the\nperformance of DCB in high-density IEEE 802.11ac/ax WLANs by means of\nsimulations. We show that there may be critical interrelations among nodes in\nthe spatial domain - even if they are located outside the carrier sense range\nof each other - in a chain reaction manner. Results also reveal that, while\nalways selecting the widest available channel normally maximizes the individual\nlong-term throughput, it often generates unfair situations where other WLANs\nstarve. Moreover, we show that there are scenarios where DCB with stochastic\nchannel width selection improves the latter approach both in terms of\nindividual throughput and fairness. It follows that there is not a unique\noptimal DCB policy for every case. Instead, smarter bandwidth adaptation is\nrequired in the challenging scenarios of next-generation WLANs. \n\n"}
{"id": "1801.01538", "contents": "Title: Understanding Hormonal Crosstalk in Arabidopsis Root Development via\n  Emulation and History Matching Abstract: A major challenge in plant developmental biology is to understand how plant\ngrowth is coordinated by interacting hormones and genes. To meet this\nchallenge, it is important to not only use experimental data, but also\nformulate a mathematical model. For the mathematical model to best describe the\ntrue biological system, it is necessary to understand the parameter space of\nthe model, along with the links between the model, the parameter space and\nexperimental observations. We develop sequential history matching methodology,\nusing Bayesian emulation, to gain substantial insight into biological model\nparameter spaces. This is achieved by finding sets of acceptable parameters in\naccordance with successive sets of physical observations. These methods are\nthen applied to a complex hormonal crosstalk model for Arabidopsis root growth.\nIn this application, we demonstrate how an initial set of 22 observed trends\nreduce the volume of the set of acceptable inputs to a proportion of 6.1 x\n10^(-7) of the original space. Additional sets of biologically relevant\nexperimental data, each of size 5, reduce the size of this space by a further\nthree and two orders of magnitude respectively. Hence, we provide insight into\nthe constraints placed upon the model structure by, and the biological\nconsequences of, measuring subsets of observations. \n\n"}
{"id": "1801.02078", "contents": "Title: Spatial Factor Models for High-Dimensional and Large Spatial Data: An\n  Application in Forest Variable Mapping Abstract: Gathering information about forest variables is an expensive and arduous\nactivity. As such, directly collecting the data required to produce\nhigh-resolution maps over large spatial domains is infeasible. Next generation\ncollection initiatives of remotely sensed Light Detection and Ranging (LiDAR)\ndata are specifically aimed at producing complete-coverage maps over large\nspatial domains. Given that LiDAR data and forest characteristics are often\nstrongly correlated, it is possible to make use of the former to model,\npredict, and map forest variables over regions of interest. This entails\ndealing with the high-dimensional ($\\sim$$10^2$) spatially dependent LiDAR\noutcomes over a large number of locations (~10^5-10^6). With this in mind, we\ndevelop the Spatial Factor Nearest Neighbor Gaussian Process (SF-NNGP) model,\nand embed it in a two-stage approach that connects the spatial structure found\nin LiDAR signals with forest variables. We provide a simulation experiment that\ndemonstrates inferential and predictive performance of the SF-NNGP, and use the\ntwo-stage modeling strategy to generate complete-coverage maps of forest\nvariables with associated uncertainty over a large region of boreal forests in\ninterior Alaska. \n\n"}
{"id": "1801.02344", "contents": "Title: Optimal Time Scheduling for Wireless-Powered Backscatter Communication\n  Networks Abstract: This letter introduces a novel wireless-powered backscatter communication\nsystem which allows sensors to utilize RF signals transmitted from a dedicated\nRF energy source to transmit data. In the proposed system, when the RF energy\nsource transmits RF signals, the sensors are able to backscatter the RF signals\nto transmit date to the gateway and/or harvest energy from the RF signals for\ntheir operations. By integrating backscattering and energy harvesting\ntechniques, we can optimize the network throughput of the system. In\nparticular, we first formulate the time scheduling problem for the system, and\nthen propose an optimal solution using convex optimization to maximize the\noverall network throughput. Numerical results show a significant throughput\ngain achieved by our proposed design over two other baseline schemes. \n\n"}
{"id": "1801.02609", "contents": "Title: Secure Beamforming in Full-Duplex SWIPT Systems With Loopback\n  Self-Interference Cancellation Abstract: Security is a critical issue in full duplex (FD) communication systems due to\nthe broadcast nature of wireless channels. In this paper, joint design of\ninformation and artificial noise beamforming vectors is proposed for the FD\nsimultaneous wireless information and power transferring (FD-SWIPT) systems\nwith loopback self-interference cancellation. To guarantee high security and\nenergy harvesting performance of the FD-SWIPT system, the proposed design is\nformulated as a secrecy rate maximization problem under energy transfer rate\nconstraints. Although the secrecy rate maximization problem is non-convex, we\nsolve it via semidefinite relaxation and a two-dimensional search. We prove the\noptimality of our proposed algorithm and demonstrate its performance via\nsimulations. \n\n"}
{"id": "1801.03783", "contents": "Title: Quantifying Gerrymandering in North Carolina Abstract: Using an ensemble of redistricting plans, we evaluate whether a given\npolitical districting faithfully represents the geo-political landscape.\nRedistricting plans are sampled by a Monte Carlo algorithm from a probability\ndistribution that adheres to realistic and non-partisan criteria. Using the\nsampled redistricting plans and historical voting data, we produce an ensemble\nof elections that reveal geo-political structure within the state. We showcase\nour methods on the two most recent districtings of NC for the U.S. House of\nRepresentatives, as well as a plan drawn by a bipartisan redistricting panel.\nWe find the two state enacted plans are highly atypical outliers whereas the\nbipartisan plan accurately represents the ensemble both in partisan outcome and\nin the fine scale structure of district-level results. \n\n"}
{"id": "1801.04212", "contents": "Title: Multinomial logistic model for coinfection diagnosis between arbovirus\n  and malaria in Kedougou Abstract: In tropical regions, populations continue to suffer morbidity and mortality\nfrom malaria and arboviral diseases. In Kedougou (Senegal), these illnesses are\nall endemic due to the climate and its geographical position. The\nco-circulation of malaria parasites and arboviruses can explain the observation\nof coinfected cases. Indeed there is strong resemblance in symptoms between\nthese diseases making problematic targeted medical care of coinfected cases.\nThis is due to the fact that the origin of illness is not obviously known. Some\ncases could be immunized against one or the other of the pathogens, immunity\ntypically acquired with factors like age and exposure as usual for endemic\narea. Then, coinfection needs to be better diagnosed. Using data collected from\npatients in Kedougou region, from 2009 to 2013, we adjusted a multinomial\nlogistic model and selected relevant variables in explaining coinfection\nstatus. We observed specific sets of variables explaining each of the diseases\nexclusively and the coinfection. We tested the independence between arboviral\nand malaria infections and derived coinfection probabilities from the model\nfitting. In case of a coinfection probability greater than a threshold value to\nbe calibrated on the data, duration of illness above 3 days and age above 10\nyears-old are mostly indicative of arboviral disease while body temperature\nhigher than 40{\\textdegree}C and presence of nausea or vomiting symptoms during\nthe rainy season are mostly indicative of malaria disease. \n\n"}
{"id": "1801.05247", "contents": "Title: Using the Maximum Entropy Principle to Combine Simulations and Solution\n  Experiments Abstract: Molecular dynamics (MD) simulations allow investigating the structural\ndynamics of biomolecular systems with unrivaled time and space resolution.\nHowever, in order to compensate for the inaccuracies of the utilized empirical\nforce fields, it is becoming common to integrate MD simulations with\nexperimental data obtained from ensemble measurements. We here review the\napproaches that can be used to combine MD and experiment under the guidance of\nthe maximum entropy principle. We mostly focus on methods based on Lagrangian\nmultipliers, either implemented as reweighting of existing simulations or\nthrough an on-the-fly optimization. We discuss how errors in the experimental\ndata can be modeled and accounted for. Finally, we use simple model systems to\nillustrate the typical difficulties arising when applying these methods. \n\n"}
{"id": "1801.05465", "contents": "Title: On a bimodal Birnbaum-Saunders distribution with applications to\n  lifetime data Abstract: The Birnbaum-Saunders distribution is a flexible and useful model which has\nbeen used in several fields. In this paper, a new bimodal version of this\ndistribution based on the alpha-skew-normal distribution is established. We\ndiscuss some of its mathematical and inferential properties. We consider\nlikelihood-based methods to estimate the model parameters. We carry out a Monte\nCarlo simulation study to evaluate the performance of the maximum likelihood\nestimators. For illustrative purposes, three real data sets are analyzed. The\nresults indicated that the proposed model outperformed some existing models in\nthe literature, in special, a recent bimodal extension of the Birnbaum-Saunders\ndistribution. \n\n"}
{"id": "1801.05725", "contents": "Title: Bayesian Estimation of Gaussian Graphical Models with Predictive\n  Covariance Selection Abstract: Gaussian graphical models are used for determining conditional relationships\nbetween variables. This is accomplished by identifying off-diagonal elements in\nthe inverse-covariance matrix that are non-zero. When the ratio of variables\n(p) to observations (n) approaches one, the maximum likelihood estimator of the\ncovariance matrix becomes unstable and requires shrinkage estimation. Whereas\nseveral classical (frequentist) methods have been introduced to address this\nissue, fully Bayesian methods remain relatively uncommon in practice and\nmethodological literatures. Here we introduce a Bayesian method for estimating\nsparse matrices, in which conditional relationships are determined with\nprojection predictive selection. With this method, that uses Kullback-Leibler\ndivergence and cross-validation for neighborhood selection, we reconstruct the\ninverse-covariance matrix in both low and high-dimensional settings. Through\nsimulation and applied examples, we characterized performance compared to\nseveral Bayesian methods and the graphical lasso, in addition to TIGER that\nsimilarly estimates the inverse-covariance matrix with regression. Our results\ndemonstrate that projection predictive selection not only has superior\nperformance compared to selecting the most probable model and Bayesian model\naveraging, particularly for high-dimensional data, but also compared to the the\nBayesian and classical glasso methods. Further, we show that estimating the\ninverse-covariance matrix with multiple regression is often more accurate, with\nrespect to various loss functions, and efficient than direct estimation. In\nlow-dimensional settings, we demonstrate that projection predictive selection\nalso provides competitive performance. We have implemented the projection\npredictive method for covariance selection in the R package GGMprojpred \n\n"}
{"id": "1801.06449", "contents": "Title: User Preference Learning Based Edge Caching for Fog Radio Access Network Abstract: In this paper, the edge caching problem in fog radio access network (F-RAN)\nis investigated. By maximizing the overall cache hit rate, the edge caching\noptimization problem is formulated to find the optimal policy. Content\npopularity in terms of time and space is considered from the perspective of\nregional users. We propose an online content popularity prediction algorithm by\nleveraging the content features and user preferences, and an offline user\npreference learning algorithm by using the {online gradient descent} (OGD)\nmethod and the {follow the (proximally) regularized leader} (FTRL-Proximal)\nmethod. Our proposed edge caching policy not only can promptly predict the\nfuture content popularity in an online fashion with low complexity, {but also}\ncan track the content popularity with spatial and temporal popularity dynamic\nin time without delay. Furthermore, we design two learning based edge caching\narchitectures. Moreover, we theoretically derive the upper bound of the\npopularity prediction error, the lower bound of the cache hit rate, and the\nregret bound of the overall cache hit rate of our proposed edge caching policy.\nSimulation results show that the overall cache hit rate of our proposed policy\nis superior to those of the traditional policies and asymptotically approaches\nthe optimal performance. \n\n"}
{"id": "1801.06623", "contents": "Title: Promises and Caveats of Uplink IoT Ultra-Dense Networks Abstract: In this paper, by means of simulations, we evaluate the uplink (UL)\nperformance of an Internet of Things (IoT) capable ultra-dense network (UDN) in\nterms of the coverage probability and the density of reliably working user\nequipments (UEs). From our study, we show the benefits and challenges that UL\nIoT UDNs will bring about in the future. In more detail, for a low-reliability\ncriterion, such as achieving a UL signal-to-interference-plus-noise ratio\n(SINR) above 0 dB, the density of reliably working UEs grows quickly with the\nnetwork densification, showing the potential of UL IoT UDNs. In contrast, for a\nhigh-reliability criterion, such as achieving a UL SINR above 10 dB, the\ndensity of reliably working UEs remains to be low in UDNs due to excessive\ninter-cell interference, which should be considered when operating UL IoT UDNs.\nMoreover, considering the existence of a non-zero antenna height difference\nbetween base stations (BSs) and UEs, the density of reliably working UEs could\neven decrease as we deploy more BSs. This calls for the usage of sophisticated\ninterference management schemes and/or beam steering/shaping technologies in UL\nIoT UDNs. \n\n"}
{"id": "1801.06936", "contents": "Title: Evolution of Regional Innovation with Spatial Knowledge Spillovers:\n  Convergence or Divergence? Abstract: This paper extends endogenous economic growth models to incorporate knowledge\nexternality. We explores whether spatial knowledge spillovers among regions\nexist, whether spatial knowledge spillovers promote regional innovative\nactivities, and whether external knowledge spillovers affect the evolution of\nregional innovations in the long run. We empirically verify the theoretical\nresults through applying spatial statistics and econometric model in the\nanalysis of panel data of 31 regions in China. An accurate estimate of the\nrange of knowledge spillovers is achieved and the convergence of regional\nknowledge growth rate is found, with clear evidences that developing regions\nbenefit more from external knowledge spillovers than developed regions. \n\n"}
{"id": "1801.07351", "contents": "Title: Tracking network dynamics: a survey of distances and similarity metrics Abstract: From longitudinal biomedical studies to social networks, graphs have emerged\nas a powerful framework for describing evolving interactions between agents in\ncomplex systems. In such studies, after pre-processing, the data can be\nrepresented by a set of graphs, each representing a system's state at different\npoints in time. The analysis of the system's dynamics depends on the selection\nof the appropriate analytical tools. After characterizing similarities between\nstates, a critical step lies in the choice of a distance between graphs capable\nof reflecting such similarities. While the literature offers a number of\ndistances that one could a priori choose from, their properties have been\nlittle investigated and no guidelines regarding the choice of such a distance\nhave yet been provided. In particular, most graph distances consider that the\nnodes are exchangeable and do not take into account node identities. Accounting\nfor the alignment of the graphs enables us to enhance these distances'\nsensitivity to perturbations in the network and detect important changes in\ngraph dynamics. Thus the selection of an adequate metric is a decisive --yet\ndelicate--practical matter.\n  In the spirit of Goldenberg, Zheng and Fienberg's seminal 2009 review, the\npurpose of this article is to provide an overview of commonly-used graph\ndistances and an explicit characterization of the structural changes that they\nare best able to capture. We use as a guiding thread to our discussion the\napplication of these distances to the analysis of both a longitudinal\nmicrobiome dataset and a brain fMRI study. We show examples of using\npermutation tests to detect the effect of covariates on the graphs'\nvariability. Synthetic examples provide intuition as to the qualities and\ndrawbacks of the different distances. Above all, we provide some guidance for\nchoosing one distance over another in certain types of applications. \n\n"}
{"id": "1801.07367", "contents": "Title: Mean-Field Game Theoretic Edge Caching in Ultra-Dense Networks Abstract: This paper investigates a cellular edge caching problem under a very large\nnumber of small base stations (SBSs) and users. In this ultra-dense edge\ncaching network (UDCN), conventional caching algorithms are inapplicable as\ntheir complexity increases with the number of small base stations (SBSs).\nFurthermore, the performance of UDCN is highly sensitive to the dynamics of\nuser demand and inter-SBS interference. To overcome such difficulties, we\npropose a distributed caching algorithm under a stochastic geometric network\nmodel, as well as a spatio-temporal user demand model that characterizes the\ncontent popularity dynamics. By exploiting mean-field game (MFG) theory, the\ncomplexity of the proposed UDCN caching algorithm becomes independent of the\nnumber of SBSs. Numerical evaluations validate that the proposed caching\nalgorithm reduces not only the long run average cost of the network but also\nthe redundant cached data respectively by 24% and 42%, compared to a baseline\ncaching algorithm. The simulation results also show that the proposed caching\nalgorithm is robust to imperfect popularity information, while ensuring a low\ncomputational complexity. \n\n"}
{"id": "1801.07826", "contents": "Title: Estimating Heterogeneous Consumer Preferences for Restaurants and Travel\n  Time Using Mobile Location Data Abstract: This paper analyzes consumer choices over lunchtime restaurants using data\nfrom a sample of several thousand anonymous mobile phone users in the San\nFrancisco Bay Area. The data is used to identify users' approximate typical\nmorning location, as well as their choices of lunchtime restaurants. We build a\nmodel where restaurants have latent characteristics (whose distribution may\ndepend on restaurant observables, such as star ratings, food category, and\nprice range), each user has preferences for these latent characteristics, and\nthese preferences are heterogeneous across users. Similarly, each item has\nlatent characteristics that describe users' willingness to travel to the\nrestaurant, and each user has individual-specific preferences for those latent\ncharacteristics. Thus, both users' willingness to travel and their base utility\nfor each restaurant vary across user-restaurant pairs. We use a Bayesian\napproach to estimation. To make the estimation computationally feasible, we\nrely on variational inference to approximate the posterior distribution, as\nwell as stochastic gradient descent as a computational approach. Our model\nperforms better than more standard competing models such as multinomial logit\nand nested logit models, in part due to the personalization of the estimates.\nWe analyze how consumers re-allocate their demand after a restaurant closes to\nnearby restaurants versus more distant restaurants with similar\ncharacteristics, and we compare our predictions to actual outcomes. Finally, we\nshow how the model can be used to analyze counterfactual questions such as what\ntype of restaurant would attract the most consumers in a given location. \n\n"}
{"id": "1801.08620", "contents": "Title: Queue-Aware Joint Dynamic Interference Coordination and Heterogeneous\n  QoS Provisioning in OFDMA Networks Abstract: We propose algorithms for cloud radio access networks that not only provide\nheterogeneous quality of-service (QoS) for rate- and, importantly,\ndelay-sensitive applications, but also jointly optimize the frequency reuse\npattern. Importantly, unlike related works, we account for random arrivals,\nthrough queue awareness and, unlike majority of works focusing on a single\nframe only, we consider QoS measures averaged over multiple frames involving a\nset of closed loop controls. We model this problem as multi-cell optimization\nto maximize a sum utility subject to the QoS constraints, expressed as minimum\nmean-rate or maximum mean-delay. Since we consider dynamic interference\ncoordination jointly with dynamic user association, the problem is not convex,\neven after integer relaxation. We translate the problem into an optimization of\nframe rates, amenable to a decomposition into intertwined primal and dual\nproblems. The solution to this optimization problem provides joint decisions on\nscheduling, dynamic interference coordination, and, importantly, unlike most\nworks in this area, on dynamic user association. Additionally, we propose a\nnovel method to manage infeasible loads. Extensive simulations confirm that the\ndesign responds to instantaneous loads, heterogeneous user and AP locations,\nchannel conditions, and QoS constraints while, if required, keeping outage low\nwhen dealing with infeasible loads. Comparisons to the baseline proportional\nfair scheme illustrate the gains achieved. \n\n"}
{"id": "1801.09475", "contents": "Title: Quantum simulation of photosynthetic energy transfer Abstract: Near-unity energy transfer efficiency has been widely observed in natural\nphotosynthetic complexes. This phenomenon has attracted broad interest from\ndifferent fields, such as physics, biology, chemistry and material science, as\nit may offer valuable insights into efficient solar-energy harvesting.\nRecently, quantum coherent effects have been discovered in photosynthetic light\nharvesting, and their potential role on energy transfer has seen heated debate.\nHere, we perform an experimental quantum simulation of photosynthetic energy\ntransfer using nuclear magnetic resonance (NMR). We show that an N- chromophore\nphotosynthetic complex, with arbitrary structure and bath spectral density, can\nbe effectively simulated by a system with log2 N qubits. The computational cost\nof simulating such a system with a theoretical tool, like the hierarchical\nequation of motion, which is exponential in N, can be potentially reduced to\nrequiring a just polynomial number of qubits N using NMR quantum simulation.\nThe benefits of performing such quantum simulation in NMR are even greater when\nthe spectral density is complex, as in natural photosynthetic complexes. These\nfindings may shed light on quantum coherence in energy transfer and help to\nprovide design principles for efficient artificial light harvesting. \n\n"}
{"id": "1801.10516", "contents": "Title: Are `Water Smart Landscapes' Contagious? An epidemic approach on\n  networks to study peer effects Abstract: We test the existence of a neighborhood based peer effect around\nparticipation in an incentive based conservation program called `Water Smart\nLandscapes' (WSL) in the city of Las Vegas, Nevada. We use 15 years of\ngeo-coded daily records of WSL program applications and approvals compiled by\nthe Southern Nevada Water Authority and Clark County Tax Assessors rolls for\nhome characteristics. We use this data to test whether a spatially mediated\npeer effect can be observed in WSL participation likelihood at the household\nlevel. We show that epidemic spreading models provide more flexibility in\nmodeling assumptions, and also provide one mechanism for addressing problems\nassociated with correlated unobservables than hazards models which can also be\napplied to address the same questions. We build networks of neighborhood based\npeers for 16 randomly selected neighborhoods in Las Vegas and test for the\nexistence of a peer based influence on WSL participation by using a\nSusceptible-Exposed-Infected-Recovered epidemic spreading model (SEIR), in\nwhich a home can become infected via autoinfection or through contagion from\nits infected neighbors. We show that this type of epidemic model can be\ndirectly recast to an additive-multiplicative hazard model, but not to purely\nmultiplicative one. Using both inference and prediction approaches we find\nevidence of peer effects in several Las Vegas neighborhoods. \n\n"}
{"id": "1802.00032", "contents": "Title: Coupling geometry on binary bipartite networks: hypotheses testing on\n  pattern geometry and nestedness Abstract: Upon a matrix representation of a binary bipartite network, via the\npermutation invariance, a coupling geometry is computed to approximate the\nminimum energy macrostate of a network's system. Such a macrostate is supposed\nto constitute the intrinsic structures of the system, so that the coupling\ngeometry should be taken as information contents, or even the nonparametric\nminimum sufficient statistics of the network data. Then pertinent null and\nalternative hypotheses, such as nestedness, are to be formulated according to\nthe macrostate. That is, any efficient testing statistic needs to be a function\nof this coupling geometry. These conceptual architectures and mechanisms are by\nand large still missing in community ecology literature, and rendered\nmisconceptions prevalent in this research area. Here the algorithmically\ncomputed coupling geometry is shown consisting of deterministic multiscale\nblock patterns, which are framed by two marginal ultrametric trees on row and\ncolumn axes, and stochastic uniform randomness within each block found on the\nfinest scale. Functionally a series of increasingly larger ensembles of matrix\nmimicries is derived by conforming to the multiscale block configurations. Here\nmatrix mimicking is meant to be subject to constraints of row and column sums\nsequences. Based on such a series of ensembles, a profile of distributions\nbecomes a natural device for checking the validity of testing statistics or\nstructural indexes. An energy based index is used for testing whether network\ndata indeed contains structural geometry. A new version block-based nestedness\nindex is also proposed. Its validity is checked and compared with the existing\nones. A computing paradigm, called Data Mechanics, and its application on one\nreal data network are illustrated throughout the developments and discussions\nin this paper. \n\n"}
{"id": "1802.01053", "contents": "Title: Using Poisson Binomial GLMs to Reveal Voter Preferences Abstract: We present a new modeling technique for solving the problem of ecological\ninference, in which individual-level associations are inferred from labeled\ndata available only at the aggregate level. We model aggregate count data as\narising from the Poisson binomial, the distribution of the sum of independent\nbut not identically distributed Bernoulli random variables. We relate\nindividual-level probabilities to individual covariates using both a logistic\nregression and a neural network. A normal approximation is derived via the\nLyapunov Central Limit Theorem, allowing us to efficiently fit these models on\nlarge datasets. We apply this technique to the problem of revealing voter\npreferences in the 2016 presidential election, fitting a model to a sample of\nover four million voters from the highly contested swing state of Pennsylvania.\nWe validate the model at the precinct level via a holdout set, and at the\nindividual level using weak labels, finding that the model is predictive and it\nlearns intuitively reasonable associations. \n\n"}
{"id": "1802.03529", "contents": "Title: Revealing hidden scenes by photon-efficient occlusion-based\n  opportunistic active imaging Abstract: The ability to see around corners, i.e., recover details of a hidden scene\nfrom its reflections in the surrounding environment, is of considerable\ninterest in a wide range of applications. However, the diffuse nature of light\nreflected from typical surfaces leads to mixing of spatial information in the\ncollected light, precluding useful scene reconstruction. Here, we employ a\ncomputational imaging technique that opportunistically exploits the presence of\noccluding objects, which obstruct probe-light propagation in the hidden scene,\nto undo the mixing and greatly improve scene recovery. Importantly, our\ntechnique obviates the need for the ultrafast time-of-flight measurements\nemployed by most previous approaches to hidden-scene imaging. Moreover, it does\nso in a photon-efficient manner based on an accurate forward model and a\ncomputational algorithm that, together, respect the physics of three-bounce\nlight propagation and single-photon detection. Using our methodology, we\ndemonstrate reconstruction of hidden-surface reflectivity patterns in a\nmeter-scale environment from non-time-resolved measurements. Ultimately, our\ntechnique represents an instance of a rich and promising new imaging modality\nwith important potential implications for imaging science. \n\n"}
{"id": "1802.04233", "contents": "Title: Embedding Complexity In the Data Representation Instead of In the Model:\n  A Case Study Using Heterogeneous Medical Data Abstract: Electronic Health Records have become popular sources of data for secondary\nresearch, but their use is hampered by the amount of effort it takes to\novercome the sparsity, irregularity, and noise that they contain. Modern\nlearning architectures can remove the need for expert-driven feature\nengineering, but not the need for expert-driven preprocessing to abstract away\nthe inherent messiness of clinical data. This preprocessing effort is often the\ndominant component of a typical clinical prediction project. In this work we\npropose using semantic embedding methods to directly couple the raw, messy\nclinical data to downstream learning architectures with truly minimal\npreprocessing. We examine this step from the perspective of capturing and\nencoding complex data dependencies in the data representation instead of in the\nmodel, which has the nice benefit of allowing downstream processing to be done\nwith fast, lightweight, and simple models accessible to researchers without\nmachine learning expertise. We demonstrate with three typical clinical\nprediction tasks that the highly compressed, embedded data representations\ncapture a large amount of useful complexity, although in some cases the\ncompression is not completely lossless. \n\n"}
{"id": "1802.04447", "contents": "Title: Graph Coarsening with Preserved Spectral Properties Abstract: Large-scale graphs are widely used to represent object relationships in many\nreal world applications. The occurrence of large-scale graphs presents\nsignificant computational challenges to process, analyze, and extract\ninformation. Graph coarsening techniques are commonly used to reduce the\ncomputational load while attempting to maintain the basic structural properties\nof the original graph. As there is no consensus on the specific graph\nproperties preserved by coarse graphs, how to measure the differences between\noriginal and coarse graphs remains a key challenge. In this work, we introduce\na new perspective regarding the graph coarsening based on concepts from\nspectral graph theory. We propose and justify new distance functions that\ncharacterize the differences between original and coarse graphs. We show that\nthe proposed spectral distance naturally captures the structural differences in\nthe graph coarsening process. In addition, we provide efficient graph\ncoarsening algorithms to generate graphs which provably preserve the spectral\nproperties from original graphs. Experiments show that our proposed algorithms\nconsistently achieve better results compared to previous graph coarsening\nmethods on graph classification and block recovery tasks. \n\n"}
{"id": "1802.04664", "contents": "Title: Recovering Loss to Followup Information Using Denoising Autoencoders Abstract: Loss to followup is a significant issue in healthcare and has serious\nconsequences for a study's validity and cost. Methods available at present for\nrecovering loss to followup information are restricted by their expressive\ncapabilities and struggle to model highly non-linear relations and complex\ninteractions. In this paper we propose a model based on overcomplete denoising\nautoencoders to recover loss to followup information. Designed to work with\nhigh volume data, results on various simulated and real life datasets show our\nmodel is appropriate under varying dataset and loss to followup conditions and\noutperforms the state-of-the-art methods by a wide margin ($\\ge 20\\%$ in some\nscenarios) while preserving the dataset utility for final analysis. \n\n"}
{"id": "1802.06710", "contents": "Title: Discovering Effect Modification and Randomization Inference in Air\n  Pollution Studies Abstract: Studies have shown that exposure to air pollution, even at low levels,\nsignificantly increases mortality. As regulatory actions are becoming\nprohibitively expensive, robust evidence to guide the development of targeted\ninterventions to reduce air pollution exposure is needed. In this paper, we\nintroduce a novel statistical method that splits the data into two subsamples:\n(a) Using the first subsample, we consider a data-driven search for $\\textit{de\nnovo}$ discovery of subgroups that could have exposure effects that differ from\nthe population mean; and then (b) using the second subsample, we quantify\nevidence of effect modification among the subgroups with nonparametric\nrandomization-based tests. We also develop a sensitivity analysis method to\nassess the robustness of the conclusions to unmeasured confounding bias. Via\nsimulation studies and theoretical arguments, we demonstrate that since we\ndiscover the subgroups in the first subsample, hypothesis testing on the second\nsubsample can focus on theses subgroups only, thus substantially increasing the\nstatistical power of the test. We apply our method to the data of 1,612,414\nMedicare beneficiaries in New England region in the United States for the\nperiod 2000 to 2006. We find that seniors aged between 81-85 with low income\nand seniors aged above 85 have statistically significant higher causal effects\nof exposure to PM$_{2.5}$ on 5-year mortality rate compared to the population\nmean. \n\n"}
{"id": "1802.07408", "contents": "Title: Physics and Human-Based Information Fusion for Improved Resident Space\n  Object Tracking Abstract: Maintaining a catalog of Resident Space Objects (RSOs) can be cast in a\ntypical Bayesian multi-object estimation problem, where the various sources of\nuncertainty in the problem - the orbital mechanics, the kinematic states of the\nidentified objects, the data sources, etc. - are modeled as random variables\nwith associated probability distributions. In the context of Space Situational\nAwareness, however, the information available to a space analyst on many\nuncertain components is scarce, preventing their appropriate modeling with a\nrandom variable and thus their exploitation in a RSO tracking algorithm. A\ntypical example are human-based data sources such as Two-Line Elements (TLEs),\nwhich are publicly available but lack any statistical description of their\naccuracy. In this paper, we propose the first exploitation of uncertain\nvariables in a RSO tracking problem, allowing for a representation of the\nuncertain components reflecting the information available to the space analyst,\nhowever scarce, and nothing more. In particular, we show that a human-based\ndata source and a physics-based data source can be embedded in a unified and\nrigorous Bayesian estimator in order to track a RSO. We illustrate this concept\non a scenario where real TLEs queried from the U.S. Strategic Command are fused\nwith realistically simulated radar observations in order to track a Low-Earth\nOrbit satellite. \n\n"}
{"id": "1802.07479", "contents": "Title: Optimal Base Station Antenna Downtilt in Downlink Cellular Networks Abstract: From very recent studies, the area spectral efficiency (ASE) performance of\ndownlink (DL) cellular networks will continuously decrease and finally to zero\nwith the network densification in a fully loaded ultra-dense network (UDN) when\nthe absolute height difference between a base station (BS) antenna and a user\nequipment (UE) antenna is larger than zero, which is referred as the ASE Crash.\nWe revisit this issue by considering the impact of the BS antenna downtilt on\nthe downlink network capacity. In general, there exists a height difference\nbetween a BS and a UE in practical networks. It is common to utilize antenna\ndowntilt to adjust the direction of the vertical antenna pattern, and thus\nincrease received signal power or reduce inter-cell interference power to\nimprove network performance. This paper focuses on investigating the\nrelationship between the base station antenna downtilt and the downlink network\ncapacity in terms of the coverage probability and the ASE. The analytical\nresults of the coverage probability and the ASE are derived, and we find that\nthere exists an optimal antenna downtilt to achieve the maximal coverage\nprobability for each base station density. Moreover, we derive numerically\nsolvable expressions for the optimal antenna downtilt, which is a function of\nthe base station density. Our theoretical and numerical results show that after\napplying the optimal antenna downtilt, the network performance can be improved\nsignificantly. Specifically, with the optimal antenna downtilt, the ASE crash\ncan be delayed by nearly one order of magnitude in terms of the base station\ndensity. \n\n"}
{"id": "1802.08616", "contents": "Title: Mastery Learning in Practice: A (Mostly) Descriptive Analysis of Log\n  Data from the Cognitive Tutor Algebra I Effectiveness Trial Abstract: Mastery learning, the notion that students learn best if they move on from\nstudying a topic only after having demonstrated mastery, sits at the foundation\nof the theory of intelligent tutoring. This paper is an exploration of how\nmastery learning plays out in practice, based on log data from a large\nrandomized effectiveness trial of the Cognitive Tutor Algebra I (CTAI)\ncurriculum. We find that students frequently progressed from CTAI sections they\nwere working on without demonstrating mastery and worked units out of order.\nMoreover, these behaviors were substantially more common in the second year of\nthe study, in which the CTAI effect was significantly larger. We explore the\nvarious ways students departed from the official CTAI curriculum, focusing on\nheterogeneity between years, states, schools, and students. The paper concludes\nwith an observational study of the effect on post-test scores of teachers\nreassigning students out of their current sections before they mastered the\nrequisite skills, finding that reassignment appears to lowers posttest\nscores--a finding that is fairly resilient to confounding from omitted\ncovariates--but that the effect varies substantially between classrooms. \n\n"}
{"id": "1802.10570", "contents": "Title: Statistical shape analysis in a Bayesian framework for shapes in two and\n  three dimensions Abstract: In this paper, we describe a novel shape classification method which is\nembedded in the Bayesian paradigm. We discuss the modelling and the resulting\nshape classification algorithm for two and three dimensional data shapes. We\nconclude by evaluating the efficiency and efficacy of the proposed algorithm on\nthe Kimia shape database for the two dimensional case. \n\n"}
{"id": "1803.01462", "contents": "Title: Optimal Status Updating for an Energy Harvesting Sensor with a Noisy\n  Channel Abstract: Consider an energy harvesting sensor continuously monitors a system and sends\ntime-stamped status update to a destination. The destination keeps track of the\nsystem status through the received updates. Under the energy causality\nconstraint at the sensor, our objective is to design an optimal online status\nupdating policy to minimize the long-term average Age of Information (AoI) at\nthe destination. We focus on the scenario where the the channel between the\nsource and the destination is noisy, and each transmitted update may fail\nindependently with a constant probability. We assume there is no channel state\ninformation or transmission feedback available to the sensor. We prove that\nwithin a broadly defined class of online policies, the best-effort uniform\nupdating policy, which was shown to be optimal when the channel is perfect, is\nstill optimal in the presence of update failures. Our proof relies on tools\nfrom Martingale processes, and the construction of a sequence of virtual\npolicies. \n\n"}
{"id": "1803.02916", "contents": "Title: A Bayesian framework for molecular strain identification from mixed\n  diagnostic samples Abstract: We provide a mathematical formulation and develop a computational framework\nfor identifying multiple strains of microorganisms from mixed samples of DNA.\nOur method is applicable in public health domains where efficient\nidentification of pathogens is paramount, e.g., for the monitoring of disease\noutbreaks. We formulate strain identification as an inverse problem that aims\nat simultaneously estimating a binary matrix (encoding presence or absence of\nmutations in each strain) and a real-valued vector (representing the mixture of\nstrains) such that their product is approximately equal to the measured data\nvector. The problem at hand has a similar structure to blind deconvolution,\nexcept for the presence of binary constraints, which we enforce in our\napproach. Following a Bayesian approach, we derive a posterior density. We\npresent two computational methods for solving the non-convex maximum a\nposteriori estimation problem. The first one is a local optimization method\nthat is made efficient and scalable by decoupling the problem into smaller\nindependent subproblems, whereas the second one yields a global minimizer by\nconverting the problem into a convex mixed-integer quadratic programming\nproblem. The decoupling approach also provides an efficient way to integrate\nover the posterior. This provides useful information about the ambiguity of the\nunderdetermined problem and, thus, the uncertainty associated with numerical\nsolutions. We evaluate the potential and limitations of our framework in silico\nusing synthetic and experimental data with available ground truths. \n\n"}
{"id": "1803.03019", "contents": "Title: Generalized Linear Models for Geometrical Current predictors. An\n  application to predict garment fit Abstract: The aim of this paper is to model an ordinal response variable in terms of\nvector-valued functional data included on a vector-valued RKHS. In particular,\nwe focus on the vector-valued RKHS obtained when a geometrical object (body) is\ncharacterized by a current and on the ordinal regression model. A common way to\nsolve this problem in functional data analysis is to express the data in the\northonormal basis given by decomposition of the covariance operator. But our\ndata present very important differences with respect to the usual functional\ndata setting. On the one hand, they are vector-valued functions, and on the\nother, they are functions in an RKHS with a previously defined norm. We propose\nto use three different bases: the orthonormal basis given by the kernel that\ndefines the RKHS, a basis obtained from decomposition of the integral operator\ndefined using the covariance function, and a third basis that combines the\nprevious two. The three approaches are compared and applied to an interesting\nproblem: building a model to predict the fit of children's garment sizes, based\non a 3D database of the Spanish child population. \n\n"}
{"id": "1803.03285", "contents": "Title: Massive UAV-to-Ground Communication and its Stable Movement Control: A\n  Mean-Field Approach Abstract: This paper proposes a real-time movement control algorithm for massive\nunmanned aerial vehicles (UAVs) that provide emergency cellular connections in\nan urban disaster site. While avoiding the inter-UAV collision under temporal\nwind dynamics, the proposed algorithm minimizes each UAV's energy consumption\nper unit downlink rate. By means of a mean-field game theoretic flocking\napproach, the velocity control of each UAV only requires its own location and\nchannel states. Numerical results validate the performance of the algorithm in\nterms of the number of collisions and energy consumption per data rate, under a\nrealistic 3GPP UAV channel model. \n\n"}
{"id": "1803.03497", "contents": "Title: Modelos de Resposta para Experimentos Randomizados em Redes Sociais de\n  Larga Escala Abstract: A/B tests are randomized experiments frequently used by companies that offer\nservices on the Web for assessing the impact of new features. During an\nexperiment, each user is randomly redirected to one of two versions of the\nwebsite, called treatments. Several response models were proposed to describe\nthe behavior of a user in a social network website, where the treatment\nassigned to her neighbors must be taken into account. However, there is no\nconsensus as to which model should be applied to a given dataset. In this work,\nwe propose a new response model, derive theoretical limits for the estimation\nerror of several models, and obtain empirical results for cases where the\nresponse model was misspecified. \n\n"}
{"id": "1803.04353", "contents": "Title: Partial Identifiability of Restricted Latent Class Models Abstract: Latent class models have wide applications in social and biological sciences.\nIn many applications, pre-specified restrictions are imposed on the parameter\nspace of latent class models, through a design matrix, to reflect\npractitioners' assumptions about how the observed responses depend on subjects'\nlatent traits. Though widely used in various fields, such restricted latent\nclass models suffer from non-identifiability due to their discreteness nature\nand complex structure of restrictions. This work addresses the fundamental\nidentifiability issue of restricted latent class models by developing a general\nframework for strict and partial identifiability of the model parameters. Under\ncorrect model specification, the developed identifiability conditions only\ndepend on the design matrix and are easily checkable, which provide useful\npractical guidelines for designing statistically valid diagnostic tests.\nFurthermore, the new theoretical framework is applied to establish, for the\nfirst time, identifiability of several designs from cognitive diagnosis\napplications. \n\n"}
{"id": "1803.06053", "contents": "Title: The world of research has gone berserk: modeling the consequences of\n  requiring \"greater statistical stringency\" for scientific publication Abstract: In response to growing concern about the reliability and reproducibility of\npublished science, researchers have proposed adopting measures of greater\nstatistical stringency, including suggestions to require larger sample sizes\nand to lower the highly criticized p<0.05 significance threshold. While pros\nand cons are vigorously debated, there has been little to no modeling of how\nadopting these measures might affect what type of science is published. In this\npaper, we develop a novel optimality model that, given current incentives to\npublish, predicts a researcher's most rational use of resources in terms of the\nnumber of studies to undertake, the statistical power to devote to each study,\nand the desirable pre-study odds to pursue. We then develop a methodology that\nallows one to estimate the reliability of published research by considering a\ndistribution of preferred research strategies. Using this approach, we\ninvestigate the merits of adopting measures of `greater statistical stringency'\nwith the goal of informing the ongoing debate. \n\n"}
{"id": "1803.06336", "contents": "Title: Applying the Delta method in metric analytics: A practical guide with\n  novel ideas Abstract: During the last decade, the information technology industry has adopted a\ndata-driven culture, relying on online metrics to measure and monitor business\nperformance. Under the setting of big data, the majority of such metrics\napproximately follow normal distributions, opening up potential opportunities\nto model them directly without extra model assumptions and solve big data\nproblems via closed-form formulas using distributed algorithms at a fraction of\nthe cost of simulation-based procedures like bootstrap. However, certain\nattributes of the metrics, such as their corresponding data generating\nprocesses and aggregation levels, pose numerous challenges for constructing\ntrustworthy estimation and inference procedures. Motivated by four real-life\nexamples in metric development and analytics for large-scale A/B testing, we\nprovide a practical guide to applying the Delta method, one of the most\nimportant tools from the classic statistics literature, to address the\naforementioned challenges. We emphasize the central role of the Delta method in\nmetric analytics by highlighting both its classic and novel applications. \n\n"}
{"id": "1803.06518", "contents": "Title: Provable Convex Co-clustering of Tensors Abstract: Cluster analysis is a fundamental tool for pattern discovery of complex\nheterogeneous data. Prevalent clustering methods mainly focus on vector or\nmatrix-variate data and are not applicable to general-order tensors, which\narise frequently in modern scientific and business applications. Moreover,\nthere is a gap between statistical guarantees and computational efficiency for\nexisting tensor clustering solutions due to the nature of their non-convex\nformulations. In this work, we bridge this gap by developing a provable convex\nformulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator\nenjoys stability guarantees and its computational and storage costs are\npolynomial in the size of the data. We further establish a non-asymptotic error\nbound for the CoCo estimator, which reveals a surprising \"blessing of\ndimensionality\" phenomenon that does not exist in vector or matrix-variate\ncluster analysis. Our theoretical findings are supported by extensive simulated\nstudies. Finally, we apply the CoCo estimator to the cluster analysis of\nadvertisement click tensor data from a major online company. Our clustering\nresults provide meaningful business insights to improve advertising\neffectiveness. \n\n"}
{"id": "1803.06730", "contents": "Title: Combining Probabilistic Load Forecasts Abstract: Probabilistic load forecasts provide comprehensive information about future\nload uncertainties. In recent years, many methodologies and techniques have\nbeen proposed for probabilistic load forecasting. Forecast combination, a\nwidely recognized best practice in point forecasting literature, has never been\nformally adopted to combine probabilistic load forecasts. This paper proposes a\nconstrained quantile regression averaging (CQRA) method to create an improved\nensemble from several individual probabilistic forecasts. We formulate the CQRA\nparameter estimation problem as a linear program with the objective of\nminimizing the pinball loss, with the constraints that the parameters are\nnonnegative and summing up to one. We demonstrate the effectiveness of the\nproposed method using two publicly available datasets, the ISO New England data\nand Irish smart meter data. Comparing with the best individual probabilistic\nforecast, the ensemble can reduce the pinball score by 4.39% on average. The\nproposed ensemble also demonstrates superior performance over nine other\nbenchmark ensembles. \n\n"}
{"id": "1803.06763", "contents": "Title: Differentially Private Data Release via Statistical Election to\n  Partition Sequentially Abstract: Differential Privacy (DP) formalizes privacy in mathematical terms and\nprovides a robust concept for privacy protection. DIfferentially Private Data\nSynthesis (DIPS) techniques produce and release synthetic individual-level data\nin the DP framework. One key challenge to developing DIPS methods is\npreservation of the statistical utility of synthetic data, especially in\nhigh-dimensional settings. We propose a new DIPS approach, STatistical Election\nto Partition Sequentially (STEPS) that partitions data by attributes according\nto their importance ranks according to either a practical or statistical\nimportance measure. STEPS aims to achieve better original information\npreservation for the attributes with higher importance ranks and produce thus\nmore useful synthetic data overall. We present an algorithm to implement the\nSTEPS procedure and employ the privacy budget composability to ensure the\noverall privacy cost is controlled at the pre-specified value. We apply the\nSTEPS procedure to both simulated data and the 2000-2012 Current Population\nSurvey youth voter data. The results suggest STEPS can better preserve the\npopulation-level information and the original information for some analyses\ncompared to PrivBayes, a modified Uniform histogram approach, and the flat\nLaplace sanitizer. \n\n"}
{"id": "1803.07166", "contents": "Title: Latent Space Modeling of Multidimensional Networks with Application to\n  the Exchange of Votes in Eurovision Song Contest Abstract: The Eurovision Song Contest is a popular TV singing competition held annually\namong country members of the European Broadcasting Union. In this competition,\neach member can be both contestant and jury, as it can participate with a song\nand/or vote for other countries' tunes. Throughout the years, the voting system\nhas repeatedly been accused of being biased by the presence of tactical voting,\naccording to which votes would represent strategic interests rather than actual\nmusical preferences of the voting countries. In this work, we develop a latent\nspace model to investigate the presence of a latent structure underlying the\nexchange of votes. Focusing on the period from 1998 to 2015, we represent the\nvote exchange as a multivariate network: each edition is a network, where\ncountries are the nodes and two countries are linked by an edge if one voted\nfor the other. The different networks are taken to be independent replicates of\na common latent space capturing the overall relationships among the countries.\nProximity denotes similarity, and countries close in the latent space are\nassumed to be more likely to exchange votes. Therefore, if the exchange of\nvotes depends on the similarity between countries, the quality of the competing\nsongs might not be a relevant factor in the determination of the voting\npreferences, and this would suggest the presence of bias. A Bayesian\nhierarchical modelling approach is employed to model the probability of a\nconnection between any two countries as a function of their distance in the\nlatent space, and of network-specific parameters and edge-specific covariates.\nThe inferred latent space is found to be relevant in the determination of edge\nprobabilities, however, the positions of the countries in such space only\npartially correspond to their actual geographical positions. \n\n"}
{"id": "1803.07418", "contents": "Title: Large-Scale Model Selection with Misspecification Abstract: Model selection is crucial to high-dimensional learning and inference for\ncontemporary big data applications in pinpointing the best set of covariates\namong a sequence of candidate interpretable models. Most existing work assumes\nimplicitly that the models are correctly specified or have fixed\ndimensionality. Yet both features of model misspecification and high\ndimensionality are prevalent in practice. In this paper, we exploit the\nframework of model selection principles in misspecified models originated in Lv\nand Liu (2014) and investigate the asymptotic expansion of Bayesian principle\nof model selection in the setting of high-dimensional misspecified models. With\na natural choice of prior probabilities that encourages interpretability and\nincorporates Kullback-Leibler divergence, we suggest the high-dimensional\ngeneralized Bayesian information criterion with prior probability (HGBIC_p) for\nlarge-scale model selection with misspecification. Our new information\ncriterion characterizes the impacts of both model misspecification and high\ndimensionality on model selection. We further establish the consistency of\ncovariance contrast matrix estimation and the model selection consistency of\nHGBIC_p in ultra-high dimensions under some mild regularity conditions. The\nadvantages of our new method are supported by numerical studies. \n\n"}
{"id": "1803.07734", "contents": "Title: Adaptive Sequential MCMC for Combined State and Parameter Estimation Abstract: In the case of a linear state space model, we implement an MCMC sampler with\ntwo phases. In the learning phase, a self-tuning sampler is used to learn the\nparameter mean and covariance structure. In the estimation phase, the parameter\nmean and covariance structure informs the proposed mechanism and is also used\nin a delayed-acceptance algorithm. Information on the resulting state of the\nsystem is given by a Gaussian mixture. In on-line mode, the algorithm is\nadaptive and uses a sliding window approach to accelerate sampling speed and to\nmaintain appropriate acceptance rates. We apply the algorithm to joined state\nand parameter estimation in the case of irregularly sampled GPS time series\ndata. \n\n"}
{"id": "1803.08424", "contents": "Title: Autoplotly - Automatic Generation of Interactive Visualizations for\n  Popular Statistical Results Abstract: The autoplotly package provides functionalities to automatically generate\ninteractive visualizations for many popular statistical results supported by\nggfortify package with plotly and ggplot2 style. The generated visualizations\ncan also be easily extended using ggplot2 and plotly syntax while staying\ninteractive. \n\n"}
{"id": "1803.08584", "contents": "Title: Curvature of Hypergraphs via Multi-Marginal Optimal Transport Abstract: We introduce a novel definition of curvature for hypergraphs, a natural\ngeneralization of graphs, by introducing a multi-marginal optimal transport\nproblem for a naturally defined random walk on the hypergraph. This curvature,\ntermed \\emph{coarse scalar curvature}, generalizes a recent definition of Ricci\ncurvature for Markov chains on metric spaces by Ollivier [Journal of Functional\nAnalysis 256 (2009) 810-864], and is related to the scalar curvature when the\nhypergraph arises naturally from a Riemannian manifold. We investigate basic\nproperties of the coarse scalar curvature and obtain several bounds. Empirical\nexperiments indicate that coarse scalar curvatures are capable of detecting\n\"bridges\" across connected components in hypergraphs, suggesting it is an\nappropriate generalization of curvature on simple graphs. \n\n"}
{"id": "1803.09015", "contents": "Title: Difference-in-Differences with Multiple Time Periods Abstract: In this article, we consider identification, estimation, and inference\nprocedures for treatment effect parameters using Difference-in-Differences\n(DiD) with (i) multiple time periods, (ii) variation in treatment timing, and\n(iii) when the \"parallel trends assumption\" holds potentially only after\nconditioning on observed covariates. We show that a family of causal effect\nparameters are identified in staggered DiD setups, even if differences in\nobserved characteristics create non-parallel outcome dynamics between groups.\nOur identification results allow one to use outcome regression, inverse\nprobability weighting, or doubly-robust estimands. We also propose different\naggregation schemes that can be used to highlight treatment effect\nheterogeneity across different dimensions as well as to summarize the overall\neffect of participating in the treatment. We establish the asymptotic\nproperties of the proposed estimators and prove the validity of a\ncomputationally convenient bootstrap procedure to conduct asymptotically valid\nsimultaneous (instead of pointwise) inference. Finally, we illustrate the\nrelevance of our proposed tools by analyzing the effect of the minimum wage on\nteen employment from 2001--2007. Open-source software is available for\nimplementing the proposed methods. \n\n"}
{"id": "1803.09730", "contents": "Title: Resilient Active Information Gathering with Mobile Robots Abstract: Applications of safety, security, and rescue in robotics, such as multi-robot\ntarget tracking, involve the execution of information acquisition tasks by\nteams of mobile robots. However, in failure-prone or adversarial environments,\nrobots get attacked, their communication channels get jammed, and their sensors\nmay fail, resulting in the withdrawal of robots from the collective task, and\nconsequently the inability of the remaining active robots to coordinate with\neach other. As a result, traditional design paradigms become insufficient and,\nin contrast, resilient designs against system-wide failures and attacks become\nimportant. In general, resilient design problems are hard, and even though they\noften involve objective functions that are monotone or submodular, scalable\napproximation algorithms for their solution have been hitherto unknown. In this\npaper, we provide the first algorithm, enabling the following capabilities:\nminimal communication, i.e., the algorithm is executed by the robots based only\non minimal communication between them; system-wide resiliency, i.e., the\nalgorithm is valid for any number of denial-of-service attacks and failures;\nand provable approximation performance, i.e., the algorithm ensures for all\nmonotone (and not necessarily submodular) objective functions a solution that\nis finitely close to the optimal. We quantify our algorithm's approximation\nperformance using a notion of curvature for monotone set functions. We support\nour theoretical analyses with simulated and real-world experiments, by\nconsidering an active information gathering scenario, namely, multi-robot\ntarget tracking. \n\n"}
{"id": "1803.10975", "contents": "Title: A simulation comparison of tournament designs for the World Men's\n  Handball Championships Abstract: The study aims to compare different designs for the World Men's Handball\nChampionships. This event, organised in every two years, has adopted four\nhybrid formats consisting of knockout and round-robin stages in recent decades,\nincluding a change of design between the two recent championships in 2017 and\n2019. They are evaluated under two extremal seeding policies with respect to\nvarious outcome measures through Monte-Carlo simulations. We find that the\nability to give the first four positions to the strongest teams, as well as the\nexpected quality and outcome uncertainty of the final is not necessarily a\nmonotonic function of the number of matches played: the most frugal format is\nthe second best with respect to these outcome measures, making it a good\ncompromise in an unavoidable trade-off. A possible error is identified in a\nparticular design. The relative performance of the formats is independent of\nthe seeding rules and the competitive balance of the teams. The recent reform\nis demonstrated to have increased the probability of winning for the top teams.\nOur results have useful implications for the organisers of hybrid tournaments. \n\n"}
{"id": "1803.11130", "contents": "Title: Incentive Design in a Distributed Problem with Strategic Agents Abstract: In this paper, we consider a general distributed system with multiple agents\nwho select and then implement actions in the system. The system has an operator\nwith a centralized objective. The agents, on the other hand, are selfinterested\nand strategic in the sense that each agent optimizes its own individual\nobjective. The operator aims to mitigate this misalignment by designing an\nincentive scheme for the agents. The problem is difficult due to the cost\nfunctions of the agents being coupled, the objective of the operator not being\nsocial welfare, and the operator having no direct control over actions being\nimplemented by the agents. This problem has been studied in many fields,\nparticularly in mechanism design and cost allocation. However, mechanism design\ntypically assumes that the operator has knowledge of the cost functions of the\nagents and the actions being implemented by the operator. On the other hand,\ncost allocation classically assumes that agents do not anticipate the effect of\ntheir actions on the incentive that they obtain. We remove these assumptions\nand present an incentive rule for this setup by bridging the gap between\nmechanism design and classical cost allocation. We analyze whether the proposed\ndesign satisfies various desirable properties such as social optimality, budget\nbalance, participation constraint, and so on. We also analyze which of these\nproperties can be satisfied if the assumptions of cost functions of the agents\nbeing private and the agents being anticipatory are relaxed. \n\n"}
{"id": "1803.11194", "contents": "Title: A Large Scale Spatio-temporal Binomial Regression Model for Estimating\n  Seroprevalence Trends Abstract: This paper develops a large-scale Bayesian spatio-temporal binomial\nregression model for the purpose of investigating regional trends in antibody\nprevalence to Borrelia burgdorferi, the causative agent of Lyme disease. The\nproposed model uses Gaussian predictive processes to estimate the spatially\nvarying trends and a conditional autoregressive model to account for\nspatio-temporal dependence. Careful consideration is made to develop a novel\nframework that is scalable to large spatio-temporal data. The proposed model is\nused to analyze approximately 16 million Borrelia burgdorferi test results\ncollected on dogs located throughout the conterminous United States over a\nsixty month period. This analysis identifies several regions of increasing\ncanine risk. Specifically, this analysis reveals evidence that Lyme disease is\ngetting worse in some endemic regions and that it could potentially be\nspreading to other non-endemic areas. Further, given the zoonotic nature of\nthis vector-borne disease, this analysis could potentially reveal areas of\nincreasing human risk. \n\n"}
{"id": "1804.02090", "contents": "Title: Microsimulation Model Calibration using Incremental Mixture Approximate\n  Bayesian Computation Abstract: Microsimulation models (MSMs) are used to predict population-level effects of\nhealth care policies by simulating individual-level outcomes. Simulated\noutcomes are governed by unknown parameters that are chosen so that the model\naccurately predicts specific targets, a process referred to as model\ncalibration. Calibration targets can come from randomized controlled trials,\nobservational studies, and expert opinion, and are typically summary\nstatistics. A well calibrated model can reproduce a wide range of targets. MSM\ncalibration generally involves searching a high dimensional parameter space and\npredicting many targets through model simulation. This requires efficient\nmethods for exploring the parameter space and sufficient computational\nresources. We develop Incremental Mixture Approximate Bayesian Computation\n(IMABC) as a method for MSM calibration and implement it via a high-performance\ncomputing workflow, which provides the necessary computational scale. IMABC\nbegins with a rejection-based approximate Bayesian computation (ABC) step,\ndrawing a sample of parameters from the prior distribution and simulating\ncalibration targets. Next, the sample is iteratively updated by drawing\nadditional points from a mixture of multivariate normal distributions, centered\nat the points that yield simulated targets that are near observed targets.\nPosterior estimates are obtained by weighting sampled parameter vectors to\naccount for the adaptive sampling scheme. We demonstrate IMABC by calibrating a\nMSM for the natural history of colorectal cancer to obtain simulated draws from\nthe joint posterior distribution of model parameters. \n\n"}
{"id": "1804.04586", "contents": "Title: Reliability Analysis of Polymeric Materials Abstract: Polymeric materials are widely used in many applications and are especially\nuseful when combined with other polymers to make polymer composites. The\nappealing features of these materials come from their having comparable levels\nof strength and endurance to what one would find in metal alloys while being\nmore lightweight and economical. However, these materials are still susceptible\nto degradation over time and so it is of great importance to manufacturers to\nassess their product's lifetime. Because these materials are meant to last over\na span of several years or even decades, accelerated testing is often the\nmethod of choice in assessing product lifetimes in a more feasible time frame.\nIn this article, a brief introduction is given to the methods of accelerated\ntesting and analysis used with polymer materials. Special attention is given to\ndegradation testing and modeling due to the growing popularity of these\ntechniques along with a brief discussion of fatigue testing. References are\nprovided for further reading in each of these areas. \n\n"}
{"id": "1804.04588", "contents": "Title: Bayesian Modeling of Air Pollution Extremes Using Nested Multivariate\n  Max-Stable Processes Abstract: Capturing the potentially strong dependence among the peak concentrations of\nmultiple air pollutants across a spatial region is crucial for assessing the\nrelated public health risks. In order to investigate the multivariate spatial\ndependence properties of air pollution extremes, we introduce a new class of\nmultivariate max-stable processes. Our proposed model admits a hierarchical\ntree-based formulation, in which the data are conditionally independent given\nsome latent nested $\\alpha$-stable random factors. The hierarchical structure\nfacilitates Bayesian inference and offers a convenient and interpretable\ncharacterization. We fit this nested multivariate max-stable model to the\nmaxima of air pollution concentrations and temperatures recorded at a number of\nsites in the Los Angeles area, showing that the proposed model succeeds in\ncapturing their complex tail dependence structure. \n\n"}
{"id": "1804.05015", "contents": "Title: Large-scale diversity estimation through surname origin inference Abstract: The study of surnames as both linguistic and geographical markers of the past\nhas proven valuable in several research fields spanning from biology and\ngenetics to demography and social mobility. This article builds upon the\nexisting literature to conceive and develop a surname origin classifier based\non a data-driven typology. This enables us to explore a methodology to describe\nlarge-scale estimates of the relative diversity of social groups, especially\nwhen such data is scarcely available. We subsequently analyze the\nrepresentativeness of surname origins for 15 socio-professional groups in\nFrance. \n\n"}
{"id": "1804.05057", "contents": "Title: 5G Wireless Network Slicing for eMBB, URLLC, and mMTC: A\n  Communication-Theoretic View Abstract: The grand objective of 5G wireless technology is to support three generic\nservices with vastly heterogeneous requirements: enhanced mobile broadband\n(eMBB), massive machine-type communications (mMTC), and ultra-reliable\nlow-latency communications (URLLC). Service heterogeneity can be accommodated\nby network slicing, through which each service is allocated resources to\nprovide performance guarantees and isolation from the other services. Slicing\nof the Radio Access Network (RAN) is typically done by means of orthogonal\nresource allocation among the services. This work studies the potential\nadvantages of allowing for non-orthogonal sharing of RAN resources in uplink\ncommunications from a set of eMBB, mMTC and URLLC devices to a common base\nstation. The approach is referred to as Heterogeneous Non-Orthogonal Multiple\nAccess (H-NOMA), in contrast to the conventional NOMA techniques that involve\nusers with homogeneous requirements and hence can be investigated through a\nstandard multiple access channel. The study devises a communication-theoretic\nmodel that accounts for the heterogeneous requirements and characteristics of\nthe three services. The concept of reliability diversity is introduced as a\ndesign principle that leverages the different reliability requirements across\nthe services in order to ensure performance guarantees with non-orthogonal RAN\nslicing. This study reveals that H-NOMA can lead, in some regimes, to\nsignificant gains in terms of performance trade-offs among the three generic\nservices as compared to orthogonal slicing. \n\n"}
{"id": "1804.05430", "contents": "Title: Simultaneous disease mapping and hot spot detection with application to\n  childhood obesity surveillance from electronic health records Abstract: Electronic health records (EHRs) have become a platform for data-driven\nsurveillance on a granular level in recent years. In this paper, we make use of\nEHRs for early prevention of childhood obesity. The proposed method\nsimultaneously provides smooth disease mapping and outlier information for\nobesity prevalence, which are useful for raising public awareness and\nfacilitating targeted intervention. More precisely, we consider a penalized\nmultilevel generalized linear model. We decompose regional contribution into\nsmooth and sparse signals, which are automatically identified by a combination\nof fusion and sparse penalties imposed on the likelihood function. In addition,\nwe weigh the proposed likelihood to account for the missingness and potential\nnon-representativeness arising from the EHR data. We develop a novel\nalternating minimization algorithm, which is computationally efficient, easy to\nimplement, and guarantees convergence. Simulation studies demonstrate superior\nperformance of the proposed method. Finally, we apply our method to the\nUniversity of Wisconsin Population Health Information Exchange database. \n\n"}
{"id": "1804.05939", "contents": "Title: Origin of Information Encoding in Nucleic Acids through a\n  Dissipation-Replication Relation Abstract: Ultraviolet light incident on organic material can initiate its spontaneous\ndissipative structuring into chromophores which can then catalyze their own\nreplication. This may have been the case for one of the most ancient of all\nchromophores dissipating the Archean UVC photon flux, the nucleic acids. Under\nthe empirically established imperative of increasing entropy production,\nnucleic acids with affinity to particular amino acids which foment UVC photon\ndissipation would have been \"thermodynamically selected\" through this\ndissipation-replication relation. Indeed, we show here that those amino acids\nwith characteristics most relevant to fomenting UVC photon dissipation are\nprecisely those with greatest affinity to their codons or anticodons. This\ncould provide a physical-chemical mechanism for the accumulation of information\nin nucleic acids relevant to the dissipation of the externally imposed photon\npotential. This mechanism could provide a non-equilibrium thermodynamic\nfoundation, based on increasing global entropy production of the biosphere, for\nthe tenants of Darwinian natural selection. We show how this mechanism might\nhave begun operating at the origin of life in the Archean, and how, in fact, it\nstill operates today, albeit indirectly through complex biosynthetic pathways\nnow operating in the visible. \n\n"}
{"id": "1804.06327", "contents": "Title: Classifying Antimicrobial and Multifunctional Peptides with Bayesian\n  Network Models Abstract: Bayesian network models are finding success in characterizing\nenzyme-catalyzed reactions, slow conformational changes, predicting enzyme\ninhibition, and genomics. In this work, we apply them to statistical modeling\nof peptides by simultaneously identifying amino acid sequence motifs and using\na motif-based model to clarify the role motifs may play in antimicrobial\nactivity. We construct models of increasing sophistication, demonstrating how\nchemical knowledge of a peptide system may be embedded without requiring new\nderivation of model fitting equations after changing model structure. These\nmodels are used to construct classifiers with good performance (94% accuracy,\nMatthews correlation coefficient of 0.87) at predicting antimicrobial activity\nin peptides, while at the same time being built of interpretable parameters. We\ndemonstrate use of these models to identify peptides that are potentially both\nantimicrobial and antifouling, and show that the background distribution of\namino acids could play a greater role in activity than sequence motifs do. This\nprovides an advancement in the type of peptide activity modeling that can be\ndone and the ease in which models can be constructed. \n\n"}
{"id": "1804.06469", "contents": "Title: Bayesian parameter estimation for relativistic heavy-ion collisions Abstract: I develop and apply a Bayesian method for quantitatively estimating\nproperties of the quark-gluon plasma (QGP), an extremely hot and dense state of\nfluid-like matter created in relativistic heavy-ion collisions.\n  The QGP cannot be directly observed -- it is extraordinarily tiny and\nephemeral, about $10^{-14}$ meters in size and living $10^{-23}$ seconds before\nfreezing into discrete particles -- but it can be indirectly characterized by\nmatching the output of a computational collision model to experimental\nobservations. The model, which takes the QGP properties of interest as input\nparameters, is calibrated to fit the experimental data, thereby extracting a\nposterior probability distribution for the parameters.\n  In this dissertation, I construct a specific computational model of heavy-ion\ncollisions and formulate the Bayesian parameter estimation method, which is\nbased on general statistical techniques. I then apply these tools to estimate\nfundamental QGP properties, including its key transport coefficients and\ncharacteristics of the initial state of heavy-ion collisions.\n  Perhaps most notably, I report the most precise estimate to date of the\ntemperature-dependent specific shear viscosity $\\eta/s$, the measurement of\nwhich is a primary goal of heavy-ion physics. The estimated minimum value is\n$\\eta/s = 0.085_{-0.025}^{+0.026}$ (posterior median and 90% uncertainty),\nremarkably close to the conjectured lower bound of $1/4\\pi \\simeq 0.08$. The\nanalysis also shows that $\\eta/s$ likely increases slowly as a function of\ntemperature.\n  Other estimated quantities include the temperature-dependent bulk viscosity\n$\\zeta/s$, the scaling of initial state entropy deposition, and the duration of\nthe pre-equilibrium stage that precedes QGP formation. \n\n"}
{"id": "1804.06593", "contents": "Title: Coexistence of URLLC and eMBB services in the C-RAN Uplink: An\n  Information-Theoretic Study Abstract: The performance of orthogonal and non-orthogonal multiple access is studied\nfor the multiplexing of enhanced Mobile BroadBand (eMBB) and Ultra-Reliable\nLow-Latency Communications (URLLC) users in the uplink of a multi-cell Cloud\nRadio Access Network (C-RAN) architecture. While eMBB users can operate over\nlong codewords spread in time and frequency, URLLC users' transmissions are\nrandom and localized in time due to their low-latency requirements. These\nrequirements also call for decoding of their packets to be carried out at the\nedge nodes (ENs), whereas eMBB traffic can leverage the interference management\ncapabilities of centralized decoding at the cloud. Using information-theoretic\narguments, the performance trade-offs between eMBB and URLLC traffic types are\ninvestigated in terms of rate for the former, and rate, access latency, and\nreliability for the latter. The analysis includes non-orthogonal multiple\naccess (NOMA) with different decoding architectures, such as puncturing and\nsuccessive interference cancellation (SIC). The study sheds light into\neffective design choices as a function of inter-cell interference,\nsignal-to-noise ratio levels, and fronthaul capacity constraints. \n\n"}
{"id": "1804.07091", "contents": "Title: Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly\n  Detection Abstract: Automatic detection of anomalies in space- and time-varying measurements is\nan important tool in several fields, e.g., fraud detection, climate analysis,\nor healthcare monitoring. We present an algorithm for detecting anomalous\nregions in multivariate spatio-temporal time-series, which allows for spotting\nthe interesting parts in large amounts of data, including video and text data.\nIn opposition to existing techniques for detecting isolated anomalous data\npoints, we propose the \"Maximally Divergent Intervals\" (MDI) framework for\nunsupervised detection of coherent spatial regions and time intervals\ncharacterized by a high Kullback-Leibler divergence compared with all other\ndata given. In this regard, we define an unbiased Kullback-Leibler divergence\nthat allows for ranking regions of different size and show how to enable the\nalgorithm to run on large-scale data sets in reasonable time using an interval\nproposal technique. Experiments on both synthetic and real data from various\ndomains, such as climate analysis, video surveillance, and text forensics,\ndemonstrate that our method is widely applicable and a valuable tool for\nfinding interesting events in different types of data. \n\n"}
{"id": "1804.07491", "contents": "Title: MIMO Channel Hardening: A Physical Model based Analysis Abstract: In a multiple-input-multiple-output (MIMO) communication system, the\nmultipath fading is averaged over radio links. This well-known channel\nhardening phenomenon plays a central role in the design of massive MIMO\nsystems. The aim of this paper is to study channel hardening using a physical\nchannel model in which the influences of propagation rays and antenna array\ntopologies are highlighted. A measure of channel hardening is derived through\nthe coefficient of variation of the channel gain. Our analyses and closed form\nresults based on the used physical model are consistent with those of the\nliterature relying on more abstract Rayleigh fading models, but offer further\ninsights on the relationship with channel characteristics. \n\n"}
{"id": "1804.09253", "contents": "Title: DeepTriangle: A Deep Learning Approach to Loss Reserving Abstract: We propose a novel approach for loss reserving based on deep neural networks.\nThe approach allows for joint modeling of paid losses and claims outstanding,\nand incorporation of heterogeneous inputs. We validate the models on loss\nreserving data across lines of business, and show that they improve on the\npredictive accuracy of existing stochastic methods. The models require minimal\nfeature engineering and expert input, and can be automated to produce forecasts\nmore frequently than manual workflows. \n\n"}
{"id": "1805.00159", "contents": "Title: Detecting Galaxy-Filament Alignments in the Sloan Digital Sky Survey III Abstract: Previous studies have shown the filamentary structures in the cosmic web\ninfluence the alignments of nearby galaxies. We study this effect in the LOWZ\nsample of the Sloan Digital Sky Survey using the \"Cosmic Web Reconstruction\"\nfilament catalogue. We find that LOWZ galaxies exhibit a small but\nstatistically significant alignment in the direction parallel to the\norientation of nearby filaments. This effect is detectable even in the absence\nof nearby galaxy clusters, which suggests it is an effect from the matter\ndistribution in the filament. A nonparametric regression model suggests that\nthe alignment effect with filaments extends over separations of 30-40 Mpc. We\nfind that galaxies that are bright and early-forming align more strongly with\nthe directions of nearby filaments than those that are faint and late-forming;\nhowever, trends with stellar mass are less statistically significant, within\nthe narrow range of stellar mass of this sample. \n\n"}
{"id": "1805.01271", "contents": "Title: NFL Injuries Before and After the 2011 Collective Bargaining Agreement\n  (CBA) Abstract: The National Football League's (NFL) 2011 collective bargaining agreement\n(CBA) with its players placed a number of contact and quantity limitations on\npractices and workouts. Some coaches and others have expressed a concern that\nthis has led to poor conditioning and a subsequent increase in injuries. We\nsought to assess whether the 2011 CBA's practice restrictions affected the\nnumber of overall, conditioning-dependent, and/or non-conditioning-dependent\ninjuries in the NFL or the number of games missed due to those injuries. The\nstudy population was player-seasons from 2007-2016. We included regular season,\nnon-illness, non-head, game-loss injuries. Injuries were identified using a\ndatabase from Football Outsiders. The primary outcomes were overall,\nconditioning-dependent and non-conditioning-dependent injury counts by season.\nWe examined time trends in injury counts before (2007-2010) and after\n(2011-2016) the CBA using a Poisson interrupted time series model. The number\nof game-loss regular season, non-head, non-illness injuries grew from 701 in\n2007 to 804 in 2016 (15% increase). The number of regular season weeks missed\nexhibited a similar increase. Conditioning-dependent injuries increased from\n197 in 2007 to 271 in 2011 (38% rise), but were lower and remained relatively\nunchanged at 220-240 injuries per season thereafter. Non-conditioning injuries\ndecreased by 37% in the first three years of the new CBA before returning to\nhistoric levels in 2014-2016. Poisson models for all, conditioning-dependent,\nand non-conditioning-dependent game-loss injury counts did not show\nstatistically significant or meaningful detrimental changes associated with the\nCBA. We did not observe an increase in injuries following the 2011 CBA. Other\nconcurrent injury-related rule and regulation changes limit specific causal\ninferences about the practice restrictions, however. \n\n"}
{"id": "1805.01868", "contents": "Title: Algorithmic Decision Making in the Presence of Unmeasured Confounding Abstract: On a variety of complex decision-making tasks, from doctors prescribing\ntreatment to judges setting bail, machine learning algorithms have been shown\nto outperform expert human judgments. One complication, however, is that it is\noften difficult to anticipate the effects of algorithmic policies prior to\ndeployment, making the decision to adopt them risky. In particular, one\ngenerally cannot use historical data to directly observe what would have\nhappened had the actions recommended by the algorithm been taken. One standard\nstrategy is to model potential outcomes for alternative decisions assuming that\nthere are no unmeasured confounders (i.e., to assume ignorability). But if this\nignorability assumption is violated, the predicted and actual effects of an\nalgorithmic policy can diverge sharply. In this paper we present a flexible,\nBayesian approach to gauge the sensitivity of predicted policy outcomes to\nunmeasured confounders. We show that this policy evaluation problem is a\ngeneralization of estimating heterogeneous treatment effects in observational\nstudies, and so our methods can immediately be applied to that setting.\nFinally, we show, both theoretically and empirically, that under certain\nconditions it is possible to construct near-optimal algorithmic policies even\nwhen ignorability is violated. We demonstrate the efficacy of our methods on a\nlarge dataset of judicial actions, in which one must decide whether defendants\nawaiting trial should be required to pay bail or can be released without\npayment. \n\n"}
{"id": "1805.02109", "contents": "Title: Predicting Race and Ethnicity From the Sequence of Characters in a Name Abstract: To answer questions about racial inequality and fairness, we often need a way\nto infer race and ethnicity from names. One way to infer race and ethnicity\nfrom names is by relying on the Census Bureau's list of popular last names. The\nlist, however, suffers from at least three limitations: 1. it only contains\nlast names, 2. it only includes popular last names, and 3. it is updated once\nevery 10 years. To provide better generalization, and higher accuracy when\nfirst names are available, we model the relationship between characters in a\nname and race and ethnicity using various techniques. A model using Long\nShort-Term Memory works best with out-of-sample accuracy of .85. The\nbest-performing last-name model achieves out-of-sample accuracy of .81. To\nillustrate the utility of the models, we apply them to campaign finance data to\nestimate the share of donations made by people of various racial groups, and to\nnews data to estimate the coverage of various races and ethnicities in the\nnews. \n\n"}
{"id": "1805.02988", "contents": "Title: Hierarchical inference for genome-wide association studies: a view on\n  methodology with software Abstract: We provide a view on high-dimensional statistical inference for genome-wide\nassociation studies (GWAS). It is in part a review but covers also new\ndevelopments for meta analysis with multiple studies and novel software in\nterms of an R-package hierinf. Inference and assessment of significance is\nbased on very high-dimensional multivariate (generalized) linear models: in\ncontrast to often used marginal approaches, this provides a step towards more\ncausal-oriented inference. \n\n"}
{"id": "1805.03098", "contents": "Title: Functional Variable Selection for EMG-based Control of a Robotic Hand\n  Prosthetic Abstract: State-of-the-art robotic hand prosthetics generate finger and wrist movement\nthrough pattern recognition (PR) algorithms using features of forearm\nelectromyogram (EMG) signals, but re- quires extensive training and is prone to\npoor predictions for conditions outside the training data (Peerdeman et al.,\n2011; Scheme et al., 2010). We propose a novel approach to develop a dynamic\nrobotic limb by utilizing the recent history of EMG signals in a model that\naccounts for physiological features of hand movement which are ignored by PR\nalgorithms. We do this by viewing EMG signals as functional covariates and\ndevelop a functional linear model that quantifies the effect of the EMG signals\non finger/wrist velocity through a bivariate coefficient function that is\nallowed to vary with current finger/wrist position. The model is made par-\nsimonious and interpretable through a two-step variable selection procedure,\ncalled Sequential Adaptive Functional Empirical group LASSO (SAFE-gLASSO).\nNumerical studies show excel- lent selection and prediction properties of\nSAFE-gLASSO compared to popular alternatives. For our motivating dataset, the\nmethod correctly identifies the few EMG signals that are known to be important\nfor an able-bodied subject with negligible false positives and the model can be\ndirectly implemented in a robotic prosthetic. \n\n"}
{"id": "1805.03597", "contents": "Title: Using Machine Learning to Assess the Risk of and Prevent Water Main\n  Breaks Abstract: Water infrastructure in the United States is beginning to show its age,\nparticularly through water main breaks. Main breaks cause major disruptions in\neveryday life for residents and businesses. Water main failures in Syracuse,\nN.Y. (as in most cities) are handled reactively rather than proactively. A\nbarrier to proactive maintenance is the city's inability to predict the risk of\nfailure on parts of its infrastructure. In response, we worked with the city to\nbuild a ML system to assess the risk of a water mains breaking. Using\nhistorical data on which mains have failed, descriptors of pipes, and other\ndata sources, we evaluated several models' abilities to predict breaks three\nyears into the future. Our results show that our system using gradient boosted\ndecision trees performed the best out of several algorithms and expert\nheuristics, achieving precision at 1\\% (P@1) of 0.62. Our model outperforms a\nrandom baseline (P@1 of 0.08) and expert heuristics such as water main age (P@1\nof 0.10) and history of past main breaks (P@1 of 0.48). The model is deployed\nin the City of Syracuse. We are running a pilot by calculating the risk of\nfailure for each city block over the period 2016-2018 using data up to the end\nof 2015 and, as of the end of 2017, there have been 33 breaks on our riskiest\n52 mains. This has been a successful initiative for the city of Syracuse in\nimproving their infrastructure and we believe this approach can be applied to\nother cities. \n\n"}
{"id": "1805.03735", "contents": "Title: Sequence Aggregation Rules for Anomaly Detection in Computer Network\n  Traffic Abstract: We evaluate methods for applying unsupervised anomaly detection to\ncybersecurity applications on computer network traffic data, or flow. We borrow\nfrom the natural language processing literature and conceptualize flow as a\nsort of \"language\" spoken between machines. Five sequence aggregation rules are\nevaluated for their efficacy in flagging multiple attack types in a labeled\nflow dataset, CICIDS2017. For sequence modeling, we rely on long short-term\nmemory (LSTM) recurrent neural networks (RNN). Additionally, a simple\nfrequency-based model is described and its performance with respect to attack\ndetection is compared to the LSTM models. We conclude that the frequency-based\nmodel tends to perform as well as or better than the LSTM models for the tasks\nat hand, with a few notable exceptions. \n\n"}
{"id": "1805.04582", "contents": "Title: TensOrMachine: Probabilistic Boolean Tensor Decomposition Abstract: Boolean tensor decomposition approximates data of multi-way binary\nrelationships as product of interpretable low-rank binary factors, following\nthe rules of Boolean algebra. Here, we present its first probabilistic\ntreatment. We facilitate scalable sampling-based posterior inference by\nexploitation of the combinatorial structure of the factor conditionals. Maximum\na posteriori decompositions feature higher accuracies than existing techniques\nthroughout a wide range of simulated conditions. Moreover, the probabilistic\napproach facilitates the treatment of missing data and enables model selection\nwith much greater accuracy. We investigate three real-world data-sets. First,\ntemporal interaction networks in a hospital ward and behavioural data of\nuniversity students demonstrate the inference of instructive latent patterns.\nNext, we decompose a tensor with more than 10 billion data points, indicating\nrelations of gene expression in cancer patients. Not only does this demonstrate\nscalability, it also provides an entirely novel perspective on relational\nproperties of continuous data and, in the present example, on the molecular\nheterogeneity of cancer. Our implementation is available on GitHub:\nhttps://github.com/TammoR/LogicalFactorisationMachines. \n\n"}
{"id": "1805.05170", "contents": "Title: FastLORS: Joint Modeling for eQTL Mapping in R Abstract: Yang et al. (2013) introduced LORS, a method that jointly models the\nexpression of genes, SNPs, and hidden factors for eQTL mapping. LORS solves a\nconvex optimization problem and has guaranteed convergence. However, it can be\ncomputationally expensive for large datasets. In this paper we introduce\nFast-LORS which uses the proximal gradient method to solve the LORS problem\nwith significantly reduced computational burden. We apply Fast-LORS and LORS to\ndata from the third phase of the International HapMap Project and obtain\ncomparable results. Nevertheless, Fast-LORS shows substantial computational\nimprovement compared to LORS. \n\n"}
{"id": "1805.05617", "contents": "Title: Aggregating multiple types of complex data in stock market prediction: A\n  model-independent framework Abstract: The increasing richness in volume, and especially types of data in the\nfinancial domain provides unprecedented opportunities to understand the stock\nmarket more comprehensively and makes the price prediction more accurate than\nbefore. However, they also bring challenges to classic statistic approaches\nsince those models might be constrained to a certain type of data. Aiming at\naggregating differently sourced information and offering type-free capability\nto existing models, a framework for predicting stock market of scenarios with\nmixed data, including scalar data, compositional data (pie-like) and functional\ndata (curve-like), is established. The presented framework is\nmodel-independent, as it serves like an interface to multiple types of data and\ncan be combined with various prediction models. And it is proved to be\neffective through numerical simulations. Regarding to price prediction, we\nincorporate the trading volume (scalar data), intraday return series\n(functional data), and investors' emotions from social media (compositional\ndata) through the framework to competently forecast whether the market goes up\nor down at opening in the next day. The strong explanatory power of the\nframework is further demonstrated. Specifically, it is found that the intraday\nreturns impact the following opening prices differently between bearish market\nand bullish market. And it is not at the beginning of the bearish market but\nthe subsequent period in which the investors' \"fear\" comes to be indicative.\nThe framework would help extend existing prediction models easily to scenarios\nwith multiple types of data and shed light on a more systemic understanding of\nthe stock market. \n\n"}
{"id": "1805.06923", "contents": "Title: Functional Mediation Analysis with an Application to Functional Magnetic\n  Resonance Imaging Data Abstract: Causal mediation analysis is widely utilized to separate the causal effect of\ntreatment into its direct effect on the outcome and its indirect effect through\nan intermediate variable (the mediator). In this study we introduce a\nfunctional mediation analysis framework in which the three key variables, the\ntreatment, mediator, and outcome, are all continuous functions. With functional\nmeasures, causal assumptions and interpretations are not immediately\nwell-defined. Motivated by a functional magnetic resonance imaging (fMRI)\nstudy, we propose two functional mediation models based on the influence of the\nmediator: (1) a concurrent mediation model and (2) a historical mediation\nmodel. We further discuss causal assumptions, and elucidate causal\ninterpretations. Our proposed models enable the estimation of individual causal\neffect curves, where both the direct and indirect effects vary across time.\nApplied to a task-based fMRI study, we illustrate how our functional mediation\nframework provides a new perspective for studying dynamic brain connectivity.\nThe R package cfma is available on CRAN. \n\n"}
{"id": "1805.08323", "contents": "Title: A Class of Spatially Correlated Self-Exciting Models Abstract: The statistical modeling of multivariate count data observed on a space-time\nlattice has generally focused on using a hierarchical modeling approach where\nspace-time correlation structure is placed on a continuous, latent, process.\nThe count distribution is then assumed to be conditionally independent given\nthe latent process. However, in many real-world applications, especially in the\nmodeling of criminal or terrorism data, the conditional independence between\nthe count distributions is inappropriate. In this manuscript we propose a class\nof models that capture spatial variation and also account for the possibility\nof data model dependence. The resulting model allows both data model\ndependence, or self-excitation, as well as spatial dependence in a latent\nstructure. We demonstrate how second-order properties can be used to\ncharacterize the spatio-temporal process and how misspecificaiton of error may\ninflate self-excitation in a model. Finally, we give an algorithm for efficient\nBayesian inference for the model demonstrating its use in capturing the\nspatio-temporal structure of burglaries in Chicago from 2010-2015. \n\n"}
{"id": "1805.08463", "contents": "Title: Variational Learning on Aggregate Outputs with Gaussian Processes Abstract: While a typical supervised learning framework assumes that the inputs and the\noutputs are measured at the same levels of granularity, many applications,\nincluding global mapping of disease, only have access to outputs at a much\ncoarser level than that of the inputs. Aggregation of outputs makes\ngeneralization to new inputs much more difficult. We consider an approach to\nthis problem based on variational learning with a model of output aggregation\nand Gaussian processes, where aggregation leads to intractability of the\nstandard evidence lower bounds. We propose new bounds and tractable\napproximations, leading to improved prediction accuracy and scalability to\nlarge datasets, while explicitly taking uncertainty into account. We develop a\nframework which extends to several types of likelihoods, including the Poisson\nmodel for aggregated count data. We apply our framework to a challenging and\nimportant problem, the fine-scale spatial modelling of malaria incidence, with\nover 1 million observations. \n\n"}
{"id": "1805.08740", "contents": "Title: A change of perspective in network centrality Abstract: Typing Yesterday into the search-bar of your browser provides a long list of\nwebsites with, in top places, a link to a video by The Beatles. The order your\nbrowser shows its search results is a notable example of the use of network\ncentrality. Centrality is a measure of the importance of the nodes in a network\nand it plays a crucial role in a huge number of fields, ranging from sociology\nto engineering, and from biology to economics. Many metrics are available to\nevaluate centrality. However, centrality measures are generally based on ad hoc\nassumptions, and there is no commonly accepted way to compare the effectiveness\nand reliability of different metrics. Here we propose a new perspective where\ncentrality definition arises naturally from the most basic feature of a\nnetwork, its adjacency matrix. Following this perspective, different centrality\nmeasures naturally emerge, including the degree, eigenvector, and hub-authority\ncentrality. Within this theoretical framework, the accuracy of different\nmetrics can be compared. Tests on a large set of networks show that the\nstandard centrality metrics perform unsatisfactorily, highlighting intrinsic\nlimitations of these metrics for describing the centrality of nodes in complex\nnetworks. More informative multi-component centrality metrics are proposed as\nthe natural extension of standard metrics. \n\n"}
{"id": "1805.09038", "contents": "Title: Trans-Gaussian Kriging in a Bayesian framework : a case study Abstract: In the context of Gaussian Process Regression or Kriging, we propose a\nfull-Bayesian solution to deal with hyperparameters of the covariance function.\nThis solution can be extended to the Trans-Gaussian Kriging framework, which\nmakes it possible to deal with spatial data sets that violate assumptions\nrequired for Kriging. It is shown to be both elegant and efficient. We propose\nan application to computer experiments in the field of nuclear safety, where it\nis necessary to model non-destructive testing procedures based on eddy currents\nto detect possible wear in steam generator tubes. \n\n"}
{"id": "1805.10054", "contents": "Title: Stochastic algorithms with descent guarantees for ICA Abstract: Independent component analysis (ICA) is a widespread data exploration\ntechnique, where observed signals are modeled as linear mixtures of independent\ncomponents. From a machine learning point of view, it amounts to a matrix\nfactorization problem with a statistical independence criterion. Infomax is one\nof the most used ICA algorithms. It is based on a loss function which is a\nnon-convex log-likelihood. We develop a new majorization-minimization framework\nadapted to this loss function. We derive an online algorithm for the streaming\nsetting, and an incremental algorithm for the finite sum setting, with the\nfollowing benefits. First, unlike most algorithms found in the literature, the\nproposed methods do not rely on any critical hyper-parameter like a step size,\nnor do they require a line-search technique. Second, the algorithm for the\nfinite sum setting, although stochastic, guarantees a decrease of the loss\nfunction at each iteration. Experiments demonstrate progress on the\nstate-of-the-art for large scale datasets, without the necessity for any manual\nparameter tuning. \n\n"}
{"id": "1805.10097", "contents": "Title: Penalized polytomous ordinal logistic regression using cumulative\n  logits. Application to network inference of zero-inflated variables Abstract: We consider the problem of variable selection when the response is ordinal,\nthat is an ordered categorical variable. In particular, we are interested in\nselecting quantitative explanatory variables linked with the ordinal response\nvariable and we want to determine which predictors are relevant. In this\nframework, we choose to use the polytomous ordinal logistic regression model\nusing cumulative logits which generalizes the logistic regression. We then\nintroduce the Lasso estimation of the regression coefficients using the\nFrank-Wolfe algorithm. To deal with the choice of the penalty parameter, we use\nthe stability selection method and we develop a new method based on the\nknockoffs idea. This knockoffs method is general and suitable to any regression\nand besides, gives an order of importance of the covariates. Finally, we\nprovide some experimental results to corroborate our method. We then present an\napplication of this regression method for network inference of zero-inflated\nvariables and use it in practice on real abundance data in an agronomic\ncontext. \n\n"}
{"id": "1805.10214", "contents": "Title: Bias correction in daily maximum and minimum temperature measurements\n  through Gaussian process modeling Abstract: The Global Historical Climatology Network-Daily database contains, among\nother variables, daily maximum and minimum temperatures from weather stations\naround the globe. It is long known that climatological summary statistics based\non daily temperature minima and maxima will not be accurate, if the bias due to\nthe time at which the observations were collected is not accounted for. Despite\nsome previous work, to our knowledge, there does not exist a satisfactory\nsolution to this important problem. In this paper, we carefully detail the\nproblem and develop a novel approach to address it. Our idea is to impute the\nhourly temperatures at the location of the measurements by borrowing\ninformation from the nearby stations that record hourly temperatures, which\nthen can be used to create accurate summaries of temperature extremes. The key\ndifficulty is that these imputations of the temperature curves must satisfy the\nconstraint of falling between the observed daily minima and maxima, and\nattaining those values at least once in a twenty-four hour period. We develop a\nspatiotemporal Gaussian process model for imputing the hourly measurements from\nthe nearby stations, and then develop a novel and easy to implement Markov\nChain Monte Carlo technique to sample from the posterior distribution\nsatisfying the above constraints. We validate our imputation model using hourly\ntemperature data from four meteorological stations in Iowa, of which one is\nhidden and the data replaced with daily minima and maxima, and show that the\nimputed temperatures recover the hidden temperatures well. We also demonstrate\nthat our model can exploit information contained in the data to infer the time\nof daily measurements. \n\n"}
{"id": "1805.10244", "contents": "Title: Detecting Influence Campaigns in Social Networks Using the Ising Model Abstract: We consider the problem of identifying coordinated influence campaigns\nconducted by automated agents or bots in a social network. We study several\ndifferent Twitter datasets which contain such campaigns and find that the bots\nexhibit heterophily - they interact more with humans than with each other. We\nuse this observation to develop a probability model for the network structure\nand bot labels based on the Ising model from statistical physics. We present a\nmethod to find the maximum likelihood assignment of bot labels by solving a\nminimum cut problem. Our algorithm allows for the simultaneous detection of\nmultiple bots that are potentially engaging in a coordinated influence\ncampaign, in contrast to other methods that identify bots one at a time. We\nfind that our algorithm is able to more accurately find bots than existing\nmethods when compared to a human labeled ground truth. We also look at the\ncontent posted by the bots we identify and find that they seem to have a\ncoordinated agenda. \n\n"}
{"id": "1805.10933", "contents": "Title: Analysis of association football playing styles: an innovative method to\n  cluster networks Abstract: In this work we develop an innovative hierarchical clustering method to\ndivide a sample of undirected weighted networks into groups. The methodology\nconsists of two phases: the first phase is aimed at putting the single networks\nin a broader framework by including the characteristics of the population in\nthe data, while the second phase creates a subdivision of the sample on the\nbasis of the similarity between the community structures of the processed\nnetworks. Starting from the representation of the team's playing style as a\nnetwork, we apply the method to group the Italian Serie A teams' performances\nand consequently detect the main 15 tactics shown during the 2015-2016 season.\nThe information obtained is used to verify the effect of the styles of play on\nthe number of goals scored, and we prove the key role of one of them by\nimplementing an extension of the Dixon and Coles model (Dixon and Coles, 1997). \n\n"}
{"id": "1805.11126", "contents": "Title: Statistical Methods in Computed Tomography Image Estimation Abstract: Purpose: There is increasing interest in computed tomography (CT) image\nestimations from magnetic resonance (MR) images. The estimated CT images can be\nutilised for attenuation correction, patient positioning, and dose planning in\ndiagnostic and radiotherapy workflows. This study aims to introduce a novel\nstatistical learning approach for improving CT estimation from MR images and to\ncompare the performance of our method with the existing model based CT image\nestimation methods.\n  Methods: The statistical learning approach proposed here consists of two\nstages. At the training stage, prior knowledges about tissue-types from CT\nimages were used together with a Gaussian mixture model (GMM) to explore CT\nimage estimations from MR images. Since the prior knowledges are not available\nat the prediction stage, a classifier based on RUSBoost algorithm was trained\nto estimate the tissue-types from MR images. For a new patient, the trained\nclassifier and GMMs were used to predict CT image from MR images. The\nclassifier and GMMs were validated by using voxel level 10-fold\ncross-validation and patient-level leave-one-out cross-validation,\nrespectively.\n  Results: The proposed approach has outperformance in CT estimation quality in\ncomparison with the existing model based methods, especially on bone tissues.\nOur method improved CT image estimation by 5% and 23% on the whole brain and\nbone tissues, respectively.\n  Conclusions: Evaluation of our method shows that it is a promising method to\ngenerate CT image substitutes for the implementation of fully MR-based\nradiotherapy and PET/MRI applications. \n\n"}
{"id": "1805.11456", "contents": "Title: Elastic Functional Principal Component Regression Abstract: We study regression using functional predictors in situations where these\nfunctions contain both phase and amplitude variability. In other words, the\nfunctions are misaligned due to errors in time measurements, and these errors\ncan significantly degrade both model estimation and prediction performance. The\ncurrent techniques either ignore the phase variability, or handle it via\npre-processing, i.e., use an off-the-shelf technique for functional alignment\nand phase removal. We develop a functional principal component regression model\nwhich has comprehensive approach in handling phase and amplitude variability.\nThe model utilizes a mathematical representation of the data known as the\nsquare-root slope function. These functions preserve the $\\mathbf{L}^2$ norm\nunder warping and are ideally suited for simultaneous estimation of regression\nand warping parameters. Using both simulated and real-world data sets, we\ndemonstrate our approach and evaluate its prediction performance relative to\ncurrent models. In addition, we propose an extension to functional logistic and\nmultinomial logistic regression \n\n"}
{"id": "1805.11956", "contents": "Title: Short-term Load Forecasting with Deep Residual Networks Abstract: We present in this paper a model for forecasting short-term power loads based\non deep residual networks. The proposed model is able to integrate domain\nknowledge and researchers' understanding of the task by virtue of different\nneural network building blocks. Specifically, a modified deep residual network\nis formulated to improve the forecast results. Further, a two-stage ensemble\nstrategy is used to enhance the generalization capability of the proposed\nmodel. We also apply the proposed model to probabilistic load forecasting using\nMonte Carlo dropout. Three public datasets are used to prove the effectiveness\nof the proposed model. Multiple test cases and comparison with existing models\nshow that the proposed model is able to provide accurate load forecasting\nresults and has high generalization capability. \n\n"}
{"id": "1806.00225", "contents": "Title: Model-based clustering for populations of networks Abstract: Until recently obtaining data on populations of networks was typically rare.\nHowever, with the advancement of automatic monitoring devices and the growing\nsocial and scientific interest in networks, such data has become more widely\navailable. From sociological experiments involving cognitive social structures\nto fMRI scans revealing large-scale brain networks of groups of patients, there\nis a growing awareness that we urgently need tools to analyse populations of\nnetworks and particularly to model the variation between networks due to\ncovariates. We propose a model-based clustering method based on mixtures of\ngeneralized linear (mixed) models that can be employed to describe the joint\ndistribution of a populations of networks in a parsimonious manner and to\nidentify subpopulations of networks that share certain topological properties\nof interest (degree distribution, community structure, effect of covariates on\nthe presence of an edge, etc.). Maximum likelihood estimation for the proposed\nmodel can be efficiently carried out with an implementation of the EM\nalgorithm. We assess the performance of this method on simulated data and\nconclude with an example application on advice networks in a small business. \n\n"}
{"id": "1806.02078", "contents": "Title: Convolutional Sequence to Sequence Non-intrusive Load Monitoring Abstract: A convolutional sequence to sequence non-intrusive load monitoring model is\nproposed in this paper. Gated linear unit convolutional layers are used to\nextract information from the sequences of aggregate electricity consumption.\nResidual blocks are also introduced to refine the output of the neural network.\nThe partially overlapped output sequences of the network are averaged to\nproduce the final output of the model. We apply the proposed model to the REDD\ndataset and compare it with the convolutional sequence to point model in the\nliterature. Results show that the proposed model is able to give satisfactory\ndisaggregation performance for appliances with varied characteristics. \n\n"}
{"id": "1806.02228", "contents": "Title: Observing water level extremes in the Mekong River Basin: The benefit of\n  long-repeat orbit missions in a multi-mission satellite altimetry approach Abstract: Single-mission altimetric water level observations of rivers are spatially\nand temporally limited, and thus they are often unable to quantify the full\nextent of extreme flood events. Moreover, only missions with a short-repeat\norbit, such as Envisat, Jason-2, or SARAL, could provide meaningful time series\nof water level variations directly. However, long or non-repeat orbit missions\nsuch as CryoSat-2 have a very dense spatial resolution under the trade-off of a\nrepeat time insufficient for time series extraction. Combining data from\nmultiple altimeter missions into a multi-mission product allows for increasing\nthe spatial and temporal resolution of the data. In this study, we combined\nwater level data from CryoSat-2 with various observations from other altimeter\nmissions in the Mekong River Basin between 2008 and 2016 into one multi-mission\nwater level time series using the approach of universal kriging. In contrast to\nformer multi-mission altimetry methods, this approach allows for the\nincorporation of CryoSat-2 data as well as data from other long or non-repeat\norbit missions, such as Envisat-EM or SARAL-DP. Additionally, for the first\ntime, data from tributaries are incorporated. The multi-mission time series\nincluding CryoSat-2 data adequately reflects the general inter-annual flood\nbehaviour and the extreme floodings in 2008 and 2011. It performs better than\nsingle-mission time series or multi-mission time series based only on\nshort-repeat orbit data. The Probability of Detection of the floodings with the\nmulti-mission altimetry was around 80\\% while Envisat and Jason-2\nsingle-mission altimetry could only detect around 40% of the floodings\ncorrectly. However, small flash floods still remain undetectable. \n\n"}
{"id": "1806.02588", "contents": "Title: Designing Experiments to Measure Incrementality on Facebook Abstract: The importance of Facebook advertising has risen dramatically in recent\nyears, with the platform accounting for almost 20% of the global online ad\nspend in 2017. An important consideration in advertising is incrementality: how\nmuch of the change in an experimental metric is an advertising campaign\nresponsible for. To measure incrementality, Facebook provide lift studies. As\nFacebook lift studies differ from standard A/B tests, the online\nexperimentation literature does not describe how to calculate parameters such\nas power and minimum sample size. Facebook also offer multi-cell lift tests,\nwhich can be used to compare campaigns that don't have statistically identical\naudiences. In this case, there is no literature describing how to measure the\nsignificance of the difference in incrementality between cells, or how to\nestimate the power or minimum sample size. We fill these gaps in the literature\nby providing the statistical power and required sample size calculation for\nFacebook lift studies. We then generalise the statistical significance, power,\nand required sample size calculation to multi-cell lift studies. We represent\nour results theoretically in terms of the distributions of test metrics and in\npractical terms relating to the metrics used by practitioners, making all of\nour code publicly available. \n\n"}
{"id": "1806.03860", "contents": "Title: Air-Ground Integrated Vehicular Network Slicing with Content Pushing and\n  Caching Abstract: In this paper, an Air-Ground Integrated VEhicular Network (AGIVEN)\narchitecture is proposed, where the aerial High Altitude Platforms (HAPs)\nproactively push contents to vehicles through large-area broadcast while the\nground roadside units (RSUs) provide high-rate unicast services on demand. To\nefficiently manage the multi-dimensional heterogeneous resources, a\nservice-oriented network slicing approach is introduced, where the AGIVEN is\nvirtually divided into multiple slices and each slice supports a specific\napplication with guaranteed quality of service (QoS). Specifically, the\nfundamental problem of multi-resource provisioning in AGIVEN slicing is\ninvestigated, by taking into account typical vehicular applications of\nlocation-based map and popularity-based content services. For the\nlocation-based map service, the capability of HAP-vehicle proactive pushing is\nderived with respect to the HAP broadcast rate and vehicle cache size, wherein\na saddle point exists indicating the optimal communication-cache resource\ntrading. For the popular contents of common interests, the average on-board\ncontent hit ratio is obtained, with HAPs pushing newly generated contents to\nkeep on-board cache fresh. Then, the minimal RSU transmission rate is derived\nto meet the average delay requirements of each slice. The obtained analytical\nresults reveal the service-dependent resource provisioning and trading\nrelationships among RSU transmission rate, HAP broadcast rate, and vehicle\ncache size, which provides guidelines for multi-resource network slicing in\npractice. Simulation results demonstrate that the proposed AGIVEN network\nslicing approach matches the multi-resources across slices, whereby the RSU\ntransmission rate can be saved by 40% while maintaining the same QoS. \n\n"}
{"id": "1806.04193", "contents": "Title: Stochastic Geometric Coverage Analysis in mmWave Cellular Networks with\n  Realistic Channel and Antenna Radiation Models Abstract: Millimeter-wave (mmWave) bands will play an important role in 5G wireless\nsystems. The system performance can be assessed by using models from stochastic\ngeometry that cater for the directivity in the desired signal transmissions as\nwell as the interference, and by calculating the\nsignal-to-interference-plus-noise ratio (SINR) coverage. Nonetheless, the\ncorrectness of the existing coverage expressions derived through stochastic\ngeometry may be questioned, as it is not clear whether they capture the impact\nof the detailed mmWave channel and antenna features. In this study, we propose\nan SINR coverage analysis framework that includes realistic channel model (from\nNYU) and antenna element radiation patterns (with isotropic/directional\nradiation). We first introduce two parameters, aligned gain and misaligned\ngain, associated with the desired signal beam and the interfering signal beam,\nrespectively. We provide the distributions of the aligned and misaligned gains\nthrough curve fitting of system-simulation results. The distribution of these\ngains is used to determine the distribution of the SINR. We compare the\nobtained analytical SINR coverage with the corresponding SINR coverage\ncalculated via system-level simulations. The results show that both aligned and\nmisaligned gains can be modeled as exponential-logarithmically distributed\nrandom variables with the highest accuracy, and can further be approximated as\nexponentially distributed random variables with reasonable accuracy. These\napproximations are thus expected to be useful to evaluate the system\nperformance under ultra-reliable and low-latency communication (URLLC) and\nevolved mobile broadband (eMBB) scenarios, respectively. \n\n"}
{"id": "1806.05035", "contents": "Title: Development of probabilistic dam breach model using Bayesian inference Abstract: Dam breach models are commonly used to predict outflow hydrographs of\npotentially failing dams and are key ingredients for evaluating flood risk. In\nthis paper a new dam breach modeling framework is introduced that shall improve\nthe reliability of hydrograph predictions of homogeneous earthen embankment\ndams. Striving for a small number of parameters, the simplified physics-based\nmodel describes the processes of failing embankment dams by breach enlargement,\ndriven by progressive surface erosion. Therein the erosion rate of dam material\nis modeled by empirical sediment transport formulations. Embedding the model\ninto a Bayesian multilevel framework allows for quantitative analysis of\ndifferent categories of uncertainties. To this end, data available in\nliterature of observed peak discharge and final breach width of historical dam\nfailures was used to perform model inversion by applying Markov Chain Monte\nCarlo simulation. Prior knowledge is mainly based on non-informative\ndistribution functions. The resulting posterior distribution shows that the\nmain source of uncertainty is a correlated subset of parameters, consisting of\nthe residual error term and the epistemic term quantifying the breach erosion\nrate. The prediction intervals of peak discharge and final breach width are\ncongruent with values known from literature. To finally predict the outflow\nhydrograph for real case applications, an alternative residual model was\nformulated that assumes perfect data and a perfect model. The fully\nprobabilistic fashion of hydrograph prediction has the potential to improve the\nadequate risk management of downstream flooding. \n\n"}
{"id": "1806.05287", "contents": "Title: Asymptotic distribution of least square estimators for linear models\n  with dependent errors Abstract: In this paper, we consider the usual linear regression model in the case\nwhere the error process is assumed strictly stationary. We use a result from\nHannan (1973), who proved a Central Limit Theorem for the usual least square\nestimator under general conditions on the design and on the error process.\nWhatever the design satisfying Hannan's conditions, we define an estimator of\nthe covariance matrix and we prove its consistency under very mild conditions.\nAs an application, we show how to modify the usual tests on the linear model in\nthis dependent context, in such a way that the type-I error rate remains\nasymptotically correct, and we illustrate the performance of this procedure\nthrough different sets of simulations. \n\n"}
{"id": "1806.06125", "contents": "Title: Impact of Channel Models on the End-to-End Performance of mmWave\n  Cellular Networks Abstract: Communication at mmWave frequencies is one of the major innovations of the\nfifth generation of cellular networks, because of the potential multi-gigabit\ndata rate given by the large amounts of available bandwidth. The mmWave\nchannel, however, makes reliable communications particularly challenging, given\nthe harsh propagation environment and the sensitivity to blockage. Therefore,\nproper modeling of the mmWave channel is fundamental for accurate results in\nsystem simulations of mmWave cellular networks. Nonetheless, complex models,\nsuch as the 3GPP channel model for frequencies above 6 GHz, may introduce a\nsignificant overhead in terms of computational complexity. In this paper we\ninvestigate the trade offs related to the accuracy and the simplicity of the\nchannel model in end-to-end network simulations, and the impact on the\nperformance evaluation of transport protocols. \n\n"}
{"id": "1806.06551", "contents": "Title: A cautionary tale on using imputation methods for inference in matched\n  pairs design Abstract: Imputation procedures in biomedical fields have turned into statistical\npractice, since further analyses can be conducted ignoring the former presence\nof missing values. In particular, non-parametric imputation schemes like the\nrandom forest or a combination with the stochastic gradient boosting have shown\nfavorable imputation performance compared to the more traditionally used MICE\nprocedure. However, their effect on valid statistical inference has not been\nanalyzed so far. This paper closes this gap by investigating their validity for\ninferring mean differences in incompletely observed pairs while opposing them\nto a recent approach that only works with the given observations at hand. Our\nfindings indicate that machine learning schemes for (multiply) imputing missing\nvalues may inflate type-I-error or result in comparably low power in small to\nmoderate matched pairs, even after modifying the test statistics using Rubin's\nmultiple imputation rule. In addition to an extensive simulation study, an\nillustrative data example from a breast cancer gene study has been considered. \n\n"}
{"id": "1806.07274", "contents": "Title: Efficient data augmentation for multivariate probit models with panel\n  data: An application to general practitioner decision-making about\n  contraceptives Abstract: This article considers the problem of estimating a multivariate probit model\nin a panel data setting with emphasis on sampling a high-dimensional\ncorrelation matrix and improving the overall efficiency of the data\naugmentation approach. We reparameterise the correlation matrix in a principled\nway and then carry out efficient Bayesian inference using Hamiltonian Monte\nCarlo. We also propose a novel antithetic variable method to generate samples\nfrom the posterior distribution of the random effects and regression\ncoefficients, resulting in significant gains in efficiency. We apply the\nmethodology by analysing stated preference data obtained from Australian\ngeneral practitioners evaluating alternative contraceptive products. Our\nanalysis suggests that the joint probability of discussing combinations of\ncontraceptive products with a patient shows medical practice variation among\nthe general practitioners, which indicates some resistance to even discuss\nthese products, let alone recommend them. \n\n"}
{"id": "1806.08069", "contents": "Title: Deep Gaussian Process-Based Bayesian Inference for Contaminant Source\n  Localization Abstract: This paper proposes a Bayesian framework for localization of multiple sources\nin the event of accidental hazardous contaminant release. The framework\nassimilates sensor measurements of the contaminant concentration with an\nintegrated multizone computational fluid dynamics (multizone-CFD) based\ncontaminant fate and transport model. To ensure online tractability, the\nframework uses deep Gaussian process (DGP) based emulator of the multizone-CFD\nmodel. To effectively represent the transient response of the multizone-CFD\nmodel, the DGP emulator is reformulated using a matrix-variate Gaussian process\nprior. The resultant deep matrix-variate Gaussian process emulator (DMGPE) is\nused to define the likelihood of the Bayesian framework, while Markov Chain\nMonte Carlo approach is used to sample from the posterior distribution. The\nproposed method is evaluated for single and multiple contaminant sources\nlocalization tasks modeled by CONTAM simulator in a single-story building of 30\nzones, demonstrating that proposed approach accurately perform inference on\nlocations of contaminant sources. Moreover, the DMGP emulator outperforms both\nGP and DGP emulator with fewer number of hyperparameters. \n\n"}
{"id": "1806.09896", "contents": "Title: Bayesian Multi-study Factor Analysis for High-throughput Biological Data Abstract: This paper presents a new modeling strategy for joint unsupervised analysis\nof multiple high-throughput biological studies. As in Multi-study Factor\nAnalysis, our goals are to identify both common factors shared across studies\nand study-specific factors. Our approach is motivated by the growing body of\nhigh-throughput studies in biomedical research, as exemplified by the\ncomprehensive set of expression data on breast tumors considered in our case\nstudy. To handle high-dimensional studies, we extend Multi-study Factor\nAnalysis using a Bayesian approach that imposes sparsity. Specifically, we\ngeneralize the sparse Bayesian infinite factor model to multiple studies. We\nalso devise novel solutions for the identification of the loading matrices: we\nrecover the loading matrices of interest ex-post, by adapting the orthogonal\nProcrustes approach. Computationally, we propose an efficient and fast Gibbs\nsampling approach. Through an extensive simulation analysis, we show that the\nproposed approach performs very well in a range of different scenarios, and\noutperforms standard Factor analysis in all the scenarios identifying\nreplicable signal in unsupervised genomic applications. The results of our\nanalysis of breast cancer gene expression across seven studies identified\nreplicable gene patterns, clearly related to well-known breast cancer pathways.\nAn R package is implemented and available on GitHub. \n\n"}
{"id": "1806.10655", "contents": "Title: An Optimal Experimental Design Framework for Adaptive Inflation and\n  Covariance Localization for Ensemble Filters Abstract: We develop an optimal experimental design framework for adapting the\ncovariance inflation and localization in data assimilation problems. Covariance\ninflation and localization are ubiquitously employed to alleviate the effect of\nusing ensembles of finite sizes in all practical data assimilation systems. The\nchoice of both the inflation factor and the localization radius can have a\nsignificant impact on the performance of the assimilation scheme. These\nparameters are generally tuned by trial and error, rendering them expensive to\noptimize in practice. Spatially and temporally varying inflation parameter and\nlocalization radii have been recently proposed and have been empirically proven\nto enhance the performance of the employed assimilation filter. In this study,\nwe present a variational framework for adaptive tuning of the inflation and\nlocalization parameters. Each of these parameters is optimized independently,\nwith an objective to minimize the uncertainty in the posterior state. The\nproposed framework does not assume uncorrelated observations or prior errors\nand can in principle be applied without expert knowledge about the model and\nthe observations. Thus, it is adequate for handling dense as well as sparse\nobservational networks. We present the mathematical formulation, algorithmic\ndescription of the approach, and numerical experiments using the two-layer\nLorenz-96 model. \n\n"}
{"id": "1806.10698", "contents": "Title: A comparative study of artificial intelligence and human doctors for the\n  purpose of triage and diagnosis Abstract: Online symptom checkers have significant potential to improve patient care,\nhowever their reliability and accuracy remain variable. We hypothesised that an\nartificial intelligence (AI) powered triage and diagnostic system would compare\nfavourably with human doctors with respect to triage and diagnostic accuracy.\nWe performed a prospective validation study of the accuracy and safety of an AI\npowered triage and diagnostic system. Identical cases were evaluated by both an\nAI system and human doctors. Differential diagnoses and triage outcomes were\nevaluated by an independent judge, who was blinded from knowing the source (AI\nsystem or human doctor) of the outcomes. Independently of these cases,\nvignettes from publicly available resources were also assessed to provide a\nbenchmark to previous studies and the diagnostic component of the MRCGP exam.\nOverall we found that the Babylon AI powered Triage and Diagnostic System was\nable to identify the condition modelled by a clinical vignette with accuracy\ncomparable to human doctors (in terms of precision and recall). In addition, we\nfound that the triage advice recommended by the AI System was, on average,\nsafer than that of human doctors, when compared to the ranges of acceptable\ntriage provided by independent expert judges, with only a minimal reduction in\nappropriateness. \n\n"}
{"id": "1806.10749", "contents": "Title: On Adaptive Linear-Quadratic Regulators Abstract: Performance of adaptive control policies is assessed through the regret with\nrespect to the optimal regulator, which reflects the increase in the operating\ncost due to uncertainty about the dynamics parameters. However, available\nresults in the literature do not provide a quantitative characterization of the\neffect of the unknown parameters on the regret. Further, there are problems\nregarding the efficient implementation of some of the existing adaptive\npolicies. Finally, results regarding the accuracy with which the system's\nparameters are identified are scarce and rather incomplete.\n  This study aims to comprehensively address these three issues. First, by\nintroducing a novel decomposition of adaptive policies, we establish a sharp\nexpression for the regret of an arbitrary policy in terms of the deviations\nfrom the optimal regulator. Second, we show that adaptive policies based on\nslight modifications of the Certainty Equivalence scheme are efficient.\nSpecifically, we establish a regret of (nearly) square-root rate for two\nfamilies of randomized adaptive policies. The presented regret bounds are\nobtained by using anti-concentration results on the random matrices employed\nfor randomizing the estimates of the unknown parameters. Moreover, we study the\nminimal additional information on dynamics matrices that using them the regret\nwill become of logarithmic order. Finally, the rates at which the unknown\nparameters of the system are being identified are presented. \n\n"}
{"id": "1807.00647", "contents": "Title: Quantum entanglement shared in hydrogen bonds and its usage as a\n  resource in molecular recognition Abstract: Quantum tunneling events occurring through biochemical bonds are capable to\ngenerate quantum correlations between bonded systems, which in turn makes the\nconventional second law of thermodynamics approach insufficient to investigate\nthese systems. This means that the utilization of these correlations in their\nbiological functions could give an evolutionary advantage to biomolecules to an\nextent beyond the predictions of molecular biology that are generally based on\nthe second law in its standard form. To explore this possibility, we first\ncompare the tunneling assisted quantum entanglement shared in the ground states\nof covalent and hydrogen bonds. Only the latter appears to be useful from a\nquantum information point of view. Also, significant amounts of quantum\nentanglement can be found in the thermal state of hydrogen bond. Then, we focus\non an illustrative example of ligand binding in which a receptor protein or an\nenzyme is restricted to recognize its ligands using the same set of\nproton-acceptors and donors residing on its binding site. In particular, we\nshow that such a biomolecule can discriminate between $3^n - 1$ agonist ligands\nif it uses the entanglement shared in $n$ intermolecular hydrogen bonds as a\nresource in molecular recognition. Finally, we consider the molecular\nrecognition events encountered in both the contemporary genetic machinery and\nits hypothetical primordial ancestor in pre-DNA world, and discuss whether\nthere may have been a place for the utilization of quantum entanglement in the\nevolutionary history of this system. \n\n"}
{"id": "1807.01902", "contents": "Title: A Bayesian model for lithology/fluid class prediction using a Markov\n  mesh prior fitted from a training image Abstract: We consider a Bayesian model for inversion of observed amplitude variation\nwith offset (AVO) data into lithology/fluid classes, and study in particular\nhow the choice of prior distribution for the lithology/fluid classes influences\nthe inversion results. Two distinct prior distributions are considered, a\nsimple manually specified Markov random field prior with a first order\nneighborhood and a Markov mesh model with a much larger neighborhood estimated\nfrom a training image. They are chosen to model both horisontal connectivity\nand vertical thickness distribution of the lithology/fluid classes, and are\ncompared on an offshore clastic oil reservoir in the North Sea. We combine both\npriors with the same linearised Gaussian likelihood function based on a\nconvolved linearised Zoeppritz relation and estimate properties of the\nresulting two posterior distributions by simulating from these distributions\nwith the Metropolis-Hastings algorithm.\n  The influence of the prior on the marginal posterior probabilities for the\nlithology/fluid classes is clearly observable, but modest. The importance of\nthe prior on the connectivity properties in the posterior realisations,\nhowever, is much stronger. The larger neighborhood of the Markov mesh prior\nenables it to identify and model connectivity and curvature much better than\nwhat can be done by the first order neighborhood Markov random field prior. As\na result, we conclude that the posterior realisations based on the Markov mesh\nprior appear with much higher lateral connectivity, which is geologically\nplausible. \n\n"}
{"id": "1807.02348", "contents": "Title: Data-driven causal path discovery without prior knowledge - a benchmark\n  study Abstract: Causal discovery broadens the inference possibilities, as correlation does\nnot inform about the relationship direction. The common approaches were\nproposed for cases in which prior knowledge is desired, when the impact of a\ntreatment/intervention variable is discovered or to analyze time-related\ndependencies. In some practical applications, more universal techniques are\nneeded and have already been presented. Therefore, the aim of the study was to\nassess the accuracies in determining causal paths in a dataset without\nconsidering the ground truth and the contextual information. This benchmark was\nperformed on the database with cause-effect pairs, using a framework consisting\nof generalized correlations (GC), kernel regression gradients (GR) and absolute\nresiduals criteria (AR), along with causal additive modeling (CAM). The best\noverall accuracy, 80%, was achieved for the (majority voting) combination of\nGC, AR, and CAM, however, the most similar sensitivity and specificity values\nwere obtained for AR. Bootstrap simulation established the probability of\ncorrect causal path determination (which pairs should remain indeterminate).\nThe mean accuracy was then improved to 83% for the selected subset of pairs.\nThe described approach can be used for preliminary dependence assessment, as an\ninitial step for commonly used causality assessment frameworks or for\ncomparison with prior assumptions. \n\n"}
{"id": "1807.05466", "contents": "Title: Non-separable Nearest-Neighbor Gaussian Process Model for Antarctic\n  Surface Mass Balance and Ice Core Site Selection Abstract: Surface mass balance (SMB) is an important factor in the estimation of sea\nlevel change, and data are collected to estimate models for prediction of SMB\nover the Antarctic ice sheets. Using a quality-controlled aggregate dataset of\nSMB field measurements with significantly more observations than previous\nanalyses, a fully Bayesian nearest-neighbor Gaussian process model is posed to\nestimate Antarctic SMB and propose new field measurement locations. A\ncorresponding Antarctic SMB map is rendered using this model and is compared\nwith previous estimates. A prediction uncertainty map is created to identify\nregions of high SMB uncertainty. The model estimates net SMB to be 2345 Gton\n$\\text{yr}^{-1}$, with 95% credible interval (2273,2413) Gton $\\text{yr}^{-1}$.\nOverall, these results suggest lower Antarctic SMB than previously reported.\nUsing the model's uncertainty quantification, we propose 25 new measurement\nsites for field study utilizing a design to minimize integrated mean squared\nerror. \n\n"}
{"id": "1807.05834", "contents": "Title: A Statistical Approach to Inferring Business Locations Based on Purchase\n  Behavior Abstract: Transaction data obtained by Personal Financial Management (PFM) services\nfrom financial institutes such as banks and credit card companies contain a\ndescription string from which the merchant, and an encoded store identifier may\nbe parsed. However, the physical location of the purchase is absent from this\ndescription. In this paper we present a method designed to recover this\nvaluable spatial information and map merchant and identifier tuples to physical\nmap locations. We begin by constructing a graph of customer sharing between\nbusinesses, and based on a small set of known \"seed\" locations we formulate\nthis task as a maximum likelihood problem based on a model of customer sharing\nbetween nearby businesses. We test our method extensively on real world data\nand provide statistics on the displacement error in many cities. \n\n"}
{"id": "1807.08087", "contents": "Title: Capacity Analysis for Full Duplex Self-backhauled Small Cells Abstract: Full duplex (FD) communication enables simultaneous transmission and\nreception on the same frequency band. Though it has the potential of doubling\nthe throughput on isolated links, in reality, higher interference and\nasymmetric traffic demands in the uplink and downlink could significantly\nreduce the gains of FD operations. In this paper, we consider the application\nof FD operation in self-backhauled small cells, where multiple FD capable small\ncell base stations (SBS) are wirelessly backhauled by a FD capable macro-cell\nBS (MBS). To increase the capacity of the backhaul link, the MBS is equipped\nwith multiple antennas to enable space division multiple access (SDMA). A\nscheduling method using back-pressure algorithm and geometric programming is\nproposed for link selection and interference mitigation. Simulation results\nshow that with FD SDMA backhaul links, the proposed scheduler almost doubles\nthroughput under asymmetric traffic demand and various network conditions. \n\n"}
{"id": "1807.08926", "contents": "Title: A decision theoretic approach to model evaluation in computational drug\n  discovery Abstract: Artificial intelligence, trained via machine learning or computational\nstatistics algorithms, holds much promise for the improvement of small molecule\ndrug discovery. However, structure-activity data are high dimensional with low\nsignal-to-noise ratios and proper validation of predictive methods is\ndifficult. It is poorly understood which, if any, of the currently available\nmachine learning algorithms will best predict new candidate drugs. 25 publicly\navailable molecular datasets were extracted from ChEMBL. Neural nets, random\nforests, support vector machines (regression) and ridge regression were then\nfitted to the structure-activity data. A new validation method, based on\nquantile splits on the activity distribution function, is proposed for the\nconstruction of training and testing sets. Model validation based on random\npartitioning of available data favours models which overfit and `memorize' the\ntraining set, namely random forests and deep neural nets. Partitioning based on\nquantiles of the activity distribution correctly penalizes models which can\nextrapolate onto structurally different molecules outside of the training data.\nThis approach favours more constrained models, namely ridge regression and\nsupport vector regression. In addition, our new rank-based loss functions give\nconsiderably different results from mean squared error highlighting the\nnecessity to define model optimality with respect to the decision task at hand.\nModel performance should be evaluated from a decision theoretic perspective\nwith subjective loss functions. Data-splitting based on the separation of high\nand low activity data provides a robust methodology for determining the best\nextrapolating model. Simpler, traditional statistical methods such as ridge\nregression outperform state-of-the-art machine learning methods in this\nsetting. \n\n"}
{"id": "1807.10021", "contents": "Title: Judging the Judges: Evaluating the Performance of International\n  Gymnastics Judges Abstract: Judging a gymnastics routine is a noisy process, and the performance of\njudges varies widely. In collaboration with the F\\'ed\\'eration Internationale\nde Gymnastique (FIG) and Longines, we are designing and implementing an\nimproved statistical engine to analyze the performance of gymnastics judges\nduring and after major competitions like the Olympic Games and the World\nChampionships. The engine, called the Judge Evaluation Program (JEP), has three\nobjectives: (1) provide constructive feedback to judges, executive committees\nand national federations; (2) assign the best judges to the most important\ncompetitions; (3) detect bias and outright cheating.\n  Using data from international gymnastics competitions held during the\n2013-2016 Olympic cycle, we first develop a marking score evaluating the\naccuracy of the marks given by gymnastics judges. Judging a gymnastics routine\nis a random process, and we can model this process very accurately using\nheteroscedastic random variables. The marking score scales the difference\nbetween the mark of a judge and the theoretical performance of a gymnast as a\nfunction of the standard deviation of the judging error estimated from data for\neach apparatus. This dependence between judging variability and performance\nquality has never been properly studied. We then study ranking scores assessing\nto what extent judges rate gymnasts in the correct order, and explain why we\nultimately chose not to implement them. We also study outlier detection to\npinpoint gymnasts who were poorly evaluated by judges. Finally, we discuss\ninteresting observations and discoveries that led to recommendations and rule\nchanges at the FIG. \n\n"}
{"id": "1807.10055", "contents": "Title: Judging the Judges: A General Framework for Evaluating the Performance\n  of International Sports Judges Abstract: The monitoring of judges and referees in sports has become an important topic\ndue to the increasing media exposure of international sporting events and the\nlarge monetary sums involved. In this article, we present a method to assess\nthe accuracy of sports judges and estimate their bias. Our method is broadly\napplicable to all sports where panels of judges evaluate athletic performances\non a finite scale. We analyze judging scores from eight different sports with\ncomparable judging systems: diving, dressage, figure skating, freestyle skiing\n(aerials), freestyle snowboard (halfpipe, slopestyle), gymnastics, ski jumping\nand synchronized swimming. With the notable exception of dressage, we identify,\nfor each aforementioned sport, a general and accurate pattern of the intrinsic\njudging error as a function of the performance level of the athlete. This\nintrinsic judging inaccuracy is heteroscedastic and can be approximated by a\nquadratic curve, indicating increased consensus among judges towards the best\nathletes. Using this observation, the framework developed to assess the\nperformance of international gymnastics judges is applicable to all these\nsports: we can evaluate the performance of judges compared to their peers and\ndistinguish cheating from unintentional misjudging. Our analysis also leads to\nvaluable insights about the judging practices of the sports under\nconsideration. In particular, it reveals a systemic judging problem in\ndressage, where judges disagree on what constitutes a good performance. \n\n"}
{"id": "1807.10840", "contents": "Title: Nonparametric estimation of utility functions Abstract: Inferring a decision maker's utility function typically involves an\nelicitation phase where the decision maker responds to a series of elicitation\nqueries, followed by an estimation phase where the state-of-the-art is to\neither fit the response data to a parametric form (such as the exponential or\npower function) or perform linear interpolation. We introduce a Bayesian\nnonparametric method involving Gaussian stochastic processes for estimating a\nutility function. Advantages include the flexibility to fit a large class of\nfunctions, favorable theoretical properties, and a fully probabilistic view of\nthe decision maker's preference properties including risk attitude. Using\nextensive simulation experiments as well as two real datasets from the\nliterature, we demonstrate that the proposed approach yields estimates with\nlower mean squared errors. While our focus is primarily on single-attribute\nutility functions, one of the real datasets involves three attributes; the\nresults indicate that nonparametric methods also seem promising for\nmulti-attribute utility function estimation. \n\n"}
{"id": "1807.10869", "contents": "Title: Residual Balancing: A Method of Constructing Weights for Marginal\n  Structural Models Abstract: When making causal inferences, post-treatment confounders complicate analyses\nof time-varying treatment effects. Conditioning on these variables naively to\nestimate marginal effects may inappropriately block causal pathways and may\ninduce spurious associations between treatment and the outcome, leading to\nbias. To avoid such bias, researchers often use marginal structural models\n(MSMs) with inverse probability weighting (IPW). However, IPW requires models\nfor the conditional distributions of treatment and is highly sensitive to their\nmisspecification. Moreover, IPW is relatively inefficient, susceptible to\nfinite-sample bias, and difficult to use with continuous treatments. We\nintroduce an alternative method of constructing weights for MSMs, which we call\n\"residual balancing.\" In contrast to IPW, it requires modeling the conditional\nmeans of the post-treatment confounders rather than the conditional\ndistributions of treatment, and it is therefore easier to use with continuous\nexposures. Numeric simulations suggest that residual balancing is both more\nefficient and more robust to model misspecification than IPW and its variants.\nWe illustrate the method by estimating (a) the cumulative effect of negative\nadvertising on election outcomes and (b) the controlled direct effect of shared\ndemocracy on public support for war. Open source software is available for\nimplementing the proposed method. \n\n"}
{"id": "1807.11314", "contents": "Title: Robust Calibration of Radio Interferometers in Multi-Frequency Scenario Abstract: This paper investigates calibration of sensor arrays in the radio astronomy\ncontext. Current and future radio telescopes require computationally efficient\nalgorithms to overcome the new technical challenges as large collecting area,\nwide field of view and huge data volume. Specifically, we study the calibration\nof radio interferometry stations with significant direction dependent\ndistortions. We propose an iterative robust calibration algorithm based on a\nrelaxed maximum likelihood estimator for a specific context: i) observations\nare affected by the presence of outliers and ii) parameters of interest have a\nspecific structure depending on frequency. Variation of parameters across\nfrequency is addressed through a distributed procedure, which is consistent\nwith the new radio synthesis arrays where the full observing bandwidth is\ndivided into multiple frequency channels. Numerical simulations reveal that the\nproposed robust distributed calibration estimator outperforms the conventional\nnon-robust algorithm and/or the mono-frequency case. \n\n"}
{"id": "1808.02214", "contents": "Title: Allocations of Cold Standbys to Series and Parallel Systems with\n  Dependent Components Abstract: In the context of industrial engineering, cold-standby redundancies\nallocation strategy is usually adopted to improve the reliability of coherent\nsystems. This paper investigates optimal allocation strategies of cold standbys\nfor series and parallel systems comprised of dependent components with\nleft/right tail weakly stochastic arrangement increasing lifetimes. For the\ncase of heterogeneous and independent matched cold standbys, it is proved that\nbetter redundancies should be put in the nodes having weaker [better]\ncomponents for series [parallel] systems. For the case of homogeneous and\nindependent cold standbys, it is shown that more redundancies should be put in\nstandby with weaker [better] components to enhance the reliability of series\n[parallel] systems. The results developed here generalize and extend those\ncorresponding ones in the literature to the case of series and parallel systems\nwith dependent components. Numerical examples are also presented to provide\nguidance for the practical use of our theoretical findings. \n\n"}
{"id": "1808.03273", "contents": "Title: Random forest prediction of Alzheimer's disease using pairwise selection\n  from time series data Abstract: Time-dependent data collected in studies of Alzheimer's disease usually has\nmissing and irregularly sampled data points. For this reason time series\nmethods which assume regular sampling cannot be applied directly to the data\nwithout a pre-processing step. In this paper we use a machine learning method\nto learn the relationship between pairs of data points at different time\nseparations. The input vector comprises a summary of the time series history\nand includes both demographic and non-time varying variables such as genetic\ndata. The dataset used is from the 2017 TADPOLE grand challenge which aims to\npredict the onset of Alzheimer's disease using including demographic, physical\nand cognitive data. The challenge is a three-fold diagnosis classification into\nAD, MCI and control groups, the prediction of ADAS-13 score and the normalised\nventricle volume. While the competition proceeds, forecasting methods may be\ncompared using a leaderboard dataset selected from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) and with standard metrics for measuring\naccuracy. For diagnosis, we find an mAUC of 0.82, and a classification accuracy\nof 0.73. The results show that the method is effective and comparable with\nother methods. \n\n"}
{"id": "1808.04312", "contents": "Title: Analysing Multiple Epidemic Data Sources Abstract: Evidence-based knowledge of infectious disease burden, including prevalence,\nincidence, severity and transmission, in different population strata and\nlocations, and possibly in real time, is crucial to the planning and evaluation\nof public health policies. Direct observation of a disease process is rarely\npossible. However, latent characteristics of an epidemic and its evolution can\noften be inferred from the synthesis of indirect information from various\nroutine data sources, as well as expert opinion. The simultaneous synthesis of\nmultiple data sources, often conveniently carried out in a Bayesian framework,\nposes a number of statistical and computational challenges: the heterogeneity\nin type, relevance and granularity of the data, together with selection and\ninformative observation biases, lead to complex probabilistic models that are\ndifficult to build and fit, and challenging to criticize. Using motivating case\nstudies of influenza, this chapter illustrates the cycle of model development\nand criticism in the context of Bayesian evidence synthesis, highlighting the\nchallenges of complex model building, computationally efficient inference, and\nconflicting evidence. \n\n"}
{"id": "1808.04360", "contents": "Title: Stochastic on-time arrival problem in transit networks Abstract: This article considers the stochastic on-time arrival problem in transit\nnetworks where both the travel time and the waiting time for transit services\nare stochastic. A specific challenge of this problem is the combinatorial\nsolution space due to the unknown ordering of transit line arrivals. We propose\na network structure appropriate to the online decision-making of a passenger,\nincluding boarding, waiting and transferring. In this framework, we design a\ndynamic programming algorithm that is pseudo-polynomial in the number of\ntransit stations and travel time budget, and exponential in the number of\ntransit lines at a station, which is a small number in practice. To reduce the\nsearch space, we propose a definition of transit line dominance, and techniques\nto identify dominance, which decrease the computation time by up to 90% in\nnumerical experiments. Extensive numerical experiments are conducted on both a\nsynthetic network and the Chicago transit network. \n\n"}
{"id": "1808.04698", "contents": "Title: Probabilistic forecasting of heterogeneous consumer transaction-sales\n  time series Abstract: We present new Bayesian methodology for consumer sales forecasting. With a\nfocus on multi-step ahead forecasting of daily sales of many supermarket items,\nwe adapt dynamic count mixture models to forecast individual customer\ntransactions, and introduce novel dynamic binary cascade models for predicting\ncounts of items per transaction. These transactions-sales models can\nincorporate time-varying trend, seasonal, price, promotion, random effects and\nother outlet-specific predictors for individual items. Sequential Bayesian\nanalysis involves fast, parallel filtering on sets of decoupled items and is\nadaptable across items that may exhibit widely varying characteristics. A\nmulti-scale approach enables information sharing across items with related\npatterns over time to improve prediction while maintaining scalability to many\nitems. A motivating case study in many-item, multi-period, multi-step ahead\nsupermarket sales forecasting provides examples that demonstrate improved\nforecast accuracy in multiple metrics, and illustrates the benefits of full\nprobabilistic models for forecast accuracy evaluation and comparison.\n  Keywords: Bayesian forecasting; decouple/recouple; dynamic binary cascade;\nforecast calibration; intermittent demand; multi-scale forecasting; predicting\nrare events; sales per transaction; supermarket sales forecasting \n\n"}
{"id": "1808.04753", "contents": "Title: Estimating the size of a hidden finite set: large-sample behavior of\n  estimators Abstract: A finite set is \"hidden\" if its elements are not directly enumerable or if\nits size cannot be ascertained via a deterministic query. In public health,\nepidemiology, demography, ecology and intelligence analysis, researchers have\ndeveloped a wide variety of indirect statistical approaches, under different\nmodels for sampling and observation, for estimating the size of a hidden set.\nSome methods make use of random sampling with known or estimable sampling\nprobabilities, and others make structural assumptions about relationships (e.g.\nordering or network information) between the elements that comprise the hidden\nset. In this review, we describe models and methods for learning about the size\nof a hidden finite set, with special attention to asymptotic properties of\nestimators. We study the properties of these methods under two asymptotic\nregimes, \"infill\" in which the number of fixed-size samples increases, but the\npopulation size remains constant, and \"outfill\" in which the sample size and\npopulation size grow together. Statistical properties under these two regimes\ncan be dramatically different. \n\n"}
{"id": "1808.04905", "contents": "Title: Multi-Sector and Multi-Panel Performance in 5G mmWave Cellular Networks Abstract: The next generation of cellular networks (5G) will exploit the mmWave\nspectrum to increase the available capacity. Communication at such high\nfrequencies, however, suffers from high path loss and blockage, therefore\ndirectional transmissions using antenna arrays and dense deployments are\nneeded. Thus, when evaluating the performance of mmWave mobile networks, it is\nnecessary to accurately model the complex channel, the directionality of the\ntransmission, but also the interplay that these elements can have with the\nwhole protocol stack, both in the radio access and in the higher layers. In\nthis paper, we improve the channel model abstraction of the mmWave module for\nns-3, by introducing the support of a more realistic antenna array model,\ncompliant with 3GPP NR requirements, and of multiple antenna arrays at the base\nstations and mobile handsets. We then study the end-to-end performance of a\nmmWave cellular network by varying the channel and antenna array\nconfigurations, and show that increasing the number of antenna arrays and,\nconsequently, the number of sectors is beneficial for both throughput and\nlatency. \n\n"}
{"id": "1808.05865", "contents": "Title: Using path signatures to predict a diagnosis of Alzheimer's disease Abstract: The path signature is a means of feature generation that can encode nonlinear\ninteractions in the data as well as the usual linear features. It can\ndistinguish the ordering of time-sequenced changes: for example whether or not\nthe hippocampus shrinks fast, then slowly or the converse. It provides\ninterpretable features and its output is a fixed length vector irrespective of\nthe number of input points so it can encode longitudinal data of varying length\nand with missing data points. In this paper we demonstrate the path signature\nin providing features to distinguish a set of people with Alzheimer's disease\nfrom a matched set of healthy individuals. The data used are volume\nmeasurements of the whole brain, ventricles and hippocampus from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI). The path signature method\nis shown to be a useful tool for the processing of sequential data which is\nbecoming increasingly available as monitoring technologies are applied. \n\n"}
{"id": "1808.05977", "contents": "Title: Revisiting the proton-radius problem using constrained Gaussian\n  processes Abstract: Background: The \"proton radius puzzle\" refers to an eight-year old problem\nthat highlights major inconsistencies in the extraction of the charge radius of\nthe proton from muonic Lamb-shift experiments as compared against experiments\nusing elastic electron scattering. For the latter, the determination of the\ncharge radius involves an extrapolation of the experimental form factor to zero\nmomentum transfer.\n  Purpose: To estimate the proton radius by introducing a novel non-parametric\napproach to model the electric form factor of the proton.\n  Methods: Within a Bayesian paradigm, we develop a model flexible enough to\nfit the data without any parametric assumptions on the form factor. The\nBayesian estimation is guided by imposing only two physical constraints on the\nform factor: (a) its value at zero momentum transfer (normalization) and (b)\nits overall shape, assumed to be a monotonically decreasing function of the\nmomentum transfer. Variants of these assumptions are explored to assess the\nimpact of these constraints.\n  Results: So far our results are inconclusive in regard to the proton puzzle,\nas they depend on both, the assumed constrains and the range of experimental\ndata used. For example, if only low momentum-transfer data is used, adopting\nonly the normalization constraint provides a value compatible with the smaller\nmuonic result, while imposing only the shape constraint favors the larger\nelectronic value.\n  Conclusions: We have presented a novel technique to estimate the proton\nradius from electron scattering data based on a non-parametric Gaussian\nprocess. We have shown the impact of the physical constraints imposed on the\nform factor and of the range of experimental data used. In this regard, we are\nhopeful that as this technique is refined and with the anticipated new results\nfrom the PRad experiment, we will get closer to resolve of the puzzle. \n\n"}
{"id": "1808.08199", "contents": "Title: Applications of the Fractional-Random-Weight Bootstrap Abstract: The bootstrap, based on resampling, has, for several decades, been a widely\nused method for computing confidence intervals for applications where no exact\nmethod is available and when sample sizes are not large enough to be able to\nrely on easy-to-compute large-sample approximate methods, such a Wald\n(normal-approximation) confidence intervals. Simulation based bootstrap\nintervals have been proven useful in that their actual coverage probabilities\nare close to the nominal confidence level in small samples. Small samples\nanalytical approximations such as the Wald method, however, tend to have\ncoverage probabilities that greatly exceed the nominal confidence level. There\nare, however, many applications where the resampling bootstrap method cannot be\nused. These include situations where the data are heavily censored, logistic\nregression when the success response is a rare event or where there is\ninsufficient mixing of successes and failures across the explanatory\nvariable(s), and designed experiments where the number of parameters is close\nto the number of observations. The thing that these three situations have in\ncommon is that there may be a substantial proportion of the resamples where is\nnot possible to estimate all of the parameters in the model. This paper reviews\nthe fractional-random-weight bootstrap method and demonstrates how it can be\nused to avoid these problems and construct confidence intervals. For the\nexamples, it is seen that the fractional-random-weight bootstrap method is easy\nto use and has advantages over the resampling method in many challenging\napplications. \n\n"}
{"id": "1808.10173", "contents": "Title: An Introduction to Inductive Statistical Inference: from Parameter\n  Estimation to Decision-Making Abstract: These lecture notes aim at a post-Bachelor audience with a background at an\nintroductory level in Applied Mathematics and Applied Statistics. They discuss\nthe logic and methodology of the Bayes-Laplace approach to inductive\nstatistical inference that places common sense and the guiding lines of the\nscientific method at the heart of systematic analyses of quantitative-empirical\ndata. Following an exposition of exactly solvable cases of single- and\ntwo-parameter estimation problems, the main focus is laid on Markov Chain Monte\nCarlo (MCMC) simulations on the basis of Hamiltonian Monte Carlo sampling of\nposterior joint probability distributions for regression parameters occurring\nin generalised linear models for a univariate outcome variable. The modelling\nof fixed effects as well as of correlated varying effects via multi-level\nmodels in non-centred parametrisation is considered. The simulation of\nposterior predictive distributions is outlined. The assessment of a model's\nrelative out-of-sample posterior predictive accuracy with information\nentropy-based criteria WAIC and LOOIC and model comparison with Bayes factors\nare addressed. A brief discussion on the description of the generation of\nstationary time series data by means of autoregressive models is contained.\nConcluding, a conceptual link to the behavioural subjective expected utility\nrepresentation of a single decision-maker's choice behaviour in static one-shot\ndecision problems is established. Vectorised codes for MCMC simulations of\nmulti-dimensional posterior joint probability distributions with the Stan\nprobabilistic programming language implemented in the statistical software R\nare provided. The lecture notes are fully hyperlinked. They direct the reader\nto original scientific research papers, online resources on inductive\nstatistical inference, and to pertinent biographical information. \n\n"}
{"id": "1808.10563", "contents": "Title: Penalized Component Hub Models Abstract: Social network analysis presupposes that observed social behavior is\ninfluenced by an unobserved network. Traditional approaches to inferring the\nlatent network use pairwise descriptive statistics that rely on a variety of\nmeasures of co-occurrence. While these techniques have proven useful in a wide\nrange of applications, the literature does not describe the generating\nmechanism of the observed data from the network.\n  In a previous article, the authors presented a technique which used a finite\nmixture model as the connection between the unobserved network and the observed\nsocial behavior. This model assumed that each group was the result of a star\ngraph on a subset of the population. Thus, each group was the result of a\nleader who selected members of the population to be in the group. They called\nthese hub models.\n  This approach treats the network values as parameters of a model. However,\nthis leads to a general challenge in estimating parameters which must be\naddressed. For small datasets there can be far more parameters to estimate than\nthere are observations. Under these conditions, the estimated network can be\nunstable.\n  In this article, we propose a solution which penalizes the number of nodes\nwhich can exert a leadership role. We implement this as a pseudo-Expectation\nMaximization algorithm.\n  We demonstrate this technique through a series of simulations which show that\nwhen the number of leaders is sparse, parameter estimation is improved.\nFurther, we apply this technique to a dataset of animal behavior and an example\nof recommender systems. \n\n"}
{"id": "1809.00052", "contents": "Title: Your Actions or Your Associates? Predicting Certification and Dropout in\n  MOOCs with Behavioral and Social Features Abstract: The high level of attrition and low rate of certification in Massive Open\nOnline Courses (MOOCs) has prompted a great deal of research. Prior researchers\nhave focused on predicting dropout based upon behavioral features such as\nstudent confusion, click-stream patterns, and social interactions. However, few\nstudies have focused on combining student logs with forum data.\n  In this work, we use data from two different offerings of the same MOOC. We\nconduct a survival analysis to identify likely dropouts. We then examine two\nclasses of features, social and behavioral, and apply a combination of modeling\nand feature-selection methods to identify the most relevant features to predict\nboth dropout and certification. We examine the utility of three different model\ntypes and we consider the impact of different definitions of dropout on the\npredictors. Finally, we assess the reliability of the models over time by\nevaluating whether or not models from week 1 can predict dropout in week 2, and\nso on. The outcomes of this study will help instructors identify students\nlikely to fail or dropout as soon as the first two weeks and provide them with\nmore support. \n\n"}
{"id": "1809.00463", "contents": "Title: Shrinkage for Covariance Estimation: Asymptotics, Confidence Intervals,\n  Bounds and Applications in Sensor Monitoring and Finance Abstract: When shrinking a covariance matrix towards (a multiple) of the identity\nmatrix, the trace of the covariance matrix arises naturally as the optimal\nscaling factor for the identity target. The trace also appears in other\ncontext, for example when measuring the size of a matrix or the amount of\nuncertainty.\n  Of particular interest is the case when the dimension of the covariance\nmatrix is large. Then the problem arises that the sample covariance matrix is\nsingular if the dimension is larger than the sample size. Another issue is that\nusually the estimation has to based on correlated time series data. We study\nthe estimation of the trace functional allowing for a high-dimensional time\nseries model, where the dimension is allowed to grow with the sample size -\nwithout any constraint. Based on a recent result, we investigate a confidence\ninterval for the trace, which also allows us to propose lower and upper bounds\nfor the shrinkage covariance estimator as well as bounds for the variance of\nprojections. In addition, we provide a novel result dealing with shrinkage\ntowards a diagonal target.\n  We investigate the accuracy of the confidence interval by a simulation study,\nwhich indicates good performance, and analyze three stock market data sets to\nillustrate the proposed bounds, where the dimension (number of stocks) ranges\nbetween $32$ and $475$. Especially, we apply the results to portfolio\noptimization and determine bounds for the risk associated to the\nvariance-minimizing portfolio. \n\n"}
{"id": "1809.01399", "contents": "Title: Non-Orthogonal Multiplexing of Ultra-Reliable and Broadband Services in\n  Fog-Radio Architectures Abstract: The fifth generation (5G) of cellular systems is introducing Ultra-Reliable\nLow-Latency Communications (URLLC) services alongside more conventional\nenhanced Mobile BroadBand (eMBB) traffic. Furthermore, the 5G cellular\narchitecture is evolving from a base station-centric deployment to a fog-like\nset-up that accommodates a flexible functional split between cloud and edge. In\nthis paper, a novel solution is proposed that enables the non-orthogonal\ncoexistence of URLLC and eMBB services by processing URLLC traffic at the Edge\nNodes (ENs), while eMBB communications are handled centrally at a cloud\nprocessor as in a Cloud-Radio Access Network (C-RAN) system. This solution\nguarantees the low-latency requirements of the URLLC service by means of edge\nprocessing, e.g., for vehicle-to-cellular use cases, as well as the high\nspectral efficiency for eMBB traffic via centralized baseband processing. Both\nuplink and downlink are analyzed by accounting for the heterogeneous\nperformance requirements of eMBB and URLLC traffic and by considering practical\naspects such as fading, lack of channel state information for URLLC\ntransmitters, rate adaptation for eMBB transmitters, finite fronthaul capacity,\nand different coexistence strategies, such as puncturing. \n\n"}
{"id": "1809.05651", "contents": "Title: Mitigating Included- and Omitted-Variable Bias in Estimates of Disparate\n  Impact Abstract: Managers, employers, policymakers, and others often seek to understand\nwhether decisions are biased against certain groups. One popular analytic\nstrategy is to estimate disparities after adjusting for observed covariates,\ntypically with a regression model. This approach, however, suffers from two key\nstatistical challenges. First, omitted-variable bias can skew results if the\nmodel does not adjust for all relevant factors; second, and conversely,\nincluded-variable bias -- a lesser-known phenomenon -- can skew results if the\nset of covariates includes irrelevant factors. Here we introduce a new,\nthree-step statistical method, which we call risk-adjusted regression, to\naddress both concerns in settings where decision makers have clearly measurable\nobjectives. In the first step, we use all available covariates to estimate the\nvalue, or inversely, the risk, of taking a certain action, such as approving a\nloan application or hiring a job candidate. Second, we measure disparities in\ndecisions after adjusting for these risk estimates alone, mitigating the\nproblem of included-variable bias. Finally, in the third step, we assess the\nsensitivity of results to potential mismeasurement of risk, addressing concerns\nabout omitted-variable bias. To do so, we develop a novel, non-parametric\nsensitivity analysis that yields tight bounds on the true disparity in terms of\nthe average gap between true and estimated risk -- a single interpretable\nparameter that facilitates credible estimates. We demonstrate this approach on\na detailed dataset of 2.2 million police stops of pedestrians in New York City,\nand show that traditional statistical tests of discrimination can substantially\nunderestimate the magnitude of disparities. \n\n"}
{"id": "1809.06405", "contents": "Title: Bayesian analysis of absolute continuous Marshall-Olkin bivariate Pareto\n  distribution with location and scale parameters Abstract: This paper provides two different novel approaches of slice sampling to\nestimate the parameters of absolute continuous Marshall-Olkin bivariate Pareto\ndistribution with location and scale parameters. We carry out the bayesian\nanalysis taking gamma prior for shape and scale parameters and truncated normal\nfor location parameters. Credible intervals and coverage probabilities are also\nprovided for all methods. A real-life data analysis is shown for illustrative\npurpose. \n\n"}
{"id": "1809.06636", "contents": "Title: Comparison between Suitable Priors for Additive Bayesian Networks Abstract: Additive Bayesian networks are types of graphical models that extend the\nusual Bayesian generalized linear model to multiple dependent variables through\nthe factorisation of the joint probability distribution of the underlying\nvariables. When fitting an ABN model, the choice of the prior of the parameters\nis of crucial importance. If an inadequate prior - like a too weakly\ninformative one - is used, data separation and data sparsity lead to issues in\nthe model selection process. In this work a simulation study between two weakly\nand a strongly informative priors is presented. As weakly informative prior we\nuse a zero mean Gaussian prior with a large variance, currently implemented in\nthe R-package abn. The second prior belongs to the Student's t-distribution,\nspecifically designed for logistic regressions and, finally, the strongly\ninformative prior is again Gaussian with mean equal to true parameter value and\na small variance. We compare the impact of these priors on the accuracy of the\nlearned additive Bayesian network in function of different parameters. We\ncreate a simulation study to illustrate Lindley's paradox based on the prior\nchoice. We then conclude by highlighting the good performance of the\ninformative Student's t-prior and the limited impact of the Lindley's paradox.\nFinally, suggestions for further developments are provided. \n\n"}
{"id": "1809.08060", "contents": "Title: State-dependent Hawkes processes and their application to limit order\n  book modelling Abstract: We study statistical aspects of state-dependent Hawkes processes, which are\nan extension of Hawkes processes where a self- and cross-exciting counting\nprocess and a state process are fully coupled, interacting with each other. The\nexcitation kernel of the counting process depends on the state process that,\nreciprocally, switches state when there is an event in the counting process. We\nfirst establish the existence and uniqueness of state-dependent Hawkes\nprocesses and explain how they can be simulated. Then we develop maximum\nlikelihood estimation methodology for parametric specifications of the process.\nWe apply state-dependent Hawkes processes to high-frequency limit order book\ndata, allowing us to build a novel model that captures the feedback loop\nbetween the order flow and the shape of the limit order book. We estimate two\nspecifications of the model, using the bid-ask spread and the queue imbalance\nas state variables, and find that excitation effects in the order flow are\nstrongly state-dependent. Additionally, we find that the endogeneity of the\norder flow, measured by the magnitude of excitation, is also state-dependent,\nbeing more pronounced in disequilibrium states of the limit order book. \n\n"}
{"id": "1809.08427", "contents": "Title: Pachinko Prediction: A Bayesian method for event prediction from social\n  media data Abstract: The combination of large open data sources with machine learning approaches\npresents a potentially powerful way to predict events such as protest or social\nunrest. However, accounting for uncertainty in such models, particularly when\nusing diverse, unstructured datasets such as social media, is essential to\nguarantee the appropriate use of such methods. Here we develop a Bayesian\nmethod for predicting social unrest events in Australia using social media\ndata. This method uses machine learning methods to classify individual postings\nto social media as being relevant, and an empirical Bayesian approach to\ncalculate posterior event probabilities. We use the method to predict events in\nAustralian cities over a period in 2017/18. \n\n"}
{"id": "1809.08771", "contents": "Title: Modeling longitudinal data using matrix completion Abstract: In clinical practice and biomedical research, measurements are often\ncollected sparsely and irregularly in time while the data acquisition is\nexpensive and inconvenient. Examples include measurements of spine bone mineral\ndensity, cancer growth through mammography or biopsy, a progression of\ndefective vision, or assessment of gait in patients with neurological\ndisorders. Since the data collection is often costly and inconvenient,\nestimation of progression from sparse observations is of great interest for\npractitioners.\n  From the statistical standpoint, such data is often analyzed in the context\nof a mixed-effect model where time is treated as both a fixed-effect\n(population progression curve) and a random-effect (individual variability).\nAlternatively, researchers analyze Gaussian processes or functional data where\nobservations are assumed to be drawn from a certain distribution of processes.\nThese models are flexible but rely on probabilistic assumptions, require very\ncareful implementation, specific to the given problem, and tend to be slow in\npractice.\n  In this study, we propose an alternative elementary framework for analyzing\nlongitudinal data, relying on matrix completion. Our method yields estimates of\nprogression curves by iterative application of the Singular Value\nDecomposition. Our framework covers multivariate longitudinal data, regression,\nand can be easily extended to other settings. As it relies on existing tools\nfor matrix algebra it is efficient and easy to implement.\n  We apply our methods to understand trends of progression of motor impairment\nin children with Cerebral Palsy. Our model approximates individual progression\ncurves and explains 30% of the variability. Low-rank representation of\nprogression trends enables identification of different progression trends in\nsubtypes of Cerebral Palsy. \n\n"}
{"id": "1809.08801", "contents": "Title: Beyond Binomial and Negative Binomial: Adaptation in Bernoulli Parameter\n  Estimation Abstract: Estimating the parameter of a Bernoulli process arises in many applications,\nincluding photon-efficient active imaging where each illumination period is\nregarded as a single Bernoulli trial. Motivated by acquisition efficiency when\nmultiple Bernoulli processes are of interest, we formulate the allocation of\ntrials under a constraint on the mean as an optimal resource allocation\nproblem. An oracle-aided trial allocation demonstrates that there can be a\nsignificant advantage from varying the allocation for different processes and\ninspires a simple trial allocation gain quantity. Motivated by realizing this\ngain without an oracle, we present a trellis-based framework for representing\nand optimizing stopping rules. Considering the convenient case of Beta priors,\nthree implementable stopping rules with similar performances are explored, and\nthe simplest of these is shown to asymptotically achieve the oracle-aided trial\nallocation. These approaches are further extended to estimating functions of a\nBernoulli parameter. In simulations inspired by realistic active imaging\nscenarios, we demonstrate significant mean-squared error improvements: up to\n4.36 dB for the estimation of p and up to 1.80 dB for the estimation of log p. \n\n"}
{"id": "1809.09555", "contents": "Title: Context in Synthetic Biology: Memory Effects of Environments with\n  Mono-molecular Reactions Abstract: Synthetic biology aims at designing modular genetic circuits that can be\nassembled according to the desired function. When embedded in a cell, a circuit\nmodule becomes a small subnetwork within a larger environmental network, and\nits dynamics is therefore affected by potentially unknown interactions with the\nenvironment. It is well-known that the presence of the environment not only\ncauses extrinsic noise but also memory effects, which means that the dynamics\nof the subnetwork is affected by its past states via a memory function that is\ncharacteristic of the environment. We study several generic scenarios for the\ncoupling between a small module and a larger environment, with the environment\nconsisting of a chain of mono-molecular reactions. By mapping the dynamics of\nthis coupled system onto random walks, we are able to give exact analytical\nexpressions for the arising memory functions. Hence, our results give insights\ninto the possible types of memory functions and thereby help to better predict\nsubnetwork dynamics. \n\n"}
{"id": "1809.09590", "contents": "Title: Evaluating Federal Policies Using Bayesian Time Series Models:\n  Estimating the Causal Impact of the Hospital Readmissions Reduction Program Abstract: Researchers are often faced with evaluating the effect of a policy or program\nthat was simultaneously initiated across an entire population of units at a\nsingle point in time, and its effects over the targeted population can manifest\nat any time period afterwards. In the presence of data measured over time,\nBayesian time series models have been used to impute what would have happened\nafter the policy was initiated, had the policy not taken place, in order to\nestimate causal effects. However, the considerations regarding the definition\nof the target estimands, the underlying assumptions, the plausibility of such\nassumptions, and the choice of an appropriate model have not been thoroughly\ninvestigated. In this paper, we establish useful estimands for the evaluation\nof large-scale policies. We discuss that imputation of missing potential\noutcomes relies on an assumption which, even though untestable, can be\npartially evaluated using observed data. We illustrate an approach to evaluate\nthis key causal assumption and facilitate model elicitation based on data from\nthe time interval before policy initiation and using classic statistical\ntechniques. As an illustration, we study the Hospital Readmissions Reduction\nProgram (HRRP), a US federal intervention aiming to improve health outcomes for\npatients with pneumonia, acute myocardial infraction, or congestive failure\nadmitted to a hospital. We evaluate the effect of the HRRP on population\nmortality among the elderly across the US and in four geographic subregions,\nand at different time windows. We find that the HRRP increased mortality from\npneumonia and acute myocardial infraction across at least one geographical\nregion and time horizon, and is likely to have had a detrimental effect on\npublic health. \n\n"}
{"id": "1809.09620", "contents": "Title: RAFP-Pred: Robust Prediction of Antifreeze Proteins using Localized\n  Analysis of n-Peptide Compositions Abstract: In extreme cold weather, living organisms produce Antifreeze Proteins (AFPs)\nto counter the otherwise lethal intracellular formation of ice. Structures and\nsequences of various AFPs exhibit a high degree of heterogeneity, consequently\nthe prediction of the AFPs is considered to be a challenging task. In this\nresearch, we propose to handle this arduous manifold learning task using the\nnotion of localized processing. In particular an AFP sequence is segmented into\ntwo sub-segments each of which is analyzed for amino acid and di-peptide\ncompositions. We propose to use only the most significant features using the\nconcept of information gain (IG) followed by a random forest classification\napproach. The proposed RAFP-Pred achieved an excellent performance on a number\nof standard datasets. We report a high Youden's index\n(sensitivity+specificity-1) value of 0.75 on the standard independent test data\nset outperforming the AFP-PseAAC, AFP\\_PSSM, AFP-Pred and iAFP by a margin of\n0.05, 0.06, 0.14 and 0.68 respectively. The verification rate on the UniProKB\ndataset is found to be 83.19\\% which is substantially superior to the 57.18\\%\nreported for the iAFP method. \n\n"}
{"id": "1809.10074", "contents": "Title: Bayesian Data Synthesis and Disclosure Risk Quantification: An\n  Application to the Consumer Expenditure Surveys Abstract: The release of synthetic data generated from a model estimated on the data\nhelps statistical agencies disseminate respondent-level data with high utility\nand privacy protection. Motivated by the challenge of disseminating sensitive\nvariables containing geographic information in the Consumer Expenditure Surveys\n(CE) at the U.S. Bureau of Labor Statistics, we propose two non-parametric\nBayesian models as data synthesizers for the county identifier of each data\nrecord: a Bayesian latent class model and a Bayesian areal model. Both data\nsynthesizers use Dirichlet Process priors to cluster observations of similar\ncharacteristics and allow borrowing information across observations. We develop\ninnovative disclosure risks measures to quantify inherent risks in the\nconfidential CE data and how those data risks are ameliorated by our proposed\nsynthesizers. By creating a lower bound and an upper bound of disclosure risks\nunder a minimum and a maximum disclosure risks scenarios respectively, our\nproposed inherent risks measures provide a range of acceptable disclosure risks\nfor evaluating risks level in the synthetic datasets. \n\n"}
{"id": "1809.10443", "contents": "Title: New Radio Beam-based Access to Unlicensed Spectrum: Design Challenges\n  and Solutions Abstract: This paper elaborates on the design challenges, opportunities, and solutions\nfor New Radio-based access to Unlicensed spectrum (NR-U) by taking into account\nthe beam-based transmissions and the worldwide regulatory requirements. NR-U\nintends to expand the applicability of 5th generation New Radio access\ntechnology to support operation in unlicensed bands by adhering to\nListen-Before-Talk (LBT) requirements for accessing the channel. LBT was\nalready adopted by different variants of 4th generation Long Term Evolution\n(LTE) in unlicensed spectrum, i.e., Licensed-Assisted Access and MulteFire, to\nguarantee fair coexistence among different radio access technologies. In the\ncase of beam-based transmissions, the NR-U coexistence framework is\nsignificantly different as compared to LTE in unlicensed spectrum due to the\nuse of directional antennas, which enhance the spatial reuse but also\ncomplicate the interference management. In particular, beam-based transmissions\nare needed in the unlicensed spectrum at millimeter-wave (mmWave) bands, which\nis an attractive candidate for NR-U due to its large amount of allocated\nspectrum. As a consequence, some major design principles need to be revisited\nto address coexistence for beam-based NR-U. In this paper, different problems\nand the potential solutions related to channel access procedures, frame\nstructure, initial access procedures, re-transmission procedures, and\nscheduling schemes are discussed. A simulation evaluation of different\nLBT-based channel access procedures for NR-U/Wi-Fi indoor mmWave coexistence\nscenarios is also provided. \n\n"}
{"id": "1809.10925", "contents": "Title: Data depth and floating body Abstract: Little known relations of the renown concept of the halfspace depth for\nmultivariate data with notions from convex and affine geometry are discussed.\nHalfspace depth may be regarded as a measure of symmetry for random vectors. As\nsuch, the depth stands as a generalization of a measure of symmetry for convex\nsets, well studied in geometry. Under a mild assumption, the upper level sets\nof the halfspace depth coincide with the convex floating bodies used in the\ndefinition of the affine surface area for convex bodies in Euclidean spaces.\nThese connections enable us to partially resolve some persistent open problems\nregarding theoretical properties of the depth. \n\n"}
{"id": "1810.00031", "contents": "Title: Active Fairness in Algorithmic Decision Making Abstract: Society increasingly relies on machine learning models for automated decision\nmaking. Yet, efficiency gains from automation have come paired with concern for\nalgorithmic discrimination that can systematize inequality. Recent work has\nproposed optimal post-processing methods that randomize classification\ndecisions for a fraction of individuals, in order to achieve fairness measures\nrelated to parity in errors and calibration. These methods, however, have\nraised concern due to the information inefficiency, intra-group unfairness, and\nPareto sub-optimality they entail. The present work proposes an alternative\nactive framework for fair classification, where, in deployment, a\ndecision-maker adaptively acquires information according to the needs of\ndifferent groups or individuals, towards balancing disparities in\nclassification performance. We propose two such methods, where information\ncollection is adapted to group- and individual-level needs respectively. We\nshow on real-world datasets that these can achieve: 1) calibration and single\nerror parity (e.g., equal opportunity); and 2) parity in both false positive\nand false negative rates (i.e., equal odds). Moreover, we show that by\nleveraging their additional degree of freedom, active approaches can\nsubstantially outperform randomization-based classifiers previously considered\noptimal, while avoiding limitations such as intra-group unfairness. \n\n"}
{"id": "1810.01079", "contents": "Title: Three-body charmed baryon Decays with SU(3) flavor symmetry Abstract: We study the three-body anti-triplet ${\\bf B_c}\\to {\\bf B_n}MM'$ decays with\nthe $SU(3)$ flavor ($SU(3)_f$) symmetry, where ${\\bf B_c}$ denotes the charmed\nbaryon anti-triplet of $(\\Xi_c^0,-\\Xi_c^+,\\Lambda_c^+)$, and ${\\bf B_n}$ and\n$M(M')$ represent baryon and meson octets, respectively. By considering only\nthe S-wave $MM'$-pair contributions without resonance effects, the decays of\n${\\bf B_c}\\to {\\bf B_n}MM'$ can be decomposed into irreducible forms with 11\nparameters under $SU(3)_f$, which are fitted by the 14 existing data, resulting\nin a reasonable value of $\\chi^2/d.o.f=2.8$ for the fit. Consequently, we find\nthat the triangle sum rule of ${\\cal A}(\\Lambda_c^+\\to n\\bar K^0 \\pi^+)-{\\cal\nA}(\\Lambda_c^+\\to pK^- \\pi^+)-\\sqrt 2 {\\cal A}(\\Lambda_c^+\\to p\\bar K^0\n\\pi^0)=0$ given by the isospin symmetry holds under $SU(3)_f$, where ${\\cal A}$\nstands for the decay amplitude. In addition, we predict that ${\\cal\nB}(\\Lambda_c^+\\to n \\pi^{+} \\bar{K}^{0})=(0.9\\pm 0.8)\\times 10^{-2}$, which is\n$3-4$ times smaller than the BESIII observation, indicating the existence of\nthe resonant states. For the to-be-observed ${\\bf B_c}\\to {\\bf B_n}MM'$ decays,\nwe compute the branching fractions with the $SU(3)_f$ amplitudes to be compared\nto the BESIII and LHCb measurements in the future. \n\n"}
{"id": "1810.01710", "contents": "Title: Multilevel Monte Carlo Acceleration of Seismic Wave Propagation under\n  Uncertainty Abstract: We interpret uncertainty in a model for seismic wave propagation by treating\nthe model parameters as random variables, and apply the Multilevel Monte Carlo\n(MLMC) method to reduce the cost of approximating expected values of selected,\nphysically relevant, quantities of interest (QoI) with respect to the random\nvariables. Targeting source inversion problems, where the source of an\nearthquake is inferred from ground motion recordings on the Earth's surface, we\nconsider two QoI that measure the discrepancies between computed seismic\nsignals and given reference signals: one QoI, $\\hbox{QoI}_E$, is defined in\nterms of the $L^2$-misfit, which is directly related to maximum likelihood\nestimates of the source parameters; the other, $\\hbox{QoI}_W$, is based on the\nquadratic Wasserstein distance between probability distributions, and\nrepresents one possible choice in a class of such misfit functions that have\nbecome increasingly popular to solve seismic inversion in recent years. We\nsimulate seismic wave propagation, including seismic attenuation, using a\npublicly available code in widespread use, based on the spectral element\nmethod. Using random coefficients and deterministic initial and boundary data,\nwe present benchmark numerical experiments with synthetic data in a\ntwo-dimensional physical domain and a one-dimensional velocity model where the\nassumed parameter uncertainty is motivated by realistic Earth models. Here, the\ncomputational cost of the standard Monte Carlo method was reduced by up to 97%\nfor $\\hbox{QoI}_E$, and up to 78% for $\\hbox{QoI}_W$, using a relevant range of\ntolerances. Shifting to three-dimensional domains is straight-forward and will\nfurther increase the relative computational work reduction. \n\n"}
{"id": "1810.02397", "contents": "Title: Bayesian Model Selection for a Class of Spatially-Explicit Capture\n  Recapture Models Abstract: A vast amount of ecological knowledge generated recently has hinged upon the\nability of model selection methods to discriminate among various ecological\nhypotheses. The last decade has seen the rise of Bayesian hierarchical models\nin ecology. Consequently, popular tools, such as the AIC, become largely\ninapplicable and other tools are not universally applicable. We focus on a\nclass of competing Bayesian spatially explicit capture recapture (SECR) models\nand first apply some of the recommended Bayesian model selection tools: (1)\nBayes Factor - using (a) Gelfand-Dey (b) harmonic mean methods, (2) DIC, (3)\nWAIC and (4) the posterior predictive loss function. In all, we evaluate 25\nvariants of model selection tools in our study. We evaluate these model\nselection tools from the standpoint of model selection and parameter estimation\nby contrasting the choice recommended by a tool with a `true' model. In all, we\ngenerate 120 simulated data sets using the true model and assess the frequency\nwith which the true model is selected and how well the tool estimates N\n(population size). We find that when information content is low, no particular\ntool can be recommended to help realise, simultaneously, both the goals of\nmodel selection and parameter estimation. In such scenarios, we recommend that\npractitioners utilise our application of Bayes Factor for parameter estimation\nand recommend the posterior predictive loss approach for model selection when\ninformation content is low. When both the objectives are taken together, we\nrecommend the use of our applications of Bayes Factor for Bayesian SECR models.\nOur study reveals that although new model selection tools are emerging (eg:\nWAIC) in the applied statistics literature, an uncritical absorption of these\nnew tools (i.e. without assessing their efficacies for the problem at hand)\ninto ecological practice may mislead inferences. \n\n"}
{"id": "1810.03279", "contents": "Title: Modeling Brain Connectivity with Graphical Models on Frequency Domain Abstract: Multichannel electroencephalograms (EEGs) have been widely used to study\ncortical connectivity during acquisition of motor skills. In this paper, we\nintroduce copula Gaussian graphical models on spectral domain to characterize\ndependence in oscillatory activity between channels. To obtain a simple and\nrobust representation of brain connectivity that can explain the most variation\nin the observed signals, we propose a framework based on maximizing penalized\nlikelihood with Lasso regularization to search for the sparse precision matrix.\nTo address the optimization problem, graphical Lasso, Ledoit-Wolf and sparse\nestimation of a covariance matrix (SPCOV) algorithms were modified and\nimplemented. Simulations show the benefit of using the proposed algorithms in\nterms of robustness and small estimation errors. Furthermore, analysis of the\nEEG data in a motor skill task conducted using algorithms of modified graphical\nLASSO and Ledoit-Wolf, reveal a sparse pattern of brain connectivity among\ncortices which is consistent with the results from other work in the\nliterature. \n\n"}
{"id": "1810.05450", "contents": "Title: Fast approximate inference for variable selection in Dirichlet process\n  mixtures, with an application to pan-cancer proteomics Abstract: The Dirichlet Process (DP) mixture model has become a popular choice for\nmodel-based clustering, largely because it allows the number of clusters to be\ninferred. The sequential updating and greedy search (SUGS) algorithm (Wang and\nDunson, 2011) was proposed as a fast method for performing approximate Bayesian\ninference in DP mixture models, by posing clustering as a Bayesian model\nselection (BMS) problem and avoiding the use of computationally costly Markov\nchain Monte Carlo methods. Here we consider how this approach may be extended\nto permit variable selection for clustering, and also demonstrate the benefits\nof Bayesian model averaging (BMA) in place of BMS. Through an array of\nsimulation examples and well-studied examples from cancer transcriptomics, we\nshow that our method performs competitively with the current state-of-the-art,\nwhile also offering computational benefits. We apply our approach to\nreverse-phase protein array (RPPA) data from The Cancer Genome Atlas (TCGA) in\norder to perform a pan-cancer proteomic characterisation of 5,157 tumour\nsamples. We have implemented our approach, together with the original SUGS\nalgorithm, in an open-source R package named sugsvarsel, which accelerates\nanalysis by performing intensive computations in C++ and provides automated\nparallel processing. The R package is freely available from:\nhttps://github.com/ococrook/sugsvarsel \n\n"}
{"id": "1810.08564", "contents": "Title: Nonparametric Bayesian Lomax delegate racing for survival analysis with\n  competing risks Abstract: We propose Lomax delegate racing (LDR) to explicitly model the mechanism of\nsurvival under competing risks and to interpret how the covariates accelerate\nor decelerate the time to event. LDR explains non-monotonic covariate effects\nby racing a potentially infinite number of sub-risks, and consequently relaxes\nthe ubiquitous proportional-hazards assumption which may be too restrictive.\nMoreover, LDR is naturally able to model not only censoring, but also missing\nevent times or event types. For inference, we develop a Gibbs sampler under\ndata augmentation for moderately sized data, along with a stochastic gradient\ndescent maximum a posteriori inference algorithm for big data applications.\nIllustrative experiments are provided on both synthetic and real datasets, and\ncomparison with various benchmark algorithms for survival analysis with\ncompeting risks demonstrates distinguished performance of LDR. \n\n"}
{"id": "1810.09894", "contents": "Title: Heterogeneous large datasets integration using Bayesian factor\n  regression Abstract: Two key challenges in modern statistical applications are the large amount of\ninformation recorded per individual, and that such data are often not collected\nall at once but in batches. These batch effects can be complex, causing\ndistortions in both mean and variance. We propose a novel sparse latent factor\nregression model to integrate such heterogeneous data. The model provides a\ntool for data exploration via dimensionality reduction while correcting for a\nrange of batch effects. We study the use of several sparse priors (local and\nnon-local) to learn the dimension of the latent factors. Our model is fitted in\na deterministic fashion by means of an EM algorithm for which we derive\nclosed-form updates, contributing a novel scalable algorithm for non-local\npriors of interest beyond the immediate scope of this paper. We present several\nexamples, with a focus on bioinformatics applications. Our results show an\nincrease in the accuracy of the dimensionality reduction, with non-local priors\nsubstantially improving the reconstruction of factor cardinality, as well as\nthe need to account for batch effects to obtain reliable results. Our model\nprovides a novel approach to latent factor regression that balances sparsity\nwith sensitivity and is highly computationally efficient. \n\n"}
{"id": "1810.10036", "contents": "Title: The cumulative mass profile of the Milky Way as determined by globular\n  cluster kinematics from Gaia DR2 Abstract: We present new mass estimates and cumulative mass profiles (CMPs) with\nBayesian credible regions for the Milky Way (MW) Galaxy, given the kinematic\ndata of globular clusters as provided by (1) the $\\textit{Gaia}$ DR2\ncollaboration and the HSTPROMO team, and (2) the new catalog in Vasiliev\n(2019). We use globular clusters beyond 15kpc to estimate the CMP of the MW,\nassuming a total gravitational potential model $\\Phi(r) =\n\\Phi_{\\circ}r^{-\\gamma}$, which approximates an NFW-type potential at large\ndistances when $\\gamma=0.5$. We compare the resulting CMPs given data sets (1)\nand (2), and find the results to be nearly identical. The median estimate for\nthe total mass is $M_{200}= 0.70 \\times 10^{12} M_{\\odot}$ and the $50\\%$\nBayesian credible interval is $(0.62, 0.81)\\times10^{12}M_{\\odot}$. However,\nbecause the Vasiliev catalog contains more complete data at large $r$, the MW\ntotal mass is slightly more constrained by these data. In this work, we also\nsupply instructions for how to create a CMP for the MW with Bayesian credible\nregions, given a model for $M(<r)$ and samples drawn from a posterior\ndistribution. With the CMP, we can report median estimates and $50\\%$ Bayesian\ncredible regions for the MW mass within any distance (e.g., $M(r=25\\text{kpc})=\n0.26~(0.20, 0.36)\\times10^{12}M_{\\odot}$, $M(r=50\\text{kpc})= 0.37~(0.29, 0.51)\n\\times10^{12}M_{\\odot}$, $M(r=100\\text{kpc}) = 0.53~(0.41, 0.74)\n\\times10^{12}M_{\\odot}$, etc.), making it easy to compare our results directly\nto other studies. \n\n"}
{"id": "1810.11185", "contents": "Title: Beyond A/B Testing: Sequential Randomization for Developing\n  Interventions in Scaled Digital Learning Environments Abstract: Randomized experiments ensure robust causal inference that are critical to\neffective learning analytics research and practice. However, traditional\nrandomized experiments, like A/B tests, are limiting in large scale digital\nlearning environments. While traditional experiments can accurately compare two\ntreatment options, they are less able to inform how to adapt interventions to\ncontinually meet learners' diverse needs. In this work, we introduce a trial\ndesign for developing adaptive interventions in scaled digital learning\nenvironments -- the sequential randomized trial (SRT). With the goal of\nimproving learner experience and developing interventions that benefit all\nlearners at all times, SRTs inform how to sequence, time, and personalize\ninterventions. In this paper, we provide an overview of SRTs, and we illustrate\nthe advantages they hold compared to traditional experiments. We describe a\nnovel SRT run in a large scale data science MOOC. The trial results\ncontextualize how learner engagement can be addressed through inclusive\nculturally targeted reminder emails. We also provide practical advice for\nresearchers who aim to run their own SRTs to develop adaptive interventions in\nscaled digital learning environments. \n\n"}
{"id": "1810.12177", "contents": "Title: Variational Calibration of Computer Models Abstract: Bayesian calibration of black-box computer models offers an established\nframework to obtain a posterior distribution over model parameters. Traditional\nBayesian calibration involves the emulation of the computer model and an\nadditive model discrepancy term using Gaussian processes; inference is then\ncarried out using MCMC. These choices pose computational and statistical\nchallenges and limitations, which we overcome by proposing the use of\napproximate Deep Gaussian processes and variational inference techniques. The\nresult is a practical and scalable framework for calibration, which obtains\ncompetitive performance compared to the state-of-the-art. \n\n"}
{"id": "1810.12345", "contents": "Title: Analyzing Ideological Communities in Congressional Voting Networks Abstract: We here study the behavior of political party members aiming at identifying\nhow ideological communities are created and evolve over time in diverse\n(fragmented and non-fragmented) party systems. Using public voting data of both\nBrazil and the US, we propose a methodology to identify and characterize\nideological communities, their member polarization, and how such communities\nevolve over time, covering a 15-year period. Our results reveal very distinct\npatterns across the two case studies, in terms of both structural and dynamic\nproperties. \n\n"}
{"id": "1811.00731", "contents": "Title: The age of secrecy and unfairness in recidivism prediction Abstract: In our current society, secret algorithms make important decisions about\nindividuals. There has been substantial discussion about whether these\nalgorithms are unfair to groups of individuals. While noble, this pursuit is\ncomplex and ultimately stagnating because there is no clear definition of\nfairness and competing definitions are largely incompatible. We argue that the\nfocus on the question of fairness is misplaced, as these algorithms fail to\nmeet a more important and yet readily obtainable goal: transparency. As a\nresult, creators of secret algorithms can provide incomplete or misleading\ndescriptions about how their models work, and various other kinds of errors can\neasily go unnoticed. By partially reverse engineering the COMPAS algorithm -- a\nrecidivism-risk scoring algorithm used throughout the criminal justice system\n-- we show that it does not seem to depend linearly on the defendant's age,\ndespite statements to the contrary by the algorithm's creator. Furthermore, by\nsubtracting from COMPAS its (hypothesized) nonlinear age component, we show\nthat COMPAS does not necessarily depend on race, contradicting ProPublica's\nanalysis, which assumed linearity in age. In other words, faulty assumptions\nabout a proprietary algorithm lead to faulty conclusions that go unchecked\nwithout careful reverse engineering. Were the algorithm transparent in the\nfirst place, this would likely not have occurred. The most important result in\nthis work is that we find that there are many defendants with low risk score\nbut long criminal histories, suggesting that data inconsistencies occur\nfrequently in criminal justice databases. We argue that transparency satisfies\na different notion of procedural fairness by providing both the defendants and\nthe public with the opportunity to scrutinize the methodology and calculations\nbehind risk scores for recidivism. \n\n"}
{"id": "1811.00964", "contents": "Title: The X Factor: A Robust and Powerful Approach to X-chromosome-Inclusive\n  Whole-genome Association Studies Abstract: The X-chromosome is often excluded from genome-wide association studies\nbecause of analytical challenges. Some of the problems, such as the random,\nskewed or no X-inactivation model uncertainty, have been investigated. Other\nconsiderations have received little to no attention, such as the value in\nconsidering non-additive and gene-sex interaction effects, and the inferential\nconsequence of choosing different baseline alleles (i.e.\\ the reference vs.\\\nthe alternative allele). Here we propose a unified and flexible\nregression-based association test for X-chromosomal variants. We provide\ntheoretical justifications for its robustness in the presence of various model\nuncertainties, as well as for its improved power when compared with the\nexisting approaches under certain scenarios. For completeness, we also revisit\nthe autosomes and show that the proposed framework leads to a more robust\napproach than the standard method. Finally, we provide supporting evidence by\nrevisiting several published association studies. Supplementary materials for\nthis article are available online. \n\n"}
{"id": "1811.01315", "contents": "Title: Modeling Stated Preference for Mobility-on-Demand Transit: A Comparison\n  of Machine Learning and Logit Models Abstract: Logit models are usually applied when studying individual travel behavior,\ni.e., to predict travel mode choice and to gain behavioral insights on traveler\npreferences. Recently, some studies have applied machine learning to model\ntravel mode choice and reported higher out-of-sample predictive accuracy than\ntraditional logit models (e.g., multinomial logit). However, little research\nfocuses on comparing the interpretability of machine learning with logit\nmodels. In other words, how to draw behavioral insights from the\nhigh-performance \"black-box\" machine-learning models remains largely unsolved\nin the field of travel behavior modeling.\n  This paper aims at providing a comprehensive comparison between the two\napproaches by examining the key similarities and differences in model\ndevelopment, evaluation, and behavioral interpretation between logit and\nmachine-learning models for travel mode choice modeling. To complement the\ntheoretical discussions, the paper also empirically evaluates the two\napproaches on the stated-preference survey data for a new type of transit\nsystem integrating high-frequency fixed-route services and ridesourcing. The\nresults show that machine learning can produce significantly higher predictive\naccuracy than logit models. Moreover, machine learning and logit models largely\nagree on many aspects of behavioral interpretations. In addition, machine\nlearning can automatically capture the nonlinear relationship between the input\nfeatures and choice outcomes. The paper concludes that there is great potential\nin merging ideas from machine learning and conventional statistical methods to\ndevelop refined models for travel behavior research and suggests some new\nresearch directions. \n\n"}
{"id": "1811.01498", "contents": "Title: DSIC: Deep Learning based Self-Interference Cancellation for In-Band\n  Full Duplex Wireless Abstract: In-band full duplex wireless is of utmost interest to future wireless\ncommunication and networking due to great potentials of spectrum efficiency.\nIBFD wireless, however, is throttled by its key challenge, namely\nself-interference. Therefore, effective self-interference cancellation is the\nkey to enable IBFD wireless. This paper proposes a real-time non-linear\nself-interference cancellation solution based on deep learning. In this\nsolution, a self-interference channel is modeled by a deep neural network\n(DNN). Synchronized self-interference channel data is first collected to train\nthe DNN of the self-interference channel. Afterwards, the trained DNN is used\nto cancel the self-interference at a wireless node. This solution has been\nimplemented on a USRP SDR testbed and evaluated in real world in multiple\nscenarios with various modulations in transmitting information including\nnumbers, texts as well as images. It results in the performance of 17dB in\ndigital cancellation, which is very close to the self-interference power and\nnearly cancels the self-interference at a SDR node in the testbed. The solution\nyields an average of 8.5% bit error rate (BER) over many scenarios and\ndifferent modulation schemes. \n\n"}
{"id": "1811.02069", "contents": "Title: On the asymptotics of Maronna's robust PCA Abstract: The eigenvalue decomposition (EVD) parameters of the second order statistics\nare ubiquitous in statistical analysis and signal processing. Notably, the EVD\nof robust scatter $M$-estimators is a popular choice to perform robust\nprobabilistic PCA or other dimension reduction related applications. Towards\nthe goal of characterizing the behavior of these quantities, this paper\nproposes new asymptotics for the EVD parameters (i.e. eigenvalues, eigenvectors\nand principal subspace) of the scatter $M$-estimator in the context of complex\nelliptically symmetric distributions. First, their Gaussian asymptotic\ndistribution is obtained by extending standard results on the sample covariance\nmatrix in a Gaussian context. Second, their convergence rate towards the EVD\nparameters of a Gaussian-Core Wishart Equivalent is derived. This second result\nrepresents the main contribution in the sense that it quantifies when it is\nacceptable to directly plug-in well-established results on the EVD of\nWishart-distributed matrix for characterizing the EVD of $M$-estimators.\nEventually, some examples (low-rank adaptive filtering and Intrinsic bias\nanalysis) are provided to illustrate where the obtained results can be\nleveraged. \n\n"}
{"id": "1811.03410", "contents": "Title: Quantifying Link Stability in Ad Hoc Wireless Networks Subject to\n  Ornstein-Uhlenbeck Mobility Abstract: The performance of mobile ad hoc networks in general and that of the routing\nalgorithm, in particular, can be heavily affected by the intrinsic dynamic\nnature of the underlying topology. In this paper, we build a new\nanalytical/numerical framework that characterizes nodes' mobility and the\nevolution of links between them. This formulation is based on a stationary\nMarkov chain representation of link connectivity. The existence of a link\nbetween two nodes depends on their distance, which is governed by the mobility\nmodel. In our analysis, nodes move randomly according to an Ornstein-Uhlenbeck\nprocess using one tuning parameter to obtain different levels of randomness in\nthe mobility pattern. Finally, we propose an entropy-rate-based metric that\nquantifies link uncertainty and evaluates its stability. Numerical results show\nthat the proposed approach can accurately reflect the random mobility in the\nnetwork and fully captures the link dynamics. It may thus be considered a\nvaluable performance metric for the evaluation of the link stability and\nconnectivity in these networks. \n\n"}
{"id": "1811.03745", "contents": "Title: A Fundamental Measure of Treatment Effect Heterogeneity Abstract: We offer a non-parametric plug-in estimator for an important measure of\ntreatment effect variability and provide minimum conditions under which the\nestimator is asymptotically efficient. The stratum specific treatment effect\nfunction or so-called blip function, is the average treatment effect for a\nrandomly drawn stratum of confounders. The mean of the blip function is the\naverage treatment effect (ATE), whereas the variance of the blip function\n(VTE), the main subject of this paper, measures overall clinical effect\nheterogeneity, perhaps providing a strong impetus to refine treatment based on\nthe confounders. VTE is also an important measure for assessing reliability of\nthe treatment for an individual. The CV-TMLE provides simultaneous plug-in\nestimates and inference for both ATE and VTE, guaranteeing asymptotic\nefficiency under one less condition than for TMLE. This condition is difficult\nto guarantee a priori, particularly when using highly adaptive machine learning\nthat we need to employ in order to eliminate bias. Even in defiance of this\ncondition, CV-TMLE sampling distributions maintain normality, not guaranteed\nfor TMLE, and have a lower mean squared error than their TMLE counterparts. In\naddition to verifying the theoretical properties of TMLE and CV-TMLE through\nsimulations, we point out some of the challenges in estimating VTE, which lacks\ndouble robustness and might be unavoidably biased if the true VTE is small and\nsample size insufficient. We will provide an application of the estimator on a\ndata set for treatment of acute trauma patients. \n\n"}
{"id": "1811.06819", "contents": "Title: Descoberta de rela\\c{c}\\~oes alom\\'etricas entre popula\\c{c}\\~ao e crime\n  dentro de uma grande metr\\'opole Abstract: Recently humanity has just crossed an important landmark in its history with\nthe majority of people now living in large cities. This population\nconcentration is capable of boosting the growth of positive indicators such as\ninnovation, the production of new patents and supercreative employment, but\nincreases the spread of diseases and the occurrence of crimes. Faced with the\nrealization that crime rates grow year after year in these large urban centers,\nwe sought to understand the dynamics of crime within cities. We investigate at\nthe subscale of the neighborhoods of a highly populated city the incidence of\nproperty crimes in terms of both the resident and the floating population. Our\nresults show that a relevant allometric relation could only be observed between\nproperty crimes and floating population. More precisely, the evidence of a\nsuperlinear behavior indicates that a disproportional number of property crimes\noccurs in regions where an increased flow of people takes place in the city.\nFor comparison, we also found that the number of crimes of peace disturbance\nonly correlates well, and in a superlinear fashion too, with the resident\npopulation. Our study raises the interesting possibility that the\nsuperlinearity observed in previous studies [Bettencourt et al., Proc. Natl.\nAcad. Sci. USA 104, 7301 (2007) and Melo et al., Sci. Rep. 4, 6239 (2014)] for\nhomicides versus population at the city scale could have its origin in the fact\nthat the floating population, and not the resident one, should be taken as the\nrelevant variable determining the intrinsic microdynamical behavior of the\nsystem. This finding was the motivation for the codification of a framework\nthat supports the analysis of population and crime data to propose city\ndivisions that allow the allocation of police by floating population and\nresident population statistics. \n\n"}
{"id": "1811.09738", "contents": "Title: First measurements of absolute branching fractions of $\\Xi_c^0$ at Belle Abstract: We present the first measurements of absolute branching fractions of\n$\\Xi_c^0$ decays into $\\Xi^- \\pi^+$, $\\Lambda K^- \\pi^+$, and $p K^- K^- \\pi^+$\nfinal states. The measurements are made using a data set comprising $(772\\pm\n11)\\times 10^{6}$ $B\\bar{B}$ pairs collected at the $\\Upsilon(4S)$ resonance\nwith the Belle detector at the KEKB $e^+e^-$ collider. We first measure the\nabsolute branching fraction for $B^- \\to \\bar{\\Lambda}_c^- \\Xi_c^0$ using a\nmissing-mass technique; the result is ${\\cal B}(B^- \\to \\bar{\\Lambda}_c^-\n\\Xi_c^0) = (9.51 \\pm 2.10 \\pm 0.88) \\times 10^{-4}$. We subsequently measure\nthe product branching fractions ${\\cal B}(B^- \\to \\bar{\\Lambda}_c^-\n\\Xi_c^0){\\cal B}(\\Xi_c^0 \\to \\Xi^- \\pi^+)$, ${\\cal B}( B^- \\to\n\\bar{\\Lambda}_c^- \\Xi_c^0) {\\cal B}(\\Xi_c^0 \\to \\Lambda K^- \\pi^+)$, and ${\\cal\nB}( B^- \\to \\bar{\\Lambda}_c^- \\Xi_c^0) {\\cal B}(\\Xi_c^0 \\to p K^- K^- \\pi^+)$\nwith improved precision. Dividing these product branching fractions by the\nresult for $B^- \\to \\bar{\\Lambda}_c^- \\Xi_c^0$ yields the following branching\nfractions: ${\\cal B}(\\Xi_c^0 \\to \\Xi^- \\pi^+)= (1.80 \\pm 0.50 \\pm 0.14)\\%$,\n${\\cal B}(\\Xi_c^0 \\to \\Lambda K^- \\pi^+)=(1.17 \\pm 0.37 \\pm 0.09)\\%$, and\n${\\cal B}(\\Xi_c^0 \\to p K^- K^- \\pi^+)=(0.58 \\pm 0.23 \\pm 0.05)\\%.$ For the\nabove branching fractions, the first uncertainties are statistical and the\nsecond are systematic. Our result for ${\\cal B}(\\Xi_c^0 \\to \\Xi^- \\pi^+)$ can\nbe combined with $\\Xi_c^0$ branching fractions measured relative to $\\Xi_c^0\n\\to \\Xi^- \\pi^+$ to yield other absolute $\\Xi_c^0$ branching fractions. \n\n"}
{"id": "1811.11025", "contents": "Title: CVEK: Robust Estimation and Testing for Nonlinear Effects using Kernel\n  Machine Ensemble Abstract: The R package CVEK introduces a suite of flexible machine learning models and\nrobust hypothesis tests for learning the joint nonlinear effects of multiple\ncovariates in limited samples. It implements the Cross-validated Ensemble of\nKernels (CVEK)(Liu and Coull 2017), an ensemble-based kernel machine learning\nmethod that adaptively learns the joint nonlinear effect of multiple covariates\nfrom data, and provides powerful hypothesis tests for both main effects of\nfeatures and interactions among features. The R Package CVEK provides a\nflexible, easy-to-use implementation of CVEK, and offers a wide range of\nchoices for the kernel family (for instance, polynomial, radial basis\nfunctions, Mat\\'ern, neural network, and others), model selection criteria,\nensembling method (averaging, exponential weighting, cross-validated stacking),\nand the type of hypothesis test (asymptotic or parametric bootstrap). Through\nextensive simulations we demonstrate the validity and robustness of this\napproach, and provide practical guidelines on how to design an estimation\nstrategy for optimal performance in different data scenarios. \n\n"}
{"id": "1811.11709", "contents": "Title: High-dimensional Log-Error-in-Variable Regression with Applications to\n  Microbial Compositional Data Analysis Abstract: In microbiome and genomic studies, the regression of compositional data has\nbeen a crucial tool for identifying microbial taxa or genes that are associated\nwith clinical phenotypes. To account for the variation in sequencing depth, the\nclassic log-contrast model is often used where read counts are normalized into\ncompositions. However, zero read counts and the randomness in covariates remain\ncritical issues. In this article, we introduce a surprisingly simple,\ninterpretable, and efficient method for the estimation of compositional data\nregression through the lens of a novel high-dimensional log-error-in-variable\nregression model. The proposed method provides both corrections on sequencing\ndata with possible overdispersion and simultaneously avoids any subjective\nimputation of zero read counts. We provide theoretical justifications with\nmatching upper and lower bounds for the estimation error. The merit of the\nprocedure is illustrated through real data analysis and simulation studies. \n\n"}
{"id": "1811.11714", "contents": "Title: Variational Selection of Features for Molecular Kinetics Abstract: The modeling of atomistic biomolecular simulations using kinetic models such\nas Markov state models (MSMs) has had many notable algorithmic advances in\nrecent years. The variational principle has opened the door for a nearly fully\nautomated toolkit for selecting models that predict the long-time kinetics from\nmolecular dynamics simulations. However, one yet-unoptimized step of the\npipeline involves choosing the features, or collective variables, from which\nthe model should be constructed. In order to build intuitive models, these\ncollective variables are often sought to be interpretable and familiar\nfeatures, such as torsional angles or contact distances in a protein structure.\nHowever, previous approaches for evaluating the chosen features rely on\nconstructing a full MSM, which in turn requires additional hyperparameters to\nbe chosen, and hence leads to a computationally expensive framework. Here, we\npresent a method to optimize the feature choice directly, without requiring the\nconstruction of the final kinetic model. We demonstrate our rigorous\npreprocessing algorithm on a canonical set of twelve fast-folding protein\nsimulations, and show that our procedure leads to more efficient model\nselection. \n\n"}
{"id": "1811.12144", "contents": "Title: The mechanism of RNA base fraying: molecular dynamics simulations\n  analyzed with core-set Markov state models Abstract: The process of RNA base fraying (i.e. the transient opening of the termini of\na helix) is involved in many aspects of RNA dynamics. We here use molecular\ndynamics simulations and Markov state models to characterize the kinetics of\nRNA fraying and its sequence and direction dependence. In particular, we first\nintroduce a method for determining biomolecular dynamics employing core-set\nMarkov state models constructed using an advanced clustering technique. The\nmethod is validated on previously reported simulations. We then use the method\nto analyze extensive trajectories for four different RNA model duplexes.\nResults obtained using D. E. Shaw research and AMBER force fields are compared\nand discussed in detail, and show a non-trivial interplay between the stability\nof intermediate states and the overall fraying kinetics. \n\n"}
{"id": "1811.12788", "contents": "Title: Optimal Uncertainty Quantification on moment class using canonical\n  moments Abstract: We gain robustness on the quantification of a risk measurement by accounting\nfor all sources of uncertainties tainting the inputs of a computer code. We\nevaluate the maximum quantile over a class of distributions defined only by\nconstraints on their moments. The methodology is based on the theory of\ncanonical moments that appears to be a well-suited framework for practical\noptimization. \n\n"}
{"id": "1812.00096", "contents": "Title: Intraday forecasts of a volatility index: Functional time series methods\n  with dynamic updating Abstract: As a forward-looking measure of future equity market volatility, the VIX\nindex has gained immense popularity in recent years to become a key measure of\nrisk for market analysts and academics. We consider discrete reported intraday\nVIX tick values as realisations of a collection of curves observed sequentially\non equally spaced and dense grids over time and utilise functional data\nanalysis techniques to produce one-day-ahead forecasts of these curves. The\nproposed method facilitates the investigation of dynamic changes in the index\nover very short time intervals as showcased using the 15-second high-frequency\nVIX index values. With the help of dynamic updating techniques, our point and\ninterval forecasts are shown to enjoy improved accuracy over conventional time\nseries models. \n\n"}
{"id": "1812.02522", "contents": "Title: A novel health risk model based on intraday physical activity time\n  series collected by smartphones Abstract: We compiled a demo application and collected a motion database of more than\n10,000 smartphone users to produce a health risk model trained on physical\nactivity streams. We turned to adversarial domain adaptation and employed the\nUK Biobank dataset of motion data, augmented by a rich set of clinical\ninformation as the source domain to train the model using a deep residual\nconvolutional neuron network (ResNet). The model risk score is a biomarker of\nageing, since it was predictive of lifespan and healthspan (as defined by the\nonset of specified diseases), and was elevated in groups associated with\nlife-shortening lifestyles, such as smoking. We ascertained the target domain\nperformance in a smaller cohort of the mobile application that included users\nwho were willing to share answers to a short questionnaire related to their\ndisease and smoking status. We thus conclude that the proposed pipeline\ncombining deep convolutional and Domain Adversarial neuron networks (DANN) is a\npowerful tool for disease risk and lifestyle-associated hazard assessment from\nmobile motion sensors that are transferable across devices and populations. \n\n"}
{"id": "1812.04369", "contents": "Title: Variational Bayesian Weighted Complex Network Reconstruction Abstract: Complex network reconstruction is a hot topic in many fields. Currently, the\nmost popular data-driven reconstruction framework is based on lasso. However,\nit is found that, in the presence of noise, lasso loses efficiency for weighted\nnetworks. This paper builds a new framework to cope with this problem. The key\nidea is to employ a series of linear regression problems to model the\nrelationship between network nodes, and then to use an efficient variational\nBayesian algorithm to infer the unknown coefficients. The numerical experiments\nconducted on both synthetic and real data demonstrate that the new method\noutperforms lasso with regard to both reconstruction accuracy and running\nspeed. \n\n"}
{"id": "1812.04567", "contents": "Title: A flat persistence diagram for improved visualization of persistent\n  homology Abstract: Visualization in the emerging field of topological data analysis has\nprogressed from persistence barcodes and persistence diagrams to display of\ntwo-parameter persistent homology. Although persistence barcodes and diagrams\nhave permitted insight into the geometry underlying complex datasets,\nvisualization of even single-parameter persistent homology has significant room\nfor improvement. Here, we propose a modification to the conventional\npersistence diagram - the flat persistence diagram - that more efficiently\ndisplays information relevant to persistent homology and simultaneously\ncorrects for visual bias present in the former. Flat persistence diagrams\ndisplay equivalent information as their predecessor, while providing\nresearchers with an intuitive horizontal reference axis in contrast to the\nusual diagonal reference line. Reducing visual bias through the use of\nappropriate graphical displays not only provides more accurate, but also deeper\ninsights into the topology that underlies complex datasets. Introducing flat\npersistence diagrams into widespread use would bring researchers one step\ncloser to practical application of topological data analysis. \n\n"}
{"id": "1812.05529", "contents": "Title: High dimensional inference for the structural health monitoring of lock\n  gates Abstract: Locks and dams are critical pieces of inland waterways. However, many\ncomponents of existing locks have been in operation past their designed\nlifetime. To ensure safe and cost effective operations, it is therefore\nimportant to monitor the structural health of locks. To support lock gate\nmonitoring, this work considers a high dimensional Bayesian inference problem\nthat combines noisy real time strain observations with a detailed finite\nelement model. To solve this problem, we develop a new technique that combines\nKarhunen-Lo\\`eve decompositions, stochastic differential equation\nrepresentations of Gaussian processes, and Kalman smoothing that scales\nlinearly with the number of observations and could be used for near real-time\nmonitoring. We use quasi-periodic Gaussian processes to model thermal\ninfluences on the strain and infer spatially distributed boundary conditions in\nthe model, which are also characterized with Gaussian process prior\ndistributions. The power of this approach is demonstrated on a small synthetic\nexample and then with real observations of Mississippi River Lock 27, which is\nlocated near St. Louis, MO USA. The results show that our approach is able to\nprobabilistically characterize the posterior distribution over nearly 1.4\nmillion parameters in under an hour on a standard desktop computer. \n\n"}
{"id": "1812.05691", "contents": "Title: Dose-response modeling in high-throughput cancer drug screenings: An\n  end-to-end approach Abstract: Personalized cancer treatments based on the molecular profile of a patient's\ntumor are an emerging and exciting class of treatments in oncology. As genomic\ntumor profiling is becoming more common, targeted treatments to specific\nmolecular alterations are gaining traction. To discover new potential\ntherapeutics that may apply to broad classes of tumors matching some molecular\npattern, experimentalists and pharmacologists rely on high-throughput, in-vitro\nscreens of many compounds against many different cell lines. We propose a\nhierarchical Bayesian model of how cancer cell lines respond to drugs in these\nexperiments and develop a method for fitting the model to real-world\nhigh-throughput screening data. Through a case study, the model is shown to\ncapture nontrivial associations between molecular features and drug response,\nsuch as requiring both wild type TP53 and overexpression of MDM2 to be\nsensitive to Nutlin-3(a). In quantitative benchmarks, the model outperforms a\nstandard approach in biology, with ~20% lower predictive error on held out\ndata. When combined with a conditional randomization testing procedure, the\nmodel discovers biomarkers of therapeutic response that recapitulate known\nbiology and suggest new avenues for investigation. All code for the paper is\npublicly available at https://github.com/tansey/deep-dose-response. \n\n"}
{"id": "1812.05908", "contents": "Title: Social Network Analysis: Bibliographic Network Analysis of the Field and\n  its Evolution / Part 1. Basic Statistics and Citation Network Analysis Abstract: In this paper, we present the results of the study on the development of\nsocial network analysis (SNA) discipline and its evolution over time, using the\nanalysis of bibliographic networks. The dataset consists of articles from the\nWeb of Science Clarivate Analytics database and those published in the main\njournals in the field (70,000+ publications), created by searching for the key\nword \"social network*.\" From the collected data, we constructed several\nnetworks (citation and two-mode, linking publications with authors, keywords\nand journals). Analyzing the obtained networks, we evaluated the trends in the\nfield`s growth, noted the most cited works, created a list of authors and\njournals with the largest amount of works, and extracted the most often used\nkeywords in the SNA field. Next, using the Search path count approach, we\nextracted the main path, key-route paths and link islands in the citation\nnetwork. Based on the probabilistic flow node values, we identified the most\nimportant articles. Our results show that authors from the social sciences, who\nwere most active through the whole history of the field development,\nexperienced the \"invasion\" of physicists from 2000's. However, starting from\nthe 2010's, a new very active group of animal social network analysis has\nemerged. \n\n"}
{"id": "1812.06361", "contents": "Title: Bernoulli Ballot Polling: A Manifest Improvement for Risk-Limiting\n  Audits Abstract: We present a method and software for ballot-polling risk-limiting audits\n(RLAs) based on Bernoulli sampling: ballots are included in the sample with\nprobability $p$, independently. Bernoulli sampling has several advantages: (1)\nit does not require a ballot manifest; (2) it can be conducted independently at\ndifferent locations, rather than requiring a central authority to select the\nsample from the whole population of cast ballots or requiring stratified\nsampling; (3) it can start in polling places on election night, before margins\nare known. If the reported margins for the 2016 U.S. Presidential election are\ncorrect, a Bernoulli ballot-polling audit with a risk limit of 5% and a\nsampling rate of $p_0 = 1\\%$ would have had at least a 99% probability of\nconfirming the outcome in 42 states. (The other states were more likely to have\nneeded to examine additional ballots.) Logistical and security advantages that\nauditing in the polling place affords may outweigh the cost of examining more\nballots than some other methods might require. \n\n"}
{"id": "1812.07694", "contents": "Title: Wasserstein Covariance for Multiple Random Densities Abstract: A common feature of methods for analyzing samples of probability density\nfunctions is that they respect the geometry inherent to the space of densities.\nOnce a metric is specified for this space, the Fr\\'echet mean is typically used\nto quantify and visualize the average density from the sample. For\none-dimensional densities, the Wasserstein metric is popular due to its\ntheoretical appeal and interpretive value as an optimal transport metric,\nleading to the Wasserstein-Fr\\'echet mean or barycenter as the mean density. We\nextend the existing methodology for samples of densities in two key directions.\nFirst, motivated by applications in neuroimaging, we consider dependent density\ndata, where a $p$-vector of univariate random densities is observed for each\nsampling unit. Second, we introduce a Wasserstein covariance measure and\npropose intuitively appealing estimators for both fixed and diverging $p$,\nwhere the latter corresponds to continuously-indexed densities. We also give\ntheory demonstrating consistency and asymptotic normality, while accounting for\nerrors introduced in the unavoidable preparatory density estimation step. The\nutility of the Wasserstein covariance matrix is demonstrated through\napplications to functional connectivity in the brain using functional magnetic\nresonance imaging data and to the secular evolution of mortality for various\ncountries. \n\n"}
{"id": "1812.08213", "contents": "Title: Analyzing and biasing simulations with PLUMED Abstract: This chapter discusses how the PLUMED plugin for molecular dynamics can be\nused to analyze and bias molecular dynamics trajectories. The chapter begins by\nintroducing the notion of a collective variable and by then explaining how the\nfree energy can be computed as a function of one or more collective variables.\nA number of practical issues mostly around periodic boundary conditions that\narise when these types of calculations are performed using PLUMED are then\ndiscussed. Later parts of the chapter discuss how PLUMED can be used to perform\nenhanced sampling simulations that introduce simulation biases or multiple\nreplicas of the system and Monte Carlo exchanges between these replicas. This\nsection is then followed by a discussion on how free-energy surfaces and\nassociated error bars can be extracted from such simulations by using weighted\nhistogram and block averaging techniques. \n\n"}
{"id": "1812.08508", "contents": "Title: Singly Cabibbo suppressed decays of $\\Lambda_{c}^+$ with SU(3) flavor\n  symmetry Abstract: We analyze the weak processes of anti-triplet charmed baryons decaying to\noctet baryons and mesons with the SU(3) flavor symmetry and topological quark\ndiagram scheme. We study the decay branching ratios without neglecting the\ncontributions from ${\\cal O}(\\overline{15})$ for the first time in the SU(3)\nflavor symmetry approach. The fitting results for the Cabibbo allowed and\nsuppressed decays of $\\Lambda_{c}^+$ are all consistent with the experimental\ndata. We predict all singly Cabibbo suppressed decays. In particular, we find\nthat ${\\cal B}(\\Lambda_c^+\\to p \\pi^0)=(1.3\\pm0.7)\\times 10^{-4}$, which is\nslightly below the current experimental upper limit of $2.7\\times 10^{-4}$ and\ncan be tested by the ongoing experiment at BESIII as well as the future one at\nBelle-II. \n\n"}
{"id": "1812.09320", "contents": "Title: The Cost of Delay in Status Updates and their Value: Non-linear Ageing Abstract: We consider a status update communication system consisting of a\nsource-destination link. A stochastic process is observed at the source, where\nsamples are extracted at random time instances, and delivered to the\ndestination, thus, providing status updates for the source. In this paper, we\nexpand the concept of information ageing by introducing the cost of update\ndelay (CoUD) metric to characterize the cost of having stale information at the\ndestination. The CoUD captures the freshness of the information at the\ndestination and can be used to reflect the information structure of the source.\nMoreover, we introduce the value of information of update (VoIU) metric that\ncaptures the reduction of CoUD upon reception of an update. Using the CoUD, its\nby-product metric called peak cost of update delay (PCoUD), and the VoIU, we\nevaluate the performance of an M/M/1 system in various settings that consider\nexact expressions and bounds. Our results indicate that the performance of CoUD\ndiffers depending on the cost assigned per time unit, however the optimal\npolicy remains the same for linear ageing and varies for non-linear ageing.\nWhen it comes to the VoIU the performance difference appears only when the cost\nincreases non-linearly with time. The study illustrates the importance of the\nnewly introduced variants of age, furthermore supported in the case of VoIU by\nits tractability. \n\n"}
{"id": "1901.01187", "contents": "Title: PopNetCod: A Popularity-based Caching Policy for Network Coding enabled\n  Named Data Networking Abstract: In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers. \n\n"}
{"id": "1901.02936", "contents": "Title: The Mahalanobis kernel for heritability estimation in genome-wide\n  association studies: fixed-effects and random-effects methods Abstract: Linear mixed models (LMMs) are widely used for heritability estimation in\ngenome-wide association studies (GWAS). In standard approaches to heritability\nestimation with LMMs, a genetic relationship matrix (GRM) must be specified. In\nGWAS, the GRM is frequently a correlation matrix estimated from the study\npopulation's genotypes, which corresponds to a normalized Euclidean distance\nkernel. In this paper, we show that reliance on the Euclidean distance kernel\ncontributes to several unresolved modeling inconsistencies in heritability\nestimation for GWAS. These inconsistencies can cause biased heritability\nestimates in the presence of linkage disequilibrium (LD), depending on the\ndistribution of causal variants. We show that these biases can be resolved (at\nleast at the modeling level) if one adopts a Mahalanobis distance-based GRM for\nLMM analysis. Additionally, we propose a new definition of partitioned\nheritability -- the heritability attributable to a subset of genes or single\nnucleotide polymorphisms (SNPs) -- using the Mahalanobis GRM, and show that it\ninherits many of the nice consistency properties identified in our original\nanalysis. Partitioned heritability is a relatively new area for GWAS analysis,\nwhere inconsistency issues related to LD have previously been known to be\nespecially pernicious. \n\n"}
{"id": "1901.04592", "contents": "Title: Interpretable machine learning: definitions, methods, and applications Abstract: Machine-learning models have demonstrated great success in learning complex\npatterns that enable them to make predictions about unobserved data. In\naddition to using models for prediction, the ability to interpret what a model\nhas learned is receiving an increasing amount of attention. However, this\nincreased focus has led to considerable confusion about the notion of\ninterpretability. In particular, it is unclear how the wide array of proposed\ninterpretation methods are related, and what common concepts can be used to\nevaluate them.\n  We aim to address these concerns by defining interpretability in the context\nof machine learning and introducing the Predictive, Descriptive, Relevant (PDR)\nframework for discussing interpretations. The PDR framework provides three\noverarching desiderata for evaluation: predictive accuracy, descriptive\naccuracy and relevancy, with relevancy judged relative to a human audience.\nMoreover, to help manage the deluge of interpretation methods, we introduce a\ncategorization of existing techniques into model-based and post-hoc categories,\nwith sub-groups including sparsity, modularity and simulatability. To\ndemonstrate how practitioners can use the PDR framework to evaluate and\nunderstand interpretations, we provide numerous real-world examples. These\nexamples highlight the often under-appreciated role played by human audiences\nin discussions of interpretability. Finally, based on our framework, we discuss\nlimitations of existing methods and directions for future work. We hope that\nthis work will provide a common vocabulary that will make it easier for both\npractitioners and researchers to discuss and choose from the full range of\ninterpretation methods. \n\n"}
{"id": "1901.05070", "contents": "Title: An Inattention Model for Traveler Behavior with e-Coupons Abstract: In this study, we consider traveler coupon redemption behavior from the\nperspective of an urban mobility service. Assuming traveler behavior is in\naccordance with the principle of utility maximization, we first formulate a\nbaseline dynamical model for traveler's expected future trip sequence under the\nframework of Markov decision processes and from which we derive approximations\nof the optimal coupon redemption policy. However, we find that this baseline\nmodel cannot explain perfectly observed coupon redemption behavior of traveler\nfor a car-sharing service. To resolve this deviation from utility-maximizing\nbehavior, we suggest a hypothesis that travelers may not be aware of all\ncoupons available to them. Based on this hypothesis, we formulate an\ninattention model on unawareness, which is complementary to the existing models\nof inattention, and incorporate it into the baseline model. Estimation results\nshow that the proposed model better explains the coupon redemption dataset than\nthe baseline model. We also conduct a simulation experiment to quantify the\nnegative impact of unawareness on coupons' promotional effects. These results\ncan be used by mobility service operators to design effective coupon\ndistribution schemes in practice. \n\n"}
{"id": "1901.05191", "contents": "Title: Multivariate mixed membership modeling: Inferring domain-specific risk\n  profiles Abstract: Characterizing the shared memberships of individuals in a classification\nscheme poses severe interpretability issues, even when using a moderate number\nof classes (say 4). Mixed membership models quantify this phenomenon, but they\ntypically focus on goodness-of-fit more than on interpretable inference. To\nachieve a good numerical fit, these models may in fact require many extreme\nprofiles, making the results difficult to interpret. We introduce a new class\nof multivariate mixed membership models that, when variables can be partitioned\ninto subject-matter based domains, can provide a good fit to the data using\nfewer profiles than standard formulations. The proposed model explicitly\naccounts for the blocks of variables corresponding to the distinct domains\nalong with a cross-domain correlation structure, which provides new information\nabout shared membership of individuals in a complex classification scheme. We\nspecify a multivariate logistic normal distribution for the membership vectors,\nwhich allows easy introduction of auxiliary information leveraging a latent\nmultivariate logistic regression. A Bayesian approach to inference, relying on\nP\\'olya gamma data augmentation, facilitates efficient posterior computation\nvia Markov Chain Monte Carlo. We apply this methodology to a spatially explicit\nstudy of malaria risk over time on the Brazilian Amazon frontier. \n\n"}
{"id": "1901.05722", "contents": "Title: Prediction of the 2019 IHF World Men's Handball Championship - An\n  underdispersed sparse count data regression model Abstract: In this work, we compare several different modeling approaches for count data\napplied to the scores of handball matches with regard to their predictive\nperformances based on all matches from the four previous IHF World Men's\nHandball Championships 2011 - 2017: (underdispersed) Poisson regression models,\nGaussian response models and negative binomial models. All models are based on\nthe teams' covariate information. Within this comparison, the Gaussian response\nmodel turns out to be the best-performing prediction method on the training\ndata and is, therefore, chosen as the final model. Based on its estimates, the\nIHF World Men's Handball Championship 2019 is simulated repeatedly and winning\nprobabilities are obtained for all teams. The model clearly favors Denmark\nbefore France. Additionally, we provide survival probabilities for all teams\nand at all tournament stages as well as probabilities for all teams to qualify\nfor the main round. \n\n"}
{"id": "1901.07504", "contents": "Title: Bayesian additive regression trees and the General BART model Abstract: Bayesian additive regression trees (BART) is a flexible prediction\nmodel/machine learning approach that has gained widespread popularity in recent\nyears. As BART becomes more mainstream, there is an increased need for a paper\nthat walks readers through the details of BART, from what it is to why it\nworks. This tutorial is aimed at providing such a resource. In addition to\nexplaining the different components of BART using simple examples, we also\ndiscuss a framework, the General BART model, that unifies some of the recent\nBART extensions, including semiparametric models, correlated outcomes,\nstatistical matching problems in surveys, and models with weaker distributional\nassumptions. By showing how these models fit into a single framework, we hope\nto demonstrate a simple way of applying BART to research problems that go\nbeyond the original independent continuous or binary outcomes framework. \n\n"}
{"id": "1901.08941", "contents": "Title: Computational landscape of user behavior on social media Abstract: With the increasing abundance of 'digital footprints' left by human\ninteractions in online environments, e.g., social media and app use, the\nability to model complex human behavior has become increasingly possible. Many\napproaches have been proposed, however, most previous model frameworks are\nfairly restrictive. We introduce a new social modeling approach that enables\nthe creation of models directly from data with minimal a priori restrictions on\nthe model class. In particular, we infer the minimally complex, maximally\npredictive representation of an individual's behavior when viewed in isolation\nand as driven by a social input. We then apply this framework to a\nheterogeneous catalog of human behavior collected from fifteen thousand users\non the microblogging platform Twitter. The models allow us to describe how a\nuser processes their past behavior and their social inputs. Despite the\ndiversity of observed user behavior, most models inferred fall into a small\nsubclass of all possible finite-state processes. Thus, our work demonstrates\nthat user behavior, while quite complex, belies simple underlying computational\nstructures. \n\n"}
{"id": "1901.09729", "contents": "Title: Estimation and simulation of the transaction arrival process in intraday\n  electricity markets Abstract: We examine the novel problem of the estimation of transaction arrival\nprocesses in the intraday electricity markets. We model the inter-arrivals\nusing multiple time-varying parametric densities based on the generalized F\ndistribution estimated by maximum likelihood. We analyse both the in-sample\ncharacteristics and the probabilistic forecasting performance. In a rolling\nwindow forecasting study, we simulate many trajectories to evaluate the\nforecasts and gain significant insights into the model fit. The prediction\naccuracy is evaluated by a functional version of the MAE (mean absolute error),\nRMSE (root mean squared error) and CRPS (continuous ranked probability score)\nfor the simulated count processes. This paper fills the gap in the literature\nregarding the intensity estimation of transaction arrivals and is a major\ncontribution to the topic, yet leaves much of the field for further\ndevelopment. The study presented in this paper is conducted based on the German\nIntraday Continuous electricity market data, but this method can be easily\napplied to any other continuous intraday electricity market. For the German\nmarket, a specific generalized gamma distribution setup explains the overall\nbehaviour significantly best, especially as the tail behaviour of the process\nis well covered. \n\n"}
{"id": "1901.10225", "contents": "Title: Centered Partition Process: Informative Priors for Clustering Abstract: There is a very rich literature proposing Bayesian approaches for clustering\nstarting with a prior probability distribution on partitions. Most approaches\nassume exchangeability, leading to simple representations in terms of\nExchangeable Partition Probability Functions (EPPF). Gibbs-type priors\nencompass a broad class of such cases, including Dirichlet and Pitman-Yor\nprocesses. Even though there have been some proposals to relax the\nexchangeability assumption, allowing covariate-dependence and partial\nexchangeability, limited consideration has been given on how to include\nconcrete prior knowledge on the partition. For example, we are motivated by an\nepidemiological application, in which we wish to cluster birth defects into\ngroups and we have prior knowledge of an initial clustering provided by\nexperts. As a general approach for including such prior knowledge, we propose a\nCentered Partition (CP) process that modifies the EPPF to favor partitions\nclose to an initial one. Some properties of the CP prior are described, a\ngeneral algorithm for posterior computation is developed, and we illustrate the\nmethodology through simulation examples and an application to the motivating\nepidemiology study of birth defects. \n\n"}
{"id": "astro-ph/0606593", "contents": "Title: Asymmetric Spatiotemporal Evolution of Prebiotic Homochirality Abstract: The role of asymmetry on the evolution of prebiotic homochirality is\ninvestigated in the context of autocatalytic polymerization reaction networks.\nA model featuring enantiometric cross-inhibition and chiral bias is used to\nstudy the diffusion equations controlling the spatiotemporal development of\nleft and right-handed domains. Bounds on the chiral bias are obtained\nconsistent with present-day constraints on the emergence of life on early\nEarth. The viability of biasing mechanisms such as weak neutral currents and\ncircularly polarized UV light is discussed. The results can be applied to any\nhypothetical planetary platform. \n\n"}
{"id": "cond-mat/0101149", "contents": "Title: Elastically-Driven Linker Aggregation between Two Semi-Flexible\n  Polyelectrolytes Abstract: The behavior of mobile linkers connecting two semi-flexible charged polymers,\nsuch as polyvalent counterions connecting DNA or F-actin chains, is studied\ntheoretically. The chain bending rigidity induces an effective repulsion\nbetween linkers at large distances while the inter-chain electrostatic\nrepulsion leads to an effective short range inter-linker attraction. We find a\nrounded phase transition from a dilute linker gas where the chains form large\nloops between linkers to a dense disordered linker fluid connecting parallel\nchains. The onset of chain pairing occurs within the rounded transition. \n\n"}
{"id": "cond-mat/0111291", "contents": "Title: Folding of a Small Helical Protein Using Hydrogen Bonds and\n  Hydrophobicity Forces Abstract: A reduced protein model with five to six atoms per amino acid and five amino\nacid types is developed and tested on a three-helix-bundle protein, a 46-amino\nacid fragment from staphylococcal protein A. The model does not rely on the\nwidely used Go approximation where non-native interactions are ignored. We find\nthat the collapse transition is considerably more abrupt for the protein A\nsequence than for random sequences with the same composition. The chain\ncollapse is found to be at least as fast as helix formation. Energy\nminimization restricted to the thermodynamically favored topology gives a\nstructure that has a root-mean-square deviation of 1.8 A from the native\nstructure. The sequence-dependent part of our potential is pairwise additive.\nOur calculations suggest that fine-tuning this potential by parameter\noptimization is of limited use. \n\n"}
{"id": "cond-mat/0602232", "contents": "Title: Low frequency limit for thermally activated escape with periodic driving Abstract: The period-average rate in the low frequency limit for thermally activated\nescape with periodic driving is derived in a closed analytical form. We define\nthe low frequency limit as the one where there is no essential dependence on\nfrequency so that the formal limit $\\Omega \\to 0$ in the appropriate equations\ncan be taken. We develop a perturbation theory of the action in the modulation\namplitude and obtain a cumbersom but closed and tractable formula for arbitrary\nvalues of the modulation ampitude to noise intensity ratio $A/D$ except a\nnarrow region near the bifurcation point and a simple analytical formula for\nthe limiting case of moderately strong modulation. The present theory yields\nanalytical description for the retardation of the exponential growth of the\nescape rate enhancement (i.e., transition from a log-linear regime to more\nmoderate growth and even reverse behavior). The theory is developed for an\narbitrary potential with an activation barrier but is exemplified by the cases\nof cubic (metastable) and quartic (bistable) potentials. \n\n"}
{"id": "cond-mat/9910162", "contents": "Title: Binding of molecules to DNA and other semiflexible polymers Abstract: A theory is presented for the binding of small molecules such as surfactants\nto semiflexible polymers. The persistence length is assumed to be large\ncompared to the monomer size but much smaller than the total chain length. Such\npolymers (e.g. DNA) represent an intermediate case between flexible polymers\nand stiff, rod-like ones, whose association with small molecules was previously\nstudied. The chains are not flexible enough to actively participate in the\nself-assembly, yet their fluctuations induce long-range attractive interactions\nbetween bound molecules. In cases where the binding significantly affects the\nlocal chain stiffness, those interactions lead to a very sharp, cooperative\nassociation. This scenario is of relevance to the association of DNA with\nsurfactants and compact proteins such as RecA. External tension exerted on the\nchain is found to significantly modify the binding by suppressing the\nfluctuation-induced interaction. \n\n"}
{"id": "physics/0004040", "contents": "Title: The Physical Origin of Intrinsic Bends in Double Helical DNA Abstract: The macroscopic curvature induced in the double helical B-DNA by regularly\nrepeated adenine tracts (A-tracts) is a long known, but still unexplained\nphenomenon. This effect plays a key role in DNA studies because it is unique in\nthe amount and the variety of the available experimental information and,\ntherefore, is likely to serve as a gate to the unknown general mechanisms of\nrecognition and regulation of genome sequences. We report the results of\nmolecular dynamics simulations of a 25-mer B-DNA fragment with a sequence\nincluding three A-tract phased with the helical screw. It represents the first\nmodel system where properly directed static curvature emerges spontaneously in\nconditions excluding any initial bias except the base pair sequence. The effect\nhas been reproduced in three independent MD trajectories of 10-20 ns with\nimportant qualitative details suggesting that the final bent state is a strong\nattractor of trajectories form a broad domain of the conformational space. The\nensemble of curved conformations, however, reveals significant microscopic\nheterogeneity in contradiction to all existing theoretical models of bending.\nAnalysis of these unexpected observations leads to a new, significantly\ndifferent hypothesis of the possible mechanism of intrinsic bends in the double\nhelical DNA. \n\n"}
{"id": "physics/0004049", "contents": "Title: Molecular Dynamics Studies of Sequence-directed Curvature in Bending\n  Locus of Trypanosome Kinetoplast DNA Abstract: The macroscopic curvature induced in the double helical B-DNA by regularly\nrepeated adenine tracts (A-tracts) plays an exceptional role in structural\nstudies of DNA because this effect presents the most well-documented example of\nsequence specific conformational modulations. Recently, a new hypothesis of its\nphysical origin has been put forward, based upon the results of molecular\ndynamics simulations of a 25-mer fragment with three A-tracts phased with the\nhelical screw. Its sequence, however, had never been encountered in\nexperimental studies, but was constructed empirically so as to maximize the\nmagnitude of bending in specific computational conditions. Here we report the\nresults of a similar investigation of another 25-mer B-DNA fragment now with a\nnatural base pair sequence found in a bent locus of a minicircle DNA. It is\nshown that the static curvature of a considerable magnitude and stable\ndirection towards the minor grooves of A-tracts emerges spontaneously in\nconditions excluding any initial bias except the base pair sequence. Comparison\nof the bending dynamics of these two DNA fragments reveals both qualitative\nsimilarities and interesting differences. The results suggest that the A-tract\ninduced bending obtained in simulations reproduces the natural phenomenon and\nvalidates the earlier conclusions concerning its possible mechanism. \n\n"}
{"id": "physics/0203002", "contents": "Title: Finding an Upper Limit in the Presence of Unknown Background Abstract: Experimenters report an upper limit if the signal they are trying to detect\nis non-existent or below their experiment's sensitivity. Such experiments may\nbe contaminated with a background too poorly understood to subtract. If the\nbackground is distributed differently in some parameter from the expected\nsignal, it is possible to take advantage of this difference to get a stronger\nlimit than would be possible if the difference in distribution were ignored. We\ndiscuss the ``Maximum Gap'' method, which finds the best gap between events for\nsetting an upper limit, and generalize to ``Optimum Interval'' methods, which\nuse intervals with especially few events. These methods, which apply to the\ncase of relatively small backgrounds, do not use binning, are relatively\ninsensitive to cuts on the range of the parameter, are parameter independent\n(i.e., do not change when a one-one change of variables is made), and provide\ntrue, though possibly conservative, classical one-sided confidence intervals. \n\n"}
{"id": "physics/0302103", "contents": "Title: Unbiased simulation of structural transitions in calmodulin Abstract: We introduce an approach for performing \"very long\" computer simulations of\nthe dynamics of simplified, folded proteins. Using an alpha-carbon protein\nmodel and a fine grid to mimic continuum computations at increased speed, we\nperform unbiased simulations which exhibit many large-scale conformational\ntransitions at low cost. In the case of the 72-residue N-terminal domain of\ncalmodulin, the approach yields structural transitions between the calcium-free\nand calcium-bound structures at a rate of roughly one per day on a single Intel\nprocessor. Stable intermediates can be clearly characterized. The model employs\nGo-like interactions to stabilize two (or more) experimentally-determined\nstructures. The approach is trivially parallelizable and readily generalizes to\nmore complex potentials at minimal cost. \n\n"}
{"id": "physics/0406051", "contents": "Title: Effects of T2 Relaxation and Diffusion on Longitudinal Magnetization\n  State and Signal Build for HOMOGENIZED Cross Peaks Abstract: An analytical expression has been developed to describe the effects of T2\nrelaxation and diffusing spatially modulated longitudinal spins during the\nsignal build period of an HOMOGENIZED cross peak. Diffusion of the longitudinal\nspins results in a lengthening of the effective dipolar demagnetization time,\ndelaying the re-phasing of coupled anti-phase states in the quantum picture. In\nthe classical picture the unwinding rate of spatially twisted magnetization is\nno longer constant, but decays exponentially with time. The expression is\nexperimentally verified for the HOMOGENIZED spectrum of 100mM TSP in H2O at\n4.7T.\n  More Keywords: magnetic resonance spectroscopy, MRS, intermolecular multiple\nquantum coherence, iMQC, intermolecular zero quantum coherence, iZQC, distant\ndipolar field, DDF \n\n"}
{"id": "physics/0505009", "contents": "Title: Scaling in Complex Systems: Analytical Theory of Charged Pores Abstract: In this paper we find an analytical solution of the equilibrium ion\ndistribution for a toroidal model of a ionic channel, using the Perfect\nScreening Theorem (PST). The ions are charged hard spheres, and are treated\nusing a variational Mean Spherical Approximation (VMSA) .\n  Understanding ion channels is still a very open problem, because of the many\nexquisite tuning details of real life channels. It is clear that the electric\nfield plays a major role in the channel behaviour, and for that reason there\nhas been a lot of work on simple models that are able to provide workable\ntheories. Recently a number of interesting papers have appeared that discuss\nmodels in which the effect of the geometry, excluded volume and non-linear\nbehaviour is considered.\n  We present here a 3D model of ionic channels which consists of a charged,\ndeformable torus with a circular or elliptical cross section, which can be flat\nor vertical (close to a cylinder). Extensive comparisons to MC simulations were\nperformed.\n  The new solution opens new possibilities, such as studying flexible pores,\nand water phase transformations inside the pores using an approach similar to\nthat used on flat crystal surfaces . \n\n"}
{"id": "physics/0508112", "contents": "Title: Gene regulatory networks: a coarse-grained, equation-free approach to\n  multiscale computation Abstract: We present computer-assisted methods for analyzing stochastic models of gene\nregulatory networks. The main idea that underlies this equation-free analysis\nis the design and execution of appropriately-initialized short bursts of\nstochastic simulations; the results of these are processed to estimate\ncoarse-grained quantities of interest, such as mesoscopic transport\ncoefficients. In particular, using a simple model of a genetic toggle switch,\nwe illustrate the computation of an effective free energy and of a\nstate-dependent effective diffusion coefficient that characterize an\nunavailable effective Fokker-Planck equation. Additionally we illustrate the\nlinking of equation-free techniques with continuation methods for performing a\nform of stochastic \"bifurcation analysis\"; estimation of mean switching times\nin the case of a bistable switch is also implemented in this equation-free\ncontext. The accuracy of our methods is tested by direct comparison with\nlong-time stochastic simulations. This type of equation-free analysis appears\nto be a promising approach to computing features of the long-time,\ncoarse-grained behavior of certain classes of complex stochastic models of gene\nregulatory networks, circumventing the need for long Monte Carlo simulations. \n\n"}
{"id": "physics/0511026", "contents": "Title: Conformational changes in glycine tri- and hexapeptide Abstract: We have investigated the potential energy surfaces for glycine chains\nconsisting of three and six amino acids. For these molecules we have calculated\npotential energy surfaces as a function of the Ramachandran angles phi and psi,\nwhich are widely used for the characterization of the polypeptide chains. These\nparticular degrees of freedom are essential for the characterization of\nproteins folding process. Calculations have been carried out within ab initio\ntheoretical framework based on the density functional theory and accounting for\nall the electrons in the system. We have determined stable conformations and\ncalculated the energy barriers for transitions between them. Using a\nthermodynamic approach, we have estimated the times of the characteristic\ntransitions between these conformations. The results of our calculations have\nbeen compared with those obtained by other theoretical methods and with the\navailable experimental data extracted from the Protein Data Base. This\ncomparison demonstrates a reasonable correspondence of the most prominent\nminima on the calculated potential energy surfaces to the experimentally\nmeasured angles phi and psi for the glycine chains appearing in native\nproteins. We have also investigated the influence of the secondary structure of\npolypeptide chains on the formation of the potential energy landscape. This\nanalysis has been performed for the sheet and the helix conformations of chains\nof six amino acids. \n\n"}
{"id": "physics/0511036", "contents": "Title: Ab initio study of alanine polypeptide chains twisting Abstract: We have investigated the potential energy surfaces for alanine chains\nconsisting of three and six amino acids. For these molecules we have calculated\npotential energy surfaces as a function of the Ramachandran angles Phi and Psi,\nwhich are widely used for the characterization of the polypeptide chains. These\nparticular degrees of freedom are essential for the characterization of\nproteins folding process. Calculations have been carried out within ab initio\ntheoretical framework based on the density functional theory and accounting for\nall the electrons in the system. We have determined stable conformations and\ncalculated the energy barriers for transitions between them. Using a\nthermodynamic approach, we have estimated the times of characteristic\ntransitions between these conformations. The results of our calculations have\nbeen compared with those obtained by other theoretical methods and with the\navailable experimental data extracted from the Protein Data Base. This\ncomparison demonstrates a reasonable correspondence of the most prominent\nminima on the calculated potential energy surfaces to the experimentally\nmeasured angles Phi and Psi for alanine chains appearing in native proteins. We\nhave also investigated the influence of the secondary structure of polypeptide\nchains on the formation of the potential energy landscape. This analysis has\nbeen performed for the sheet and the helix conformations of chains of six amino\nacids. \n\n"}
{"id": "physics/0601093", "contents": "Title: Resolution exchange simulation with incremental coarsening Abstract: We previously developed an algorithm, called resolution exchange, which\nimproves canonical sampling of atomic resolution models by swapping\nconformations between high- and low-resolution simulations[1]. Here, we\ndemonstrate a generally applicable incremental coarsening procedure and apply\nthe algorithm to a larger peptide, met-enkephalin. In addition, we demonstrate\na combination of resolution and temperature exchange, in which the coarser\nsimulations are also at elevated temperatures. Both simulations are implemented\nin a ``top-down'' mode, to allow efficient allocation of CPU time among the\ndifferent replicas. \n\n"}
{"id": "physics/0702148", "contents": "Title: Reliability of rank order in sampled networks Abstract: In complex scale-free networks, ranking the individual nodes based upon their\nimportance has useful applications, such as the identification of hubs for\nepidemic control, or bottlenecks for controlling traffic congestion. However,\nin most real situations, only limited sub-structures of entire networks are\navailable, and therefore the reliability of the order relationships in sampled\nnetworks requires investigation. With a set of randomly sampled nodes from the\nunderlying original networks, we rank individual nodes by three centrality\nmeasures: degree, betweenness, and closeness. The higher-ranking nodes from the\nsampled networks provide a relatively better characterisation of their ranks in\nthe original networks than the lower-ranking nodes. A closeness-based order\nrelationship is more reliable than any other quantity, due to the global nature\nof the closeness measure. In addition, we show that if access to hubs is\nlimited during the sampling process, an increase in the sampling fraction can\nin fact decrease the sampling accuracy. Finally, an estimation method for\nassessing sampling accuracy is suggested. \n\n"}
{"id": "physics/9907028", "contents": "Title: A Minimal Model of B-DNA Abstract: Recently it has been found that stable and accurate molecular dynamics (MD)\nof B-DNA duplexes can be obtained in relatively inexpensive computational\nconditions with the bulk solvent represented implicitly, but the minor groove\nfilled with explicit water (J. Am. Chem. Soc. 1998, 120, 10928). The present\npaper further explores these simulation conditions in order to understand the\nmain factors responsible for the observed surprisingly good agreement with\nexperimental data. It appears that in the case of the EcoRI dodecamer certain\nsequence specific hydration patterns in the minor groove earlier known from\nexperimental data are formed spontaneously in the course of MD simulations. The\neffect is reliably reproduced in several independent computational experiments\nin different simulation conditions. With all major groove water removed,\nclosely similar results are obtained, with even better reproducibility. On the\nother hand, without explicit hydration, metastable dynamics around a B-DNA like\nstate can be obtained which, however, only poorly compares with experimental\ndata. It appears, therefore, that a right-handed DNA helix with explicitly\nhydrated minor groove is a minimal model system where the experimental\nproperties of B-DNA can be reproduced with non-trivial sequence-dependent\neffects. Its small size makes possible virtually exhaustive sampling, which is\na major advantage with respect to alternative approaches. An appendix is\nincluded with a correction to the implicit leapfrog integrator used in internal\ncoordinate MD. \n\n"}
{"id": "physics/9912053", "contents": "Title: Effective interaction between helical bio-molecules Abstract: The effective interaction between two parallel strands of helical\nbio-molecules, such as deoxyribose nucleic acids (DNA), is calculated using\ncomputer simulations of the \"primitive\" model of electrolytes. In particular we\nstudy a simple model for B-DNA incorporating explicitly its charge pattern as a\ndouble-helix structure. The effective force and the effective torque exerted\nonto the molecules depend on the central distance and on the relative\norientation. The contributions of nonlinear screening by monovalent counterions\nto these forces and torques are analyzed and calculated for different salt\nconcentrations. As a result, we find that the sign of the force depends\nsensitively on the relative orientation. For intermolecular distances smaller\nthan $6\\AA$ it can be both attractive and repulsive. Furthermore we report a\nnonmonotonic behaviour of the effective force for increasing salt\nconcentration. Both features cannot be described within linear screening\ntheories. For large distances, on the other hand, the results agree with linear\nscreening theories provided the charge of the bio-molecules is suitably\nrenormalized. \n\n"}
{"id": "q-bio/0401030", "contents": "Title: A First Principles Density-Functional Calculation of the Electronic and\n  Vibrational Structure of the Key Melanin Monomers Abstract: We report first principles density functional calculations for hydroquinone\n(HQ), indolequinone (IQ) and semiquinone (SQ). These molecules are believed to\nbe the basic building blocks of the eumelanins, a class of bio-macromolecules\nwith important biological functions (including photoprotection) and with\npotential for certain bioengineering applications. We have used the DeltaSCF\n(difference of self consistent fields) method to study the energy gap between\nthe highest occupied molecular orbital (HOMO) and the lowest unoccupied\nmolecular orbital (LUMO), Delta_HL. We show that Delta_HL is similar in IQ and\nSQ but approximately twice as large in HQ. This may have important implications\nfor our understanding of the observed broad band optical absorption of the\neumelanins. The possibility of using this difference in Delta_HL to molecularly\nengineer the electronic properties of eumelanins is discussed. We calculate the\ninfrared and Raman spectra of the three redox forms from first principles. Each\nof the molecules have significantly different infrared and Raman signatures,\nand so these spectra could be used in situ to non-destructively identify the\nmonomeric content of macromolecules. It is hoped that this may be a helpful\nanalytical tool in determining the structure of eumelanin macromolecules and\nhence in helping to determine the structure-property-function relationships\nthat control the behaviour of the eumelanins. \n\n"}
{"id": "q-bio/0408018", "contents": "Title: 5,6-dihydroxyindole-2-carboxylic acid (DHICA): a First Principles\n  Density-Functional Study Abstract: We report first principles density functional calculations for\n5,6-dihydroxyindole-2-carboxylic acid (DHICA) and several reduced forms. DHICA\nand 5,6-dihydroxyindole (DHI) are believed to be the basic building blocks of\nthe eumelanins. Our results show that carboxylation has a significant effect on\nthe physical properties of the molecules. In particular, the relative\nstabilities and the HOMO-LUMO gaps (calculated with the $\\Delta$SCF method) of\nthe various redox forms are strongly affected. We predict that, in contrast to\nDHI, the density of unpaired electrons, and hence the ESR signal, in DHICA is\nnegligibly small. \n\n"}
