{"id": "0704.2902", "contents": "Title: Recommending Related Papers Based on Digital Library Access Records Abstract: An important goal for digital libraries is to enable researchers to more\neasily explore related work. While citation data is often used as an indicator\nof relatedness, in this paper we demonstrate that digital access records (e.g.\nhttp-server logs) can be used as indicators as well. In particular, we show\nthat measures based on co-access provide better coverage than co-citation, that\nthey are available much sooner, and that they are more accurate for recent\npapers. \n\n"}
{"id": "0710.2228", "contents": "Title: Recommendation model based on opinion diffusion Abstract: Information overload in the modern society calls for highly efficient\nrecommendation algorithms. In this letter we present a novel diffusion based\nrecommendation model, with users' ratings built into a transition matrix. To\nspeed up computation we introduce a Green function method. The numerical tests\non a benchmark database show that our prediction is superior to the standard\nrecommendation methods. \n\n"}
{"id": "0802.1296", "contents": "Title: On quantum statistics in data analysis Abstract: Originally, quantum probability theory was developed to analyze statistical\nphenomena in quantum systems, where classical probability theory does not\napply, because the lattice of measurable sets is not necessarily distributive.\nOn the other hand, it is well known that the lattices of concepts, that arise\nin data analysis, are in general also non-distributive, albeit for completely\ndifferent reasons. In his recent book, van Rijsbergen argues that many of the\nlogical tools developed for quantum systems are also suitable for applications\nin information retrieval. I explore the mathematical support for this idea on\nan abstract vector space model, covering several forms of data analysis\n(information retrieval, data mining, collaborative filtering, formal concept\nanalysis...), and roughly based on an idea from categorical quantum mechanics.\nIt turns out that quantum (i.e., noncommutative) probability distributions\narise already in this rudimentary mathematical framework. We show that a\nBell-type inequality must be satisfied by the standard similarity measures, if\nthey are used for preference predictions. The fact that already a very general,\nabstract version of the vector space model yields simple counterexamples for\nsuch inequalities seems to be an indicator of a genuine need for quantum\nstatistics in data analysis. \n\n"}
{"id": "0901.4571", "contents": "Title: Everyone is a Curator: Human-Assisted Preservation for ORE Aggregations Abstract: The Open Archives Initiative (OAI) has recently created the Object Reuse and\nExchange (ORE) project that defines Resource Maps (ReMs) for describing\naggregations of web resources. These aggregations are susceptible to many of\nthe same preservation challenges that face other web resources. In this paper,\nwe investigate how the aggregations of web resources can be preserved outside\nof the typical repository environment and instead rely on the thousands of\ninteractive users in the web community and the Web Infrastructure (the\ncollection of web archives, search engines, and personal archiving services) to\nfacilitate preservation. Inspired by Web 2.0 services such as digg,\ndeli.cio.us, and Yahoo! Buzz, we have developed a lightweight system called\nReMember that attempts to harness the collective abilities of the web community\nfor preservation purposes instead of solely placing the burden of curatorial\nresponsibilities on a small number of experts. \n\n"}
{"id": "0903.0034", "contents": "Title: Measuring Independence of Datasets Abstract: A data stream model represents setting where approximating pairwise, or\n$k$-wise, independence with sublinear memory is of considerable importance. In\nthe streaming model the joint distribution is given by a stream of $k$-tuples,\nwith the goal of testing correlations among the components measured over the\nentire stream. In the streaming model, Indyk and McGregor (SODA 08) recently\ngave exciting new results for measuring pairwise independence. The Indyk and\nMcGregor methods provide $\\log{n}$-approximation under statistical distance\nbetween the joint and product distributions in the streaming model. Indyk and\nMcGregor leave, as their main open question, the problem of improving their\n$\\log n$-approximation for the statistical distance metric.\n  In this paper we solve the main open problem posed by of Indyk and McGregor\nfor the statistical distance for pairwise independence and extend this result\nto any constant $k$. In particular, we present an algorithm that computes an\n$(\\epsilon, \\delta)$-approximation of the statistical distance between the\njoint and product distributions defined by a stream of $k$-tuples. Our\nalgorithm requires $O(({1\\over \\epsilon}\\log({nm\\over \\delta}))^{(30+k)^k})$\nmemory and a single pass over the data stream. \n\n"}
{"id": "0904.2012", "contents": "Title: Simplicial Databases Abstract: In this paper, we define a category DB, called the category of simplicial\ndatabases, whose objects are databases and whose morphisms are data-preserving\nmaps. Along the way we give a precise formulation of the category of relational\ndatabases, and prove that it is a full subcategory of DB. We also prove that\nlimits and colimits always exist in DB and that they correspond to queries such\nas select, join, union, etc.\n  One feature of our construction is that the schema of a simplicial database\nhas a natural geometric structure: an underlying simplicial set. The geometry\nof a schema is a way of keeping track of relationships between distinct tables,\nand can be thought of as a system of foreign keys. The shape of a schema is\ngenerally intuitive (e.g. the schema for round-trip flights is a circle\nconsisting of an edge from $A$ to $B$ and an edge from $B$ to $A$), and as\nsuch, may be useful for analyzing data.\n  We give several applications of our approach, as well as possible advantages\nit has over the relational model. We also indicate some directions for further\nresearch. \n\n"}
{"id": "0906.0612", "contents": "Title: Community detection in graphs Abstract: The modern science of networks has brought significant advances to our\nunderstanding of complex systems. One of the most relevant features of graphs\nrepresenting real systems is community structure, or clustering, i. e. the\norganization of vertices in clusters, with many edges joining vertices of the\nsame cluster and comparatively few edges joining vertices of different\nclusters. Such clusters, or communities, can be considered as fairly\nindependent compartments of a graph, playing a similar role like, e. g., the\ntissues or the organs in the human body. Detecting communities is of great\nimportance in sociology, biology and computer science, disciplines where\nsystems are often represented as graphs. This problem is very hard and not yet\nsatisfactorily solved, despite the huge effort of a large interdisciplinary\ncommunity of scientists working on it over the past few years. We will attempt\na thorough exposition of the topic, from the definition of the main elements of\nthe problem, to the presentation of most methods developed, with a special\nfocus on techniques designed by statistical physicists, from the discussion of\ncrucial issues like the significance of clustering and how methods should be\ntested and compared against each other, to the description of applications to\nreal networks. \n\n"}
{"id": "0908.3280", "contents": "Title: On the Relationship between Trading Network and WWW Network: A\n  Preferential Attachment Perspective Abstract: This paper describes the relationship between trading network and WWW network\nfrom preferential attachment mechanism perspective. This mechanism is known to\nbe the underlying principle in the network evolution and has been incorporated\nto formulate two famous web pages ranking algorithms, PageRank and HITS. We\npoint out the differences between trading network and WWW network in this\nmechanism, derive the formulation of HITS-based ranking algorithm for trading\nnetwork as a direct consequence of the differences, and apply the same\nframework when deriving the formulation back to the HITS formulation that turns\nto become a technique to accelerate its convergences. \n\n"}
{"id": "1001.2186", "contents": "Title: Building reputation systems for better ranking Abstract: How to rank web pages, scientists and online resources has recently attracted\nincreasing attention from both physicists and computer scientists. In this\npaper, we study the ranking problem of rating systems where users vote objects\nby discrete ratings. We propose an algorithm that can simultaneously evaluate\nthe user reputation and object quality in an iterative refinement way.\nAccording to both the artificially generated data and the real data from\nMovieLens and Amazon, our algorithm can considerably enhance the ranking\naccuracy. This work highlights the significance of reputation systems in the\nInternet era and points out a way to evaluate and compare the performances of\ndifferent reputation systems. \n\n"}
{"id": "1002.3238", "contents": "Title: Exploring a Multidimensional Representation of Documents and Queries\n  (extended version) Abstract: In Information Retrieval (IR), whether implicitly or explicitly, queries and\ndocuments are often represented as vectors. However, it may be more beneficial\nto consider documents and/or queries as multidimensional objects. Our belief is\nthis would allow building \"truly\" interactive IR systems, i.e., where\ninteraction is fully incorporated in the IR framework.\n  The probabilistic formalism of quantum physics represents events and\ndensities as multidimensional objects. This paper presents our first step\ntowards building an interactive IR framework upon this formalism, by stating\nhow the first interaction of the retrieval process, when the user types a\nquery, can be formalised. Our framework depends on a number of parameters\naffecting the final document ranking. In this paper we experimentally\ninvestigate the effect of these parameters, showing that the proposed\nrepresentation of documents and queries as multidimensional objects can compete\nwith standard approaches, with the additional prospect to be applied to\ninteractive retrieval. \n\n"}
{"id": "1002.4658", "contents": "Title: Principal Component Analysis with Contaminated Data: The High\n  Dimensional Case Abstract: We consider the dimensionality-reduction problem (finding a subspace\napproximation of observed data) for contaminated data in the high dimensional\nregime, where the number of observations is of the same magnitude as the number\nof variables of each observation, and the data set contains some (arbitrarily)\ncorrupted observations. We propose a High-dimensional Robust Principal\nComponent Analysis (HR-PCA) algorithm that is tractable, robust to contaminated\npoints, and easily kernelizable. The resulting subspace has a bounded deviation\nfrom the desired one, achieves maximal robustness -- a breakdown point of 50%\nwhile all existing algorithms have a breakdown point of zero, and unlike\nordinary PCA algorithms, achieves optimality in the limit case where the\nproportion of corrupted points goes to zero. \n\n"}
{"id": "1003.3661", "contents": "Title: An HTTP-Based Versioning Mechanism for Linked Data Abstract: Dereferencing a URI returns a representation of the current state of the\nresource identified by that URI. But, on the Web representations of prior\nstates of a resource are also available, for example, as resource versions in\nContent Management Systems or archival resources in Web Archives such as the\nInternet Archive. This paper introduces a resource versioning mechanism that is\nfully based on HTTP and uses datetime as a global version indicator. The\napproach allows \"follow your nose\" style navigation both from the current\ntime-generic resource to associated time-specific version resources as well as\namong version resources. The proposed versioning mechanism is congruent with\nthe Architecture of the World Wide Web, and is based on the Memento framework\nthat extends HTTP with transparent content negotiation in the datetime\ndimension. The paper shows how the versioning approach applies to Linked Data,\nand by means of a demonstrator built for DBpedia, it also illustrates how it\ncan be used to conduct a time-series analysis across versions of Linked Data\ndescriptions. \n\n"}
{"id": "1003.4146", "contents": "Title: A Mathematical Approach to the Study of the United States Code Abstract: The United States Code (Code) is a document containing over 22 million words\nthat represents a large and important source of Federal statutory law. Scholars\nand policy advocates often discuss the direction and magnitude of changes in\nvarious aspects of the Code. However, few have mathematically formalized the\nnotions behind these discussions or directly measured the resulting\nrepresentations. This paper addresses the current state of the literature in\ntwo ways. First, we formalize a representation of the United States Code as the\nunion of a hierarchical network and a citation network over vertices containing\nthe language of the Code. This representation reflects the fact that the Code\nis a hierarchically organized document containing language and explicit\ncitations between provisions. Second, we use this formalization to measure\naspects of the Code as codified in October 2008, November 2009, and March 2010.\nThese measurements allow for a characterization of the actual changes in the\nCode over time. Our findings indicate that in the recent past, the Code has\ngrown in its amount of structure, interdependence, and language. \n\n"}
{"id": "1003.5455", "contents": "Title: Towards physical laws for software architecture Abstract: Starting from the pioneering works on software architecture precious\nguidelines have emerged to indicate how computer programs should be organized.\nFor example the \"separation of concerns\" suggests to split a program into\nmodules that overlap in functionality as little as possible. However these\nrecommendations are mainly conceptual and are thus hard to express in a\nquantitative form. Hence software architecture relies on the individual\nexperience and skill of the designers rather than on quantitative laws. In this\narticle I apply the methods developed for the classification of information on\nthe World-Wide-Web to study the organization of Open Source programs in an\nattempt to establish the statistical laws governing software architecture. \n\n"}
{"id": "1005.2308", "contents": "Title: Finding Your Literature Match -- A Recommender System Abstract: The universe of potentially interesting, searchable literature is expanding\ncontinuously. Besides the normal expansion, there is an additional influx of\nliterature because of interdisciplinary boundaries becoming more and more\ndiffuse. Hence, the need for accurate, efficient and intelligent search tools\nis bigger than ever. Even with a sophisticated search engine, looking for\ninformation can still result in overwhelming results. An overload of\ninformation has the intrinsic danger of scaring visitors away, and any\norganization, for-profit or not-for-profit, in the business of providing\nscholarly information wants to capture and keep the attention of its target\naudience. Publishers and search engine engineers alike will benefit from a\nservice that is able to provide visitors with recommendations that closely meet\ntheir interests. Providing visitors with special deals, new options and\nhighlights may be interesting to a certain degree, but what makes more sense\n(especially from a commercial point of view) than to let visitors do most of\nthe work by the mere action of making choices? Hiring psychics is not an\noption, so a technological solution is needed to recommend items that a visitor\nis likely to be looking for. In this presentation we will introduce such a\nsolution and argue that it is practically feasible to incorporate this approach\ninto a useful addition to any information retrieval system with enough usage. \n\n"}
{"id": "1007.3622", "contents": "Title: A generalized risk approach to path inference based on hidden Markov\n  models Abstract: Motivated by the unceasing interest in hidden Markov models (HMMs), this\npaper re-examines hidden path inference in these models, using primarily a\nrisk-based framework. While the most common maximum a posteriori (MAP), or\nViterbi, path estimator and the minimum error, or Posterior Decoder (PD), have\nlong been around, other path estimators, or decoders, have been either only\nhinted at or applied more recently and in dedicated applications generally\nunfamiliar to the statistical learning community. Over a decade ago, however, a\nfamily of algorithmically defined decoders aiming to hybridize the two standard\nones was proposed (Brushe et al., 1998). The present paper gives a careful\nanalysis of this hybridization approach, identifies several problems and issues\nwith it and other previously proposed approaches, and proposes practical\nresolutions of those. Furthermore, simple modifications of the classical\ncriteria for hidden path recognition are shown to lead to a new class of\ndecoders. Dynamic programming algorithms to compute these decoders in the usual\nforward-backward manner are presented. A particularly interesting subclass of\nsuch estimators can be also viewed as hybrids of the MAP and PD estimators.\nSimilar to previously proposed MAP-PD hybrids, the new class is parameterized\nby a small number of tunable parameters. Unlike their algorithmic predecessors,\nthe new risk-based decoders are more clearly interpretable, and, most\nimportantly, work \"out of the box\" in practice, which is demonstrated on some\nreal bioinformatics tasks and data. Some further generalizations and\napplications are discussed in conclusion. \n\n"}
{"id": "1011.5395", "contents": "Title: The Sample Complexity of Dictionary Learning Abstract: A large set of signals can sometimes be described sparsely using a\ndictionary, that is, every element can be represented as a linear combination\nof few elements from the dictionary. Algorithms for various signal processing\napplications, including classification, denoising and signal separation, learn\na dictionary from a set of signals to be represented. Can we expect that the\nrepresentation found by such a dictionary for a previously unseen example from\nthe same source will have L_2 error of the same magnitude as those for the\ngiven examples? We assume signals are generated from a fixed distribution, and\nstudy this questions from a statistical learning theory perspective.\n  We develop generalization bounds on the quality of the learned dictionary for\ntwo types of constraints on the coefficient selection, as measured by the\nexpected L_2 error in representation when the dictionary is used. For the case\nof l_1 regularized coefficient selection we provide a generalization bound of\nthe order of O(sqrt(np log(m lambda)/m)), where n is the dimension, p is the\nnumber of elements in the dictionary, lambda is a bound on the l_1 norm of the\ncoefficient vector and m is the number of samples, which complements existing\nresults. For the case of representing a new signal as a combination of at most\nk dictionary elements, we provide a bound of the order O(sqrt(np log(m k)/m))\nunder an assumption on the level of orthogonality of the dictionary (low Babel\nfunction). We further show that this assumption holds for most dictionaries in\nhigh dimensions in a strong probabilistic sense. Our results further yield fast\nrates of order 1/m as opposed to 1/sqrt(m) using localized Rademacher\ncomplexity. We provide similar results in a general setting using kernels with\nweak smoothness requirements. \n\n"}
{"id": "1101.1042", "contents": "Title: The Accelerating Growth of Online Tagging Systems Abstract: Research on the growth of online tagging systems not only is interesting in\nits own right, but also yields insights for website management and semantic web\nanalysis. Traditional models that describing the growth of online systems can\nbe divided between linear and nonlinear versions. Linear models, including the\nBA model (Brabasi and Albert, 1999), assume that the average activity of users\nis a constant independent of population. Hence the total activity is a linear\nfunction of population. On the contrary, nonlinear models suggest that the\naverage activity is affected by the size of the population and the total\nactivity is a nonlinear function of population. In the current study,\nsupporting evidences for the nonlinear growth assumption are obtained from data\non Internet users' tagging behavior. A power law relationship between the\nnumber of new tags (F) and the population (P), which can be expressed as F ~ P\n^ gamma (gamma > 1), is found. I call this pattern accelerating growth and find\nit relates the to time-invariant heterogeneity in individual activities. I also\nshow how a greater heterogeneity leads to a faster growth. \n\n"}
{"id": "1102.1027", "contents": "Title: Collective Classification of Textual Documents by Guided\n  Self-Organization in T-Cell Cross-Regulation Dynamics Abstract: We present and study an agent-based model of T-Cell cross-regulation in the\nadaptive immune system, which we apply to binary classification. Our method\nexpands an existing analytical model of T-cell cross-regulation (Carneiro et\nal. in Immunol Rev 216(1):48-68, 2007) that was used to study the\nself-organizing dynamics of a single population of T-Cells in interaction with\nan idealized antigen presenting cell capable of presenting a single antigen.\nWith agent-based modeling we are able to study the self-organizing dynamics of\nmultiple populations of distinct T-cells which interact via antigen presenting\ncells that present hundreds of distinct antigens. Moreover, we show that such\nself-organizing dynamics can be guided to produce an effective binary\nclassification of antigens, which is competitive with existing machine learning\nmethods when applied to biomedical text classification. More specifically, here\nwe test our model on a dataset of publicly available full-text biomedical\narticles provided by the BioCreative challenge (Krallinger in The biocreative\nii. 5 challenge overview, p 19, 2009). We study the robustness of our model's\nparameter configurations, and show that it leads to encouraging results\ncomparable to state-of-the-art classifiers. Our results help us understand both\nT-cell cross-regulation as a general principle of guided self-organization, as\nwell as its applicability to document classification. Therefore, we show that\nour bio-inspired algorithm is a promising novel method for biomedical article\nclassification and for binary document classification in general. \n\n"}
{"id": "1103.0942", "contents": "Title: Generalization error bounds for stationary autoregressive models Abstract: We derive generalization error bounds for stationary univariate\nautoregressive (AR) models. We show that imposing stationarity is enough to\ncontrol the Gaussian complexity without further regularization. This lets us\nuse structural risk minimization for model selection. We demonstrate our\nmethods by predicting interest rate movements. \n\n"}
{"id": "1103.5231", "contents": "Title: Leaders in Social Networks, the Delicious Case Abstract: Finding pertinent information is not limited to search engines. Online\ncommunities can amplify the influence of a small number of power users for the\nbenefit of all other users. Users' information foraging in depth and breadth\ncan be greatly enhanced by choosing suitable leaders. For instance in\ndelicious.com, users subscribe to leaders' collection which lead to a deeper\nand wider reach not achievable with search engines. To consolidate such\ncollective search, it is essential to utilize the leadership topology and\nidentify influential users. Google's PageRank, as a successful search algorithm\nin the World Wide Web, turns out to be less effective in networks of people. We\nthus devise an adaptive and parameter-free algorithm, the LeaderRank, to\nquantify user influence. We show that LeaderRank outperforms PageRank in terms\nof ranking effectiveness, as well as robustness against manipulations and noisy\ndata. These results suggest that leaders who are aware of their clout may\nreinforce the development of social networks, and thus the power of collective\nsearch. \n\n"}
{"id": "1104.4063", "contents": "Title: Fast redshift clustering with the Baire (ultra) metric Abstract: The Baire metric induces an ultrametric on a dataset and is of linear\ncomputational complexity, contrasted with the standard quadratic time\nagglomerative hierarchical clustering algorithm. We apply the Baire distance to\nspectrometric and photometric redshifts from the Sloan Digital Sky Survey\nusing, in this work, about half a million astronomical objects. We want to know\nhow well the (more cos\\ tly to determine) spectrometric redshifts can predict\nthe (more easily obtained) photometric redshifts, i.e. we seek to regress the\nspectrometric on the photometric redshifts, and we develop a clusterwise\nnearest neighbor regression procedure for this. \n\n"}
{"id": "1105.0540", "contents": "Title: Pruning nearest neighbor cluster trees Abstract: Nearest neighbor (k-NN) graphs are widely used in machine learning and data\nmining applications, and our aim is to better understand what they reveal about\nthe cluster structure of the unknown underlying distribution of points.\nMoreover, is it possible to identify spurious structures that might arise due\nto sampling variability?\n  Our first contribution is a statistical analysis that reveals how certain\nsubgraphs of a k-NN graph form a consistent estimator of the cluster tree of\nthe underlying distribution of points. Our second and perhaps most important\ncontribution is the following finite sample guarantee. We carefully work out\nthe tradeoff between aggressive and conservative pruning and are able to\nguarantee the removal of all spurious cluster structures at all levels of the\ntree while at the same time guaranteeing the recovery of salient clusters. This\nis the first such finite sample result in the context of clustering. \n\n"}
{"id": "1106.1925", "contents": "Title: Ranking via Sinkhorn Propagation Abstract: It is of increasing importance to develop learning methods for ranking. In\ncontrast to many learning objectives, however, the ranking problem presents\ndifficulties due to the fact that the space of permutations is not smooth. In\nthis paper, we examine the class of rank-linear objective functions, which\nincludes popular metrics such as precision and discounted cumulative gain. In\nparticular, we observe that expectations of these gains are completely\ncharacterized by the marginals of the corresponding distribution over\npermutation matrices. Thus, the expectations of rank-linear objectives can\nalways be described through locations in the Birkhoff polytope, i.e.,\ndoubly-stochastic matrices (DSMs). We propose a technique for learning\nDSM-based ranking functions using an iterative projection operator known as\nSinkhorn normalization. Gradients of this operator can be computed via\nbackpropagation, resulting in an algorithm we call Sinkhorn propagation, or\nSinkProp. This approach can be combined with a wide range of gradient-based\napproaches to rank learning. We demonstrate the utility of SinkProp on several\ninformation retrieval data sets. \n\n"}
{"id": "1106.2229", "contents": "Title: Fast, Linear Time Hierarchical Clustering using the Baire Metric Abstract: The Baire metric induces an ultrametric on a dataset and is of linear\ncomputational complexity, contrasted with the standard quadratic time\nagglomerative hierarchical clustering algorithm. In this work we evaluate\nempirically this new approach to hierarchical clustering. We compare\nhierarchical clustering based on the Baire metric with (i) agglomerative\nhierarchical clustering, in terms of algorithm properties; (ii) generalized\nultrametrics, in terms of definition; and (iii) fast clustering through k-means\npartititioning, in terms of quality of results. For the latter, we carry out an\nin depth astronomical study. We apply the Baire distance to spectrometric and\nphotometric redshifts from the Sloan Digital Sky Survey using, in this work,\nabout half a million astronomical objects. We want to know how well the (more\ncostly to determine) spectrometric redshifts can predict the (more easily\nobtained) photometric redshifts, i.e. we seek to regress the spectrometric on\nthe photometric redshifts, and we use clusterwise regression for this. \n\n"}
{"id": "1106.6215", "contents": "Title: Towards two-dimensional search engines Abstract: We study the statistical properties of various directed networks using\nranking of their nodes based on the dominant vectors of the Google matrix known\nas PageRank and CheiRank. On average PageRank orders nodes proportionally to a\nnumber of ingoing links, while CheiRank orders nodes proportionally to a number\nof outgoing links. In this way the ranking of nodes becomes two-dimensional\nthat paves the way for development of two-dimensional search engines of new\ntype. Statistical properties of information flow on PageRank-CheiRank plane are\nanalyzed for networks of British, French and Italian Universities, Wikipedia,\nLinux Kernel, gene regulation and other networks. A special emphasis is done\nfor British Universities networks using the large database publicly available\nat UK. Methods of spam links control are also analyzed. \n\n"}
{"id": "1109.0420", "contents": "Title: Meta-song evaluation for chord recognition Abstract: We present a new approach to evaluate chord recognition systems on songs\nwhich do not have full annotations. The principle is to use online chord\ndatabases to generate high accurate \"pseudo annotations\" for these songs and\ncompute \"pseudo accuracies\" of test systems. Statistical models that model the\nrelationship between \"pseudo accuracy\" and real performance are then applied to\nestimate test systems' performance. The approach goes beyond the existing\nevaluation metrics, allowing us to carry out extensive analysis on chord\nrecognition systems, such as their generalizations to different genres. In the\nexperiments we applied this method to evaluate three state-of-the-art chord\nrecognition systems, of which the results verified its reliability. \n\n"}
{"id": "1109.5370", "contents": "Title: Higher-Order Markov Tag-Topic Models for Tagged Documents and Images Abstract: This paper studies the topic modeling problem of tagged documents and images.\nHigher-order relations among tagged documents and images are major and\nubiquitous characteristics, and play positive roles in extracting reliable and\ninterpretable topics. In this paper, we propose the tag-topic models (TTM) to\ndepict such higher-order topic structural dependencies within the Markov random\nfield (MRF) framework. First, we use the novel factor graph representation of\nlatent Dirichlet allocation (LDA)-based topic models from the MRF perspective,\nand present an efficient loopy belief propagation (BP) algorithm for\napproximate inference and parameter estimation. Second, we propose the factor\nhypergraph representation of TTM, and focus on both pairwise and higher-order\nrelation modeling among tagged documents and images. Efficient loopy BP\nalgorithm is developed to learn TTM, which encourages the topic labeling\nsmoothness among tagged documents and images. Extensive experimental results\nconfirm the incorporation of higher-order relations to be effective in\nenhancing the overall topic modeling performance, when compared with current\nstate-of-the-art topic models, in many text and image mining tasks of broad\ninterests such as word and link prediction, document classification, and tag\nrecommendation. \n\n"}
{"id": "1110.1769", "contents": "Title: On the trade-off between complexity and correlation decay in structural\n  learning algorithms Abstract: We consider the problem of learning the structure of Ising models (pairwise\nbinary Markov random fields) from i.i.d. samples. While several methods have\nbeen proposed to accomplish this task, their relative merits and limitations\nremain somewhat obscure. By analyzing a number of concrete examples, we show\nthat low-complexity algorithms often fail when the Markov random field develops\nlong-range correlations. More precisely, this phenomenon appears to be related\nto the Ising model phase transition (although it does not coincide with it). \n\n"}
{"id": "1201.0794", "contents": "Title: Sparse Nonparametric Graphical Models Abstract: We present some nonparametric methods for graphical modeling. In the discrete\ncase, where the data are binary or drawn from a finite alphabet, Markov random\nfields are already essentially nonparametric, since the cliques can take only a\nfinite number of values. Continuous data are different. The Gaussian graphical\nmodel is the standard parametric model for continuous data, but it makes\ndistributional assumptions that are often unrealistic. We discuss two\napproaches to building more flexible graphical models. One allows arbitrary\ngraphs and a nonparametric extension of the Gaussian; the other uses kernel\ndensity estimation and restricts the graphs to trees and forests. Examples of\nboth methods are presented. We also discuss possible future research directions\nfor nonparametric graphical modeling. \n\n"}
{"id": "1205.3193", "contents": "Title: A Comparative Study of Collaborative Filtering Algorithms Abstract: Collaborative filtering is a rapidly advancing research area. Every year\nseveral new techniques are proposed and yet it is not clear which of the\ntechniques work best and under what conditions. In this paper we conduct a\nstudy comparing several collaborative filtering techniques -- both classic and\nrecent state-of-the-art -- in a variety of experimental contexts. Specifically,\nwe report conclusions controlling for number of items, number of users,\nsparsity level, performance criteria, and computational complexity. Our\nconclusions identify what algorithms work well and in what conditions, and\ncontribute to both industrial deployment collaborative filtering algorithms and\nto the research community. \n\n"}
{"id": "1206.1088", "contents": "Title: Bayesian Structure Learning for Markov Random Fields with a Spike and\n  Slab Prior Abstract: In recent years a number of methods have been developed for automatically\nlearning the (sparse) connectivity structure of Markov Random Fields. These\nmethods are mostly based on L1-regularized optimization which has a number of\ndisadvantages such as the inability to assess model uncertainty and expensive\ncross-validation to find the optimal regularization parameter. Moreover, the\nmodel's predictive performance may degrade dramatically with a suboptimal value\nof the regularization parameter (which is sometimes desirable to induce\nsparseness). We propose a fully Bayesian approach based on a \"spike and slab\"\nprior (similar to L0 regularization) that does not suffer from these\nshortcomings. We develop an approximate MCMC method combining Langevin dynamics\nand reversible jump MCMC to conduct inference in this model. Experiments show\nthat the proposed model learns a good combination of the structure and\nparameter values without the need for separate hyper-parameter tuning.\nMoreover, the model's predictive performance is much more robust than L1-based\nmethods with hyper-parameter settings that induce highly sparse model\nstructures. \n\n"}
{"id": "1206.2944", "contents": "Title: Practical Bayesian Optimization of Machine Learning Algorithms Abstract: Machine learning algorithms frequently require careful tuning of model\nhyperparameters, regularization terms, and optimization parameters.\nUnfortunately, this tuning is often a \"black art\" that requires expert\nexperience, unwritten rules of thumb, or sometimes brute-force search. Much\nmore appealing is the idea of developing automatic approaches which can\noptimize the performance of a given learning algorithm to the task at hand. In\nthis work, we consider the automatic tuning problem within the framework of\nBayesian optimization, in which a learning algorithm's generalization\nperformance is modeled as a sample from a Gaussian process (GP). The tractable\nposterior distribution induced by the GP leads to efficient use of the\ninformation gathered by previous experiments, enabling optimal choices about\nwhat parameters to try next. Here we show how the effects of the Gaussian\nprocess prior and the associated inference procedure can have a large impact on\nthe success or failure of Bayesian optimization. We show that thoughtful\nchoices can lead to results that exceed expert-level performance in tuning\nmachine learning algorithms. We also describe new algorithms that take into\naccount the variable cost (duration) of learning experiments and that can\nleverage the presence of multiple cores for parallel experimentation. We show\nthat these proposed algorithms improve on previous automatic procedures and can\nreach or surpass human expert-level optimization on a diverse set of\ncontemporary algorithms including latent Dirichlet allocation, structured SVMs\nand convolutional neural networks. \n\n"}
{"id": "1207.0577", "contents": "Title: Robust Dequantized Compressive Sensing Abstract: We consider the reconstruction problem in compressed sensing in which the\nobservations are recorded in a finite number of bits. They may thus contain\nquantization errors (from being rounded to the nearest representable value) and\nsaturation errors (from being outside the range of representable values). Our\nformulation has an objective of weighted $\\ell_2$-$\\ell_1$ type, along with\nconstraints that account explicitly for quantization and saturation errors, and\nis solved with an augmented Lagrangian method. We prove a consistency result\nfor the recovered solution, stronger than those that have appeared to date in\nthe literature, showing in particular that asymptotic consistency can be\nobtained without oversampling. We present extensive computational comparisons\nwith formulations proposed previously, and variants thereof. \n\n"}
{"id": "1207.4421", "contents": "Title: Stochastic optimization and sparse statistical recovery: An optimal\n  algorithm for high dimensions Abstract: We develop and analyze stochastic optimization algorithms for problems in\nwhich the expected loss is strongly convex, and the optimum is (approximately)\nsparse. Previous approaches are able to exploit only one of these two\nstructures, yielding an $\\order(\\pdim/T)$ convergence rate for strongly convex\nobjectives in $\\pdim$ dimensions, and an $\\order(\\sqrt{(\\spindex \\log\n\\pdim)/T})$ convergence rate when the optimum is $\\spindex$-sparse. Our\nalgorithm is based on successively solving a series of $\\ell_1$-regularized\noptimization problems using Nesterov's dual averaging algorithm. We establish\nthat the error of our solution after $T$ iterations is at most\n$\\order((\\spindex \\log\\pdim)/T)$, with natural extensions to approximate\nsparsity. Our results apply to locally Lipschitz losses including the logistic,\nexponential, hinge and least-squares losses. By recourse to statistical minimax\nresults, we show that our convergence rates are optimal up to multiplicative\nconstant factors. The effectiveness of our approach is also confirmed in\nnumerical simulations, in which we compare to several baselines on a\nleast-squares regression problem. \n\n"}
{"id": "1208.1237", "contents": "Title: Fast and Robust Recursive Algorithms for Separable Nonnegative Matrix\n  Factorization Abstract: In this paper, we study the nonnegative matrix factorization problem under\nthe separability assumption (that is, there exists a cone spanned by a small\nsubset of the columns of the input nonnegative data matrix containing all\ncolumns), which is equivalent to the hyperspectral unmixing problem under the\nlinear mixing model and the pure-pixel assumption. We present a family of fast\nrecursive algorithms, and prove they are robust under any small perturbations\nof the input data matrix. This family generalizes several existing\nhyperspectral unmixing algorithms and hence provides for the first time a\ntheoretical justification of their better practical performance. \n\n"}
{"id": "1209.1873", "contents": "Title: Stochastic Dual Coordinate Ascent Methods for Regularized Loss\n  Minimization Abstract: Stochastic Gradient Descent (SGD) has become popular for solving large scale\nsupervised machine learning optimization problems such as SVM, due to their\nstrong theoretical guarantees. While the closely related Dual Coordinate Ascent\n(DCA) method has been implemented in various software packages, it has so far\nlacked good convergence analysis. This paper presents a new analysis of\nStochastic Dual Coordinate Ascent (SDCA) showing that this class of methods\nenjoy strong theoretical guarantees that are comparable or better than SGD.\nThis analysis justifies the effectiveness of SDCA for practical applications. \n\n"}
{"id": "1209.3026", "contents": "Title: Losing My Revolution: How Many Resources Shared on Social Media Have\n  Been Lost? Abstract: Social media content has grown exponentially in the recent years and the role\nof social media has evolved from just narrating life events to actually shaping\nthem. In this paper we explore how many resources shared in social media are\nstill available on the live web or in public web archives. By analyzing six\ndifferent event-centric datasets of resources shared in social media in the\nperiod from June 2009 to March 2012, we found about 11% lost and 20% archived\nafter just a year and an average of 27% lost and 41% archived after two and a\nhalf years. Furthermore, we found a nearly linear relationship between time of\nsharing of the resource and the percentage lost, with a slightly less linear\nrelationship between time of sharing and archiving coverage of the resource.\nFrom this model we conclude that after the first year of publishing, nearly 11%\nof shared resources will be lost and after that we will continue to lose 0.02%\nper day. \n\n"}
{"id": "1209.3126", "contents": "Title: Beyond Stemming and Lemmatization: Ultra-stemming to Improve Automatic\n  Text Summarization Abstract: In Automatic Text Summarization, preprocessing is an important phase to\nreduce the space of textual representation. Classically, stemming and\nlemmatization have been widely used for normalizing words. However, even using\nnormalization on large texts, the curse of dimensionality can disturb the\nperformance of summarizers. This paper describes a new method for normalization\nof words to further reduce the space of representation. We propose to reduce\neach word to its initial letters, as a form of Ultra-stemming. The results show\nthat Ultra-stemming not only preserve the content of summaries produced by this\nrepresentation, but often the performances of the systems can be dramatically\nimproved. Summaries on trilingual corpora were evaluated automatically with\nFresa. Results confirm an increase in the performance, regardless of summarizer\nsystem used. \n\n"}
{"id": "1209.6449", "contents": "Title: Fast Packed String Matching for Short Patterns Abstract: Searching for all occurrences of a pattern in a text is a fundamental problem\nin computer science with applications in many other fields, like natural\nlanguage processing, information retrieval and computational biology. In the\nlast two decades a general trend has appeared trying to exploit the power of\nthe word RAM model to speed-up the performances of classical string matching\nalgorithms. In this model an algorithm operates on words of length w, grouping\nblocks of characters, and arithmetic and logic operations on the words take one\nunit of time. In this paper we use specialized word-size packed string matching\ninstructions, based on the Intel streaming SIMD extensions (SSE) technology, to\ndesign very fast string matching algorithms in the case of short patterns. From\nour experimental results it turns out that, despite their quadratic worst case\ntime complexity, the new presented algorithms become the clear winners on the\naverage for short patterns, when compared against the most effective algorithms\nknown in literature. \n\n"}
{"id": "1211.2717", "contents": "Title: Proximal Stochastic Dual Coordinate Ascent Abstract: We introduce a proximal version of dual coordinate ascent method. We\ndemonstrate how the derived algorithmic framework can be used for numerous\nregularized loss minimization problems, including $\\ell_1$ regularization and\nstructured output SVM. The convergence rates we obtain match, and sometimes\nimprove, state-of-the-art results. \n\n"}
{"id": "1211.6687", "contents": "Title: Robustness Analysis of Hottopixx, a Linear Programming Model for\n  Factoring Nonnegative Matrices Abstract: Although nonnegative matrix factorization (NMF) is NP-hard in general, it has\nbeen shown very recently that it is tractable under the assumption that the\ninput nonnegative data matrix is close to being separable (separability\nrequires that all columns of the input matrix belongs to the cone spanned by a\nsmall subset of these columns). Since then, several algorithms have been\ndesigned to handle this subclass of NMF problems. In particular, Bittorf,\nRecht, R\\'e and Tropp (`Factoring nonnegative matrices with linear programs',\nNIPS 2012) proposed a linear programming model, referred to as Hottopixx. In\nthis paper, we provide a new and more general robustness analysis of their\nmethod. In particular, we design a provably more robust variant using a\npost-processing strategy which allows us to deal with duplicates and near\nduplicates in the dataset. \n\n"}
{"id": "1212.2287", "contents": "Title: Runtime Optimizations for Prediction with Tree-Based Models Abstract: Tree-based models have proven to be an effective solution for web ranking as\nwell as other problems in diverse domains. This paper focuses on optimizing the\nruntime performance of applying such models to make predictions, given an\nalready-trained model. Although exceedingly simple conceptually, most\nimplementations of tree-based models do not efficiently utilize modern\nsuperscalar processor architectures. By laying out data structures in memory in\na more cache-conscious fashion, removing branches from the execution flow using\na technique called predication, and micro-batching predictions using a\ntechnique called vectorization, we are able to better exploit modern processor\narchitectures and significantly improve the speed of tree-based models over\nhard-coded if-else blocks. Our work contributes to the exploration of\narchitecture-conscious runtime implementations of machine learning algorithms. \n\n"}
{"id": "1212.6110", "contents": "Title: Hyperplane Arrangements and Locality-Sensitive Hashing with Lift Abstract: Locality-sensitive hashing converts high-dimensional feature vectors, such as\nimage and speech, into bit arrays and allows high-speed similarity calculation\nwith the Hamming distance. There is a hashing scheme that maps feature vectors\nto bit arrays depending on the signs of the inner products between feature\nvectors and the normal vectors of hyperplanes placed in the feature space. This\nhashing can be seen as a discretization of the feature space by hyperplanes. If\nlabels for data are given, one can determine the hyperplanes by using learning\nalgorithms. However, many proposed learning methods do not consider the\nhyperplanes' offsets. Not doing so decreases the number of partitioned regions,\nand the correlation between Hamming distances and Euclidean distances becomes\nsmall. In this paper, we propose a lift map that converts learning algorithms\nwithout the offsets to the ones that take into account the offsets. With this\nmethod, the learning methods without the offsets give the discretizations of\nspaces as if it takes into account the offsets. For the proposed method, we\ninput several high-dimensional feature data sets and studied the relationship\nbetween the statistical characteristics of data, the number of hyperplanes, and\nthe effect of the proposed method. \n\n"}
{"id": "1301.1254", "contents": "Title: Dynamical Models and Tracking Regret in Online Convex Programming Abstract: This paper describes a new online convex optimization method which\nincorporates a family of candidate dynamical models and establishes novel\ntracking regret bounds that scale with the comparator's deviation from the best\ndynamical model in this family. Previous online optimization methods are\ndesigned to have a total accumulated loss comparable to that of the best\ncomparator sequence, and existing tracking or shifting regret bounds scale with\nthe overall variation of the comparator sequence. In many practical scenarios,\nhowever, the environment is nonstationary and comparator sequences with small\nvariation are quite weak, resulting in large losses. The proposed Dynamic\nMirror Descent method, in contrast, can yield low regret relative to highly\nvariable comparator sequences by both tracking the best dynamical model and\nforming predictions based on that model. This concept is demonstrated\nempirically in the context of sequential compressive observations of a dynamic\nscene and tracking a dynamic social network. \n\n"}
{"id": "1301.1626", "contents": "Title: Google matrix analysis of DNA sequences Abstract: For DNA sequences of various species we construct the Google matrix G of\nMarkov transitions between nearby words composed of several letters. The\nstatistical distribution of matrix elements of this matrix is shown to be\ndescribed by a power law with the exponent being close to those of outgoing\nlinks in such scale-free networks as the World Wide Web (WWW). At the same time\nthe sum of ingoing matrix elements is characterized by the exponent being\nsignificantly larger than those typical for WWW networks. This results in a\nslow algebraic decay of the PageRank probability determined by the distribution\nof ingoing elements. The spectrum of G is characterized by a large gap leading\nto a rapid relaxation process on the DNA sequence networks. We introduce the\nPageRank proximity correlator between different species which determines their\nstatistical similarity from the view point of Markov chains. The properties of\nother eigenstates of the Google matrix are also discussed. Our results\nestablish scale-free features of DNA sequence networks showing their\nsimilarities and distinctions with the WWW and linguistic networks. \n\n"}
{"id": "1301.5220", "contents": "Title: Properties of the Least Squares Temporal Difference learning algorithm Abstract: This paper presents four different ways of looking at the well-known Least\nSquares Temporal Differences (LSTD) algorithm for computing the value function\nof a Markov Reward Process, each of them leading to different insights: the\noperator-theory approach via the Galerkin method, the statistical approach via\ninstrumental variables, the linear dynamical system view as well as the limit\nof the TD iteration. We also give a geometric view of the algorithm as an\noblique projection. Furthermore, there is an extensive comparison of the\noptimization problem solved by LSTD as compared to Bellman Residual\nMinimization (BRM). We then review several schemes for the regularization of\nthe LSTD solution. We then proceed to treat the modification of LSTD for the\ncase of episodic Markov Reward Processes. \n\n"}
{"id": "1301.5650", "contents": "Title: Regularization and nonlinearities for neural language models: when are\n  they needed? Abstract: Neural language models (LMs) based on recurrent neural networks (RNN) are\nsome of the most successful word and character-level LMs. Why do they work so\nwell, in particular better than linear neural LMs? Possible explanations are\nthat RNNs have an implicitly better regularization or that RNNs have a higher\ncapacity for storing patterns due to their nonlinearities or both. Here we\nargue for the first explanation in the limit of little training data and the\nsecond explanation for large amounts of text data. We show state-of-the-art\nperformance on the popular and small Penn dataset when RNN LMs are regularized\nwith random dropout. Nonetheless, we show even better performance from a\nsimplified, much less expressive linear RNN model without off-diagonal entries\nin the recurrent matrix. We call this model an impulse-response LM (IRLM).\nUsing random dropout, column normalization and annealed learning rates, IRLMs\ndevelop neurons that keep a memory of up to 50 words in the past and achieve a\nperplexity of 102.5 on the Penn dataset. On two large datasets however, the\nsame regularization methods are unsuccessful for both models and the RNN's\nexpressivity allows it to overtake the IRLM by 10 and 20 percent perplexity,\nrespectively. Despite the perplexity gap, IRLMs still outperform RNNs on the\nMicrosoft Research Sentence Completion (MRSC) task. We develop a slightly\nmodified IRLM that separates long-context units (LCUs) from short-context units\nand show that the LCUs alone achieve a state-of-the-art performance on the MRSC\ntask of 60.8%. Our analysis indicates that a fruitful direction of research for\nneural LMs lies in developing more accessible internal representations, and\nsuggests an optimization regime of very high momentum terms for effectively\ntraining such models. \n\n"}
{"id": "1302.0420", "contents": "Title: Benchmarking some Portuguese S&T system research units: 2nd Edition Abstract: The increasing use of productivity and impact metrics for evaluation and\ncomparison, not only of individual researchers but also of institutions,\nuniversities and even countries, has prompted the development of bibliometrics.\nCurrently, metrics are becoming widely accepted as an easy and balanced way to\nassist the peer review and evaluation of scientists and/or research units,\nprovided they have adequate precision and recall.\n  This paper presents a benchmarking study of a selected list of representative\nPortuguese research units, based on a fairly complete set of parameters:\nbibliometric parameters, number of competitive projects and number of PhDs\nproduced. The study aimed at collecting productivity and impact data from the\nselected research units in comparable conditions i.e., using objective metrics\nbased on public information, retrievable on-line and/or from official sources\nand thus verifiable and repeatable. The study has thus focused on the activity\nof the 2003-06 period, where such data was available from the latest official\nevaluation.\n  The main advantage of our study was the application of automatic tools,\nachieving relevant results at a reduced cost. Moreover, the results over the\nselected units suggest that this kind of analyses will be very useful to\nbenchmark scientific productivity and impact, and assist peer review. \n\n"}
{"id": "1302.4389", "contents": "Title: Maxout Networks Abstract: We consider the problem of designing models to leverage a recently introduced\napproximate model averaging technique called dropout. We define a simple new\nmodel called maxout (so named because its output is the max of a set of inputs,\nand because it is a natural companion to dropout) designed to both facilitate\noptimization by dropout and improve the accuracy of dropout's fast approximate\nmodel averaging technique. We empirically verify that the model successfully\naccomplishes both of these tasks. We use maxout and dropout to demonstrate\nstate of the art classification performance on four benchmark datasets: MNIST,\nCIFAR-10, CIFAR-100, and SVHN. \n\n"}
{"id": "1303.0561", "contents": "Title: Top-down particle filtering for Bayesian decision trees Abstract: Decision tree learning is a popular approach for classification and\nregression in machine learning and statistics, and Bayesian\nformulations---which introduce a prior distribution over decision trees, and\nformulate learning as posterior inference given data---have been shown to\nproduce competitive performance. Unlike classic decision tree learning\nalgorithms like ID3, C4.5 and CART, which work in a top-down manner, existing\nBayesian algorithms produce an approximation to the posterior distribution by\nevolving a complete tree (or collection thereof) iteratively via local Monte\nCarlo modifications to the structure of the tree, e.g., using Markov chain\nMonte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that\ninstead works in a top-down manner, mimicking the behavior and speed of classic\nalgorithms. We demonstrate empirically that our approach delivers accuracy\ncomparable to the most popular MCMC method, but operates more than an order of\nmagnitude faster, and thus represents a better computation-accuracy tradeoff. \n\n"}
{"id": "1303.0667", "contents": "Title: Query Expansion Using Term Distribution and Term Association Abstract: Good term selection is an important issue for an automatic query expansion\n(AQE) technique. AQE techniques that select expansion terms from the target\ncorpus usually do so in one of two ways. Distribution based term selection\ncompares the distribution of a term in the (pseudo) relevant documents with\nthat in the whole corpus / random distribution. Two well-known\ndistribution-based methods are based on Kullback-Leibler Divergence (KLD) and\nBose-Einstein statistics (Bo1). Association based term selection, on the other\nhand, uses information about how a candidate term co-occurs with the original\nquery terms. Local Context Analysis (LCA) and Relevance-based Language Model\n(RM3) are examples of association-based methods. Our goal in this study is to\ninvestigate how these two classes of methods may be combined to improve\nretrieval effectiveness. We propose the following combination-based approach.\nCandidate expansion terms are first obtained using a distribution based method.\nThis set is then refined based on the strength of the association of terms with\nthe original query terms. We test our methods on 11 TREC collections. The\nproposed combinations generally yield better results than each individual\nmethod, as well as other state-of-the-art AQE approaches. En route to our\nprimary goal, we also propose some modifications to LCA and Bo1 which lead to\nimproved performance. \n\n"}
{"id": "1303.3964", "contents": "Title: Simple Search Engine Model: Selective Properties Abstract: In this paper we study the relationship between query and search engine by\nexploring the selective properties based on a simple search engine. We used the\nset theory and utilized the words and terms for defining singleton and\ndoubleton in the event spaces and then provided their implementation for\nproving the existence of the shadow of micro-cluster. \n\n"}
{"id": "1303.6906", "contents": "Title: Large scale citation matching using Apache Hadoop Abstract: During the process of citation matching links from bibliography entries to\nreferenced publications are created. Such links are indicators of topical\nsimilarity between linked texts, are used in assessing the impact of the\nreferenced document and improve navigation in the user interfaces of digital\nlibraries. In this paper we present a citation matching method and show how to\nscale it up to handle great amounts of data using appropriate indexing and a\nMapReduce paradigm in the Hadoop environment. \n\n"}
{"id": "1303.7264", "contents": "Title: Scalable Text and Link Analysis with Mixed-Topic Link Models Abstract: Many data sets contain rich information about objects, as well as pairwise\nrelations between them. For instance, in networks of websites, scientific\npapers, and other documents, each node has content consisting of a collection\nof words, as well as hyperlinks or citations to other nodes. In order to\nperform inference on such data sets, and make predictions and recommendations,\nit is useful to have models that are able to capture the processes which\ngenerate the text at each node and the links between them. In this paper, we\ncombine classic ideas in topic modeling with a variant of the mixed-membership\nblock model recently developed in the statistical physics community. The\nresulting model has the advantage that its parameters, including the mixture of\ntopics of each document and the resulting overlapping communities, can be\ninferred with a simple and scalable expectation-maximization algorithm. We test\nour model on three data sets, performing unsupervised topic classification and\nlink prediction. For both tasks, our model outperforms several existing\nstate-of-the-art methods, achieving higher accuracy with significantly less\ncomputation, analyzing a data set with 1.3 million words and 44 thousand links\nin a few minutes. \n\n"}
{"id": "1305.1040", "contents": "Title: On the Convergence and Consistency of the Blurring Mean-Shift Process Abstract: The mean-shift algorithm is a popular algorithm in computer vision and image\nprocessing. It can also be cast as a minimum gamma-divergence estimation. In\nthis paper we focus on the \"blurring\" mean shift algorithm, which is one\nversion of the mean-shift process that successively blurs the dataset. The\nanalysis of the blurring mean-shift is relatively more complicated compared to\nthe nonblurring version, yet the algorithm convergence and the estimation\nconsistency have not been well studied in the literature. In this paper we\nprove both the convergence and the consistency of the blurring mean-shift. We\nalso perform simulation studies to compare the efficiency of the blurring and\nthe nonblurring versions of the mean-shift algorithms. Our results show that\nthe blurring mean-shift has more efficiency. \n\n"}
{"id": "1306.6259", "contents": "Title: Highlighting Entanglement of Cultures via Ranking of Multilingual\n  Wikipedia Articles Abstract: How different cultures evaluate a person? Is an important person in one\nculture is also important in the other culture? We address these questions via\nranking of multilingual Wikipedia articles. With three ranking algorithms based\non network structure of Wikipedia, we assign ranking to all articles in 9\nmultilingual editions of Wikipedia and investigate general ranking structure of\nPageRank, CheiRank and 2DRank. In particular, we focus on articles related to\npersons, identify top 30 persons for each rank among different editions and\nanalyze distinctions of their distributions over activity fields such as\npolitics, art, science, religion, sport for each edition. We find that local\nheroes are dominant but also global heroes exist and create an effective\nnetwork representing entanglement of cultures. The Google matrix analysis of\nnetwork of cultures shows signs of the Zipf law distribution. This approach\nallows to examine diversity and shared characteristics of knowledge\norganization between cultures. The developed computational, data driven\napproach highlights cultural interconnections in a new perspective. \n\n"}
{"id": "1307.4980", "contents": "Title: Multi-keyword multi-click advertisement option contracts for sponsored\n  search Abstract: In sponsored search, advertisement (abbreviated ad) slots are usually sold by\na search engine to an advertiser through an auction mechanism in which\nadvertisers bid on keywords. In theory, auction mechanisms have many desirable\neconomic properties. However, keyword auctions have a number of limitations\nincluding: the uncertainty in payment prices for advertisers; the volatility in\nthe search engine's revenue; and the weak loyalty between advertiser and search\nengine. In this paper we propose a special ad option that alleviates these\nproblems. In our proposal, an advertiser can purchase an option from a search\nengine in advance by paying an upfront fee, known as the option price. He then\nhas the right, but no obligation, to purchase among the pre-specified set of\nkeywords at the fixed cost-per-clicks (CPCs) for a specified number of clicks\nin a specified period of time. The proposed option is closely related to a\nspecial exotic option in finance that contains multiple underlying assets\n(multi-keyword) and is also multi-exercisable (multi-click). This novel\nstructure has many benefits: advertisers can have reduced uncertainty in\nadvertising; the search engine can improve the advertisers' loyalty as well as\nobtain a stable and increased expected revenue over time. Since the proposed ad\noption can be implemented in conjunction with the existing keyword auctions,\nthe option price and corresponding fixed CPCs must be set such that there is no\narbitrage between the two markets. Option pricing methods are discussed and our\nexperimental results validate the development. Compared to keyword auctions, a\nsearch engine can have an increased expected revenue by selling an ad option. \n\n"}
{"id": "1307.5558", "contents": "Title: Mixtures of Common Skew-t Factor Analyzers Abstract: A mixture of common skew-t factor analyzers model is introduced for\nmodel-based clustering of high-dimensional data. By assuming common component\nfactor loadings, this model allows clustering to be performed in the presence\nof a large number of mixture components or when the number of dimensions is too\nlarge to be well-modelled by the mixtures of factor analyzers model or a\nvariant thereof. Furthermore, assuming that the component densities follow a\nskew-t distribution allows robust clustering of skewed data. The alternating\nexpectation-conditional maximization algorithm is employed for parameter\nestimation. We demonstrate excellent clustering performance when our model is\napplied to real and simulated data.This paper marks the first time that skewed\ncommon factors have been used. \n\n"}
{"id": "1307.6080", "contents": "Title: Timely crawling of high-quality ephemeral new content Abstract: Nowadays, more and more people use the Web as their primary source of\nup-to-date information. In this context, fast crawling and indexing of newly\ncreated Web pages has become crucial for search engines, especially because\nuser traffic to a significant fraction of these new pages (like news, blog and\nforum posts) grows really quickly right after they appear, but lasts only for\nseveral days.\n  In this paper, we study the problem of timely finding and crawling of such\nephemeral new pages (in terms of user interest). Traditional crawling policies\ndo not give any particular priority to such pages and may thus crawl them not\nquickly enough, and even crawl already obsolete content. We thus propose a new\nmetric, well thought out for this task, which takes into account the decrease\nof user interest for ephemeral pages over time.\n  We show that most ephemeral new pages can be found at a relatively small set\nof content sources and present a procedure for finding such a set. Our idea is\nto periodically recrawl content sources and crawl newly created pages linked\nfrom them, focusing on high-quality (in terms of user interest) content. One of\nthe main difficulties here is to divide resources between these two activities\nin an efficient way. We find the adaptive balance between crawls and recrawls\nby maximizing the proposed metric. Further, we incorporate search engine click\nlogs to give our crawler an insight about the current user demands. Efficiency\nof our approach is finally demonstrated experimentally on real-world data. \n\n"}
{"id": "1307.6887", "contents": "Title: Sequential Transfer in Multi-armed Bandit with Finite Set of Models Abstract: Learning from prior tasks and transferring that experience to improve future\nperformance is critical for building lifelong learning agents. Although results\nin supervised and reinforcement learning show that transfer may significantly\nimprove the learning performance, most of the literature on transfer is focused\non batch learning tasks. In this paper we study the problem of\n\\textit{sequential transfer in online learning}, notably in the multi-armed\nbandit framework, where the objective is to minimize the cumulative regret over\na sequence of tasks by incrementally transferring knowledge from prior tasks.\nWe introduce a novel bandit algorithm based on a method-of-moments approach for\nthe estimation of the possible tasks and derive regret bounds for it. \n\n"}
{"id": "1308.1009", "contents": "Title: Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels Abstract: The method of stable random projections is popular for efficiently computing\nthe Lp distances in high dimension (where 0<p<=2), using small space. Because\nit adopts nonadaptive linear projections, this method is naturally suitable\nwhen the data are collected in a dynamic streaming fashion (i.e., turnstile\ndata streams). In this paper, we propose to use only the signs of the projected\ndata and analyze the probability of collision (i.e., when the two signs\ndiffer). We derive a bound of the collision probability which is exact when p=2\nand becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e.,\nCauchy random projections), we show that the probability of collision can be\naccurately approximated as functions of the chi-square similarity. For example,\nwhen the (un-normalized) data are binary, the maximum approximation error of\nthe collision probability is smaller than 0.0192. In text and vision\napplications, the chi-square similarity is a popular measure for nonnegative\ndata when the features are generated from histograms. Our experiments confirm\nthat the proposed method is promising for large-scale learning applications. \n\n"}
{"id": "1308.2403", "contents": "Title: CDfdr: A Comparison Density Approach to Local False Discovery Rate\n  Estimation Abstract: Efron et al. (2001) proposed empirical Bayes formulation of the frequentist\nBenjamini and Hochbergs False Discovery Rate method (Benjamini and\nHochberg,1995). This article attempts to unify the `two cultures' using\nconcepts of comparison density and distribution function. We have also shown\nhow almost all of the existing local fdr methods can be viewed as proposing\nvarious model specification for comparison density - unifies the vast\nliterature of false discovery methods under one concept and notation. \n\n"}
{"id": "1308.2853", "contents": "Title: When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor\n  Tucker Decompositions with Structured Sparsity Abstract: Overcomplete latent representations have been very popular for unsupervised\nfeature learning in recent years. In this paper, we specify which overcomplete\nmodels can be identified given observable moments of a certain order. We\nconsider probabilistic admixture or topic models in the overcomplete regime,\nwhere the number of latent topics can greatly exceed the size of the observed\nword vocabulary. While general overcomplete topic models are not identifiable,\nwe establish generic identifiability under a constraint, referred to as topic\npersistence. Our sufficient conditions for identifiability involve a novel set\nof \"higher order\" expansion conditions on the topic-word matrix or the\npopulation structure of the model. This set of higher-order expansion\nconditions allow for overcomplete models, and require the existence of a\nperfect matching from latent topics to higher order observed words. We\nestablish that random structured topic models are identifiable w.h.p. in the\novercomplete regime. Our identifiability results allows for general\n(non-degenerate) distributions for modeling the topic proportions, and thus, we\ncan handle arbitrarily correlated topics in our framework. Our identifiability\nresults imply uniqueness of a class of tensor decompositions with structured\nsparsity which is contained in the class of Tucker decompositions, but is more\ngeneral than the Candecomp/Parafac (CP) decomposition. \n\n"}
{"id": "1308.5546", "contents": "Title: Sparse and Non-Negative BSS for Noisy Data Abstract: Non-negative blind source separation (BSS) has raised interest in various\nfields of research, as testified by the wide literature on the topic of\nnon-negative matrix factorization (NMF). In this context, it is fundamental\nthat the sources to be estimated present some diversity in order to be\nefficiently retrieved. Sparsity is known to enhance such contrast between the\nsources while producing very robust approaches, especially to noise. In this\npaper we introduce a new algorithm in order to tackle the blind separation of\nnon-negative sparse sources from noisy measurements. We first show that\nsparsity and non-negativity constraints have to be carefully applied on the\nsought-after solution. In fact, improperly constrained solutions are unlikely\nto be stable and are therefore sub-optimal. The proposed algorithm, named nGMCA\n(non-negative Generalized Morphological Component Analysis), makes use of\nproximal calculus techniques to provide properly constrained solutions. The\nperformance of nGMCA compared to other state-of-the-art algorithms is\ndemonstrated by numerical experiments encompassing a wide variety of settings,\nwith negligible parameter tuning. In particular, nGMCA is shown to provide\nrobustness to noise and performs well on synthetic mixtures of real NMR\nspectra. \n\n"}
{"id": "1308.6342", "contents": "Title: Linear and Parallel Learning of Markov Random Fields Abstract: We introduce a new embarrassingly parallel parameter learning algorithm for\nMarkov random fields with untied parameters which is efficient for a large\nclass of practical models. Our algorithm parallelizes naturally over cliques\nand, for graphs of bounded degree, its complexity is linear in the number of\ncliques. Unlike its competitors, our algorithm is fully parallel and for\nlog-linear models it is also data efficient, requiring only the local\nsufficient statistics of the data to estimate parameters. \n\n"}
{"id": "1309.0337", "contents": "Title: Scalable Probabilistic Entity-Topic Modeling Abstract: We present an LDA approach to entity disambiguation. Each topic is associated\nwith a Wikipedia article and topics generate either content words or entity\nmentions. Training such models is challenging because of the topic and\nvocabulary size, both in the millions. We tackle these problems using a novel\ndistributed inference and representation framework based on a parallel Gibbs\nsampler guided by the Wikipedia link graph, and pipelines of MapReduce allowing\nfast and memory-frugal processing of large datasets. We report state-of-the-art\nperformance on a public dataset. \n\n"}
{"id": "1309.4111", "contents": "Title: Regularized Spectral Clustering under the Degree-Corrected Stochastic\n  Blockmodel Abstract: Spectral clustering is a fast and popular algorithm for finding clusters in\nnetworks. Recently, Chaudhuri et al. (2012) and Amini et al.(2012) proposed\ninspired variations on the algorithm that artificially inflate the node degrees\nfor improved statistical performance. The current paper extends the previous\nstatistical estimation results to the more canonical spectral clustering\nalgorithm in a way that removes any assumption on the minimum degree and\nprovides guidance on the choice of the tuning parameter. Moreover, our results\nshow how the \"star shape\" in the eigenvectors--a common feature of empirical\nnetworks--can be explained by the Degree-Corrected Stochastic Blockmodel and\nthe Extended Planted Partition model, two statistical models that allow for\nhighly heterogeneous degrees. Throughout, the paper characterizes and justifies\nseveral of the variations of the spectral clustering algorithm in terms of\nthese models. \n\n"}
{"id": "1309.4938", "contents": "Title: Improving Query Expansion Using WordNet Abstract: This study proposes a new way of using WordNet for Query Expansion (QE). We\nchoose candidate expansion terms, as usual, from a set of pseudo relevant\ndocuments; however, the usefulness of these terms is measured based on their\ndefinitions provided in a hand-crafted lexical resource like WordNet.\nExperiments with a number of standard TREC collections show that this method\noutperforms existing WordNet based methods. It also compares favorably with\nestablished QE methods such as KLD and RM3. Leveraging earlier work in which a\ncombination of QE methods was found to outperform each individual method (as\nwell as other well-known QE methods), we next propose a combination-based QE\nmethod that takes into account three different aspects of a candidate expansion\nterm's usefulness: (i) its distribution in the pseudo relevant documents and in\nthe target corpus, (ii) its statistical association with query terms, and (iii)\nits semantic relation with the query, as determined by the overlap between the\nWordNet definitions of the term and query terms. This combination of diverse\nsources of information appears to work well on a number of test collections,\nviz., TREC123, TREC5, TREC678, TREC robust new and TREC910 collections, and\nyields significant improvements over competing methods on most of these\ncollections. \n\n"}
{"id": "1310.1415", "contents": "Title: Narrowing the Gap: Random Forests In Theory and In Practice Abstract: Despite widespread interest and practical use, the theoretical properties of\nrandom forests are still not well understood. In this paper we contribute to\nthis understanding in two ways. We present a new theoretically tractable\nvariant of random regression forests and prove that our algorithm is\nconsistent. We also provide an empirical evaluation, comparing our algorithm\nand other theoretically tractable random forest models to the random forest\nalgorithm used in practice. Our experiments provide insight into the relative\nimportance of different simplifications that theoreticians have made to obtain\ntractable models for analysis. \n\n"}
{"id": "1310.2409", "contents": "Title: Discriminative Relational Topic Models Abstract: Many scientific and engineering fields involve analyzing network data. For\ndocument networks, relational topic models (RTMs) provide a probabilistic\ngenerative process to describe both the link structure and document contents,\nand they have shown promise on predicting network structures and discovering\nlatent topic representations. However, existing RTMs have limitations in both\nthe restricted model expressiveness and incapability of dealing with imbalanced\nnetwork data. To expand the scope and improve the inference accuracy of RTMs,\nthis paper presents three extensions: 1) unlike the common link likelihood with\na diagonal weight matrix that allows the-same-topic interactions only, we\ngeneralize it to use a full weight matrix that captures all pairwise topic\ninteractions and is applicable to asymmetric networks; 2) instead of doing\nstandard Bayesian inference, we perform regularized Bayesian inference\n(RegBayes) with a regularization parameter to deal with the imbalanced link\nstructure issue in common real networks and improve the discriminative ability\nof learned latent representations; and 3) instead of doing variational\napproximation with strict mean-field assumptions, we present collapsed Gibbs\nsampling algorithms for the generalized relational topic models by exploring\ndata augmentation without making restricting assumptions. Under the generic\nRegBayes framework, we carefully investigate two popular discriminative loss\nfunctions, namely, the logistic log-loss and the max-margin hinge loss.\nExperimental results on several real network datasets demonstrate the\nsignificance of these extensions on improving the prediction performance, and\nthe time efficiency can be dramatically improved with a simple fast\napproximation method. \n\n"}
{"id": "1310.5142", "contents": "Title: Crowdsourced Task Routing via Matrix Factorization Abstract: We describe methods to predict a crowd worker's accuracy on new tasks based\non his accuracy on past tasks. Such prediction provides a foundation for\nidentifying the best workers to route work to in order to maximize accuracy on\nthe new task. Our key insight is to model similarity of past tasks to the\ntarget task such that past task accuracies can be optimally integrated to\npredict target task accuracy. We describe two matrix factorization (MF)\napproaches from collaborative filtering which not only exploit such task\nsimilarity, but are known to be robust to sparse data. Experiments on synthetic\nand real-world datasets provide feasibility assessment and comparative\nevaluation of MF approaches vs. two baseline methods. Across a range of data\nscales and task similarity conditions, we evaluate: 1) prediction error over\nall workers; and 2) how well each method predicts the best workers to use for\neach task. Results show the benefit of task routing over random assignment, the\nstrength of probabilistic MF over baseline methods, and the robustness of\nmethods under different conditions. \n\n"}
{"id": "1311.0701", "contents": "Title: On Fast Dropout and its Applicability to Recurrent Networks Abstract: Recurrent Neural Networks (RNNs) are rich models for the processing of\nsequential data. Recent work on advancing the state of the art has been focused\non the optimization or modelling of RNNs, mostly motivated by adressing the\nproblems of the vanishing and exploding gradients. The control of overfitting\nhas seen considerably less attention. This paper contributes to that by\nanalyzing fast dropout, a recent regularization method for generalized linear\nmodels and neural networks from a back-propagation inspired perspective. We\nshow that fast dropout implements a quadratic form of an adaptive,\nper-parameter regularizer, which rewards large weights in the light of\nunderfitting, penalizes them for overconfident predictions and vanishes at\nminima of an unregularized training loss. The derivatives of that regularizer\nare exclusively based on the training error signal. One consequence of this is\nthe absense of a global weight attractor, which is particularly appealing for\nRNNs, since the dynamics are not biased towards a certain regime. We positively\ntest the hypothesis that this improves the performance of RNNs on four musical\ndata sets. \n\n"}
{"id": "1311.1406", "contents": "Title: TOP-SPIN: TOPic discovery via Sparse Principal component INterference Abstract: We propose a novel topic discovery algorithm for unlabeled images based on\nthe bag-of-words (BoW) framework. We first extract a dictionary of visual words\nand subsequently for each image compute a visual word occurrence histogram. We\nview these histograms as rows of a large matrix from which we extract sparse\nprincipal components (PCs). Each PC identifies a sparse combination of visual\nwords which co-occur frequently in some images but seldom appear in others.\nEach sparse PC corresponds to a topic, and images whose interference with the\nPC is high belong to that topic, revealing the common parts possessed by the\nimages. We propose to solve the associated sparse PCA problems using an\nAlternating Maximization (AM) method, which we modify for purpose of\nefficiently extracting multiple PCs in a deflation scheme. Our approach attacks\nthe maximization problem in sparse PCA directly and is scalable to\nhigh-dimensional data. Experiments on automatic topic discovery and category\nprediction demonstrate encouraging performance of our approach. \n\n"}
{"id": "1311.2234", "contents": "Title: FuSSO: Functional Shrinkage and Selection Operator Abstract: We present the FuSSO, a functional analogue to the LASSO, that efficiently\nfinds a sparse set of functional input covariates to regress a real-valued\nresponse against. The FuSSO does so in a semi-parametric fashion, making no\nparametric assumptions about the nature of input functional covariates and\nassuming a linear form to the mapping of functional covariates to the response.\nWe provide a statistical backing for use of the FuSSO via proof of asymptotic\nsparsistency under various conditions. Furthermore, we observe good results on\nboth synthetic and real-world data. \n\n"}
{"id": "1311.2971", "contents": "Title: Approximate Inference in Continuous Determinantal Point Processes Abstract: Determinantal point processes (DPPs) are random point processes well-suited\nfor modeling repulsion. In machine learning, the focus of DPP-based models has\nbeen on diverse subset selection from a discrete and finite base set. This\ndiscrete setting admits an efficient sampling algorithm based on the\neigendecomposition of the defining kernel matrix. Recently, there has been\ngrowing interest in using DPPs defined on continuous spaces. While the\ndiscrete-DPP sampler extends formally to the continuous case, computationally,\nthe steps required are not tractable in general. In this paper, we present two\nefficient DPP sampling schemes that apply to a wide range of kernel functions:\none based on low rank approximations via Nystrom and random Fourier feature\ntechniques and another based on Gibbs sampling. We demonstrate the utility of\ncontinuous DPPs in repulsive mixture modeling and synthesizing human poses\nspanning activity spaces. \n\n"}
{"id": "1311.3064", "contents": "Title: Ranking users, papers and authors in online scientific communities Abstract: The ever-increasing quantity and complexity of scientific production have\nmade it difficult for researchers to keep track of advances in their own\nfields. This, together with growing popularity of online scientific\ncommunities, calls for the development of effective information filtering\ntools. We propose here a method to simultaneously compute reputation of users\nand quality of scientific artifacts in an online scientific community.\nEvaluation on artificially-generated data and real data from the Econophysics\nForum is used to determine the method's best-performing variants. We show that\nwhen the method is extended by considering author credit, its performance\nimproves on multiple levels. In particular, top papers have higher citation\ncount and top authors have higher $h$-index than top papers and top authors\nchosen by other algorithms. \n\n"}
{"id": "1311.4150", "contents": "Title: Towards Big Topic Modeling Abstract: To solve the big topic modeling problem, we need to reduce both time and\nspace complexities of batch latent Dirichlet allocation (LDA) algorithms.\nAlthough parallel LDA algorithms on the multi-processor architecture have low\ntime and space complexities, their communication costs among processors often\nscale linearly with the vocabulary size and the number of topics, leading to a\nserious scalability problem. To reduce the communication complexity among\nprocessors for a better scalability, we propose a novel communication-efficient\nparallel topic modeling architecture based on power law, which consumes orders\nof magnitude less communication time when the number of topics is large. We\ncombine the proposed communication-efficient parallel architecture with the\nonline belief propagation (OBP) algorithm referred to as POBP for big topic\nmodeling tasks. Extensive empirical results confirm that POBP has the following\nadvantages to solve the big topic modeling problem: 1) high accuracy, 2)\ncommunication-efficient, 3) fast speed, and 4) constant memory usage when\ncompared with recent state-of-the-art parallel LDA algorithms on the\nmulti-processor architecture. \n\n"}
{"id": "1312.0182", "contents": "Title: Query Segmentation for Relevance Ranking in Web Search Abstract: In this paper, we try to answer the question of how to improve the\nstate-of-the-art methods for relevance ranking in web search by query\nsegmentation. Here, by query segmentation it is meant to segment the input\nquery into segments, typically natural language phrases, so that the\nperformance of relevance ranking in search is increased. We propose employing\nthe re-ranking approach in query segmentation, which first employs a generative\nmodel to create top $k$ candidates and then employs a discriminative model to\nre-rank the candidates to obtain the final segmentation result. The method has\nbeen widely utilized for structure prediction in natural language processing,\nbut has not been applied to query segmentation, as far as we know. Furthermore,\nwe propose a new method for using the result of query segmentation in relevance\nranking, which takes both the original query words and the segmented query\nphrases as units of query representation. We investigate whether our method can\nimprove three relevance models, namely BM25, key n-gram model, and dependency\nmodel. Our experimental results on three large scale web search datasets show\nthat our method can indeed significantly improve relevance ranking in all the\nthree cases. \n\n"}
{"id": "1312.2244", "contents": "Title: Time-dependent Hierarchical Dirichlet Model for Timeline Generation Abstract: Timeline Generation aims at summarizing news from different epochs and\ntelling readers how an event evolves. It is a new challenge that combines\nsalience ranking with novelty detection. For long-term public events, the main\ntopic usually includes various aspects across different epochs and each aspect\nhas its own evolving pattern. Existing approaches neglect such hierarchical\ntopic structure involved in the news corpus in timeline generation. In this\npaper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) for\ntimeline generation. Our model can aptly detect different levels of topic\ninformation across corpus and such structure is further used for sentence\nselection. Based on the topic mined fro HDM, sentences are selected by\nconsidering different aspects such as relevance, coherence and coverage. We\ndevelop experimental systems to evaluate 8 long-term events that public\nconcern. Performance comparison between different systems demonstrates the\neffectiveness of our model in terms of ROUGE metrics. \n\n"}
{"id": "1312.2844", "contents": "Title: mARC: Memory by Association and Reinforcement of Contexts Abstract: This paper introduces the memory by Association and Reinforcement of Contexts\n(mARC). mARC is a novel data modeling technology rooted in the second\nquantization formulation of quantum mechanics. It is an all-purpose incremental\nand unsupervised data storage and retrieval system which can be applied to all\ntypes of signal or data, structured or unstructured, textual or not. mARC can\nbe applied to a wide range of information clas-sification and retrieval\nproblems like e-Discovery or contextual navigation. It can also for-mulated in\nthe artificial life framework a.k.a Conway \"Game Of Life\" Theory. In contrast\nto Conway approach, the objects evolve in a massively multidimensional space.\nIn order to start evaluating the potential of mARC we have built a mARC-based\nInternet search en-gine demonstrator with contextual functionality. We compare\nthe behavior of the mARC demonstrator with Google search both in terms of\nperformance and relevance. In the study we find that the mARC search engine\ndemonstrator outperforms Google search by an order of magnitude in response\ntime while providing more relevant results for some classes of queries. \n\n"}
{"id": "1312.2986", "contents": "Title: Notes on discrepancy in the pairwise comparisons method Abstract: The pairwise comparisons method is a convenient tool used when the relative\norder among different concepts (alternatives) needs to be determined. One\npopular implementation of the method is based on solving an eigenvalue problem\nfor the pairwise comparisons matrix. In such cases the ranking result the\nprincipal eigenvector of the pairwise comparison matrix is adopted, whilst the\neigenvalue is used to determine the index of inconsistency. A lot of research\nhas been devoted to the critical analysis of the eigenvalue based approach. One\nof them is the work (Bana e Costa and Vansnick, 2008). In their work authors\ndefine the conditions of order preservation (COP) and show that even for a\nsufficiently consistent pairwise comparisons matrices, this condition can not\nbe met. The present work defines a more precise criteria for determining when\nthe COP is met. To formulate the criteria a discrepancy factor is used\ndescribing how far the input to the ranking procedure is from the ranking\nresult. \n\n"}
{"id": "1312.6114", "contents": "Title: Auto-Encoding Variational Bayes Abstract: How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions are two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results. \n\n"}
{"id": "1401.3737", "contents": "Title: Coordinate Descent with Online Adaptation of Coordinate Frequencies Abstract: Coordinate descent (CD) algorithms have become the method of choice for\nsolving a number of optimization problems in machine learning. They are\nparticularly popular for training linear models, including linear support\nvector machine classification, LASSO regression, and logistic regression.\n  We consider general CD with non-uniform selection of coordinates. Instead of\nfixing selection frequencies beforehand we propose an online adaptation\nmechanism for this important parameter, called the adaptive coordinate\nfrequencies (ACF) method. This mechanism removes the need to estimate optimal\ncoordinate frequencies beforehand, and it automatically reacts to changing\nrequirements during an optimization run.\n  We demonstrate the usefulness of our ACF-CD approach for a variety of\noptimization problems arising in machine learning contexts. Our algorithm\noffers significant speed-ups over state-of-the-art training methods. \n\n"}
{"id": "1401.5899", "contents": "Title: Kernel Least Mean Square with Adaptive Kernel Size Abstract: Kernel adaptive filters (KAF) are a class of powerful nonlinear filters\ndeveloped in Reproducing Kernel Hilbert Space (RKHS). The Gaussian kernel is\nusually the default kernel in KAF algorithms, but selecting the proper kernel\nsize (bandwidth) is still an open important issue especially for learning with\nsmall sample sizes. In previous research, the kernel size was set manually or\nestimated in advance by Silvermans rule based on the sample distribution. This\nstudy aims to develop an online technique for optimizing the kernel size of the\nkernel least mean square (KLMS) algorithm. A sequential optimization strategy\nis proposed, and a new algorithm is developed, in which the filter weights and\nthe kernel size are both sequentially updated by stochastic gradient algorithms\nthat minimize the mean square error (MSE). Theoretical results on convergence\nare also presented. The excellent performance of the new algorithm is confirmed\nby simulations on static function estimation and short term chaotic time series\nprediction. \n\n"}
{"id": "1401.6024", "contents": "Title: Matrix factorization with Binary Components Abstract: Motivated by an application in computational biology, we consider low-rank\nmatrix factorization with $\\{0,1\\}$-constraints on one of the factors and\noptionally convex constraints on the second one. In addition to the\nnon-convexity shared with other matrix factorization schemes, our problem is\nfurther complicated by a combinatorial constraint set of size $2^{m \\cdot r}$,\nwhere $m$ is the dimension of the data points and $r$ the rank of the\nfactorization. Despite apparent intractability, we provide - in the line of\nrecent work on non-negative matrix factorization by Arora et al. (2012) - an\nalgorithm that provably recovers the underlying factorization in the exact case\nwith $O(m r 2^r + mnr + r^2 n)$ operations for $n$ datapoints. To obtain this\nresult, we use theory around the Littlewood-Offord lemma from combinatorics. \n\n"}
{"id": "1401.6124", "contents": "Title: Iterative Universal Hash Function Generator for Minhashing Abstract: Minhashing is a technique used to estimate the Jaccard Index between two sets\nby exploiting the probability of collision in a random permutation. In order to\nspeed up the computation, a random permutation can be approximated by using an\nuniversal hash function such as the $h_{a,b}$ function proposed by Carter and\nWegman. A better estimate of the Jaccard Index can be achieved by using many of\nthese hash functions, created at random. In this paper a new iterative\nprocedure to generate a set of $h_{a,b}$ functions is devised that eliminates\nthe need for a list of random values and avoid the multiplication operation\nduring the calculation. The properties of the generated hash functions remains\nthat of an universal hash function family. This is possible due to the random\nnature of features occurrence on sparse datasets. Results show that the\nuniformity of hashing the features is maintaned while obtaining a speed up of\nup to $1.38$ compared to the traditional approach. \n\n"}
{"id": "1402.2447", "contents": "Title: A comparison of linear and non-linear calibrations for speaker\n  recognition Abstract: In recent work on both generative and discriminative score to\nlog-likelihood-ratio calibration, it was shown that linear transforms give good\naccuracy only for a limited range of operating points. Moreover, these methods\nrequired tailoring of the calibration training objective functions in order to\ntarget the desired region of best accuracy. Here, we generalize the linear\nrecipes to non-linear ones. We experiment with a non-linear, non-parametric,\ndiscriminative PAV solution, as well as parametric, generative,\nmaximum-likelihood solutions that use Gaussian, Student's T and\nnormal-inverse-Gaussian score distributions. Experiments on NIST SRE'12 scores\nsuggest that the non-linear methods provide wider ranges of optimal accuracy\nand can be trained without having to resort to objective function tailoring. \n\n"}
{"id": "1402.3144", "contents": "Title: A Robust Ensemble Approach to Learn From Positive and Unlabeled Data\n  Using SVM Base Models Abstract: We present a novel approach to learn binary classifiers when only positive\nand unlabeled instances are available (PU learning). This problem is routinely\ncast as a supervised task with label noise in the negative set. We use an\nensemble of SVM models trained on bootstrap resamples of the training data for\nincreased robustness against label noise. The approach can be considered in a\nbagging framework which provides an intuitive explanation for its mechanics in\na semi-supervised setting. We compared our method to state-of-the-art\napproaches in simulations using multiple public benchmark data sets. The\nincluded benchmark comprises three settings with increasing label noise: (i)\nfully supervised, (ii) PU learning and (iii) PU learning with false positives.\nOur approach shows a marginal improvement over existing methods in the second\nsetting and a significant improvement in the third. \n\n"}
{"id": "1402.5565", "contents": "Title: Semi-Supervised Nonlinear Distance Metric Learning via Forests of\n  Max-Margin Cluster Hierarchies Abstract: Metric learning is a key problem for many data mining and machine learning\napplications, and has long been dominated by Mahalanobis methods. Recent\nadvances in nonlinear metric learning have demonstrated the potential power of\nnon-Mahalanobis distance functions, particularly tree-based functions. We\npropose a novel nonlinear metric learning method that uses an iterative,\nhierarchical variant of semi-supervised max-margin clustering to construct a\nforest of cluster hierarchies, where each individual hierarchy can be\ninterpreted as a weak metric over the data. By introducing randomness during\nhierarchy training and combining the output of many of the resulting\nsemi-random weak hierarchy metrics, we can obtain a powerful and robust\nnonlinear metric model. This method has two primary contributions: first, it is\nsemi-supervised, incorporating information from both constrained and\nunconstrained points. Second, we take a relaxed approach to constraint\nsatisfaction, allowing the method to satisfy different subsets of the\nconstraints at different levels of the hierarchy rather than attempting to\nsimultaneously satisfy all of them. This leads to a more robust learning\nalgorithm. We compare our method to a number of state-of-the-art benchmarks on\n$k$-nearest neighbor classification, large-scale image retrieval and\nsemi-supervised clustering problems, and find that our algorithm yields results\ncomparable or superior to the state-of-the-art, and is significantly more\nrobust to noise. \n\n"}
{"id": "1402.5836", "contents": "Title: Avoiding pathologies in very deep networks Abstract: Choosing appropriate architectures and regularization strategies for deep\nnetworks is crucial to good predictive performance. To shed light on this\nproblem, we analyze the analogous problem of constructing useful priors on\ncompositions of functions. Specifically, we study the deep Gaussian process, a\ntype of infinitely-wide, deep neural network. We show that in standard\narchitectures, the representational capacity of the network tends to capture\nfewer degrees of freedom as the number of layers increases, retaining only a\nsingle degree of freedom in the limit. We propose an alternate network\narchitecture which does not suffer from this pathology. We also examine deep\ncovariance functions, obtained by composing infinitely many feature transforms.\nLastly, we characterize the class of models obtained by performing dropout on\nGaussian processes. \n\n"}
{"id": "1403.1451", "contents": "Title: Real-Time Classification of Twitter Trends Abstract: Social media users give rise to social trends as they share about common\ninterests, which can be triggered by different reasons. In this work, we\nexplore the types of triggers that spark trends on Twitter, introducing a\ntypology with following four types: 'news', 'ongoing events', 'memes', and\n'commemoratives'. While previous research has analyzed trending topics in a\nlong term, we look at the earliest tweets that produce a trend, with the aim of\ncategorizing trends early on. This would allow to provide a filtered subset of\ntrends to end users. We analyze and experiment with a set of straightforward\nlanguage-independent features based on the social spread of trends to\ncategorize them into the introduced typology. Our method provides an efficient\nway to accurately categorize trending topics without need of external data,\nenabling news organizations to discover breaking news in real-time, or to\nquickly identify viral memes that might enrich marketing decisions, among\nothers. The analysis of social features also reveals patterns associated with\neach type of trend, such as tweets about ongoing events being shorter as many\nwere likely sent from mobile devices, or memes having more retweets originating\nfrom a few trend-setters. \n\n"}
{"id": "1403.3741", "contents": "Title: Near-optimal Reinforcement Learning in Factored MDPs Abstract: Any reinforcement learning algorithm that applies to all Markov decision\nprocesses (MDPs) will suffer $\\Omega(\\sqrt{SAT})$ regret on some MDP, where $T$\nis the elapsed time and $S$ and $A$ are the cardinalities of the state and\naction spaces. This implies $T = \\Omega(SA)$ time to guarantee a near-optimal\npolicy. In many settings of practical interest, due to the curse of\ndimensionality, $S$ and $A$ can be so enormous that this learning time is\nunacceptable. We establish that, if the system is known to be a \\emph{factored}\nMDP, it is possible to achieve regret that scales polynomially in the number of\n\\emph{parameters} encoding the factored MDP, which may be exponentially smaller\nthan $S$ or $A$. We provide two algorithms that satisfy near-optimal regret\nbounds in this context: posterior sampling reinforcement learning (PSRL) and an\nupper confidence bound algorithm (UCRL-Factored). \n\n"}
{"id": "1403.5607", "contents": "Title: Bayesian Optimization with Unknown Constraints Abstract: Recent work on Bayesian optimization has shown its effectiveness in global\noptimization of difficult black-box objective functions. Many real-world\noptimization problems of interest also have constraints which are unknown a\npriori. In this paper, we study Bayesian optimization for constrained problems\nin the general case that noise may be present in the constraint functions, and\nthe objective and constraints may be evaluated independently. We provide\nmotivating practical examples, and present a general framework to solve such\nproblems. We demonstrate the effectiveness of our approach on optimizing the\nperformance of online latent Dirichlet allocation subject to topic sparsity\nconstraints, tuning a neural network given test-time memory constraints, and\noptimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed\ntime, subject to passing standard convergence diagnostics. \n\n"}
{"id": "1404.0751", "contents": "Title: Subspace Learning from Extremely Compressed Measurements Abstract: We consider learning the principal subspace of a large set of vectors from an\nextremely small number of compressive measurements of each vector. Our\ntheoretical results show that even a constant number of measurements per column\nsuffices to approximate the principal subspace to arbitrary precision, provided\nthat the number of vectors is large. This result is achieved by a simple\nalgorithm that computes the eigenvectors of an estimate of the covariance\nmatrix. The main insight is to exploit an averaging effect that arises from\napplying a different random projection to each vector. We provide a number of\nsimulations confirming our theoretical results. \n\n"}
{"id": "1404.5903", "contents": "Title: Most Correlated Arms Identification Abstract: We study the problem of finding the most mutually correlated arms among many\narms. We show that adaptive arms sampling strategies can have significant\nadvantages over the non-adaptive uniform sampling strategy. Our proposed\nalgorithms rely on a novel correlation estimator. The use of this accurate\nestimator allows us to get improved results for a wide range of problem\ninstances. \n\n"}
{"id": "1405.4402", "contents": "Title: Peacock: Learning Long-Tail Topic Features for Industrial Applications Abstract: Latent Dirichlet allocation (LDA) is a popular topic modeling technique in\nacademia but less so in industry, especially in large-scale applications\ninvolving search engine and online advertising systems. A main underlying\nreason is that the topic models used have been too small in scale to be useful;\nfor example, some of the largest LDA models reported in literature have up to\n$10^3$ topics, which cover difficultly the long-tail semantic word sets. In\nthis paper, we show that the number of topics is a key factor that can\nsignificantly boost the utility of topic-modeling systems. In particular, we\nshow that a \"big\" LDA model with at least $10^5$ topics inferred from $10^9$\nsearch queries can achieve a significant improvement on industrial search\nengine and online advertising systems, both of which serving hundreds of\nmillions of users. We develop a novel distributed system called Peacock to\nlearn big LDA models from big data. The main features of Peacock include\nhierarchical distributed architecture, real-time prediction and topic\nde-duplication. We empirically demonstrate that the Peacock system is capable\nof providing significant benefits via highly scalable LDA topic models for\nseveral industrial applications. \n\n"}
{"id": "1405.5869", "contents": "Title: Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search\n  (MIPS) Abstract: We present the first provably sublinear time algorithm for approximate\n\\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first\nhashing algorithm for searching with (un-normalized) inner product as the\nunderlying similarity measure. Finding hashing schemes for MIPS was considered\nhard. We formally show that the existing Locality Sensitive Hashing (LSH)\nframework is insufficient for solving MIPS, and then we extend the existing LSH\nframework to allow asymmetric hashing schemes. Our proposal is based on an\ninteresting mathematical phenomenon in which inner products, after independent\nasymmetric transformations, can be converted into the problem of approximate\nnear neighbor search. This key observation makes efficient sublinear hashing\nscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we\nprovide an explicit construction of provably fast hashing scheme for MIPS. The\nproposed construction and the extended LSH framework could be of independent\ntheoretical interest. Our proposed algorithm is simple and easy to implement.\nWe evaluate the method, for retrieving inner products, in the collaborative\nfiltering task of item recommendations on Netflix and Movielens datasets. \n\n"}
{"id": "1406.0189", "contents": "Title: Convex Total Least Squares Abstract: We study the total least squares (TLS) problem that generalizes least squares\nregression by allowing measurement errors in both dependent and independent\nvariables. TLS is widely used in applied fields including computer vision,\nsystem identification and econometrics. The special case when all dependent and\nindependent variables have the same level of uncorrelated Gaussian noise, known\nas ordinary TLS, can be solved by singular value decomposition (SVD). However,\nSVD cannot solve many important practical TLS problems with realistic noise\nstructure, such as having varying measurement noise, known structure on the\nerrors, or large outliers requiring robust error-norms. To solve such problems,\nwe develop convex relaxation approaches for a general class of structured TLS\n(STLS). We show both theoretically and experimentally, that while the plain\nnuclear norm relaxation incurs large approximation errors for STLS, the\nre-weighted nuclear norm approach is very effective, and achieves better\naccuracy on challenging STLS problems than popular non-convex solvers. We\ndescribe a fast solution based on augmented Lagrangian formulation, and apply\nour approach to an important class of biological problems that use population\naverage measurements to infer cell-type and physiological-state specific\nexpression levels that are very hard to measure directly. \n\n"}
{"id": "1406.1485", "contents": "Title: Iterative Neural Autoregressive Distribution Estimator (NADE-k) Abstract: Training of the neural autoregressive density estimator (NADE) can be viewed\nas doing one step of probabilistic inference on missing values in data. We\npropose a new model that extends this inference scheme to multiple steps,\narguing that it is easier to learn to improve a reconstruction in $k$ steps\nrather than to learn to reconstruct in a single inference step. The proposed\nmodel is an unsupervised building block for deep learning that combines the\ndesirable properties of NADE and multi-predictive training: (1) Its test\nlikelihood can be computed analytically, (2) it is easy to generate independent\nsamples from it, and (3) it uses an inference engine that is a superset of\nvariational inference for Boltzmann machines. The proposed NADE-k is\ncompetitive with the state-of-the-art in density estimation on the two datasets\ntested. \n\n"}
{"id": "1406.2082", "contents": "Title: Fast and Flexible ADMM Algorithms for Trend Filtering Abstract: This paper presents a fast and robust algorithm for trend filtering, a\nrecently developed nonparametric regression tool. It has been shown that, for\nestimating functions whose derivatives are of bounded variation, trend\nfiltering achieves the minimax optimal error rate, while other popular methods\nlike smoothing splines and kernels do not. Standing in the way of a more\nwidespread practical adoption, however, is a lack of scalable and numerically\nstable algorithms for fitting trend filtering estimates. This paper presents a\nhighly efficient, specialized ADMM routine for trend filtering. Our algorithm\nis competitive with the specialized interior point methods that are currently\nin use, and yet is far more numerically robust. Furthermore, the proposed ADMM\nimplementation is very simple, and importantly, it is flexible enough to extend\nto many interesting related problems, such as sparse trend filtering and\nisotonic trend filtering. Software for our method is freely available, in both\nthe C and R languages. \n\n"}
{"id": "1406.2541", "contents": "Title: Predictive Entropy Search for Efficient Global Optimization of Black-box\n  Functions Abstract: We propose a novel information-theoretic approach for Bayesian optimization\ncalled Predictive Entropy Search (PES). At each iteration, PES selects the next\nevaluation point that maximizes the expected information gained with respect to\nthe global maximum. PES codifies this intractable acquisition function in terms\nof the expected reduction in the differential entropy of the predictive\ndistribution. This reformulation allows PES to obtain approximations that are\nboth more accurate and efficient than other alternatives such as Entropy Search\n(ES). Furthermore, PES can easily perform a fully Bayesian treatment of the\nmodel hyperparameters while ES cannot. We evaluate PES in both synthetic and\nreal-world applications, including optimization problems in machine learning,\nfinance, biotechnology, and robotics. We show that the increased accuracy of\nPES leads to significant gains in optimization performance. \n\n"}
{"id": "1406.3190", "contents": "Title: Online Optimization for Large-Scale Max-Norm Regularization Abstract: Max-norm regularizer has been extensively studied in the last decade as it\npromotes an effective low-rank estimation for the underlying data. However,\nsuch max-norm regularized problems are typically formulated and solved in a\nbatch manner, which prevents it from processing big data due to possible memory\nbudget. In this paper, hence, we propose an online algorithm that is scalable\nto large-scale setting. Particularly, we consider the matrix decomposition\nproblem as an example, although a simple variant of the algorithm and analysis\ncan be adapted to other important problems such as matrix completion. The\ncrucial technique in our implementation is to reformulating the max-norm to an\nequivalent matrix factorization form, where the factors consist of a (possibly\novercomplete) basis component and a coefficients one. In this way, we may\nmaintain the basis component in the memory and optimize over it and the\ncoefficients for each sample alternatively. Since the memory footprint of the\nbasis component is independent of the sample size, our algorithm is appealing\nwhen manipulating a large collection of samples. We prove that the sequence of\nthe solutions (i.e., the basis component) produced by our algorithm converges\nto a stationary point of the expected loss function asymptotically. Numerical\nstudy demonstrates encouraging results for the efficacy and robustness of our\nalgorithm compared to the widely used nuclear norm solvers. \n\n"}
{"id": "1406.3650", "contents": "Title: Smoothed Gradients for Stochastic Variational Inference Abstract: Stochastic variational inference (SVI) lets us scale up Bayesian computation\nto massive data. It uses stochastic optimization to fit a variational\ndistribution, following easy-to-compute noisy natural gradients. As with most\ntraditional stochastic optimization methods, SVI takes precautions to use\nunbiased stochastic gradients whose expectations are equal to the true\ngradients. In this paper, we explore the idea of following biased stochastic\ngradients in SVI. Our method replaces the natural gradient with a similarly\nconstructed vector that uses a fixed-window moving average of some of its\nprevious terms. We will demonstrate the many advantages of this technique.\nFirst, its computational cost is the same as for SVI and storage requirements\nonly multiply by a constant factor. Second, it enjoys significant variance\nreduction over the unbiased estimates, smaller bias than averaged gradients,\nand leads to smaller mean-squared error against the full gradient. We test our\nmethod on latent Dirichlet allocation with three large corpora. \n\n"}
{"id": "1406.3852", "contents": "Title: A low variance consistent test of relative dependency Abstract: We describe a novel non-parametric statistical hypothesis test of relative\ndependence between a source variable and two candidate target variables. Such a\ntest enables us to determine whether one source variable is significantly more\ndependent on a first target variable or a second. Dependence is measured via\nthe Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of\nempirical dependence measures (source-target 1, source-target 2). We test\nwhether the first dependence measure is significantly larger than the second.\nModeling the covariance between these HSIC statistics leads to a provably more\npowerful test than the construction of independent HSIC statistics by\nsub-sampling. The resulting test is consistent and unbiased, and (being based\non U-statistics) has favorable convergence properties. The test can be computed\nin quadratic time, matching the computational complexity of standard empirical\nHSIC estimators. The effectiveness of the test is demonstrated on several\nreal-world problems: we identify language groups from a multilingual corpus,\nand we prove that tumor location is more dependent on gene expression than\nchromosomal imbalances. Source code is available for download at\nhttps://github.com/wbounliphone/reldep. \n\n"}
{"id": "1406.4877", "contents": "Title: On the Application of Generic Summarization Algorithms to Music Abstract: Several generic summarization algorithms were developed in the past and\nsuccessfully applied in fields such as text and speech summarization. In this\npaper, we review and apply these algorithms to music. To evaluate this\nsummarization's performance, we adopt an extrinsic approach: we compare a Fado\nGenre Classifier's performance using truncated contiguous clips against the\nsummaries extracted with those algorithms on 2 different datasets. We show that\nMaximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA)\nall improve classification performance in both datasets used for testing. \n\n"}
{"id": "1406.6312", "contents": "Title: Scalable Topical Phrase Mining from Text Corpora Abstract: While most topic modeling algorithms model text corpora with unigrams, human\ninterpretation often relies on inherent grouping of terms into phrases. As\nsuch, we consider the problem of discovering topical phrases of mixed lengths.\nExisting work either performs post processing to the inference results of\nunigram-based topic models, or utilizes complex n-gram-discovery topic models.\nThese methods generally produce low-quality topical phrases or suffer from poor\nscalability on even moderately-sized datasets. We propose a different approach\nthat is both computationally efficient and effective. Our solution combines a\nnovel phrase mining framework to segment a document into single and multi-word\nphrases, and a new topic model that operates on the induced document partition.\nOur approach discovers high quality topical phrases with negligible extra cost\nto the bag-of-words topic model in a variety of datasets including research\npublication titles, abstracts, reviews, and news articles. \n\n"}
{"id": "1407.3422", "contents": "Title: A Spectral Algorithm for Inference in Hidden Semi-Markov Models Abstract: Hidden semi-Markov models (HSMMs) are latent variable models which allow\nlatent state persistence and can be viewed as a generalization of the popular\nhidden Markov models (HMMs). In this paper, we introduce a novel spectral\nalgorithm to perform inference in HSMMs. Unlike expectation maximization (EM),\nour approach correctly estimates the probability of given observation sequence\nbased on a set of training sequences. Our approach is based on estimating\nmoments from the sample, whose number of dimensions depends only\nlogarithmically on the maximum length of the hidden state persistence.\nMoreover, the algorithm requires only a few matrix inversions and is therefore\ncomputationally efficient. Empirical evaluations on synthetic and real data\ndemonstrate the advantage of the algorithm over EM in terms of speed and\naccuracy, especially for large datasets. \n\n"}
{"id": "1407.4430", "contents": "Title: Sequential Logistic Principal Component Analysis (SLPCA): Dimensional\n  Reduction in Streaming Multivariate Binary-State System Abstract: Sequential or online dimensional reduction is of interests due to the\nexplosion of streaming data based applications and the requirement of adaptive\nstatistical modeling, in many emerging fields, such as the modeling of energy\nend-use profile. Principal Component Analysis (PCA), is the classical way of\ndimensional reduction. However, traditional Singular Value Decomposition (SVD)\nbased PCA fails to model data which largely deviates from Gaussian\ndistribution. The Bregman Divergence was recently introduced to achieve a\ngeneralized PCA framework. If the random variable under dimensional reduction\nfollows Bernoulli distribution, which occurs in many emerging fields, the\ngeneralized PCA is called Logistic PCA (LPCA). In this paper, we extend the\nbatch LPCA to a sequential version (i.e. SLPCA), based on the sequential convex\noptimization theory. The convergence property of this algorithm is discussed\ncompared to the batch version of LPCA (i.e. BLPCA), as well as its performance\nin reducing the dimension for multivariate binary-state systems. Its\napplication in building energy end-use profile modeling is also investigated. \n\n"}
{"id": "1407.5158", "contents": "Title: Tight convex relaxations for sparse matrix factorization Abstract: Based on a new atomic norm, we propose a new convex formulation for sparse\nmatrix factorization problems in which the number of nonzero elements of the\nfactors is assumed fixed and known. The formulation counts sparse PCA with\nmultiple factors, subspace clustering and low-rank sparse bilinear regression\nas potential applications. We compute slow rates and an upper bound on the\nstatistical dimension of the suggested norm for rank 1 matrices, showing that\nits statistical dimension is an order of magnitude smaller than the usual\n$\\ell\\_1$-norm, trace norm and their combinations. Even though our convex\nformulation is in theory hard and does not lead to provably polynomial time\nalgorithmic schemes, we propose an active set algorithm leveraging the\nstructure of the convex problem to solve it and show promising numerical\nresults. \n\n"}
{"id": "1407.7644", "contents": "Title: Estimating the Accuracies of Multiple Classifiers Without Labeled Data Abstract: In various situations one is given only the predictions of multiple\nclassifiers over a large unlabeled test data. This scenario raises the\nfollowing questions: Without any labeled data and without any a-priori\nknowledge about the reliability of these different classifiers, is it possible\nto consistently and computationally efficiently estimate their accuracies?\nFurthermore, also in a completely unsupervised manner, can one construct a more\naccurate unsupervised ensemble classifier? In this paper, focusing on the\nbinary case, we present simple, computationally efficient algorithms to solve\nthese questions. Furthermore, under standard classifier independence\nassumptions, we prove our methods are consistent and study their asymptotic\nerror. Our approach is spectral, based on the fact that the off-diagonal\nentries of the classifiers' covariance matrix and 3-d tensor are rank-one. We\nillustrate the competitive performance of our algorithms via extensive\nexperiments on both artificial and real datasets. \n\n"}
{"id": "1408.2467", "contents": "Title: Matrix Completion under Interval Uncertainty Abstract: Matrix completion under interval uncertainty can be cast as matrix completion\nwith element-wise box constraints. We present an efficient\nalternating-direction parallel coordinate-descent method for the problem. We\nshow that the method outperforms any other known method on a benchmark in image\nin-painting in terms of signal-to-noise ratio, and that it provides\nhigh-quality solutions for an instance of collaborative filtering with\n100,198,805 recommendations within 5 minutes. \n\n"}
{"id": "1408.6806", "contents": "Title: Mathematical Knowledge Representation: Semantic Models and Formalisms Abstract: The paper provides a survey of semantic methods for solution of fundamental\ntasks in mathematical knowledge management. Ontological models and formalisms\nare discussed. We propose an ontology of mathematical knowledge, covering a\nwide range of fields of mathematics. We demonstrate applications of this\nrepresentation in mathematical formula search, and learning. \n\n"}
{"id": "1409.0080", "contents": "Title: Show Me the Money: Dynamic Recommendations for Revenue Maximization Abstract: Recommender Systems (RS) play a vital role in applications such as e-commerce\nand on-demand content streaming. Research on RS has mainly focused on the\ncustomer perspective, i.e., accurate prediction of user preferences and\nmaximization of user utilities. As a result, most existing techniques are not\nexplicitly built for revenue maximization, the primary business goal of\nenterprises. In this work, we explore and exploit a novel connection between RS\nand the profitability of a business. As recommendations can be seen as an\ninformation channel between a business and its customers, it is interesting and\nimportant to investigate how to make strategic dynamic recommendations leading\nto maximum possible revenue. To this end, we propose a novel \\model that takes\ninto account a variety of factors including prices, valuations, saturation\neffects, and competition amongst products. Under this model, we study the\nproblem of finding revenue-maximizing recommendation strategies over a finite\ntime horizon. We show that this problem is NP-hard, but approximation\nguarantees can be obtained for a slightly relaxed version, by establishing an\nelegant connection to matroid theory. Given the prohibitively high complexity\nof the approximation algorithm, we also design intelligent heuristics for the\noriginal problem. Finally, we conduct extensive experiments on two real and\nsynthetic datasets and demonstrate the efficiency, scalability, and\neffectiveness our algorithms, and that they significantly outperform several\nintuitive baselines. \n\n"}
{"id": "1409.2042", "contents": "Title: Recommendation Subgraphs for Web Discovery Abstract: Recommendations are central to the utility of many websites including\nYouTube, Quora as well as popular e-commerce stores. Such sites typically\ncontain a set of recommendations on every product page that enables visitors to\neasily navigate the website. Choosing an appropriate set of recommendations at\neach page is one of the key features of backend engines that have been deployed\nat several e-commerce sites.\n  Specifically at BloomReach, an engine consisting of several independent\ncomponents analyzes and optimizes its clients' websites. This paper focuses on\nthe structure optimizer component which improves the website navigation\nexperience that enables the discovery of novel content.\n  We begin by formalizing the concept of recommendations used for discovery. We\nformulate this as a natural graph optimization problem which in its simplest\ncase, reduces to a bipartite matching problem. In practice, solving these\nmatching problems requires superlinear time and is not scalable. Also,\nimplementing simple algorithms is critical in practice because they are\nsignificantly easier to maintain in production. This motivated us to analyze\nthree methods for solving the problem in increasing order of sophistication: a\nsampling algorithm, a greedy algorithm and a more involved partitioning based\nalgorithm.\n  We first theoretically analyze the performance of these three methods on\nrandom graph models characterizing when each method will yield a solution of\nsufficient quality and the parameter ranges when more sophistication is needed.\nWe complement this by providing an empirical analysis of these algorithms on\nsimulated and real-world production data. Our results confirm that it is not\nalways necessary to implement complicated algorithms in the real-world and that\nvery good practical results can be obtained by using heuristics that are backed\nby the confidence of concrete theoretical guarantees. \n\n"}
{"id": "1410.5410", "contents": "Title: Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner\n  Product Search (MIPS) Abstract: Recently it was shown that the problem of Maximum Inner Product Search (MIPS)\nis efficient and it admits provably sub-linear hashing algorithms. Asymmetric\ntransformations before hashing were the key in solving MIPS which was otherwise\nhard. In the prior work, the authors use asymmetric transformations which\nconvert the problem of approximate MIPS into the problem of approximate near\nneighbor search which can be efficiently solved using hashing. In this work, we\nprovide a different transformation which converts the problem of approximate\nMIPS into the problem of approximate cosine similarity search which can be\nefficiently solved using signed random projections. Theoretical analysis show\nthat the new scheme is significantly better than the original scheme for MIPS.\nExperimental evaluations strongly support the theoretical findings. \n\n"}
{"id": "1410.5518", "contents": "Title: On Symmetric and Asymmetric LSHs for Inner Product Search Abstract: We consider the problem of designing locality sensitive hashes (LSH) for\ninner product similarity, and of the power of asymmetric hashes in this\ncontext. Shrivastava and Li argue that there is no symmetric LSH for the\nproblem and propose an asymmetric LSH based on different mappings for query and\ndatabase points. However, we show there does exist a simple symmetric LSH that\nenjoys stronger guarantees and better empirical performance than the asymmetric\nLSH they suggest. We also show a variant of the settings where asymmetry is\nin-fact needed, but there a different asymmetric LSH is required. \n\n"}
{"id": "1410.7404", "contents": "Title: Maximally Informative Hierarchical Representations of High-Dimensional\n  Data Abstract: We consider a set of probabilistic functions of some input variables as a\nrepresentation of the inputs. We present bounds on how informative a\nrepresentation is about input data. We extend these bounds to hierarchical\nrepresentations so that we can quantify the contribution of each layer towards\ncapturing the information in the original data. The special form of these\nbounds leads to a simple, bottom-up optimization procedure to construct\nhierarchical representations that are also maximally informative about the\ndata. This optimization has linear computational complexity and constant sample\ncomplexity in the number of variables. These results establish a new approach\nto unsupervised learning of deep representations that is both principled and\npractical. We demonstrate the usefulness of the approach on both synthetic and\nreal-world data. \n\n"}
{"id": "1411.0541", "contents": "Title: Distributed Submodular Maximization Abstract: Many large-scale machine learning problems--clustering, non-parametric\nlearning, kernel machines, etc.--require selecting a small yet representative\nsubset from a large dataset. Such problems can often be reduced to maximizing a\nsubmodular set function subject to various constraints. Classical approaches to\nsubmodular optimization require centralized access to the full dataset, which\nis impractical for truly large-scale problems. In this paper, we consider the\nproblem of submodular function maximization in a distributed fashion. We\ndevelop a simple, two-stage protocol GreeDi, that is easily implemented using\nMapReduce style computations. We theoretically analyze our approach, and show\nthat under certain natural conditions, performance close to the centralized\napproach can be achieved. We begin with monotone submodular maximization\nsubject to a cardinality constraint, and then extend this approach to obtain\napproximation guarantees for (not necessarily monotone) submodular maximization\nsubject to more general constraints including matroid or knapsack constraints.\nIn our extensive experiments, we demonstrate the effectiveness of our approach\non several applications, including sparse Gaussian process inference and\nexemplar based clustering on tens of millions of examples using Hadoop. \n\n"}
{"id": "1411.3315", "contents": "Title: Statistically Significant Detection of Linguistic Change Abstract: We propose a new computational approach for tracking and detecting\nstatistically significant linguistic shifts in the meaning and usage of words.\nSuch linguistic shifts are especially prevalent on the Internet, where the\nrapid exchange of ideas can quickly change a word's meaning. Our meta-analysis\napproach constructs property time series of word usage, and then uses\nstatistically sound change point detection algorithms to identify significant\nlinguistic shifts.\n  We consider and analyze three approaches of increasing complexity to generate\nsuch linguistic property time series, the culmination of which uses\ndistributional characteristics inferred from word co-occurrences. Using\nrecently proposed deep neural language models, we first train vector\nrepresentations of words for each time period. Second, we warp the vector\nspaces into one unified coordinate system. Finally, we construct a\ndistance-based distributional time series for each word to track it's\nlinguistic displacement over time.\n  We demonstrate that our approach is scalable by tracking linguistic change\nacross years of micro-blogging using Twitter, a decade of product reviews using\na corpus of movie reviews from Amazon, and a century of written books using the\nGoogle Book-ngrams. Our analysis reveals interesting patterns of language usage\nchange commensurate with each medium. \n\n"}
{"id": "1411.3409", "contents": "Title: A Randomized Algorithm for CCA Abstract: We present RandomizedCCA, a randomized algorithm for computing canonical\nanalysis, suitable for large datasets stored either out of core or on a\ndistributed file system. Accurate results can be obtained in as few as two data\npasses, which is relevant for distributed processing frameworks in which\niteration is expensive (e.g., Hadoop). The strategy also provides an excellent\ninitializer for standard iterative solutions. \n\n"}
{"id": "1411.3436", "contents": "Title: SelfieBoost: A Boosting Algorithm for Deep Learning Abstract: We describe and analyze a new boosting algorithm for deep learning called\nSelfieBoost. Unlike other boosting algorithms, like AdaBoost, which construct\nensembles of classifiers, SelfieBoost boosts the accuracy of a single network.\nWe prove a $\\log(1/\\epsilon)$ convergence rate for SelfieBoost under some \"SGD\nsuccess\" assumption which seems to hold in practice. \n\n"}
{"id": "1411.3675", "contents": "Title: Scalable Link Prediction in Dynamic Networks via Non-Negative Matrix\n  Factorization Abstract: We propose a scalable temporal latent space model for link prediction in\ndynamic social networks, where the goal is to predict links over time based on\na sequence of previous graph snapshots. The model assumes that each user lies\nin an unobserved latent space and interactions are more likely to form between\nsimilar users in the latent space representation. In addition, the model allows\neach user to gradually move its position in the latent space as the network\nstructure evolves over time. We present a global optimization algorithm to\neffectively infer the temporal latent space, with a quadratic convergence rate.\nTwo alternative optimization algorithms with local and incremental updates are\nalso proposed, allowing the model to scale to larger networks without\ncompromising prediction accuracy. Empirically, we demonstrate that our model,\nwhen evaluated on a number of real-world dynamic networks, significantly\noutperforms existing approaches for temporal link prediction in terms of both\nscalability and predictive power. \n\n"}
{"id": "1411.3784", "contents": "Title: Deep Narrow Boltzmann Machines are Universal Approximators Abstract: We show that deep narrow Boltzmann machines are universal approximators of\nprobability distributions on the activities of their visible units, provided\nthey have sufficiently many hidden layers, each containing the same number of\nunits as the visible layer. We show that, within certain parameter domains,\ndeep Boltzmann machines can be studied as feedforward networks. We provide\nupper and lower bounds on the sufficient depth and width of universal\napproximators. These results settle various intuitions regarding undirected\nnetworks and, in particular, they show that deep narrow Boltzmann machines are\nat least as compact universal approximators as narrow sigmoid belief networks\nand restricted Boltzmann machines, with respect to the currently available\nbounds for those models. \n\n"}
{"id": "1411.7718", "contents": "Title: Classification with Noisy Labels by Importance Reweighting Abstract: In this paper, we study a classification problem in which sample labels are\nrandomly corrupted. In this scenario, there is an unobservable sample with\nnoise-free labels. However, before being observed, the true labels are\nindependently flipped with a probability $\\rho\\in[0,0.5)$, and the random label\nnoise can be class-conditional. Here, we address two fundamental problems\nraised by this scenario. The first is how to best use the abundant surrogate\nloss functions designed for the traditional classification problem when there\nis label noise. We prove that any surrogate loss function can be used for\nclassification with noisy labels by using importance reweighting, with\nconsistency assurance that the label noise does not ultimately hinder the\nsearch for the optimal classifier of the noise-free sample. The other is the\nopen problem of how to obtain the noise rate $\\rho$. We show that the rate is\nupper bounded by the conditional probability $P(y|x)$ of the noisy sample.\nConsequently, the rate can be estimated, because the upper bound can be easily\nreached in classification problems. Experimental results on synthetic and real\ndatasets confirm the efficiency of our methods. \n\n"}
{"id": "1412.0007", "contents": "Title: Paradigm shifts. Part I. Collagen. Confirming and complementing the work\n  of Henry Small Abstract: The paradigm shift in collagen research during the early 1970s marked by the\ndiscovery of the collagen precursor molecule procollagen was traced using\nco-citation analysis and title word frequency determination, confirming\nprevious work performed by Henry Small. \n\n"}
{"id": "1412.1684", "contents": "Title: How Many Communities Are There? Abstract: Stochastic blockmodels and variants thereof are among the most widely used\napproaches to community detection for social networks and relational data. A\nstochastic blockmodel partitions the nodes of a network into disjoint sets,\ncalled communities. The approach is inherently related to clustering with\nmixture models; and raises a similar model selection problem for the number of\ncommunities. The Bayesian information criterion (BIC) is a popular solution,\nhowever, for stochastic blockmodels, the conditional independence assumption\ngiven the communities of the endpoints among different edges is usually\nviolated in practice. In this regard, we propose composite likelihood BIC\n(CL-BIC) to select the number of communities, and we show it is robust against\npossible misspecifications in the underlying stochastic blockmodel assumptions.\nWe derive the requisite methodology and illustrate the approach using both\nsimulated and real data. Supplementary materials containing the relevant\ncomputer code are available online. \n\n"}
{"id": "1412.2416", "contents": "Title: Paradigm shifts. Part II. Reverse Transcriptase. Analysis of reference\n  stability and word frequencies Abstract: The reverse transcription paradigm shift in RNA tumor virus research marked\nby the discovery of the reverse transcriptase in 1970 was traced using\nco-citation and title word frequency analysis. It is shown that this event is\nassociated with a break in citation patterns and the occurrence of previously\nunknown technical terms. \n\n"}
{"id": "1412.3555", "contents": "Title: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence\n  Modeling Abstract: In this paper we compare different types of recurrent units in recurrent\nneural networks (RNNs). Especially, we focus on more sophisticated units that\nimplement a gating mechanism, such as a long short-term memory (LSTM) unit and\na recently proposed gated recurrent unit (GRU). We evaluate these recurrent\nunits on the tasks of polyphonic music modeling and speech signal modeling. Our\nexperiments revealed that these advanced recurrent units are indeed better than\nmore traditional recurrent units such as tanh units. Also, we found GRU to be\ncomparable to LSTM. \n\n"}
{"id": "1412.4864", "contents": "Title: Learning with Pseudo-Ensembles Abstract: We formalize the notion of a pseudo-ensemble, a (possibly infinite)\ncollection of child models spawned from a parent model by perturbing it\naccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep\nneural network trains a pseudo-ensemble of child subnetworks generated by\nrandomly masking nodes in the parent network. We present a novel regularizer\nbased on making the behavior of a pseudo-ensemble robust with respect to the\nnoise process generating it. In the fully-supervised setting, our regularizer\nmatches the performance of dropout. But, unlike dropout, our regularizer\nnaturally extends to the semi-supervised setting, where it produces\nstate-of-the-art results. We provide a case study in which we transform the\nRecursive Neural Tensor Network of (Socher et. al, 2013) into a\npseudo-ensemble, which significantly improves its performance on a real-world\nsentiment analysis benchmark. \n\n"}
{"id": "1412.7489", "contents": "Title: A Unified Perspective on Multi-Domain and Multi-Task Learning Abstract: In this paper, we provide a new neural-network based perspective on\nmulti-task learning (MTL) and multi-domain learning (MDL). By introducing the\nconcept of a semantic descriptor, this framework unifies MDL and MTL as well as\nencompassing various classic and recent MTL/MDL algorithms by interpreting them\nas different ways of constructing semantic descriptors. Our interpretation\nprovides an alternative pipeline for zero-shot learning (ZSL), where a model\nfor a novel class can be constructed without training data. Moreover, it leads\nto a new and practically relevant problem setting of zero-shot domain\nadaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model\nfor an unseen domain can be generated by its semantic descriptor. Experiments\nacross this range of problems demonstrate that our framework outperforms a\nvariety of alternatives. \n\n"}
{"id": "1501.03326", "contents": "Title: Unbiased Bayes for Big Data: Paths of Partial Posteriors Abstract: A key quantity of interest in Bayesian inference are expectations of\nfunctions with respect to a posterior distribution. Markov Chain Monte Carlo is\na fundamental tool to consistently compute these expectations via averaging\nsamples drawn from an approximate posterior. However, its feasibility is being\nchallenged in the era of so called Big Data as all data needs to be processed\nin every iteration. Realising that such simulation is an unnecessarily hard\nproblem if the goal is estimation, we construct a computationally scalable\nmethodology that allows unbiased estimation of the required expectations --\nwithout explicit simulation from the full posterior. The scheme's variance is\nfinite by construction and straightforward to control, leading to algorithms\nthat are provably unbiased and naturally arrive at a desired error tolerance.\nThis is achieved at an average computational complexity that is sub-linear in\nthe size of the dataset and its free parameters are easy to tune. We\ndemonstrate the utility and generality of the methodology on a range of common\nstatistical models applied to large-scale benchmark and real-world datasets. \n\n"}
{"id": "1501.06715", "contents": "Title: Time Aware Knowledge Extraction for Microblog Summarization on Twitter Abstract: Microblogging services like Twitter and Facebook collect millions of user\ngenerated content every moment about trending news, occurring events, and so\non. Nevertheless, it is really a nightmare to find information of interest\nthrough the huge amount of available posts that are often noise and redundant.\nIn general, social media analytics services have caught increasing attention\nfrom both side research and industry. Specifically, the dynamic context of\nmicroblogging requires to manage not only meaning of information but also the\nevolution of knowledge over the timeline. This work defines Time Aware\nKnowledge Extraction (briefly TAKE) methodology that relies on temporal\nextension of Fuzzy Formal Concept Analysis. In particular, a microblog\nsummarization algorithm has been defined filtering the concepts organized by\nTAKE in a time-dependent hierarchy. The algorithm addresses topic-based\nsummarization on Twitter. Besides considering the timing of the concepts,\nanother distinguish feature of the proposed microblog summarization framework\nis the possibility to have more or less detailed summary, according to the\nuser's needs, with good levels of quality and completeness as highlighted in\nthe experimental results. \n\n"}
{"id": "1502.03492", "contents": "Title: Gradient-based Hyperparameter Optimization through Reversible Learning Abstract: Tuning hyperparameters of learning algorithms is hard because gradients are\nusually unavailable. We compute exact gradients of cross-validation performance\nwith respect to all hyperparameters by chaining derivatives backwards through\nthe entire training procedure. These gradients allow us to optimize thousands\nof hyperparameters, including step-size and momentum schedules, weight\ninitialization distributions, richly parameterized regularization schemes, and\nneural network architectures. We compute hyperparameter gradients by exactly\nreversing the dynamics of stochastic gradient descent with momentum. \n\n"}
{"id": "1502.05556", "contents": "Title: Just Sort It! A Simple and Effective Approach to Active Preference\n  Learning Abstract: We address the problem of learning a ranking by using adaptively chosen\npairwise comparisons. Our goal is to recover the ranking accurately but to\nsample the comparisons sparingly. If all comparison outcomes are consistent\nwith the ranking, the optimal solution is to use an efficient sorting\nalgorithm, such as Quicksort. But how do sorting algorithms behave if some\ncomparison outcomes are inconsistent with the ranking? We give favorable\nguarantees for Quicksort for the popular Bradley-Terry model, under natural\nassumptions on the parameters. Furthermore, we empirically demonstrate that\nsorting algorithms lead to a very simple and effective active learning\nstrategy: repeatedly sort the items. This strategy performs as well as\nstate-of-the-art methods (and much better than random sampling) at a minuscule\nfraction of the computational cost. \n\n"}
{"id": "1502.05955", "contents": "Title: Stream Sampling for Frequency Cap Statistics Abstract: Unaggregated data, in streamed or distributed form, is prevalent and come\nfrom diverse application domains which include interactions of users with web\nservices and IP traffic. Data elements have {\\em keys} (cookies, users,\nqueries) and elements with different keys interleave. Analytics on such data\ntypically utilizes statistics stated in terms of the frequencies of keys. The\ntwo most common statistics are {\\em distinct}, which is the number of active\nkeys in a specified segment, and {\\em sum}, which is the sum of the frequencies\nof keys in the segment. Both are special cases of {\\em cap} statistics, defined\nas the sum of frequencies {\\em capped} by a parameter $T$, which are popular in\nonline advertising platforms. Aggregation by key, however, is costly, requiring\nstate proportional to the number of distinct keys, and therefore we are\ninterested in estimating these statistics or more generally, sampling the data,\nwithout aggregation. We present a sampling framework for unaggregated data that\nuses a single pass (for streams) or two passes (for distributed data) and state\nproportional to the desired sample size. Our design provides the first\neffective solution for general frequency cap statistics. Our $\\ell$-capped\nsamples provide estimates with tight statistical guarantees for cap statistics\nwith $T=\\Theta(\\ell)$ and nonnegative unbiased estimates of {\\em any} monotone\nnon-decreasing frequency statistics. An added benefit of our unified design is\nfacilitating {\\em multi-objective samples}, which provide estimates with\nstatistical guarantees for a specified set of different statistics, using a\nsingle, smaller sample. \n\n"}
{"id": "1502.06922", "contents": "Title: Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\n  and Application to Information Retrieval Abstract: This paper develops a model that addresses sentence embedding, a hot topic in\ncurrent natural language processing research, using recurrent neural networks\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\ngoes through the sentence, and when it reaches the last word, the hidden layer\nof the network provides a semantic representation of the whole sentence. In\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\nclick-through data logged by a commercial web search engine. Visualization and\nanalysis are performed to understand how the embedding process works. The model\nis found to automatically attenuate the unimportant words and detects the\nsalient keywords in the sentence. Furthermore, these detected keywords are\nfound to automatically activate different cells of the LSTM-RNN, where words\nbelonging to a similar topic activate the same cell. As a semantic\nrepresentation of the sentence, the embedding vector can be used in many\ndifferent applications. These automatic keyword detection and topic allocation\nabilities enabled by the LSTM-RNN allow the network to perform document\nretrieval, a difficult language processing task, where the similarity between\nthe query and documents can be measured by the distance between their\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\nseveral existing state of the art methods. We emphasize that the proposed model\ngenerates sentence embedding vectors that are specially useful for web document\nretrieval tasks. A comparison with a well known general sentence embedding\nmethod, the Paragraph Vector, is performed. The results show that the proposed\nmethod in this paper significantly outperforms it for web document retrieval\ntask. \n\n"}
{"id": "1502.07058", "contents": "Title: Evaluation of Deep Convolutional Nets for Document Image Classification\n  and Retrieval Abstract: This paper presents a new state-of-the-art for document image classification\nand retrieval, using features learned by deep convolutional neural networks\n(CNNs). In object and scene analysis, deep neural nets are capable of learning\na hierarchical chain of abstraction from pixel inputs to concise and\ndescriptive representations. The current work explores this capacity in the\nrealm of document analysis, and confirms that this representation strategy is\nsuperior to a variety of popular hand-crafted alternatives. Experiments also\nshow that (i) features extracted from CNNs are robust to compression, (ii) CNNs\ntrained on non-document images transfer well to document analysis tasks, and\n(iii) enforcing region-specific feature-learning is unnecessary given\nsufficient training data. This work also makes available a new labelled subset\nof the IIT-CDIP collection, containing 400,000 document images across 16\ncategories, useful for training new CNNs for document analysis. \n\n"}
{"id": "1503.02531", "contents": "Title: Distilling the Knowledge in a Neural Network Abstract: A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel. \n\n"}
{"id": "1503.04337", "contents": "Title: Communication-efficient sparse regression: a one-shot approach Abstract: We devise a one-shot approach to distributed sparse regression in the\nhigh-dimensional setting. The key idea is to average \"debiased\" or\n\"desparsified\" lasso estimators. We show the approach converges at the same\nrate as the lasso as long as the dataset is not split across too many machines.\nWe also extend the approach to generalized linear models. \n\n"}
{"id": "1503.05543", "contents": "Title: Text Segmentation based on Semantic Word Embeddings Abstract: We explore the use of semantic word embeddings in text segmentation\nalgorithms, including the C99 segmentation algorithm and new algorithms\ninspired by the distributed word vector representation. By developing a general\nframework for discussing a class of segmentation objectives, we study the\neffectiveness of greedy versus exact optimization approaches and suggest a new\niterative refinement technique for improving the performance of greedy\nstrategies. We compare our results to known benchmarks, using known metrics. We\ndemonstrate state-of-the-art performance for an untrained method with our\nContent Vector Segmentation (CVS) on the Choi test set. Finally, we apply the\nsegmentation procedure to an in-the-wild dataset consisting of text extracted\nfrom scholarly articles in the arXiv.org database. \n\n"}
{"id": "1503.05951", "contents": "Title: Rank Subspace Learning for Compact Hash Codes Abstract: The era of Big Data has spawned unprecedented interests in developing hashing\nalgorithms for efficient storage and fast nearest neighbor search. Most\nexisting work learn hash functions that are numeric quantizations of feature\nvalues in projected feature space. In this work, we propose a novel hash\nlearning framework that encodes feature's rank orders instead of numeric values\nin a number of optimal low-dimensional ranking subspaces. We formulate the\nranking subspace learning problem as the optimization of a piece-wise linear\nconvex-concave function and present two versions of our algorithm: one with\nindependent optimization of each hash bit and the other exploiting a sequential\nlearning framework. Our work is a generalization of the Winner-Take-All (WTA)\nhash family and naturally enjoys all the numeric stability benefits of rank\ncorrelation measures while being optimized to achieve high precision at very\nshort code length. We compare with several state-of-the-art hashing algorithms\nin both supervised and unsupervised domain, showing superior performance in a\nnumber of data sets. \n\n"}
{"id": "1503.08542", "contents": "Title: Nonparametric Relational Topic Models through Dependent Gamma Processes Abstract: Traditional Relational Topic Models provide a way to discover the hidden\ntopics from a document network. Many theoretical and practical tasks, such as\ndimensional reduction, document clustering, link prediction, benefit from this\nrevealed knowledge. However, existing relational topic models are based on an\nassumption that the number of hidden topics is known in advance, and this is\nimpractical in many real-world applications. Therefore, in order to relax this\nassumption, we propose a nonparametric relational topic model in this paper.\nInstead of using fixed-dimensional probability distributions in its generative\nmodel, we use stochastic processes. Specifically, a gamma process is assigned\nto each document, which represents the topic interest of this document.\nAlthough this method provides an elegant solution, it brings additional\nchallenges when mathematically modeling the inherent network structure of\ntypical document network, i.e., two spatially closer documents tend to have\nmore similar topics. Furthermore, we require that the topics are shared by all\nthe documents. In order to resolve these challenges, we use a subsampling\nstrategy to assign each document a different gamma process from the global\ngamma process, and the subsampling probabilities of documents are assigned with\na Markov Random Field constraint that inherits the document network structure.\nThrough the designed posterior inference algorithm, we can discover the hidden\ntopics and its number simultaneously. Experimental results on both synthetic\nand real-world network datasets demonstrate the capabilities of learning the\nhidden topics and, more importantly, the number of topics. \n\n"}
{"id": "1504.00091", "contents": "Title: Learning in the Presence of Corruption Abstract: In supervised learning one wishes to identify a pattern present in a joint\ndistribution $P$, of instances, label pairs, by providing a function $f$ from\ninstances to labels that has low risk $\\mathbb{E}_{P}\\ell(y,f(x))$. To do so,\nthe learner is given access to $n$ iid samples drawn from $P$. In many real\nworld problems clean samples are not available. Rather, the learner is given\naccess to samples from a corrupted distribution $\\tilde{P}$ from which to\nlearn, while the goal of predicting the clean pattern remains. There are many\ndifferent types of corruption one can consider, and as of yet there is no\ngeneral means to compare the relative ease of learning under these different\ncorruption processes. In this paper we develop a general framework for tackling\nsuch problems as well as introducing upper and lower bounds on the risk for\nlearning in the presence of corruption. Our ultimate goal is to be able to make\ninformed economic decisions in regards to the acquisition of data sets. For a\ncertain subclass of corruption processes (those that are\n\\emph{reconstructible}) we achieve this goal in a particular sense. Our lower\nbounds are in terms of the coefficient of ergodicity, a simple to calculate\nproperty of stochastic matrices. Our upper bounds proceed via a generalization\nof the method of unbiased estimators appearing in recent work of Natarajan et\nal and implicit in the earlier work of Kearns. \n\n"}
{"id": "1504.01046", "contents": "Title: Graph Connectivity in Noisy Sparse Subspace Clustering Abstract: Subspace clustering is the problem of clustering data points into a union of\nlow-dimensional linear/affine subspaces. It is the mathematical abstraction of\nmany important problems in computer vision, image processing and machine\nlearning. A line of recent work (4, 19, 24, 20) provided strong theoretical\nguarantee for sparse subspace clustering (4), the state-of-the-art algorithm\nfor subspace clustering, on both noiseless and noisy data sets. It was shown\nthat under mild conditions, with high probability no two points from different\nsubspaces are clustered together. Such guarantee, however, is not sufficient\nfor the clustering to be correct, due to the notorious \"graph connectivity\nproblem\" (15). In this paper, we investigate the graph connectivity problem for\nnoisy sparse subspace clustering and show that a simple post-processing\nprocedure is capable of delivering consistent clustering under certain \"general\nposition\" or \"restricted eigenvalue\" assumptions. We also show that our\ncondition is almost tight with adversarial noise perturbation by constructing a\ncounter-example. These results provide the first exact clustering guarantee of\nnoisy SSC for subspaces of dimension greater then 3. \n\n"}
{"id": "1504.01760", "contents": "Title: User Effort and Network Structure Mediate Access to Information in\n  Networks Abstract: Individuals' access to information in a social network depends on its\ndistributed and where in the network individuals position themselves. However,\nindividuals have limited capacity to manage their social connections and\nprocess information. In this work, we study how this limited capacity and\nnetwork structure interact to affect the diversity of information social media\nusers receive. Previous studies of the role of networks in information access\nwere limited in their ability to measure the diversity of information. We\naddress this problem by learning the topics of interest to social media users\nby observing messages they share online with their followers. We present a\nprobabilistic model that incorporates human cognitive constraints in a\ngenerative model of information sharing. We then use the topics learned by the\nmodel to measure the diversity of information users receive from their social\nmedia contacts. We confirm that users in structurally diverse network\npositions, which bridge otherwise disconnected regions of the follower graph,\nare exposed to more diverse information. In addition, we identify user effort\nas an important variable that mediates access to diverse information in social\nmedia. Users who invest more effort into their activity on the site not only\nplace themselves in more structurally diverse positions within the network than\nthe less engaged users, but they also receive more diverse information when\nlocated in similar network positions. These findings indicate that the\nrelationship between network structure and access to information in networks is\nmore nuanced than previously thought. \n\n"}
{"id": "1505.00553", "contents": "Title: On Regret-Optimal Learning in Decentralized Multi-player Multi-armed\n  Bandits Abstract: We consider the problem of learning in single-player and multiplayer\nmultiarmed bandit models. Bandit problems are classes of online learning\nproblems that capture exploration versus exploitation tradeoffs. In a\nmultiarmed bandit model, players can pick among many arms, and each play of an\narm generates an i.i.d. reward from an unknown distribution. The objective is\nto design a policy that maximizes the expected reward over a time horizon for a\nsingle player setting and the sum of expected rewards for the multiplayer\nsetting. In the multiplayer setting, arms may give different rewards to\ndifferent players. There is no separate channel for coordination among the\nplayers. Any attempt at communication is costly and adds to regret. We propose\ntwo decentralizable policies, $\\tt E^3$ ($\\tt E$-$\\tt cubed$) and $\\tt\nE^3$-$\\tt TS$, that can be used in both single player and multiplayer settings.\nThese policies are shown to yield expected regret that grows at most as\nO($\\log^{1+\\epsilon} T$). It is well known that $\\log T$ is the lower bound on\nthe rate of growth of regret even in a centralized case. The proposed\nalgorithms improve on prior work where regret grew at O($\\log^2 T$). More\nfundamentally, these policies address the question of additional cost incurred\nin decentralized online learning, suggesting that there is at most an\n$\\epsilon$-factor cost in terms of order of regret. This solves a problem of\nrelevance in many domains and had been open for a while. \n\n"}
{"id": "1505.02294", "contents": "Title: Estimation with Norm Regularization Abstract: Analysis of non-asymptotic estimation error and structured statistical\nrecovery based on norm regularized regression, such as Lasso, needs to consider\nfour aspects: the norm, the loss function, the design matrix, and the noise\nmodel. This paper presents generalizations of such estimation error analysis on\nall four aspects compared to the existing literature. We characterize the\nrestricted error set where the estimation error vector lies, establish\nrelations between error sets for the constrained and regularized problems, and\npresent an estimation error bound applicable to any norm. Precise\ncharacterizations of the bound is presented for isotropic as well as\nanisotropic subGaussian design matrices, subGaussian noise models, and convex\nloss functions, including least squares and generalized linear models. Generic\nchaining and associated results play an important role in the analysis. A key\nresult from the analysis is that the sample complexity of all such estimators\ndepends on the Gaussian width of a spherical cap corresponding to the\nrestricted error set. Further, once the number of samples $n$ crosses the\nrequired sample complexity, the estimation error decreases as\n$\\frac{c}{\\sqrt{n}}$, where $c$ depends on the Gaussian width of the unit norm\nball. \n\n"}
{"id": "1505.02445", "contents": "Title: Network Filtering for Big Data: Triangulated Maximally Filtered Graph Abstract: We propose a network-filtering method, the Triangulated Maximally Filtered\nGraph (TMFG), that provides an approximate solution to the Weighted Maximal\nPlanar Graph problem. The underlying idea of TMFG consists in building a\ntriangulation that maximizes a score function associated with the amount of\ninformation retained by the network. TMFG uses as weights any arbitrary\nsimilarity measure to arrange data into a meaningful network structure that can\nbe used for clustering, community detection and modeling. The method is fast,\nadaptable and scalable to very large datasets, it allows online updating and\nlearning as new data can be inserted and deleted with combinations of local and\nnon-local moves. TMFG permits readjustments of the network in consequence of\nchanges in the strength of the similarity measure. The method is based on local\ntopological moves and can therefore take advantage of parallel and GPUs\ncomputing. We discuss how this network-filtering method can be used intuitively\nand efficiently for big data studies and its significance from an\ninformation-theoretic perspective. \n\n"}
{"id": "1505.02865", "contents": "Title: Asymptotic Behavior of Minimal-Exploration Allocation Policies: Almost\n  Sure, Arbitrarily Slow Growing Regret Abstract: The purpose of this paper is to provide further understanding into the\nstructure of the sequential allocation (\"stochastic multi-armed bandit\", or\nMAB) problem by establishing probability one finite horizon bounds and\nconvergence rates for the sample (or \"pseudo\") regret associated with two\nsimple classes of allocation policies $\\pi$.\n  For any slowly increasing function $g$, subject to mild regularity\nconstraints, we construct two policies (the $g$-Forcing, and the $g$-Inflated\nSample Mean) that achieve a measure of regret of order $ O(g(n))$ almost surely\nas $n \\to \\infty$, bound from above and below. Additionally, almost sure upper\nand lower bounds on the remainder term are established. In the constructions\nherein, the function $g$ effectively controls the \"exploration\" of the\nclassical \"exploration/exploitation\" tradeoff. \n\n"}
{"id": "1505.03410", "contents": "Title: Mind the duality gap: safer rules for the Lasso Abstract: Screening rules allow to early discard irrelevant variables from the\noptimization in Lasso problems, or its derivatives, making solvers faster. In\nthis paper, we propose new versions of the so-called $\\textit{safe rules}$ for\nthe Lasso. Based on duality gap considerations, our new rules create safe test\nregions whose diameters converge to zero, provided that one relies on a\nconverging solver. This property helps screening out more variables, for a\nwider range of regularization parameter values. In addition to faster\nconvergence, we prove that we correctly identify the active sets (supports) of\nthe solutions in finite time. While our proposed strategy can cope with any\nsolver, its performance is demonstrated using a coordinate descent algorithm\nparticularly adapted to machine learning use cases. Significant computing time\nreductions are obtained with respect to previous safe rules. \n\n"}
{"id": "1505.03906", "contents": "Title: Training generative neural networks via Maximum Mean Discrepancy\n  optimization Abstract: We consider training a deep neural network to generate samples from an\nunknown distribution given i.i.d. data. We frame learning as an optimization\nminimizing a two-sample test statistic---informally speaking, a good generator\nnetwork produces samples that cause a two-sample test to fail to reject the\nnull hypothesis. As our two-sample test statistic, we use an unbiased estimate\nof the maximum mean discrepancy, which is the centerpiece of the nonparametric\nkernel two-sample test proposed by Gretton et al. (2012). We compare to the\nadversarial nets framework introduced by Goodfellow et al. (2014), in which\nlearning is a two-player game between a generator network and an adversarial\ndiscriminator network, both trained to outwit the other. From this perspective,\nthe MMD statistic plays the role of the discriminator. In addition to empirical\ncomparisons, we prove bounds on the generalization error incurred by optimizing\nthe empirical MMD. \n\n"}
{"id": "1505.04657", "contents": "Title: Mining User Opinions in Mobile App Reviews: A Keyword-based Approach Abstract: User reviews of mobile apps often contain complaints or suggestions which are\nvaluable for app developers to improve user experience and satisfaction.\nHowever, due to the large volume and noisy-nature of those reviews, manually\nanalyzing them for useful opinions is inherently challenging. To address this\nproblem, we propose MARK, a keyword-based framework for semi-automated review\nanalysis. MARK allows an analyst describing his interests in one or some mobile\napps by a set of keywords. It then finds and lists the reviews most relevant to\nthose keywords for further analysis. It can also draw the trends over time of\nthose keywords and detect their sudden changes, which might indicate the\noccurrences of serious issues. To help analysts describe their interests more\neffectively, MARK can automatically extract keywords from raw reviews and rank\nthem by their associations with negative reviews. In addition, based on a\nvector-based semantic representation of keywords, MARK can divide a large set\nof keywords into more cohesive subsets, or suggest keywords similar to the\nselected ones. \n\n"}
{"id": "1505.05629", "contents": "Title: Regulating Greed Over Time in Multi-Armed Bandits Abstract: In retail, there are predictable yet dramatic time-dependent patterns in\ncustomer behavior, such as periodic changes in the number of visitors, or\nincreases in customers just before major holidays. The current paradigm of\nmulti-armed bandit analysis does not take these known patterns into account.\nThis means that for applications in retail, where prices are fixed for periods\nof time, current bandit algorithms will not suffice. This work provides a\nremedy that takes the time-dependent patterns into account, and we show how\nthis remedy is implemented for the UCB, $\\varepsilon$-greedy, and UCB-L\nalgorithms, and also through a new policy called the variable arm pool\nalgorithm. In the corrected methods, exploitation (greed) is regulated over\ntime, so that more exploitation occurs during higher reward periods, and more\nexploration occurs in periods of low reward. In order to understand why regret\nis reduced with the corrected methods, we present a set of bounds that provide\ninsight into why we would want to exploit during periods of high reward, and\ndiscuss the impact on regret. Our proposed methods perform well in experiments,\nand were inspired by a high-scoring entry in the Exploration and Exploitation 3\ncontest using data from Yahoo$!$ Front Page. That entry heavily used\ntime-series methods to regulate greed over time, which was substantially more\neffective than other contextual bandit methods. \n\n"}
{"id": "1505.06538", "contents": "Title: Clustering via Content-Augmented Stochastic Blockmodels Abstract: Much of the data being created on the web contains interactions between users\nand items. Stochastic blockmodels, and other methods for community detection\nand clustering of bipartite graphs, can infer latent user communities and\nlatent item clusters from this interaction data. These methods, however,\ntypically ignore the items' contents and the information they provide about\nitem clusters, despite the tendency of items in the same latent cluster to\nshare commonalities in content. We introduce content-augmented stochastic\nblockmodels (CASB), which use item content together with user-item interaction\ndata to enhance the user communities and item clusters learned. Comparisons to\nseveral state-of-the-art benchmark methods, on datasets arising from scientists\ninteracting with scientific articles, show that content-augmented stochastic\nblockmodels provide highly accurate clusters with respect to metrics\nrepresentative of the underlying community structure. \n\n"}
{"id": "1505.07818", "contents": "Title: Domain-Adversarial Training of Neural Networks Abstract: We introduce a new representation learning approach for domain adaptation, in\nwhich data at training and test time come from similar but different\ndistributions. Our approach is directly inspired by the theory on domain\nadaptation suggesting that, for effective domain transfer to be achieved,\npredictions must be made based on features that cannot discriminate between the\ntraining (source) and test (target) domains. The approach implements this idea\nin the context of neural network architectures that are trained on labeled data\nfrom the source domain and unlabeled data from the target domain (no labeled\ntarget-domain data is necessary). As the training progresses, the approach\npromotes the emergence of features that are (i) discriminative for the main\nlearning task on the source domain and (ii) indiscriminate with respect to the\nshift between the domains. We show that this adaptation behaviour can be\nachieved in almost any feed-forward model by augmenting it with few standard\nlayers and a new gradient reversal layer. The resulting augmented architecture\ncan be trained using standard backpropagation and stochastic gradient descent,\nand can thus be implemented with little effort using any of the deep learning\npackages. We demonstrate the success of our approach for two distinct\nclassification problems (document sentiment analysis and image classification),\nwhere state-of-the-art domain adaptation performance on standard benchmarks is\nachieved. We also validate the approach for descriptor learning task in the\ncontext of person re-identification application. \n\n"}
{"id": "1506.01743", "contents": "Title: Socially Driven News Recommendation Abstract: The participatory Web has enabled the ubiquitous and pervasive access of\ninformation, accompanied by an increase of speed and reach in information\nsharing. Data dissemination services such as news aggregators are expected to\nprovide up-to-date, real-time information to the end users. News aggregators\nare in essence recommendation systems that filter and rank news stories in\norder to select the few that will appear on the users front screen at any time.\nOne of the main challenges in such systems is to address the recency and\nlatency problems, that is, to identify as soon as possible how important a news\nstory is. In this work we propose an integrated framework that aims at\npredicting the importance of news items upon their publication with a focus on\nrecent and highly popular news, employing resampling strategies, and at\ntranslating the result into concrete news rankings. We perform an extensive\nexperimental evaluation using real-life datasets of the proposed framework as\nboth a stand-alone system and when applied to news recommendations from Google\nNews. Additionally, we propose and evaluate a combinatorial solution to the\naugmentation of official media recommendations with social information. Results\nshow that the proposed approach complements and enhances the news rankings\ngenerated by state-of-the-art systems. \n\n"}
{"id": "1506.01744", "contents": "Title: Spectral Learning of Large Structured HMMs for Comparative Epigenomics Abstract: We develop a latent variable model and an efficient spectral algorithm\nmotivated by the recent emergence of very large data sets of chromatin marks\nfrom multiple human cell types. A natural model for chromatin data in one cell\ntype is a Hidden Markov Model (HMM); we model the relationship between multiple\ncell types by connecting their hidden states by a fixed tree of known\nstructure. The main challenge with learning parameters of such models is that\niterative methods such as EM are very slow, while naive spectral methods result\nin time and space complexity exponential in the number of cell types. We\nexploit properties of the tree structure of the hidden states to provide\nspectral algorithms that are more computationally efficient for current\nbiological datasets. We provide sample complexity bounds for our algorithm and\nevaluate it experimentally on biological data from nine human cell types.\nFinally, we show that beyond our specific model, some of our algorithmic ideas\ncan be applied to other graphical models. \n\n"}
{"id": "1506.02142", "contents": "Title: Dropout as a Bayesian Approximation: Representing Model Uncertainty in\n  Deep Learning Abstract: Deep learning tools have gained tremendous attention in applied machine\nlearning. However such tools for regression and classification do not capture\nmodel uncertainty. In comparison, Bayesian models offer a mathematically\ngrounded framework to reason about model uncertainty, but usually come with a\nprohibitive computational cost. In this paper we develop a new theoretical\nframework casting dropout training in deep neural networks (NNs) as approximate\nBayesian inference in deep Gaussian processes. A direct result of this theory\ngives us tools to model uncertainty with dropout NNs -- extracting information\nfrom existing models that has been thrown away so far. This mitigates the\nproblem of representing uncertainty in deep learning without sacrificing either\ncomputational complexity or test accuracy. We perform an extensive study of the\nproperties of dropout's uncertainty. Various network architectures and\nnon-linearities are assessed on tasks of regression and classification, using\nMNIST as an example. We show a considerable improvement in predictive\nlog-likelihood and RMSE compared to existing state-of-the-art methods, and\nfinish by using dropout's uncertainty in deep reinforcement learning. \n\n"}
{"id": "1506.02550", "contents": "Title: Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem Abstract: We study the $K$-armed dueling bandit problem, a variation of the standard\nstochastic bandit problem where the feedback is limited to relative comparisons\nof a pair of arms. We introduce a tight asymptotic regret lower bound that is\nbased on the information divergence. An algorithm that is inspired by the\nDeterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010)\nis proposed, and its regret is analyzed. The proposed algorithm is found to be\nthe first one with a regret upper bound that matches the lower bound.\nExperimental comparisons of dueling bandit algorithms show that the proposed\nalgorithm significantly outperforms existing ones. \n\n"}
{"id": "1506.02557", "contents": "Title: Variational Dropout and the Local Reparameterization Trick Abstract: We investigate a local reparameterizaton technique for greatly reducing the\nvariance of stochastic gradients for variational Bayesian inference (SGVB) of a\nposterior over model parameters, while retaining parallelizability. This local\nreparameterization translates uncertainty about global parameters into local\nnoise that is independent across datapoints in the minibatch. Such\nparameterizations can be trivially parallelized and have variance that is\ninversely proportional to the minibatch size, generally leading to much faster\nconvergence. Additionally, we explore a connection with dropout: Gaussian\ndropout objectives correspond to SGVB with local reparameterization, a\nscale-invariant prior and proportionally fixed posterior variance. Our method\nallows inference of more flexibly parameterized posteriors; specifically, we\npropose variational dropout, a generalization of Gaussian dropout where the\ndropout rates are learned, often leading to better models. The method is\ndemonstrated through several experiments. \n\n"}
{"id": "1506.03159", "contents": "Title: Copula variational inference Abstract: We develop a general variational inference method that preserves dependency\namong the latent variables. Our method uses copulas to augment the families of\ndistributions used in mean-field and structured approximations. Copulas model\nthe dependency that is not captured by the original variational distribution,\nand thus the augmented variational family guarantees better approximations to\nthe posterior. With stochastic optimization, inference on the augmented\ndistribution is scalable. Furthermore, our strategy is generic: it can be\napplied to any inference procedure that currently uses the mean-field or\nstructured approach. Copula variational inference has many advantages: it\nreduces bias; it is less sensitive to local optima; it is less sensitive to\nhyperparameters; and it helps characterize and interpret the dependency among\nthe latent variables. \n\n"}
{"id": "1506.03493", "contents": "Title: Bayesian Poisson Tensor Factorization for Inferring Multilateral\n  Relations from Sparse Dyadic Event Counts Abstract: We present a Bayesian tensor factorization model for inferring latent group\nstructures from dynamic pairwise interaction patterns. For decades, political\nscientists have collected and analyzed records of the form \"country $i$ took\naction $a$ toward country $j$ at time $t$\"---known as dyadic events---in order\nto form and test theories of international relations. We represent these event\ndata as a tensor of counts and develop Bayesian Poisson tensor factorization to\ninfer a low-dimensional, interpretable representation of their salient\npatterns. We demonstrate that our model's predictive performance is better than\nthat of standard non-negative tensor factorization methods. We also provide a\ncomparison of our variational updates to their maximum likelihood counterparts.\nIn doing so, we identify a better way to form point estimates of the latent\nfactors than that typically used in Bayesian Poisson matrix factorization.\nFinally, we showcase our model as an exploratory analysis tool for political\nscientists. We show that the inferred latent factor matrices capture\ninterpretable multilateral relations that both conform to and inform our\nknowledge of international affairs. \n\n"}
{"id": "1506.05600", "contents": "Title: Causality on Cross-Sectional Data: Stable Specification Search in\n  Constrained Structural Equation Modeling Abstract: Causal modeling has long been an attractive topic for many researchers and in\nrecent decades there has seen a surge in theoretical development and discovery\nalgorithms. Generally discovery algorithms can be divided into two approaches:\nconstraint-based and score-based. The constraint-based approach is able to\ndetect common causes of the observed variables but the use of independence\ntests makes it less reliable. The score-based approach produces a result that\nis easier to interpret as it also measures the reliability of the inferred\ncausal relationships, but it is unable to detect common confounders of the\nobserved variables. A drawback of both score-based and constrained-based\napproaches is the inherent instability in structure estimation. With finite\nsamples small changes in the data can lead to completely different optimal\nstructures. The present work introduces a new hypothesis-free score-based\ncausal discovery algorithm, called stable specification search, that is robust\nfor finite samples based on recent advances in stability selection using\nsubsampling and selection algorithms. Structure search is performed over\nStructural Equation Models. Our approach uses exploratory search but allows\nincorporation of prior background knowledge. We validated our approach on one\nsimulated data set, which we compare to the known ground truth, and two\nreal-world data sets for Chronic Fatigue Syndrome and Attention Deficit\nHyperactivity Disorder, which we compare to earlier medical studies. The\nresults on the simulated data set show significant improvement over alternative\napproaches and the results on the real-word data sets show consistency with the\nhypothesis driven models constructed by medical experts. \n\n"}
{"id": "1506.05860", "contents": "Title: Variational Gaussian Copula Inference Abstract: We utilize copulas to constitute a unified framework for constructing and\noptimizing variational proposals in hierarchical Bayesian models. For models\nwith continuous and non-Gaussian hidden variables, we propose a semiparametric\nand automated variational Gaussian copula approach, in which the parametric\nGaussian copula family is able to preserve multivariate posterior dependence,\nand the nonparametric transformations based on Bernstein polynomials provide\nample flexibility in characterizing the univariate marginal posteriors. \n\n"}
{"id": "1506.06490", "contents": "Title: Answer Sequence Learning with Neural Networks for Answer Selection in\n  Community Question Answering Abstract: In this paper, the answer selection problem in community question answering\n(CQA) is regarded as an answer sequence labeling task, and a novel approach is\nproposed based on the recurrent architecture for this problem. Our approach\napplies convolution neural networks (CNNs) to learning the joint representation\nof question-answer pair firstly, and then uses the joint representation as\ninput of the long short-term memory (LSTM) to learn the answer sequence of a\nquestion for labeling the matching quality of each answer. Experiments\nconducted on the SemEval 2015 CQA dataset shows the effectiveness of our\napproach. \n\n"}
{"id": "1506.06628", "contents": "Title: Modality-dependent Cross-media Retrieval Abstract: In this paper, we investigate the cross-media retrieval between images and\ntext, i.e., using image to search text (I2T) and using text to search images\n(T2I). Existing cross-media retrieval methods usually learn one couple of\nprojections, by which the original features of images and text can be projected\ninto a common latent space to measure the content similarity. However, using\nthe same projections for the two different retrieval tasks (I2T and T2I) may\nlead to a tradeoff between their respective performances, rather than their\nbest performances. Different from previous works, we propose a\nmodality-dependent cross-media retrieval (MDCR) model, where two couples of\nprojections are learned for different cross-media retrieval tasks instead of\none couple of projections. Specifically, by jointly optimizing the correlation\nbetween images and text and the linear regression from one modal space (image\nor text) to the semantic space, two couples of mappings are learned to project\nimages and text from their original feature spaces into two common latent\nsubspaces (one for I2T and the other for T2I). Extensive experiments show the\nsuperiority of the proposed MDCR compared with other methods. In particular,\nbased the 4,096 dimensional convolutional neural network (CNN) visual feature\nand 100 dimensional LDA textual feature, the mAP of the proposed method\nachieves 41.5\\%, which is a new state-of-the-art performance on the Wikipedia\ndataset. \n\n"}
{"id": "1506.07840", "contents": "Title: Diffusion Nets Abstract: Non-linear manifold learning enables high-dimensional data analysis, but\nrequires out-of-sample-extension methods to process new data points. In this\npaper, we propose a manifold learning algorithm based on deep learning to\ncreate an encoder, which maps a high-dimensional dataset and its\nlow-dimensional embedding, and a decoder, which takes the embedded data back to\nthe high-dimensional space. Stacking the encoder and decoder together\nconstructs an autoencoder, which we term a diffusion net, that performs\nout-of-sample-extension as well as outlier detection. We introduce new neural\nnet constraints for the encoder, which preserves the local geometry of the\npoints, and we prove rates of convergence for the encoder. Also, our approach\nis efficient in both computational complexity and memory requirements, as\nopposed to previous methods that require storage of all training points in both\nthe high-dimensional and the low-dimensional spaces to calculate the\nout-of-sample-extension and the pre-image. \n\n"}
{"id": "1506.08448", "contents": "Title: Neural Simpletrons - Minimalistic Directed Generative Networks for\n  Learning with Few Labels Abstract: Classifiers for the semi-supervised setting often combine strong supervised\nmodels with additional learning objectives to make use of unlabeled data. This\nresults in powerful though very complex models that are hard to train and that\ndemand additional labels for optimal parameter tuning, which are often not\ngiven when labeled data is very sparse. We here study a minimalistic\nmulti-layer generative neural network for semi-supervised learning in a form\nand setting as similar to standard discriminative networks as possible. Based\non normalized Poisson mixtures, we derive compact and local learning and neural\nactivation rules. Learning and inference in the network can be scaled using\nstandard deep learning tools for parallelized GPU implementation. With the\nsingle objective of likelihood optimization, both labeled and unlabeled data\nare naturally incorporated into learning. Empirical evaluations on standard\nbenchmarks show, that for datasets with few labels the derived minimalistic\nnetwork improves on all classical deep learning approaches and is competitive\nwith their recent variants without the need of additional labels for parameter\ntuning. Furthermore, we find that the studied network is the best performing\nmonolithic (`non-hybrid') system for few labels, and that it can be applied in\nthe limit of very few labels, where no other system has been reported to\noperate so far. \n\n"}
{"id": "1506.09039", "contents": "Title: Scalable Discrete Sampling as a Multi-Armed Bandit Problem Abstract: Drawing a sample from a discrete distribution is one of the building\ncomponents for Monte Carlo methods. Like other sampling algorithms, discrete\nsampling suffers from the high computational burden in large-scale inference\nproblems. We study the problem of sampling a discrete random variable with a\nhigh degree of dependency that is typical in large-scale Bayesian inference and\ngraphical models, and propose an efficient approximate solution with a\nsubsampling approach. We make a novel connection between the discrete sampling\nand Multi-Armed Bandits problems with a finite reward population and provide\nthree algorithms with theoretical guarantees. Empirical evaluations show the\nrobustness and efficiency of the approximate algorithms in both synthetic and\nreal-world large-scale problems. \n\n"}
{"id": "1507.00955", "contents": "Title: Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and\n  Their Combination Abstract: This paper covers the two approaches for sentiment analysis: i) lexicon based\nmethod; ii) machine learning method. We describe several techniques to\nimplement these approaches and discuss how they can be adopted for sentiment\nclassification of Twitter messages. We present a comparative study of different\nlexicon combinations and show that enhancing sentiment lexicons with emoticons,\nabbreviations and social-media slang expressions increases the accuracy of\nlexicon-based classification for Twitter. We discuss the importance of feature\ngeneration and feature selection processes for machine learning sentiment\nclassification. To quantify the performance of the main sentiment analysis\nmethods over Twitter we run these algorithms on a benchmark Twitter dataset\nfrom the SemEval-2013 competition, task 2-B. The results show that machine\nlearning method based on SVM and Naive Bayes classifiers outperforms the\nlexicon method. We present a new ensemble method that uses a lexicon based\nsentiment score as input feature for the machine learning approach. The\ncombined method proved to produce more precise classifications. We also show\nthat employing a cost-sensitive classifier for highly unbalanced datasets\nyields an improvement of sentiment classification performance up to 7%. \n\n"}
{"id": "1507.04457", "contents": "Title: Preference Completion: Large-scale Collaborative Ranking from Pairwise\n  Comparisons Abstract: In this paper we consider the collaborative ranking setting: a pool of users\neach provides a small number of pairwise preferences between $d$ possible\nitems; from these we need to predict preferences of the users for items they\nhave not yet seen. We do so by fitting a rank $r$ score matrix to the pairwise\ndata, and provide two main contributions: (a) we show that an algorithm based\non convex optimization provides good generalization guarantees once each user\nprovides as few as $O(r\\log^2 d)$ pairwise comparisons -- essentially matching\nthe sample complexity required in the related matrix completion setting (which\nuses actual numerical as opposed to pairwise information), and (b) we develop a\nlarge-scale non-convex implementation, which we call AltSVM, that trains a\nfactored form of the matrix via alternating minimization (which we show reduces\nto alternating SVM problems), and scales and parallelizes very well to large\nproblem settings. It also outperforms common baselines on many moderately large\npopular collaborative filtering datasets in both NDCG and in other measures of\nranking performance. \n\n"}
{"id": "1507.04798", "contents": "Title: Exploratory topic modeling with distributional semantics Abstract: As we continue to collect and store textual data in a multitude of domains,\nwe are regularly confronted with material whose largely unknown thematic\nstructure we want to uncover. With unsupervised, exploratory analysis, no prior\nknowledge about the content is required and highly open-ended tasks can be\nsupported. In the past few years, probabilistic topic modeling has emerged as a\npopular approach to this problem. Nevertheless, the representation of the\nlatent topics as aggregations of semi-coherent terms limits their\ninterpretability and level of detail.\n  This paper presents an alternative approach to topic modeling that maps\ntopics as a network for exploration, based on distributional semantics using\nlearned word vectors. From the granular level of terms and their semantic\nsimilarity relations global topic structures emerge as clustered regions and\ngradients of concepts. Moreover, the paper discusses the visual interactive\nrepresentation of the topic map, which plays an important role in supporting\nits exploration. \n\n"}
{"id": "1507.05150", "contents": "Title: Towards Understanding User Preferences from User Tagging Behavior for\n  Personalization Abstract: Personalizing image tags is a relatively new and growing area of research,\nand in order to advance this research community, we must review and challenge\nthe de-facto standard of defining tag importance. We believe that for greater\nprogress to be made, we must go beyond tags that merely describe objects that\nare visually represented in the image, towards more user-centric and subjective\nnotions such as emotion, sentiment, and preferences.\n  We focus on the notion of user preferences and show that the order that users\nlist tags on images is correlated to the order of preference over the tags that\nthey provided for the image. While this observation is not completely\nsurprising, to our knowledge, we are the first to explore this aspect of user\ntagging behavior systematically and report empirical results to support this\nobservation. We argue that this observation can be exploited to help advance\nthe image tagging (and related) communities.\n  Our contributions include: 1.) conducting a user study demonstrating this\nobservation, 2.) collecting a dataset with user tag preferences explicitly\ncollected. \n\n"}
{"id": "1507.05214", "contents": "Title: On the Application of Link Analysis Algorithms for Ranking Bipartite\n  Graphs Abstract: Recently bipartite graphs have been widely used to represent the relationship\ntwo sets of items for information retrieval applications. The Web offers a wide\nrange of data which can be represented by bipartite graphs, such us movies and\nreviewers in recomender systems, queries and URLs in search engines, users and\nposts in social networks. The size and the dynamic nature of such graphs\ngenerate the need for more efficient ranking methods.\n  In this thesis, at first we present the fundamental mathematical backround\nthat we use subsequently and we describe the basic principles of the\nPerron-Frobebius theory for non negative matrices as well as the the basic\nprinciples of the Markov chain theory. Then, we propose a novel algorithm named\nBipartiteRank, which is suitable to rank scenarios, that can be represented as\na bipartite graph. This algorithm is based on the random surfer model and\ninherits the basic mathematical characteristics of PageRank. What makes it\ndifferent, is the fact that it introduces an alternative type of teleportation,\nbased on the block structure of the bipartite graph in order to achieve more\nefficient ranking. Finally, we support this opinion with mathematical arguments\nand then we confirm it experimentally through a series of tests on real data. \n\n"}
{"id": "1507.05444", "contents": "Title: Canonical Correlation Forests Abstract: We introduce canonical correlation forests (CCFs), a new decision tree\nensemble method for classification and regression. Individual canonical\ncorrelation trees are binary decision trees with hyperplane splits based on\nlocal canonical correlation coefficients calculated during training. Unlike\naxis-aligned alternatives, the decision surfaces of CCFs are not restricted to\nthe coordinate system of the inputs features and therefore more naturally\nrepresent data with correlated inputs. CCFs naturally accommodate multiple\noutputs, provide a similar computational complexity to random forests, and\ninherit their impressive robustness to the choice of input parameters. As part\nof the CCF training algorithm, we also introduce projection bootstrapping, a\nnovel alternative to bagging for oblique decision tree ensembles which\nmaintains use of the full dataset in selecting split points, often leading to\nimprovements in predictive accuracy. Our experiments show that, even without\nparameter tuning, CCFs out-perform axis-aligned random forests and other\nstate-of-the-art tree ensemble methods on both classification and regression\nproblems, delivering both improved predictive accuracy and faster training\ntimes. We further show that they outperform all of the 179 classifiers\nconsidered in a recent extensive survey. \n\n"}
{"id": "1507.06065", "contents": "Title: MixEst: An Estimation Toolbox for Mixture Models Abstract: Mixture models are powerful statistical models used in many applications\nranging from density estimation to clustering and classification. When dealing\nwith mixture models, there are many issues that the experimenter should be\naware of and needs to solve. The MixEst toolbox is a powerful and user-friendly\npackage for MATLAB that implements several state-of-the-art approaches to\naddress these problems. Additionally, MixEst gives the possibility of using\nmanifold optimization for fitting the density model, a feature specific to this\ntoolbox. MixEst simplifies using and integration of mixture models in\nstatistical models and applications. For developing mixture models of new\ndensities, the user just needs to provide a few functions for that statistical\ndistribution and the toolbox takes care of all the issues regarding mixture\nmodels. MixEst is available at visionlab.ut.ac.ir/mixest and is fully\ndocumented and is licensed under GPL. \n\n"}
{"id": "1508.01774", "contents": "Title: An End-to-End Neural Network for Polyphonic Piano Music Transcription Abstract: We present a supervised neural network model for polyphonic piano music\ntranscription. The architecture of the proposed model is analogous to speech\nrecognition systems and comprises an acoustic model and a music language model.\nThe acoustic model is a neural network used for estimating the probabilities of\npitches in a frame of audio. The language model is a recurrent neural network\nthat models the correlations between pitch combinations over time. The proposed\nmodel is general and can be used to transcribe polyphonic music without\nimposing any constraints on the polyphony. The acoustic and language model\npredictions are combined using a probabilistic graphical model. Inference over\nthe output variables is performed using the beam search algorithm. We perform\ntwo sets of experiments. We investigate various neural network architectures\nfor the acoustic models and also investigate the effect of combining acoustic\nand music language model predictions using the proposed architecture. We\ncompare performance of the neural network based acoustic models with two\npopular unsupervised acoustic models. Results show that convolutional neural\nnetwork acoustic models yields the best performance across all evaluation\nmetrics. We also observe improved performance with the application of the music\nlanguage models. Finally, we present an efficient variant of beam search that\nimproves performance and reduces run-times by an order of magnitude, making the\nmodel suitable for real-time applications. \n\n"}
{"id": "1508.02496", "contents": "Title: A Practical Guide to CNNs and Fisher Vectors for Image Instance\n  Retrieval Abstract: With deep learning becoming the dominant approach in computer vision, the use\nof representations extracted from Convolutional Neural Nets (CNNs) is quickly\ngaining ground on Fisher Vectors (FVs) as favoured state-of-the-art global\nimage descriptors for image instance retrieval. While the good performance of\nCNNs for image classification are unambiguously recognised, which of the two\nhas the upper hand in the image retrieval context is not entirely clear yet. In\nthis work, we propose a comprehensive study that systematically evaluates FVs\nand CNNs for image retrieval. The first part compares the performances of FVs\nand CNNs on multiple publicly available data sets. We investigate a number of\ndetails specific to each method. For FVs, we compare sparse descriptors based\non interest point detectors with dense single-scale and multi-scale variants.\nFor CNNs, we focus on understanding the impact of depth, architecture and\ntraining data on retrieval results. Our study shows that no descriptor is\nsystematically better than the other and that performance gains can usually be\nobtained by using both types together. The second part of the study focuses on\nthe impact of geometrical transformations such as rotations and scale changes.\nFVs based on interest point detectors are intrinsically resilient to such\ntransformations while CNNs do not have a built-in mechanism to ensure such\ninvariance. We show that performance of CNNs can quickly degrade in presence of\nrotations while they are far less affected by changes in scale. We then propose\na number of ways to incorporate the required invariances in the CNN pipeline.\nOverall, our work is intended as a reference guide offering practically useful\nand simply implementable guidelines to anyone looking for state-of-the-art\nglobal descriptors best suited to their specific image instance retrieval\nproblem. \n\n"}
{"id": "1508.02933", "contents": "Title: No Regret Bound for Extreme Bandits Abstract: Algorithms for hyperparameter optimization abound, all of which work well\nunder different and often unverifiable assumptions. Motivated by the general\nchallenge of sequentially choosing which algorithm to use, we study the more\nspecific task of choosing among distributions to use for random hyperparameter\noptimization. This work is naturally framed in the extreme bandit setting,\nwhich deals with sequentially choosing which distribution from a collection to\nsample in order to minimize (maximize) the single best cost (reward). Whereas\nthe distributions in the standard bandit setting are primarily characterized by\ntheir means, a number of subtleties arise when we care about the minimal cost\nas opposed to the average cost. For example, there may not be a well-defined\n\"best\" distribution as there is in the standard bandit setting. The best\ndistribution depends on the rewards that have been obtained and on the\nremaining time horizon. Whereas in the standard bandit setting, it is sensible\nto compare policies with an oracle which plays the single best arm, in the\nextreme bandit setting, there are multiple sensible oracle models. We define a\nsensible notion of \"extreme regret\" in the extreme bandit setting, which\nparallels the concept of regret in the standard bandit setting. We then prove\nthat no policy can asymptotically achieve no extreme regret. \n\n"}
{"id": "1508.04210", "contents": "Title: Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors Abstract: We present a scalable Bayesian model for low-rank factorization of massive\ntensors with binary observations. The proposed model has the following key\nproperties: (1) in contrast to the models based on the logistic or probit\nlikelihood, using a zero-truncated Poisson likelihood for binary data allows\nour model to scale up in the number of \\emph{ones} in the tensor, which is\nespecially appealing for massive but sparse binary tensors; (2)\nside-information in form of binary pairwise relationships (e.g., an adjacency\nnetwork) between objects in any tensor mode can also be leveraged, which can be\nespecially useful in \"cold-start\" settings; and (3) the model admits simple\nBayesian inference via batch, as well as \\emph{online} MCMC; the latter allows\nscaling up even for \\emph{dense} binary data (i.e., when the number of ones in\nthe tensor/network is also massive). In addition, non-negative factor matrices\nin our model provide easy interpretability, and the tensor rank can be inferred\nfrom the data. We evaluate our model on several large-scale real-world binary\ntensors, achieving excellent computational scalability, and also demonstrate\nits usefulness in leveraging side-information provided in form of\nmode-network(s). \n\n"}
{"id": "1508.04211", "contents": "Title: Scalable Bayesian Non-Negative Tensor Factorization for Massive Count\n  Data Abstract: We present a Bayesian non-negative tensor factorization model for\ncount-valued tensor data, and develop scalable inference algorithms (both batch\nand online) for dealing with massive tensors. Our generative model can handle\noverdispersed counts as well as infer the rank of the decomposition. Moreover,\nleveraging a reparameterization of the Poisson distribution as a multinomial\nfacilitates conjugacy in the model and enables simple and efficient Gibbs\nsampling and variational Bayes (VB) inference updates, with a computational\ncost that only depends on the number of nonzeros in the tensor. The model also\nprovides a nice interpretability for the factors; in our model, each factor\ncorresponds to a \"topic\". We develop a set of online inference algorithms that\nallow further scaling up the model to massive tensors, for which batch\ninference methods may be infeasible. We apply our framework on diverse\nreal-world applications, such as \\emph{multiway} topic modeling on a scientific\npublications database, analyzing a political science data set, and analyzing a\nmassive household transactions data set. \n\n"}
{"id": "1508.04819", "contents": "Title: Analyzing Organizational Routines in Online Knowledge Collaborations: A\n  Case for Sequence Analysis in CSCW Abstract: Research into socio-technical systems like Wikipedia has overlooked important\nstructural patterns in the coordination of distributed work. This paper argues\nfor a conceptual reorientation towards sequences as a fundamental unit of\nanalysis for understanding work routines in online knowledge collaboration. We\noutline a research agenda for researchers in computer-supported cooperative\nwork (CSCW) to understand the relationships, patterns, antecedents, and\nconsequences of sequential behavior using methods already developed in fields\nlike bio-informatics. Using a data set of 37,515 revisions from 16,616 unique\neditors to 96 Wikipedia articles as a case study, we analyze the prevalence and\nsignificance of different sequences of editing patterns. We illustrate the\nmixed method potential of sequence approaches by interpreting the frequent\npatterns as general classes of behavioral motifs. We conclude by discussing the\nmethodological opportunities for using sequence analysis for expanding existing\napproaches to analyzing and theorizing about co-production routines in online\nknowledge collaboration. \n\n"}
{"id": "1508.05003", "contents": "Title: AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization Abstract: We study distributed stochastic convex optimization under the delayed\ngradient model where the server nodes perform parameter updates, while the\nworker nodes compute stochastic gradients. We discuss, analyze, and experiment\nwith a setup motivated by the behavior of real-world distributed computation\nnetworks, where the machines are differently slow at different time. Therefore,\nwe allow the parameter updates to be sensitive to the actual delays\nexperienced, rather than to worst-case bounds on the maximum delay. This\nsensitivity leads to larger stepsizes, that can help gain rapid initial\nconvergence without having to wait too long for slower machines, while\nmaintaining the same asymptotic complexity. We obtain encouraging improvements\nto overall convergence for distributed experiments on real datasets with up to\nbillions of examples and features. \n\n"}
{"id": "1508.06091", "contents": "Title: AUC Optimisation and Collaborative Filtering Abstract: In recommendation systems, one is interested in the ranking of the predicted\nitems as opposed to other losses such as the mean squared error. Although a\nvariety of ways to evaluate rankings exist in the literature, here we focus on\nthe Area Under the ROC Curve (AUC) as it widely used and has a strong\ntheoretical underpinning. In practical recommendation, only items at the top of\nthe ranked list are presented to the users. With this in mind, we propose a\nclass of objective functions over matrix factorisations which primarily\nrepresent a smooth surrogate for the real AUC, and in a special case we show\nhow to prioritise the top of the list. The objectives are differentiable and\noptimised through a carefully designed stochastic gradient-descent-based\nalgorithm which scales linearly with the size of the data. In the special case\nof square loss we show how to improve computational complexity by leveraging\npreviously computed measures. To understand theoretically the underlying matrix\nfactorisation approaches we study both the consistency of the loss functions\nwith respect to AUC, and generalisation using Rademacher theory. The resulting\ngeneralisation analysis gives strong motivation for the optimisation under\nstudy. Finally, we provide computation results as to the efficacy of the\nproposed method using synthetic and real data. \n\n"}
{"id": "1508.06901", "contents": "Title: Compressive Sensing via Low-Rank Gaussian Mixture Models Abstract: We develop a new compressive sensing (CS) inversion algorithm by utilizing\nthe Gaussian mixture model (GMM). While the compressive sensing is performed\nglobally on the entire image as implemented in our lensless camera, a low-rank\nGMM is imposed on the local image patches. This low-rank GMM is derived via\neigenvalue thresholding of the GMM trained on the projection of the measurement\ndata, thus learned {\\em in situ}. The GMM and the projection of the measurement\ndata are updated iteratively during the reconstruction. Our GMM algorithm\ndegrades to the piecewise linear estimator (PLE) if each patch is represented\nby a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also\ndeveloped for CS inversion, constituting an additional contribution of this\npaper. Extensive results on both simulation data and real data captured by the\nlensless camera demonstrate the efficacy of the proposed algorithm.\nFurthermore, we compare the CS reconstruction results using our algorithm with\nthe JPEG compression. Simulation results demonstrate that when limited\nbandwidth is available (a small number of measurements), our algorithm can\nachieve comparable results as JPEG. \n\n"}
{"id": "1508.07096", "contents": "Title: Partitioning Large Scale Deep Belief Networks Using Dropout Abstract: Deep learning methods have shown great promise in many practical\napplications, ranging from speech recognition, visual object recognition, to\ntext processing. However, most of the current deep learning methods suffer from\nscalability problems for large-scale applications, forcing researchers or users\nto focus on small-scale problems with fewer parameters.\n  In this paper, we consider a well-known machine learning model, deep belief\nnetworks (DBNs) that have yielded impressive classification performance on a\nlarge number of benchmark machine learning tasks. To scale up DBN, we propose\nan approach that can use the computing clusters in a distributed environment to\ntrain large models, while the dense matrix computations within a single machine\nare sped up using graphics processors (GPU). When training a DBN, each machine\nrandomly drops out a portion of neurons in each hidden layer, for each training\ncase, making the remaining neurons only learn to detect features that are\ngenerally helpful for producing the correct answer. Within our approach, we\nhave developed four methods to combine outcomes from each machine to form a\nunified model. Our preliminary experiment on the mnst handwritten digit\ndatabase demonstrates that our approach outperforms the state of the art test\nerror rate. \n\n"}
{"id": "1509.02897", "contents": "Title: Practical and Optimal LSH for Angular Distance Abstract: We show the existence of a Locality-Sensitive Hashing (LSH) family for the\nangular distance that yields an approximate Near Neighbor Search algorithm with\nthe asymptotically optimal running time exponent. Unlike earlier algorithms\nwith this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn\n2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also\nintroduce a multiprobe version of this algorithm, and conduct experimental\nevaluation on real and synthetic data sets.\n  We complement the above positive results with a fine-grained lower bound for\nthe quality of any LSH family for angular distance. Our lower bound implies\nthat the above LSH family exhibits a trade-off between evaluation time and\nquality that is close to optimal for a natural class of LSH functions. \n\n"}
{"id": "1509.04397", "contents": "Title: Exponential Family Matrix Completion under Structural Constraints Abstract: We consider the matrix completion problem of recovering a structured matrix\nfrom noisy and partial measurements. Recent works have proposed tractable\nestimators with strong statistical guarantees for the case where the underlying\nmatrix is low--rank, and the measurements consist of a subset, either of the\nexact individual entries, or of the entries perturbed by additive Gaussian\nnoise, which is thus implicitly suited for thin--tailed continuous data.\nArguably, common applications of matrix completion require estimators for (a)\nheterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)\nfor heterogeneous noise models (beyond Gaussian), which capture varied\nuncertainty in the measurements, and (c) heterogeneous structural constraints\nbeyond low--rank, such as block--sparsity, or a superposition structure of\nlow--rank plus elementwise sparseness, among others. In this paper, we provide\na vastly unified framework for generalized matrix completion by considering a\nmatrix completion setting wherein the matrix entries are sampled from any\nmember of the rich family of exponential family distributions; and impose\ngeneral structural constraints on the underlying matrix, as captured by a\ngeneral regularizer $\\mathcal{R}(.)$. We propose a simple convex regularized\n$M$--estimator for the generalized framework, and provide a unified and novel\nstatistical analysis for this general class of estimators. We finally\ncorroborate our theoretical results on simulated datasets. \n\n"}
{"id": "1509.04581", "contents": "Title: Kernelized Deep Convolutional Neural Network for Describing Complex\n  Images Abstract: With the impressive capability to capture visual content, deep convolutional\nneural networks (CNN) have demon- strated promising performance in various\nvision-based ap- plications, such as classification, recognition, and objec- t\ndetection. However, due to the intrinsic structure design of CNN, for images\nwith complex content, it achieves lim- ited capability on invariance to\ntranslation, rotation, and re-sizing changes, which is strongly emphasized in\nthe s- cenario of content-based image retrieval. In this paper, to address this\nproblem, we proposed a new kernelized deep convolutional neural network. We\nfirst discuss our motiva- tion by an experimental study to demonstrate the\nsensitivi- ty of the global CNN feature to the basic geometric trans-\nformations. Then, we propose to represent visual content with approximate\ninvariance to the above geometric trans- formations from a kernelized\nperspective. We extract CNN features on the detected object-like patches and\naggregate these patch-level CNN features to form a vectorial repre- sentation\nwith the Fisher vector model. The effectiveness of our proposed algorithm is\ndemonstrated on image search application with three benchmark datasets. \n\n"}
{"id": "1509.04640", "contents": "Title: Dynamic Poisson Factorization Abstract: Models for recommender systems use latent factors to explain the preferences\nand behaviors of users with respect to a set of items (e.g., movies, books,\nacademic papers). Typically, the latent factors are assumed to be static and,\ngiven these factors, the observed preferences and behaviors of users are\nassumed to be generated without order. These assumptions limit the explorative\nand predictive capabilities of such models, since users' interests and item\npopularity may evolve over time. To address this, we propose dPF, a dynamic\nmatrix factorization model based on the recent Poisson factorization model for\nrecommendations. dPF models the time evolving latent factors with a Kalman\nfilter and the actions with Poisson distributions. We derive a scalable\nvariational inference algorithm to infer the latent factors. Finally, we\ndemonstrate dPF on 10 years of user click data from arXiv.org, one of the\nlargest repository of scientific papers and a formidable source of information\nabout the behavior of scientists. Empirically we show performance improvement\nover both static and, more recently proposed, dynamic recommendation models. We\nalso provide a thorough exploration of the inferred posteriors over the latent\nvariables. \n\n"}
{"id": "1509.06041", "contents": "Title: Robust Image Sentiment Analysis Using Progressively Trained and Domain\n  Transferred Deep Networks Abstract: Sentiment analysis of online user generated content is important for many\nsocial media analytics tasks. Researchers have largely relied on textual\nsentiment analysis to develop systems to predict political elections, measure\neconomic indicators, and so on. Recently, social media users are increasingly\nusing images and videos to express their opinions and share their experiences.\nSentiment analysis of such large scale visual content can help better extract\nuser sentiments toward events or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is complementary to textual\nsentiment analysis. Motivated by the needs in leveraging large scale yet noisy\ntraining data to solve the extremely challenging problem of image sentiment\nanalysis, we employ Convolutional Neural Networks (CNN). We first design a\nsuitable CNN architecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment algorithm to label\nFlickr images. To make use of such noisy machine labeled data, we employ a\nprogressive strategy to fine-tune the deep network. Furthermore, we improve the\nperformance on Twitter images by inducing domain transfer with a small number\nof manually labeled Twitter images. We have conducted extensive experiments on\nmanually labeled Twitter images. The results show that the proposed CNN can\nachieve better performance in image sentiment analysis than competing\nalgorithms. \n\n"}
{"id": "1509.07087", "contents": "Title: Deep Temporal Sigmoid Belief Networks for Sequence Modeling Abstract: Deep dynamic generative models are developed to learn sequential dependencies\nin time-series data. The multi-layered model is designed by constructing a\nhierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential\nstack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden\nstate, inherited from the previous SBNs in the sequence, and is used to\nregulate its hidden bias. Scalable learning and inference algorithms are\nderived by introducing a recognition model that yields fast sampling from the\nvariational posterior. This recognition model is trained jointly with the\ngenerative model, by maximizing its variational lower bound on the\nlog-likelihood. Experimental results on bouncing balls, polyphonic music,\nmotion capture, and text streams show that the proposed approach achieves\nstate-of-the-art predictive performance, and has the capacity to synthesize\nvarious sequences. \n\n"}
{"id": "1510.01991", "contents": "Title: HDIdx: High-Dimensional Indexing for Efficient Approximate Nearest\n  Neighbor Search Abstract: Fast Nearest Neighbor (NN) search is a fundamental challenge in large-scale\ndata processing and analytics, particularly for analyzing multimedia contents\nwhich are often of high dimensionality. Instead of using exact NN search,\nextensive research efforts have been focusing on approximate NN search\nalgorithms. In this work, we present \"HDIdx\", an efficient high-dimensional\nindexing library for fast approximate NN search, which is open-source and\nwritten in Python. It offers a family of state-of-the-art algorithms that\nconvert input high-dimensional vectors into compact binary codes, making them\nvery efficient and scalable for NN search with very low space complexity. \n\n"}
{"id": "1510.02348", "contents": "Title: A vertex similarity index for better personalized recommendation Abstract: Recommender systems benefit us in tackling the problem of information\noverload by predicting our potential choices among diverse niche objects. So\nfar, a variety of personalized recommendation algorithms have been proposed and\nmost of them are based on similarities, such as collaborative filtering and\nmass diffusion. Here, we propose a novel vertex similarity index named CosRA,\nwhich combines advantages of both the cosine index and the resource-allocation\n(RA) index. By applying the CosRA index to real recommender systems including\nMovieLens, Netflix and RYM, we show that the CosRA-based method has better\nperformance in accuracy, diversity and novelty than some benchmark methods.\nMoreover, the CosRA index is free of parameters, which is a significant\nadvantage in real applications. Further experiments show that the introduction\nof two turnable parameters cannot remarkably improve the overall performance of\nthe CosRA index. \n\n"}
{"id": "1510.02706", "contents": "Title: Conditional Risk Minimization for Stochastic Processes Abstract: We study the task of learning from non-i.i.d. data. In particular, we aim at\nlearning predictors that minimize the conditional risk for a stochastic\nprocess, i.e. the expected loss of the predictor on the next point conditioned\non the set of training samples observed so far. For non-i.i.d. data, the\ntraining set contains information about the upcoming samples, so learning with\nrespect to the conditional distribution can be expected to yield better\npredictors than one obtains from the classical setting of minimizing the\nmarginal risk. Our main contribution is a practical estimator for the\nconditional risk based on the theory of non-parametric time-series prediction,\nand a finite sample concentration bound that establishes uniform convergence of\nthe estimator to the true conditional risk under certain regularity assumptions\non the process. \n\n"}
{"id": "1510.04822", "contents": "Title: SGD with Variance Reduction beyond Empirical Risk Minimization Abstract: We introduce a doubly stochastic proximal gradient algorithm for optimizing a\nfinite average of smooth convex functions, whose gradients depend on\nnumerically expensive expectations. Our main motivation is the acceleration of\nthe optimization of the regularized Cox partial-likelihood (the core model used\nin survival analysis), but our algorithm can be used in different settings as\nwell. The proposed algorithm is doubly stochastic in the sense that gradient\nsteps are done using stochastic gradient descent (SGD) with variance reduction,\nwhere the inner expectations are approximated by a Monte-Carlo Markov-Chain\n(MCMC) algorithm. We derive conditions on the MCMC number of iterations\nguaranteeing convergence, and obtain a linear rate of convergence under strong\nconvexity and a sublinear rate without this assumption. We illustrate the fact\nthat our algorithm improves the state-of-the-art solver for regularized Cox\npartial-likelihood on several datasets from survival analysis. \n\n"}
{"id": "1510.05336", "contents": "Title: Clustering is Easy When ....What? Abstract: It is well known that most of the common clustering objectives are NP-hard to\noptimize. In practice, however, clustering is being routinely carried out. One\napproach for providing theoretical understanding of this seeming discrepancy is\nto come up with notions of clusterability that distinguish realistically\ninteresting input data from worst-case data sets. The hope is that there will\nbe clustering algorithms that are provably efficient on such \"clusterable\"\ninstances. This paper addresses the thesis that the computational hardness of\nclustering tasks goes away for inputs that one really cares about. In other\nwords, that \"Clustering is difficult only when it does not matter\" (the\n\\emph{CDNM thesis} for short).\n  I wish to present a a critical bird's eye overview of the results published\non this issue so far and to call attention to the gap between available and\ndesirable results on this issue. A longer, more detailed version of this note\nis available as arXiv:1507.05307.\n  I discuss which requirements should be met in order to provide formal support\nto the the CDNM thesis and then examine existing results in view of these\nrequirements and list some significant unsolved research challenges in that\ndirection. \n\n"}
{"id": "1510.08628", "contents": "Title: WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet\n  Allocation Abstract: Developing efficient and scalable algorithms for Latent Dirichlet Allocation\n(LDA) is of wide interest for many applications. Previous work has developed an\nO(1) Metropolis-Hastings sampling method for each token. However, the\nperformance is far from being optimal due to random accesses to the parameter\nmatrices and frequent cache misses.\n  In this paper, we first carefully analyze the memory access efficiency of\nexisting algorithms for LDA by the scope of random access, which is the size of\nthe memory region in which random accesses fall, within a short period of time.\nWe then develop WarpLDA, an LDA sampler which achieves both the best O(1) time\ncomplexity per token and the best O(K) scope of random access. Our empirical\nresults in a wide range of testing conditions demonstrate that WarpLDA is\nconsistently 5-15x faster than the state-of-the-art Metropolis-Hastings based\nLightLDA, and is comparable or faster than the sparsity aware F+LDA. With\nWarpLDA, users can learn up to one million topics from hundreds of millions of\ndocuments in a few hours, at an unprecedentedly throughput of 11G tokens per\nsecond. \n\n"}
{"id": "1511.00146", "contents": "Title: Faster Stochastic Variational Inference using Proximal-Gradient Methods\n  with General Divergence Functions Abstract: Several recent works have explored stochastic gradient methods for\nvariational inference that exploit the geometry of the variational-parameter\nspace. However, the theoretical properties of these methods are not\nwell-understood and these methods typically only apply to\nconditionally-conjugate models. We present a new stochastic method for\nvariational inference which exploits the geometry of the variational-parameter\nspace and also yields simple closed-form updates even for non-conjugate models.\nWe also give a convergence-rate analysis of our method and many other previous\nmethods which exploit the geometry of the space. Our analysis generalizes\nexisting convergence results for stochastic mirror-descent on non-convex\nobjectives by using a more general class of divergence functions. Beyond giving\na theoretical justification for a variety of recent methods, our experiments\nshow that new algorithms derived in this framework lead to state of the art\nresults on a variety of problems. Further, due to its generality, we expect\nthat our theoretical analysis could also apply to other applications. \n\n"}
{"id": "1511.00830", "contents": "Title: The Variational Fair Autoencoder Abstract: We investigate the problem of learning representations that are invariant to\ncertain nuisance or sensitive factors of variation in the data while retaining\nas much of the remaining information as possible. Our model is based on a\nvariational autoencoding architecture with priors that encourage independence\nbetween sensitive and latent factors of variation. Any subsequent processing,\nsuch as classification, can then be performed on this purged latent\nrepresentation. To remove any remaining dependencies we incorporate an\nadditional penalty term based on the \"Maximum Mean Discrepancy\" (MMD) measure.\nWe discuss how these architectures can be efficiently trained on data and show\nin experiments that this method is more effective than previous work in\nremoving unwanted sources of variation while maintaining informative latent\nrepresentations. \n\n"}
{"id": "1511.00906", "contents": "Title: Reinventing the Triangles: Rule of Thumb for Assessing Detectability Abstract: Statistical significance of network clustering has been an unresolved problem\nsince it was observed that community detection algorithms produce false\npositives even in random graphs. After a phase transition between undetectable\nand detectable cluster structures was discovered, the connection between\nspectra of adjacency matrices and detectability limits were shown, and both\nwere calculated for a wide range of networks with arbitrary degree\ndistributions and community structure. In practice the full eigenspectrum is\nnot known, and whether a given network has any communities within detectability\nregime cannot be easily established. Based on the global clustering coefficient\nwe construct a criterion telling whether in an undirected, unweighted network\nthere is some/no detectable community structure, or if the network is in a\ntransient regime. The method is simple and faster than methods involving\nbootstrapping. \n\n"}
{"id": "1511.02124", "contents": "Title: Barrier Frank-Wolfe for Marginal Inference Abstract: We introduce a globally-convergent algorithm for optimizing the\ntree-reweighted (TRW) variational objective over the marginal polytope. The\nalgorithm is based on the conditional gradient method (Frank-Wolfe) and moves\npseudomarginals within the marginal polytope through repeated maximum a\nposteriori (MAP) calls. This modular structure enables us to leverage black-box\nMAP solvers (both exact and approximate) for variational inference, and obtains\nmore accurate results than tree-reweighted algorithms that optimize over the\nlocal consistency relaxation. Theoretically, we bound the sub-optimality for\nthe proposed algorithm despite the TRW objective having unbounded gradients at\nthe boundary of the marginal polytope. Empirically, we demonstrate the\nincreased quality of results found by tightening the relaxation over the\nmarginal polytope as well as the spanning tree polytope on synthetic and\nreal-world instances. \n\n"}
{"id": "1511.03036", "contents": "Title: Semantic processing of EHR data for clinical research Abstract: There is a growing need to semantically process and integrate clinical data\nfrom different sources for clinical research. This paper presents an approach\nto integrate EHRs from heterogeneous resources and generate integrated data in\ndifferent data formats or semantics to support various clinical research\napplications. The proposed approach builds semantic data virtualization layers\non top of data sources, which generate data in the requested semantics or\nformats on demand. This approach avoids upfront dumping to and synchronizing of\nthe data with various representations. Data from different EHR systems are\nfirst mapped to RDF data with source semantics, and then converted to\nrepresentations with harmonized domain semantics where domain ontologies and\nterminologies are used to improve reusability. It is also possible to further\nconvert data to application semantics and store the converted results in\nclinical research databases, e.g. i2b2, OMOP, to support different clinical\nresearch settings. Semantic conversions between different representations are\nexplicitly expressed using N3 rules and executed by an N3 Reasoner (EYE), which\ncan also generate proofs of the conversion processes. The solution presented in\nthis paper has been applied to real-world applications that process large scale\nEHR data. \n\n"}
{"id": "1511.04780", "contents": "Title: Causal interpretation rules for encoding and decoding models in\n  neuroimaging Abstract: Causal terminology is often introduced in the interpretation of encoding and\ndecoding models trained on neuroimaging data. In this article, we investigate\nwhich causal statements are warranted and which ones are not supported by\nempirical evidence. We argue that the distinction between encoding and decoding\nmodels is not sufficient for this purpose: relevant features in encoding and\ndecoding models carry a different meaning in stimulus- and in response-based\nexperimental paradigms. We show that only encoding models in the stimulus-based\nsetting support unambiguous causal interpretations. By combining encoding and\ndecoding models trained on the same data, however, we obtain insights into\ncausal relations beyond those that are implied by each individual model type.\nWe illustrate the empirical relevance of our theoretical findings on EEG data\nrecorded during a visuo-motor learning task. \n\n"}
{"id": "1511.05121", "contents": "Title: Deep Kalman Filters Abstract: Kalman Filters are one of the most influential models of time-varying\nphenomena. They admit an intuitive probabilistic interpretation, have a simple\nfunctional form, and enjoy widespread adoption in a variety of disciplines.\nMotivated by recent variational methods for learning deep generative models, we\nintroduce a unified algorithm to efficiently learn a broad spectrum of Kalman\nfilters. Of particular interest is the use of temporal generative models for\ncounterfactual inference. We investigate the efficacy of such models for\ncounterfactual inference, and to that end we introduce the \"Healing MNIST\"\ndataset where long-term structure, noise and actions are applied to sequences\nof digits. We show the efficacy of our method for modeling this dataset. We\nfurther show how our model can be used for counterfactual inference for\npatients, based on electronic health record data of 8,000 patients over 4.5\nyears. \n\n"}
{"id": "1511.06033", "contents": "Title: EigenRec: Generalizing PureSVD for Effective and Efficient Top-N\n  Recommendations Abstract: We introduce EigenRec; a versatile and efficient Latent-Factor framework for\nTop-N Recommendations that includes the well-known PureSVD algorithm as a\nspecial case. EigenRec builds a low dimensional model of an inter-item\nproximity matrix that combines a similarity component, with a scaling operator,\ndesigned to control the influence of the prior item popularity on the final\nmodel. Seeing PureSVD within our framework provides intuition about its inner\nworkings, exposes its inherent limitations, and also, paves the path towards\npainlessly improving its recommendation performance. A comprehensive set of\nexperiments on the MovieLens and the Yahoo datasets based on widely applied\nperformance metrics, indicate that EigenRec outperforms several\nstate-of-the-art algorithms, in terms of Standard and Long-Tail recommendation\naccuracy, exhibiting low susceptibility to sparsity, even in its most extreme\nmanifestations -- the Cold-Start problems. At the same time EigenRec has an\nattractive computational profile and it can apply readily in large-scale\nrecommendation settings. \n\n"}
{"id": "1511.06390", "contents": "Title: Unsupervised and Semi-supervised Learning with Categorical Generative\n  Adversarial Networks Abstract: In this paper we present a method for learning a discriminative classifier\nfrom unlabeled or partially labeled data. Our approach is based on an objective\nfunction that trades-off mutual information between observed examples and their\npredicted categorical class distribution, against robustness of the classifier\nto an adversarial generative model. The resulting algorithm can either be\ninterpreted as a natural generalization of the generative adversarial networks\n(GAN) framework or as an extension of the regularized information maximization\n(RIM) framework to robust classification against an optimal adversary. We\nempirically evaluate our method - which we dub categorical generative\nadversarial networks (or CatGAN) - on synthetic data as well as on challenging\nimage classification tasks, demonstrating the robustness of the learned\nclassifiers. We further qualitatively assess the fidelity of samples generated\nby the adversarial generator that is learned alongside the discriminative\nclassifier, and identify links between the CatGAN objective and discriminative\nclustering algorithms (such as RIM). \n\n"}
{"id": "1511.07643", "contents": "Title: Homophily and missing links in citation networks Abstract: Citation networks have been widely used to study the evolution of science\nthrough the lenses of the underlying patterns of knowledge flows among academic\npapers, authors, research sub-fields, and scientific journals. Here we focus on\ncitation networks to cast light on the salience of homophily, namely the\nprinciple that similarity breeds connection, for knowledge transfer between\npapers. To this end, we assess the degree to which citations tend to occur\nbetween papers that are concerned with seemingly related topics or research\nproblems. Drawing on a large data set of articles published in the journals of\nthe American Physical Society between 1893 and 2009, we propose a novel method\nfor measuring the similarity between articles through the statistical\nvalidation of the overlap between their bibliographies. Results suggest that\nthe probability of a citation made by one article to another is indeed an\nincreasing function of the similarity between the two articles. Our study also\nenables us to uncover missing citations between pairs of highly related\narticles, and may thus help identify barriers to effective knowledge flows. By\nquantifying the proportion of missing citations, we conduct a comparative\nassessment of distinct journals and research sub-fields in terms of their\nability to facilitate or impede the dissemination of knowledge. Findings\nindicate that knowledge transfer seems to be more effectively facilitated by\njournals of wide visibility, such as Physical Review Letters, than by\nlower-impact ones. Our study has important implications for authors, editors\nand reviewers of scientific journals, as well as public preprint repositories,\nas it provides a procedure for recommending relevant yet missing references and\nproperly integrating bibliographies of papers. \n\n"}
{"id": "1511.07902", "contents": "Title: Performance Limits of Stochastic Sub-Gradient Learning, Part I: Single\n  Agent Case Abstract: In this work and the supporting Part II, we examine the performance of\nstochastic sub-gradient learning strategies under weaker conditions than\nusually considered in the literature. The new conditions are shown to be\nautomatically satisfied by several important cases of interest including SVM,\nLASSO, and Total-Variation denoising formulations. In comparison, these\nproblems do not satisfy the traditional assumptions used in prior analyses and,\ntherefore, conclusions derived from these earlier treatments are not directly\napplicable to these problems. The results in this article establish that\nstochastic sub-gradient strategies can attain linear convergence rates, as\nopposed to sub-linear rates, to the steady-state regime. A realizable\nexponential-weighting procedure is employed to smooth the intermediate iterates\nand guarantee useful performance bounds in terms of convergence rate and\nexcessive risk performance. Part I of this work focuses on single-agent\nscenarios, which are common in stand-alone learning applications, while Part II\nextends the analysis to networked learners. The theoretical conclusions are\nillustrated by several examples and simulations, including comparisons with the\nFISTA procedure. \n\n"}
{"id": "1511.08327", "contents": "Title: Random Forests for Big Data Abstract: Big Data is one of the major challenges of statistical science and has\nnumerous consequences from algorithmic and theoretical viewpoints. Big Data\nalways involve massive data but they also often include online data and data\nheterogeneity. Recently some statistical methods have been adapted to process\nBig Data, like linear regression models, clustering methods and bootstrapping\nschemes. Based on decision trees combined with aggregation and bootstrap ideas,\nrandom forests were introduced by Breiman in 2001. They are a powerful\nnonparametric statistical method allowing to consider in a single and versatile\nframework regression problems, as well as two-class and multi-class\nclassification problems. Focusing on classification problems, this paper\nproposes a selective review of available proposals that deal with scaling\nrandom forests to Big Data problems. These proposals rely on parallel\nenvironments or on online adaptations of random forests. We also describe how\nrelated quantities -- such as out-of-bag error and variable importance -- are\naddressed in these methods. Then, we formulate various remarks for random\nforests in the Big Data context. Finally, we experiment five variants on two\nmassive datasets (15 and 120 millions of observations), a simulated one as well\nas real world data. One variant relies on subsampling while three others are\nrelated to parallel implementations of random forests and involve either\nvarious adaptations of bootstrap to Big Data or to \"divide-and-conquer\"\napproaches. The fifth variant relates on online learning of random forests.\nThese numerical experiments lead to highlight the relative performance of the\ndifferent variants, as well as some of their limitations. \n\n"}
{"id": "1512.00792", "contents": "Title: Microclustering: When the Cluster Sizes Grow Sublinearly with the Size\n  of the Data Set Abstract: Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some tasks, this\nassumption is undesirable. For example, when performing entity resolution, the\nsize of each cluster is often unrelated to the size of the data set.\nConsequently, each cluster contains a negligible fraction of the total number\nof data points. Such tasks therefore require models that yield clusters whose\nsizes grow sublinearly with the size of the data set. We address this\nrequirement by defining the \\emph{microclustering property} and introducing a\nnew model that exhibits this property. We compare this model to several\ncommonly used clustering models by checking model fit using real and simulated\ndata sets. \n\n"}
{"id": "1512.03542", "contents": "Title: Distilling Knowledge from Deep Networks with Applications to Healthcare\n  Domain Abstract: Exponential growth in Electronic Healthcare Records (EHR) has resulted in new\nopportunities and urgent needs for discovery of meaningful data-driven\nrepresentations and patterns of diseases in Computational Phenotyping research.\nDeep Learning models have shown superior performance for robust prediction in\ncomputational phenotyping tasks, but suffer from the issue of model\ninterpretability which is crucial for clinicians involved in decision-making.\nIn this paper, we introduce a novel knowledge-distillation approach called\nInterpretable Mimic Learning, to learn interpretable phenotype features for\nmaking robust prediction while mimicking the performance of deep learning\nmodels. Our framework uses Gradient Boosting Trees to learn interpretable\nfeatures from deep learning models such as Stacked Denoising Autoencoder and\nLong Short-Term Memory. Exhaustive experiments on a real-world clinical\ntime-series dataset show that our method obtains similar or better performance\nthan the deep learning models, and it provides interpretable phenotypes for\nclinical decision making. \n\n"}
{"id": "1512.04633", "contents": "Title: Efficient Algorithms for Personalized PageRank Abstract: We present new, more efficient algorithms for estimating random walk scores\nsuch as Personalized PageRank from a given source node to one or several target\nnodes. These scores are useful for personalized search and recommendations on\nnetworks including social networks, user-item networks, and the web. Past work\nhas proposed using Monte Carlo or using linear algebra to estimate scores from\na single source to every target, making them inefficient for a single pair. Our\ncontribution is a new bidirectional algorithm which combines linear algebra and\nMonte Carlo to achieve significant speed improvements. On a diverse set of six\ngraphs, our algorithm is 70x faster than past state-of-the-art algorithms. We\nalso present theoretical analysis: while past algorithms require $\\Omega(n)$\ntime to estimate a random walk score of typical size $\\frac{1}{n}$ on an\n$n$-node graph to a given constant accuracy, our algorithm requires only\n$O(\\sqrt{m})$ expected time for an average target, where $m$ is the number of\nedges, and is provably accurate.\n  In addition to our core bidirectional estimator for personalized PageRank, we\npresent an alternative algorithm for undirected graphs, a generalization to\narbitrary walk lengths and Markov Chains, an algorithm for personalized search\nranking, and an algorithm for sampling random paths from a given source to a\ngiven set of targets. We expect our bidirectional methods can be extended in\nother ways and will be useful subroutines in other graph analysis problems. \n\n"}
{"id": "1601.01892", "contents": "Title: Song Recommendation with Non-Negative Matrix Factorization and Graph\n  Total Variation Abstract: This work formulates a novel song recommender system as a matrix completion\nproblem that benefits from collaborative filtering through Non-negative Matrix\nFactorization (NMF) and content-based filtering via total variation (TV) on\ngraphs. The graphs encode both playlist proximity information and song\nsimilarity, using a rich combination of audio, meta-data and social features.\nAs we demonstrate, our hybrid recommendation system is very versatile and\nincorporates several well-known methods while outperforming them. Particularly,\nwe show on real-world data that our model overcomes w.r.t. two evaluation\nmetrics the recommendation of models solely based on low-rank information,\ngraph-based information or a combination of both. \n\n"}
{"id": "1601.03354", "contents": "Title: Identifier Namespaces in Mathematical Notation Abstract: In this thesis, we look at the problem of assigning each identifier of a\ndocument to a namespace. At the moment, there does not exist a special dataset\nwhere all identifiers are grouped to namespaces, and therefore we need to\ncreate such a dataset ourselves.\n  To do that, we need to find groups of documents that use identifiers in the\nsame way. This can be done with cluster analysis methods. We argue that\ndocuments can be represented by the identifiers they contain, and this approach\nis similar to representing textual information in the Vector Space Model.\nBecause of this, we can apply traditional document clustering techniques for\nnamespace discovery.\n  Because the problem is new, there is no gold standard dataset, and it is hard\nto evaluate the performance of our method. To overcome it, we first use Java\nsource code as a dataset for our experiments, since it contains the namespace\ninformation. We verify that our method can partially recover namespaces from\nsource code using only information about identifiers.\n  The algorithms are evaluated on the English Wikipedia, and the proposed\nmethod can extract namespaces on a variety of topics. After extraction, the\nnamespaces are organized into a hierarchical structure by using existing\nclassification schemes such as MSC, PACS and ACM. We also apply it to the\nRussian Wikipedia, and the results are consistent across the languages.\n  To our knowledge, the problem of introducing namespaces to mathematics has\nnot been studied before, and prior to our work there has been no dataset where\nidentifiers are grouped into namespaces. Thus, our result is not only a good\nstart, but also a good indicator that automatic namespace discovery is\npossible. \n\n"}
{"id": "1601.06439", "contents": "Title: Who Ordered This?: Exploiting Implicit User Tag Order Preferences for\n  Personalized Image Tagging Abstract: What makes a person pick certain tags over others when tagging an image? Does\nthe order that a person presents tags for a given image follow an implicit bias\nthat is personal? Can these biases be used to improve existing automated image\ntagging systems? We show that tag ordering, which has been largely overlooked\nby the image tagging community, is an important cue in understanding user\ntagging behavior and can be used to improve auto-tagging systems. Inspired by\nthe assumption that people order their tags, we propose a new way of measuring\ntag preferences, and also propose a new personalized tagging objective function\nthat explicitly considers a user's preferred tag orderings. We also provide a\n(partially) greedy algorithm that produces good solutions to our new objective\nand under certain conditions produces an optimal solution. We validate our\nmethod on a subset of Flickr images that spans 5000 users, over 5200 tags, and\nover 90,000 images. Our experiments show that exploiting personalized tag\norders improves the average performance of state-of-art approaches both on\nper-image and per-user bases. \n\n"}
{"id": "1601.07621", "contents": "Title: Revealing Fundamental Physics from the Daya Bay Neutrino Experiment\n  using Deep Neural Networks Abstract: Experiments in particle physics produce enormous quantities of data that must\nbe analyzed and interpreted by teams of physicists. This analysis is often\nexploratory, where scientists are unable to enumerate the possible types of\nsignal prior to performing the experiment. Thus, tools for summarizing,\nclustering, visualizing and classifying high-dimensional data are essential. In\nthis work, we show that meaningful physical content can be revealed by\ntransforming the raw data into a learned high-level representation using deep\nneural networks, with measurements taken at the Daya Bay Neutrino Experiment as\na case study. We further show how convolutional deep neural networks can\nprovide an effective classification filter with greater than 97% accuracy\nacross different classes of physics events, significantly better than other\nmachine learning approaches. \n\n"}
{"id": "1602.00357", "contents": "Title: DeepCare: A Deep Dynamic Memory Model for Predictive Medicine Abstract: Personalized predictive medicine necessitates the modeling of patient illness\nand care processes, which inherently have long-term temporal dependencies.\nHealthcare observations, recorded in electronic medical records, are episodic\nand irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural\nnetwork that reads medical records, stores previous illness history, infers\ncurrent illness states and predicts future medical outcomes. At the data level,\nDeepCare represents care episodes as vectors in space, models patient health\nstate trajectories through explicit memory of historical records. Built on Long\nShort-Term Memory (LSTM), DeepCare introduces time parameterizations to handle\nirregular timed events by moderating the forgetting and consolidation of memory\ncells. DeepCare also incorporates medical interventions that change the course\nof illness and shape future medical risk. Moving up to the health state level,\nhistorical and present health states are then aggregated through multiscale\ntemporal pooling, before passing through a neural network that estimates future\noutcomes. We demonstrate the efficacy of DeepCare for disease progression\nmodeling, intervention recommendation, and future risk prediction. On two\nimportant cohorts with heavy social and economic burden -- diabetes and mental\nhealth -- the results show improved modeling and risk prediction accuracy. \n\n"}
{"id": "1602.02047", "contents": "Title: Utiliza\\c{c}\\~ao de Grafos e Matriz de Similaridade na Sumariza\\c{c}\\~ao\n  Autom\\'atica de Documentos Baseada em Extra\\c{c}\\~ao de Frases Abstract: The internet increased the amount of information available. However, the\nreading and understanding of this information are costly tasks. In this\nscenario, the Natural Language Processing (NLP) applications enable very\nimportant solutions, highlighting the Automatic Text Summarization (ATS), which\nproduce a summary from one or more source texts. Automatically summarizing one\nor more texts, however, is a complex task because of the difficulties inherent\nto the analysis and generation of this summary. This master's thesis describes\nthe main techniques and methodologies (NLP and heuristics) to generate\nsummaries. We have also addressed and proposed some heuristics based on graphs\nand similarity matrix to measure the relevance of judgments and to generate\nsummaries by extracting sentences. We used the multiple languages (English,\nFrench and Spanish), CSTNews (Brazilian Portuguese), RPM (French) and DECODA\n(French) corpus to evaluate the developped systems. The results obtained were\nquite interesting. \n\n"}
{"id": "1602.02181", "contents": "Title: Active Information Acquisition Abstract: We propose a general framework for sequential and dynamic acquisition of\nuseful information in order to solve a particular task. While our goal could in\nprinciple be tackled by general reinforcement learning, our particular setting\nis constrained enough to allow more efficient algorithms. In this paper, we\nwork under the Learning to Search framework and show how to formulate the goal\nof finding a dynamic information acquisition policy in that framework. We apply\nour formulation on two tasks, sentiment analysis and image recognition, and\nshow that the learned policies exhibit good statistical performance. As an\nemergent byproduct, the learned policies show a tendency to focus on the most\nprominent parts of each instance and give harder instances more attention\nwithout explicitly being trained to do so. \n\n"}
{"id": "1602.02285", "contents": "Title: A Deep Learning Approach to Unsupervised Ensemble Learning Abstract: We show how deep learning methods can be applied in the context of\ncrowdsourcing and unsupervised ensemble learning. First, we prove that the\npopular model of Dawid and Skene, which assumes that all classifiers are\nconditionally independent, is {\\em equivalent} to a Restricted Boltzmann\nMachine (RBM) with a single hidden node. Hence, under this model, the posterior\nprobabilities of the true labels can be instead estimated via a trained RBM.\nNext, to address the more general case, where classifiers may strongly violate\nthe conditional independence assumption, we propose to apply RBM-based Deep\nNeural Net (DNN). Experimental results on various simulated and real-world\ndatasets demonstrate that our proposed DNN approach outperforms other\nstate-of-the-art methods, in particular when the data violates the conditional\nindependence assumption. \n\n"}
{"id": "1602.02514", "contents": "Title: Fast K-Means with Accurate Bounds Abstract: We propose a novel accelerated exact k-means algorithm, which performs better\nthan the current state-of-the-art low-dimensional algorithm in 18 of 22\nexperiments, running up to 3 times faster. We also propose a general\nimprovement of existing state-of-the-art accelerated exact k-means algorithms\nthrough better estimates of the distance bounds used to reduce the number of\ndistance calculations, and get a speedup in 36 of 44 experiments, up to 1.8\ntimes faster.\n  We have conducted experiments with our own implementations of existing\nmethods to ensure homogeneous evaluation of performance, and we show that our\nimplementations perform as well or better than existing available\nimplementations. Finally, we propose simplified variants of standard approaches\nand show that they are faster than their fully-fledged counterparts in 59 of 62\nexperiments. \n\n"}
{"id": "1602.02666", "contents": "Title: A Variational Analysis of Stochastic Gradient Algorithms Abstract: Stochastic Gradient Descent (SGD) is an important algorithm in machine\nlearning. With constant learning rates, it is a stochastic process that, after\nan initial phase of convergence, generates samples from a stationary\ndistribution. We show that SGD with constant rates can be effectively used as\nan approximate posterior inference algorithm for probabilistic modeling.\nSpecifically, we show how to adjust the tuning parameters of SGD such as to\nmatch the resulting stationary distribution to the posterior. This analysis\nrests on interpreting SGD as a continuous-time stochastic process and then\nminimizing the Kullback-Leibler divergence between its stationary distribution\nand the target posterior. (This is in the spirit of variational inference.) In\nmore detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then\nuse properties of this process to derive the optimal parameters. This\ntheoretical framework also connects SGD to modern scalable inference\nalgorithms; we analyze the recently proposed stochastic gradient Fisher scoring\nunder this perspective. We demonstrate that SGD with properly chosen constant\nrates gives a new way to optimize hyperparameters in probabilistic models. \n\n"}
{"id": "1602.02850", "contents": "Title: Toward Optimal Feature Selection in Naive Bayes for Text Categorization Abstract: Automated feature selection is important for text categorization to reduce\nthe feature size and to speed up the learning process of classifiers. In this\npaper, we present a novel and efficient feature selection framework based on\nthe Information Theory, which aims to rank the features with their\ndiscriminative capacity for classification. We first revisit two information\nmeasures: Kullback-Leibler divergence and Jeffreys divergence for binary\nhypothesis testing, and analyze their asymptotic properties relating to type I\nand type II errors of a Bayesian classifier. We then introduce a new divergence\nmeasure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure\nmulti-distribution divergence for multi-class classification. Based on the\nJMH-divergence, we develop two efficient feature selection methods, termed\nmaximum discrimination ($MD$) and $MD-\\chi^2$ methods, for text categorization.\nThe promising results of extensive experiments demonstrate the effectiveness of\nthe proposed approaches. \n\n"}
{"id": "1602.03014", "contents": "Title: Herding as a Learning System with Edge-of-Chaos Dynamics Abstract: Herding defines a deterministic dynamical system at the edge of chaos. It\ngenerates a sequence of model states and parameters by alternating parameter\nperturbations with state maximizations, where the sequence of states can be\ninterpreted as \"samples\" from an associated MRF model. Herding differs from\nmaximum likelihood estimation in that the sequence of parameters does not\nconverge to a fixed point and differs from an MCMC posterior sampling approach\nin that the sequence of states is generated deterministically. Herding may be\ninterpreted as a\"perturb and map\" method where the parameter perturbations are\ngenerated using a deterministic nonlinear dynamical system rather than randomly\nfrom a Gumbel distribution. This chapter studies the distinct statistical\ncharacteristics of the herding algorithm and shows that the fast convergence\nrate of the controlled moments may be attributed to edge of chaos dynamics. The\nherding algorithm can also be generalized to models with latent variables and\nto a discriminative learning setting. The perceptron cycling theorem ensures\nthat the fast moment matching property is preserved in the more general\nframework. \n\n"}
{"id": "1602.03534", "contents": "Title: Unsupervised Transductive Domain Adaptation Abstract: Supervised learning with large scale labeled datasets and deep layered models\nhas made a paradigm shift in diverse areas in learning and recognition.\nHowever, this approach still suffers generalization issues under the presence\nof a domain shift between the training and the test data distribution. In this\nregard, unsupervised domain adaptation algorithms have been proposed to\ndirectly address the domain shift problem. In this paper, we approach the\nproblem from a transductive perspective. We incorporate the domain shift and\nthe transductive target inference into our framework by jointly solving for an\nasymmetric similarity metric and the optimal transductive target label\nassignment. We also show that our model can easily be extended for deep feature\nlearning in order to learn features which are discriminative in the target\ndomain. Our experiments show that the proposed method significantly outperforms\nstate-of-the-art algorithms in both object recognition and digit classification\nexperiments by a large margin. \n\n"}
{"id": "1602.04265", "contents": "Title: Lasso Guarantees for Time Series Estimation Under Subgaussian Tails and\n  $ \\beta $-Mixing Abstract: Many theoretical results on estimation of high dimensional time series\nrequire specifying an underlying data generating model (DGM). Instead, along\nthe footsteps of~\\cite{wong2017lasso}, this paper relies only on (strict)\nstationarity and $ \\beta $-mixing condition to establish consistency of lasso\nwhen data comes from a $\\beta$-mixing process with marginals having subgaussian\ntails. Because of the general assumptions, the data can come from DGMs\ndifferent than standard time series models such as VAR or ARCH. When the true\nDGM is not VAR, the lasso estimates correspond to those of the best linear\npredictors using the past observations. We establish non-asymptotic\ninequalities for estimation and prediction errors of the lasso estimates.\nTogether with~\\cite{wong2017lasso}, we provide lasso guarantees that cover full\nspectrum of the parameters in specifications of $ \\beta $-mixing subgaussian\ntime series. Applications of these results potentially extend to non-Gaussian,\nnon-Markovian and non-linear times series models as the examples we provide\ndemonstrate. In order to prove our results, we derive a novel Hanson-Wright\ntype concentration inequality for $\\beta$-mixing subgaussian random vectors\nthat may be of independent interest. \n\n"}
{"id": "1602.04805", "contents": "Title: DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution\n  Regression Abstract: Performing exact posterior inference in complex generative models is often\ndifficult or impossible due to an expensive to evaluate or intractable\nlikelihood function. Approximate Bayesian computation (ABC) is an inference\nframework that constructs an approximation to the true likelihood based on the\nsimilarity between the observed and simulated data as measured by a predefined\nset of summary statistics. Although the choice of appropriate problem-specific\nsummary statistics crucially influences the quality of the likelihood\napproximation and hence also the quality of the posterior sample in ABC, there\nare only few principled general-purpose approaches to the selection or\nconstruction of such summary statistics. In this paper, we develop a novel\nframework for this task using kernel-based distribution regression. We model\nthe functional relationship between data distributions and the optimal choice\n(with respect to a loss function) of summary statistics using kernel-based\ndistribution regression. We show that our approach can be implemented in a\ncomputationally and statistically efficient way using the random Fourier\nfeatures framework for large-scale kernel learning. In addition to that, our\nframework shows superior performance when compared to related methods on toy\nand real-world problems. \n\n"}
{"id": "1602.04915", "contents": "Title: Gradient Descent Converges to Minimizers Abstract: We show that gradient descent converges to a local minimizer, almost surely\nwith random initialization. This is proved by applying the Stable Manifold\nTheorem from dynamical systems theory. \n\n"}
{"id": "1602.04983", "contents": "Title: Contextual Media Retrieval Using Natural Language Queries Abstract: The widespread integration of cameras in hand-held and head-worn devices as\nwell as the ability to share content online enables a large and diverse visual\ncapture of the world that millions of users build up collectively every day. We\nenvision these images as well as associated meta information, such as GPS\ncoordinates and timestamps, to form a collective visual memory that can be\nqueried while automatically taking the ever-changing context of mobile users\ninto account. As a first step towards this vision, in this work we present\nXplore-M-Ego: a novel media retrieval system that allows users to query a\ndynamic database of images and videos using spatio-temporal natural language\nqueries. We evaluate our system using a new dataset of real user queries as\nwell as through a usability study. One key finding is that there is a\nconsiderable amount of inter-user variability, for example in the resolution of\nspatial relations in natural language utterances. We show that our retrieval\nsystem can cope with this variability using personalisation through an online\nlearning-based retrieval formulation. \n\n"}
{"id": "1602.06042", "contents": "Title: Structured Sparse Regression via Greedy Hard-Thresholding Abstract: Several learning applications require solving high-dimensional regression\nproblems where the relevant features belong to a small number of (overlapping)\ngroups. For very large datasets and under standard sparsity constraints, hard\nthresholding methods have proven to be extremely efficient, but such methods\nrequire NP hard projections when dealing with overlapping groups. In this\npaper, we show that such NP-hard projections can not only be avoided by\nappealing to submodular optimization, but such methods come with strong\ntheoretical guarantees even in the presence of poorly conditioned data (i.e.\nsay when two features have correlation $\\geq 0.99$), which existing analyses\ncannot handle. These methods exhibit an interesting computation-accuracy\ntrade-off and can be extended to significantly harder problems such as sparse\noverlapping groups. Experiments on both real and synthetic data validate our\nclaims and demonstrate that the proposed methods are orders of magnitude faster\nthan other greedy and convex relaxation techniques for learning with\ngroup-structured sparsity. \n\n"}
{"id": "1602.07046", "contents": "Title: An Improved Gap-Dependency Analysis of the Noisy Power Method Abstract: We consider the noisy power method algorithm, which has wide applications in\nmachine learning and statistics, especially those related to principal\ncomponent analysis (PCA) under resource (communication, memory or privacy)\nconstraints. Existing analysis of the noisy power method shows an\nunsatisfactory dependency over the \"consecutive\" spectral gap\n$(\\sigma_k-\\sigma_{k+1})$ of an input data matrix, which could be very small\nand hence limits the algorithm's applicability. In this paper, we present a new\nanalysis of the noisy power method that achieves improved gap dependency for\nboth sample complexity and noise tolerance bounds. More specifically, we\nimprove the dependency over $(\\sigma_k-\\sigma_{k+1})$ to dependency over\n$(\\sigma_k-\\sigma_{q+1})$, where $q$ is an intermediate algorithm parameter and\ncould be much larger than the target rank $k$. Our proofs are built upon a\nnovel characterization of proximity between two subspaces that differ from\ncanonical angle characterizations analyzed in previous works. Finally, we apply\nour improved bounds to distributed private PCA and memory-efficient streaming\nPCA and obtain bounds that are superior to existing results in the literature. \n\n"}
{"id": "1602.07387", "contents": "Title: Discrete Distribution Estimation under Local Privacy Abstract: The collection and analysis of user data drives improvements in the app and\nweb ecosystems, but comes with risks to privacy. This paper examines discrete\ndistribution estimation under local privacy, a setting wherein service\nproviders can learn the distribution of a categorical statistic of interest\nwithout collecting the underlying data. We present new mechanisms, including\nhashed K-ary Randomized Response (KRR), that empirically meet or exceed the\nutility of existing mechanisms at all privacy levels. New theoretical results\ndemonstrate the order-optimality of KRR and the existing RAPPOR mechanism at\ndifferent privacy regimes. \n\n"}
{"id": "1602.07811", "contents": "Title: Narrative Smoothing: Dynamic Conversational Network for the Analysis of\n  TV Series Plots Abstract: Modern popular TV series often develop complex storylines spanning several\nseasons, but are usually watched in quite a discontinuous way. As a result, the\nviewer generally needs a comprehensive summary of the previous season plot\nbefore the new one starts. The generation of such summaries requires first to\nidentify and characterize the dynamics of the series subplots. One way of doing\nso is to study the underlying social network of interactions between the\ncharacters involved in the narrative. The standard tools used in the Social\nNetworks Analysis field to extract such a network rely on an integration of\ntime, either over the whole considered period, or as a sequence of several\ntime-slices. However, they turn out to be inappropriate in the case of TV\nseries, due to the fact the scenes showed onscreen alternatively focus on\nparallel storylines, and do not necessarily respect a traditional chronology.\nThis makes existing extraction methods inefficient to describe the dynamics of\nrelationships between characters, or to get a relevant instantaneous view of\nthe current social state in the plot. This is especially true for characters\nshown as interacting with each other at some previous point in the plot but\ntemporarily neglected by the narrative. In this article, we introduce narrative\nsmoothing, a novel, still exploratory, network extraction method. It smooths\nthe relationship dynamics based on the plot properties, aiming at solving some\nof the limitations present in the standard approaches. In order to assess our\nmethod, we apply it to a new corpus of 3 popular TV series, and compare it to\nboth standard approaches. Our results are promising, showing narrative\nsmoothing leads to more relevant observations when it comes to the\ncharacterization of the protagonists and their relationships. It could be used\nas a basis for further modeling the intertwined storylines constituting TV\nseries plots. \n\n"}
{"id": "1602.07863", "contents": "Title: Learning Gaussian Graphical Models With Fractional Marginal\n  Pseudo-likelihood Abstract: We propose a Bayesian approximate inference method for learning the\ndependence structure of a Gaussian graphical model. Using pseudo-likelihood, we\nderive an analytical expression to approximate the marginal likelihood for an\narbitrary graph structure without invoking any assumptions about\ndecomposability. The majority of the existing methods for learning Gaussian\ngraphical models are either restricted to decomposable graphs or require\nspecification of a tuning parameter that may have a substantial impact on\nlearned structures. By combining a simple sparsity inducing prior for the graph\nstructures with a default reference prior for the model parameters, we obtain a\nfast and easily applicable scoring function that works well for even\nhigh-dimensional data. We demonstrate the favourable performance of our\napproach by large-scale comparisons against the leading methods for learning\nnon-decomposable Gaussian graphical models. A theoretical justification for our\nmethod is provided by showing that it yields a consistent estimator of the\ngraph structure. \n\n"}
{"id": "1603.04733", "contents": "Title: Structured and Efficient Variational Deep Learning with Matrix Gaussian\n  Posteriors Abstract: We introduce a variational Bayesian neural network where the parameters are\ngoverned via a probability distribution on random matrices. Specifically, we\nemploy a matrix variate Gaussian \\cite{gupta1999matrix} parameter posterior\ndistribution where we explicitly model the covariance among the input and\noutput dimensions of each layer. Furthermore, with approximate covariance\nmatrices we can achieve a more efficient way to represent those correlations\nthat is also cheaper than fully factorized parameter posteriors. We further\nshow that with the \"local reprarametrization trick\"\n\\cite{kingma2015variational} on this posterior distribution we arrive at a\nGaussian Process \\cite{rasmussen2006gaussian} interpretation of the hidden\nunits in each layer and we, similarly with \\cite{gal2015dropout}, provide\nconnections with deep Gaussian processes. We continue in taking advantage of\nthis duality and incorporate \"pseudo-data\" \\cite{snelson2005sparse} in our\nmodel, which in turn allows for more efficient sampling while maintaining the\nproperties of the original model. The validity of the proposed approach is\nverified through extensive experiments. \n\n"}
{"id": "1603.05691", "contents": "Title: Do Deep Convolutional Nets Really Need to be Deep and Convolutional? Abstract: Yes, they do. This paper provides the first empirical demonstration that deep\nconvolutional models really need to be both deep and convolutional, even when\ntrained with methods such as distillation that allow small or shallow models of\nhigh accuracy to be trained. Although previous research showed that shallow\nfeed-forward nets sometimes can learn the complex functions previously learned\nby deep nets while using the same number of parameters as the deep models they\nmimic, in this paper we demonstrate that the same methods cannot be used to\ntrain accurate models on CIFAR-10 unless the student models contain multiple\nlayers of convolution. Although the student models do not have to be as deep as\nthe teacher model they mimic, the students need multiple convolutional layers\nto learn functions of comparable accuracy as the deep convolutional teacher. \n\n"}
{"id": "1603.07646", "contents": "Title: Recursive Neural Language Architecture for Tag Prediction Abstract: We consider the problem of learning distributed representations for tags from\ntheir associated content for the task of tag recommendation. Considering\ntagging information is usually very sparse, effective learning from content and\ntag association is very crucial and challenging task. Recently, various neural\nrepresentation learning models such as WSABIE and its variants show promising\nperformance, mainly due to compact feature representations learned in a\nsemantic space. However, their capacity is limited by a linear compositional\napproach for representing tags as sum of equal parts and hurt their\nperformance. In this work, we propose a neural feedback relevance model for\nlearning tag representations with weighted feature representations. Our\nexperiments on two widely used datasets show significant improvement for\nquality of recommendations over various baselines. \n\n"}
{"id": "1604.04505", "contents": "Title: A short note on extension theorems and their connection to universal\n  consistency in machine learning Abstract: Statistical machine learning plays an important role in modern statistics and\ncomputer science. One main goal of statistical machine learning is to provide\nuniversally consistent algorithms, i.e., the estimator converges in probability\nor in some stronger sense to the Bayes risk or to the Bayes decision function.\nKernel methods based on minimizing the regularized risk over a reproducing\nkernel Hilbert space (RKHS) belong to these statistical machine learning\nmethods. It is in general unknown which kernel yields optimal results for a\nparticular data set or for the unknown probability measure. Hence various\nkernel learning methods were proposed to choose the kernel and therefore also\nits RKHS in a data adaptive manner. Nevertheless, many practitioners often use\nthe classical Gaussian RBF kernel or certain Sobolev kernels with good success.\nThe goal of this short note is to offer one possible theoretical explanation\nfor this empirical fact. \n\n"}
{"id": "1604.04558", "contents": "Title: Accessing accurate documents by mining auxiliary document information Abstract: Earlier techniques of text mining included algorithms like k-means, Naive\nBayes, SVM which classify and cluster the text document for mining relevant\ninformation about the documents. The need for improving the mining techniques\nhas us searching for techniques using the available algorithms. This paper\nproposes one technique which uses the auxiliary information that is present\ninside the text documents to improve the mining. This auxiliary information can\nbe a description to the content. This information can be either useful or\ncompletely useless for mining. The user should assess the worth of the\nauxiliary information before considering this technique for text mining. In\nthis paper, a combination of classical clustering algorithms is used to mine\nthe datasets. The algorithm runs in two stages which carry out mining at\ndifferent levels of abstraction. The clustered documents would then be\nclassified based on the necessary groups. The proposed technique is aimed at\nimproved results of document clustering. \n\n"}
{"id": "1604.06498", "contents": "Title: Stabilized Sparse Online Learning for Sparse Data Abstract: Stochastic gradient descent (SGD) is commonly used for optimization in\nlarge-scale machine learning problems. Langford et al. (2009) introduce a\nsparse online learning method to induce sparsity via truncated gradient. With\nhigh-dimensional sparse data, however, the method suffers from slow convergence\nand high variance due to the heterogeneity in feature sparsity. To mitigate\nthis issue, we introduce a stabilized truncated stochastic gradient descent\nalgorithm. We employ a soft-thresholding scheme on the weight vector where the\nimposed shrinkage is adaptive to the amount of information available in each\nfeature. The variability in the resulted sparse weight vector is further\ncontrolled by stability selection integrated with the informative truncation.\nTo facilitate better convergence, we adopt an annealing strategy on the\ntruncation rate, which leads to a balanced trade-off between exploration and\nexploitation in learning a sparse weight vector. Numerical experiments show\nthat our algorithm compares favorably with the original algorithm in terms of\nprediction accuracy, achieved sparsity and stability. \n\n"}
{"id": "1605.04770", "contents": "Title: Automatic Image Annotation via Label Transfer in the Semantic Space Abstract: Automatic image annotation is among the fundamental problems in computer\nvision and pattern recognition, and it is becoming increasingly important in\norder to develop algorithms that are able to search and browse large-scale\nimage collections. In this paper, we propose a label propagation framework\nbased on Kernel Canonical Correlation Analysis (KCCA), which builds a latent\nsemantic space where correlation of visual and textual features are well\npreserved into a semantic embedding. The proposed approach is robust and can\nwork either when the training set is well annotated by experts, as well as when\nit is noisy such as in the case of user-generated tags in social media. We\nreport extensive results on four popular datasets. Our results show that our\nKCCA-based framework can be applied to several state-of-the-art label transfer\nmethods to obtain significant improvements. Our approach works even with the\nnoisy tags of social users, provided that appropriate denoising is performed.\nExperiments on a large scale setting show that our method can provide some\nbenefits even when the semantic space is estimated on a subset of training\nimages. \n\n"}
{"id": "1605.05721", "contents": "Title: Linearized GMM Kernels and Normalized Random Fourier Features Abstract: The method of \"random Fourier features (RFF)\" has become a popular tool for\napproximating the \"radial basis function (RBF)\" kernel. The variance of RFF is\nactually large. Interestingly, the variance can be substantially reduced by a\nsimple normalization step as we theoretically demonstrate. We name the improved\nscheme as the \"normalized RFF (NRFF)\".\n  We also propose the \"generalized min-max (GMM)\" kernel as a measure of data\nsimilarity. GMM is positive definite as there is an associated hashing method\nnamed \"generalized consistent weighted sampling (GCWS)\" which linearizes this\nnonlinear kernel. We provide an extensive empirical evaluation of the RBF\nkernel and the GMM kernel on more than 50 publicly available datasets. For a\nmajority of the datasets, the (tuning-free) GMM kernel outperforms the\nbest-tuned RBF kernel.\n  We conduct extensive experiments for comparing the linearized RBF kernel\nusing NRFF with the linearized GMM kernel using GCWS. We observe that, to reach\na comparable classification accuracy, GCWS typically requires substantially\nfewer samples than NRFF, even on datasets where the original RBF kernel\noutperforms the original GMM kernel. The empirical success of GCWS (compared to\nNRFF) can also be explained from a theoretical perspective. Firstly, the\nrelative variance (normalized by the squared expectation) of GCWS is\nsubstantially smaller than that of NRFF, except for the very high similarity\nregion (where the variances of both methods are close to zero). Secondly, if we\nmake a model assumption on the data, we can show analytically that GCWS\nexhibits much smaller variance than NRFF for estimating the same object (e.g.,\nthe RBF kernel), except for the very high similarity region. \n\n"}
{"id": "1605.06220", "contents": "Title: Convergence of Contrastive Divergence with Annealed Learning Rate in\n  Exponential Family Abstract: In our recent paper, we showed that in exponential family, contrastive\ndivergence (CD) with fixed learning rate will give asymptotically consistent\nestimates \\cite{wu2016convergence}. In this paper, we establish consistency and\nconvergence rate of CD with annealed learning rate $\\eta_t$. Specifically,\nsuppose CD-$m$ generates the sequence of parameters $\\{\\theta_t\\}_{t \\ge 0}$\nusing an i.i.d. data sample $\\mathbf{X}_1^n \\sim p_{\\theta^*}$ of size $n$,\nthen $\\delta_n(\\mathbf{X}_1^n) = \\limsup_{t \\to \\infty} \\Vert \\sum_{s=t_0}^t\n\\eta_s \\theta_s / \\sum_{s=t_0}^t \\eta_s - \\theta^* \\Vert$ converges in\nprobability to 0 at a rate of $1/\\sqrt[3]{n}$. The number ($m$) of MCMC\ntransitions in CD only affects the coefficient factor of convergence rate. Our\nproof is not a simple extension of the one in \\cite{wu2016convergence}. which\ndepends critically on the fact that $\\{\\theta_t\\}_{t \\ge 0}$ is a homogeneous\nMarkov chain conditional on the observed sample $\\mathbf{X}_1^n$. Under\nannealed learning rate, the homogeneous Markov property is not available and we\nhave to develop an alternative approach based on super-martingales. Experiment\nresults of CD on a fully-visible $2\\times 2$ Boltzmann Machine are provided to\ndemonstrate our theoretical results. \n\n"}
{"id": "1605.06376", "contents": "Title: Fast $\\epsilon$-free Inference of Simulation Models with Bayesian\n  Conditional Density Estimation Abstract: Many statistical models can be simulated forwards but have intractable\nlikelihoods. Approximate Bayesian Computation (ABC) methods are used to infer\nproperties of these models from data. Traditionally these methods approximate\nthe posterior over parameters by conditioning on data being inside an\n$\\epsilon$-ball around the observed data, which is only correct in the limit\n$\\epsilon\\!\\rightarrow\\!0$. Monte Carlo methods can then draw samples from the\napproximate posterior to approximate predictions or error bars on parameters.\nThese algorithms critically slow down as $\\epsilon\\!\\rightarrow\\!0$, and in\npractice draw samples from a broader distribution than the posterior. We\npropose a new approach to likelihood-free inference based on Bayesian\nconditional density estimation. Preliminary inferences based on limited\nsimulation data are used to guide later simulations. In some cases, learning an\naccurate parametric representation of the entire true posterior distribution\nrequires fewer model simulations than Monte Carlo ABC methods need to produce a\nsingle sample from an approximate posterior. \n\n"}
{"id": "1605.07571", "contents": "Title: Sequential Neural Models with Stochastic Layers Abstract: How can we efficiently propagate uncertainty in a latent state representation\nwith recurrent neural networks? This paper introduces stochastic recurrent\nneural networks which glue a deterministic recurrent neural network and a state\nspace model together to form a stochastic and sequential neural generative\nmodel. The clear separation of deterministic and stochastic layers allows a\nstructured variational inference network to track the factorization of the\nmodel's posterior distribution. By retaining both the nonlinear recursive\nstructure of a recurrent neural network and averaging over the uncertainty in a\nlatent path, like a state space model, we improve the state of the art results\non the Blizzard and TIMIT speech modeling data sets by a large margin, while\nachieving comparable performances to competing methods on polyphonic music\nmodeling. \n\n"}
{"id": "1605.07725", "contents": "Title: Adversarial Training Methods for Semi-Supervised Text Classification Abstract: Adversarial training provides a means of regularizing supervised learning\nalgorithms while virtual adversarial training is able to extend supervised\nlearning algorithms to the semi-supervised setting. However, both methods\nrequire making small perturbations to numerous entries of the input vector,\nwhich is inappropriate for sparse high-dimensional inputs such as one-hot word\nrepresentations. We extend adversarial and virtual adversarial training to the\ntext domain by applying perturbations to the word embeddings in a recurrent\nneural network rather than to the original input itself. The proposed method\nachieves state of the art results on multiple benchmark semi-supervised and\npurely supervised tasks. We provide visualizations and analysis showing that\nthe learned word embeddings have improved in quality and that while training,\nthe model is less prone to overfitting. Code is available at\nhttps://github.com/tensorflow/models/tree/master/research/adversarial_text. \n\n"}
{"id": "1605.07891", "contents": "Title: Query Expansion with Locally-Trained Word Embeddings Abstract: Continuous space word embeddings have received a great deal of attention in\nthe natural language processing and machine learning communities for their\nability to model term similarity and other relationships. We study the use of\nterm relatedness in the context of query expansion for ad hoc information\nretrieval. We demonstrate that word embeddings such as word2vec and GloVe, when\ntrained globally, underperform corpus and query specific embeddings for\nretrieval tasks. These results suggest that other tasks benefiting from global\nembeddings may also benefit from local embeddings. \n\n"}
{"id": "1606.00577", "contents": "Title: Source-LDA: Enhancing probabilistic topic models using prior knowledge\n  sources Abstract: A popular approach to topic modeling involves extracting co-occurring n-grams\nof a corpus into semantic themes. The set of n-grams in a theme represents an\nunderlying topic, but most topic modeling approaches are not able to label\nthese sets of words with a single n-gram. Such labels are useful for topic\nidentification in summarization systems. This paper introduces a novel approach\nto labeling a group of n-grams comprising an individual topic. The approach\ntaken is to complement the existing topic distributions over words with a known\ndistribution based on a predefined set of topics. This is done by integrating\nexisting labeled knowledge sources representing known potential topics into the\nprobabilistic topic model. These knowledge sources are translated into a\ndistribution and used to set the hyperparameters of the Dirichlet generated\ndistribution over words. In the inference these modified distributions guide\nthe convergence of the latent topics to conform with the complementary\ndistributions. This approach ensures that the topic inference process is\nconsistent with existing knowledge. The label assignment from the complementary\nknowledge sources are then transferred to the latent topics of the corpus. The\nresults show both accurate label assignment to topics as well as improved topic\ngeneration than those obtained using various labeling approaches based off\nLatent Dirichlet allocation (LDA). \n\n"}
{"id": "1606.00979", "contents": "Title: Question Answering over Knowledge Base with Neural Attention Combining\n  Global Knowledge Information Abstract: With the rapid growth of knowledge bases (KBs) on the web, how to take full\nadvantage of them becomes increasingly important. Knowledge base-based question\nanswering (KB-QA) is one of the most promising approaches to access the\nsubstantial knowledge. Meantime, as the neural network-based (NN-based) methods\ndevelop, NN-based KB-QA has already achieved impressive results. However,\nprevious work did not put emphasis on question representation, and the question\nis converted into a fixed vector regardless of its candidate answers. This\nsimple representation strategy is unable to express the proper information of\nthe question. Hence, we present a neural attention-based model to represent the\nquestions dynamically according to the different focuses of various candidate\nanswer aspects. In addition, we leverage the global knowledge inside the\nunderlying KB, aiming at integrating the rich KB information into the\nrepresentation of the answers. And it also alleviates the out of vocabulary\n(OOV) problem, which helps the attention model to represent the question more\nprecisely. The experimental results on WEBQUESTIONS demonstrate the\neffectiveness of the proposed approach. \n\n"}
{"id": "1606.01855", "contents": "Title: Bayesian Poisson Tucker Decomposition for Learning the Structure of\n  International Relations Abstract: We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling\ncountry--country interaction event data. These data consist of interaction\nevents of the form \"country $i$ took action $a$ toward country $j$ at time\n$t$.\" BPTD discovers overlapping country--community memberships, including the\nnumber of latent communities. In addition, it discovers directed\ncommunity--community interaction networks that are specific to \"topics\" of\naction types and temporal \"regimes.\" We show that BPTD yields an efficient MCMC\ninference algorithm and achieves better predictive performance than related\nmodels. We also demonstrate that it discovers interpretable latent structure\nthat agrees with our knowledge of international relations. \n\n"}
{"id": "1606.02276", "contents": "Title: Multilingual Visual Sentiment Concept Matching Abstract: The impact of culture in visual emotion perception has recently captured the\nattention of multimedia research. In this study, we pro- vide powerful\ncomputational linguistics tools to explore, retrieve and browse a dataset of\n16K multilingual affective visual concepts and 7.3M Flickr images. First, we\ndesign an effective crowdsourc- ing experiment to collect human judgements of\nsentiment connected to the visual concepts. We then use word embeddings to\nrepre- sent these concepts in a low dimensional vector space, allowing us to\nexpand the meaning around concepts, and thus enabling insight about\ncommonalities and differences among different languages. We compare a variety\nof concept representations through a novel evaluation task based on the notion\nof visual semantic relatedness. Based on these representations, we design\nclustering schemes to group multilingual visual concepts, and evaluate them\nwith novel metrics based on the crowdsourced sentiment annotations as well as\nvisual semantic relatedness. The proposed clustering framework enables us to\nanalyze the full multilingual dataset in-depth and also show an application on\na facial data subset, exploring cultural in- sights of portrait-related\naffective visual concepts. \n\n"}
{"id": "1606.03044", "contents": "Title: The \"Horse'' Inside: Seeking Causes Behind the Behaviours of Music\n  Content Analysis Systems Abstract: Building systems that possess the sensitivity and intelligence to identify\nand describe high-level attributes in music audio signals continues to be an\nelusive goal, but one that surely has broad and deep implications for a wide\nvariety of applications. Hundreds of papers have so far been published toward\nthis goal, and great progress appears to have been made. Some systems produce\nremarkable accuracies at recognising high-level semantic concepts, such as\nmusic style, genre and mood. However, it might be that these numbers do not\nmean what they seem. In this paper, we take a state-of-the-art music content\nanalysis system and investigate what causes it to achieve exceptionally high\nperformance in a benchmark music audio dataset. We dissect the system to\nunderstand its operation, determine its sensitivities and limitations, and\npredict the kinds of knowledge it could and could not possess about music. We\nperform a series of experiments to illuminate what the system has actually\nlearned to do, and to what extent it is performing the intended music listening\ntask. Our results demonstrate how the initial manifestation of music\nintelligence in this state-of-the-art can be deceptive. Our work provides\nconstructive directions toward developing music content analysis systems that\ncan address the music information and creation needs of real-world users. \n\n"}
{"id": "1606.03490", "contents": "Title: The Mythos of Model Interpretability Abstract: Supervised machine learning models boast remarkable predictive capabilities.\nBut can you trust your model? Will it work in deployment? What else can it tell\nyou about the world? We want models to be not only good, but interpretable. And\nyet the task of interpretation appears underspecified. Papers provide diverse\nand sometimes non-overlapping motivations for interpretability, and offer\nmyriad notions of what attributes render models interpretable. Despite this\nambiguity, many papers proclaim interpretability axiomatically, absent further\nexplanation. In this paper, we seek to refine the discourse on\ninterpretability. First, we examine the motivations underlying interest in\ninterpretability, finding them to be diverse and occasionally discordant. Then,\nwe address model properties and techniques thought to confer interpretability,\nidentifying transparency to humans and post-hoc explanations as competing\nnotions. Throughout, we discuss the feasibility and desirability of different\nnotions, and question the oft-made assertions that linear models are\ninterpretable and that deep neural networks are not. \n\n"}
{"id": "1606.05325", "contents": "Title: ACDC: $\\alpha$-Carving Decision Chain for Risk Stratification Abstract: In many healthcare settings, intuitive decision rules for risk stratification\ncan help effective hospital resource allocation. This paper introduces a novel\nvariant of decision tree algorithms that produces a chain of decisions, not a\ngeneral tree. Our algorithm, $\\alpha$-Carving Decision Chain (ACDC),\nsequentially carves out \"pure\" subsets of the majority class examples. The\nresulting chain of decision rules yields a pure subset of the minority class\nexamples. Our approach is particularly effective in exploring large and\nclass-imbalanced health datasets. Moreover, ACDC provides an interactive\ninterpretation in conjunction with visual performance metrics such as Receiver\nOperating Characteristics curve and Lift chart. \n\n"}
{"id": "1606.05642", "contents": "Title: Balancing New Against Old Information: The Role of Surprise in Learning Abstract: Surprise describes a range of phenomena from unexpected events to behavioral\nresponses. We propose a measure of surprise and use it for surprise-driven\nlearning. Our surprise measure takes into account data likelihood as well as\nthe degree of commitment to a belief via the entropy of the belief\ndistribution. We find that surprise-minimizing learning dynamically adjusts the\nbalance between new and old information without the need of knowledge about the\ntemporal statistics of the environment. We apply our framework to a dynamic\ndecision-making task and a maze exploration task. Our surprise minimizing\nframework is suitable for learning in complex environments, even if the\nenvironment undergoes gradual or sudden changes and could eventually provide a\nframework to study the behavior of humans and animals encountering surprising\nevents. \n\n"}
{"id": "1606.06366", "contents": "Title: FSMJ: Feature Selection with Maximum Jensen-Shannon Divergence for Text\n  Categorization Abstract: In this paper, we present a new wrapper feature selection approach based on\nJensen-Shannon (JS) divergence, termed feature selection with maximum\nJS-divergence (FSMJ), for text categorization. Unlike most existing feature\nselection approaches, the proposed FSMJ approach is based on real-valued\nfeatures which provide more information for discrimination than binary-valued\nfeatures used in conventional approaches. We show that the FSMJ is a greedy\napproach and the JS-divergence monotonically increases when more features are\nselected. We conduct several experiments on real-life data sets, compared with\nthe state-of-the-art feature selection approaches for text categorization. The\nsuperior performance of the proposed FSMJ approach demonstrates its\neffectiveness and further indicates its wide potential applications on data\nmining. \n\n"}
{"id": "1606.07103", "contents": "Title: Deep Feature Fusion Network for Answer Quality Prediction in Community\n  Question Answering Abstract: Community Question Answering (cQA) forums have become a popular medium for\nsoliciting direct answers to specific questions of users from experts or other\nexperienced users on a given topic. However, for a given question, users\nsometimes have to sift through a large number of low-quality or irrelevant\nanswers to find out the answer which satisfies their information need. To\nalleviate this, the problem of Answer Quality Prediction (AQP) aims to predict\nthe quality of an answer posted in response to a forum question. Current AQP\nsystems either learn models using - a) various hand-crafted features (HCF) or\nb) use deep learning (DL) techniques which automatically learn the required\nfeature representations.\n  In this paper, we propose a novel approach for AQP known as - \"Deep Feature\nFusion Network (DFFN)\" which leverages the advantages of both hand-crafted\nfeatures and deep learning based systems. Given a question-answer pair along\nwith its metadata, DFFN independently - a) learns deep features using a\nConvolutional Neural Network (CNN) and b) computes hand-crafted features using\nvarious external resources and then combines them using a deep neural network\ntrained to predict the final answer quality. DFFN achieves state-of-the-art\nperformance on the standard SemEval-2015 and SemEval-2016 benchmark datasets\nand outperforms baseline approaches which individually employ either HCF or DL\nbased techniques alone. \n\n"}
{"id": "1606.07287", "contents": "Title: Picture It In Your Mind: Generating High Level Visual Representations\n  From Textual Descriptions Abstract: In this paper we tackle the problem of image search when the query is a short\ntextual description of the image the user is looking for. We choose to\nimplement the actual search process as a similarity search in a visual feature\nspace, by learning to translate a textual query into a visual representation.\nSearching in the visual feature space has the advantage that any update to the\ntranslation model does not require to reprocess the, typically huge, image\ncollection on which the search is performed. We propose Text2Vis, a neural\nnetwork that generates a visual representation, in the visual feature space of\nthe fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis\noptimizes two loss functions, using a stochastic loss-selection method. A\nvisual-focused loss is aimed at learning the actual text-to-visual feature\nmapping, while a text-focused loss is aimed at modeling the higher-level\nsemantic concepts expressed in language and countering the overfit on\nnon-relevant visual components of the visual loss. We report preliminary\nresults on the MS-COCO dataset. \n\n"}
{"id": "1606.07565", "contents": "Title: Adaptability of Neural Networks on Varying Granularity IR Tasks Abstract: Recent work in Information Retrieval (IR) using Deep Learning models has\nyielded state of the art results on a variety of IR tasks. Deep neural networks\n(DNN) are capable of learning ideal representations of data during the training\nprocess, removing the need for independently extracting features. However, the\nstructures of these DNNs are often tailored to perform on specific datasets. In\naddition, IR tasks deal with text at varying levels of granularity from single\nfactoids to documents containing thousands of words. In this paper, we examine\nthe role of the granularity on the performance of common state of the art DNN\nstructures in IR. \n\n"}
{"id": "1606.07608", "contents": "Title: Using Word Embeddings for Automatic Query Expansion Abstract: In this paper a framework for Automatic Query Expansion (AQE) is proposed\nusing distributed neural language model word2vec. Using semantic and contextual\nrelation in a distributed and unsupervised framework, word2vec learns a low\ndimensional embedding for each vocabulary entry. Using such a framework, we\ndevise a query expansion technique, where related terms to a query are obtained\nby K-nearest neighbor approach. We explore the performance of the AQE methods,\nwith and without feedback query expansion, and a variant of simple K-nearest\nneighbor in the proposed framework. Experiments on standard TREC ad-hoc data\n(Disk 4, 5 with query sets 301-450, 601-700) and web data (WT10G data with\nquery set 451-550) shows significant improvement over standard term-overlapping\nbased retrieval methods. However the proposed method fails to achieve\ncomparable performance with statistical co-occurrence based feedback method\nsuch as RM3. We have also found that the word2vec based query expansion methods\nperform similarly with and without any feedback information. \n\n"}
{"id": "1606.08658", "contents": "Title: Clustering-Based Relational Unsupervised Representation Learning with an\n  Explicit Distributed Representation Abstract: The goal of unsupervised representation learning is to extract a new\nrepresentation of data, such that solving many different tasks becomes easier.\nExisting methods typically focus on vectorized data and offer little support\nfor relational data, which additionally describe relationships among instances.\nIn this work we introduce an approach for relational unsupervised\nrepresentation learning. Viewing a relational dataset as a hypergraph, new\nfeatures are obtained by clustering vertices and hyperedges. To find a\nrepresentation suited for many relational learning tasks, a wide range of\nsimilarities between relational objects is considered, e.g. feature and\nstructural similarities. We experimentally evaluate the proposed approach and\nshow that models learned on such latent representations perform better, have\nlower complexity, and outperform the existing approaches on classification\ntasks. \n\n"}
{"id": "1607.00570", "contents": "Title: Representation learning for very short texts using weighted word\n  embedding aggregation Abstract: Short text messages such as tweets are very noisy and sparse in their use of\nvocabulary. Traditional textual representations, such as tf-idf, have\ndifficulty grasping the semantic meaning of such texts, which is important in\napplications such as event detection, opinion mining, news recommendation, etc.\nWe constructed a method based on semantic word embeddings and frequency\ninformation to arrive at low-dimensional representations for short texts\ndesigned to capture semantic similarity. For this purpose we designed a\nweight-based model and a learning procedure based on a novel median-based loss\nfunction. This paper discusses the details of our model and the optimization\nmethods, together with the experimental results on both Wikipedia and Twitter\ndata. We find that our method outperforms the baseline approaches in the\nexperiments, and that it generalizes well on different word embeddings without\nretraining. Our method is therefore capable of retaining most of the semantic\ninformation in the text, and is applicable out-of-the-box. \n\n"}
{"id": "1607.01668", "contents": "Title: Tensor Decomposition for Signal Processing and Machine Learning Abstract: Tensors or {\\em multi-way arrays} are functions of three or more indices\n$(i,j,k,\\cdots)$ -- similar to matrices (two-way arrays), which are functions\nof two indices $(r,c)$ for (row,column). Tensors have a rich history,\nstretching over almost a century, and touching upon numerous disciplines; but\nthey have only recently become ubiquitous in signal and data analytics at the\nconfluence of signal processing, statistics, data mining and machine learning.\nThis overview article aims to provide a good starting point for researchers and\npractitioners interested in learning about and working with tensors. As such,\nit focuses on fundamentals and motivation (using various application examples),\naiming to strike an appropriate balance of breadth {\\em and depth} that will\nenable someone having taken first graduate courses in matrix algebra and\nprobability to get started doing research and/or developing tensor algorithms\nand software. Some background in applied optimization is useful but not\nstrictly required. The material covered includes tensor rank and rank\ndecomposition; basic tensor factorization models and their relationships and\nproperties (including fairly good coverage of identifiability); broad coverage\nof algorithms ranging from alternating optimization to stochastic gradient;\nstatistical performance analysis; and applications ranging from source\nseparation to collaborative filtering, mixture and topic modeling,\nclassification, and multilinear subspace learning. \n\n"}
{"id": "1607.02024", "contents": "Title: Mini-Batch Spectral Clustering Abstract: The cost of computing the spectrum of Laplacian matrices hinders the\napplication of spectral clustering to large data sets. While approximations\nrecover computational tractability, they can potentially affect clustering\nperformance. This paper proposes a practical approach to learn spectral\nclustering based on adaptive stochastic gradient optimization. Crucially, the\nproposed approach recovers the exact spectrum of Laplacian matrices in the\nlimit of the iterations, and the cost of each iteration is linear in the number\nof samples. Extensive experimental validation on data sets with up to half a\nmillion samples demonstrate its scalability and its ability to outperform\nstate-of-the-art approximate methods to learn spectral clustering for a given\ncomputational budget. \n\n"}
{"id": "1607.02501", "contents": "Title: Actionable and Political Text Classification using Word Embeddings and\n  LSTM Abstract: In this work, we apply word embeddings and neural networks with Long\nShort-Term Memory (LSTM) to text classification problems, where the\nclassification criteria are decided by the context of the application. We\nexamine two applications in particular. The first is that of Actionability,\nwhere we build models to classify social media messages from customers of\nservice providers as Actionable or Non-Actionable. We build models for over 30\ndifferent languages for actionability, and most of the models achieve accuracy\naround 85%, with some reaching over 90% accuracy. We also show that using LSTM\nneural networks with word embeddings vastly outperform traditional techniques.\nSecond, we explore classification of messages with respect to political\nleaning, where social media messages are classified as Democratic or\nRepublican. The model is able to classify messages with a high accuracy of\n87.57%. As part of our experiments, we vary different hyperparameters of the\nneural networks, and report the effect of such variation on the accuracy. These\nactionability models have been deployed to production and help company agents\nprovide customer support by prioritizing which messages to respond to. The\nmodel for political leaning has been opened and made available for wider use. \n\n"}
{"id": "1607.04903", "contents": "Title: Learning Unitary Operators with Help From u(n) Abstract: A major challenge in the training of recurrent neural networks is the\nso-called vanishing or exploding gradient problem. The use of a norm-preserving\ntransition operator can address this issue, but parametrization is challenging.\nIn this work we focus on unitary operators and describe a parametrization using\nthe Lie algebra $\\mathfrak{u}(n)$ associated with the Lie group $U(n)$ of $n\n\\times n$ unitary matrices. The exponential map provides a correspondence\nbetween these spaces, and allows us to define a unitary matrix using $n^2$ real\ncoefficients relative to a basis of the Lie algebra. The parametrization is\nclosed under additive updates of these coefficients, and thus provides a simple\nspace in which to do gradient descent. We demonstrate the effectiveness of this\nparametrization on the problem of learning arbitrary unitary operators,\ncomparing to several baselines and outperforming a recently-proposed\nlower-dimensional parametrization. We additionally use our parametrization to\ngeneralize a recently-proposed unitary recurrent neural network to arbitrary\nunitary matrices, using it to solve standard long-memory tasks. \n\n"}
{"id": "1607.05002", "contents": "Title: Geometric Mean Metric Learning Abstract: We revisit the task of learning a Euclidean metric from data. We approach\nthis problem from first principles and formulate it as a surprisingly simple\noptimization problem. Indeed, our formulation even admits a closed form\nsolution. This solution possesses several very attractive properties: (i) an\ninnate geometric appeal through the Riemannian geometry of positive definite\nmatrices; (ii) ease of interpretability; and (iii) computational speed several\norders of magnitude faster than the widely used LMNN and ITML methods.\nFurthermore, on standard benchmark datasets, our closed-form solution\nconsistently attains higher classification accuracy. \n\n"}
{"id": "1607.05746", "contents": "Title: Bayesian Non-Exhaustive Classification A Case Study: Online Name\n  Disambiguation using Temporal Record Streams Abstract: The name entity disambiguation task aims to partition the records of multiple\nreal-life persons so that each partition contains records pertaining to a\nunique person. Most of the existing solutions for this task operate in a batch\nmode, where all records to be disambiguated are initially available to the\nalgorithm. However, more realistic settings require that the name\ndisambiguation task be performed in an online fashion, in addition to, being\nable to identify records of new ambiguous entities having no preexisting\nrecords. In this work, we propose a Bayesian non-exhaustive classification\nframework for solving online name disambiguation task. Our proposed method uses\na Dirichlet process prior with a Normal * Normal * Inverse Wishart data model\nwhich enables identification of new ambiguous entities who have no records in\nthe training data. For online classification, we use one sweep Gibbs sampler\nwhich is very efficient and effective. As a case study we consider\nbibliographic data in a temporal stream format and disambiguate authors by\npartitioning their papers into homogeneous groups. Our experimental results\ndemonstrate that the proposed method is better than existing methods for\nperforming online name disambiguation task. \n\n"}
{"id": "1607.06450", "contents": "Title: Layer Normalization Abstract: Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques. \n\n"}
{"id": "1608.00159", "contents": "Title: Learning Tree-Structured Detection Cascades for Heterogeneous Networks\n  of Embedded Devices Abstract: In this paper, we present a new approach to learning cascaded classifiers for\nuse in computing environments that involve networks of heterogeneous and\nresource-constrained, low-power embedded compute and sensing nodes. We present\na generalization of the classical linear detection cascade to the case of\ntree-structured cascades where different branches of the tree execute on\ndifferent physical compute nodes in the network. Different nodes have access to\ndifferent features, as well as access to potentially different computation and\nenergy resources. We concentrate on the problem of jointly learning the\nparameters for all of the classifiers in the cascade given a fixed cascade\narchitecture and a known set of costs required to carry out the computation at\neach node.To accomplish the objective of joint learning of all detectors, we\npropose a novel approach to combining classifier outputs during training that\nbetter matches the hard cascade setting in which the learned system will be\ndeployed. This work is motivated by research in the area of mobile health where\nenergy efficient real time detectors integrating information from multiple\nwireless on-body sensors and a smart phone are needed for real-time monitoring\nand delivering just- in-time adaptive interventions. We apply our framework to\ntwo activity recognition datasets as well as the problem of cigarette smoking\ndetection from a combination of wrist-worn actigraphy data and respiration\nchest band data. \n\n"}
{"id": "1608.00528", "contents": "Title: Impartial Predictive Modeling and the Use of Proxy Variables Abstract: Fairness aware data mining (FADM) aims to prevent algorithms from\ndiscriminating against protected groups. The literature has come to an impasse\nas to what constitutes explainable variability as opposed to discrimination.\nThis distinction hinges on a rigorous understanding of the role of proxy\nvariables; i.e., those variables which are associated both the protected\nfeature and the outcome of interest. We demonstrate that fairness is achieved\nby ensuring impartiality with respect to sensitive characteristics and provide\na framework for impartiality by accounting for different perspectives on the\ndata generating process. In particular, fairness can only be precisely defined\nin a full-data scenario in which all covariates are observed. We then analyze\nhow these models may be conservatively estimated via regression in partial-data\nsettings. Decomposing the regression estimates provides insights into\npreviously unexplored distinctions between explainable variability and\ndiscrimination that illuminate the use of proxy variables in fairness aware\ndata mining. \n\n"}
{"id": "1608.00778", "contents": "Title: Exponential Family Embeddings Abstract: Word embeddings are a powerful approach for capturing semantic similarity\namong terms in a vocabulary. In this paper, we develop exponential family\nembeddings, a class of methods that extends the idea of word embeddings to\nother types of high-dimensional data. As examples, we studied neural data with\nreal-valued observations, count data from a market basket analysis, and ratings\ndata from a movie recommendation system. The main idea is to model each\nobservation conditioned on a set of other observations. This set is called the\ncontext, and the way the context is defined is a modeling choice that depends\non the problem. In language the context is the surrounding words; in\nneuroscience the context is close-by neurons; in market basket data the context\nis other items in the shopping cart. Each type of embedding model defines the\ncontext, the exponential family of conditional distributions, and how the\nlatent embedding vectors are shared across data. We infer the embeddings with a\nscalable algorithm based on stochastic gradient descent. On all three\napplications - neural activity of zebrafish, users' shopping behavior, and\nmovie ratings - we found exponential family embedding models to be more\neffective than other types of dimension reduction. They better reconstruct\nheld-out data and find interesting qualitative structure. \n\n"}
{"id": "1608.00874", "contents": "Title: Modelling and computation using NCoRM mixtures for density regression Abstract: Normalized compound random measures are flexible nonparametric priors for\nrelated distributions. We consider building general nonparametric regression\nmodels using normalized compound random measure mixture models. Posterior\ninference is made using a novel pseudo-marginal Metropolis-Hastings sampler for\nnormalized compound random measure mixture models. The algorithm makes use of a\nnew general approach to the unbiased estimation of Laplace functionals of\ncompound random measures (which includes completely random measures as a\nspecial case). The approach is illustrated on problems of density regression. \n\n"}
{"id": "1608.01976", "contents": "Title: Kernel Ridge Regression via Partitioning Abstract: In this paper, we investigate a divide and conquer approach to Kernel Ridge\nRegression (KRR). Given n samples, the division step involves separating the\npoints based on some underlying disjoint partition of the input space (possibly\nvia clustering), and then computing a KRR estimate for each partition. The\nconquering step is simple: for each partition, we only consider its own local\nestimate for prediction. We establish conditions under which we can give\ngeneralization bounds for this estimator, as well as achieve optimal minimax\nrates. We also show that the approximation error component of the\ngeneralization error is lesser than when a single KRR estimate is fit on the\ndata: thus providing both statistical and computational advantages over a\nsingle KRR estimate over the entire data (or an averaging over random\npartitions as in other recent work, [30]). Lastly, we provide experimental\nvalidation for our proposed estimator and our assumptions. \n\n"}
{"id": "1608.02861", "contents": "Title: Classification with the pot-pot plot Abstract: We propose a procedure for supervised classification that is based on\npotential functions. The potential of a class is defined as a kernel density\nestimate multiplied by the class's prior probability. The method transforms the\ndata to a potential-potential (pot-pot) plot, where each data point is mapped\nto a vector of potentials. Separation of the classes, as well as classification\nof new data points, is performed on this plot. For this, either the\n$\\alpha$-procedure ($\\alpha$-P) or $k$-nearest neighbors ($k$-NN) are employed.\nFor data that are generated from continuous distributions, these classifiers\nprove to be strongly Bayes-consistent. The potentials depend on the kernel and\nits bandwidth used in the density estimate. We investigate several variants of\nbandwidth selection, including joint and separate pre-scaling and a bandwidth\nregression approach. The new method is applied to benchmark data from the\nliterature, including simulated data sets as well as 50 sets of real data. It\ncompares favorably to known classification methods such as LDA, QDA, max kernel\ndensity estimates, $k$-NN, and $DD$-plot classification using depth functions. \n\n"}
{"id": "1608.03905", "contents": "Title: Using Centroids of Word Embeddings and Word Mover's Distance for\n  Biomedical Document Retrieval in Question Answering Abstract: We propose a document retrieval method for question answering that represents\ndocuments and questions as weighted centroids of word embeddings and reranks\nthe retrieved documents with a relaxation of Word Mover's Distance. Using\nbiomedical questions and documents from BIOASQ, we show that our method is\ncompetitive with PUBMED. With a top-k approximation, our method is fast, and\neasily portable to other domains and languages. \n\n"}
{"id": "1608.04207", "contents": "Title: Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction\n  Tasks Abstract: There is a lot of research interest in encoding variable length sentences\ninto fixed length vectors, in a way that preserves the sentence meanings. Two\ncommon methods include representations based on averaging word vectors, and\nrepresentations based on the hidden states of recurrent neural networks such as\nLSTMs. The sentence vectors are used as features for subsequent machine\nlearning tasks or for pre-training in the context of deep learning. However,\nnot much is known about the properties that are encoded in these sentence\nrepresentations and about the language information they capture. We propose a\nframework that facilitates better understanding of the encoded representations.\nWe define prediction tasks around isolated aspects of sentence structure\n(namely sentence length, word content, and word order), and score\nrepresentations by the ability to train a classifier to solve each prediction\ntask when using the representation as input. We demonstrate the potential\ncontribution of the approach by analyzing different sentence representation\nmechanisms. The analysis sheds light on the relative strengths of different\nsentence embedding methods with respect to these low level prediction tasks,\nand on the effect of the encoded vector's dimensionality on the resulting\nrepresentations. \n\n"}
{"id": "1608.06253", "contents": "Title: Multi-Dueling Bandits and Their Application to Online Ranker Evaluation Abstract: New ranking algorithms are continually being developed and refined,\nnecessitating the development of efficient methods for evaluating these\nrankers. Online ranker evaluation focuses on the challenge of efficiently\ndetermining, from implicit user feedback, which ranker out of a finite set of\nrankers is the best. Online ranker evaluation can be modeled by dueling ban-\ndits, a mathematical model for online learning under limited feedback from\npairwise comparisons. Comparisons of pairs of rankers is performed by\ninterleaving their result sets and examining which documents users click on.\nThe dueling bandits model addresses the key issue of which pair of rankers to\ncompare at each iteration, thereby providing a solution to the\nexploration-exploitation trade-off. Recently, methods for simultaneously\ncomparing more than two rankers have been developed. However, the question of\nwhich rankers to compare at each iteration was left open. We address this\nquestion by proposing a generalization of the dueling bandits model that uses\nsimultaneous comparisons of an unrestricted number of rankers. We evaluate our\nalgorithm on synthetic data and several standard large-scale online ranker\nevaluation datasets. Our experimental results show that the algorithm yields\norders of magnitude improvement in performance compared to stateof- the-art\ndueling bandit algorithms. \n\n"}
{"id": "1608.07253", "contents": "Title: Learning Latent Vector Spaces for Product Search Abstract: We introduce a novel latent vector space model that jointly learns the latent\nrepresentations of words, e-commerce products and a mapping between the two\nwithout the need for explicit annotations. The power of the model lies in its\nability to directly model the discriminative relation between products and a\nparticular word. We compare our method to existing latent vector space models\n(LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank\nsetting. Our latent vector space model achieves its enhanced performance as it\nlearns better product representations. Furthermore, the mapping from words to\nproducts and the representations of words benefit directly from the errors\npropagated back from the product representations during parameter estimation.\nWe provide an in-depth analysis of the performance of our model and analyze the\nstructure of the learned representations. \n\n"}
{"id": "1608.08176", "contents": "Title: What is Wrong with Topic Modeling? (and How to Fix it Using Search-based\n  Software Engineering) Abstract: Context: Topic modeling finds human-readable structures in unstructured\ntextual data. A widely used topic modeler is Latent Dirichlet allocation. When\nrun on different datasets, LDA suffers from \"order effects\" i.e. different\ntopics are generated if the order of training data is shuffled. Such order\neffects introduce a systematic error for any study. This error can relate to\nmisleading results;specifically, inaccurate topic descriptions and a reduction\nin the efficacy of text mining classification results. Objective: To provide a\nmethod in which distributions generated by LDA are more stable and can be used\nfor further analysis. Method: We use LDADE, a search-based software engineering\ntool that tunes LDA's parameters using DE (Differential Evolution). LDADE is\nevaluated on data from a programmer information exchange site (Stackoverflow),\ntitle and abstract text of thousands ofSoftware Engineering (SE) papers, and\nsoftware defect reports from NASA. Results were collected across different\nimplementations of LDA (Python+Scikit-Learn, Scala+Spark); across different\nplatforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using\nGibbs sampling). Results were scored via topic stability and text mining\nclassification accuracy. Results: In all treatments: (i) standard LDA exhibits\nvery large topic instability; (ii) LDADE's tunings dramatically reduce cluster\ninstability; (iii) LDADE also leads to improved performances for supervised as\nwell as unsupervised learning. Conclusion: Due to topic instability, using\nstandard LDA with its \"off-the-shelf\" settings should now be depreciated. Also,\nin future, we should require SE papers that use LDA to test and (if needed)\nmitigate LDA topic instability. Finally, LDADE is a candidate technology for\neffectively and efficiently reducing that instability. \n\n"}
{"id": "1608.08182", "contents": "Title: Data Poisoning Attacks on Factorization-Based Collaborative Filtering Abstract: Recommendation and collaborative filtering systems are important in modern\ninformation and e-commerce applications. As these systems are becoming\nincreasingly popular in the industry, their outputs could affect business\ndecision making, introducing incentives for an adversarial party to compromise\nthe availability or integrity of such systems. We introduce a data poisoning\nattack on collaborative filtering systems. We demonstrate how a powerful\nattacker with full knowledge of the learner can generate malicious data so as\nto maximize his/her malicious objectives, while at the same time mimicking\nnormal user behavior to avoid being detected. While the complete knowledge\nassumption seems extreme, it enables a robust assessment of the vulnerability\nof collaborative filtering schemes to highly motivated attacks. We present\nefficient solutions for two popular factorization-based collaborative filtering\nalgorithms: the \\emph{alternative minimization} formulation and the\n\\emph{nuclear norm minimization} method. Finally, we test the effectiveness of\nour proposed algorithms on real-world data and discuss potential defensive\nstrategies. \n\n"}
{"id": "1608.08852", "contents": "Title: A Mathematical Framework for Feature Selection from Real-World Data with\n  Non-Linear Observations Abstract: In this paper, we study the challenge of feature selection based on a\nrelatively small collection of sample pairs $\\{(x_i, y_i)\\}_{1 \\leq i \\leq m}$.\nThe observations $y_i \\in \\mathbb{R}$ are thereby supposed to follow a noisy\nsingle-index model, depending on a certain set of signal variables. A major\ndifficulty is that these variables usually cannot be observed directly, but\nrather arise as hidden factors in the actual data vectors $x_i \\in\n\\mathbb{R}^d$ (feature variables). We will prove that a successful variable\nselection is still possible in this setup, even when the applied estimator does\nnot have any knowledge of the underlying model parameters and only takes the\n'raw' samples $\\{(x_i, y_i)\\}_{1 \\leq i \\leq m}$ as input. The model\nassumptions of our results will be fairly general, allowing for non-linear\nobservations, arbitrary convex signal structures as well as strictly convex\nloss functions. This is particularly appealing for practical purposes, since in\nmany applications, already standard methods, e.g., the Lasso or logistic\nregression, yield surprisingly good outcomes. Apart from a general discussion\nof the practical scope of our theoretical findings, we will also derive a\nrigorous guarantee for a specific real-world problem, namely sparse feature\nextraction from (proteomics-based) mass spectrometry data. \n\n"}
{"id": "1609.00978", "contents": "Title: Local Maxima in the Likelihood of Gaussian Mixture Models: Structural\n  Results and Algorithmic Consequences Abstract: We provide two fundamental results on the population (infinite-sample)\nlikelihood function of Gaussian mixture models with $M \\geq 3$ components. Our\nfirst main result shows that the population likelihood function has bad local\nmaxima even in the special case of equally-weighted mixtures of well-separated\nand spherical Gaussians. We prove that the log-likelihood value of these bad\nlocal maxima can be arbitrarily worse than that of any global optimum, thereby\nresolving an open question of Srebro (2007). Our second main result shows that\nthe EM algorithm (or a first-order variant of it) with random initialization\nwill converge to bad critical points with probability at least\n$1-e^{-\\Omega(M)}$. We further establish that a first-order variant of EM will\nnot converge to strict saddle points almost surely, indicating that the poor\nperformance of the first-order method can be attributed to the existence of bad\nlocal maxima rather than bad saddle points. Overall, our results highlight the\nnecessity of careful initialization when using the EM algorithm in practice,\neven when applied in highly favorable settings. \n\n"}
{"id": "1609.01872", "contents": "Title: Chaining Bounds for Empirical Risk Minimization Abstract: This paper extends the standard chaining technique to prove excess risk upper\nbounds for empirical risk minimization with random design settings even if the\nmagnitude of the noise and the estimates is unbounded. The bound applies to\nmany loss functions besides the squared loss, and scales only with the\nsub-Gaussian or subexponential parameters without further statistical\nassumptions such as the bounded kurtosis condition over the hypothesis class. A\ndetailed analysis is provided for slope constrained and penalized linear least\nsquares regression with a sub-Gaussian setting, which often proves tight sample\ncomplexity bounds up to logartihmic factors. \n\n"}
{"id": "1609.02171", "contents": "Title: The Effect of Class Imbalance and Order on Crowdsourced Relevance\n  Judgments Abstract: In this paper we study the effect on crowd worker efficiency and\neffectiveness of the dominance of one class in the data they process. We aim at\nunderstanding if there is any positive or negative bias in workers seeing many\nnegative examples in the identification of positive labels. To test our\nhypothesis, we design an experiment where crowd workers are asked to judge the\nrelevance of documents presented in different orders. Our findings indicate\nthat there is a significant improvement in the quality of relevance judgements\nwhen presenting relevant results before the non-relevant ones. \n\n"}
{"id": "1609.02521", "contents": "Title: DiSMEC - Distributed Sparse Machines for Extreme Multi-label\n  Classification Abstract: Extreme multi-label classification refers to supervised multi-label learning\ninvolving hundreds of thousands or even millions of labels. Datasets in extreme\nclassification exhibit fit to power-law distribution, i.e. a large fraction of\nlabels have very few positive instances in the data distribution. Most\nstate-of-the-art approaches for extreme multi-label classification attempt to\ncapture correlation among labels by embedding the label matrix to a\nlow-dimensional linear sub-space. However, in the presence of power-law\ndistributed extremely large and diverse label spaces, structural assumptions\nsuch as low rank can be easily violated.\n  In this work, we present DiSMEC, which is a large-scale distributed framework\nfor learning one-versus-rest linear classifiers coupled with explicit capacity\ncontrol to control model size. Unlike most state-of-the-art methods, DiSMEC\ndoes not make any low rank assumptions on the label matrix. Using double layer\nof parallelization, DiSMEC can learn classifiers for datasets consisting\nhundreds of thousands labels within few hours. The explicit capacity control\nmechanism filters out spurious parameters which keep the model compact in size,\nwithout losing prediction accuracy. We conduct extensive empirical evaluation\non publicly available real-world datasets consisting upto 670,000 labels. We\ncompare DiSMEC with recent state-of-the-art approaches, including - SLEEC which\nis a leading approach for learning sparse local embeddings, and FastXML which\nis a tree-based approach optimizing ranking based loss function. On some of the\ndatasets, DiSMEC can significantly boost prediction accuracies - 10% better\ncompared to SLECC and 15% better compared to FastXML, in absolute terms. \n\n"}
{"id": "1609.03219", "contents": "Title: Sharing Hash Codes for Multiple Purposes Abstract: Locality sensitive hashing (LSH) is a powerful tool for sublinear-time\napproximate nearest neighbor search, and a variety of hashing schemes have been\nproposed for different dissimilarity measures. However, hash codes\nsignificantly depend on the dissimilarity, which prohibits users from adjusting\nthe dissimilarity at query time. In this paper, we propose {multiple purpose\nLSH (mp-LSH) which shares the hash codes for different dissimilarities. mp-LSH\nsupports L2, cosine, and inner product dissimilarities, and their corresponding\nweighted sums, where the weights can be adjusted at query time. It also allows\nus to modify the importance of pre-defined groups of features. Thus, mp-LSH\nenables us, for example, to retrieve similar items to a query with the user\npreference taken into account, to find a similar material to a query with some\nproperties (stability, utility, etc.) optimized, and to turn on or off a part\nof multi-modal information (brightness, color, audio, text, etc.) in\nimage/video retrieval. We theoretically and empirically analyze the performance\nof three variants of mp-LSH, and demonstrate their usefulness on real-world\ndata sets. \n\n"}
{"id": "1609.03344", "contents": "Title: Finite-sample and asymptotic analysis of generalization ability with an\n  application to penalized regression Abstract: In this paper, we study the performance of extremum estimators from the\nperspective of generalization ability (GA): the ability of a model to predict\noutcomes in new samples from the same population. By adapting the classical\nconcentration inequalities, we derive upper bounds on the empirical\nout-of-sample prediction errors as a function of the in-sample errors,\nin-sample data size, heaviness in the tails of the error distribution, and\nmodel complexity. We show that the error bounds may be used for tuning key\nestimation hyper-parameters, such as the number of folds $K$ in\ncross-validation. We also show how $K$ affects the bias-variance trade-off for\ncross-validation. We demonstrate that the $\\mathcal{L}_2$-norm difference\nbetween penalized and the corresponding un-penalized regression estimates is\ndirectly explained by the GA of the estimates and the GA of empirical moment\nconditions. Lastly, we prove that all penalized regression estimates are\n$L_2$-consistent for both the $n \\geqslant p$ and the $n < p$ cases.\nSimulations are used to demonstrate key results.\n  Keywords: generalization ability, upper bound of generalization error,\npenalized regression, cross-validation, bias-variance trade-off,\n$\\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso,\nhigh-dimensional data. \n\n"}
{"id": "1609.03457", "contents": "Title: Math-Aware Search Engines: Physics Applications and Overview Abstract: Search engines for equations now exist, which return results matching the\nquery's mathematical meaning or structural presentation. Operating over\nscientific papers, online encyclopedias, and math discussion forums, their\ncontent includes physics, math, and other sciences. They enable physicists to\navoid jargon and more easily target mathematical content within and across\ndisciplines. As a natural extension of keyword-based search, they open up a new\nworld for discovering both exact and approximate mathematical solutions;\nphysical systems' analogues and alternative models; and physics' patterns.\n  This review presents the existing math-aware search engines, discusses\nmethods for maximizing their search success, and overviews their math-matching\ncapabilities. Proposed applications to physics are also given, to contribute\ntowards developers' and physicists' exploration of the newly available search\nhorizons. \n\n"}
{"id": "1609.03544", "contents": "Title: Online Data Thinning via Multi-Subspace Tracking Abstract: In an era of ubiquitous large-scale streaming data, the availability of data\nfar exceeds the capacity of expert human analysts. In many settings, such data\nis either discarded or stored unprocessed in datacenters. This paper proposes a\nmethod of online data thinning, in which large-scale streaming datasets are\nwinnowed to preserve unique, anomalous, or salient elements for timely expert\nanalysis. At the heart of this proposed approach is an online anomaly detection\nmethod based on dynamic, low-rank Gaussian mixture models. Specifically, the\nhigh-dimensional covariances matrices associated with the Gaussian components\nare associated with low-rank models. According to this model, most observations\nlie near a union of subspaces. The low-rank modeling mitigates the curse of\ndimensionality associated with anomaly detection for high-dimensional data, and\nrecent advances in subspace clustering and subspace tracking allow the proposed\nmethod to adapt to dynamic environments. Furthermore, the proposed method\nallows subsampling, is robust to missing data, and uses a mini-batch online\noptimization approach. The resulting algorithms are scalable, efficient, and\nare capable of operating in real time. Experiments on wide-area motion imagery\nand e-mail databases illustrate the efficacy of the proposed approach. \n\n"}
{"id": "1609.03675", "contents": "Title: Deep Coevolutionary Network: Embedding User and Item Features for\n  Recommendation Abstract: Recommender systems often use latent features to explain the behaviors of\nusers and capture the properties of items. As users interact with different\nitems over time, user and item features can influence each other, evolve and\nco-evolve over time. The compatibility of user and item's feature further\ninfluence the future interaction between users and items. Recently, point\nprocess based models have been proposed in the literature aiming to capture the\ntemporally evolving nature of these latent features. However, these models\noften make strong parametric assumptions about the evolution process of the\nuser and item latent features, which may not reflect the reality, and has\nlimited power in expressing the complex and nonlinear dynamics underlying these\nprocesses. To address these limitations, we propose a novel deep coevolutionary\nnetwork model (DeepCoevolve), for learning user and item features based on\ntheir interaction graph. DeepCoevolve use recurrent neural network (RNN) over\nevolving networks to define the intensity function in point processes, which\nallows the model to capture complex mutual influence between users and items,\nand the feature evolution over time. We also develop an efficient procedure for\ntraining the model parameters, and show that the learned models lead to\nsignificant improvements in recommendation and activity prediction compared to\nprevious state-of-the-arts parametric models. \n\n"}
{"id": "1609.04120", "contents": "Title: Private Topic Modeling Abstract: We develop a privatised stochastic variational inference method for Latent\nDirichlet Allocation (LDA). The iterative nature of stochastic variational\ninference presents challenges: multiple iterations are required to obtain\naccurate posterior distributions, yet each iteration increases the amount of\nnoise that must be added to achieve a reasonable degree of privacy. We propose\na practical algorithm that overcomes this challenge by combining: (1) an\nimproved composition method for differential privacy, called the moments\naccountant, which provides a tight bound on the privacy cost of multiple\nvariational inference iterations and thus significantly decreases the amount of\nadditive noise; and (2) privacy amplification resulting from subsampling of\nlarge-scale data. Focusing on conjugate exponential family models, in our\nprivate variational inference, all the posterior distributions will be\nprivatised by simply perturbing expected sufficient statistics. Using Wikipedia\ndata, we illustrate the effectiveness of our algorithm for large-scale data. \n\n"}
{"id": "1609.06578", "contents": "Title: Twitter Opinion Topic Model: Extracting Product Opinions from Tweets by\n  Leveraging Hashtags and Sentiment Lexicon Abstract: Aspect-based opinion mining is widely applied to review data to aggregate or\nsummarize opinions of a product, and the current state-of-the-art is achieved\nwith Latent Dirichlet Allocation (LDA)-based model. Although social media data\nlike tweets are laden with opinions, their \"dirty\" nature (as natural language)\nhas discouraged researchers from applying LDA-based opinion model for product\nreview mining. Tweets are often informal, unstructured and lacking labeled data\nsuch as categories and ratings, making it challenging for product opinion\nmining. In this paper, we propose an LDA-based opinion model named Twitter\nOpinion Topic Model (TOTM) for opinion mining and sentiment analysis. TOTM\nleverages hashtags, mentions, emoticons and strong sentiment words that are\npresent in tweets in its discovery process. It improves opinion prediction by\nmodeling the target-opinion interaction directly, thus discovering target\nspecific opinion words, neglected in existing approaches. Moreover, we propose\na new formulation of incorporating sentiment prior information into a topic\nmodel, by utilizing an existing public sentiment lexicon. This is novel in that\nit learns and updates with the data. We conduct experiments on 9 million tweets\non electronic products, and demonstrate the improved performance of TOTM in\nboth quantitative evaluations and qualitative analysis. We show that\naspect-based opinion analysis on massive volume of tweets provides useful\nopinions on products. \n\n"}
{"id": "1609.06942", "contents": "Title: Randomized Independent Component Analysis Abstract: Independent component analysis (ICA) is a method for recovering statistically\nindependent signals from observations of unknown linear combinations of the\nsources. Some of the most accurate ICA decomposition methods require searching\nfor the inverse transformation which minimizes different approximations of the\nMutual Information, a measure of statistical independence of random vectors.\nTwo such approximations are the Kernel Generalized Variance or the Kernel\nCanonical Correlation which has been shown to reach the highest performance of\nICA methods. However, the computational effort necessary just for computing\nthese measures is cubic in the sample size. Hence, optimizing them becomes even\nmore computationally demanding, in terms of both space and time. Here, we\npropose a couple of alternative novel measures based on randomized features of\nthe samples - the Randomized Generalized Variance and the Randomized Canonical\nCorrelation. The computational complexity of calculating the proposed\nalternatives is linear in the sample size and provide a controllable\napproximation of their Kernel-based non-random versions. We also show that\noptimization of the proposed statistical properties yields a comparable\nseparation error at an order of magnitude faster compared to Kernel-based\nmeasures. \n\n"}
{"id": "1609.07363", "contents": "Title: Changepoint Detection in the Presence of Outliers Abstract: Many traditional methods for identifying changepoints can struggle in the\npresence of outliers, or when the noise is heavy-tailed. Often they will infer\nadditional changepoints in order to fit the outliers. To overcome this problem,\ndata often needs to be pre-processed to remove outliers, though this is\ndifficult for applications where the data needs to be analysed online. We\npresent an approach to changepoint detection that is robust to the presence of\noutliers. The idea is to adapt existing penalised cost approaches for detecting\nchanges so that they use loss functions that are less sensitive to outliers. We\nargue that loss functions that are bounded, such as the classical biweight\nloss, are particularly suitable -- as we show that only bounded loss functions\nare robust to arbitrarily extreme outliers. We present an efficient dynamic\nprogramming algorithm that can find the optimal segmentation under our\npenalised cost criteria. Importantly, this algorithm can be used in settings\nwhere the data needs to be analysed online. We show that we can consistently\nestimate the number of changepoints, and accurately estimate their locations,\nusing the biweight loss function. We demonstrate the usefulness of our approach\nfor applications such as analysing well-log data, detecting copy number\nvariation, and detecting tampering of wireless devices. \n\n"}
{"id": "1610.01644", "contents": "Title: Understanding intermediate layers using linear classifier probes Abstract: Neural network models have a reputation for being black boxes. We propose to\nmonitor the features at every layer of a model and measure how suitable they\nare for classification. We use linear classifiers, which we refer to as\n\"probes\", trained entirely independently of the model itself.\n  This helps us better understand the roles and dynamics of the intermediate\nlayers. We demonstrate how this can be used to develop a better intuition about\nmodels and to diagnose potential problems.\n  We apply this technique to the popular models Inception v3 and Resnet-50.\nAmong other things, we observe experimentally that the linear separability of\nfeatures increase monotonically along the depth of the model. \n\n"}
{"id": "1610.01858", "contents": "Title: A Robust Framework for Classifying Evolving Document Streams in an\n  Expert-Machine-Crowd Setting Abstract: An emerging challenge in the online classification of social media data\nstreams is to keep the categories used for classification up-to-date. In this\npaper, we propose an innovative framework based on an Expert-Machine-Crowd\n(EMC) triad to help categorize items by continuously identifying novel concepts\nin heterogeneous data streams often riddled with outliers. We unify constrained\nclustering and outlier detection by formulating a novel optimization problem:\nCOD-Means. We design an algorithm to solve the COD-Means problem and show that\nCOD-Means will not only help detect novel categories but also seamlessly\ndiscover human annotation errors and improve the overall quality of the\ncategorization process. Experiments on diverse real data sets demonstrate that\nour approach is both effective and efficient. \n\n"}
{"id": "1610.02757", "contents": "Title: Dataiku's Solution to SPHERE's Activity Recognition Challenge Abstract: Our team won the second prize of the Safe Aging with SPHERE Challenge\norganized by SPHERE, in conjunction with ECML-PKDD and Driven Data. The goal of\nthe competition was to recognize activities performed by humans, using sensor\ndata. This paper presents our solution. It is based on a rich pre-processing\nand state of the art machine learning methods. From the raw train data, we\ngenerate a synthetic train set with the same statistical characteristics as the\ntest set. We then perform feature engineering. The machine learning modeling\npart is based on stacking weak learners through a grid searched XGBoost\nalgorithm. Finally, we use post-processing to smooth our predictions over time. \n\n"}
{"id": "1610.03048", "contents": "Title: Optimal Download Cost of Private Information Retrieval for Arbitrary\n  Message Length Abstract: A private information retrieval scheme is a mechanism that allows a user to\nretrieve any one out of $K$ messages from $N$ non-communicating replicated\ndatabases, each of which stores all $K$ messages, without revealing anything\nabout the identity of the desired message index to any individual database. If\nthe size of each message is $L$ bits and the total download required by a PIR\nscheme from all $N$ databases is $D$ bits, then $D$ is called the download cost\nand the ratio $L/D$ is called an achievable rate. For fixed $K,N\\in\\mathbb{N}$,\nthe capacity of PIR, denoted by $C$, is the supremum of achievable rates over\nall PIR schemes and over all message sizes, and was recently shown to be\n$C=(1+1/N+1/N^2+\\cdots+1/N^{K-1})^{-1}$. In this work, for arbitrary $K, N$, we\nexplore the minimum download cost $D_L$ across all PIR schemes (not restricted\nto linear schemes) for arbitrary message lengths $L$ under arbitrary choices of\nalphabet (not restricted to finite fields) for the message and download\nsymbols. If the same $M$-ary alphabet is used for the message and download\nsymbols, then we show that the optimal download cost in $M$-ary symbols is\n$D_L=\\lceil\\frac{L}{C}\\rceil$. If the message symbols are in $M$-ary alphabet\nand the downloaded symbols are in $M'$-ary alphabet, then we show that the\noptimal download cost in $M'$-ary symbols, $D_L\\in\\left\\{\\left\\lceil\n\\frac{L'}{C}\\right\\rceil,\\left\\lceil \\frac{L'}{C}\\right\\rceil-1,\\left\\lceil\n\\frac{L'}{C}\\right\\rceil-2\\right\\}$, where $L'= \\lceil L \\log_{M'} M\\rceil$. \n\n"}
{"id": "1610.04019", "contents": "Title: Voice Conversion from Non-parallel Corpora Using Variational\n  Auto-encoder Abstract: We propose a flexible framework for spectral conversion (SC) that facilitates\ntraining with unaligned corpora. Many SC frameworks require parallel corpora,\nphonetic alignments, or explicit frame-wise correspondence for learning\nconversion functions or for synthesizing a target spectrum with the aid of\nalignments. However, these requirements gravely limit the scope of practical\napplications of SC due to scarcity or even unavailability of parallel corpora.\nWe propose an SC framework based on variational auto-encoder which enables us\nto exploit non-parallel corpora. The framework comprises an encoder that learns\nspeaker-independent phonetic representations and a decoder that learns to\nreconstruct the designated speaker. It removes the requirement of parallel\ncorpora or phonetic alignments to train a spectral conversion system. We report\nobjective and subjective evaluations to validate our proposed method and\ncompare it to SC methods that have access to aligned corpora. \n\n"}
{"id": "1610.04804", "contents": "Title: Dynamic Stacked Generalization for Node Classification on Networks Abstract: We propose a novel stacked generalization (stacking) method as a dynamic\nensemble technique using a pool of heterogeneous classifiers for node label\nclassification on networks. The proposed method assigns component models a set\nof functional coefficients, which can vary smoothly with certain topological\nfeatures of a node. Compared to the traditional stacking model, the proposed\nmethod can dynamically adjust the weights of individual models as we move\nacross the graph and provide a more versatile and significantly more accurate\nstacking model for label prediction on a network. We demonstrate the benefits\nof the proposed model using both a simulation study and real data analysis. \n\n"}
{"id": "1610.06382", "contents": "Title: Anfrage-getriebener Wissenstransfer zur Unterstuetzung von\n  Datenanalysten Abstract: In larger organizations, multiple teams of data scientists have to integrate\ndata from heterogeneous data sources as preparation for data analysis tasks.\nWriting effective analytical queries requires data scientists to have in-depth\nknowledge of the existence, semantics, and usage context of data sources. Once\ngathered, such knowledge is informally shared within a specific team of data\nscientists, but usually is neither formalized nor shared with other teams.\nPotential synergies remain unused. We therefore introduce a novel approach\nwhich extends data management systems with additional knowledge-sharing\ncapabilities to facilitate user collaboration without altering established data\nanalysis workflows. Relevant collective knowledge from the query log is\nextracted to support data source discovery and incremental data integration.\nExtracted knowledge is formalized and provided at query time. \n\n"}
{"id": "1610.06468", "contents": "Title: Ten Blue Links on Mars Abstract: This paper explores a simple question: How would we provide a high-quality\nsearch experience on Mars, where the fundamental physical limit is\nspeed-of-light propagation delays on the order of tens of minutes? On Earth,\nusers are accustomed to nearly instantaneous response times from search\nengines. Is it possible to overcome orders-of-magnitude longer latency to\nprovide a tolerable user experience on Mars? In this paper, we formulate the\nsearching from Mars problem as a tradeoff between \"effort\" (waiting for\nresponses from Earth) and \"data transfer\" (pre-fetching or caching data on\nMars). The contribution of our work is articulating this design space and\npresenting two case studies that explore the effectiveness of baseline\ntechniques, using publicly available data from the TREC Total Recall and\nSessions Tracks. We intend for this research problem to be aspirational and\ninspirational - even if one is not convinced by the premise of Mars\ncolonization, there are Earth-based scenarios such as searching from a rural\nvillage in India that share similar constraints, thus making the problem worthy\nof exploration and attention from researchers. \n\n"}
{"id": "1610.06664", "contents": "Title: Stochastic Gradient MCMC with Stale Gradients Abstract: Stochastic gradient MCMC (SG-MCMC) has played an important role in\nlarge-scale Bayesian learning, with well-developed theoretical convergence\nproperties. In such applications of SG-MCMC, it is becoming increasingly\npopular to employ distributed systems, where stochastic gradients are computed\nbased on some outdated parameters, yielding what are termed stale gradients.\nWhile stale gradients could be directly used in SG-MCMC, their impact on\nconvergence properties has not been well studied. In this paper we develop\ntheory to show that while the bias and MSE of an SG-MCMC algorithm depend on\nthe staleness of stochastic gradients, its estimation variance (relative to the\nexpected estimate, based on a prescribed number of samples) is independent of\nit. In a simple Bayesian distributed system with SG-MCMC, where stale gradients\nare computed asynchronously by a set of workers, our theory indicates a linear\nspeedup on the decrease of estimation variance w.r.t. the number of workers.\nExperiments on synthetic data and deep neural networks validate our theory,\ndemonstrating the effectiveness and scalability of SG-MCMC with stale\ngradients. \n\n"}
{"id": "1610.07116", "contents": "Title: Online Classification with Complex Metrics Abstract: We present a framework and analysis of consistent binary classification for\ncomplex and non-decomposable performance metrics such as the F-measure and the\nJaccard measure. The proposed framework is general, as it applies to both batch\nand online learning, and to both linear and non-linear models. Our work follows\nrecent results showing that the Bayes optimal classifier for many complex\nmetrics is given by a thresholding of the conditional probability of the\npositive class. This manuscript extends this thresholding characterization --\nshowing that the utility is strictly locally quasi-concave with respect to the\nthreshold for a wide range of models and performance metrics. This, in turn,\nmotivates simple normalized gradient ascent updates for threshold estimation.\nWe present a finite-sample regret analysis for the resulting procedure. In\nparticular, the risk for the batch case converges to the Bayes risk at the same\nrate as that of the underlying conditional probability estimation, and the risk\nof proposed online algorithm converges at a rate that depends on the\nconditional probability estimation risk. For instance, in the special case\nwhere the conditional probability model is logistic regression, our procedure\nachieves $O(\\frac{1}{\\sqrt{n}})$ sample complexity, both for batch and online\ntraining. Empirical evaluation shows that the proposed algorithms out-perform\nalternatives in practice, with comparable or better prediction performance and\nreduced run time for various metrics and datasets. \n\n"}
{"id": "1610.07703", "contents": "Title: Scalable Dynamic Topic Modeling with Clustered Latent Dirichlet\n  Allocation (CLDA) Abstract: Topic modeling, a method for extracting the underlying themes from a\ncollection of documents, is an increasingly important component of the design\nof intelligent systems enabling the sense-making of highly dynamic and diverse\nstreams of text data. Traditional methods such as Dynamic Topic Modeling (DTM)\ndo not lend themselves well to direct parallelization because of dependencies\nfrom one time step to another. In this paper, we introduce and empirically\nanalyze Clustered Latent Dirichlet Allocation (CLDA), a method for extracting\ndynamic latent topics from a collection of documents. Our approach is based on\ndata decomposition in which the data is partitioned into segments, followed by\ntopic modeling on the individual segments. The resulting local models are then\ncombined into a global solution using clustering. The decomposition and\nresulting parallelization leads to very fast runtime even on very large\ndatasets. Our approach furthermore provides insight into how the composition of\ntopics changes over time and can also be applied using other data partitioning\nstrategies over any discrete features of the data, such as geographic features\nor classes of users. In this paper CLDA is applied successfully to seventeen\nyears of NIPS conference papers (2,484 documents and 3,280,697 words),\nseventeen years of computer science journal abstracts (533,560 documents and\n32,551,540 words), and to forty years of the PubMed corpus (4,025,978 documents\nand 273,853,980 words). \n\n"}
{"id": "1610.08077", "contents": "Title: A statistical framework for fair predictive algorithms Abstract: Predictive modeling is increasingly being employed to assist human\ndecision-makers. One purported advantage of replacing human judgment with\ncomputer models in high stakes settings-- such as sentencing, hiring, policing,\ncollege admissions, and parole decisions-- is the perceived \"neutrality\" of\ncomputers. It is argued that because computer models do not hold personal\nprejudice, the predictions they produce will be equally free from prejudice.\nThere is growing recognition that employing algorithms does not remove the\npotential for bias, and can even amplify it, since training data were\ninevitably generated by a process that is itself biased. In this paper, we\nprovide a probabilistic definition of algorithmic bias. We propose a method to\nremove bias from predictive models by removing all information regarding\nprotected variables from the permitted training data. Unlike previous work in\nthis area, our framework is general enough to accommodate arbitrary data types,\ne.g. binary, continuous, etc. Motivated by models currently in use in the\ncriminal justice system that inform decisions on pre-trial release and\nparoling, we apply our proposed method to a dataset on the criminal histories\nof individuals at the time of sentencing to produce \"race-neutral\" predictions\nof re-arrest. In the process, we demonstrate that the most common approach to\ncreating \"race-neutral\" models-- omitting race as a covariate-- still results\nin racially disparate predictions. We then demonstrate that the application of\nour proposed method to these data removes racial disparities from predictions\nwith minimal impact on predictive accuracy. \n\n"}
{"id": "1610.08136", "contents": "Title: Learning to Match Using Local and Distributed Representations of Text\n  for Web Search Abstract: Models such as latent semantic analysis and those based on neural embeddings\nlearn distributed representations of text, and match the query against the\ndocument in the latent semantic space. In traditional information retrieval\nmodels, on the other hand, terms have discrete or local representations, and\nthe relevance of a document is determined by the exact matches of query terms\nin the body text. We hypothesize that matching with distributed representations\ncomplements matching with traditional local representations, and that a\ncombination of the two is favorable. We propose a novel document ranking model\ncomposed of two separate deep neural networks, one that matches the query and\nthe document using a local representation, and another that matches the query\nand the document using learned distributed representations. The two networks\nare jointly trained as part of a single neural network. We show that this\ncombination or `duet' performs significantly better than either neural network\nindividually on a Web page ranking task, and also significantly outperforms\ntraditional baselines and other recently proposed models based on neural\nnetworks. \n\n"}
{"id": "1610.08442", "contents": "Title: Inferring individual attributes from search engine queries and auxiliary\n  information Abstract: Internet data has surfaced as a primary source for investigation of different\naspects of human behavior. A crucial step in such studies is finding a suitable\ncohort (i.e., a set of users) that shares a common trait of interest to\nresearchers. However, direct identification of users sharing this trait is\noften impossible, as the data available to researchers is usually anonymized to\npreserve user privacy. To facilitate research on specific topics of interest,\nespecially in medicine, we introduce an algorithm for identifying a trait of\ninterest in anonymous users. We illustrate how a small set of labeled examples,\ntogether with statistical information about the entire population, can be\naggregated to obtain labels on unseen examples. We validate our approach using\nlabeled data from the political domain.\n  We provide two applications of the proposed algorithm to the medical domain.\nIn the first, we demonstrate how to identify users whose search patterns\nindicate they might be suffering from certain types of cancer. In the second,\nwe detail an algorithm to predict the distribution of diseases given their\nincidence in a subset of the population at study, making it possible to predict\ndisease spread from partial epidemiological data. \n\n"}
{"id": "1610.08611", "contents": "Title: Causal Network Learning from Multiple Interventions of Unknown\n  Manipulated Targets Abstract: In this paper, we discuss structure learning of causal networks from multiple\ndata sets obtained by external intervention experiments where we do not know\nwhat variables are manipulated. For example, the conditions in these\nexperiments are changed by changing temperature or using drugs, but we do not\nknow what target variables are manipulated by the external interventions. From\nsuch data sets, the structure learning becomes more difficult. For this case,\nwe first discuss the identifiability of causal structures. Next we present a\ngraph-merging method for learning causal networks for the case that the sample\nsizes are large for these interventions. Then for the case that the sample\nsizes of these interventions are relatively small, we propose a data-pooling\nmethod for learning causal networks in which we pool all data sets of these\ninterventions together for the learning. Further we propose a re-sampling\napproach to evaluate the edges of the causal network learned by the\ndata-pooling method. Finally we illustrate the proposed learning methods by\nsimulations. \n\n"}
{"id": "1610.08696", "contents": "Title: Learning Bound for Parameter Transfer Learning Abstract: We consider a transfer-learning problem by using the parameter transfer\napproach, where a suitable parameter of feature mapping is learned through one\ntask and applied to another objective task. Then, we introduce the notion of\nthe local stability and parameter transfer learnability of parametric feature\nmapping,and thereby derive a learning bound for parameter transfer algorithms.\nAs an application of parameter transfer learning, we discuss the performance of\nsparse coding in self-taught learning. Although self-taught learning algorithms\nwith plentiful unlabeled data often show excellent empirical performance, their\ntheoretical analysis has not been studied. In this paper, we also provide the\nfirst theoretical learning bound for self-taught learning. \n\n"}
{"id": "1610.08861", "contents": "Title: On Bochner's and Polya's Characterizations of Positive-Definite Kernels\n  and the Respective Random Feature Maps Abstract: Positive-definite kernel functions are fundamental elements of kernel methods\nand Gaussian processes. A well-known construction of such functions comes from\nBochner's characterization, which connects a positive-definite function with a\nprobability distribution. Another construction, which appears to have attracted\nless attention, is Polya's criterion that characterizes a subset of these\nfunctions. In this paper, we study the latter characterization and derive a\nnumber of novel kernels little known previously.\n  In the context of large-scale kernel machines, Rahimi and Recht (2007)\nproposed a random feature map (random Fourier) that approximates a kernel\nfunction, through independent sampling of the probability distribution in\nBochner's characterization. The authors also suggested another feature map\n(random binning), which, although not explicitly stated, comes from Polya's\ncharacterization. We show that with the same number of random samples, the\nrandom binning map results in an Euclidean inner product closer to the kernel\nthan does the random Fourier map. The superiority of the random binning map is\nconfirmed empirically through regressions and classifications in the\nreproducing kernel Hilbert space. \n\n"}
{"id": "1610.09038", "contents": "Title: Professor Forcing: A New Algorithm for Training Recurrent Networks Abstract: The Teacher Forcing algorithm trains recurrent networks by supplying observed\nsequence values as inputs during training and using the network's own\none-step-ahead predictions to do multi-step sampling. We introduce the\nProfessor Forcing algorithm, which uses adversarial domain adaptation to\nencourage the dynamics of the recurrent network to be the same when training\nthe network and when sampling from the network over multiple time steps. We\napply Professor Forcing to language modeling, vocal synthesis on raw waveforms,\nhandwriting generation, and image generation. Empirically we find that\nProfessor Forcing acts as a regularizer, improving test likelihood on character\nlevel Penn Treebank and sequential MNIST. We also find that the model\nqualitatively improves samples, especially when sampling for a large number of\ntime steps. This is supported by human evaluation of sample quality. Trade-offs\nbetween Professor Forcing and Scheduled Sampling are discussed. We produce\nT-SNEs showing that Professor Forcing successfully makes the dynamics of the\nnetwork during training and sampling more similar. \n\n"}
{"id": "1610.09075", "contents": "Title: Missing Data Imputation for Supervised Learning Abstract: Missing data imputation can help improve the performance of prediction models\nin situations where missing data hide useful information. This paper compares\nmethods for imputing missing categorical data for supervised classification\ntasks. We experiment on two machine learning benchmark datasets with missing\ncategorical data, comparing classifiers trained on non-imputed (i.e., one-hot\nencoded) or imputed data with different levels of additional missing-data\nperturbation. We show imputation methods can increase predictive accuracy in\nthe presence of missing-data perturbation, which can actually improve\nprediction accuracy by regularizing the classifier. We achieve the\nstate-of-the-art on the Adult dataset with missing-data perturbation and\nk-nearest-neighbors (k-NN) imputation. \n\n"}
{"id": "1610.09322", "contents": "Title: Homotopy Analysis for Tensor PCA Abstract: Developing efficient and guaranteed nonconvex algorithms has been an\nimportant challenge in modern machine learning. Algorithms with good empirical\nperformance such as stochastic gradient descent often lack theoretical\nguarantees. In this paper, we analyze the class of homotopy or continuation\nmethods for global optimization of nonconvex functions. These methods start\nfrom an objective function that is efficient to optimize (e.g. convex), and\nprogressively modify it to obtain the required objective, and the solutions are\npassed along the homotopy path. For the challenging problem of tensor PCA, we\nprove global convergence of the homotopy method in the \"high noise\" regime. The\nsignal-to-noise requirement for our algorithm is tight in the sense that it\nmatches the recovery guarantee for the best degree-4 sum-of-squares algorithm.\nIn addition, we prove a phase transition along the homotopy path for tensor\nPCA. This allows to simplify the homotopy method to a local search algorithm,\nviz., tensor power iterations, with a specific initialization and a noise\ninjection procedure, while retaining the theoretical guarantees. \n\n"}
{"id": "1610.09780", "contents": "Title: Flexible Models for Microclustering with Application to Entity\n  Resolution Abstract: Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some applications, this\nassumption is inappropriate. For example, when performing entity resolution,\nthe size of each cluster should be unrelated to the size of the data set, and\neach cluster should contain a negligible fraction of the total number of data\npoints. These applications require models that yield clusters whose sizes grow\nsublinearly with the size of the data set. We address this requirement by\ndefining the microclustering property and introducing a new class of models\nthat can exhibit this property. We compare models within this class to two\ncommonly used clustering models using four entity-resolution data sets. \n\n"}
{"id": "1611.00144", "contents": "Title: Product-based Neural Networks for User Response Prediction Abstract: Predicting user responses, such as clicks and conversions, is of great\nimportance and has found its usage in many Web applications including\nrecommender systems, web search and online advertising. The data in those\napplications is mostly categorical and contains multiple fields; a typical\nrepresentation is to transform it into a high-dimensional sparse binary feature\nrepresentation via one-hot encoding. Facing with the extreme sparsity,\ntraditional models may limit their capacity of mining shallow patterns from the\ndata, i.e. low-order feature combinations. Deep models like deep neural\nnetworks, on the other hand, cannot be directly applied for the\nhigh-dimensional input because of the huge feature space. In this paper, we\npropose a Product-based Neural Networks (PNN) with an embedding layer to learn\na distributed representation of the categorical data, a product layer to\ncapture interactive patterns between inter-field categories, and further fully\nconnected layers to explore high-order feature interactions. Our experimental\nresults on two large-scale real-world ad click datasets demonstrate that PNNs\nconsistently outperform the state-of-the-art models on various metrics. \n\n"}
{"id": "1611.01802", "contents": "Title: Self-Wiring Question Answering Systems Abstract: Question answering (QA) has been the subject of a resurgence over the past\nyears. The said resurgence has led to a multitude of question answering (QA)\nsystems being developed both by companies and research facilities. While a few\ncomponents of QA systems get reused across implementations, most systems do not\nleverage the full potential of component reuse. Hence, the development of QA\nsystems is currently still a tedious and time-consuming process. We address the\nchallenge of accelerating the creation of novel or tailored QA systems by\npresenting a concept for a self-wiring approach to composing QA systems. Our\napproach will allow the reuse of existing, web-based QA systems or modules\nwhile developing new QA platforms. To this end, it will rely on QA modules\nbeing described using the Web Ontology Language. Based on these descriptions,\nour approach will be able to automatically compose QA systems using a\ndata-driven approach automatically. \n\n"}
{"id": "1611.01891", "contents": "Title: Joint Multimodal Learning with Deep Generative Models Abstract: We investigate deep generative models that can exchange multiple modalities\nbi-directionally, e.g., generating images from corresponding texts and vice\nversa. Recently, some studies handle multiple modalities on deep generative\nmodels, such as variational autoencoders (VAEs). However, these models\ntypically assume that modalities are forced to have a conditioned relation,\ni.e., we can only generate modalities in one direction. To achieve our\nobjective, we should extract a joint representation that captures high-level\nconcepts among all modalities and through which we can exchange them\nbi-directionally. As described herein, we propose a joint multimodal\nvariational autoencoder (JMVAE), in which all modalities are independently\nconditioned on joint representation. In other words, it models a joint\ndistribution of modalities. Furthermore, to be able to generate missing\nmodalities from the remaining modalities properly, we develop an additional\nmethod, JMVAE-kl, that is trained by reducing the divergence between JMVAE's\nencoder and prepared networks of respective modalities. Our experiments show\nthat our proposed method can obtain appropriate joint representation from\nmultiple modalities and that it can generate and reconstruct them more properly\nthan conventional VAEs. We further demonstrate that JMVAE can generate multiple\nmodalities bi-directionally. \n\n"}
{"id": "1611.02041", "contents": "Title: Does Distributionally Robust Supervised Learning Give Robust\n  Classifiers? Abstract: Distributionally Robust Supervised Learning (DRSL) is necessary for building\nreliable machine learning systems. When machine learning is deployed in the\nreal world, its performance can be significantly degraded because test data may\nfollow a different distribution from training data. DRSL with f-divergences\nexplicitly considers the worst-case distribution shift by minimizing the\nadversarially reweighted training loss. In this paper, we analyze this DRSL,\nfocusing on the classification scenario. Since the DRSL is explicitly\nformulated for a distribution shift scenario, we naturally expect it to give a\nrobust classifier that can aggressively handle shifted distributions. However,\nsurprisingly, we prove that the DRSL just ends up giving a classifier that\nexactly fits the given training distribution, which is too pessimistic. This\npessimism comes from two sources: the particular losses used in classification\nand the fact that the variety of distributions to which the DRSL tries to be\nrobust is too wide. Motivated by our analysis, we propose simple DRSL that\novercomes this pessimism and empirically demonstrate its effectiveness. \n\n"}
{"id": "1611.06310", "contents": "Title: Local minima in training of neural networks Abstract: There has been a lot of recent interest in trying to characterize the error\nsurface of deep models. This stems from a long standing question. Given that\ndeep networks are highly nonlinear systems optimized by local gradient methods,\nwhy do they not seem to be affected by bad local minima? It is widely believed\nthat training of deep models using gradient methods works so well because the\nerror surface either has no local minima, or if they exist they need to be\nclose in value to the global minimum. It is known that such results hold under\nvery strong assumptions which are not satisfied by real models. In this paper\nwe present examples showing that for such theorem to be true additional\nassumptions on the data, initialization schemes and/or the model classes have\nto be made. We look at the particular case of finite size datasets. We\ndemonstrate that in this scenario one can construct counter-examples (datasets\nor initialization schemes) when the network does become susceptible to bad\nlocal minima over the weight space. \n\n"}
{"id": "1611.06585", "contents": "Title: Variational Boosting: Iteratively Refining Posterior Approximations Abstract: We propose a black-box variational inference method to approximate\nintractable distributions with an increasingly rich approximating class. Our\nmethod, termed variational boosting, iteratively refines an existing\nvariational approximation by solving a sequence of optimization problems,\nallowing the practitioner to trade computation time for accuracy. We show how\nto expand the variational approximating class by incorporating additional\ncovariance structure and by introducing new components to form a mixture. We\napply variational boosting to synthetic and real statistical models, and show\nthat resulting posterior inferences compare favorably to existing posterior\napproximation algorithms in both accuracy and efficiency. \n\n"}
{"id": "1611.06652", "contents": "Title: Scalable Adaptive Stochastic Optimization Using Random Projections Abstract: Adaptive stochastic gradient methods such as AdaGrad have gained popularity\nin particular for training deep neural networks. The most commonly used and\nstudied variant maintains a diagonal matrix approximation to second order\ninformation by accumulating past gradients which are used to tune the step size\nadaptively. In certain situations the full-matrix variant of AdaGrad is\nexpected to attain better performance, however in high dimensions it is\ncomputationally impractical. We present Ada-LR and RadaGrad two computationally\nefficient approximations to full-matrix AdaGrad based on randomized\ndimensionality reduction. They are able to capture dependencies between\nfeatures and achieve similar performance to full-matrix AdaGrad but at a much\nsmaller computational cost. We show that the regret of Ada-LR is close to the\nregret of full-matrix AdaGrad which can have an up-to exponentially smaller\ndependence on the dimension than the diagonal variant. Empirically, we show\nthat Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task\nof training convolutional neural networks as well as recurrent neural networks,\nRadaGrad achieves faster convergence than diagonal AdaGrad. \n\n"}
{"id": "1611.06668", "contents": "Title: MV-RNN: A Multi-View Recurrent Neural Network for Sequential\n  Recommendation Abstract: Sequential recommendation is a fundamental task for network applications, and\nit usually suffers from the item cold start problem due to the insufficiency of\nuser feedbacks. There are currently three kinds of popular approaches which are\nrespectively based on matrix factorization (MF) of collaborative filtering,\nMarkov chain (MC), and recurrent neural network (RNN). Although widely used,\nthey have some limitations. MF based methods could not capture dynamic user's\ninterest. The strong Markov assumption greatly limits the performance of MC\nbased methods. RNN based methods are still in the early stage of incorporating\nadditional information. Based on these basic models, many methods with\nadditional information only validate incorporating one modality in a separate\nway. In this work, to make the sequential recommendation and deal with the item\ncold start problem, we propose a Multi-View Recurrent Neural Network (MV-RNN})\nmodel. Given the latent feature, MV-RNN can alleviate the item cold start\nproblem by incorporating visual and textual information. First, At the input of\nMV-RNN, three different combinations of multi-view features are studied, like\nconcatenation, fusion by addition and fusion by reconstructing the original\nmulti-modal data. MV-RNN applies the recurrent structure to dynamically capture\nthe user's interest. Second, we design a separate structure and a united\nstructure on the hidden state of MV-RNN to explore a more effective way to\nhandle multi-view features. Experiments on two real-world datasets show that\nMV-RNN can effectively generate the personalized ranking list, tackle the\nmissing modalities problem and significantly alleviate the item cold start\nproblem. \n\n"}
{"id": "1611.07135", "contents": "Title: Leveraging Citation Networks to Visualize Scholarly Influence Over Time Abstract: Assessing the influence of a scholar's work is an important task for funding\norganizations, academic departments, and researchers. Common methods, such as\nmeasures of citation counts, can ignore much of the nuance and\nmultidimensionality of scholarly influence. We present an approach for\ngenerating dynamic visualizations of scholars' careers. This approach uses an\nanimated node-link diagram showing the citation network accumulated around the\nresearcher over the course of the career in concert with key indicators,\nhighlighting influence both within and across fields. We developed our design\nin collaboration with one funding organization---the Pew Biomedical Scholars\nprogram---but the methods are generalizable to visualizations of scholarly\ninfluence. We applied the design method to the Microsoft Academic Graph, which\nincludes more than 120 million publications. We validate our abstractions\nthroughout the process through collaboration with the Pew Biomedical Scholars\nprogram officers and summative evaluations with their scholars. \n\n"}
{"id": "1611.07270", "contents": "Title: Investigating the influence of noise and distractors on the\n  interpretation of neural networks Abstract: Understanding neural networks is becoming increasingly important. Over the\nlast few years different types of visualisation and explanation methods have\nbeen proposed. However, none of them explicitly considered the behaviour in the\npresence of noise and distracting elements. In this work, we will show how\nnoise and distracting dimensions can influence the result of an explanation\nmodel. This gives a new theoretical insights to aid selection of the most\nappropriate explanation model within the deep-Taylor decomposition framework. \n\n"}
{"id": "1611.07743", "contents": "Title: Tunable Sensitivity to Large Errors in Neural Network Training Abstract: When humans learn a new concept, they might ignore examples that they cannot\nmake sense of at first, and only later focus on such examples, when they are\nmore useful for learning. We propose incorporating this idea of tunable\nsensitivity for hard examples in neural network learning, using a new\ngeneralization of the cross-entropy gradient step, which can be used in place\nof the gradient in any gradient-based training method. The generalized gradient\nis parameterized by a value that controls the sensitivity of the training\nprocess to harder training examples. We tested our method on several benchmark\ndatasets. We propose, and corroborate in our experiments, that the optimal\nlevel of sensitivity to hard example is positively correlated with the depth of\nthe network. Moreover, the test prediction error obtained by our method is\ngenerally lower than that of the vanilla cross-entropy gradient learner. We\ntherefore conclude that tunable sensitivity can be helpful for neural network\nlearning. \n\n"}
{"id": "1611.08083", "contents": "Title: Survey of Expressivity in Deep Neural Networks Abstract: We survey results on neural network expressivity described in \"On the\nExpressive Power of Deep Neural Networks\". The paper motivates and develops\nthree natural measures of expressiveness, which all display an exponential\ndependence on the depth of the network. In fact, all of these measures are\nrelated to a fourth quantity, trajectory length. This quantity grows\nexponentially in the depth of the network, and is responsible for the depth\nsensitivity observed. These results translate to consequences for networks\nduring and after training. They suggest that parameters earlier in a network\nhave greater influence on its expressive power -- in particular, given a layer,\nits influence on expressivity is determined by the remaining depth of the\nnetwork after that layer. This is verified with experiments on MNIST and\nCIFAR-10. We also explore the effect of training on the input-output map, and\nfind that it trades off between the stability and expressivity. \n\n"}
{"id": "1611.08292", "contents": "Title: Identifying Significant Predictive Bias in Classifiers Abstract: We present a novel subset scan method to detect if a probabilistic binary\nclassifier has statistically significant bias -- over or under predicting the\nrisk -- for some subgroup, and identify the characteristics of this subgroup.\nThis form of model checking and goodness-of-fit test provides a way to\ninterpretably detect the presence of classifier bias or regions of poor\nclassifier fit. This allows consideration of not just subgroups of a priori\ninterest or small dimensions, but the space of all possible subgroups of\nfeatures. To address the difficulty of considering these exponentially many\npossible subgroups, we use subset scan and parametric bootstrap-based methods.\nExtending this method, we can penalize the complexity of the detected subgroup\nand also identify subgroups with high classification errors. We demonstrate\nthese methods and find interesting results on the COMPAS crime recidivism and\ncredit delinquency data. \n\n"}
{"id": "1611.08480", "contents": "Title: Distributed Optimization of Multi-Class SVMs Abstract: Training of one-vs.-rest SVMs can be parallelized over the number of classes\nin a straight forward way. Given enough computational resources, one-vs.-rest\nSVMs can thus be trained on data involving a large number of classes. The same\ncannot be stated, however, for the so-called all-in-one SVMs, which require\nsolving a quadratic program of size quadratically in the number of classes. We\ndevelop distributed algorithms for two all-in-one SVM formulations (Lee et al.\nand Weston and Watkins) that parallelize the computation evenly over the number\nof classes. This allows us to compare these models to one-vs.-rest SVMs on\nunprecedented scale. The results indicate superior accuracy on text\nclassification data. \n\n"}
{"id": "1612.00148", "contents": "Title: Domain Adaptation for Named Entity Recognition in Online Media with Word\n  Embeddings Abstract: Content on the Internet is heterogeneous and arises from various domains like\nNews, Entertainment, Finance and Technology. Understanding such content\nrequires identifying named entities (persons, places and organizations) as one\nof the key steps. Traditionally Named Entity Recognition (NER) systems have\nbeen built using available annotated datasets (like CoNLL, MUC) and demonstrate\nexcellent performance. However, these models fail to generalize onto other\ndomains like Sports and Finance where conventions and language use can differ\nsignificantly. Furthermore, several domains do not have large amounts of\nannotated labeled data for training robust Named Entity Recognition models. A\nkey step towards this challenge is to adapt models learned on domains where\nlarge amounts of annotated training data are available to domains with scarce\nannotated data.\n  In this paper, we propose methods to effectively adapt models learned on one\ndomain onto other domains using distributed word representations. First we\nanalyze the linguistic variation present across domains to identify key\nlinguistic insights that can boost performance across domains. We propose\nmethods to capture domain specific semantics of word usage in addition to\nglobal semantics. We then demonstrate how to effectively use such domain\nspecific knowledge to learn NER models that outperform previous baselines in\nthe domain adaptation setting. \n\n"}
{"id": "1612.00260", "contents": "Title: Factory of realities: on the emergence of virtual spatiotemporal\n  structures Abstract: The ubiquitous nature of modern Information Retrieval and Virtual World give\nrise to new realities. To what extent are these \"realities\" real? Which\n\"physics\" should be applied to quantitatively describe them? In this essay I\ndwell on few examples. The first is Adaptive neural networks, which are not\nnetworks and not neural, but still provide service similar to classical ANNs in\nextended fashion. The second is the emergence of objects looking like\nEinsteinian spacetime, which describe the behavior of an Internet surfer like\ngeodesic motion. The third is the demonstration of nonclassical and even\nstronger-than-quantum probabilities in Information Retrieval, their use.\n  Immense operable datasets provide new operationalistic environments, which\nbecome to greater and greater extent \"realities\". In this essay, I consider the\noverall Information Retrieval process as an objective physical process,\nrepresenting it according to Melucci metaphor in terms of physical-like\nexperiments. Various semantic environments are treated as analogs of various\nrealities. The readers' attention is drawn to topos approach to physical\ntheories, which provides a natural conceptual and technical framework to cope\nwith the new emerging realities. \n\n"}
{"id": "1612.00374", "contents": "Title: Spatial Decompositions for Large Scale SVMs Abstract: Although support vector machines (SVMs) are theoretically well understood,\ntheir underlying optimization problem becomes very expensive, if, for example,\nhundreds of thousands of samples and a non-linear kernel are considered.\nSeveral approaches have been proposed in the past to address this serious\nlimitation. In this work we investigate a decomposition strategy that learns on\nsmall, spatially defined data chunks. Our contributions are two fold: On the\ntheoretical side we establish an oracle inequality for the overall learning\nmethod using the hinge loss, and show that the resulting rates match those\nknown for SVMs solving the complete optimization problem with Gaussian kernels.\nOn the practical side we compare our approach to learning SVMs on small,\nrandomly chosen chunks. Here it turns out that for comparable training times\nour approach is significantly faster during testing and also reduces the test\nerror in most cases significantly. Furthermore, we show that our approach\neasily scales up to 10 million training samples: including hyper-parameter\nselection using cross validation, the entire training only takes a few hours on\na single machine. Finally, we report an experiment on 32 million training\nsamples. All experiments used liquidSVM (Steinwart and Thomann, 2017). \n\n"}
{"id": "1612.00516", "contents": "Title: Canonical Correlation Analysis for Analyzing Sequences of Medical\n  Billing Codes Abstract: We propose using canonical correlation analysis (CCA) to generate features\nfrom sequences of medical billing codes. Applying this novel use of CCA to a\ndatabase of medical billing codes for patients with diverticulitis, we first\ndemonstrate that the CCA embeddings capture meaningful relationships among the\ncodes. We then generate features from these embeddings and establish their\nusefulness in predicting future elective surgery for diverticulitis, an\nimportant marker in efforts for reducing costs in healthcare. \n\n"}
{"id": "1612.00662", "contents": "Title: Predicting Patient State-of-Health using Sliding Window and Recurrent\n  Classifiers Abstract: Bedside monitors in Intensive Care Units (ICUs) frequently sound incorrectly,\nslowing response times and desensitising nurses to alarms (Chambrin, 2001),\ncausing true alarms to be missed (Hug et al., 2011). We compare sliding window\npredictors with recurrent predictors to classify patient state-of-health from\nICU multivariate time series; we report slightly improved performance for the\nRNN for three out of four targets. \n\n"}
{"id": "1612.01158", "contents": "Title: Properties and Bayesian fitting of restricted Boltzmann machines Abstract: A restricted Boltzmann machine (RBM) is an undirected graphical model\nconstructed for discrete or continuous random variables, with two layers, one\nhidden and one visible, and no conditional dependency within a layer. In recent\nyears, RBMs have risen to prominence due to their connection to deep learning.\nBy treating a hidden layer of one RBM as the visible layer in a second RBM, a\ndeep architecture can be created. RBMs are thought to thereby have the ability\nto encode very complex and rich structures in data, making them attractive for\nsupervised learning. However, the generative behavior of RBMs is largely\nunexplored and typical fitting methodology does not easily allow for\nuncertainty quantification in addition to point estimates. In this paper, we\ndiscuss the relationship between RBM parameter specification in the binary case\nand model properties such as degeneracy, instability and uninterpretability. We\nalso describe the associated difficulties that can arise with likelihood-based\ninference and further discuss the potential Bayes fitting of such (highly\nflexible) models, especially as Gibbs sampling (quasi-Bayes) methods are often\nadvocated for the RBM model structure. \n\n"}
{"id": "1612.02179", "contents": "Title: Model-based Adversarial Imitation Learning Abstract: Generative adversarial learning is a popular new approach to training\ngenerative models which has been proven successful for other related problems\nas well. The general idea is to maintain an oracle $D$ that discriminates\nbetween the expert's data distribution and that of the generative model $G$.\nThe generative model is trained to capture the expert's distribution by\nmaximizing the probability of $D$ misclassifying the data it generates.\nOverall, the system is \\emph{differentiable} end-to-end and is trained using\nbasic backpropagation. This type of learning was successfully applied to the\nproblem of policy imitation in a model-free setup. However, a model-free\napproach does not allow the system to be differentiable, which requires the use\nof high-variance gradient estimations. In this paper we introduce the Model\nbased Adversarial Imitation Learning (MAIL) algorithm. A model-based approach\nfor the problem of adversarial imitation learning. We show how to use a forward\nmodel to make the system fully differentiable, which enables us to train\npolicies using the (stochastic) gradient of $D$. Moreover, our approach\nrequires relatively few environment interactions, and fewer hyper-parameters to\ntune. We test our method on the MuJoCo physics simulator and report initial\nresults that surpass the current state-of-the-art. \n\n"}
{"id": "1612.03316", "contents": "Title: Label Visualization and Exploration in IR Abstract: There is a renaissance in visual analytics systems for data analysis and\nsharing, in particular, in the current wave of big data applications. We\nintroduce RAVE, a prototype that automates the generation of an interface that\nuses facets and visualization techniques for exploring and analyzing relevance\nassessments data sets collected via crowdsourcing. We present a technical\ndescription of the main components and demonstrate its use. \n\n"}
{"id": "1612.03365", "contents": "Title: Multiple Instance Learning: A Survey of Problem Characteristics and\n  Applications Abstract: Multiple instance learning (MIL) is a form of weakly supervised learning\nwhere training instances are arranged in sets, called bags, and a label is\nprovided for the entire bag. This formulation is gaining interest because it\nnaturally fits various problems and allows to leverage weakly labeled data.\nConsequently, it has been used in diverse application fields such as computer\nvision and document classification. However, learning from bags raises\nimportant challenges that are unique to MIL. This paper provides a\ncomprehensive survey of the characteristics which define and differentiate the\ntypes of MIL problems. Until now, these problem characteristics have not been\nformally identified and described. As a result, the variations in performance\nof MIL algorithms from one data set to another are difficult to explain. In\nthis paper, MIL problem characteristics are grouped into four broad categories:\nthe composition of the bags, the types of data distribution, the ambiguity of\ninstance labels, and the task to be performed. Methods specialized to address\neach category are reviewed. Then, the extent to which these characteristics\nmanifest themselves in key MIL application areas are described. Finally,\nexperiments are conducted to compare the performance of 16 state-of-the-art MIL\nmethods on selected problem characteristics. This paper provides insight on how\nthe problem characteristics affect MIL algorithms, recommendations for future\nbenchmarking and promising avenues for research. \n\n"}
{"id": "1612.04111", "contents": "Title: Parsimonious Online Learning with Kernels via Sparse Projections in\n  Function Space Abstract: Despite their attractiveness, popular perception is that techniques for\nnonparametric function approximation do not scale to streaming data due to an\nintractable growth in the amount of storage they require. To solve this problem\nin a memory-affordable way, we propose an online technique based on functional\nstochastic gradient descent in tandem with supervised sparsification based on\ngreedy function subspace projections. The method, called parsimonious online\nlearning with kernels (POLK), provides a controllable tradeoff? between its\nsolution accuracy and the amount of memory it requires. We derive conditions\nunder which the generated function sequence converges almost surely to the\noptimal function, and we establish that the memory requirement remains finite.\nWe evaluate POLK for kernel multi-class logistic regression and kernel\nhinge-loss classification on three canonical data sets: a synthetic Gaussian\nmixture model, the MNIST hand-written digits, and the Brodatz texture database.\nOn all three tasks, we observe a favorable tradeoff of objective function\nevaluation, classification performance, and complexity of the nonparametric\nregressor extracted the proposed method. \n\n"}
{"id": "1612.04899", "contents": "Title: Semi-Supervised Phone Classification using Deep Neural Networks and\n  Stochastic Graph-Based Entropic Regularization Abstract: We describe a graph-based semi-supervised learning framework in the context\nof deep neural networks that uses a graph-based entropic regularizer to favor\nsmooth solutions over a graph induced by the data. The main contribution of\nthis work is a computationally efficient, stochastic graph-regularization\ntechnique that uses mini-batches that are consistent with the graph structure,\nbut also provides enough stochasticity (in terms of mini-batch data diversity)\nfor convergence of stochastic gradient descent methods to good solutions. For\nthis work, we focus on results of frame-level phone classification accuracy on\nthe TIMIT speech corpus but our method is general and scalable to much larger\ndata sets. Results indicate that our method significantly improves\nclassification accuracy compared to the fully-supervised case when the fraction\nof labeled data is low, and it is competitive with other methods in the fully\nlabeled case. \n\n"}
{"id": "1612.06083", "contents": "Title: Hierarchical Partitioning of the Output Space in Multi-label Data Abstract: Hierarchy Of Multi-label classifiers (HOMER) is a multi-label learning\nalgorithm that breaks the initial learning task to several, easier sub-tasks by\nfirst constructing a hierarchy of labels from a given label set and secondly\nemploying a given base multi-label classifier (MLC) to the resulting\nsub-problems. The primary goal is to effectively address class imbalance and\nscalability issues that often arise in real-world multi-label classification\nproblems. In this work, we present the general setup for a HOMER model and a\nsimple extension of the algorithm that is suited for MLCs that output rankings.\nFurthermore, we provide a detailed analysis of the properties of the algorithm,\nboth from an aspect of effectiveness and computational complexity. A secondary\ncontribution involves the presentation of a balanced variant of the k means\nalgorithm, which serves in the first step of the label hierarchy construction.\nWe conduct extensive experiments on six real-world datasets, studying\nempirically HOMER's parameters and providing examples of instantiations of the\nalgorithm with different clustering approaches and MLCs, The empirical results\ndemonstrate a significant improvement over the given base MLC. \n\n"}
{"id": "1612.06753", "contents": "Title: Video Stream Retrieval of Unseen Queries using Semantic Memory Abstract: Retrieval of live, user-broadcast video streams is an under-addressed and\nincreasingly relevant challenge. The on-line nature of the problem requires\ntemporal evaluation and the unforeseeable scope of potential queries motivates\nan approach which can accommodate arbitrary search queries. To account for the\nbreadth of possible queries, we adopt a no-example approach to query retrieval,\nwhich uses a query's semantic relatedness to pre-trained concept classifiers.\nTo adapt to shifting video content, we propose memory pooling and memory\nwelling methods that favor recent information over long past content. We\nidentify two stream retrieval tasks, instantaneous retrieval at any particular\ntime and continuous retrieval over a prolonged duration, and propose means for\nevaluating them. Three large scale video datasets are adapted to the challenge\nof stream retrieval. We report results for our search methods on the new stream\nretrieval tasks, as well as demonstrate their efficacy in a traditional,\nnon-streaming video task. \n\n"}
{"id": "1612.07993", "contents": "Title: RSSL: Semi-supervised Learning in R Abstract: In this paper, we introduce a package for semi-supervised learning research\nin the R programming language called RSSL. We cover the purpose of the package,\nthe methods it includes and comment on their use and implementation. We then\nshow, using several code examples, how the package can be used to replicate\nwell-known results from the semi-supervised learning literature. \n\n"}
{"id": "1701.00595", "contents": "Title: Leveraging Multi-aspect Time-related Influence in Location\n  Recommendation Abstract: Point-Of-Interest (POI) recommendation aims to mine a user's visiting history\nand find her/his potentially preferred places. Although location recommendation\nmethods have been studied and improved pervasively, the challenges w.r.t\nemploying various influences including temporal aspect still remain. Inspired\nby the fact that time includes numerous granular slots (e.g. minute, hour, day,\nweek and etc.), in this paper, we define a new problem to perform\nrecommendation through exploiting all diversified temporal factors. In\nparticular, we argue that most existing methods only focus on a limited number\nof time-related features and neglect others. Furthermore, considering a\nspecific granularity (e.g. time of a day) in recommendation cannot always apply\nto each user or each dataset. To address the challenges, we propose a\nprobabilistic generative model, named after Multi-aspect Time-related Influence\n(MATI) to promote POI recommendation. We also develop a novel optimization\nalgorithm based on Expectation Maximization (EM). Our MATI model firstly\ndetects a user's temporal multivariate orientation using her check-in log in\nLocation-based Social Networks(LBSNs). It then performs recommendation using\ntemporal correlations between the user and proposed locations. Our method is\nadaptable to various types of recommendation systems and can work efficiently\nin multiple time-scales. Extensive experimental results on two large-scale LBSN\ndatasets verify the effectiveness of our method over other competitors. \n\n"}
{"id": "1701.03006", "contents": "Title: Compressive Sensing via Convolutional Factor Analysis Abstract: We solve the compressive sensing problem via convolutional factor analysis,\nwhere the convolutional dictionaries are learned {\\em in situ} from the\ncompressed measurements. An alternating direction method of multipliers (ADMM)\nparadigm for compressive sensing inversion based on convolutional factor\nanalysis is developed. The proposed algorithm provides reconstructed images as\nwell as features, which can be directly used for recognition ($e.g.$,\nclassification) tasks. When a deep (multilayer) model is constructed, a\nstochastic unpooling process is employed to build a generative model. During\nreconstruction and testing, we project the upper layer dictionary to the data\nlevel and only a single layer deconvolution is required. We demonstrate that\nusing $\\sim30\\%$ (relative to pixel numbers) compressed measurements, the\nproposed model achieves the classification accuracy comparable to the original\ndata on MNIST. We also observe that when the compressed measurements are very\nlimited ($e.g.$, $<10\\%$), the upper layer dictionary can provide better\nreconstruction results than the bottom layer. \n\n"}
{"id": "1701.04783", "contents": "Title: Joint Deep Modeling of Users and Items Using Reviews for Recommendation Abstract: A large amount of information exists in reviews written by users. This source\nof information has been ignored by most of the current recommender systems\nwhile it can potentially alleviate the sparsity problem and improve the quality\nof recommendations. In this paper, we present a deep model to learn item\nproperties and user behaviors jointly from review text. The proposed model,\nnamed Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel\nneural networks coupled in the last layers. One of the networks focuses on\nlearning user behaviors exploiting reviews written by the user, and the other\none learns item properties from the reviews written for the item. A shared\nlayer is introduced on the top to couple these two networks together. The\nshared layer enables latent factors learned for users and items to interact\nwith each other in a manner similar to factorization machine techniques.\nExperimental results demonstrate that DeepCoNN significantly outperforms all\nbaseline recommender systems on a variety of datasets. \n\n"}
{"id": "1701.04862", "contents": "Title: Towards Principled Methods for Training Generative Adversarial Networks Abstract: The goal of this paper is not to introduce a single algorithm or method, but\nto make theoretical steps towards fully understanding the training dynamics of\ngenerative adversarial networks. In order to substantiate our theoretical\nanalysis, we perform targeted experiments to verify our assumptions, illustrate\nour claims, and quantify the phenomena. This paper is divided into three\nsections. The first section introduces the problem at hand. The second section\nis dedicated to studying and proving rigorously the problems including\ninstability and saturation that arize when training generative adversarial\nnetworks. The third section examines a practical and theoretically grounded\ndirection towards solving these problems, while introducing new tools to study\nthem. \n\n"}
{"id": "1701.04931", "contents": "Title: Characterizing Linguistic Attributes for Automatic Classification of\n  Intent Based Racist/Radicalized Posts on Tumblr Micro-Blogging Website Abstract: Research shows that many like-minded people use popular microblogging\nwebsites for posting hateful speech against various religions and race.\nAutomatic identification of racist and hate promoting posts is required for\nbuilding social media intelligence and security informatics based solutions.\nHowever, just keyword spotting based techniques cannot be used to accurately\nidentify the intent of a post. In this paper, we address the challenge of the\npresence of ambiguity in such posts by identifying the intent of author. We\nconduct our study on Tumblr microblogging website and develop a cascaded\nensemble learning classifier for identifying the posts having racist or\nradicalized intent. We train our model by identifying various semantic,\nsentiment and linguistic features from free-form text. Our experimental results\nshows that the proposed approach is effective and the emotion tone, social\ntendencies, language cues and personality traits of a narrative are\ndiscriminatory features for identifying the racist intent behind a post. \n\n"}
{"id": "1701.05265", "contents": "Title: Online Structure Learning for Sum-Product Networks with Gaussian Leaves Abstract: Sum-product networks have recently emerged as an attractive representation\ndue to their dual view as a special type of deep neural network with clear\nsemantics and a special type of probabilistic graphical model for which\ninference is always tractable. Those properties follow from some conditions\n(i.e., completeness and decomposability) that must be respected by the\nstructure of the network. As a result, it is not easy to specify a valid\nsum-product network by hand and therefore structure learning techniques are\ntypically used in practice. This paper describes the first online structure\nlearning technique for continuous SPNs with Gaussian leaves. We also introduce\nan accompanying new parameter learning technique. \n\n"}
{"id": "1701.07795", "contents": "Title: Match-Tensor: a Deep Relevance Model for Search Abstract: The application of Deep Neural Networks for ranking in search engines may\nobviate the need for the extensive feature engineering common to current\nlearning-to-rank methods. However, we show that combining simple relevance\nmatching features like BM25 with existing Deep Neural Net models often\nsubstantially improves the accuracy of these models, indicating that they do\nnot capture essential local relevance matching signals. We describe a novel\ndeep Recurrent Neural Net-based model that we call Match-Tensor. The\narchitecture of the Match-Tensor model simultaneously accounts for both local\nrelevance matching and global topicality signals allowing for a rich interplay\nbetween them when computing the relevance of a document to a query. On a large\nheld-out test set consisting of social media documents, we demonstrate not only\nthat Match-Tensor outperforms BM25 and other classes of DNNs but also that it\nlargely subsumes signals present in these models. \n\n"}
{"id": "1701.08511", "contents": "Title: Binary adaptive embeddings from order statistics of random projections Abstract: We use some of the largest order statistics of the random projections of a\nreference signal to construct a binary embedding that is adapted to signals\ncorrelated with such signal. The embedding is characterized from the analytical\nstandpoint and shown to provide improved performance on tasks such as\nclassification in a reduced-dimensionality space. \n\n"}
{"id": "1702.00317", "contents": "Title: On SGD's Failure in Practice: Characterizing and Overcoming Stalling Abstract: Stochastic Gradient Descent (SGD) is widely used in machine learning problems\nto efficiently perform empirical risk minimization, yet, in practice, SGD is\nknown to stall before reaching the actual minimizer of the empirical risk. SGD\nstalling has often been attributed to its sensitivity to the conditioning of\nthe problem; however, as we demonstrate, SGD will stall even when applied to a\nsimple linear regression problem with unity condition number for standard\nlearning rates. Thus, in this work, we numerically demonstrate and\nmathematically argue that stalling is a crippling and generic limitation of SGD\nand its variants in practice. Once we have established the problem of stalling,\nwe generalize an existing framework for hedging against its effects, which (1)\ndeters SGD and its variants from stalling, (2) still provides convergence\nguarantees, and (3) makes SGD and its variants more practical methods for\nminimization. \n\n"}
{"id": "1702.01717", "contents": "Title: Search Intelligence: Deep Learning For Dominant Category Prediction Abstract: Deep Neural Networks, and specifically fully-connected convolutional neural\nnetworks are achieving remarkable results across a wide variety of domains.\nThey have been trained to achieve state-of-the-art performance when applied to\nproblems such as speech recognition, image classification, natural language\nprocessing and bioinformatics. Most of these deep learning models when applied\nto classification employ the softmax activation function for prediction and aim\nto minimize cross-entropy loss. In this paper, we have proposed a supervised\nmodel for dominant category prediction to improve search recall across all eBay\nclassifieds platforms. The dominant category label for each query in the last\n90 days is first calculated by summing the total number of collaborative clicks\namong all categories. The category having the highest number of collaborative\nclicks for the given query will be considered its dominant category. Second,\neach query is transformed to a numeric vector by mapping each unique word in\nthe query document to a unique integer value; all padded to equal length based\non the maximum document length within the pre-defined vocabulary size. A\nfully-connected deep convolutional neural network (CNN) is then applied for\nclassification. The proposed model achieves very high classification accuracy\ncompared to other state-of-the-art machine learning techniques. \n\n"}
{"id": "1702.01824", "contents": "Title: Predicting Pairwise Relations with Neural Similarity Encoders Abstract: Matrix factorization is at the heart of many machine learning algorithms, for\nexample, dimensionality reduction (e.g. kernel PCA) or recommender systems\nrelying on collaborative filtering. Understanding a singular value\ndecomposition (SVD) of a matrix as a neural network optimization problem\nenables us to decompose large matrices efficiently while dealing naturally with\nmissing values in the given matrix. But most importantly, it allows us to learn\nthe connection between data points' feature vectors and the matrix containing\ninformation about their pairwise relations. In this paper we introduce a novel\nneural network architecture termed Similarity Encoder (SimEc), which is\ndesigned to simultaneously factorize a given target matrix while also learning\nthe mapping to project the data points' feature vectors into a similarity\npreserving embedding space. This makes it possible to, for example, easily\ncompute out-of-sample solutions for new data points. Additionally, we\ndemonstrate that SimEc can preserve non-metric similarities and even predict\nmultiple pairwise relations between data points at once. \n\n"}
{"id": "1702.03222", "contents": "Title: Mining Electronic Health Records: A Survey Abstract: The continuously increasing cost of the US healthcare system has received\nsignificant attention. Central to the ideas aimed at curbing this trend is the\nuse of technology, in the form of the mandate to implement electronic health\nrecords (EHRs). EHRs consist of patient information such as demographics,\nmedications, laboratory test results, diagnosis codes and procedures. Mining\nEHRs could lead to improvement in patient health management as EHRs contain\ndetailed information related to disease prognosis for large patient\npopulations. In this manuscript, we provide a structured and comprehensive\noverview of data mining techniques for modeling EHR data. We first provide a\ndetailed understanding of the major application areas to which EHR mining has\nbeen applied and then discuss the nature of EHR data and its accompanying\nchallenges. Next, we describe major approaches used for EHR mining, the metrics\nassociated with EHRs, and the various study designs. With this foundation, we\nthen provide a systematic and methodological organization of existing data\nmining techniques used to model EHRs and discuss ideas for future research. We\nconclude this survey with a comprehensive summary of clinical data mining\napplications of EHR data, as illustrated in the online supplement. \n\n"}
{"id": "1702.03334", "contents": "Title: Batch Policy Gradient Methods for Improving Neural Conversation Models Abstract: We study reinforcement learning of chatbots with recurrent neural network\narchitectures when the rewards are noisy and expensive to obtain. For instance,\na chatbot used in automated customer service support can be scored by quality\nassurance agents, but this process can be expensive, time consuming and noisy.\nPrevious reinforcement learning work for natural language processing uses\non-policy updates and/or is designed for on-line learning settings. We\ndemonstrate empirically that such strategies are not appropriate for this\nsetting and develop an off-policy batch policy gradient method (BPG). We\ndemonstrate the efficacy of our method via a series of synthetic experiments\nand an Amazon Mechanical Turk experiment on a restaurant recommendations\ndataset. \n\n"}
{"id": "1702.03859", "contents": "Title: Offline bilingual word vectors, orthogonal transformations and the\n  inverted softmax Abstract: Usually bilingual word vectors are trained \"online\". Mikolov et al. showed\nthey can also be found \"offline\", whereby two pre-trained embeddings are\naligned with a linear transformation, using dictionaries compiled from expert\nknowledge. In this work, we prove that the linear transformation between two\nspaces should be orthogonal. This transformation can be obtained using the\nsingular value decomposition. We introduce a novel \"inverted softmax\" for\nidentifying translation pairs, with which we improve the precision @1 of\nMikolov's original mapping from 34% to 43%, when translating a test set\ncomposed of both common and rare English words into Italian. Orthogonal\ntransformations are more robust to noise, enabling us to learn the\ntransformation without expert bilingual signal by constructing a\n\"pseudo-dictionary\" from the identical character strings which appear in both\nlanguages, achieving 40% precision on the same test set. Finally, we extend our\nmethod to retrieve the true translations of English sentences from a corpus of\n200k Italian sentences with a precision @1 of 68%. \n\n"}
{"id": "1702.04690", "contents": "Title: Simple rules for complex decisions Abstract: From doctors diagnosing patients to judges setting bail, experts often base\ntheir decisions on experience and intuition rather than on statistical models.\nWhile understandable, relying on intuition over models has often been found to\nresult in inferior outcomes. Here we present a new method,\nselect-regress-and-round, for constructing simple rules that perform well for\ncomplex decisions. These rules take the form of a weighted checklist, can be\napplied mentally, and nonetheless rival the performance of modern machine\nlearning algorithms. Our method for creating these rules is itself simple, and\ncan be carried out by practitioners with basic statistics knowledge. We\ndemonstrate this technique with a detailed case study of judicial decisions to\nrelease or detain defendants while they await trial. In this application, as in\nmany policy settings, the effects of proposed decision rules cannot be directly\nobserved from historical data: if a rule recommends releasing a defendant that\nthe judge in reality detained, we do not observe what would have happened under\nthe proposed action. We address this key counterfactual estimation problem by\ndrawing on tools from causal inference. We find that simple rules significantly\noutperform judges and are on par with decisions derived from random forests\ntrained on all available features. Generalizing to 22 varied decision-making\ndomains, we find this basic result replicates. We conclude with an analytical\nframework that helps explain why these simple decision rules perform as well as\nthey do. \n\n"}
{"id": "1702.04832", "contents": "Title: Dynamic Partition Models Abstract: We present a new approach for learning compact and intuitive distributed\nrepresentations with binary encoding. Rather than summing up expert votes as in\nproducts of experts, we employ for each variable the opinion of the most\nreliable expert. Data points are hence explained through a partitioning of the\nvariables into expert supports. The partitions are dynamically adapted based on\nwhich experts are active. During the learning phase we adopt a smoothed version\nof this model that uses separate mixtures for each data dimension. In our\nexperiments we achieve accurate reconstructions of high-dimensional data points\nwith at most a dozen experts. \n\n"}
{"id": "1702.04837", "contents": "Title: Sketched Ridge Regression: Optimization Perspective, Statistical\n  Perspective, and Model Averaging Abstract: We address the statistical and optimization impacts of the classical sketch\nand Hessian sketch used to approximately solve the Matrix Ridge Regression\n(MRR) problem. Prior research has quantified the effects of classical sketch on\nthe strictly simpler least squares regression (LSR) problem. We establish that\nclassical sketch has a similar effect upon the optimization properties of MRR\nas it does on those of LSR: namely, it recovers nearly optimal solutions. By\ncontrast, Hessian sketch does not have this guarantee, instead, the\napproximation error is governed by a subtle interplay between the \"mass\" in the\nresponses and the optimal objective value.\n  For both types of approximation, the regularization in the sketched MRR\nproblem results in significantly different statistical properties from those of\nthe sketched LSR problem. In particular, there is a bias-variance trade-off in\nsketched MRR that is not present in sketched LSR. We provide upper and lower\nbounds on the bias and variance of sketched MRR, these bounds show that\nclassical sketch significantly increases the variance, while Hessian sketch\nsignificantly increases the bias. Empirically, sketched MRR solutions can have\nrisks that are higher by an order-of-magnitude than those of the optimal MRR\nsolutions.\n  We establish theoretically and empirically that model averaging greatly\ndecreases the gap between the risks of the true and sketched solutions to the\nMRR problem. Thus, in parallel or distributed settings, sketching combined with\nmodel averaging is a powerful technique that quickly obtains near-optimal\nsolutions to the MRR problem while greatly mitigating the increased statistical\nrisk incurred by sketching. \n\n"}
{"id": "1702.06383", "contents": "Title: Differential Geometric Retrieval of Deep Features Abstract: Comparing images to recommend items from an image-inventory is a subject of\ncontinued interest. Added with the scalability of deep-learning architectures\nthe once `manual' job of hand-crafting features have been largely alleviated,\nand images can be compared according to features generated from a deep\nconvolutional neural network. In this paper, we compare distance metrics (and\ndivergences) to rank features generated from a neural network, for\ncontent-based image retrieval. Specifically, after modelling individual images\nusing approximations of mixture models or sparse covariance estimators, we\nresort to their information-theoretic and Riemann geometric comparisons. We\nshow that using approximations of mixture models enable us to compute a\ndistance measure based on the Wasserstein metric that requires less effort than\nother computationally intensive optimal transport plans; finally, an affine\ninvariant metric is used to compare the optimal transport metric to its Riemann\ngeometric counterpart -- we conclude that although expensive, retrieval metric\nbased on Wasserstein geometry is more suitable than information theoretic\ncomparison of images. In short, we combine GPU scalability in learning deep\nfeature vectors with statistically efficient metrics that we foresee being\nutilised in a commercial setting. \n\n"}
{"id": "1702.06861", "contents": "Title: On the Power of Truncated SVD for General High-rank Matrix Estimation\n  Problems Abstract: We show that given an estimate $\\widehat{A}$ that is close to a general\nhigh-rank positive semi-definite (PSD) matrix $A$ in spectral norm (i.e.,\n$\\|\\widehat{A}-A\\|_2 \\leq \\delta$), the simple truncated SVD of $\\widehat{A}$\nproduces a multiplicative approximation of $A$ in Frobenius norm. This\nobservation leads to many interesting results on general high-rank matrix\nestimation problems, which we briefly summarize below ($A$ is an $n\\times n$\nhigh-rank PSD matrix and $A_k$ is the best rank-$k$ approximation of $A$):\n  (1) High-rank matrix completion: By observing\n$\\Omega(\\frac{n\\max\\{\\epsilon^{-4},k^2\\}\\mu_0^2\\|A\\|_F^2\\log\nn}{\\sigma_{k+1}(A)^2})$ elements of $A$ where $\\sigma_{k+1}\\left(A\\right)$ is\nthe $\\left(k+1\\right)$-th singular value of $A$ and $\\mu_0$ is the incoherence,\nthe truncated SVD on a zero-filled matrix satisfies $\\|\\widehat{A}_k-A\\|_F \\leq\n(1+O(\\epsilon))\\|A-A_k\\|_F$ with high probability.\n  (2)High-rank matrix de-noising: Let $\\widehat{A}=A+E$ where $E$ is a Gaussian\nrandom noise matrix with zero mean and $\\nu^2/n$ variance on each entry. Then\nthe truncated SVD of $\\widehat{A}$ satisfies $\\|\\widehat{A}_k-A\\|_F \\leq\n(1+O(\\sqrt{\\nu/\\sigma_{k+1}(A)}))\\|A-A_k\\|_F + O(\\sqrt{k}\\nu)$.\n  (3) Low-rank Estimation of high-dimensional covariance: Given $N$\ni.i.d.~samples $X_1,\\cdots,X_N\\sim\\mathcal N_n(0,A)$, can we estimate $A$ with\na relative-error Frobenius norm bound? We show that if $N =\n\\Omega\\left(n\\max\\{\\epsilon^{-4},k^2\\}\\gamma_k(A)^2\\log N\\right)$ for\n$\\gamma_k(A)=\\sigma_1(A)/\\sigma_{k+1}(A)$, then $\\|\\widehat{A}_k-A\\|_F \\leq\n(1+O(\\epsilon))\\|A-A_k\\|_F$ with high probability, where\n$\\widehat{A}=\\frac{1}{N}\\sum_{i=1}^N{X_iX_i^\\top}$ is the sample covariance. \n\n"}
{"id": "1702.07021", "contents": "Title: One Size Fits Many: Column Bundle for Multi-X Learning Abstract: Much recent machine learning research has been directed towards leveraging\nshared statistics among labels, instances and data views, commonly referred to\nas multi-label, multi-instance and multi-view learning. The underlying premises\nare that there exist correlations among input parts and among output targets,\nand the predictive performance would increase when the correlations are\nincorporated. In this paper, we propose Column Bundle (CLB), a novel deep\nneural network for capturing the shared statistics in data. CLB is generic that\nthe same architecture can be applied for various types of shared statistics by\nchanging only input and output handling. CLB is capable of scaling to thousands\nof input parts and output labels by avoiding explicit modeling of pairwise\nrelations. We evaluate CLB on different types of data: (a) multi-label, (b)\nmulti-view, (c) multi-view/multi-label and (d) multi-instance. CLB demonstrates\na comparable and competitive performance in all datasets against\nstate-of-the-art methods designed specifically for each type. \n\n"}
{"id": "1702.07186", "contents": "Title: Stability of Topic Modeling via Matrix Factorization Abstract: Topic models can provide us with an insight into the underlying latent\nstructure of a large corpus of documents. A range of methods have been proposed\nin the literature, including probabilistic topic models and techniques based on\nmatrix factorization. However, in both cases, standard implementations rely on\nstochastic elements in their initialization phase, which can potentially lead\nto different results being generated on the same corpus when using the same\nparameter values. This corresponds to the concept of \"instability\" which has\npreviously been studied in the context of $k$-means clustering. In many\napplications of topic modeling, this problem of instability is not considered\nand topic models are treated as being definitive, even though the results may\nchange considerably if the initialization process is altered. In this paper we\ndemonstrate the inherent instability of popular topic modeling approaches,\nusing a number of new measures to assess stability. To address this issue in\nthe context of matrix factorization for topic modeling, we propose the use of\nensemble learning strategies. Based on experiments performed on annotated text\ncorpora, we show that a K-Fold ensemble strategy, combining both ensembles and\nstructured initialization, can significantly reduce instability, while\nsimultaneously yielding more accurate topic models. \n\n"}
{"id": "1702.07680", "contents": "Title: Consistent Alignment of Word Embedding Models Abstract: Word embedding models offer continuous vector representations that can\ncapture rich contextual semantics based on their word co-occurrence patterns.\nWhile these word vectors can provide very effective features used in many NLP\ntasks such as clustering similar words and inferring learning relationships,\nmany challenges and open research questions remain. In this paper, we propose a\nsolution that aligns variations of the same model (or different models) in a\njoint low-dimensional latent space leveraging carefully generated synthetic\ndata points. This generative process is inspired by the observation that a\nvariety of linguistic relationships is captured by simple linear operations in\nembedded space. We demonstrate that our approach can lead to substantial\nimprovements in recovering embeddings of local neighborhoods. \n\n"}
{"id": "1702.08343", "contents": "Title: Approximate Inference with Amortised MCMC Abstract: We propose a novel approximate inference algorithm that approximates a target\ndistribution by amortising the dynamics of a user-selected MCMC sampler. The\nidea is to initialise MCMC using samples from an approximation network, apply\nthe MCMC operator to improve these samples, and finally use the samples to\nupdate the approximation network thereby improving its quality. This provides a\nnew generic framework for approximate inference, allowing us to deploy highly\ncomplex, or implicitly defined approximation families with intractable\ndensities, including approximations produced by warping a source of randomness\nthrough a deep neural network. Experiments consider image modelling with deep\ngenerative models as a challenging test for the method. Deep models trained\nusing amortised MCMC are shown to generate realistic looking samples as well as\nproducing diverse imputations for images with regions of missing pixels. \n\n"}
{"id": "1702.08608", "contents": "Title: Towards A Rigorous Science of Interpretable Machine Learning Abstract: As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning. \n\n"}
{"id": "1702.08651", "contents": "Title: Speeding Up Latent Variable Gaussian Graphical Model Estimation via\n  Nonconvex Optimizations Abstract: We study the estimation of the latent variable Gaussian graphical model\n(LVGGM), where the precision matrix is the superposition of a sparse matrix and\na low-rank matrix. In order to speed up the estimation of the sparse plus\nlow-rank components, we propose a sparsity constrained maximum likelihood\nestimator based on matrix factorization, and an efficient alternating gradient\ndescent algorithm with hard thresholding to solve it. Our algorithm is orders\nof magnitude faster than the convex relaxation based methods for LVGGM. In\naddition, we prove that our algorithm is guaranteed to linearly converge to the\nunknown sparse and low-rank components up to the optimal statistical precision.\nExperiments on both synthetic and genomic data demonstrate the superiority of\nour algorithm over the state-of-the-art algorithms and corroborate our theory. \n\n"}
{"id": "1703.00839", "contents": "Title: Encrypted accelerated least squares regression Abstract: Information that is stored in an encrypted format is, by definition, usually\nnot amenable to statistical analysis or machine learning methods. In this paper\nwe present detailed analysis of coordinate and accelerated gradient descent\nalgorithms which are capable of fitting least squares and penalised ridge\nregression models, using data encrypted under a fully homomorphic encryption\nscheme. Gradient descent is shown to dominate in terms of encrypted\ncomputational speed, and theoretical results are proven to give parameter\nbounds which ensure correctness of decryption. The characteristics of encrypted\ncomputation are empirically shown to favour a non-standard acceleration\ntechnique. This demonstrates the possibility of approximating conventional\nstatistical regression methods using encrypted data without compromising\nprivacy. \n\n"}
{"id": "1703.01234", "contents": "Title: A Bayesian computer model analysis of Robust Bayesian analyses Abstract: We harness the power of Bayesian emulation techniques, designed to aid the\nanalysis of complex computer models, to examine the structure of complex\nBayesian analyses themselves. These techniques facilitate robust Bayesian\nanalyses and/or sensitivity analyses of complex problems, and hence allow\nglobal exploration of the impacts of choices made in both the likelihood and\nprior specification. We show how previously intractable problems in robustness\nstudies can be overcome using emulation techniques, and how these methods allow\nother scientists to quickly extract approximations to posterior results\ncorresponding to their own particular subjective specification. The utility and\nflexibility of our method is demonstrated on a reanalysis of a real application\nwhere Bayesian methods were employed to capture beliefs about river flow. We\ndiscuss the obvious extensions and directions of future research that such an\napproach opens up. \n\n"}
{"id": "1703.02102", "contents": "Title: Revisiting stochastic off-policy action-value gradients Abstract: Off-policy stochastic actor-critic methods rely on approximating the\nstochastic policy gradient in order to derive an optimal policy. One may also\nderive the optimal policy by approximating the action-value gradient. The use\nof action-value gradients is desirable as policy improvement occurs along the\ndirection of steepest ascent. This has been studied extensively within the\ncontext of natural gradient actor-critic algorithms and more recently within\nthe context of deterministic policy gradients. In this paper we briefly discuss\nthe off-policy stochastic counterpart to deterministic action-value gradients,\nas well as an incremental approach for following the policy gradient in lieu of\nthe natural gradient. \n\n"}
{"id": "1703.03020", "contents": "Title: Spectral Graph Convolutions for Population-based Disease Prediction Abstract: Exploiting the wealth of imaging and non-imaging information for disease\nprediction tasks requires models capable of representing, at the same time,\nindividual features as well as data associations between subjects from\npotentially large populations. Graphs provide a natural framework for such\ntasks, yet previous graph-based approaches focus on pairwise similarities\nwithout modelling the subjects' individual characteristics and features. On the\nother hand, relying solely on subject-specific imaging feature vectors fails to\nmodel the interaction and similarity between subjects, which can reduce\nperformance. In this paper, we introduce the novel concept of Graph\nConvolutional Networks (GCN) for brain analysis in populations, combining\nimaging and non-imaging data. We represent populations as a sparse graph where\nits vertices are associated with image-based feature vectors and the edges\nencode phenotypic information. This structure was used to train a GCN model on\npartially labelled graphs, aiming to infer the classes of unlabelled nodes from\nthe node features and pairwise associations between subjects. We demonstrate\nthe potential of the method on the challenging ADNI and ABIDE databases, as a\nproof of concept of the benefit from integrating contextual information in\nclassification tasks. This has a clear impact on the quality of the\npredictions, leading to 69.5% accuracy for ABIDE (outperforming the current\nstate of the art of 66.8%) and 77% for ADNI for prediction of MCI conversion,\nsignificantly outperforming standard linear classifiers where only individual\nfeatures are considered. \n\n"}
{"id": "1703.03112", "contents": "Title: Dynamic Intention-Aware Recommendation System Abstract: Recommender systems have been actively and extensively studied over past\ndecades. In the meanwhile, the boom of Big Data is driving fundamental changes\nin the development of recommender systems. In this paper, we propose a dynamic\nintention-aware recommender system to better facilitate users to find desirable\nproducts and services. Compare to prior work, our proposal possesses the\nfollowing advantages: (1) it takes user intentions and demands into account\nthrough intention mining techniques. By unearthing user intentions from the\nhistorical user-item interactions, and various user digital traces harvested\nfrom social media and Internet of Things, it is capable of delivering more\nsatisfactory recommendations by leveraging rich online and offline user data;\n(2) it embraces the benefits of embedding heterogeneous source information and\nshared representations of multiple domains to provide accurate and effective\nrecommendations comprehensively; (3) it recommends products or services\nproactively and timely by capturing the dynamic influences, which can\nsignificantly reduce user involvements and efforts. \n\n"}
{"id": "1703.04247", "contents": "Title: DeepFM: A Factorization-Machine based Neural Network for CTR Prediction Abstract: Learning sophisticated feature interactions behind user behaviors is critical\nin maximizing CTR for recommender systems. Despite great progress, existing\nmethods seem to have a strong bias towards low- or high-order interactions, or\nrequire expertise feature engineering. In this paper, we show that it is\npossible to derive an end-to-end learning model that emphasizes both low- and\nhigh-order feature interactions. The proposed model, DeepFM, combines the power\nof factorization machines for recommendation and deep learning for feature\nlearning in a new neural network architecture. Compared to the latest Wide \\&\nDeep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\"\nparts, with no need of feature engineering besides raw features. Comprehensive\nexperiments are conducted to demonstrate the effectiveness and efficiency of\nDeepFM over the existing models for CTR prediction, on both benchmark data and\ncommercial data. \n\n"}
{"id": "1703.04854", "contents": "Title: Distributed-Representation Based Hybrid Recommender System with Short\n  Item Descriptions Abstract: Collaborative filtering (CF) aims to build a model from users' past behaviors\nand/or similar decisions made by other users, and use the model to recommend\nitems for users. Despite of the success of previous collaborative filtering\napproaches, they are all based on the assumption that there are sufficient\nrating scores available for building high-quality recommendation models. In\nreal world applications, however, it is often difficult to collect sufficient\nrating scores, especially when new items are introduced into the system, which\nmakes the recommendation task challenging. We find that there are often \"short\"\ntexts describing features of items, based on which we can approximate the\nsimilarity of items and make recommendation together with rating scores. In\nthis paper we \"borrow\" the idea of vector representation of words to capture\nthe information of short texts and embed it into a matrix factorization\nframework. We empirically show that our approach is effective by comparing it\nwith state-of-the-art approaches. \n\n"}
{"id": "1703.09207", "contents": "Title: Fairness in Criminal Justice Risk Assessments: The State of the Art Abstract: Objectives: Discussions of fairness in criminal justice risk assessments\ntypically lack conceptual precision. Rhetoric too often substitutes for careful\nanalysis. In this paper, we seek to clarify the tradeoffs between different\nkinds of fairness and between fairness and accuracy.\n  Methods: We draw on the existing literatures in criminology, computer science\nand statistics to provide an integrated examination of fairness and accuracy in\ncriminal justice risk assessments. We also provide an empirical illustration\nusing data from arraignments.\n  Results: We show that there are at least six kinds of fairness, some of which\nare incompatible with one another and with accuracy.\n  Conclusions: Except in trivial cases, it is impossible to maximize accuracy\nand fairness at the same time, and impossible simultaneously to satisfy all\nkinds of fairness. In practice, a major complication is different base rates\nacross different legally protected groups. There is a need to consider\nchallenging tradeoffs. \n\n"}
{"id": "1703.09766", "contents": "Title: Unifying the Stochastic Spectral Descent for Restricted Boltzmann\n  Machines with Bernoulli or Gaussian Inputs Abstract: Stochastic gradient descent based algorithms are typically used as the\ngeneral optimization tools for most deep learning models. A Restricted\nBoltzmann Machine (RBM) is a probabilistic generative model that can be stacked\nto construct deep architectures. For RBM with Bernoulli inputs, non-Euclidean\nalgorithm such as stochastic spectral descent (SSD) has been specifically\ndesigned to speed up the convergence with improved use of the gradient\nestimation by sampling methods. However, the existing algorithm and\ncorresponding theoretical justification depend on the assumption that the\npossible configurations of inputs are finite, like binary variables. The\npurpose of this paper is to generalize SSD for Gaussian RBM being capable of\nmod- eling continuous data, regardless of the previous assumption. We propose\nthe gradient descent methods in non-Euclidean space of parameters, via de-\nriving the upper bounds of logarithmic partition function for RBMs based on\nSchatten-infinity norm. We empirically show that the advantage and improvement\nof SSD over stochastic gradient descent (SGD). \n\n"}
{"id": "1703.09845", "contents": "Title: Bringing Salary Transparency to the World: Computing Robust Compensation\n  Insights via LinkedIn Salary Abstract: The recently launched LinkedIn Salary product has been designed with the goal\nof providing compensation insights to the world's professionals and thereby\nhelping them optimize their earning potential. We describe the overall design\nand architecture of the statistical modeling system underlying this product. We\nfocus on the unique data mining challenges while designing and implementing the\nsystem, and describe the modeling components such as Bayesian hierarchical\nsmoothing that help to compute and present robust compensation insights to\nusers. We report on extensive evaluation with nearly one year of de-identified\ncompensation data collected from over one million LinkedIn users, thereby\ndemonstrating the efficacy of the statistical models. We also highlight the\nlessons learned through the deployment of our system at LinkedIn. \n\n"}
{"id": "1703.09956", "contents": "Title: Marginal likelihood based model comparison in Fuzzy Bayesian Learning Abstract: In a recent paper [1] we introduced the Fuzzy Bayesian Learning (FBL)\nparadigm where expert opinions can be encoded in the form of fuzzy rule bases\nand the hyper-parameters of the fuzzy sets can be learned from data using a\nBayesian approach. The present paper extends this work for selecting the most\nappropriate rule base among a set of competing alternatives, which best\nexplains the data, by calculating the model evidence or marginal likelihood. We\nexplain why this is an attractive alternative over simply minimizing a mean\nsquared error metric of prediction and show the validity of the proposition\nusing synthetic examples and a real world case study in the financial services\nsector. \n\n"}
{"id": "1704.01610", "contents": "Title: A Subjective Logic Formalisation of the Principle of Polyrepresentation\n  for Information Needs Abstract: Interactive Information Retrieval refers to the branch of Information\nRetrieval that considers the retrieval process with respect to a wide range of\ncontexts, which may affect the user's information seeking experience. The\nidentification and representation of such contexts has been the object of the\nprinciple of Polyrepresentation, a theoretical framework for reasoning about\ndifferent representations arising from interactive information retrieval in a\ngiven context. Although the principle of Polyrepresentation has received\nattention from many researchers, not much empirical work has been done based on\nit. One reason may be that it has not yet been formalised mathematically. In\nthis paper we propose an up-to-date and exible mathematical formalisation of\nthe principle of Polyrepresentation for information needs. Specifically, we\napply Subjective Logic to model different representations of information needs\nas beliefs marked by degrees of uncertainty. We combine such beliefs using\ndifferent logical operators, and we discuss these combinations with respect to\ndifferent retrieval scenarios and situations. A formal model is introduced and\ndiscussed, with illustrative applications to the modelling of information\nneeds. \n\n"}
{"id": "1704.01701", "contents": "Title: Learning Certifiably Optimal Rule Lists for Categorical Data Abstract: We present the design and implementation of a custom discrete optimization\ntechnique for building rule lists over a categorical feature space. Our\nalgorithm produces rule lists with optimal training performance, according to\nthe regularized empirical risk, with a certificate of optimality. By leveraging\nalgorithmic bounds, efficient data structures, and computational reuse, we\nachieve several orders of magnitude speedup in time and a massive reduction of\nmemory consumption. We demonstrate that our approach produces optimal rule\nlists on practical problems in seconds. Our results indicate that it is\npossible to construct optimal sparse rule lists that are approximately as\naccurate as the COMPAS proprietary risk prediction tool on data from Broward\nCounty, Florida, but that are completely interpretable. This framework is a\nnovel alternative to CART and other decision tree methods for interpretable\nmodeling. \n\n"}
{"id": "1704.02532", "contents": "Title: Deep Reinforcement Learning framework for Autonomous Driving Abstract: Reinforcement learning is considered to be a strong AI paradigm which can be\nused to teach machines through interaction with the environment and learning\nfrom their mistakes. Despite its perceived utility, it has not yet been\nsuccessfully applied in automotive applications. Motivated by the successful\ndemonstrations of learning of Atari games and Go by Google DeepMind, we propose\na framework for autonomous driving using deep reinforcement learning. This is\nof particular relevance as it is difficult to pose autonomous driving as a\nsupervised learning problem due to strong interactions with the environment\nincluding other vehicles, pedestrians and roadworks. As it is a relatively new\narea of research for autonomous driving, we provide a short overview of deep\nreinforcement learning and then describe our proposed framework. It\nincorporates Recurrent Neural Networks for information integration, enabling\nthe car to handle partially observable scenarios. It also integrates the recent\nwork on attention models to focus on relevant information, thereby reducing the\ncomputational complexity for deployment on embedded hardware. The framework was\ntested in an open source 3D car racing simulator called TORCS. Our simulation\nresults demonstrate learning of autonomous maneuvering in a scenario of complex\nroad curvatures and simple interaction of other vehicles. \n\n"}
{"id": "1704.03507", "contents": "Title: Unsupervised Learning of Parsimonious General-Purpose Embeddings for\n  User and Location Modelling Abstract: Many social network applications depend on robust representations of\nspatio-temporal data. In this work, we present an embedding model based on\nfeed-forward neural networks which transforms social media check-ins into dense\nfeature vectors encoding geographic, temporal, and functional aspects for\nmodelling places, neighborhoods, and users. We employ the embedding model in a\nvariety of applications including location recommendation, urban functional\nzone study, and crime prediction. For location recommendation, we propose a\nSpatio-Temporal Embedding Similarity algorithm (STES) based on the embedding\nmodel.\n  In a range of experiments on real life data collected from Foursquare, we\ndemonstrate our model's effectiveness at characterizing places and people and\nits applicability in aforementioned problem domains. Finally, we select eight\nmajor cities around the globe and verify the robustness and generality of our\nmodel by porting pre-trained models from one city to another, thereby\nalleviating the need for costly local training. \n\n"}
{"id": "1704.03755", "contents": "Title: Unsupervised part learning for visual recognition Abstract: Part-based image classification aims at representing categories by small sets\nof learned discriminative parts, upon which an image representation is built.\nConsidered as a promising avenue a decade ago, this direction has been\nneglected since the advent of deep neural networks. In this context, this paper\nbrings two contributions: first, it shows that despite the recent success of\nend-to-end holistic models, explicit part learning can boosts classification\nperformance. Second, this work proceeds one step further than recent part-based\nmodels (PBM), focusing on how to learn parts without using any labeled data.\nInstead of learning a set of parts per class, as generally done in the PBM\nliterature, the proposed approach both constructs a partition of a given set of\nimages into visually similar groups, and subsequently learn a set of\ndiscriminative parts per group in a fully unsupervised fashion. This strategy\nopens the door to the use of PBM in new applications for which the notion of\nimage categories is irrelevant, such as instance-based image retrieval, for\nexample. We experimentally show that our learned parts can help building\nefficient image representations, for classification as well as for indexing\ntasks, resulting in performance superior to holistic state-of-the art Deep\nConvolutional Neural Networks (DCNN) encoding. \n\n"}
{"id": "1704.04567", "contents": "Title: Asynchronous Parallel Empirical Variance Guided Algorithms for the\n  Thresholding Bandit Problem Abstract: This paper considers the multi-armed thresholding bandit problem --\nidentifying all arms whose expected rewards are above a predefined threshold\nvia as few pulls (or rounds) as possible -- proposed by Locatelli et al. [2016]\nrecently. Although the proposed algorithm in Locatelli et al. [2016] achieves\nthe optimal round complexity in a certain sense, there still remain unsolved\nissues. This paper proposes an asynchronous parallel thresholding algorithm and\nits parameter-free version to improve the efficiency and the applicability. On\none hand, the proposed two algorithms use the empirical variance to guide the\npull decision at each round, and significantly improve the round complexity of\nthe \"optimal\" algorithm when all arms have bounded high order moments. The\nproposed algorithms can be proven to be optimal. On the other hand, most bandit\nalgorithms assume that the reward can be observed immediately after the pull or\nthe next decision would not be made before all rewards are observed. Our\nproposed asynchronous parallel algorithms allow making the choice of the next\npull with unobserved rewards from earlier pulls, which avoids such an\nunrealistic assumption and significantly improves the identification process.\nOur theoretical analysis justifies the effectiveness and the efficiency of\nproposed asynchronous parallel algorithms. \n\n"}
{"id": "1704.04718", "contents": "Title: Deep Learning Based Regression and Multi-class Models for Acute Oral\n  Toxicity Prediction with Automatic Chemical Feature Extraction Abstract: For quantitative structure-property relationship (QSPR) studies in\nchemoinformatics, it is important to get interpretable relationship between\nchemical properties and chemical features. However, the predictive power and\ninterpretability of QSPR models are usually two different objectives that are\ndifficult to achieve simultaneously. A deep learning architecture using\nmolecular graph encoding convolutional neural networks (MGE-CNN) provided a\nuniversal strategy to construct interpretable QSPR models with high predictive\npower. Instead of using application-specific preset molecular descriptors or\nfingerprints, the models can be resolved using raw and pertinent features\nwithout manual intervention or selection. In this study, we developed acute\noral toxicity (AOT) models of compounds using the MGE-CNN architecture as a\ncase study. Three types of high-level predictive models: regression model\n(deepAOT-R), multi-classification model (deepAOT-C) and multi-task model\n(deepAOT-CR) for AOT evaluation were constructed. These models highly\noutperformed previously reported models. For the two external datasets\ncontaining 1673 (test set I) and 375 (test set II) compounds, the R2 and mean\nabsolute error (MAE) of deepAOT-R on the test set I were 0.864 and 0.195, and\nthe prediction accuracy of deepAOT-C was 95.5% and 96.3% on the test set I and\nII, respectively. The two external prediction accuracy of deepAOT-CR is 95.0%\nand 94.1%, while the R2 and MAE are 0.861 and 0.204 for test set I,\nrespectively. \n\n"}
{"id": "1704.04839", "contents": "Title: Mixture modeling on related samples by $\\psi$-stick breaking and kernel\n  perturbation Abstract: There has been great interest recently in applying nonparametric kernel\nmixtures in a hierarchical manner to model multiple related data samples\njointly. In such settings several data features are commonly present: (i) the\nrelated samples often share some, if not all, of the mixture components but\nwith differing weights, (ii) only some, not all, of the mixture components vary\nacross the samples, and (iii) often the shared mixture components across\nsamples are not aligned perfectly in terms of their location and spread, but\nrather display small misalignments either due to systematic cross-sample\ndifference or more often due to uncontrolled, extraneous causes. Properly\nincorporating these features in mixture modeling will enhance the efficiency of\ninference, whereas ignoring them not only reduces efficiency but can jeopardize\nthe validity of the inference due to issues such as confounding. We introduce\ntwo techniques for incorporating these features in modeling related data\nsamples using kernel mixtures. The first technique, called $\\psi$-stick\nbreaking, is a joint generative process for the mixing weights through the\nbreaking of both a stick shared by all the samples for the components that do\nnot vary in size across samples and an idiosyncratic stick for each sample for\nthose components that do vary in size. The second technique is to imbue random\nperturbation into the kernels, thereby accounting for cross-sample\nmisalignment. These techniques can be used either separately or together in\nboth parametric and nonparametric kernel mixtures. We derive efficient Bayesian\ninference recipes based on MCMC sampling for models featuring these techniques,\nand illustrate their work through both simulated data and a real flow cytometry\ndata set in prediction/estimation, cross-sample calibration, and testing\nmulti-sample differences. \n\n"}
{"id": "1704.06256", "contents": "Title: Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption Abstract: We consider the robust phase retrieval problem of recovering the unknown\nsignal from the magnitude-only measurements, where the measurements can be\ncontaminated by both sparse arbitrary corruption and bounded random noise. We\npropose a new nonconvex algorithm for robust phase retrieval, namely Robust\nWirtinger Flow to jointly estimate the unknown signal and the sparse\ncorruption. We show that our proposed algorithm is guaranteed to converge\nlinearly to the unknown true signal up to a minimax optimal statistical\nprecision in such a challenging setting. Compared with existing robust phase\nretrieval methods, we achieve an optimal sample complexity of $O(n)$ in both\nnoisy and noise-free settings. Thorough experiments on both synthetic and real\ndatasets corroborate our theory. \n\n"}
{"id": "1704.06687", "contents": "Title: Scatteract: Automated extraction of data from scatter plots Abstract: Charts are an excellent way to convey patterns and trends in data, but they\ndo not facilitate further modeling of the data or close inspection of\nindividual data points. We present a fully automated system for extracting the\nnumerical values of data points from images of scatter plots. We use deep\nlearning techniques to identify the key components of the chart, and optical\ncharacter recognition together with robust regression to map from pixels to the\ncoordinate system of the chart. We focus on scatter plots with linear scales,\nwhich already have several interesting challenges. Previous work has done fully\nautomatic extraction for other types of charts, but to our knowledge this is\nthe first approach that is fully automatic for scatter plots. Our method\nperforms well, achieving successful data extraction on 89% of the plots in our\ntest set. \n\n"}
{"id": "1704.07147", "contents": "Title: A Neural Network model with Bidirectional Whitening Abstract: We present here a new model and algorithm which performs an efficient Natural\ngradient descent for Multilayer Perceptrons. Natural gradient descent was\noriginally proposed from a point of view of information geometry, and it\nperforms the steepest descent updates on manifolds in a Riemannian space. In\nparticular, we extend an approach taken by the \"Whitened neural networks\"\nmodel. We make the whitening process not only in feed-forward direction as in\nthe original model, but also in the back-propagation phase. Its efficacy is\nshown by an application of this \"Bidirectional whitened neural networks\" model\nto a handwritten character recognition data (MNIST data). \n\n"}
{"id": "1704.07626", "contents": "Title: Taxonomy Induction using Hypernym Subsequences Abstract: We propose a novel, semi-supervised approach towards domain taxonomy\ninduction from an input vocabulary of seed terms. Unlike all previous\napproaches, which typically extract direct hypernym edges for terms, our\napproach utilizes a novel probabilistic framework to extract hypernym\nsubsequences. Taxonomy induction from extracted subsequences is cast as an\ninstance of the minimumcost flow problem on a carefully designed directed\ngraph. Through experiments, we demonstrate that our approach outperforms\nstateof- the-art taxonomy induction approaches across four languages.\nImportantly, we also show that our approach is robust to the presence of noise\nin the input vocabulary. To the best of our knowledge, no previous approaches\nhave been empirically proven to manifest noise-robustness in the input\nvocabulary. \n\n"}
{"id": "1704.07751", "contents": "Title: Fine-Grained Entity Typing with High-Multiplicity Assignments Abstract: As entity type systems become richer and more fine-grained, we expect the\nnumber of types assigned to a given entity to increase. However, most\nfine-grained typing work has focused on datasets that exhibit a low degree of\ntype multiplicity. In this paper, we consider the high-multiplicity regime\ninherent in data sources such as Wikipedia that have semi-open type systems. We\nintroduce a set-prediction approach to this problem and show that our model\noutperforms unstructured baselines on a new Wikipedia-based fine-grained typing\ncorpus. \n\n"}
{"id": "1704.08227", "contents": "Title: Accelerating Stochastic Gradient Descent For Least Squares Regression Abstract: There is widespread sentiment that it is not possible to effectively utilize\nfast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy\nball) for the purposes of stochastic optimization due to their instability and\nerror accumulation, a notion made precise in d'Aspremont 2008 and Devolder,\nGlineur, and Nesterov 2014. This work considers these issues for the special\ncase of stochastic approximation for the least squares regression problem, and\nour main result refutes the conventional wisdom by showing that acceleration\ncan be made robust to statistical errors. In particular, this work introduces\nan accelerated stochastic gradient method that provably achieves the minimax\noptimal statistical risk faster than stochastic gradient descent. Critical to\nthe analysis is a sharp characterization of accelerated stochastic gradient\ndescent as a stochastic process. We hope this characterization gives insights\ntowards the broader question of designing simple and effective accelerated\nstochastic methods for more general convex and non-convex optimization\nproblems. \n\n"}
{"id": "1704.08756", "contents": "Title: A Network Perspective on Stratification of Multi-Label Data Abstract: In the recent years, we have witnessed the development of multi-label\nclassification methods which utilize the structure of the label space in a\ndivide and conquer approach to improve classification performance and allow\nlarge data sets to be classified efficiently. Yet most of the available data\nsets have been provided in train/test splits that did not account for\nmaintaining a distribution of higher-order relationships between labels among\nsplits or folds. We present a new approach to stratifying multi-label data for\nclassification purposes based on the iterative stratification approach proposed\nby Sechidis et. al. in an ECML PKDD 2011 paper. Our method extends the\niterative approach to take into account second-order relationships between\nlabels. Obtained results are evaluated using statistical properties of obtained\nstrata as presented by Sechidis. We also propose new statistical measures\nrelevant to second-order quality: label pairs distribution, the percentage of\nlabel pairs without positive evidence in folds and label pair - fold pairs that\nhave no positive evidence for the label pair. We verify the impact of new\nmethods on classification performance of Binary Relevance, Label Powerset and a\nfast greedy community detection based label space partitioning classifier.\nRandom Forests serve as base classifiers. We check the variation of the number\nof communities obtained per fold, and the stability of their modularity score.\nSecond-Order Iterative Stratification is compared to standard k-fold, label\nset, and iterative stratification. The proposed approach lowers the variance of\nclassification quality, improves label pair oriented measures and example\ndistribution while maintaining a competitive quality in label-oriented\nmeasures. We also witness an increase in stability of network characteristics. \n\n"}
{"id": "1704.08803", "contents": "Title: Neural Ranking Models with Weak Supervision Abstract: Despite the impressive improvements achieved by unsupervised deep neural\nnetworks in computer vision and NLP tasks, such improvements have not yet been\nobserved in ranking for information retrieval. The reason may be the complexity\nof the ranking problem, as it is not obvious how to learn from queries and\ndocuments when no supervised signal is available. Hence, in this paper, we\npropose to train a neural ranking model using weak supervision, where labels\nare obtained automatically without human annotators or any external resources\n(e.g., click data). To this aim, we use the output of an unsupervised ranking\nmodel, such as BM25, as a weak supervision signal. We further train a set of\nsimple yet effective ranking models based on feed-forward neural networks. We\nstudy their effectiveness under various learning scenarios (point-wise and\npair-wise models) and using different input representations (i.e., from\nencoding query-document pairs into dense/sparse vectors to using word embedding\nrepresentation). We train our networks using tens of millions of training\ninstances and evaluate it on two standard collections: a homogeneous news\ncollection(Robust) and a heterogeneous large-scale web collection (ClueWeb).\nOur experiments indicate that employing proper objective functions and letting\nthe networks to learn the input representation based on weakly supervised data\nleads to impressive performance, with over 13% and 35% MAP improvements over\nthe BM25 model on the Robust and the ClueWeb collections. Our findings also\nsuggest that supervised neural ranking models can greatly benefit from\npre-training on large amounts of weakly labeled data that can be easily\nobtained from unsupervised IR models. \n\n"}
{"id": "1704.08913", "contents": "Title: Adaptation and learning over networks for nonlinear system modeling Abstract: In this chapter, we analyze nonlinear filtering problems in distributed\nenvironments, e.g., sensor networks or peer-to-peer protocols. In these\nscenarios, the agents in the environment receive measurements in a streaming\nfashion, and they are required to estimate a common (nonlinear) model by\nalternating local computations and communications with their neighbors. We\nfocus on the important distinction between single-task problems, where the\nunderlying model is common to all agents, and multitask problems, where each\nagent might converge to a different model due to, e.g., spatial dependencies or\nother factors. Currently, most of the literature on distributed learning in the\nnonlinear case has focused on the single-task case, which may be a strong\nlimitation in real-world scenarios. After introducing the problem and reviewing\nthe existing approaches, we describe a simple kernel-based algorithm tailored\nfor the multitask case. We evaluate the proposal on a simulated benchmark task,\nand we conclude by detailing currently open problems and lines of research. \n\n"}
{"id": "1705.00470", "contents": "Title: Learning Multimodal Transition Dynamics for Model-Based Reinforcement\n  Learning Abstract: In this paper we study how to learn stochastic, multimodal transition\ndynamics in reinforcement learning (RL) tasks. We focus on evaluating\ntransition function estimation, while we defer planning over this model to\nfuture work. Stochasticity is a fundamental property of many task environments.\nHowever, discriminative function approximators have difficulty estimating\nmultimodal stochasticity. In contrast, deep generative models do capture\ncomplex high-dimensional outcome distributions. First we discuss why, amongst\nsuch models, conditional variational inference (VI) is theoretically most\nappealing for model-based RL. Subsequently, we compare different VI models on\ntheir ability to learn complex stochasticity on simulated functions, as well as\non a typical RL gridworld with multimodal dynamics. Results show VI\nsuccessfully predicts multimodal outcomes, but also robustly ignores these for\ndeterministic parts of the transition dynamics. In summary, we show a robust\nmethod to learn multimodal transitions using function approximation, which is a\nkey preliminary for model-based RL in stochastic domains. \n\n"}
{"id": "1705.00740", "contents": "Title: Regularizing Model Complexity and Label Structure for Multi-Label Text\n  Classification Abstract: Multi-label text classification is a popular machine learning task where each\ndocument is assigned with multiple relevant labels. This task is challenging\ndue to high dimensional features and correlated labels. Multi-label text\nclassifiers need to be carefully regularized to prevent the severe over-fitting\nin the high dimensional space, and also need to take into account label\ndependencies in order to make accurate predictions under uncertainty. We\ndemonstrate significant and practical improvement by carefully regularizing the\nmodel complexity during training phase, and also regularizing the label search\nspace during prediction phase. Specifically, we regularize the classifier\ntraining using Elastic-net (L1+L2) penalty for reducing model complexity/size,\nand employ early stopping to prevent overfitting. At prediction time, we apply\nsupport inference to restrict the search space to label sets encountered in the\ntraining set, and F-optimizer GFM to make optimal predictions for the F1\nmetric. We show that although support inference only provides density\nestimations on existing label combinations, when combined with GFM predictor,\nthe algorithm can output unseen label combinations. Taken collectively, our\nexperiments show state of the art results on many benchmark datasets. Beyond\nperformance and practical contributions, we make some interesting observations.\nContrary to the prior belief, which deems support inference as purely an\napproximate inference procedure, we show that support inference acts as a\nstrong regularizer on the label prediction structure. It allows the classifier\nto take into account label dependencies during prediction even if the\nclassifiers had not modeled any label dependencies during training. \n\n"}
{"id": "1705.00995", "contents": "Title: Fuzzy Approach Topic Discovery in Health and Medical Corpora Abstract: The majority of medical documents and electronic health records (EHRs) are in\ntext format that poses a challenge for data processing and finding relevant\ndocuments. Looking for ways to automatically retrieve the enormous amount of\nhealth and medical knowledge has always been an intriguing topic. Powerful\nmethods have been developed in recent years to make the text processing\nautomatic. One of the popular approaches to retrieve information based on\ndiscovering the themes in health & medical corpora is topic modeling, however,\nthis approach still needs new perspectives. In this research we describe fuzzy\nlatent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy\nperspective. FLSA can handle health & medical corpora redundancy issue and\nprovides a new method to estimate the number of topics. The quantitative\nevaluations show that FLSA produces superior performance and features to latent\nDirichlet allocation (LDA), the most popular topic model. \n\n"}
{"id": "1705.01015", "contents": "Title: Deep Learning for Tumor Classification in Imaging Mass Spectrometry Abstract: Motivation: Tumor classification using Imaging Mass Spectrometry (IMS) data\nhas a high potential for future applications in pathology. Due to the\ncomplexity and size of the data, automated feature extraction and\nclassification steps are required to fully process the data. Deep learning\noffers an approach to learn feature extraction and classification combined in a\nsingle model. Commonly these steps are handled separately in IMS data analysis,\nhence deep learning offers an alternative strategy worthwhile to explore.\nResults: Methodologically, we propose an adapted architecture based on deep\nconvolutional networks to handle the characteristics of mass spectrometry data,\nas well as a strategy to interpret the learned model in the spectral domain\nbased on a sensitivity analysis. The proposed methods are evaluated on two\nchallenging tumor classification tasks and compared to a baseline approach.\nCompetitiveness of the proposed methods are shown on both tasks by studying the\nperformance via cross-validation. Moreover, the learned models are analyzed by\nthe proposed sensitivity analysis revealing biologically plausible effects as\nwell as confounding factors of the considered task. Thus, this study may serve\nas a starting point for further development of deep learning approaches in IMS\nclassification tasks. \n\n"}
{"id": "1705.01509", "contents": "Title: Neural Models for Information Retrieval Abstract: Neural ranking models for information retrieval (IR) use shallow or deep\nneural networks to rank search results in response to a query. Traditional\nlearning to rank models employ machine learning techniques over hand-crafted IR\nfeatures. By contrast, neural models learn representations of language from raw\ntext that can bridge the gap between query and document vocabulary. Unlike\nclassical IR models, these new machine learning based approaches are\ndata-hungry, requiring large scale training data before they can be deployed.\nThis tutorial introduces basic concepts and intuitions behind neural IR models,\nand places them in the context of traditional retrieval models. We begin by\nintroducing fundamental concepts of IR and different neural and non-neural\napproaches to learning vector representations of text. We then review shallow\nneural IR methods that employ pre-trained neural term embeddings without\nlearning the IR task end-to-end. We introduce deep neural networks next,\ndiscussing popular deep architectures. Finally, we review the current DNN\nmodels for information retrieval. We conclude with a discussion on potential\nfuture directions for neural IR. \n\n"}
{"id": "1705.04293", "contents": "Title: Bayesian Approaches to Distribution Regression Abstract: Distribution regression has recently attracted much interest as a generic\nsolution to the problem of supervised learning where labels are available at\nthe group level, rather than at the individual level. Current approaches,\nhowever, do not propagate the uncertainty in observations due to sampling\nvariability in the groups. This effectively assumes that small and large groups\nare estimated equally well, and should have equal weight in the final\nregression. We account for this uncertainty with a Bayesian distribution\nregression formalism, improving the robustness and performance of the model\nwhen group sizes vary. We frame our models in a neural network style, allowing\nfor simple MAP inference using backpropagation to learn the parameters, as well\nas MCMC-based inference which can fully propagate uncertainty. We demonstrate\nour approach on illustrative toy datasets, as well as on a challenging problem\nof predicting age from images. \n\n"}
{"id": "1705.05598", "contents": "Title: Learning how to explain neural networks: PatternNet and\n  PatternAttribution Abstract: DeConvNet, Guided BackProp, LRP, were invented to better understand deep\nneural networks. We show that these methods do not produce the theoretically\ncorrect explanation for a linear model. Yet they are used on multi-layer\nnetworks with millions of parameters. This is a cause for concern since linear\nmodels are simple neural networks. We argue that explanation methods for neural\nnets should work reliably in the limit of simplicity, the linear models. Based\non our analysis of linear models we propose a generalization that yields two\nexplanation techniques (PatternNet and PatternAttribution) that are\ntheoretically sound for linear models and produce improved explanations for\ndeep networks. \n\n"}
{"id": "1705.06504", "contents": "Title: TableQA: Question Answering on Tabular Data Abstract: Tabular data is difficult to analyze and to search through, yielding for new\ntools and interfaces that would allow even non tech-savvy users to gain\ninsights from open datasets without resorting to specialized data analysis\ntools or even without having to fully understand the dataset structure. The\ngoal of our demonstration is to showcase answering natural language questions\nfrom tabular data, and to discuss related system configuration and model\ntraining aspects. Our prototype is publicly available and open-sourced (see\nhttps://svakulenko.ai.wu.ac.at/tableqa). \n\n"}
{"id": "1705.07025", "contents": "Title: Effective Representations of Clinical Notes Abstract: Clinical notes are a rich source of information about patient state. However,\nusing them to predict clinical events with machine learning models is\nchallenging. They are very high dimensional, sparse and have complex structure.\nFurthermore, training data is often scarce because it is expensive to obtain\nreliable labels for many clinical events. These difficulties have traditionally\nbeen addressed by manual feature engineering encoding task specific domain\nknowledge. We explored the use of neural networks and transfer learning to\nlearn representations of clinical notes that are useful for predicting future\nclinical events of interest, such as all causes mortality, inpatient\nadmissions, and emergency room visits. Our data comprised 2.7 million notes and\n115 thousand patients at Stanford Hospital. We used the learned\nrepresentations, along with commonly used bag of words and topic model\nrepresentations, as features for predictive models of clinical events. We\nevaluated the effectiveness of these representations with respect to the\nperformance of the models trained on small datasets. Models using the neural\nnetwork derived representations performed significantly better than models\nusing the baseline representations with small ($N < 1000$) training datasets.\nThe learned representations offer significant performance gains over commonly\nused baseline representations for a range of predictive modeling tasks and\ncohort sizes, offering an effective alternative to task specific feature\nengineering when plentiful labeled training data is not available. \n\n"}
{"id": "1705.07107", "contents": "Title: Gradient Estimators for Implicit Models Abstract: Implicit models, which allow for the generation of samples but not for\npoint-wise evaluation of probabilities, are omnipresent in real-world problems\ntackled by machine learning and a hot topic of current research. Some examples\ninclude data simulators that are widely used in engineering and scientific\nresearch, generative adversarial networks (GANs) for image synthesis, and\nhot-off-the-press approximate inference techniques relying on implicit\ndistributions. The majority of existing approaches to learning implicit models\nrely on approximating the intractable distribution or optimisation objective\nfor gradient-based optimisation, which is liable to produce inaccurate updates\nand thus poor models. This paper alleviates the need for such approximations by\nproposing the Stein gradient estimator, which directly estimates the score\nfunction of the implicitly defined distribution. The efficacy of the proposed\nestimator is empirically demonstrated by examples that include meta-learning\nfor approximate inference, and entropy regularised GANs that provide improved\nsample diversity. \n\n"}
{"id": "1705.07366", "contents": "Title: Forward Thinking: Building Deep Random Forests Abstract: The success of deep neural networks has inspired many to wonder whether other\nlearners could benefit from deep, layered architectures. We present a general\nframework called forward thinking for deep learning that generalizes the\narchitectural flexibility and sophistication of deep neural networks while also\nallowing for (i) different types of learning functions in the network, other\nthan neurons, and (ii) the ability to adaptively deepen the network as needed\nto improve results. This is done by training one layer at a time, and once a\nlayer is trained, the input data are mapped forward through the layer to create\na new learning problem. The process is then repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new dataset, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. In the case where the neurons of deep neural nets are\nreplaced with decision trees, we call the result a Forward Thinking Deep Random\nForest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the\nMNIST dataset. We also provide a general mathematical formulation that allows\nfor other types of deep learning problems to be considered. \n\n"}
{"id": "1705.07384", "contents": "Title: Balanced Policy Evaluation and Learning Abstract: We present a new approach to the problems of evaluating and learning\npersonalized decision policies from observational data of past contexts,\ndecisions, and outcomes. Only the outcome of the enacted decision is available\nand the historical policy is unknown. These problems arise in personalized\nmedicine using electronic health records and in internet advertising. Existing\napproaches use inverse propensity weighting (or, doubly robust versions) to\nmake historical outcome (or, residual) data look like it were generated by a\nnew policy being evaluated or learned. But this relies on a plug-in approach\nthat rejects data points with a decision that disagrees with the new policy,\nleading to high variance estimates and ineffective learning. We propose a new,\nbalance-based approach that too makes the data look like the new policy but\ndoes so directly by finding weights that optimize for balance between the\nweighted data and the target policy in the given, finite sample, which is\nequivalent to minimizing worst-case or posterior conditional mean square error.\nOur policy learner proceeds as a two-level optimization problem over policies\nand weights. We demonstrate that this approach markedly outperforms existing\nones both in evaluation and learning, which is unsurprising given the wider\nsupport of balance-based weights. We establish extensive theoretical\nconsistency guarantees and regret bounds that support this empirical success. \n\n"}
{"id": "1705.07673", "contents": "Title: A Linear-Time Kernel Goodness-of-Fit Test Abstract: We propose a novel adaptive test of goodness-of-fit, with computational cost\nlinear in the number of samples. We learn the test features that best indicate\nthe differences between observed samples and a reference model, by minimizing\nthe false negative rate. These features are constructed via Stein's method,\nmeaning that it is not necessary to compute the normalising constant of the\nmodel. We analyse the asymptotic Bahadur efficiency of the new test, and prove\nthat under a mean-shift alternative, our test always has greater relative\nefficiency than a previous linear-time kernel test, regardless of the choice of\nparameters for that test. In experiments, the performance of our method exceeds\nthat of the earlier linear-time test, and matches or exceeds the power of a\nquadratic-time kernel test. In high dimensions and where model structure may be\nexploited, our goodness of fit test performs far better than a quadratic-time\ntwo-sample test based on the Maximum Mean Discrepancy, with samples drawn from\nthe model. \n\n"}
{"id": "1705.08153", "contents": "Title: Techniques for visualizing LSTMs applied to electrocardiograms Abstract: This paper explores four different visualization techniques for long\nshort-term memory (LSTM) networks applied to continuous-valued time series. On\nthe datasets analysed, we find that the best visualization technique is to\nlearn an input deletion mask that optimally reduces the true class score. With\na specific focus on single-lead electrocardiograms from the MIT-BIH arrhythmia\ndataset, we show that salient input features for the LSTM classifier align well\nwith medical theory. \n\n"}
{"id": "1705.08197", "contents": "Title: Learning to Succeed while Teaching to Fail: Privacy in Closed Machine\n  Learning Systems Abstract: Security, privacy, and fairness have become critical in the era of data\nscience and machine learning. More and more we see that achieving universally\nsecure, private, and fair systems is practically impossible. We have seen for\nexample how generative adversarial networks can be used to learn about the\nexpected private training data; how the exploitation of additional data can\nreveal private information in the original one; and how what looks like\nunrelated features can teach us about each other. Confronted with this\nchallenge, in this paper we open a new line of research, where the security,\nprivacy, and fairness is learned and used in a closed environment. The goal is\nto ensure that a given entity (e.g., the company or the government), trusted to\ninfer certain information with our data, is blocked from inferring protected\ninformation from it. For example, a hospital might be allowed to produce\ndiagnosis on the patient (the positive task), without being able to infer the\ngender of the subject (negative task). Similarly, a company can guarantee that\ninternally it is not using the provided data for any undesired task, an\nimportant goal that is not contradicting the virtually impossible challenge of\nblocking everybody from the undesired task. We design a system that learns to\nsucceed on the positive task while simultaneously fail at the negative one, and\nillustrate this with challenging cases where the positive task is actually\nharder than the negative one being blocked. Fairness, to the information in the\nnegative task, is often automatically obtained as a result of this proposed\napproach. The particular framework and examples open the door to security,\nprivacy, and fairness in very important closed scenarios, ranging from private\ndata accumulation companies like social networks to law-enforcement and\nhospitals. \n\n"}
{"id": "1705.08360", "contents": "Title: Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families Abstract: We propose a fast method with statistical guarantees for learning an\nexponential family density model where the natural parameter is in a\nreproducing kernel Hilbert space, and may be infinite-dimensional. The model is\nlearned by fitting the derivative of the log density, the score, thus avoiding\nthe need to compute a normalization constant. Our approach improves the\ncomputational efficiency of an earlier solution by using a low-rank,\nNystr\\\"om-like solution. The new solution retains the consistency and\nconvergence rates of the full-rank solution (exactly in Fisher distance, and\nnearly in other distances), with guarantees on the degree of cost and storage\nreduction. We evaluate the method in experiments on density estimation and in\nthe construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an\nexisting score learning approach using a denoising autoencoder, our estimator\nis empirically more data-efficient when estimating the score, runs faster, and\nhas fewer parameters (which can be tuned in a principled and interpretable\nway), in addition to providing statistical guarantees. \n\n"}
{"id": "1705.08665", "contents": "Title: Bayesian Compression for Deep Learning Abstract: Compression and computational efficiency in deep learning have become a\nproblem of great significance. In this work, we argue that the most principled\nand effective way to attack this problem is by adopting a Bayesian point of\nview, where through sparsity inducing priors we prune large parts of the\nnetwork. We introduce two novelties in this paper: 1) we use hierarchical\npriors to prune nodes instead of individual weights, and 2) we use the\nposterior uncertainties to determine the optimal fixed point precision to\nencode the weights. Both factors significantly contribute to achieving the\nstate of the art in terms of compression rates, while still staying competitive\nwith methods designed to optimize for speed or energy efficiency. \n\n"}
{"id": "1705.08736", "contents": "Title: Non-Stationary Spectral Kernels Abstract: We propose non-stationary spectral kernels for Gaussian process regression.\nWe propose to model the spectral density of a non-stationary kernel function as\na mixture of input-dependent Gaussian process frequency density surfaces. We\nsolve the generalised Fourier transform with such a model, and present a family\nof non-stationary and non-monotonic kernels that can learn input-dependent and\npotentially long-range, non-monotonic covariances between inputs. We derive\nefficient inference using model whitening and marginalized posterior, and show\nwith case studies that these kernels are necessary when modelling even rather\nsimple time series, image or geospatial data with non-stationary\ncharacteristics. \n\n"}
{"id": "1705.08741", "contents": "Title: Train longer, generalize better: closing the generalization gap in large\n  batch training of neural networks Abstract: Background: Deep learning models are typically trained using stochastic\ngradient descent or one of its variants. These methods update the weights using\ntheir gradient, estimated from a small fraction of the training data. It has\nbeen observed that when using large batch sizes there is a persistent\ndegradation in generalization performance - known as the \"generalization gap\"\nphenomena. Identifying the origin of this gap and closing it had remained an\nopen problem.\n  Contributions: We examine the initial high learning rate training phase. We\nfind that the weight distance from its initialization grows logarithmically\nwith the number of weight updates. We therefore propose a \"random walk on\nrandom landscape\" statistical model which is known to exhibit similar\n\"ultra-slow\" diffusion behavior. Following this hypothesis we conducted\nexperiments to show empirically that the \"generalization gap\" stems from the\nrelatively small number of updates rather than the batch size, and can be\ncompletely eliminated by adapting the training regime used. We further\ninvestigate different techniques to train models in the large-batch regime and\npresent a novel algorithm named \"Ghost Batch Normalization\" which enables\nsignificant decrease in the generalization gap without increasing the number of\nupdates. To validate our findings we conduct several additional experiments on\nMNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices\nand beliefs concerning training of deep models and suggest they may not be\noptimal to achieve good generalization. \n\n"}
{"id": "1705.08804", "contents": "Title: Beyond Parity: Fairness Objectives for Collaborative Filtering Abstract: We study fairness in collaborative-filtering recommender systems, which are\nsensitive to discrimination that exists in historical data. Biased data can\nlead collaborative-filtering methods to make unfair predictions for users from\nminority groups. We identify the insufficiency of existing fairness metrics and\npropose four new metrics that address different forms of unfairness. These\nfairness metrics can be optimized by adding fairness terms to the learning\nobjective. Experiments on synthetic and real data show that our new metrics can\nbetter measure fairness than the baseline, and that the fairness objectives\neffectively help reduce unfairness. \n\n"}
{"id": "1705.08848", "contents": "Title: Joint Distribution Optimal Transportation for Domain Adaptation Abstract: This paper deals with the unsupervised domain adaptation problem, where one\nwants to estimate a prediction function $f$ in a given target domain without\nany labeled sample by exploiting the knowledge available from a source domain\nwhere labels are known. Our work makes the following assumption: there exists a\nnon-linear transformation between the joint feature/label space distributions\nof the two domain $\\mathcal{P}_s$ and $\\mathcal{P}_t$. We propose a solution of\nthis problem with optimal transport, that allows to recover an estimated target\n$\\mathcal{P}^f_t=(X,f(X))$ by optimizing simultaneously the optimal coupling\nand $f$. We show that our method corresponds to the minimization of a bound on\nthe target error, and provide an efficient algorithmic solution, for which\nconvergence is proved. The versatility of our approach, both in terms of class\nof hypothesis or loss functions is demonstrated with real world classification\nand regression problems, for which we reach or surpass state-of-the-art\nresults. \n\n"}
{"id": "1705.08931", "contents": "Title: Proximity Variational Inference Abstract: Variational inference is a powerful approach for approximate posterior\ninference. However, it is sensitive to initialization and can be subject to\npoor local optima. In this paper, we develop proximity variational inference\n(PVI). PVI is a new method for optimizing the variational objective that\nconstrains subsequent iterates of the variational parameters to robustify the\noptimization path. Consequently, PVI is less sensitive to initialization and\noptimization quirks and finds better local optima. We demonstrate our method on\nthree proximity statistics. We study PVI on a Bernoulli factor model and\nsigmoid belief network with both real and synthetic data and compare to\ndeterministic annealing (Katahira et al., 2008). We highlight the flexibility\nof PVI by designing a proximity statistic for Bayesian deep learning models\nsuch as the variational autoencoder (Kingma and Welling, 2014; Rezende et al.,\n2014). Empirically, we show that PVI consistently finds better local optima and\ngives better predictive performance. \n\n"}
{"id": "1705.10689", "contents": "Title: Auditing Search Engines for Differential Satisfaction Across\n  Demographics Abstract: Many online services, such as search engines, social media platforms, and\ndigital marketplaces, are advertised as being available to any user, regardless\nof their age, gender, or other demographic factors. However, there are growing\nconcerns that these services may systematically underserve some groups of\nusers. In this paper, we present a framework for internally auditing such\nservices for differences in user satisfaction across demographic groups, using\nsearch engines as a case study. We first explain the pitfalls of na\\\"ively\ncomparing the behavioral metrics that are commonly used to evaluate search\nengines. We then propose three methods for measuring latent differences in user\nsatisfaction from observed differences in evaluation metrics. To develop these\nmethods, we drew on ideas from the causal inference literature and the\nmultilevel modeling literature. Our framework is broadly applicable to other\nonline services, and provides general insight into interpreting their\nevaluation metrics. \n\n"}
{"id": "1706.01084", "contents": "Title: Joint Text Embedding for Personalized Content-based Recommendation Abstract: Learning a good representation of text is key to many recommendation\napplications. Examples include news recommendation where texts to be\nrecommended are constantly published everyday. However, most existing\nrecommendation techniques, such as matrix factorization based methods, mainly\nrely on interaction histories to learn representations of items. While latent\nfactors of items can be learned effectively from user interaction data, in many\ncases, such data is not available, especially for newly emerged items.\n  In this work, we aim to address the problem of personalized recommendation\nfor completely new items with text information available. We cast the problem\nas a personalized text ranking problem and propose a general framework that\ncombines text embedding with personalized recommendation. Users and textual\ncontent are embedded into latent feature space. The text embedding function can\nbe learned end-to-end by predicting user interactions with items. To alleviate\nsparsity in interaction data, and leverage large amount of text data with\nlittle or no user interactions, we further propose a joint text embedding model\nthat incorporates unsupervised text embedding with a combination module.\nExperimental results show that our model can significantly improve the\neffectiveness of recommendation systems on real-world datasets. \n\n"}
{"id": "1706.01242", "contents": "Title: Bayesian LSTMs in medicine Abstract: The medical field stands to see significant benefits from the recent advances\nin deep learning. Knowing the uncertainty in the decision made by any machine\nlearning algorithm is of utmost importance for medical practitioners. This\nstudy demonstrates the utility of using Bayesian LSTMs for classification of\nmedical time series. Four medical time series datasets are used to show the\naccuracy improvement Bayesian LSTMs provide over standard LSTMs. Moreover, we\nshow cherry-picked examples of confident and uncertain classifications of the\nmedical time series. With simple modifications of the common practice for deep\nlearning, significant improvements can be made for the medical practitioner and\npatient. \n\n"}
{"id": "1706.01442", "contents": "Title: The Capacity of Private Information Retrieval from Byzantine and\n  Colluding Databases Abstract: We consider the problem of single-round private information retrieval (PIR)\nfrom $N$ replicated databases. We consider the case when $B$ databases are\noutdated (unsynchronized), or even worse, adversarial (Byzantine), and\ntherefore, can return incorrect answers. In the PIR problem with Byzantine\ndatabases (BPIR), a user wishes to retrieve a specific message from a set of\n$M$ messages with zero-error, irrespective of the actions performed by the\nByzantine databases. We consider the $T$-privacy constraint in this paper,\nwhere any $T$ databases can collude, and exchange the queries submitted by the\nuser. We derive the information-theoretic capacity of this problem, which is\nthe maximum number of \\emph{correct symbols} that can be retrieved privately\n(under the $T$-privacy constraint) for every symbol of the downloaded data. We\ndetermine the exact BPIR capacity to be\n$C=\\frac{N-2B}{N}\\cdot\\frac{1-\\frac{T}{N-2B}}{1-(\\frac{T}{N-2B})^M}$, if $2B+T\n< N$. This capacity expression shows that the effect of Byzantine databases on\nthe retrieval rate is equivalent to removing $2B$ databases from the system,\nwith a penalty factor of $\\frac{N-2B}{N}$, which signifies that even though the\nnumber of databases needed for PIR is effectively $N-2B$, the user still needs\nto access the entire $N$ databases. The result shows that for the\nunsynchronized PIR problem, if the user does not have any knowledge about the\nfraction of the messages that are mis-synchronized, the single-round capacity\nis the same as the BPIR capacity. Our achievable scheme extends the optimal\nachievable scheme for the robust PIR (RPIR) problem to correct the\n\\emph{errors} introduced by the Byzantine databases as opposed to\n\\emph{erasures} in the RPIR problem. Our converse proof uses the idea of the\ncut-set bound in the network coding problem against adversarial nodes. \n\n"}
{"id": "1706.01574", "contents": "Title: Extracting Hierarchies of Search Tasks & Subtasks via a Bayesian\n  Nonparametric Approach Abstract: A significant amount of search queries originate from some real world\ninformation need or tasks. In order to improve the search experience of the end\nusers, it is important to have accurate representations of tasks. As a result,\nsignificant amount of research has been devoted to extracting proper\nrepresentations of tasks in order to enable search systems to help users\ncomplete their tasks, as well as providing the end user with better query\nsuggestions, for better recommendations, for satisfaction prediction, and for\nimproved personalization in terms of tasks. Most existing task extraction\nmethodologies focus on representing tasks as flat structures. However, tasks\noften tend to have multiple subtasks associated with them and a more\nnaturalistic representation of tasks would be in terms of a hierarchy, where\neach task can be composed of multiple (sub)tasks. To this end, we propose an\nefficient Bayesian nonparametric model for extracting hierarchies of such tasks\n\\& subtasks. We evaluate our method based on real world query log data both\nthrough quantitative and crowdsourced experiments and highlight the importance\nof considering task/subtask hierarchies. \n\n"}
{"id": "1706.02361", "contents": "Title: The Effects of Noisy Labels on Deep Convolutional Neural Networks for\n  Music Tagging Abstract: Deep neural networks (DNN) have been successfully applied to music\nclassification including music tagging. However, there are several open\nquestions regarding the training, evaluation, and analysis of DNNs. In this\narticle, we investigate specific aspects of neural networks, the effects of\nnoisy labels, to deepen our understanding of their properties. We analyse and\n(re-)validate a large music tagging dataset to investigate the reliability of\ntraining and evaluation. Using a trained network, we compute label vector\nsimilarities which is compared to groundtruth similarity.\n  The results highlight several important aspects of music tagging and neural\nnetworks. We show that networks can be effective despite relatively large error\nrates in groundtruth datasets, while conjecturing that label noise can be the\ncause of varying tag-wise performance differences. Lastly, the analysis of our\ntrained network provides valuable insight into the relationships between music\ntags. These results highlight the benefit of using data-driven methods to\naddress automatic music tagging. \n\n"}
{"id": "1706.02480", "contents": "Title: Forward Thinking: Building and Training Neural Networks One Layer at a\n  Time Abstract: We present a general framework for training deep neural networks without\nbackpropagation. This substantially decreases training time and also allows for\nconstruction of deep networks with many sorts of learners, including networks\nwhose layers are defined by functions that are not easily differentiated, like\ndecision trees. The main idea is that layers can be trained one at a time, and\nonce they are trained, the input data are mapped forward through the layer to\ncreate a new learning problem. The process is repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new data set, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. We call this forward thinking and demonstrate a proof of\nconcept by achieving state-of-the-art accuracy on the MNIST dataset for\nconvolutional neural networks. We also provide a general mathematical\nformulation of forward thinking that allows for other types of deep learning\nproblems to be considered. \n\n"}
{"id": "1706.02899", "contents": "Title: Assessing the Performance of Deep Learning Algorithms for Newsvendor\n  Problem Abstract: In retailer management, the Newsvendor problem has widely attracted attention\nas one of basic inventory models. In the traditional approach to solving this\nproblem, it relies on the probability distribution of the demand. In theory, if\nthe probability distribution is known, the problem can be considered as fully\nsolved. However, in any real world scenario, it is almost impossible to even\napproximate or estimate a better probability distribution for the demand. In\nrecent years, researchers start adopting machine learning approach to learn a\ndemand prediction model by using other feature information. In this paper, we\npropose a supervised learning that optimizes the demand quantities for products\nbased on feature information. We demonstrate that the original Newsvendor loss\nfunction as the training objective outperforms the recently suggested quadratic\nloss function. The new algorithm has been assessed on both the synthetic data\nand real-world data, demonstrating better performance. \n\n"}
{"id": "1706.03078", "contents": "Title: Group Invariance, Stability to Deformations, and Complexity of Deep\n  Convolutional Representations Abstract: The success of deep convolutional architectures is often attributed in part\nto their ability to learn multiscale and invariant representations of natural\nsignals. However, a precise study of these properties and how they affect\nlearning guarantees is still missing. In this paper, we consider deep\nconvolutional representations of signals; we study their invariance to\ntranslations and to more general groups of transformations, their stability to\nthe action of diffeomorphisms, and their ability to preserve signal\ninformation. This analysis is carried by introducing a multilayer kernel based\non convolutional kernel networks and by studying the geometry induced by the\nkernel mapping. We then characterize the corresponding reproducing kernel\nHilbert space (RKHS), showing that it contains a large class of convolutional\nneural networks with homogeneous activation functions. This analysis allows us\nto separate data representation from learning, and to provide a canonical\nmeasure of model complexity, the RKHS norm, which controls both stability and\ngeneralization of any learned model. In addition to models in the constructed\nRKHS, our stability analysis also applies to convolutional networks with\ngeneric activations such as rectified linear units, and we discuss its\nrelationship with recent generalization bounds based on spectral norms. \n\n"}
{"id": "1706.03369", "contents": "Title: On the Sampling Problem for Kernel Quadrature Abstract: The standard Kernel Quadrature method for numerical integration with random\npoint sets (also called Bayesian Monte Carlo) is known to converge in root mean\nsquare error at a rate determined by the ratio $s/d$, where $s$ and $d$ encode\nthe smoothness and dimension of the integrand. However, an empirical\ninvestigation reveals that the rate constant $C$ is highly sensitive to the\ndistribution of the random points. In contrast to standard Monte Carlo\nintegration, for which optimal importance sampling is well-understood, the\nsampling distribution that minimises $C$ for Kernel Quadrature does not admit a\nclosed form. This paper argues that the practical choice of sampling\ndistribution is an important open problem. One solution is considered; a novel\nautomatic approach based on adaptive tempering and sequential Monte Carlo.\nEmpirical results demonstrate a dramatic reduction in integration error of up\nto 4 orders of magnitude can be achieved with the proposed method. \n\n"}
{"id": "1706.03993", "contents": "Title: Getting deep recommenders fit: Bloom embeddings for sparse binary\n  input/output networks Abstract: Recommendation algorithms that incorporate techniques from deep learning are\nbecoming increasingly popular. Due to the structure of the data coming from\nrecommendation domains (i.e., one-hot-encoded vectors of item preferences),\nthese algorithms tend to have large input and output dimensionalities that\ndominate their overall size. This makes them difficult to train, due to the\nlimited memory of graphical processing units, and difficult to deploy on mobile\ndevices with limited hardware. To address these difficulties, we propose Bloom\nembeddings, a compression technique that can be applied to the input and output\nof neural network models dealing with sparse high-dimensional binary-coded\ninstances. Bloom embeddings are computationally efficient, and do not seriously\ncompromise the accuracy of the model up to 1/5 compression ratios. In some\ncases, they even improve over the original accuracy, with relative increases up\nto 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4\nalternative methods, obtaining favorable results. We also discuss a number of\nfurther advantages of Bloom embeddings, such as 'on-the-fly' constant-time\noperation, zero or marginal space requirements, training time speedups, or the\nfact that they do not require any change to the core model architecture or\ntraining configuration. \n\n"}
{"id": "1706.04148", "contents": "Title: Personalizing Session-based Recommendations with Hierarchical Recurrent\n  Neural Networks Abstract: Session-based recommendations are highly relevant in many modern on-line\nservices (e.g. e-commerce, video streaming) and recommendation settings.\nRecently, Recurrent Neural Networks have been shown to perform very well in\nsession-based settings. While in many session-based recommendation domains user\nidentifiers are hard to come by, there are also domains in which user profiles\nare readily available. We propose a seamless way to personalize RNN models with\ncross-session information transfer and devise a Hierarchical RNN model that\nrelays end evolves latent hidden states of the RNNs across user sessions.\nResults on two industry datasets show large improvements over the session-only\nRNNs. \n\n"}
{"id": "1706.04161", "contents": "Title: Lost Relatives of the Gumbel Trick Abstract: The Gumbel trick is a method to sample from a discrete probability\ndistribution, or to estimate its normalizing partition function. The method\nrelies on repeatedly applying a random perturbation to the distribution in a\nparticular way, each time solving for the most likely configuration. We derive\nan entire family of related methods, of which the Gumbel trick is one member,\nand show that the new methods have superior properties in several settings with\nminimal additional computational cost. In particular, for the Gumbel trick to\nyield computational benefits for discrete graphical models, Gumbel\nperturbations on all configurations are typically replaced with so-called\nlow-rank perturbations. We show how a subfamily of our new methods adapts to\nthis setting, proving new upper and lower bounds on the log partition function\nand deriving a family of sequential samplers for the Gibbs distribution.\nFinally, we balance the discussion by showing how the simpler analytical form\nof the Gumbel trick enables additional theoretical results. \n\n"}
{"id": "1706.04289", "contents": "Title: Leveraging Node Attributes for Incomplete Relational Data Abstract: Relational data are usually highly incomplete in practice, which inspires us\nto leverage side information to improve the performance of community detection\nand link prediction. This paper presents a Bayesian probabilistic approach that\nincorporates various kinds of node attributes encoded in binary form in\nrelational models with Poisson likelihood. Our method works flexibly with both\ndirected and undirected relational networks. The inference can be done by\nefficient Gibbs sampling which leverages sparsity of both networks and node\nattributes. Extensive experiments show that our models achieve the\nstate-of-the-art link prediction results, especially with highly incomplete\nrelational data. \n\n"}
{"id": "1706.04646", "contents": "Title: Differentially Private Learning of Undirected Graphical Models using\n  Collective Graphical Models Abstract: We investigate the problem of learning discrete, undirected graphical models\nin a differentially private way. We show that the approach of releasing noisy\nsufficient statistics using the Laplace mechanism achieves a good trade-off\nbetween privacy, utility, and practicality. A naive learning algorithm that\nuses the noisy sufficient statistics \"as is\" outperforms general-purpose\ndifferentially private learning algorithms. However, it has three limitations:\nit ignores knowledge about the data generating process, rests on uncertain\ntheoretical foundations, and exhibits certain pathologies. We develop a more\nprincipled approach that applies the formalism of collective graphical models\nto perform inference over the true sufficient statistics within an\nexpectation-maximization framework. We show that this learns better models than\ncompeting approaches on both synthetic data and on real human mobility data\nused as a case study. \n\n"}
{"id": "1706.04918", "contents": "Title: Robust Submodular Maximization: A Non-Uniform Partitioning Approach Abstract: We study the problem of maximizing a monotone submodular function subject to\na cardinality constraint $k$, with the added twist that a number of items\n$\\tau$ from the returned set may be removed. We focus on the worst-case setting\nconsidered in (Orlin et al., 2016), in which a constant-factor approximation\nguarantee was given for $\\tau = o(\\sqrt{k})$. In this paper, we solve a key\nopen problem raised therein, presenting a new Partitioned Robust (PRo)\nsubmodular maximization algorithm that achieves the same guarantee for more\ngeneral $\\tau = o(k)$. Our algorithm constructs partitions consisting of\nbuckets with exponentially increasing sizes, and applies standard submodular\noptimization subroutines on the buckets in order to construct the robust\nsolution. We numerically demonstrate the performance of PRo in data\nsummarization and influence maximization, demonstrating gains over both the\ngreedy algorithm and the algorithm of (Orlin et al., 2016). \n\n"}
{"id": "1706.05394", "contents": "Title: A Closer Look at Memorization in Deep Networks Abstract: We examine the role of memorization in deep learning, drawing connections to\ncapacity, generalization, and adversarial robustness. While deep networks are\ncapable of memorizing noise data, our results suggest that they tend to\nprioritize learning simple patterns first. In our experiments, we expose\nqualitative differences in gradient-based optimization of deep neural networks\n(DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned\nexplicit regularization (e.g., dropout) we can degrade DNN training performance\non noise datasets without compromising generalization on real data. Our\nanalysis suggests that the notions of effective capacity which are dataset\nindependent are unlikely to explain the generalization performance of deep\nnetworks when trained with gradient based methods because training data itself\nplays an important role in determining the degree of memorization. \n\n"}
{"id": "1706.05730", "contents": "Title: Addressing Item-Cold Start Problem in Recommendation Systems using Model\n  Based Approach and Deep Learning Abstract: Traditional recommendation systems rely on past usage data in order to\ngenerate new recommendations. Those approaches fail to generate sensible\nrecommendations for new users and items into the system due to missing\ninformation about their past interactions. In this paper, we propose a solution\nfor successfully addressing item-cold start problem which uses model-based\napproach and recent advances in deep learning. In particular, we use latent\nfactor model for recommendation, and predict the latent factors from item's\ndescriptions using convolutional neural network when they cannot be obtained\nfrom usage data. Latent factors obtained by applying matrix factorization to\nthe available usage data are used as ground truth to train the convolutional\nneural network. To create latent factor representations for the new items, the\nconvolutional neural network uses their textual description. The results from\nthe experiments reveal that the proposed approach significantly outperforms\nseveral baseline estimators. \n\n"}
{"id": "1706.07450", "contents": "Title: Revised Note on Learning Algorithms for Quadratic Assignment with Graph\n  Neural Networks Abstract: Inverse problems correspond to a certain type of optimization problems\nformulated over appropriate input distributions. Recently, there has been a\ngrowing interest in understanding the computational hardness of these\noptimization problems, not only in the worst case, but in an average-complexity\nsense under this same input distribution.\n  In this revised note, we are interested in studying another aspect of\nhardness, related to the ability to learn how to solve a problem by simply\nobserving a collection of previously solved instances. These 'planted\nsolutions' are used to supervise the training of an appropriate predictive\nmodel that parametrizes a broad class of algorithms, with the hope that the\nresulting model will provide good accuracy-complexity tradeoffs in the average\nsense.\n  We illustrate this setup on the Quadratic Assignment Problem, a fundamental\nproblem in Network Science. We observe that data-driven models based on Graph\nNeural Networks offer intriguingly good performance, even in regimes where\nstandard relaxation based techniques appear to suffer. \n\n"}
{"id": "1706.07625", "contents": "Title: Specializing Joint Representations for the task of Product\n  Recommendation Abstract: We propose a unified product embedded representation that is optimized for\nthe task of retrieval-based product recommendation. To this end, we introduce a\nnew way to fuse modality-specific product embeddings into a joint product\nembedding, in order to leverage both product content information, such as\ntextual descriptions and images, and product collaborative filtering signal. By\nintroducing the fusion step at the very end of our architecture, we are able to\ntrain each modality separately, allowing us to keep a modular architecture that\nis preferable in real-world recommendation deployments. We analyze our\nperformance on normal and hard recommendation setups such as cold-start and\ncross-category recommendations and achieve good performance on a large product\nshopping dataset. \n\n"}
{"id": "1706.07642", "contents": "Title: A Variance Maximization Criterion for Active Learning Abstract: Active learning aims to train a classifier as fast as possible with as few\nlabels as possible. The core element in virtually any active learning strategy\nis the criterion that measures the usefulness of the unlabeled data based on\nwhich new points to be labeled are picked. We propose a novel approach which we\nrefer to as maximizing variance for active learning or MVAL for short. MVAL\nmeasures the value of unlabeled instances by evaluating the rate of change of\noutput variables caused by changes in the next sample to be queried and its\npotential labelling. In a sense, this criterion measures how unstable the\nclassifier's output is for the unlabeled data points under perturbations of the\ntraining data. MVAL maintains, what we refer to as, retraining information\nmatrices to keep track of these output scores and exploits two kinds of\nvariance to measure the informativeness and representativeness, respectively.\nBy fusing these variances, MVAL is able to select the instances which are both\ninformative and representative. We employ our technique both in combination\nwith logistic regression and support vector machines and demonstrate that MVAL\nachieves state-of-the-art performance in experiments on a large number of\nstandard benchmark datasets. \n\n"}
{"id": "1706.08217", "contents": "Title: An Effective Way to Improve YouTube-8M Classification Accuracy in Google\n  Cloud Platform Abstract: Large-scale datasets have played a significant role in progress of neural\nnetwork and deep learning areas. YouTube-8M is such a benchmark dataset for\ngeneral multi-label video classification. It was created from over 7 million\nYouTube videos (450,000 hours of video) and includes video labels from a\nvocabulary of 4716 classes (3.4 labels/video on average). It also comes with\npre-extracted audio & visual features from every second of video (3.2 billion\nfeature vectors in total). Google cloud recently released the datasets and\norganized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.\nCompetitors are challenged to develop classification algorithms that assign\nvideo-level labels using the new and improved Youtube-8M V2 dataset. Inspired\nby the competition, we started exploration of audio understanding and\nclassification using deep learning algorithms and ensemble methods. We built\nseveral baseline predictions according to the benchmark paper and public github\ntensorflow code. Furthermore, we improved global prediction accuracy (GAP) from\nbase level 77% to 80.7% through approaches of ensemble. \n\n"}
{"id": "1706.08928", "contents": "Title: Classical Music Clustering Based on Acoustic Features Abstract: In this paper we cluster 330 classical music pieces collected from MusicNet\ndatabase based on their musical note sequence. We use shingling and chord\ntrajectory matrices to create signature for each music piece and performed\nspectral clustering to find the clusters. Based on different resolution, the\noutput clusters distinctively indicate composition from different classical\nmusic era and different composing style of the musicians. \n\n"}
{"id": "1706.09067", "contents": "Title: Structured Recommendation Abstract: Current recommender systems largely focus on static, unstructured content. In\nmany scenarios, we would like to recommend content that has structure, such as\na trajectory of points-of-interests in a city, or a playlist of songs. Dubbed\nStructured Recommendation, this problem differs from the typical structured\nprediction problem in that there are multiple correct answers for a given\ninput. Motivated by trajectory recommendation, we focus on sequential\nstructures but in contrast to classical Viterbi decoding we require that valid\npredictions are sequences with no repeated elements. We propose an approach to\nsequence recommendation based on the structured support vector machine. For\nprediction, we modify the inference procedure to avoid predicting loops in the\nsequence. For training, we modify the objective function to account for the\nexistence of multiple ground truths for a given input. We also modify the\nloss-augmented inference procedure to exclude the known ground truths.\nExperiments on real-world trajectory recommendation datasets show the benefits\nof our approach over existing, non-structured recommendation approaches. \n\n"}
{"id": "1706.09317", "contents": "Title: Alternative Semantic Representations for Zero-Shot Human Action\n  Recognition Abstract: A proper semantic representation for encoding side information is key to the\nsuccess of zero-shot learning. In this paper, we explore two alternative\nsemantic representations especially for zero-shot human action recognition:\ntextual descriptions of human actions and deep features extracted from still\nimages relevant to human actions. Such side information are accessible on Web\nwith little cost, which paves a new way in gaining side information for\nlarge-scale zero-shot human action recognition. We investigate different\nencoding methods to generate semantic representations for human actions from\nsuch side information. Based on our zero-shot visual recognition method, we\nconducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic\nrepresentations . The results suggest that our proposed text- and image-based\nsemantic representations outperform traditional attributes and word vectors\nconsiderably for zero-shot human action recognition. In particular, the\nimage-based semantic representations yield the favourable performance even\nthough the representation is extracted from a small number of images per class. \n\n"}
{"id": "1706.09627", "contents": "Title: Deep learning bank distress from news and numerical financial data Abstract: In this paper we focus our attention on the exploitation of the information\ncontained in financial news to enhance the performance of a classifier of bank\ndistress. Such information should be analyzed and inserted into the predictive\nmodel in the most efficient way and this task deals with all the issues related\nto text analysis and specifically analysis of news media. Among the different\nmodels proposed for such purpose, we investigate one of the possible deep\nlearning approaches, based on a doc2vec representation of the textual data, a\nkind of neural network able to map the sequential and symbolic text input onto\na reduced latent semantic space. Afterwards, a second supervised neural network\nis trained combining news data with standard financial figures to classify\nbanks whether in distressed or tranquil states, based on a small set of known\ndistress events. Then the final aim is not only the improvement of the\npredictive performance of the classifier but also to assess the importance of\nnews data in the classification process. Does news data really bring more\nuseful information not contained in standard financial variables? Our results\nseem to confirm such hypothesis. \n\n"}
{"id": "1706.10192", "contents": "Title: Co-PACRR: A Context-Aware Neural IR Model for Ad-hoc Retrieval Abstract: Neural IR models, such as DRMM and PACRR, have achieved strong results by\nsuccessfully capturing relevance matching signals. We argue that the context of\nthese matching signals is also important. Intuitively, when extracting,\nmodeling, and combining matching signals, one would like to consider the\nsurrounding text (local context) as well as other signals from the same\ndocument that can contribute to the overall relevance score. In this work, we\nhighlight three potential shortcomings caused by not considering context\ninformation and propose three neural ingredients to address them: a\ndisambiguation component, cascade k-max pooling, and a shuffling combination\nlayer. Incorporating these components into the PACRR model yields Co-PACRR, a\nnovel context-aware neural IR model. Extensive comparisons with established\nmodels on Trec Web Track data confirm that the proposed model can achieve\nsuperior search results. In addition, an ablation analysis is conducted to gain\ninsights into the impact of and interactions between different components. We\nrelease our code to enable future comparisons. \n\n"}
{"id": "1707.00061", "contents": "Title: Racial Disparity in Natural Language Processing: A Case Study of Social\n  Media African-American English Abstract: We highlight an important frontier in algorithmic fairness: disparity in the\nquality of natural language processing algorithms when applied to language from\nauthors of different social groups. For example, current systems sometimes\nanalyze the language of females and minorities more poorly than they do of\nwhites and males. We conduct an empirical analysis of racial disparity in\nlanguage identification for tweets written in African-American English, and\ndiscuss implications of disparity in NLP. \n\n"}
{"id": "1707.00469", "contents": "Title: Speeding Up String Matching by Weak Factor Recognition Abstract: String matching is the problem of finding all the substrings of a text which\nmatch a given pattern. It is one of the most investigated problems in computer\nscience, mainly due to its very diverse applications in several fields.\nRecently, much research in the string matching field has focused on the\nefficiency and flexibility of the searching procedure and quite effective\ntechniques have been proposed for speeding up the existing solutions. In this\ncontext, algorithms based on factors recognition are among the best solutions.\nIn this paper, we present a simple and very efficient algorithm for string\nmatching based on a weak factor recognition and hashing. Our algorithm has a\nquadratic worst-case running time. However, despite its quadratic complexity,\nexperimental results show that our algorithm obtains in most cases the best\nrunning times when compared, under various conditions, against the most\neffective algorithms present in literature. In the case of small alphabets and\nlong patterns, the gain in running times reaches 28%. This makes our proposed\nalgorithm one of the most flexible solutions in practical cases. \n\n"}
{"id": "1707.01780", "contents": "Title: On the Role of Text Preprocessing in Neural Network Architectures: An\n  Evaluation Study on Text Categorization and Sentiment Analysis Abstract: Text preprocessing is often the first step in the pipeline of a Natural\nLanguage Processing (NLP) system, with potential impact in its final\nperformance. Despite its importance, text preprocessing has not received much\nattention in the deep learning literature. In this paper we investigate the\nimpact of simple text preprocessing decisions (particularly tokenizing,\nlemmatizing, lowercasing and multiword grouping) on the performance of a\nstandard neural text classifier. We perform an extensive evaluation on standard\nbenchmarks from text categorization and sentiment analysis. While our\nexperiments show that a simple tokenization of input text is generally\nadequate, they also highlight significant degrees of variability across\npreprocessing techniques. This reveals the importance of paying attention to\nthis usually-overlooked step in the pipeline, particularly when comparing\ndifferent models. Finally, our evaluation provides insights into the best\npreprocessing practices for training word embeddings. \n\n"}
{"id": "1707.02494", "contents": "Title: Analysis of Footnote Chasing and Citation Searching in an Academic\n  Search Engine Abstract: In interactive information retrieval, researchers consider the user behavior\ntowards systems and search tasks in order to adapt search results by analyzing\ntheir past interactions. In this paper, we analyze the user behavior towards\nMarcia Bates' search stratagems such as 'footnote chasing' and 'citation\nsearch' in an academic search engine. We performed a preliminary analysis of\ntheir frequency and stage of use in the social sciences search engine sowiport.\nIn addition, we explored the impact of these stratagems on the whole search\nprocess performance. We can conclude that the appearance of these two search\nfeatures in real retrieval sessions lead to an improvement of the precision in\nterms of positive interactions with 16% when using footnote chasing and 17% for\nthe citation search stratagem. \n\n"}
{"id": "1707.03367", "contents": "Title: Wextractor: Follow-up of the evolution of prices in web pages Abstract: In the e-commerce world, the follow-up of prices in detail web pages is of\ngreat interest for things like buying a product when it falls below some\nthreshold. For doing this task, instead of bookmarking the pages and revisiting\nthem, in this paper we propose a novel web data extraction system, called\nWextractor. It consists of an extraction method and a web app for listing the\nretrieved prices. As for the final user, the main feature of Wextractor is\nusability because (s)he only has to signal the pages of interest and our system\nautomatically extracts the price from the page. \n\n"}
{"id": "1707.03815", "contents": "Title: Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via\n  Ranking Abstract: Methods that learn representations of nodes in a graph play a critical role\nin network analysis since they enable many downstream learning tasks. We\npropose Graph2Gauss - an approach that can efficiently learn versatile node\nembeddings on large scale (attributed) graphs that show strong performance on\ntasks such as link prediction and node classification. Unlike most approaches\nthat represent nodes as point vectors in a low-dimensional continuous space, we\nembed each node as a Gaussian distribution, allowing us to capture uncertainty\nabout the representation. Furthermore, we propose an unsupervised method that\nhandles inductive learning scenarios and is applicable to different types of\ngraphs: plain/attributed, directed/undirected. By leveraging both the network\nstructure and the associated node attributes, we are able to generalize to\nunseen nodes without additional training. To learn the embeddings we adopt a\npersonalized ranking formulation w.r.t. the node distances that exploits the\nnatural ordering of the nodes imposed by the network structure. Experiments on\nreal world networks demonstrate the high performance of our approach,\noutperforming state-of-the-art network embedding methods on several different\ntasks. Additionally, we demonstrate the benefits of modeling uncertainty - by\nanalyzing it we can estimate neighborhood diversity and detect the intrinsic\nlatent dimensionality of a graph. \n\n"}
{"id": "1707.04596", "contents": "Title: DocTag2Vec: An Embedding Based Multi-label Learning Approach for\n  Document Tagging Abstract: Tagging news articles or blog posts with relevant tags from a collection of\npredefined ones is coined as document tagging in this work. Accurate tagging of\narticles can benefit several downstream applications such as recommendation and\nsearch. In this work, we propose a novel yet simple approach called DocTag2Vec\nto accomplish this task. We substantially extend Word2Vec and Doc2Vec---two\npopular models for learning distributed representation of words and documents.\nIn DocTag2Vec, we simultaneously learn the representation of words, documents,\nand tags in a joint vector space during training, and employ the simple\n$k$-nearest neighbor search to predict tags for unseen documents. In contrast\nto previous multi-label learning methods, DocTag2Vec directly deals with raw\ntext instead of provided feature vector, and in addition, enjoys advantages\nlike the learning of tag representation, and the ability of handling newly\ncreated tags. To demonstrate the effectiveness of our approach, we conduct\nexperiments on several datasets and show promising results against\nstate-of-the-art methods. \n\n"}
{"id": "1707.04916", "contents": "Title: Multi-label Music Genre Classification from Audio, Text, and Images\n  Using Deep Features Abstract: Music genres allow to categorize musical items that share common\ncharacteristics. Although these categories are not mutually exclusive, most\nrelated research is traditionally focused on classifying tracks into a single\nclass. Furthermore, these categories (e.g., Pop, Rock) tend to be too broad for\ncertain applications. In this work we aim to expand this task by categorizing\nmusical items into multiple and fine-grained labels, using three different data\nmodalities: audio, text, and images. To this end we present MuMu, a new dataset\nof more than 31k albums classified into 250 genre classes. For every album we\nhave collected the cover image, text reviews, and audio tracks. Additionally,\nwe propose an approach for multi-label genre classification based on the\ncombination of feature embeddings learned with state-of-the-art deep learning\nmethodologies. Experiments show major differences between modalities, which not\nonly introduce new baselines for multi-label genre classification, but also\nsuggest that combining them yields improved results. \n\n"}
{"id": "1707.05176", "contents": "Title: Latent Relational Metric Learning via Memory-based Attention for\n  Collaborative Ranking Abstract: This paper proposes a new neural architecture for collaborative ranking with\nimplicit feedback. Our model, LRML (\\textit{Latent Relational Metric Learning})\nis a novel metric learning approach for recommendation. More specifically,\ninstead of simple push-pull mechanisms between user and item pairs, we propose\nto learn latent relations that describe each user item interaction. This helps\nto alleviate the potential geometric inflexibility of existing metric learing\napproaches. This enables not only better performance but also a greater extent\nof modeling capability, allowing our model to scale to a larger number of\ninteractions. In order to do so, we employ a augmented memory module and learn\nto attend over these memory blocks to construct latent relations. The\nmemory-based attention module is controlled by the user-item interaction,\nmaking the learned relation vector specific to each user-item pair. Hence, this\ncan be interpreted as learning an exclusive and optimal relational translation\nfor each user-item interaction. The proposed architecture demonstrates the\nstate-of-the-art performance across multiple recommendation benchmarks. LRML\noutperforms other metric learning models by $6\\%-7.5\\%$ in terms of Hits@10 and\nnDCG@10 on large datasets such as Netflix and MovieLens20M. Moreover,\nqualitative studies also demonstrate evidence that our proposed model is able\nto infer and encode explicit sentiment, temporal and attribute information\ndespite being only trained on implicit feedback. As such, this ascertains the\nability of LRML to uncover hidden relational structure within implicit\ndatasets. \n\n"}
{"id": "1707.05409", "contents": "Title: Neural Matching Models for Question Retrieval and Next Question\n  Prediction in Conversation Abstract: The recent boom of AI has seen the emergence of many human-computer\nconversation systems such as Google Assistant, Microsoft Cortana, Amazon Echo\nand Apple Siri. We introduce and formalize the task of predicting questions in\nconversations, where the goal is to predict the new question that the user will\nask, given the past conversational context. This task can be modeled as a\n\"sequence matching\" problem, where two sequences are given and the aim is to\nlearn a model that maps any pair of sequences to a matching probability. Neural\nmatching models, which adopt deep neural networks to learn sequence\nrepresentations and matching scores, have attracted immense research interests\nof information retrieval and natural language processing communities. In this\npaper, we first study neural matching models for the question retrieval task\nthat has been widely explored in the literature, whereas the effectiveness of\nneural models for this task is relatively unstudied. We further evaluate the\nneural matching models in the next question prediction task in conversations.\nWe have used the publicly available Quora data and Ubuntu chat logs in our\nexperiments. Our evaluations investigate the potential of neural matching\nmodels with representation learning for question retrieval and next question\nprediction in conversations. Experimental results show that neural matching\nmodels perform well for both tasks. \n\n"}
{"id": "1707.05470", "contents": "Title: DeepProbe: Information Directed Sequence Understanding and Chatbot\n  Design via Recurrent Neural Networks Abstract: Information extraction and user intention identification are central topics\nin modern query understanding and recommendation systems. In this paper, we\npropose DeepProbe, a generic information-directed interaction framework which\nis built around an attention-based sequence to sequence (seq2seq) recurrent\nneural network. DeepProbe can rephrase, evaluate, and even actively ask\nquestions, leveraging the generative ability and likelihood estimation made\npossible by seq2seq models. DeepProbe makes decisions based on a derived\nuncertainty (entropy) measure conditioned on user inputs, possibly with\nmultiple rounds of interactions. Three applications, namely a rewritter, a\nrelevance scorer and a chatbot for ad recommendation, were built around\nDeepProbe, with the first two serving as precursory building blocks for the\nthird. We first use the seq2seq model in DeepProbe to rewrite a user query into\none of standard query form, which is submitted to an ordinary recommendation\nsystem. Secondly, we evaluate DeepProbe's seq2seq model-based relevance\nscoring. Finally, we build a chatbot prototype capable of making active user\ninteractions, which can ask questions that maximize information gain, allowing\nfor a more efficient user intention idenfication process. We evaluate first two\napplications by 1) comparing with baselines by BLEU and AUC, and 2) human judge\nevaluation. Both demonstrate significant improvements compared with current\nstate-of-the-art systems, proving their values as useful tools on their own,\nand at the same time laying a good foundation for the ongoing chatbot\napplication. \n\n"}
{"id": "1707.06613", "contents": "Title: Decoupled classifiers for fair and efficient machine learning Abstract: When it is ethical and legal to use a sensitive attribute (such as gender or\nrace) in machine learning systems, the question remains how to do so. We show\nthat the naive application of machine learning algorithms using sensitive\nfeatures leads to an inherent tradeoff in accuracy between groups. We provide a\nsimple and efficient decoupling technique, that can be added on top of any\nblack-box machine learning algorithm, to learn different classifiers for\ndifferent groups. Transfer learning is used to mitigate the problem of having\ntoo little data on any one group.\n  The method can apply to a range of fairness criteria. In particular, we\nrequire the application designer to specify as joint loss function that makes\nexplicit the trade-off between fairness and accuracy. Our reduction is shown to\nefficiently find the minimum loss as long as the objective has a certain\nnatural monotonicity property which may be of independent interest in the study\nof fairness in algorithms. \n\n"}
{"id": "1707.06618", "contents": "Title: Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex\n  Optimization Abstract: We present a unified framework to analyze the global convergence of Langevin\ndynamics based algorithms for nonconvex finite-sum optimization with $n$\ncomponent functions. At the core of our analysis is a direct analysis of the\nergodicity of the numerical approximations to Langevin dynamics, which leads to\nfaster convergence rates. Specifically, we show that gradient Langevin dynamics\n(GLD) and stochastic gradient Langevin dynamics (SGLD) converge to the almost\nminimizer within $\\tilde O\\big(nd/(\\lambda\\epsilon) \\big)$ and $\\tilde\nO\\big(d^7/(\\lambda^5\\epsilon^5) \\big)$ stochastic gradient evaluations\nrespectively, where $d$ is the problem dimension, and $\\lambda$ is the spectral\ngap of the Markov chain generated by GLD. Both results improve upon the best\nknown gradient complexity results (Raginsky et al., 2017). Furthermore, for the\nfirst time we prove the global convergence guarantee for variance reduced\nstochastic gradient Langevin dynamics (SVRG-LD) to the almost minimizer within\n$\\tilde O\\big(\\sqrt{n}d^5/(\\lambda^4\\epsilon^{5/2})\\big)$ stochastic gradient\nevaluations, which outperforms the gradient complexities of GLD and SGLD in a\nwide regime. Our theoretical analyses shed some light on using Langevin\ndynamics based algorithms for nonconvex optimization with provable guarantees. \n\n"}
{"id": "1707.07435", "contents": "Title: Deep Learning based Recommender System: A Survey and New Perspectives Abstract: With the ever-growing volume of online information, recommender systems have\nbeen an effective strategy to overcome such information overload. The utility\nof recommender systems cannot be overstated, given its widespread adoption in\nmany web applications, along with its potential impact to ameliorate many\nproblems related to over-choice. In recent years, deep learning has garnered\nconsiderable interest in many research fields such as computer vision and\nnatural language processing, owing not only to stellar performance but also the\nattractive property of learning feature representations from scratch. The\ninfluence of deep learning is also pervasive, recently demonstrating its\neffectiveness when applied to information retrieval and recommender systems\nresearch. Evidently, the field of deep learning in recommender system is\nflourishing. This article aims to provide a comprehensive review of recent\nresearch efforts on deep learning based recommender systems. More concretely,\nwe provide and devise a taxonomy of deep learning based recommendation models,\nalong with providing a comprehensive summary of the state-of-the-art. Finally,\nwe expand on current trends and provide new perspectives pertaining to this new\nexciting development of the field. \n\n"}
{"id": "1707.07821", "contents": "Title: Concept Drift Detection and Adaptation with Hierarchical Hypothesis\n  Testing Abstract: A fundamental issue for statistical classification models in a streaming\nenvironment is that the joint distribution between predictor and response\nvariables changes over time (a phenomenon also known as concept drifts), such\nthat their classification performance deteriorates dramatically. In this paper,\nwe first present a hierarchical hypothesis testing (HHT) framework that can\ndetect and also adapt to various concept drift types (e.g., recurrent or\nirregular, gradual or abrupt), even in the presence of imbalanced data labels.\nA novel concept drift detector, namely Hierarchical Linear Four Rates (HLFR),\nis implemented under the HHT framework thereafter. By substituting a\nwidely-acknowledged retraining scheme with an adaptive training strategy, we\nfurther demonstrate that the concept drift adaptation capability of HLFR can be\nsignificantly boosted. The theoretical analysis on the Type-I and Type-II\nerrors of HLFR is also performed. Experiments on both simulated and real-world\ndatasets illustrate that our methods outperform state-of-the-art methods in\nterms of detection precision, detection delay as well as the adaptability\nacross different concept drift types. \n\n"}
{"id": "1707.07831", "contents": "Title: Linear Discriminant Generative Adversarial Networks Abstract: We develop a novel method for training of GANs for unsupervised and class\nconditional generation of images, called Linear Discriminant GAN (LD-GAN). The\ndiscriminator of an LD-GAN is trained to maximize the linear separability\nbetween distributions of hidden representations of generated and targeted\nsamples, while the generator is updated based on the decision hyper-planes\ncomputed by performing LDA over the hidden representations. LD-GAN provides a\nconcrete metric of separation capacity for the discriminator, and we\nexperimentally show that it is possible to stabilize the training of LD-GAN\nsimply by calibrating the update frequencies between generators and\ndiscriminators in the unsupervised case, without employment of normalization\nmethods and constraints on weights. In the class conditional generation tasks,\nthe proposed method shows improved training stability together with better\ngeneralization performance compared to WGAN that employs an auxiliary\nclassifier. \n\n"}
{"id": "1707.08029", "contents": "Title: Price and Profit Awareness in Recommender Systems Abstract: Academic research in the field of recommender systems mainly focuses on the\nproblem of maximizing the users' utility by trying to identify the most\nrelevant items for each user. However, such items are not necessarily the ones\nthat maximize the utility of the service provider (e.g., an online retailer) in\nterms of the business value, such as profit. One approach to increasing the\nproviders' utility is to incorporate purchase-oriented information, e.g., the\nprice, sales probabilities, and the resulting profit, into the recommendation\nalgorithms. In this paper we specifically focus on price- and profit-aware\nrecommender systems. We provide a brief overview of the relevant literature and\nuse numerical simulations to illustrate the potential business benefit of such\napproaches. \n\n"}
{"id": "1707.08309", "contents": "Title: Probabilistic Graphical Models for Credibility Analysis in Evolving\n  Online Communities Abstract: One of the major hurdles preventing the full exploitation of information from\nonline communities is the widespread concern regarding the quality and\ncredibility of user-contributed content. Prior works in this domain operate on\na static snapshot of the community, making strong assumptions about the\nstructure of the data (e.g., relational tables), or consider only shallow\nfeatures for text classification.\n  To address the above limitations, we propose probabilistic graphical models\nthat can leverage the joint interplay between multiple factors in online\ncommunities --- like user interactions, community dynamics, and textual content\n--- to automatically assess the credibility of user-contributed online content,\nand the expertise of users and their evolution with user-interpretable\nexplanation. To this end, we devise new models based on Conditional Random\nFields for different settings like incorporating partial expert knowledge for\nsemi-supervised learning, and handling discrete labels as well as numeric\nratings for fine-grained analysis. This enables applications such as extracting\nreliable side-effects of drugs from user-contributed posts in healthforums, and\nidentifying credible content in news communities.\n  Online communities are dynamic, as users join and leave, adapt to evolving\ntrends, and mature over time. To capture this dynamics, we propose generative\nmodels based on Hidden Markov Model, Latent Dirichlet Allocation, and Brownian\nMotion to trace the continuous evolution of user expertise and their language\nmodel over time. This allows us to identify expert users and credible content\njointly over time, improving state-of-the-art recommender systems by explicitly\nconsidering the maturity of users. This also enables applications such as\nidentifying helpful product reviews, and detecting fake and anomalous reviews\nwith limited information. \n\n"}
{"id": "1707.09430", "contents": "Title: Human in the Loop: Interactive Passive Automata Learning via\n  Evidence-Driven State-Merging Algorithms Abstract: We present an interactive version of an evidence-driven state-merging (EDSM)\nalgorithm for learning variants of finite state automata. Learning these\nautomata often amounts to recovering or reverse engineering the model\ngenerating the data despite noisy, incomplete, or imperfectly sampled data\nsources rather than optimizing a purely numeric target function. Domain\nexpertise and human knowledge about the target domain can guide this process,\nand typically is captured in parameter settings. Often, domain expertise is\nsubconscious and not expressed explicitly. Directly interacting with the\nlearning algorithm makes it easier to utilize this knowledge effectively. \n\n"}
{"id": "1707.09561", "contents": "Title: Fine-Gray competing risks model with high-dimensional covariates:\n  estimation and Inference Abstract: The purpose of this paper is to construct confidence intervals for the\nregression coefficients in the Fine-Gray model for competing risks data with\nrandom censoring, where the number of covariates can be larger than the sample\nsize. Despite strong motivation from biomedical applications, a\nhigh-dimensional Fine-Gray model has attracted relatively little attention\namong the methodological or theoretical literature. We fill in this gap by\ndeveloping confidence intervals based on a one-step bias-correction for a\nregularized estimation. We develop a theoretical framework for the partial\nlikelihood, which does not have independent and identically distributed entries\nand therefore presents many technical challenges. We also study the\napproximation error from the weighting scheme under random censoring for\ncompeting risks and establish new concentration results for time-dependent\nprocesses. In addition to the theoretical results and algorithms, we present\nextensive numerical experiments and an application to a study of non-cancer\nmortality among prostate cancer patients using the linked Medicare-SEER data. \n\n"}
{"id": "1708.00651", "contents": "Title: A Multi-Objective Learning to re-Rank Approach to Optimize Online\n  Marketplaces for Multiple Stakeholders Abstract: Multi-objective recommender systems address the difficult task of\nrecommending items that are relevant to multiple, possibly conflicting,\ncriteria. However these systems are most often designed to address the\nobjective of one single stakeholder, typically, in online commerce, the\nconsumers whose input and purchasing decisions ultimately determine the success\nof the recommendation systems. In this work, we address the multi-objective,\nmulti-stakeholder, recommendation problem involving one or more objective(s)\nper stakeholder. In addition to the consumer stakeholder, we also consider two\nother stakeholders; the suppliers who provide the goods and services for sale\nand the intermediary who is responsible for helping connect consumers to\nsuppliers via its recommendation algorithms. We analyze the multi-objective,\nmulti-stakeholder, problem from the point of view of the online marketplace\nintermediary whose objective is to maximize its commission through its\nrecommender system. We define a multi-objective problem relating all our three\nstakeholders which we solve with a novel learning-to-re-rank approach that\nmakes use of a novel regularization function based on the Kendall tau\ncorrelation metric and its kernel version; given an initial ranking of item\nrecommendations built for the consumer, we aim to re-rank it such that the new\nranking is also optimized for the secondary objectives while staying close to\nthe initial ranking. We evaluate our approach on a real-world dataset of hotel\nrecommendations provided by Expedia where we show the effectiveness of our\napproach against a business-rules oriented baseline model. \n\n"}
{"id": "1708.01715", "contents": "Title: Training Deep AutoEncoders for Collaborative Filtering Abstract: This paper proposes a novel model for the rating prediction task in\nrecommender systems which significantly outperforms previous state-of-the art\nmodels on a time-split Netflix data set. Our model is based on deep autoencoder\nwith 6 layers and is trained end-to-end without any layer-wise pre-training. We\nempirically demonstrate that: a) deep autoencoder models generalize much better\nthan the shallow ones, b) non-linear activation functions with negative parts\nare crucial for training deep models, and c) heavy use of regularization\ntechniques such as dropout is necessary to prevent over-fiting. We also propose\na new training algorithm based on iterative output re-feeding to overcome\nnatural sparseness of collaborate filtering. The new algorithm significantly\nspeeds up training and improves model performance. Our code is available at\nhttps://github.com/NVIDIA/DeepRecommender \n\n"}
{"id": "1708.02238", "contents": "Title: A Convolutional Neural Network for Search Term Detection Abstract: Pathfinding in hospitals is challenging for patients, visitors, and even\nemployees. Many people have experienced getting lost due to lack of clear\nguidance, large footprint of hospitals, and confusing array of hospital wings.\nIn this paper, we propose Halo; An indoor navigation application based on\nvoice-user interaction to help provide directions for users without assistance\nof a localization system. The main challenge is accurate detection of origin\nand destination search terms. A custom convolutional neural network (CNN) is\nproposed to detect origin and destination search terms from transcription of a\nsubmitted speech query. The CNN is trained based on a set of queries tailored\nspecifically for hospital and clinic environments. Performance of the proposed\nmodel is studied and compared with Levenshtein distance-based word matching. \n\n"}
{"id": "1708.04312", "contents": "Title: Collaborative Filtering using Denoising Auto-Encoders for Market Basket\n  Data Abstract: Recommender systems (RS) help users navigate large sets of items in the\nsearch for \"interesting\" ones. One approach to RS is Collaborative Filtering\n(CF), which is based on the idea that similar users are interested in similar\nitems. Most model-based approaches to CF seek to train a\nmachine-learning/data-mining model based on sparse data; the model is then used\nto provide recommendations. While most of the proposed approaches are effective\nfor small-size situations, the combinatorial nature of the problem makes it\nimpractical for medium-to-large instances. In this work we present a novel\napproach to CF that works by training a Denoising Auto-Encoder (DAE) on\ncorrupted baskets, i.e., baskets from which one or more items have been\nremoved. The DAE is then forced to learn to reconstruct the original basket\ngiven its corrupted input. Due to recent advancements in optimization and other\ntechnologies for training neural-network models (such as DAE), the proposed\nmethod results in a scalable and practical approach to CF. The contribution of\nthis work is twofold: (1) to identify missing items in observed baskets and,\nthus, directly providing a CF model; and, (2) to construct a generative model\nof baskets which may be used, for instance, in simulation analysis or as part\nof a more complex analytical method. \n\n"}
{"id": "1708.04358", "contents": "Title: Continuous Representation of Location for Geolocation and Lexical\n  Dialectology using Mixture Density Networks Abstract: We propose a method for embedding two-dimensional locations in a continuous\nvector space using a neural network-based model incorporating mixtures of\nGaussian distributions, presenting two model variants for text-based\ngeolocation and lexical dialectology. Evaluated over Twitter data, the proposed\nmodel outperforms conventional regression-based geolocation and provides a\nbetter estimate of uncertainty. We also show the effectiveness of the\nrepresentation for predicting words from location in lexical dialectology, and\nevaluate it using the DARE dataset. \n\n"}
{"id": "1708.05851", "contents": "Title: Image2song: Song Retrieval via Bridging Image Content and Lyric Words Abstract: Image is usually taken for expressing some kinds of emotions or purposes,\nsuch as love, celebrating Christmas. There is another better way that combines\nthe image and relevant song to amplify the expression, which has drawn much\nattention in the social network recently. Hence, the automatic selection of\nsongs should be expected. In this paper, we propose to retrieve semantic\nrelevant songs just by an image query, which is named as the image2song\nproblem. Motivated by the requirements of establishing correlation in\nsemantic/content, we build a semantic-based song retrieval framework, which\nlearns the correlation between image content and lyric words. This model uses a\nconvolutional neural network to generate rich tags from image regions, a\nrecurrent neural network to model lyric, and then establishes correlation via a\nmulti-layer perceptron. To reduce the content gap between image and lyric, we\npropose to make the lyric modeling focus on the main image content via a tag\nattention. We collect a dataset from the social-sharing multimodal data to\nstudy the proposed problem, which consists of (image, music clip, lyric)\ntriplets. We demonstrate that our proposed model shows noticeable results in\nthe image2song retrieval task and provides suitable songs. Besides, the\nsong2image task is also performed. \n\n"}
{"id": "1708.08123", "contents": "Title: Impact of Feature Selection on Micro-Text Classification Abstract: Social media datasets, especially Twitter tweets, are popular in the field of\ntext classification. Tweets are a valuable source of micro-text (sometimes\nreferred to as \"micro-blogs\"), and have been studied in domains such as\nsentiment analysis, recommendation systems, spam detection, clustering, among\nothers. Tweets often include keywords referred to as \"Hashtags\" that can be\nused as labels for the tweet. Using tweets encompassing 50 labels, we studied\nthe impact of word versus character-level feature selection and extraction on\ndifferent learners to solve a multi-class classification task. We show that\nfeature extraction of simple character-level groups performs better than simple\nword groups and pre-processing methods like normalizing using Porter's Stemming\nand Part-of-Speech (\"POS\")-Lemmatization. \n\n"}
{"id": "1708.08994", "contents": "Title: Clustering Patients with Tensor Decomposition Abstract: In this paper we present a method for the unsupervised clustering of\nhigh-dimensional binary data, with a special focus on electronic healthcare\nrecords. We present a robust and efficient heuristic to face this problem using\ntensor decomposition. We present the reasons why this approach is preferable\nfor tasks such as clustering patient records, to more commonly used\ndistance-based methods. We run the algorithm on two datasets of healthcare\nrecords, obtaining clinically meaningful results. \n\n"}
{"id": "1708.09025", "contents": "Title: Unsupervised Terminological Ontology Learning based on Hierarchical\n  Topic Modeling Abstract: In this paper, we present hierarchical relationbased latent Dirichlet\nallocation (hrLDA), a data-driven hierarchical topic model for extracting\nterminological ontologies from a large number of heterogeneous documents. In\ncontrast to traditional topic models, hrLDA relies on noun phrases instead of\nunigrams, considers syntax and document structures, and enriches topic\nhierarchies with topic relations. Through a series of experiments, we\ndemonstrate the superiority of hrLDA over existing topic models, especially for\nbuilding hierarchies. Furthermore, we illustrate the robustness of hrLDA in the\nsettings of noisy data sets, which are likely to occur in many practical\nscenarios. Our ontology evaluation results show that ontologies extracted from\nhrLDA are very competitive with the ontologies created by domain experts. \n\n"}
{"id": "1708.09252", "contents": "Title: THAP: A Matlab Toolkit for Learning with Hawkes Processes Abstract: As a powerful tool of asynchronous event sequence analysis, point processes\nhave been studied for a long time and achieved numerous successes in different\nfields. Among various point process models, Hawkes process and its variants\nattract many researchers in statistics and computer science these years because\nthey capture the self- and mutually-triggering patterns between different\nevents in complicated sequences explicitly and quantitatively and are broadly\napplicable to many practical problems. In this paper, we describe an\nopen-source toolkit implementing many learning algorithms and analysis tools\nfor Hawkes process model and its variants. Our toolkit systematically\nsummarizes recent state-of-the-art algorithms as well as most classic\nalgorithms of Hawkes processes, which is beneficial for both academical\neducation and research. Source code can be downloaded from\nhttps://github.com/HongtengXu/Hawkes-Process-Toolkit. \n\n"}
{"id": "1709.00740", "contents": "Title: Deep rank-based transposition-invariant distances on musical sequences Abstract: Distances on symbolic musical sequences are needed for a variety of\napplications, from music retrieval to automatic music generation. These musical\nsequences belong to a given corpus (or style) and it is obvious that a good\ndistance on musical sequences should take this information into account; being\nable to define a distance ex nihilo which could be applicable to all music\nstyles seems implausible. A distance could also be invariant under some\ntransformations, such as transpositions, so that it can be used as a distance\nbetween musical motives rather than musical sequences. However, to our\nknowledge, none of the approaches to devise musical distances seem to address\nthese issues. This paper introduces a method to build transposition-invariant\ndistances on symbolic musical sequences which are learned from data. It is a\nhybrid distance which combines learned feature representations of musical\nsequences with a handcrafted rank distance. This distance depends less on the\nmusical encoding of the data than previous methods and gives perceptually good\nresults. We demonstrate its efficiency on the dataset of chorale melodies by\nJ.S. Bach. \n\n"}
{"id": "1709.01006", "contents": "Title: Learning Implicit Generative Models Using Differentiable Graph Tests Abstract: Recently, there has been a growing interest in the problem of learning rich\nimplicit models - those from which we can sample, but can not evaluate their\ndensity. These models apply some parametric function, such as a deep network,\nto a base measure, and are learned end-to-end using stochastic optimization.\nOne strategy of devising a loss function is through the statistics of two\nsample tests - if we can fool a statistical test, the learned distribution\nshould be a good model of the true data. However, not all tests can easily fit\ninto this framework, as they might not be differentiable with respect to the\ndata points, and hence with respect to the parameters of the implicit model.\nMotivated by this problem, in this paper we show how two such classical tests,\nthe Friedman-Rafsky and k-nearest neighbour tests, can be effectively smoothed\nusing ideas from undirected graphical models - the matrix tree theorem and\ncardinality potentials. Moreover, as we show experimentally, smoothing can\nsignificantly increase the power of the test, which might of of independent\ninterest. Finally, we apply our method to learn implicit models. \n\n"}
{"id": "1709.01427", "contents": "Title: Stochastic Gradient Descent: Going As Fast As Possible But Not Faster Abstract: When applied to training deep neural networks, stochastic gradient descent\n(SGD) often incurs steady progression phases, interrupted by catastrophic\nepisodes in which loss and gradient norm explode. A possible mitigation of such\nevents is to slow down the learning process. This paper presents a novel\napproach to control the SGD learning rate, that uses two statistical tests. The\nfirst one, aimed at fast learning, compares the momentum of the normalized\ngradient vectors to that of random unit vectors and accordingly gracefully\nincreases or decreases the learning rate. The second one is a change point\ndetection test, aimed at the detection of catastrophic learning episodes; upon\nits triggering the learning rate is instantly halved. Both abilities of\nspeeding up and slowing down the learning rate allows the proposed approach,\ncalled SALeRA, to learn as fast as possible but not faster. Experiments on\nstandard benchmarks show that SALeRA performs well in practice, and compares\nfavorably to the state of the art. \n\n"}
{"id": "1709.01532", "contents": "Title: Interacting Attention-gated Recurrent Networks for Recommendation Abstract: Capturing the temporal dynamics of user preferences over items is important\nfor recommendation. Existing methods mainly assume that all time steps in\nuser-item interaction history are equally relevant to recommendation, which\nhowever does not apply in real-world scenarios where user-item interactions can\noften happen accidentally. More importantly, they learn user and item dynamics\nseparately, thus failing to capture their joint effects on user-item\ninteractions. To better model user and item dynamics, we present the\nInteracting Attention-gated Recurrent Network (IARN) which adopts the attention\nmodel to measure the relevance of each time step. In particular, we propose a\nnovel attention scheme to learn the attention scores of user and item history\nin an interacting way, thus to account for the dependencies between user and\nitem dynamics in shaping user-item interactions. By doing so, IARN can\nselectively memorize different time steps of a user's history when predicting\nher preferences over different items. Our model can therefore provide\nmeaningful interpretations for recommendation results, which could be further\nenhanced by auxiliary features. Extensive validation on real-world datasets\nshows that IARN consistently outperforms state-of-the-art methods. \n\n"}
{"id": "1709.01584", "contents": "Title: Using Posters to Recommend Anime and Mangas in a Cold-Start Scenario Abstract: Item cold-start is a classical issue in recommender systems that affects\nanime and manga recommendations as well. This problem can be framed as follows:\nhow to predict whether a user will like a manga that received few ratings from\nthe community? Content-based techniques can alleviate this issue but require\nextra information, that is usually expensive to gather. In this paper, we use a\ndeep learning technique, Illustration2Vec, to easily extract tag information\nfrom the manga and anime posters (e.g., sword, or ponytail). We propose BALSE\n(Blended Alternate Least Squares with Explanation), a new model for\ncollaborative filtering, that benefits from this extra information to recommend\nmangas. We show, using real data from an online manga recommender system called\nMangaki, that our model improves substantially the quality of recommendations,\nespecially for less-known manga, and is able to provide an interpretation of\nthe taste of the users. \n\n"}
{"id": "1709.01846", "contents": "Title: Symmetric Variational Autoencoder and Connections to Adversarial\n  Learning Abstract: A new form of the variational autoencoder (VAE) is proposed, based on the\nsymmetric Kullback-Leibler divergence. It is demonstrated that learning of the\nresulting symmetric VAE (sVAE) has close connections to previously developed\nadversarial-learning methods. This relationship helps unify the previously\ndistinct techniques of VAE and adversarially learning, and provides insights\nthat allow us to ameliorate shortcomings with some previously developed\nadversarial methods. In addition to an analysis that motivates and explains the\nsVAE, an extensive set of experiments validate the utility of the approach. \n\n"}
{"id": "1709.03163", "contents": "Title: Variational inference for the multi-armed contextual bandit Abstract: In many biomedical, science, and engineering problems, one must sequentially\ndecide which action to take next so as to maximize rewards. One general class\nof algorithms for optimizing interactions with the world, while simultaneously\nlearning how the world operates, is the multi-armed bandit setting and, in\nparticular, the contextual bandit case. In this setting, for each executed\naction, one observes rewards that are dependent on a given 'context', available\nat each interaction with the world. The Thompson sampling algorithm has\nrecently been shown to enjoy provable optimality properties for this set of\nproblems, and to perform well in real-world settings. It facilitates generative\nand interpretable modeling of the problem at hand. Nevertheless, the design and\ncomplexity of the model limit its application, since one must both sample from\nthe distributions modeled and calculate their expected rewards. We here show\nhow these limitations can be overcome using variational inference to\napproximate complex models, applying to the reinforcement learning case\nadvances developed for the inference case in the machine learning community\nover the past two decades. We consider contextual multi-armed bandit\napplications where the true reward distribution is unknown and complex, which\nwe approximate with a mixture model whose parameters are inferred via\nvariational inference. We show how the proposed variational Thompson sampling\napproach is accurate in approximating the true distribution, and attains\nreduced regrets even with complex reward distributions. The proposed algorithm\nis valuable for practical scenarios where restrictive modeling assumptions are\nundesirable. \n\n"}
{"id": "1709.06669", "contents": "Title: A textual transform of multivariate time-series for prognostics Abstract: Prognostics or early detection of incipient faults is an important industrial\nchallenge for condition-based and preventive maintenance. Physics-based\napproaches to modeling fault progression are infeasible due to multiple\ninteracting components, uncontrolled environmental factors and observability\nconstraints. Moreover, such approaches to prognostics do not generalize to new\ndomains. Consequently, domain-agnostic data-driven machine learning approaches\nto prognostics are desirable. Damage progression is a path-dependent process\nand explicitly modeling the temporal patterns is critical for accurate\nestimation of both the current damage state and its progression leading to\ntotal failure. In this paper, we present a novel data-driven approach to\nprognostics that employs a novel textual representation of multivariate\ntemporal sensor observations for predicting the future health state of the\nmonitored equipment early in its life. This representation enables us to\nutilize well-understood concepts from text-mining for modeling, prediction and\nunderstanding distress patterns in a domain agnostic way. The approach has been\ndeployed and successfully tested on large scale multivariate time-series data\nfrom commercial aircraft engines. We report experiments on well-known publicly\navailable benchmark datasets and simulation datasets. The proposed approach is\nshown to be superior in terms of prediction accuracy, lead time to prediction\nand interpretability. \n\n"}
{"id": "1709.07545", "contents": "Title: Attention-based Mixture Density Recurrent Networks for History-based\n  Recommendation Abstract: The goal of personalized history-based recommendation is to automatically\noutput a distribution over all the items given a sequence of previous purchases\nof a user. In this work, we present a novel approach that uses a recurrent\nnetwork for summarizing the history of purchases, continuous vectors\nrepresenting items for scalability, and a novel attention-based recurrent\nmixture density network, which outputs each component in a mixture\nsequentially, for modelling a multi-modal conditional distribution. We evaluate\nthe proposed approach on two publicly available datasets, MovieLens-20M and\nRecSys15. The experiments show that the proposed approach, which explicitly\nmodels the multi-modal nature of the predictive distribution, is able to\nimprove the performance over various baselines in terms of precision, recall\nand nDCG. \n\n"}
{"id": "1709.08267", "contents": "Title: HDLTex: Hierarchical Deep Learning for Text Classification Abstract: The continually increasing number of documents produced each year\nnecessitates ever improving information processing methods for searching,\nretrieving, and organizing text. Central to these information processing\nmethods is document classification, which has become an important application\nfor supervised learning. Recently the performance of these traditional\nclassifiers has degraded as the number of documents has increased. This is\nbecause along with this growth in the number of documents has come an increase\nin the number of categories. This paper approaches this problem differently\nfrom current document classification methods that view the problem as\nmulti-class classification. Instead we perform hierarchical classification\nusing an approach we call Hierarchical Deep Learning for Text classification\n(HDLTex). HDLTex employs stacks of deep learning architectures to provide\nspecialized understanding at each level of the document hierarchy. \n\n"}
{"id": "1709.09450", "contents": "Title: A Literature Based Approach to Define the Scope of Biomedical\n  Ontologies: A Case Study on a Rehabilitation Therapy Ontology Abstract: In this article, we investigate our early attempts at building an ontology\ndescribing rehabilitation therapies following brain injury. These therapies are\nwide-ranging, involving interventions of many different kinds. As a result,\nthese therapies are hard to describe. As well as restricting actual practice,\nthis is also a major impediment to evidence-based medicine as it is hard to\nmeaningfully compare two treatment plans.\n  Ontology development requires significant effort from both ontologists and\ndomain experts. Knowledge elicited from domain experts forms the scope of the\nontology. The process of knowledge elicitation is expensive, consumes experts'\ntime and might have biases depending on the selection of the experts. Various\nmethodologies and techniques exist for enabling this knowledge elicitation,\nincluding community groups and open development practices. A related problem is\nthat of defining scope. By defining the scope, we can decide whether a concept\n(i.e. term) should be represented in the ontology. This is the opposite of\nknowledge elicitation, in the sense that it defines what should not be in the\nontology. This can be addressed by pre-defining a set of competency questions.\n  These approaches are, however, expensive and time-consuming. Here, we\ndescribe our work toward an alternative approach, bootstrapping the ontology\nfrom an initially small corpus of literature that will define the scope of the\nontology, expanding this to a set covering the domain, then using information\nextraction to define an initial terminology to provide the basis and the\ncompetencies for the ontology. Here, we discuss four approaches to building a\nsuitable corpus that is both sufficiently covering and precise. \n\n"}
{"id": "1710.00284", "contents": "Title: Efficient and Effective Single-Document Summarizations and A\n  Word-Embedding Measurement of Quality Abstract: Our task is to generate an effective summary for a given document with\nspecific realtime requirements. We use the softplus function to enhance keyword\nrankings to favor important sentences, based on which we present a number of\nsummarization algorithms using various keyword extraction and topic clustering\nmethods. We show that our algorithms meet the realtime requirements and yield\nthe best ROUGE recall scores on DUC-02 over all previously-known algorithms. We\nshow that our algorithms meet the realtime requirements and yield the best\nROUGE recall scores on DUC-02 over all previously-known algorithms. To evaluate\nthe quality of summaries without human-generated benchmarks, we define a\nmeasure called WESM based on word-embedding using Word Mover's Distance. We\nshow that the orderings of the ROUGE and WESM scores of our algorithms are\nhighly comparable, suggesting that WESM may serve as a viable alternative for\nmeasuring the quality of a summary. \n\n"}
{"id": "1710.02261", "contents": "Title: Scalable Tucker Factorization for Sparse Tensors - Algorithms and\n  Discoveries Abstract: Given sparse multi-dimensional data (e.g., (user, movie, time; rating) for\nmovie recommendations), how can we discover latent concepts/relations and\npredict missing values? Tucker factorization has been widely used to solve such\nproblems with multi-dimensional data, which are modeled as tensors. However,\nmost Tucker factorization algorithms regard and estimate missing entries as\nzeros, which triggers a highly inaccurate decomposition. Moreover, few methods\nfocusing on an accuracy exhibit limited scalability since they require huge\nmemory and heavy computational costs while updating factor matrices. In this\npaper, we propose P-Tucker, a scalable Tucker factorization method for sparse\ntensors. P-Tucker performs alternating least squares with a row-wise update\nrule in a fully parallel way, which significantly reduces memory requirements\nfor updating factor matrices. Furthermore, we offer two variants of P-Tucker: a\ncaching algorithm P-Tucker-Cache and an approximation algorithm\nP-Tucker-Approx, both of which accelerate the update process. Experimental\nresults show that P-Tucker exhibits 1.7-14.1x speed-up and 1.4-4.8x less error\ncompared to the state-of-the-art. In addition, P-Tucker scales near linearly\nwith the number of observable entries in a tensor and number of threads. Thanks\nto P-Tucker, we successfully discover hidden concepts and relations in a\nlarge-scale real-world tensor, while existing methods cannot reveal latent\nfeatures due to their limited scalability or low accuracy. \n\n"}
{"id": "1710.02271", "contents": "Title: Unsupervised Extraction of Representative Concepts from Scientific\n  Literature Abstract: This paper studies the automated categorization and extraction of scientific\nconcepts from titles of scientific articles, in order to gain a deeper\nunderstanding of their key contributions and facilitate the construction of a\ngeneric academic knowledgebase. Towards this goal, we propose an unsupervised,\ndomain-independent, and scalable two-phase algorithm to type and extract key\nconcept mentions into aspects of interest (e.g., Techniques, Applications,\netc.). In the first phase of our algorithm we propose PhraseType, a\nprobabilistic generative model which exploits textual features and limited POS\ntags to broadly segment text snippets into aspect-typed phrases. We extend this\nmodel to simultaneously learn aspect-specific features and identify academic\ndomains in multi-domain corpora, since the two tasks mutually enhance each\nother. In the second phase, we propose an approach based on adaptor grammars to\nextract fine grained concept mentions from the aspect-typed phrases without the\nneed for any external resources or human effort, in a purely data-driven\nmanner. We apply our technique to study literature from diverse scientific\ndomains and show significant gains over state-of-the-art concept extraction\ntechniques. We also present a qualitative analysis of the results obtained. \n\n"}
{"id": "1710.02766", "contents": "Title: Bayesian Alignments of Warped Multi-Output Gaussian Processes Abstract: We propose a novel Bayesian approach to modelling nonlinear alignments of\ntime series based on latent shared information. We apply the method to the\nreal-world problem of finding common structure in the sensor data of wind\nturbines introduced by the underlying latent and turbulent wind field. The\nproposed model allows for both arbitrary alignments of the inputs and\nnon-parametric output warpings to transform the observations. This gives rise\nto multiple deep Gaussian process models connected via latent generating\nprocesses. We present an efficient variational approximation based on nested\nvariational compression and show how the model can be used to extract shared\ninformation between dependent time series, recovering an interpretable\nfunctional decomposition of the learning problem. We show results for an\nartificial data set and real-world data of two wind turbines. \n\n"}
{"id": "1710.02973", "contents": "Title: LD-SDS: Towards an Expressive Spoken Dialogue System based on\n  Linked-Data Abstract: In this work we discuss the related challenges and describe an approach\ntowards the fusion of state-of-the-art technologies from the Spoken Dialogue\nSystems (SDS) and the Semantic Web and Information Retrieval domains. We\nenvision a dialogue system named LD-SDS that will support advanced, expressive,\nand engaging user requests, over multiple, complex, rich, and open-domain data\nsources that will leverage the wealth of the available Linked Data.\nSpecifically, we focus on: a) improving the identification, disambiguation and\nlinking of entities occurring in data sources and user input; b) offering\nadvanced query services for exploiting the semantics of the data, with\nreasoning and exploratory capabilities; and c) expanding the typical\ninformation seeking dialogue model (slot filling) to better reflect real-world\nconversational search scenarios. \n\n"}
{"id": "1710.04735", "contents": "Title: On the Runtime-Efficacy Trade-off of Anomaly Detection Techniques for\n  Real-Time Streaming Data Abstract: Ever growing volume and velocity of data coupled with decreasing attention\nspan of end users underscore the critical need for real-time analytics. In this\nregard, anomaly detection plays a key role as an application as well as a means\nto verify data fidelity. Although the subject of anomaly detection has been\nresearched for over 100 years in a multitude of disciplines such as, but not\nlimited to, astronomy, statistics, manufacturing, econometrics, marketing, most\nof the existing techniques cannot be used as is on real-time data streams.\nFurther, the lack of characterization of performance -- both with respect to\nreal-timeliness and accuracy -- on production data sets makes model selection\nvery challenging. To this end, we present an in-depth analysis, geared towards\nreal-time streaming data, of anomaly detection techniques. Given the\nrequirements with respect to real-timeliness and accuracy, the analysis\npresented in this paper should serve as a guide for selection of the \"best\"\nanomaly detection technique. To the best of our knowledge, this is the first\ncharacterization of anomaly detection techniques proposed in very diverse set\nof fields, using production data sets corresponding to a wide set of\napplication domains. \n\n"}
{"id": "1710.05053", "contents": "Title: Automated Scalable Bayesian Inference via Hilbert Coresets Abstract: The automation of posterior inference in Bayesian data analysis has enabled\nexperts and nonexperts alike to use more sophisticated models, engage in faster\nexploratory modeling and analysis, and ensure experimental reproducibility.\nHowever, standard automated posterior inference algorithms are not tractable at\nthe scale of massive modern datasets, and modifications to make them so are\ntypically model-specific, require expert tuning, and can break theoretical\nguarantees on inferential quality. Building on the Bayesian coresets framework,\nthis work instead takes advantage of data redundancy to shrink the dataset\nitself as a preprocessing step, providing fully-automated, scalable Bayesian\ninference with theoretical guarantees. We begin with an intuitive reformulation\nof Bayesian coreset construction as sparse vector sum approximation, and\ndemonstrate that its automation and performance-based shortcomings arise from\nthe use of the supremum norm. To address these shortcomings we develop Hilbert\ncoresets, i.e., Bayesian coresets constructed under a norm induced by an\ninner-product on the log-likelihood function space. We propose two Hilbert\ncoreset construction algorithms---one based on importance sampling, and one\nbased on the Frank-Wolfe algorithm---along with theoretical guarantees on\napproximation quality as a function of coreset size. Since the exact\ncomputation of the proposed inner-products is model-specific, we automate the\nconstruction with a random finite-dimensional projection of the log-likelihood\nfunctions. The resulting automated coreset construction algorithm is simple to\nimplement, and experiments on a variety of models with real and synthetic\ndatasets show that it provides high-quality posterior approximations and a\nsignificant reduction in the computational cost of inference. \n\n"}
{"id": "1710.05780", "contents": "Title: A retrieval-based dialogue system utilizing utterance and context\n  embeddings Abstract: Finding semantically rich and computer-understandable representations for\ntextual dialogues, utterances and words is crucial for dialogue systems (or\nconversational agents), as their performance mostly depends on understanding\nthe context of conversations. Recent research aims at finding distributed\nvector representations (embeddings) for words, such that semantically similar\nwords are relatively close within the vector-space. Encoding the \"meaning\" of\ntext into vectors is a current trend, and text can range from words, phrases\nand documents to actual human-to-human conversations. In recent research\napproaches, responses have been generated utilizing a decoder architecture,\ngiven the vector representation of the current conversation. In this paper, the\nutilization of embeddings for answer retrieval is explored by using\nLocality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor\n(ANN) model, to find similar conversations in a corpus and rank possible\ncandidates. Experimental results on the well-known Ubuntu Corpus (in English)\nand a customer service chat dataset (in Dutch) show that, in combination with a\ncandidate selection method, retrieval-based approaches outperform generative\nones and reveal promising future research directions towards the usability of\nsuch a system. \n\n"}
{"id": "1710.06276", "contents": "Title: Smooth and Sparse Optimal Transport Abstract: Entropic regularization is quickly emerging as a new standard in optimal\ntransport (OT). It enables to cast the OT computation as a differentiable and\nunconstrained convex optimization problem, which can be efficiently solved\nusing the Sinkhorn algorithm. However, entropy keeps the transportation plan\nstrictly positive and therefore completely dense, unlike unregularized OT. This\nlack of sparsity can be problematic in applications where the transportation\nplan itself is of interest. In this paper, we explore regularizing the primal\nand dual OT formulations with a strongly convex term, which corresponds to\nrelaxing the dual and primal constraints with smooth approximations. We show\nhow to incorporate squared $2$-norm and group lasso regularizations within that\nframework, leading to sparse and group-sparse transportation plans. On the\ntheoretical side, we bound the approximation error introduced by regularizing\nthe primal and dual formulations. Our results suggest that, for the regularized\nprimal, the approximation error can often be smaller with squared $2$-norm than\nwith entropic regularization. We showcase our proposed framework on the task of\ncolor transfer. \n\n"}
{"id": "1710.06382", "contents": "Title: Convergence diagnostics for stochastic gradient descent with constant\n  step size Abstract: Many iterative procedures in stochastic optimization exhibit a transient\nphase followed by a stationary phase. During the transient phase the procedure\nconverges towards a region of interest, and during the stationary phase the\nprocedure oscillates in that region, commonly around a single point. In this\npaper, we develop a statistical diagnostic test to detect such phase transition\nin the context of stochastic gradient descent with constant learning rate. We\npresent theory and experiments suggesting that the region where the proposed\ndiagnostic is activated coincides with the convergence region. For a class of\nloss functions, we derive a closed-form solution describing such region.\nFinally, we suggest an application to speed up convergence of stochastic\ngradient descent by halving the learning rate each time stationarity is\ndetected. This leads to a new variant of stochastic gradient descent, which in\nmany settings is comparable to state-of-art. \n\n"}
{"id": "1710.06997", "contents": "Title: Learning Visual Features from Snapshots for Web Search Abstract: When applying learning to rank algorithms to Web search, a large number of\nfeatures are usually designed to capture the relevance signals. Most of these\nfeatures are computed based on the extracted textual elements, link analysis,\nand user logs. However, Web pages are not solely linked texts, but have\nstructured layout organizing a large variety of elements in different styles.\nSuch layout itself can convey useful visual information, indicating the\nrelevance of a Web page. For example, the query-independent layout (i.e., raw\npage layout) can help identify the page quality, while the query-dependent\nlayout (i.e., page rendered with matched query words) can further tell rich\nstructural information (e.g., size, position and proximity) of the matching\nsignals. However, such visual information of layout has been seldom utilized in\nWeb search in the past. In this work, we propose to learn rich visual features\nautomatically from the layout of Web pages (i.e., Web page snapshots) for\nrelevance ranking. Both query-independent and query-dependent snapshots are\nconsidered as the new inputs. We then propose a novel visual perception model\ninspired by human's visual search behaviors on page viewing to extract the\nvisual features. This model can be learned end-to-end together with traditional\nhuman-crafted features. We also show that such visual features can be\nefficiently acquired in the online setting with an extended inverted indexing\nscheme. Experiments on benchmark collections demonstrate that learning visual\nfeatures from Web page snapshots can significantly improve the performance of\nrelevance ranking in ad-hoc Web retrieval tasks. \n\n"}
{"id": "1710.07400", "contents": "Title: Ligand Pose Optimization with Atomic Grid-Based Convolutional Neural\n  Networks Abstract: Docking is an important tool in computational drug discovery that aims to\npredict the binding pose of a ligand to a target protein through a combination\nof pose scoring and optimization. A scoring function that is differentiable\nwith respect to atom positions can be used for both scoring and gradient-based\noptimization of poses for docking. Using a differentiable grid-based atomic\nrepresentation as input, we demonstrate that a scoring function learned by\ntraining a convolutional neural network (CNN) to identify binding poses can\nalso be applied to pose optimization. We also show that an iteratively-trained\nCNN that includes poses optimized by the first CNN in its training set performs\neven better at optimizing randomly initialized poses than either the first CNN\nscoring function or AutoDock Vina. \n\n"}
{"id": "1710.07742", "contents": "Title: Towards Black-box Iterative Machine Teaching Abstract: In this paper, we make an important step towards the black-box machine\nteaching by considering the cross-space machine teaching, where the teacher and\nthe learner use different feature representations and the teacher can not fully\nobserve the learner's model. In such scenario, we study how the teacher is\nstill able to teach the learner to achieve faster convergence rate than the\ntraditional passive learning. We propose an active teacher model that can\nactively query the learner (i.e., make the learner take exams) for estimating\nthe learner's status and provably guide the learner to achieve faster\nconvergence. The sample complexities for both teaching and query are provided.\nIn the experiments, we compare the proposed active teacher with the omniscient\nteacher and verify the effectiveness of the active teacher model. \n\n"}
{"id": "1710.07797", "contents": "Title: Optimal Rates for Learning with Nystr\\\"om Stochastic Gradient Methods Abstract: In the setting of nonparametric regression, we propose and study a\ncombination of stochastic gradient methods with Nystr\\\"om subsampling, allowing\nmultiple passes over the data and mini-batches. Generalization error bounds for\nthe studied algorithm are provided. Particularly, optimal learning rates are\nderived considering different possible choices of the step-size, the mini-batch\nsize, the number of iterations/passes, and the subsampling level. In comparison\nwith state-of-the-art algorithms such as the classic stochastic gradient\nmethods and kernel ridge regression with Nystr\\\"om, the studied algorithm has\nadvantages on the computational complexity, while achieving the same optimal\nlearning rates. Moreover, our results indicate that using mini-batches can\nreduce the total computational cost while achieving the same optimal\nstatistical results. \n\n"}
{"id": "1710.08446", "contents": "Title: Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At\n  Every Step Abstract: Generative adversarial networks (GANs) are a family of generative models that\ndo not minimize a single training criterion. Unlike other generative models,\nthe data distribution is learned via a game between a generator (the generative\nmodel) and a discriminator (a teacher providing training signal) that each\nminimize their own cost. GANs are designed to reach a Nash equilibrium at which\neach player cannot reduce their cost without changing the other players'\nparameters. One useful approach for the theory of GANs is to show that a\ndivergence between the training distribution and the model distribution obtains\nits minimum value at equilibrium. Several recent research directions have been\nmotivated by the idea that this divergence is the primary guide for the\nlearning process and that every step of learning should decrease the\ndivergence. We show that this view is overly restrictive. During GAN training,\nthe discriminator provides learning signal in situations where the gradients of\nthe divergences between distributions would not be useful. We provide empirical\ncounterexamples to the view of GAN training as divergence minimization.\nSpecifically, we demonstrate that GANs are able to learn distributions in\nsituations where the divergence minimization point of view predicts they would\nfail. We also show that gradient penalties motivated from the divergence\nminimization perspective are equally helpful when applied in other contexts in\nwhich the divergence minimization perspective does not predict they would be\nhelpful. This contributes to a growing body of evidence that GAN training may\nbe more usefully viewed as approaching Nash equilibria via trajectories that do\nnot necessarily minimize a specific divergence at each step. \n\n"}
{"id": "1710.09182", "contents": "Title: Early identification of important patents through network centrality Abstract: One of the most challenging problems in technological forecasting is to\nidentify as early as possible those technologies that have the potential to\nlead to radical changes in our society. In this paper, we use the US patent\ncitation network (1926-2010) to test our ability to early identify a list of\nhistorically significant patents through citation network analysis. We show\nthat in order to effectively uncover these patents shortly after they are\nissued, we need to go beyond raw citation counts and take into account both the\ncitation network topology and temporal information. In particular, an\nage-normalized measure of patent centrality, called rescaled PageRank, allows\nus to identify the significant patents earlier than citation count and PageRank\nscore. In addition, we find that while high-impact patents tend to rely on\nother high-impact patents in a similar way as scientific papers, the patents'\ncitation dynamics is significantly slower than that of papers, which makes the\nearly identification of significant patents more challenging than that of\nsignificant papers. \n\n"}
{"id": "1710.10177", "contents": "Title: Combining Aspects of Genetic Algorithms with Weighted Recommender\n  Hybridization Abstract: Recommender systems are established means to inspire users to watch\ninteresting movies, discover baby names, or read books. The recommendation\nquality further improves by combining the results of multiple recommendation\nalgorithms using hybridization methods. In this paper, we focus on the task of\ncombining unscored recommendations into a single ensemble. Our proposed method\nis inspired by genetic algorithms. It repeatedly selects items from the\nrecommendations to create a population of items that will be used for the final\nensemble. We compare our method with a weighted voting method and test the\nperformance of both in a movie- and name-recommendation scenario. We were able\nto outperform the weighted method on both datasets by 20.3 % and 31.1 % and\ndecreased the overall execution time by up to 19.9 %. Our results do not only\npropose a new kind of hybridization method, but introduce the field of\nrecommender hybridization to further work with genetic algorithms. \n\n"}
{"id": "1710.10201", "contents": "Title: New Methods for Metadata Extraction from Scientific Literature Abstract: Within the past few decades we have witnessed digital revolution, which moved\nscholarly communication to electronic media and also resulted in a substantial\nincrease in its volume. Nowadays keeping track with the latest scientific\nachievements poses a major challenge for the researchers. Scientific\ninformation overload is a severe problem that slows down scholarly\ncommunication and knowledge propagation across the academia. Modern research\ninfrastructures facilitate studying scientific literature by providing\nintelligent search tools, proposing similar and related documents, visualizing\ncitation and author networks, assessing the quality and impact of the articles,\nand so on. In order to provide such high quality services the system requires\nthe access not only to the text content of stored documents, but also to their\nmachine-readable metadata. Since in practice good quality metadata is not\nalways available, there is a strong demand for a reliable automatic method of\nextracting machine-readable metadata directly from source documents. This\nresearch addresses these problems by proposing an automatic, accurate and\nflexible algorithm for extracting wide range of metadata directly from\nscientific articles in born-digital form. Extracted information includes basic\ndocument metadata, structured full text and bibliography section. Designed as a\nuniversal solution, proposed algorithm is able to handle a vast variety of\npublication layouts with high precision and thus is well-suited for analyzing\nheterogeneous document collections. This was achieved by employing supervised\nand unsupervised machine-learning algorithms trained on large, diverse\ndatasets. The evaluation we conducted showed good performance of proposed\nmetadata extraction algorithm. The comparison with other similar solutions also\nproved our algorithm performs better than competition for most metadata types. \n\n"}
{"id": "1710.10345", "contents": "Title: The Implicit Bias of Gradient Descent on Separable Data Abstract: We examine gradient descent on unregularized logistic regression problems,\nwith homogeneous linear predictors on linearly separable datasets. We show the\npredictor converges to the direction of the max-margin (hard margin SVM)\nsolution. The result also generalizes to other monotone decreasing loss\nfunctions with an infimum at infinity, to multi-class problems, and to training\na weight layer in a deep network in a certain restricted setting. Furthermore,\nwe show this convergence is very slow, and only logarithmic in the convergence\nof the loss itself. This can help explain the benefit of continuing to optimize\nthe logistic or cross-entropy loss even after the training error is zero and\nthe training loss is extremely small, and, as we show, even if the validation\nloss increases. Our methodology can also aid in understanding implicit\nregularization n more complex models and with other optimization methods. \n\n"}
{"id": "1710.10513", "contents": "Title: Crime incidents embedding using restricted Boltzmann machines Abstract: We present a new approach for detecting related crime series, by unsupervised\nlearning of the latent feature embeddings from narratives of crime record via\nthe Gaussian-Bernoulli Restricted Boltzmann Machines (RBM). This is a\ndrastically different approach from prior work on crime analysis, which\ntypically considers only time and location and at most category information.\nAfter the embedding, related cases are closer to each other in the Euclidean\nfeature space, and the unrelated cases are far apart, which is a good property\ncan enable subsequent analysis such as detection and clustering of related\ncases. Experiments over several series of related crime incidents hand labeled\nby the Atlanta Police Department reveal the promise of our embedding methods. \n\n"}
{"id": "1710.10881", "contents": "Title: Fast Linear Model for Knowledge Graph Embeddings Abstract: This paper shows that a simple baseline based on a Bag-of-Words (BoW)\nrepresentation learns surprisingly good knowledge graph embeddings. By casting\nknowledge base completion and question answering as supervised classification\nproblems, we observe that modeling co-occurences of entities and relations\nleads to state-of-the-art performance with a training time of a few minutes\nusing the open sourced library fastText. \n\n"}
{"id": "1710.11239", "contents": "Title: Time-lagged autoencoders: Deep learning of slow collective variables for\n  molecular kinetics Abstract: Inspired by the success of deep learning techniques in the physical and\nchemical sciences, we apply a modification of an autoencoder type deep neural\nnetwork to the task of dimension reduction of molecular dynamics data. We can\nshow that our time-lagged autoencoder reliably finds low-dimensional embeddings\nfor high-dimensional feature spaces which capture the slow dynamics of the\nunderlying stochastic processes - beyond the capabilities of linear dimension\nreduction techniques. \n\n"}
{"id": "1711.00142", "contents": "Title: Sampling and Reconstruction of Graph Signals via Weak Submodularity and\n  Semidefinite Relaxation Abstract: We study the problem of sampling a bandlimited graph signal in the presence\nof noise, where the objective is to select a node subset of prescribed\ncardinality that minimizes the signal reconstruction mean squared error (MSE).\nTo that end, we formulate the task at hand as the minimization of MSE subject\nto binary constraints, and approximate the resulting NP-hard problem via\nsemidefinite programming (SDP) relaxation. Moreover, we provide an alternative\nformulation based on maximizing a monotone weak submodular function and propose\na randomized-greedy algorithm to find a sub-optimal subset. We then derive a\nworst-case performance guarantee on the MSE returned by the randomized greedy\nalgorithm for general non-stationary graph signals. The efficacy of the\nproposed methods is illustrated through numerical simulations on synthetic and\nreal-world graphs. Notably, the randomized greedy algorithm yields an\norder-of-magnitude speedup over state-of-the-art greedy sampling schemes, while\nincurring only a marginal MSE performance loss. \n\n"}
{"id": "1711.00165", "contents": "Title: Deep Neural Networks as Gaussian Processes Abstract: It has long been known that a single-layer fully-connected neural network\nwith an i.i.d. prior over its parameters is equivalent to a Gaussian process\n(GP), in the limit of infinite network width. This correspondence enables exact\nBayesian inference for infinite width neural networks on regression tasks by\nmeans of evaluating the corresponding GP. Recently, kernel functions which\nmimic multi-layer random neural networks have been developed, but only outside\nof a Bayesian framework. As such, previous work has not identified that these\nkernels can be used as covariance functions for GPs and allow fully Bayesian\nprediction with a deep neural network.\n  In this work, we derive the exact equivalence between infinitely wide deep\nnetworks and GPs. We further develop a computationally efficient pipeline to\ncompute the covariance function for these GPs. We then use the resulting GPs to\nperform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10.\nWe observe that trained neural network accuracy approaches that of the\ncorresponding GP with increasing layer width, and that the GP uncertainty is\nstrongly correlated with trained network prediction error. We further find that\ntest performance increases as finite-width trained networks are made wider and\nmore similar to a GP, and thus that GP predictions typically outperform those\nof finite-width networks. Finally we connect the performance of these GPs to\nthe recent theory of signal propagation in random neural networks. \n\n"}
{"id": "1711.00658", "contents": "Title: Candidates vs. Noises Estimation for Large Multi-Class Classification\n  Problem Abstract: This paper proposes a method for multi-class classification problems, where\nthe number of classes K is large. The method, referred to as Candidates vs.\nNoises Estimation (CANE), selects a small subset of candidate classes and\nsamples the remaining classes. We show that CANE is always consistent and\ncomputationally efficient. Moreover, the resulting estimator has low\nstatistical variance approaching that of the maximum likelihood estimator, when\nthe observed label belongs to the selected candidates with high probability. In\npractice, we use a tree structure with leaves as classes to promote fast beam\nsearch for candidate selection. We further apply the CANE method to estimate\nword probabilities in learning large neural language models. Extensive\nexperimental results show that CANE achieves better prediction accuracy over\nthe Noise-Contrastive Estimation (NCE), its variants and a number of the\nstate-of-the-art tree classifiers, while it gains significant speedup compared\nto standard O(K) methods. \n\n"}
{"id": "1711.00804", "contents": "Title: Framework for evaluation of sound event detection in web videos Abstract: The largest source of sound events is web videos. Most videos lack sound\nevent labels at segment level, however, a significant number of them do respond\nto text queries, from a match found using metadata by search engines. In this\npaper we explore the extent to which a search query can be used as the true\nlabel for detection of sound events in videos. We present a framework for\nlarge-scale sound event recognition on web videos. The framework crawls videos\nusing search queries corresponding to 78 sound event labels drawn from three\ndatasets. The datasets are used to train three classifiers, and we obtain a\nprediction on 3.7 million web video segments. We evaluated performance using\nthe search query as true label and compare it with human labeling. Both types\nof ground truth exhibited close performance, to within 10%, and similar\nperformance trend with increasing number of evaluated segments. Hence, our\nexperiments show potential for using search query as a preliminary true label\nfor sound event recognition in web videos. \n\n"}
{"id": "1711.00843", "contents": "Title: Generalized Probabilistic Bisection for Stochastic Root-Finding Abstract: We consider numerical schemes for root finding of noisy responses through\ngeneralizing the Probabilistic Bisection Algorithm (PBA) to the more practical\ncontext where the sampling distribution is unknown and location-dependent. As\nin standard PBA, we rely on a knowledge state for the approximate posterior of\nthe root location. To implement the corresponding Bayesian updating, we also\ncarry out inference of oracle accuracy, namely learning the probability of\ncorrect response. To this end we utilize batched querying in combination with a\nvariety of frequentist and Bayesian estimators based on majority vote, as well\nas the underlying functional responses, if available. For guiding sampling\nselection we investigate both Information Directed sampling, as well as\nQuantile sampling. Our numerical experiments show that these strategies perform\nquite differently; in particular we demonstrate the efficiency of randomized\nquantile sampling which is reminiscent of Thompson sampling. Our work is\nmotivated by the root-finding sub-routine in pricing of Bermudan financial\nderivatives, illustrated in the last section of the paper. \n\n"}
{"id": "1711.01297", "contents": "Title: Implicit Weight Uncertainty in Neural Networks Abstract: Modern neural networks tend to be overconfident on unseen, noisy or\nincorrectly labelled data and do not produce meaningful uncertainty measures.\nBayesian deep learning aims to address this shortcoming with variational\napproximations (such as Bayes by Backprop or Multiplicative Normalising Flows).\nHowever, current approaches have limitations regarding flexibility and\nscalability. We introduce Bayes by Hypernet (BbH), a new method of variational\napproximation that interprets hypernetworks as implicit distributions. It\nnaturally uses neural networks to model arbitrarily complex distributions and\nscales to modern deep learning architectures. In our experiments, we\ndemonstrate that our method achieves competitive accuracies and predictive\nuncertainties on MNIST and a CIFAR5 task, while being the most robust against\nadversarial attacks. \n\n"}
{"id": "1711.01870", "contents": "Title: Interpretable Feature Recommendation for Signal Analytics Abstract: This paper presents an automated approach for interpretable feature\nrecommendation for solving signal data analytics problems. The method has been\ntested by performing experiments on datasets in the domain of prognostics where\ninterpretation of features is considered very important. The proposed approach\nis based on Wide Learning architecture and provides means for interpretation of\nthe recommended features. It is to be noted that such an interpretation is not\navailable with feature learning approaches like Deep Learning (such as\nConvolutional Neural Network) or feature transformation approaches like\nPrincipal Component Analysis. Results show that the feature recommendation and\ninterpretation techniques are quite effective for the problems at hand in terms\nof performance and drastic reduction in time to develop a solution. It is\nfurther shown by an example, how this human-in-loop interpretation system can\nbe used as a prescriptive system. \n\n"}
{"id": "1711.02295", "contents": "Title: Quality-Efficiency Trade-offs in Machine Learning for Text Processing Abstract: Data mining, machine learning, and natural language processing are powerful\ntechniques that can be used together to extract information from large texts.\nDepending on the task or problem at hand, there are many different approaches\nthat can be used. The methods available are continuously being optimized, but\nnot all these methods have been tested and compared in a set of problems that\ncan be solved using supervised machine learning algorithms. The question is\nwhat happens to the quality of the methods if we increase the training data\nsize from, say, 100 MB to over 1 GB? Moreover, are quality gains worth it when\nthe rate of data processing diminishes? Can we trade quality for time\nefficiency and recover the quality loss by just being able to process more\ndata? We attempt to answer these questions in a general way for text processing\ntasks, considering the trade-offs involving training data size, learning time,\nand quality obtained. We propose a performance trade-off framework and apply it\nto three important text processing problems: Named Entity Recognition,\nSentiment Analysis and Document Classification. These problems were also chosen\nbecause they have different levels of object granularity: words, paragraphs,\nand documents. For each problem, we selected several supervised machine\nlearning algorithms and we evaluated the trade-offs of them on large publicly\navailable data sets (news, reviews, patents). To explore these trade-offs, we\nuse different data subsets of increasing size ranging from 50 MB to several GB.\nWe also consider the impact of the data set and the evaluation technique. We\nfind that the results do not change significantly and that most of the time the\nbest algorithms is the fastest. However, we also show that the results for\nsmall data (say less than 100 MB) are different from the results for big data\nand in those cases the best algorithm is much harder to determine. \n\n"}
{"id": "1711.02317", "contents": "Title: Multi-Player Bandits Revisited Abstract: Multi-player Multi-Armed Bandits (MAB) have been extensively studied in the\nliterature, motivated by applications to Cognitive Radio systems. Driven by\nsuch applications as well, we motivate the introduction of several levels of\nfeedback for multi-player MAB algorithms. Most existing work assume that\nsensing information is available to the algorithm. Under this assumption, we\nimprove the state-of-the-art lower bound for the regret of any decentralized\nalgorithms and introduce two algorithms, RandTopM and MCTopM, that are shown to\nempirically outperform existing algorithms. Moreover, we provide strong\ntheoretical guarantees for these algorithms, including a notion of asymptotic\noptimality in terms of the number of selections of bad arms. We then introduce\na promising heuristic, called Selfish, that can operate without sensing\ninformation, which is crucial for emerging applications to Internet of Things\nnetworks. We investigate the empirical performance of this algorithm and\nprovide some first theoretical elements for the understanding of its behavior. \n\n"}
{"id": "1711.02795", "contents": "Title: Approximate message passing for nonconvex sparse regularization with\n  stability and asymptotic analysis Abstract: We analyse a linear regression problem with nonconvex regularization called\nsmoothly clipped absolute deviation (SCAD) under an overcomplete Gaussian basis\nfor Gaussian random data. We propose an approximate message passing (AMP)\nalgorithm considering nonconvex regularization, namely SCAD-AMP, and\nanalytically show that the stability condition corresponds to the de\nAlmeida--Thouless condition in spin glass literature. Through asymptotic\nanalysis, we show the correspondence between the density evolution of SCAD-AMP\nand the replica symmetric solution. Numerical experiments confirm that for a\nsufficiently large system size, SCAD-AMP achieves the optimal performance\npredicted by the replica method. Through replica analysis, a phase transition\nbetween replica symmetric (RS) and replica symmetry breaking (RSB) region is\nfound in the parameter space of SCAD. The appearance of the RS region for a\nnonconvex penalty is a significant advantage that indicates the region of\nsmooth landscape of the optimization problem. Furthermore, we analytically show\nthat the statistical representation performance of the SCAD penalty is better\nthan that of L1-based methods, and the minimum representation error under RS\nassumption is obtained at the edge of the RS/RSB phase. The correspondence\nbetween the convergence of the existing coordinate descent algorithm and RS/RSB\ntransition is also indicated. \n\n"}
{"id": "1711.03638", "contents": "Title: Provably Accurate Double-Sparse Coding Abstract: Sparse coding is a crucial subroutine in algorithms for various signal\nprocessing, deep learning, and other machine learning applications. The central\ngoal is to learn an overcomplete dictionary that can sparsely represent a given\ninput dataset. However, a key challenge is that storage, transmission, and\nprocessing of the learned dictionary can be untenably high if the data\ndimension is high. In this paper, we consider the double-sparsity model\nintroduced by Rubinstein et al. (2010b) where the dictionary itself is the\nproduct of a fixed, known basis and a data-adaptive sparse component. First, we\nintroduce a simple algorithm for double-sparse coding that can be amenable to\nefficient implementation via neural architectures. Second, we theoretically\nanalyze its performance and demonstrate asymptotic sample complexity and\nrunning time benefits over existing (provable) approaches for sparse coding. To\nour knowledge, our work introduces the first computationally efficient\nalgorithm for double-sparse coding that enjoys rigorous statistical guarantees.\nFinally, we support our analysis via several numerical experiments on simulated\ndata, confirming that our method can indeed be useful in problem sizes\nencountered in practical applications. \n\n"}
{"id": "1711.04845", "contents": "Title: Invariances and Data Augmentation for Supervised Music Transcription Abstract: This paper explores a variety of models for frame-based music transcription,\nwith an emphasis on the methods needed to reach state-of-the-art on human\nrecordings. The translation-invariant network discussed in this paper, which\ncombines a traditional filterbank with a convolutional neural network, was the\ntop-performing model in the 2017 MIREX Multiple Fundamental Frequency\nEstimation evaluation. This class of models shares parameters in the\nlog-frequency domain, which exploits the frequency invariance of music to\nreduce the number of model parameters and avoid overfitting to the training\ndata. All models in this paper were trained with supervision by labeled data\nfrom the MusicNet dataset, augmented by random label-preserving pitch-shift\ntransformations. \n\n"}
{"id": "1711.05323", "contents": "Title: On Optimal Generalizability in Parametric Learning Abstract: We consider the parametric learning problem, where the objective of the\nlearner is determined by a parametric loss function. Employing empirical risk\nminimization with possibly regularization, the inferred parameter vector will\nbe biased toward the training samples. Such bias is measured by the cross\nvalidation procedure in practice where the data set is partitioned into a\ntraining set used for training and a validation set, which is not used in\ntraining and is left to measure the out-of-sample performance. A classical\ncross validation strategy is the leave-one-out cross validation (LOOCV) where\none sample is left out for validation and training is done on the rest of the\nsamples that are presented to the learner, and this process is repeated on all\nof the samples. LOOCV is rarely used in practice due to the high computational\ncomplexity. In this paper, we first develop a computationally efficient\napproximate LOOCV (ALOOCV) and provide theoretical guarantees for its\nperformance. Then we use ALOOCV to provide an optimization algorithm for\nfinding the regularizer in the empirical risk minimization framework. In our\nnumerical experiments, we illustrate the accuracy and efficiency of ALOOCV as\nwell as our proposed framework for the optimization of the regularizer. \n\n"}
{"id": "1711.05374", "contents": "Title: Optimizing Kernel Machines using Deep Learning Abstract: Building highly non-linear and non-parametric models is central to several\nstate-of-the-art machine learning systems. Kernel methods form an important\nclass of techniques that induce a reproducing kernel Hilbert space (RKHS) for\ninferring non-linear models through the construction of similarity functions\nfrom data. These methods are particularly preferred in cases where the training\ndata sizes are limited and when prior knowledge of the data similarities is\navailable. Despite their usefulness, they are limited by the computational\ncomplexity and their inability to support end-to-end learning with a\ntask-specific objective. On the other hand, deep neural networks have become\nthe de facto solution for end-to-end inference in several learning paradigms.\nIn this article, we explore the idea of using deep architectures to perform\nkernel machine optimization, for both computational efficiency and end-to-end\ninferencing. To this end, we develop the DKMO (Deep Kernel Machine\nOptimization) framework, that creates an ensemble of dense embeddings using\nNystrom kernel approximations and utilizes deep learning to generate\ntask-specific representations through the fusion of the embeddings.\nIntuitively, the filters of the network are trained to fuse information from an\nensemble of linear subspaces in the RKHS. Furthermore, we introduce the kernel\ndropout regularization to enable improved training convergence. Finally, we\nextend this framework to the multiple kernel case, by coupling a global fusion\nlayer with pre-trained deep kernel machines for each of the constituent\nkernels. Using case studies with limited training data, and lack of explicit\nfeature sources, we demonstrate the effectiveness of our framework over\nconventional model inferencing techniques. \n\n"}
{"id": "1711.05411", "contents": "Title: Z-Forcing: Training Stochastic Recurrent Networks Abstract: Many efforts have been devoted to training generative latent variable models\nwith autoregressive decoders, such as recurrent neural networks (RNN).\nStochastic recurrent models have been successful in capturing the variability\nobserved in natural sequential data such as speech. We unify successful ideas\nfrom recently proposed architectures into a stochastic recurrent model: each\nstep in the sequence is associated with a latent variable that is used to\ncondition the recurrent dynamics for future steps. Training is performed with\namortized variational inference where the approximate posterior is augmented\nwith a RNN that runs backward through the sequence. In addition to maximizing\nthe variational lower bound, we ease training of the latent variables by adding\nan auxiliary cost which forces them to reconstruct the state of the backward\nrecurrent network. This provides the latent variables with a task-independent\nobjective that enhances the performance of the overall model. We found this\nstrategy to perform better than alternative approaches such as KL annealing.\nAlthough being conceptually simple, our model achieves state-of-the-art results\non standard speech benchmarks such as TIMIT and Blizzard and competitive\nperformance on sequential MNIST. Finally, we apply our model to language\nmodeling on the IMDB dataset where the auxiliary cost helps in learning\ninterpretable latent variables. Source Code:\n\\url{https://github.com/anirudh9119/zforcing_nips17} \n\n"}
{"id": "1711.06100", "contents": "Title: Sequences, Items And Latent Links: Recommendation With Consumed Item\n  Packs Abstract: Recommenders personalize the web content by typically using collaborative\nfiltering to relate users (or items) based on explicit feedback, e.g., ratings.\nThe difficulty of collecting this feedback has recently motivated to consider\nimplicit feedback (e.g., item consumption along with the corresponding time).\n  In this paper, we introduce the notion of consumed item pack (CIP) which\nenables to link users (or items) based on their implicit analogous consumption\nbehavior. Our proposal is generic, and we show that it captures three novel\nimplicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word\nembedding-based (DEEPCIP), as well as a state-of-the-art technique using\nimplicit feedback (FISM). We show that our recommenders handle incremental\nupdates incorporating freshly consumed items. We demonstrate that all three\nrecommenders provide a recommendation quality that is competitive with\nstate-of-the-art ones, including one incorporating both explicit and implicit\nfeedback. \n\n"}
{"id": "1711.06424", "contents": "Title: A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit Abstract: Determining the appropriate batch size for mini-batch gradient descent is\nalways time consuming as it often relies on grid search. This paper considers a\nresizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed\nbandit for achieving best performance in grid search by selecting an\nappropriate batch size at each epoch with a probability defined as a function\nof its previous success/failure. This probability encourages exploration of\ndifferent batch size and then later exploitation of batch size with history of\nsuccess. At each epoch, the RMGD samples a batch size from its probability\ndistribution, then uses the selected batch size for mini-batch gradient\ndescent. After obtaining the validation loss at each epoch, the probability\ndistribution is updated to incorporate the effectiveness of the sampled batch\nsize. The RMGD essentially assists the learning process to explore the possible\ndomain of the batch size and exploit successful batch size. Experimental\nresults show that the RMGD achieves performance better than the best performing\nsingle batch size. Furthermore, it, obviously, attains this performance in a\nshorter amount of time than grid search. It is surprising that the RMGD\nachieves better performance than grid search. \n\n"}
{"id": "1711.06664", "contents": "Title: Predict Responsibly: Improving Fairness and Accuracy by Learning to\n  Defer Abstract: In many machine learning applications, there are multiple decision-makers\ninvolved, both automated and human. The interaction between these agents often\ngoes unaddressed in algorithmic development. In this work, we explore a simple\nversion of this interaction with a two-stage framework containing an automated\nmodel and an external decision-maker. The model can choose to say \"Pass\", and\npass the decision downstream, as explored in rejection learning. We extend this\nconcept by proposing \"learning to defer\", which generalizes rejection learning\nby considering the effect of other agents in the decision-making process. We\npropose a learning algorithm which accounts for potential biases held by\nexternal decision-makers in a system. Experiments demonstrate that learning to\ndefer can make systems not only more accurate but also less biased. Even when\nworking with inconsistent or biased users, we show that deferring models still\ngreatly improve the accuracy and/or fairness of the entire system. \n\n"}
{"id": "1711.06788", "contents": "Title: MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural\n  Networks Abstract: We introduce MinimalRNN, a new recurrent neural network architecture that\nachieves comparable performance as the popular gated RNNs with a simplified\nstructure. It employs minimal updates within RNN, which not only leads to\nefficient learning and testing but more importantly better interpretability and\ntrainability. We demonstrate that by endorsing the more restrictive update\nrule, MinimalRNN learns disentangled RNN states. We further examine the\nlearning dynamics of different RNN structures using input-output Jacobians, and\nshow that MinimalRNN is able to capture longer range dependencies than existing\nRNN architectures. \n\n"}
{"id": "1711.06968", "contents": "Title: Intelligent Word Embeddings of Free-Text Radiology Reports Abstract: Radiology reports are a rich resource for advancing deep learning\napplications in medicine by leveraging the large volume of data continuously\nbeing updated, integrated, and shared. However, there are significant\nchallenges as well, largely due to the ambiguity and subtlety of natural\nlanguage. We propose a hybrid strategy that combines semantic-dictionary\nmapping and word2vec modeling for creating dense vector embeddings of free-text\nradiology reports. Our method leverages the benefits of both\nsemantic-dictionary mapping as well as unsupervised learning. Using the vector\nrepresentation, we automatically classify the radiology reports into three\nclasses denoting confidence in the diagnosis of intracranial hemorrhage by the\ninterpreting radiologist. We performed experiments with varying hyperparameter\nsettings of the word embeddings and a range of different classifiers. Best\nperformance achieved was a weighted precision of 88% and weighted recall of\n90%. Our work offers the potential to leverage unstructured electronic health\nrecord data by allowing direct analysis of narrative clinical notes. \n\n"}
{"id": "1711.07076", "contents": "Title: Does mitigating ML's impact disparity require treatment disparity? Abstract: Following related work in law and policy, two notions of disparity have come\nto shape the study of fairness in algorithmic decision-making. Algorithms\nexhibit treatment disparity if they formally treat members of protected\nsubgroups differently; algorithms exhibit impact disparity when outcomes differ\nacross subgroups, even if the correlation arises unintentionally. Naturally, we\ncan achieve impact parity through purposeful treatment disparity. In one thread\nof technical work, papers aim to reconcile the two forms of parity proposing\ndisparate learning processes (DLPs). Here, the learning algorithm can see group\nmembership during training but produce a classifier that is group-blind at test\ntime. In this paper, we show theoretically that: (i) When other features\ncorrelate to group membership, DLPs will (indirectly) implement treatment\ndisparity, undermining the policy desiderata they are designed to address; (ii)\nWhen group membership is partly revealed by other features, DLPs induce\nwithin-class discrimination; and (iii) In general, DLPs provide a suboptimal\ntrade-off between accuracy and impact parity. Based on our technical analysis,\nwe argue that transparent treatment disparity is preferable to occluded methods\nfor achieving impact parity. Experimental results on several real-world\ndatasets highlight the practical consequences of applying DLPs vs. per-group\nthresholds. \n\n"}
{"id": "1711.07099", "contents": "Title: Compression-Based Regularization with an Application to Multi-Task\n  Learning Abstract: This paper investigates, from information theoretic grounds, a learning\nproblem based on the principle that any regularity in a given dataset can be\nexploited to extract compact features from data, i.e., using fewer bits than\nneeded to fully describe the data itself, in order to build meaningful\nrepresentations of a relevant content (multiple labels). We begin by\nintroducing the noisy lossy source coding paradigm with the log-loss fidelity\ncriterion which provides the fundamental tradeoffs between the\n\\emph{cross-entropy loss} (average risk) and the information rate of the\nfeatures (model complexity). Our approach allows an information theoretic\nformulation of the \\emph{multi-task learning} (MTL) problem which is a\nsupervised learning framework in which the prediction models for several\nrelated tasks are learned jointly from common representations to achieve better\ngeneralization performance. Then, we present an iterative algorithm for\ncomputing the optimal tradeoffs and its global convergence is proven provided\nthat some conditions hold. An important property of this algorithm is that it\nprovides a natural safeguard against overfitting, because it minimizes the\naverage risk taking into account a penalization induced by the model\ncomplexity. Remarkably, empirical results illustrate that there exists an\noptimal information rate minimizing the \\emph{excess risk} which depends on the\nnature and the amount of available training data. An application to\nhierarchical text categorization is also investigated, extending previous\nworks. \n\n"}
{"id": "1711.07168", "contents": "Title: Stein Variational Message Passing for Continuous Graphical Models Abstract: We propose a novel distributed inference algorithm for continuous graphical\nmodels, by extending Stein variational gradient descent (SVGD) to leverage the\nMarkov dependency structure of the distribution of interest. Our approach\ncombines SVGD with a set of structured local kernel functions defined on the\nMarkov blanket of each node, which alleviates the curse of high dimensionality\nand simultaneously yields a distributed algorithm for decentralized inference\ntasks. We justify our method with theoretical analysis and show that the use of\nlocal kernels can be viewed as a new type of localized approximation that\nmatches the target distribution on the conditional distributions of each node\nover its Markov blanket. Our empirical results show that our method outperforms\na variety of baselines including standard MCMC and particle message passing\nmethods. \n\n"}
{"id": "1711.07575", "contents": "Title: Finding Differentially Covarying Needles in a Temporally Evolving\n  Haystack: A Scan Statistics Perspective Abstract: Recent results in coupled or temporal graphical models offer schemes for\nestimating the relationship structure between features when the data come from\nrelated (but distinct) longitudinal sources. A novel application of these ideas\nis for analyzing group-level differences, i.e., in identifying if trends of\nestimated objects (e.g., covariance or precision matrices) are different across\ndisparate conditions (e.g., gender or disease). Often, poor effect sizes make\ndetecting the differential signal over the full set of features difficult: for\nexample, dependencies between only a subset of features may manifest\ndifferently across groups. In this work, we first give a parametric model for\nestimating trends in the space of SPD matrices as a function of one or more\ncovariates. We then generalize scan statistics to graph structures, to search\nover distinct subsets of features (graph partitions) whose temporal dependency\nstructure may show statistically significant group-wise differences. We\ntheoretically analyze the Family Wise Error Rate (FWER) and bounds on Type 1\nand Type 2 error. On a cohort of individuals with risk factors for Alzheimer's\ndisease (but otherwise cognitively healthy), we find scientifically interesting\ngroup differences where the default analysis, i.e., models estimated on the\nfull graph, do not survive reasonable significance thresholds. \n\n"}
{"id": "1711.07693", "contents": "Title: Regret Analysis for Continuous Dueling Bandit Abstract: The dueling bandit is a learning framework wherein the feedback information\nin the learning process is restricted to a noisy comparison between a pair of\nactions. In this research, we address a dueling bandit problem based on a cost\nfunction over a continuous space. We propose a stochastic mirror descent\nalgorithm and show that the algorithm achieves an $O(\\sqrt{T\\log T})$-regret\nbound under strong convexity and smoothness assumptions for the cost function.\nSubsequently, we clarify the equivalence between regret minimization in dueling\nbandit and convex optimization for the cost function. Moreover, when\nconsidering a lower bound in convex optimization, our algorithm is shown to\nachieve the optimal convergence rate in convex optimization and the optimal\nregret in dueling bandit except for a logarithmic factor. \n\n"}
{"id": "1711.09159", "contents": "Title: Quantifying the Effects of Enforcing Disentanglement on Variational\n  Autoencoders Abstract: The notion of disentangled autoencoders was proposed as an extension to the\nvariational autoencoder by introducing a disentanglement parameter $\\beta$,\ncontrolling the learning pressure put on the possible underlying latent\nrepresentations. For certain values of $\\beta$ this kind of autoencoders is\ncapable of encoding independent input generative factors in separate elements\nof the code, leading to a more interpretable and predictable model behaviour.\nIn this paper we quantify the effects of the parameter $\\beta$ on the model\nperformance and disentanglement. After training multiple models with the same\nvalue of $\\beta$, we establish the existence of consistent variance in one of\nthe disentanglement measures, proposed in literature. The negative consequences\nof the disentanglement to the autoencoder's discriminative ability are also\nasserted while varying the amount of examples available during training. \n\n"}
{"id": "1711.09446", "contents": "Title: Balancing Speed and Quality in Online Learning to Rank for Information\n  Retrieval Abstract: In Online Learning to Rank (OLTR) the aim is to find an optimal ranking model\nby interacting with users. When learning from user behavior, systems must\ninteract with users while simultaneously learning from those interactions.\nUnlike other Learning to Rank (LTR) settings, existing research in this field\nhas been limited to linear models. This is due to the speed-quality tradeoff\nthat arises when selecting models: complex models are more expressive and can\nfind the best rankings but need more user interactions to do so, a requirement\nthat risks frustrating users during training. Conversely, simpler models can be\noptimized on fewer interactions and thus provide a better user experience, but\nthey will converge towards suboptimal rankings. This tradeoff creates a\ndeadlock, since novel models will not be able to improve either the user\nexperience or the final convergence point, without sacrificing the other. Our\ncontribution is twofold. First, we introduce a fast OLTR model called Sim-MGD\nthat addresses the speed aspect of the speed-quality tradeoff. Sim-MGD ranks\ndocuments based on similarities with reference documents. It converges rapidly\nand, hence, gives a better user experience but it does not converge towards the\noptimal rankings. Second, we contribute Cascading Multileave Gradient Descent\n(C-MGD) for OLTR that directly addresses the speed-quality tradeoff by using a\ncascade that enables combinations of the best of two worlds: fast learning and\nhigh quality final convergence. C-MGD can provide the better user experience of\nSim-MGD while maintaining the same convergence as the state-of-the-art MGD\nmodel. This opens the door for future work to design new models for OLTR\nwithout having to deal with the speed-quality tradeoff. \n\n"}
{"id": "1711.09535", "contents": "Title: Learning with Biased Complementary Labels Abstract: In this paper, we study the classification problem in which we have access to\neasily obtainable surrogate for true labels, namely complementary labels, which\nspecify classes that observations do \\textbf{not} belong to. Let $Y$ and\n$\\bar{Y}$ be the true and complementary labels, respectively. We first model\nthe annotation of complementary labels via transition probabilities\n$P(\\bar{Y}=i|Y=j), i\\neq j\\in\\{1,\\cdots,c\\}$, where $c$ is the number of\nclasses. Previous methods implicitly assume that $P(\\bar{Y}=i|Y=j), \\forall\ni\\neq j$, are identical, which is not true in practice because humans are\nbiased toward their own experience. For example, as shown in Figure 1, if an\nannotator is more familiar with monkeys than prairie dogs when providing\ncomplementary labels for meerkats, she is more likely to employ \"monkey\" as a\ncomplementary label. We therefore reason that the transition probabilities will\nbe different. In this paper, we propose a framework that contributes three main\ninnovations to learning with \\textbf{biased} complementary labels: (1) It\nestimates transition probabilities with no bias. (2) It provides a general\nmethod to modify traditional loss functions and extends standard deep neural\nnetwork classifiers to learn with biased complementary labels. (3) It\ntheoretically ensures that the classifier learned with complementary labels\nconverges to the optimal one learned with true labels. Comprehensive\nexperiments on several benchmark datasets validate the superiority of our\nmethod to current state-of-the-art methods. \n\n"}
{"id": "1711.09649", "contents": "Title: One-Shot Coresets: The Case of k-Clustering Abstract: Scaling clustering algorithms to massive data sets is a challenging task.\nRecently, several successful approaches based on data summarization methods,\nsuch as coresets and sketches, were proposed. While these techniques provide\nprovably good and small summaries, they are inherently problem dependent - the\npractitioner has to commit to a fixed clustering objective before even\nexploring the data. However, can one construct small data summaries for a wide\nrange of clustering problems simultaneously? In this work, we affirmatively\nanswer this question by proposing an efficient algorithm that constructs such\none-shot summaries for k-clustering problems while retaining strong theoretical\nguarantees. \n\n"}
{"id": "1711.09825", "contents": "Title: Separating Self-Expression and Visual Content in Hashtag Supervision Abstract: The variety, abundance, and structured nature of hashtags make them an\ninteresting data source for training vision models. For instance, hashtags have\nthe potential to significantly reduce the problem of manual supervision and\nannotation when learning vision models for a large number of concepts. However,\na key challenge when learning from hashtags is that they are inherently\nsubjective because they are provided by users as a form of self-expression. As\na consequence, hashtags may have synonyms (different hashtags referring to the\nsame visual content) and may be ambiguous (the same hashtag referring to\ndifferent visual content). These challenges limit the effectiveness of\napproaches that simply treat hashtags as image-label pairs. This paper presents\nan approach that extends upon modeling simple image-label pairs by modeling the\njoint distribution of images, hashtags, and users. We demonstrate the efficacy\nof such approaches in image tagging and retrieval experiments, and show how the\njoint model can be used to perform user-conditional retrieval and tagging. \n\n"}
{"id": "1711.10057", "contents": "Title: Predicting Adolescent Suicide Attempts with Neural Networks Abstract: Though suicide is a major public health problem in the US, machine learning\nmethods are not commonly used to predict an individual's risk of\nattempting/committing suicide. In the present work, starting with an anonymized\ncollection of electronic health records for 522,056 unique, California-resident\nadolescents, we develop neural network models to predict suicide attempts. We\nframe the problem as a binary classification problem in which we use a\npatient's data from 2006-2009 to predict either the presence (1) or absence (0)\nof a suicide attempt in 2010. After addressing issues such as severely\nimbalanced classes and the variable length of a patient's history, we build\nneural networks with depths varying from two to eight hidden layers. For test\nset observations where we have at least five ED/hospital visits' worth of data\non a patient, our depth-4 model achieves a sensitivity of 0.703, specificity of\n0.980, and AUC of 0.958. \n\n"}
{"id": "1711.10127", "contents": "Title: Variational Inference for Gaussian Process Models with Linear Complexity Abstract: Large-scale Gaussian process inference has long faced practical challenges\ndue to time and space complexity that is superlinear in dataset size. While\nsparse variational Gaussian process models are capable of learning from\nlarge-scale data, standard strategies for sparsifying the model can prevent the\napproximation of complex functions. In this work, we propose a novel\nvariational Gaussian process model that decouples the representation of mean\nand covariance functions in reproducing kernel Hilbert space. We show that this\nnew parametrization generalizes previous models. Furthermore, it yields a\nvariational inference problem that can be solved by stochastic gradient ascent\nwith time and space complexity that is only linear in the number of mean\nfunction parameters, regardless of the choice of kernels, likelihoods, and\ninducing points. This strategy makes the adoption of large-scale expressive\nGaussian process models possible. We run several experiments on regression\ntasks and show that this decoupled approach greatly outperforms previous sparse\nvariational Gaussian process inference procedures. \n\n"}
{"id": "1711.10168", "contents": "Title: Semi-supervised learning of hierarchical representations of molecules\n  using neural message passing Abstract: With the rapid increase of compound databases available in medicinal and\nmaterial science, there is a growing need for learning representations of\nmolecules in a semi-supervised manner. In this paper, we propose an\nunsupervised hierarchical feature extraction algorithm for molecules (or more\ngenerally, graph-structured objects with fixed number of types of nodes and\nedges), which is applicable to both unsupervised and semi-supervised tasks. Our\nmethod extends recently proposed Paragraph Vector algorithm and incorporates\nneural message passing to obtain hierarchical representations of subgraphs. We\napplied our method to an unsupervised task and demonstrated that it outperforms\nexisting proposed methods in several benchmark datasets. We also experimentally\nshowed that semi-supervised tasks enhanced predictive performance compared with\nsupervised ones with labeled molecules only. \n\n"}
{"id": "1711.10327", "contents": "Title: Generative Interest Estimation for Document Recommendations Abstract: Learning distributed representations of documents has pushed the\nstate-of-the-art in several natural language processing tasks and was\nsuccessfully applied to the field of recommender systems recently. In this\npaper, we propose a novel content-based recommender system based on learned\nrepresentations and a generative model of user interest. Our method works as\nfollows: First, we learn representations on a corpus of text documents. Then,\nwe capture a user's interest as a generative model in the space of the document\nrepresentations. In particular, we model the distribution of interest for each\nuser as a Gaussian mixture model (GMM). Recommendations can be obtained\ndirectly by sampling from a user's generative model. Using Latent semantic\nanalysis (LSA) as comparison, we compute and explore document representations\non the Delicious bookmarks dataset, a standard benchmark for recommender\nsystems. We then perform density estimation in both spaces and show that\nlearned representations outperform LSA in terms of predictive performance. \n\n"}
{"id": "1711.10353", "contents": "Title: Kernel-based Inference of Functions over Graphs Abstract: The study of networks has witnessed an explosive growth over the past decades\nwith several ground-breaking methods introduced. A particularly interesting --\nand prevalent in several fields of study -- problem is that of inferring a\nfunction defined over the nodes of a network. This work presents a versatile\nkernel-based framework for tackling this inference problem that naturally\nsubsumes and generalizes the reconstruction approaches put forth recently by\nthe signal processing on graphs community. Both the static and the dynamic\nsettings are considered along with effective modeling approaches for addressing\nreal-world problems. The herein analytical discussion is complemented by a set\nof numerical examples, which showcase the effectiveness of the presented\ntechniques, as well as their merits related to state-of-the-art methods. \n\n"}
{"id": "1712.00287", "contents": "Title: Faithful Inversion of Generative Models for Effective Amortized\n  Inference Abstract: Inference amortization methods share information across multiple\nposterior-inference problems, allowing each to be carried out more efficiently.\nGenerally, they require the inversion of the dependency structure in the\ngenerative model, as the modeller must learn a mapping from observations to\ndistributions approximating the posterior. Previous approaches have involved\ninverting the dependency structure in a heuristic way that fails to capture\nthese dependencies correctly, thereby limiting the achievable accuracy of the\nresulting approximations. We introduce an algorithm for faithfully, and\nminimally, inverting the graphical model structure of any generative model.\nSuch inverses have two crucial properties: (a) they do not encode any\nindependence assertions that are absent from the model and; (b) they are local\nmaxima for the number of true independencies encoded. We prove the correctness\nof our approach and empirically show that the resulting minimally faithful\ninverses lead to better inference amortization than existing heuristic\napproaches. \n\n"}
{"id": "1712.00424", "contents": "Title: The reparameterization trick for acquisition functions Abstract: Bayesian optimization is a sample-efficient approach to solving global\noptimization problems. Along with a surrogate model, this approach relies on\ntheoretically motivated value heuristics (acquisition functions) to guide the\nsearch process. Maximizing acquisition functions yields the best performance;\nunfortunately, this ideal is difficult to achieve since optimizing acquisition\nfunctions per se is frequently non-trivial. This statement is especially true\nin the parallel setting, where acquisition functions are routinely non-convex,\nhigh-dimensional, and intractable. Here, we demonstrate how many popular\nacquisition functions can be formulated as Gaussian integrals amenable to the\nreparameterization trick and, ensuingly, gradient-based optimization. Further,\nwe use this reparameterized representation to derive an efficient Monte Carlo\nestimator for the upper confidence bound acquisition function in the context of\nparallel selection. \n\n"}
{"id": "1712.00732", "contents": "Title: SHINE: Signed Heterogeneous Information Network Embedding for Sentiment\n  Link Prediction Abstract: In online social networks people often express attitudes towards others,\nwhich forms massive sentiment links among users. Predicting the sign of\nsentiment links is a fundamental task in many areas such as personal\nadvertising and public opinion analysis. Previous works mainly focus on textual\nsentiment classification, however, text information can only disclose the \"tip\nof the iceberg\" about users' true opinions, of which the most are unobserved\nbut implied by other sources of information such as social relation and users'\nprofile. To address this problem, in this paper we investigate how to predict\npossibly existing sentiment links in the presence of heterogeneous information.\nFirst, due to the lack of explicit sentiment links in mainstream social\nnetworks, we establish a labeled heterogeneous sentiment dataset which consists\nof users' sentiment relation, social relation and profile knowledge by\nentity-level sentiment extraction method. Then we propose a novel and flexible\nend-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework\nto extract users' latent representations from heterogeneous networks and\npredict the sign of unobserved sentiment links. SHINE utilizes multiple deep\nautoencoders to map each user into a low-dimension feature space while\npreserving the network structure. We demonstrate the superiority of SHINE over\nstate-of-the-art baselines on link prediction and node recommendation in two\nreal-world datasets. The experimental results also prove the efficacy of SHINE\nin cold start scenario. \n\n"}
{"id": "1712.01447", "contents": "Title: Gaussian Process bandits with adaptive discretization Abstract: In this paper, the problem of maximizing a black-box function $f:\\mathcal{X}\n\\to \\mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process\n(GP) prior. In particular, a new algorithm for this problem is proposed, and\nhigh probability bounds on its simple and cumulative regret are established.\nThe query point selection rule in most existing methods involves an exhaustive\nsearch over an increasingly fine sequence of uniform discretizations of\n$\\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines\n$\\mathcal{X}$ which leads to a lower computational complexity, particularly\nwhen $\\mathcal{X}$ is a subset of a high dimensional Euclidean space. In\naddition to the computational gains, sufficient conditions are identified under\nwhich the regret bounds of the new algorithm improve upon the known results.\nFinally an extension of the algorithm to the case of contextual bandits is\nproposed, and high probability bounds on the contextual regret are presented. \n\n"}
{"id": "1712.01562", "contents": "Title: EmTaggeR: A Word Embedding Based Novel Method for Hashtag Recommendation\n  on Twitter Abstract: The hashtag recommendation problem addresses recommending (suggesting) one or\nmore hashtags to explicitly tag a post made on a given social network platform,\nbased upon the content and context of the post. In this work, we propose a\nnovel methodology for hashtag recommendation for microblog posts, specifically\nTwitter. The methodology, EmTaggeR, is built upon a training-testing framework\nthat builds on the top of the concept of word embedding. The training phase\ncomprises of learning word vectors associated with each hashtag, and deriving a\nword embedding for each hashtag. We provide two training procedures, one in\nwhich each hashtag is trained with a separate word embedding model applicable\nin the context of that hashtag, and another in which each hashtag obtains its\nembedding from a global context. The testing phase constitutes computing the\naverage word embedding of the test post, and finding the similarity of this\nembedding with the known embeddings of the hashtags. The tweets that contain\nthe most-similar hashtag are extracted, and all the hashtags that appear in\nthese tweets are ranked in terms of embedding similarity scores. The top-K\nhashtags that appear in this ranked list, are recommended for the given test\npost. Our system produces F1 score of 50.83%, improving over the LDA baseline\nby around 6.53 times, outperforming the best-performing system known in the\nliterature that provides a lift of 6.42 times. EmTaggeR is a fast, scalable and\nlightweight system, which makes it practical to deploy in real-life\napplications. \n\n"}
{"id": "1712.02488", "contents": "Title: Cost-sensitive detection with variational autoencoders for environmental\n  acoustic sensing Abstract: Environmental acoustic sensing involves the retrieval and processing of audio\nsignals to better understand our surroundings. While large-scale acoustic data\nmake manual analysis infeasible, they provide a suitable playground for machine\nlearning approaches. Most existing machine learning techniques developed for\nenvironmental acoustic sensing do not provide flexible control of the trade-off\nbetween the false positive rate and the false negative rate. This paper\npresents a cost-sensitive classification paradigm, in which the\nhyper-parameters of classifiers and the structure of variational autoencoders\nare selected in a principled Neyman-Pearson framework. We examine the\nperformance of the proposed approach using a dataset from the HumBug project\nwhich aims to detect the presence of mosquitoes using sound collected by simple\nembedded devices. \n\n"}
{"id": "1712.02629", "contents": "Title: Differentially Private Variational Dropout Abstract: Deep neural networks with their large number of parameters are highly\nflexible learning systems. The high flexibility in such networks brings with\nsome serious problems such as overfitting, and regularization is used to\naddress this problem. A currently popular and effective regularization\ntechnique for controlling the overfitting is dropout. Often, large data\ncollections required for neural networks contain sensitive information such as\nthe medical histories of patients, and the privacy of the training data should\nbe protected. In this paper, we modify the recently proposed variational\ndropout technique which provided an elegant Bayesian interpretation to dropout,\nand show that the intrinsic noise in the variational dropout can be exploited\nto obtain a degree of differential privacy. The iterative nature of training\nneural networks presents a challenge for privacy-preserving estimation since\nmultiple iterations increase the amount of noise added. We overcome this by\nusing a relaxed notion of differential privacy, called concentrated\ndifferential privacy, which provides tighter estimates on the overall privacy\nloss. We demonstrate the accuracy of our privacy-preserving variational dropout\nalgorithm on benchmark datasets. \n\n"}
{"id": "1712.02831", "contents": "Title: RelNN: A Deep Neural Model for Relational Learning Abstract: Statistical relational AI (StarAI) aims at reasoning and learning in noisy\ndomains described in terms of objects and relationships by combining\nprobability with first-order logic. With huge advances in deep learning in the\ncurrent years, combining deep networks with first-order logic has been the\nfocus of several recent studies. Many of the existing attempts, however, only\nfocus on relations and ignore object properties. The attempts that do consider\nobject properties are limited in terms of modelling power or scalability. In\nthis paper, we develop relational neural networks (RelNNs) by adding hidden\nlayers to relational logistic regression (the relational counterpart of\nlogistic regression). We learn latent properties for objects both directly and\nthrough general rules. Back-propagation is used for training these models. A\nmodular, layer-wise architecture facilitates utilizing the techniques developed\nwithin deep learning community to our architecture. Initial experiments on\neight tasks over three real-world datasets show that RelNNs are promising\nmodels for relational learning. \n\n"}
{"id": "1712.03158", "contents": "Title: Graph-based time-space trade-offs for approximate near neighbors Abstract: We take a first step towards a rigorous asymptotic analysis of graph-based\napproaches for finding (approximate) nearest neighbors in high-dimensional\nspaces, by analyzing the complexity of (randomized) greedy walks on the\napproximate near neighbor graph. For random data sets of size $n = 2^{o(d)}$ on\nthe $d$-dimensional Euclidean unit sphere, using near neighbor graphs we can\nprovably solve the approximate nearest neighbor problem with approximation\nfactor $c > 1$ in query time $n^{\\rho_q + o(1)}$ and space $n^{1 + \\rho_s +\no(1)}$, for arbitrary $\\rho_q, \\rho_s \\geq 0$ satisfying \\begin{align} (2c^2 -\n1) \\rho_q + 2 c^2 (c^2 - 1) \\sqrt{\\rho_s (1 - \\rho_s)} \\geq c^4. \\end{align}\nGraph-based near neighbor searching is especially competitive with hash-based\nmethods for small $c$ and near-linear memory, and in this regime the asymptotic\nscaling of a greedy graph-based search matches the recent optimal hash-based\ntrade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA'17]. We further\nstudy how the trade-offs scale when the data set is of size $n =\n2^{\\Theta(d)}$, and analyze asymptotic complexities when applying these results\nto lattice sieving. \n\n"}
{"id": "1712.03249", "contents": "Title: Social Emotion Mining Techniques for Facebook Posts Reaction Prediction Abstract: As of February 2016 Facebook allows users to express their experienced\nemotions about a post by using five so-called `reactions'. This research paper\nproposes and evaluates alternative methods for predicting these reactions to\nuser posts on public pages of firms/companies (like supermarket chains). For\nthis purpose, we collected posts (and their reactions) from Facebook pages of\nlarge supermarket chains and constructed a dataset which is available for other\nresearches. In order to predict the distribution of reactions of a new post,\nneural network architectures (convolutional and recurrent neural networks) were\ntested using pretrained word embeddings. Results of the neural networks were\nimproved by introducing a bootstrapping approach for sentiment and emotion\nmining on the comments for each post. The final model (a combination of neural\nnetwork and a baseline emotion miner) is able to predict the reaction\ndistribution on Facebook posts with a mean squared error (or misclassification\nrate) of 0.135. \n\n"}
{"id": "1712.03941", "contents": "Title: Fast Nearest-Neighbor Classification using RNN in Domains with Large\n  Number of Classes Abstract: In scenarios involving text classification where the number of classes is\nlarge (in multiples of 10000s) and training samples for each class are few and\noften verbose, nearest neighbor methods are effective but very slow in\ncomputing a similarity score with training samples of every class. On the other\nhand, machine learning models are fast at runtime but training them adequately\nis not feasible using few available training samples per class. In this paper,\nwe propose a hybrid approach that cascades 1) a fast but less-accurate\nrecurrent neural network (RNN) model and 2) a slow but more-accurate\nnearest-neighbor model using bag of syntactic features.\n  Using the cascaded approach, our experiments, performed on data set from IT\nsupport services where customer complaint text needs to be classified to return\ntop-$N$ possible error codes, show that the query-time of the slow system is\nreduced to $1/6^{th}$ while its accuracy is being improved. Our approach\noutperforms an LSH-based baseline for query-time reduction. We also derive a\nlower bound on the accuracy of the cascaded model in terms of the accuracies of\nthe individual models. In any two-stage approach, choosing the right number of\ncandidates to pass on to the second stage is crucial. We prove a result that\naids in choosing this cutoff number for the cascaded system. \n\n"}
{"id": "1712.04120", "contents": "Title: GibbsNet: Iterative Adversarial Inference for Deep Graphical Models Abstract: Directed latent variable models that formulate the joint distribution as\n$p(x,z) = p(z) p(x \\mid z)$ have the advantage of fast and exact sampling.\nHowever, these models have the weakness of needing to specify $p(z)$, often\nwith a simple fixed prior that limits the expressiveness of the model.\nUndirected latent variable models discard the requirement that $p(z)$ be\nspecified with a prior, yet sampling from them generally requires an iterative\nprocedure such as blocked Gibbs-sampling that may require many steps to draw\nsamples from the joint distribution $p(x, z)$. We propose a novel approach to\nlearning the joint distribution between the data and a latent code which uses\nan adversarially learned iterative procedure to gradually refine the joint\ndistribution, $p(x, z)$, to better match with the data distribution on each\nstep. GibbsNet is the best of both worlds both in theory and in practice.\nAchieving the speed and simplicity of a directed latent variable model, it is\nguaranteed (assuming the adversarial game reaches the virtual training criteria\nglobal minimum) to produce samples from $p(x, z)$ with only a few sampling\niterations. Achieving the expressiveness and flexibility of an undirected\nlatent variable model, GibbsNet does away with the need for an explicit $p(z)$\nand has the ability to do attribute prediction, class-conditional generation,\nand joint image-attribute modeling in a single model which is not trained for\nany of these specific tasks. We show empirically that GibbsNet is able to learn\na more complex $p(z)$ and show that this leads to improved inpainting and\niterative refinement of $p(x, z)$ for dozens of steps and stable generation\nwithout collapse for thousands of steps, despite being trained on only a few\nsteps. \n\n"}
{"id": "1712.05016", "contents": "Title: Deep Prior Abstract: The recent literature on deep learning offers new tools to learn a rich\nprobability distribution over high dimensional data such as images or sounds.\nIn this work we investigate the possibility of learning the prior distribution\nover neural network parameters using such tools. Our resulting variational\nBayes algorithm generalizes well to new tasks, even when very few training\nexamples are provided. Furthermore, this learned prior allows the model to\nextrapolate correctly far from a given task's training data on a meta-dataset\nof periodic signals. \n\n"}
{"id": "1712.05403", "contents": "Title: Learning to Attend via Word-Aspect Associative Fusion for Aspect-based\n  Sentiment Analysis Abstract: Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a\ngiven document with respect to a given aspect entity. While neural network\narchitectures have been successful in predicting the overall polarity of\nsentences, aspect-specific sentiment analysis still remains as an open problem.\nIn this paper, we propose a novel method for integrating aspect information\ninto the neural model. More specifically, we incorporate aspect information\ninto the neural model by modeling word-aspect relationships. Our novel model,\n\\textit{Aspect Fusion LSTM} (AF-LSTM) learns to attend based on associative\nrelationships between sentence words and aspect which allows our model to\nadaptively focus on the correct words given an aspect term. This ameliorates\nthe flaws of other state-of-the-art models that utilize naive concatenations to\nmodel word-aspect similarity. Instead, our model adopts circular convolution\nand circular correlation to model the similarity between aspect and words and\nelegantly incorporates this within a differentiable neural attention framework.\nFinally, our model is end-to-end differentiable and highly related to\nconvolution-correlation (holographic like) memories. Our proposed neural model\nachieves state-of-the-art performance on benchmark datasets, outperforming\nATAE-LSTM by $4\\%-5\\%$ on average across multiple datasets. \n\n"}
{"id": "1712.06132", "contents": "Title: Dynamic Boltzmann Machines for Second Order Moments and Generalized\n  Gaussian Distributions Abstract: Dynamic Boltzmann Machine (DyBM) has been shown highly efficient to predict\ntime-series data. Gaussian DyBM is a DyBM that assumes the predicted data is\ngenerated by a Gaussian distribution whose first-order moment (mean)\ndynamically changes over time but its second-order moment (variance) is fixed.\nHowever, in many financial applications, the assumption is quite limiting in\ntwo aspects. First, even when the data follows a Gaussian distribution, its\nvariance may change over time. Such variance is also related to important\ntemporal economic indicators such as the market volatility. Second, financial\ntime-series data often requires learning datasets generated by the generalized\nGaussian distribution with an additional shape parameter that is important to\napproximate heavy-tailed distributions. Addressing those aspects, we show how\nto extend DyBM that results in significant performance improvement in\npredicting financial time-series data. \n\n"}
{"id": "1712.06214", "contents": "Title: Predicting Individual Physiologically Acceptable States for Discharge\n  from a Pediatric Intensive Care Unit Abstract: Objective: Predict patient-specific vitals deemed medically acceptable for\ndischarge from a pediatric intensive care unit (ICU). Design: The means of each\npatient's hr, sbp and dbp measurements between their medical and physical\ndischarge from the ICU were computed as a proxy for their physiologically\nacceptable state space (PASS) for successful ICU discharge. These individual\nPASS values were compared via root mean squared error (rMSE) to population\nage-normal vitals, a polynomial regression through the PASS values of a\nPediatric ICU (PICU) population and predictions from two recurrent neural\nnetwork models designed to predict personalized PASS within the first twelve\nhours following ICU admission. Setting: PICU at Children's Hospital Los Angeles\n(CHLA). Patients: 6,899 PICU episodes (5,464 patients) collected between 2009\nand 2016. Interventions: None. Measurements: Each episode data contained 375\nvariables representing vitals, labs, interventions, and drugs. They also\nincluded a time indicator for PICU medical discharge and physical discharge.\nMain Results: The rMSEs between individual PASS values and population\nage-normals (hr: 25.9 bpm, sbp: 13.4 mmHg, dbp: 13.0 mmHg) were larger than the\nrMSEs corresponding to the polynomial regression (hr: 19.1 bpm, sbp: 12.3 mmHg,\ndbp: 10.8 mmHg). The rMSEs from the best performing RNN model were the lowest\n(hr: 16.4 bpm; sbp: 9.9 mmHg, dbp: 9.0 mmHg). Conclusion: PICU patients are a\nunique subset of the general population, and general age-normal vitals may not\nbe suitable as target values indicating physiologic stability at discharge.\nAge-normal vitals that were specifically derived from the medical-to-physical\ndischarge window of ICU patients may be more appropriate targets for\n'acceptable' physiologic state for critical care patients. Going beyond simple\nage bins, an RNN model can provide more personalized target values. \n\n"}
{"id": "1712.07102", "contents": "Title: On Data-Dependent Random Features for Improved Generalization in\n  Supervised Learning Abstract: The randomized-feature approach has been successfully employed in large-scale\nkernel approximation and supervised learning. The distribution from which the\nrandom features are drawn impacts the number of features required to\nefficiently perform a learning task. Recently, it has been shown that employing\ndata-dependent randomization improves the performance in terms of the required\nnumber of random features. In this paper, we are concerned with the\nrandomized-feature approach in supervised learning for good generalizability.\nWe propose the Energy-based Exploration of Random Features (EERF) algorithm\nbased on a data-dependent score function that explores the set of possible\nfeatures and exploits the promising regions. We prove that the proposed score\nfunction with high probability recovers the spectrum of the best fit within the\nmodel class. Our empirical results on several benchmark datasets further verify\nthat our method requires smaller number of random features to achieve a certain\ngeneralization error compared to the state-of-the-art while introducing\nnegligible pre-processing overhead. EERF can be implemented in a few lines of\ncode and requires no additional tuning parameters. \n\n"}
{"id": "1712.07106", "contents": "Title: Exploring High-Dimensional Structure via Axis-Aligned Decomposition of\n  Linear Projections Abstract: Two-dimensional embeddings remain the dominant approach to visualize high\ndimensional data. The choice of embeddings ranges from highly non-linear ones,\nwhich can capture complex relationships but are difficult to interpret\nquantitatively, to axis-aligned projections, which are easy to interpret but\nare limited to bivariate relationships. Linear project can be considered as a\ncompromise between complexity and interpretability, as they allow explicit axes\nlabels, yet provide significantly more degrees of freedom compared to\naxis-aligned projections. Nevertheless, interpreting the axes directions, which\nare linear combinations often with many non-trivial components, remains\ndifficult. To address this problem we introduce a structure aware decomposition\nof (multiple) linear projections into sparse sets of axis aligned projections,\nwhich jointly capture all information of the original linear ones. In\nparticular, we use tools from Dempster-Shafer theory to formally define how\nrelevant a given axis aligned project is to explain the neighborhood relations\ndisplayed in some linear projection. Furthermore, we introduce a new approach\nto discover a diverse set of high quality linear projections and show that in\npractice the information of $k$ linear projections is often jointly encoded in\n$\\sim k$ axis aligned plots. We have integrated these ideas into an interactive\nvisualization system that allows users to jointly browse both linear\nprojections and their axis aligned representatives. Using a number of case\nstudies we show how the resulting plots lead to more intuitive visualizations\nand new insight. \n\n"}
{"id": "1712.08244", "contents": "Title: How Well Can Generative Adversarial Networks Learn Densities: A\n  Nonparametric View Abstract: We study in this paper the rate of convergence for learning densities under\nthe Generative Adversarial Networks (GAN) framework, borrowing insights from\nnonparametric statistics. We introduce an improved GAN estimator that achieves\na faster rate, through simultaneously leveraging the level of smoothness in the\ntarget density and the evaluation metric, which in theory remedies the mode\ncollapse problem reported in the literature. A minimax lower bound is\nconstructed to show that when the dimension is large, the exponent in the rate\nfor the new GAN estimator is near optimal. One can view our results as\nanswering in a quantitative way how well GAN learns a wide range of densities\nwith different smoothness properties, under a hierarchy of evaluation metrics.\nAs a byproduct, we also obtain improved generalization bounds for GAN with\ndeeper ReLU discriminator network. \n\n"}
{"id": "1712.08370", "contents": "Title: Music Genre Classification with Paralleling Recurrent Convolutional\n  Neural Network Abstract: Deep learning has been demonstrated its effectiveness and efficiency in music\ngenre classification. However, the existing achievements still have several\nshortcomings which impair the performance of this classification task. In this\npaper, we propose a hybrid architecture which consists of the paralleling CNN\nand Bi-RNN blocks. They focus on spatial features and temporal frame orders\nextraction respectively. Then the two outputs are fused into one powerful\nrepresentation of musical signals and fed into softmax function for\nclassification. The paralleling network guarantees the extracting features\nrobust enough to represent music. Moreover, the experiments prove our proposed\narchitecture improve the music genre classification performance and the\nadditional Bi-RNN block is a supplement for CNNs. \n\n"}
{"id": "1712.09043", "contents": "Title: Neural Collaborative Autoencoder Abstract: In recent years, deep neural networks have yielded state-of-the-art\nperformance on several tasks. Although some recent works have focused on\ncombining deep learning with recommendation, we highlight three issues of\nexisting models. First, these models cannot work on both explicit and implicit\nfeedback, since the network structures are specially designed for one\nparticular case. Second, due to the difficulty on training deep neural\nnetworks, existing explicit models do not fully exploit the expressive\npotential of deep learning. Third, neural network models are easier to overfit\non the implicit setting than shallow models. To tackle these issues, we present\na generic recommender framework called Neural Collaborative Autoencoder (NCAE)\nto perform collaborative filtering, which works well for both explicit feedback\nand implicit feedback. NCAE can effectively capture the subtle hidden\nrelationships between interactions via a non-linear matrix factorization\nprocess. To optimize the deep architecture of NCAE, we develop a three-stage\npre-training mechanism that combines supervised and unsupervised feature\nlearning. Moreover, to prevent overfitting on the implicit setting, we propose\nan error reweighting module and a sparsity-aware data-augmentation strategy.\nExtensive experiments on three real-world datasets demonstrate that NCAE can\nsignificantly advance the state-of-the-art. \n\n"}
{"id": "1712.09482", "contents": "Title: Robust Loss Functions under Label Noise for Deep Neural Networks Abstract: In many applications of classifier learning, training data suffers from label\nnoise. Deep networks are learned using huge training data where the problem of\nnoisy labels is particularly relevant. The current techniques proposed for\nlearning deep networks under label noise focus on modifying the network\narchitecture and on algorithms for estimating true labels from noisy labels. An\nalternate approach would be to look for loss functions that are inherently\nnoise-tolerant. For binary classification there exist theoretical results on\nloss functions that are robust to label noise. In this paper, we provide some\nsufficient conditions on a loss function so that risk minimization under that\nloss function would be inherently tolerant to label noise for multiclass\nclassification problems. These results generalize the existing results on\nnoise-tolerant loss functions for binary classification. We study some of the\nwidely used loss functions in deep networks and show that the loss function\nbased on mean absolute value of error is inherently robust to label noise. Thus\nstandard back propagation is enough to learn the true classifier even under\nlabel noise. Through experiments, we illustrate the robustness of risk\nminimization with such loss functions for learning neural networks. \n\n"}
{"id": "1712.09550", "contents": "Title: Active Search for High Recall: a Non-Stationary Extension of Thompson\n  Sampling Abstract: We consider the problem of Active Search, where a maximum of relevant objects\n- ideally all relevant objects - should be retrieved with the minimum effort or\nminimum time. Typically, there are two main challenges to face when tackling\nthis problem: first, the class of relevant objects has often low prevalence\nand, secondly, this class can be multi-faceted or multi-modal: objects could be\nrelevant for completely different reasons. To solve this problem and its\nassociated issues, we propose an approach based on a non-stationary (aka\nrestless) extension of Thompson Sampling, a well-known strategy for Multi-Armed\nBandits problems. The collection is first soft-clustered into a finite set of\ncomponents and a posterior distribution of getting a relevant object inside\neach cluster is updated after receiving the user feedback about the proposed\ninstances. The \"next instance\" selection strategy is a mixed, two-level\ndecision process, where both the soft clusters and their instances are\nconsidered. This method can be considered as an insurance, where the cost of\nthe insurance is an extra exploration effort in the short run, for achieving a\nnearly \"total\" recall with less efforts in the long run. \n\n"}
{"id": "1801.00507", "contents": "Title: MACRO: A Meta-Algorithm for Conditional Risk Minimization Abstract: We study conditional risk minimization (CRM), i.e. the problem of learning a\nhypothesis of minimal risk for prediction at the next step of sequentially\narriving dependent data. Despite it being a fundamental problem, successful\nlearning in the CRM sense has so far only been demonstrated using theoretical\nalgorithms that cannot be used for real problems as they would require storing\nall incoming data. In this work, we introduce MACRO, a meta-algorithm for CRM\nthat does not suffer from this shortcoming, but nevertheless offers learning\nguarantees. Instead of storing all data it maintains and iteratively updates a\nset of learning subroutines. With suitable approximations, MACRO applied to\nreal data, yielding improved prediction performance compared to traditional\nnon-conditional learning. \n\n"}
{"id": "1801.01443", "contents": "Title: A semi-supervised fuzzy GrowCut algorithm to segment and classify\n  regions of interest of mammographic images Abstract: According to the World Health Organization, breast cancer is the most common\nform of cancer in women. It is the second leading cause of death among women\nround the world, becoming the most fatal form of cancer. Mammographic image\nsegmentation is a fundamental task to support image analysis and diagnosis,\ntaking into account shape analysis of mammary lesions and their borders.\nHowever, mammogram segmentation is a very hard process, once it is highly\ndependent on the types of mammary tissues. In this work we present a new\nsemi-supervised segmentation algorithm based on the modification of the GrowCut\nalgorithm to perform automatic mammographic image segmentation once a region of\ninterest is selected by a specialist. In our proposal, we used fuzzy Gaussian\nmembership functions to modify the evolution rule of the original GrowCut\nalgorithm, in order to estimate the uncertainty of a pixel being object or\nbackground. The main impact of the proposed method is the significant reduction\nof expert effort in the initialization of seed points of GrowCut to perform\naccurate segmentation, once it removes the need of selection of background\nseeds. We also constructed an automatic point selection process based on the\nsimulated annealing optimization method, avoiding the need of human\nintervention. The proposed approach was qualitatively compared with other\nstate-of-the-art segmentation techniques, considering the shape of segmented\nregions. In order to validate our proposal, we built an image classifier using\na classical multilayer perceptron. We used Zernike moments to extract segmented\nimage features. This analysis employed 685 mammograms from IRMA breast cancer\ndatabase, using fat and fibroid tissues. Results show that the proposed\ntechnique could achieve a classification rate of 91.28\\% for fat tissues,\nevidencing the feasibility of our approach. \n\n"}
{"id": "1801.02178", "contents": "Title: Neural Networks for Information Retrieval Abstract: Machine learning plays a role in many aspects of modern IR systems, and deep\nlearning is applied in all of them. The fast pace of modern-day research has\ngiven rise to many approaches to many IR problems. The amount of information\navailable can be overwhelming both for junior students and for experienced\nresearchers looking for new research topics and directions. The aim of this\nfull-day tutorial is to give a clear overview of current tried-and-trusted\nneural methods in IR and how they benefit IR. \n\n"}
{"id": "1801.02203", "contents": "Title: Indian Regional Movie Dataset for Recommender Systems Abstract: Indian regional movie dataset is the first database of regional Indian\nmovies, users and their ratings. It consists of movies belonging to 18\ndifferent Indian regional languages and metadata of users with varying\ndemographics. Through this dataset, the diversity of Indian regional cinema and\nits huge viewership is captured. We analyze the dataset that contains roughly\n10K ratings of 919 users and 2,851 movies using some supervised and\nunsupervised collaborative filtering techniques like Probabilistic Matrix\nFactorization, Matrix Completion, Blind Compressed Sensing etc. The dataset\nconsists of metadata information of users like age, occupation, home state and\nknown languages. It also consists of metadata of movies like genre, language,\nrelease year and cast. India has a wide base of viewers which is evident by the\nlarge number of movies released every year and the huge box-office revenue.\nThis dataset can be used for designing recommendation systems for Indian users\nand regional movies, which do not, yet, exist. The dataset can be downloaded\nfrom \\href{https://goo.gl/EmTPv6}{https://goo.gl/EmTPv6}. \n\n"}
{"id": "1801.02227", "contents": "Title: Gradient Layer: Enhancing the Convergence of Adversarial Training for\n  Generative Models Abstract: We propose a new technique that boosts the convergence of training generative\nadversarial networks. Generally, the rate of training deep models reduces\nseverely after multiple iterations. A key reason for this phenomenon is that a\ndeep network is expressed using a highly non-convex finite-dimensional model,\nand thus the parameter gets stuck in a local optimum. Because of this, methods\noften suffer not only from degeneration of the convergence speed but also from\nlimitations in the representational power of the trained network. To overcome\nthis issue, we propose an additional layer called the gradient layer to seek a\ndescent direction in an infinite-dimensional space. Because the layer is\nconstructed in the infinite-dimensional space, we are not restricted by the\nspecific model structure of finite-dimensional models. As a result, we can get\nout of the local optima in finite-dimensional models and move towards the\nglobal optimal function more directly. In this paper, this phenomenon is\nexplained from the functional gradient method perspective of the gradient\nlayer. Interestingly, the optimization procedure using the gradient layer\nnaturally constructs the deep structure of the network. Moreover, we\ndemonstrate that this procedure can be regarded as a discretization method of\nthe gradient flow that naturally reduces the objective function. Finally, the\nmethod is tested using several numerical experiments, which show its fast\nconvergence. \n\n"}
{"id": "1801.02607", "contents": "Title: Web2Text: Deep Structured Boilerplate Removal Abstract: Web pages are a valuable source of information for many natural language\nprocessing and information retrieval tasks. Extracting the main content from\nthose documents is essential for the performance of derived applications. To\naddress this issue, we introduce a novel model that performs sequence labeling\nto collectively classify all text blocks in an HTML page as either boilerplate\nor main content. Our method uses a hidden Markov model on top of potentials\nderived from DOM tree features using convolutional neural networks. The\nproposed method sets a new state-of-the-art performance for boilerplate removal\non the CleanEval benchmark. As a component of information retrieval pipelines,\nit improves retrieval performance on the ClueWeb12 collection. \n\n"}
{"id": "1801.03329", "contents": "Title: Weakly Supervised One-Shot Detection with Attention Similarity Networks Abstract: Neural network models that are not conditioned on class identities were shown\nto facilitate knowledge transfer between classes and to be well-suited for\none-shot learning tasks. Following this motivation, we further explore and\nestablish such models and present a novel neural network architecture for the\ntask of weakly supervised one-shot detection. Our model is only conditioned on\na single exemplar of an unseen class and a larger target example that may or\nmay not contain an instance of the same class as the exemplar. By pairing a\nSiamese similarity network with an attention mechanism, we design a model that\nmanages to simultaneously identify and localise instances of classes unseen at\ntraining time. In experiments with datasets from the computer vision and audio\ndomains, the proposed method considerably outperforms the baseline methods for\nthe weakly supervised one-shot detection task. \n\n"}
{"id": "1801.04354", "contents": "Title: Black-box Generation of Adversarial Text Sequences to Evade Deep\n  Learning Classifiers Abstract: Although various techniques have been proposed to generate adversarial\nsamples for white-box attacks on text, little attention has been paid to\nblack-box attacks, which are more realistic scenarios. In this paper, we\npresent a novel algorithm, DeepWordBug, to effectively generate small text\nperturbations in a black-box setting that forces a deep-learning classifier to\nmisclassify a text input. We employ novel scoring strategies to identify the\ncritical tokens that, if modified, cause the classifier to make an incorrect\nprediction. Simple character-level transformations are applied to the\nhighest-ranked tokens in order to minimize the edit distance of the\nperturbation, yet change the original classification. We evaluated DeepWordBug\non eight real-world text datasets, including text classification, sentiment\nanalysis, and spam detection. We compare the result of DeepWordBug with two\nbaselines: Random (Black-box) and Gradient (White-box). Our experimental\nresults indicate that DeepWordBug reduces the prediction accuracy of current\nstate-of-the-art deep-learning models, including a decrease of 68\\% on average\nfor a Word-LSTM model and 48\\% on average for a Char-CNN model. \n\n"}
{"id": "1801.05532", "contents": "Title: Reinforcement Learning based Recommender System using Biclustering\n  Technique Abstract: A recommender system aims to recommend items that a user is interested in\namong many items. The need for the recommender system has been expanded by the\ninformation explosion. Various approaches have been suggested for providing\nmeaningful recommendations to users. One of the proposed approaches is to\nconsider a recommender system as a Markov decision process (MDP) problem and\ntry to solve it using reinforcement learning (RL). However, existing RL-based\nmethods have an obvious drawback. To solve an MDP in a recommender system, they\nencountered a problem with the large number of discrete actions that bring RL\nto a larger class of problems. In this paper, we propose a novel RL-based\nrecommender system. We formulate a recommender system as a gridworld game by\nusing a biclustering technique that can reduce the state and action space\nsignificantly. Using biclustering not only reduces space but also improves the\nrecommendation quality effectively handling the cold-start problem. In\naddition, our approach can provide users with some explanation why the system\nrecommends certain items. Lastly, we examine the proposed algorithm on a\nreal-world dataset and achieve a better performance than the widely used\nrecommendation algorithm. \n\n"}
{"id": "1801.05881", "contents": "Title: A Pipeline for Post-Crisis Twitter Data Acquisition Abstract: Due to instant availability of data on social media platforms like Twitter,\nand advances in machine learning and data management technology, real-time\ncrisis informatics has emerged as a prolific research area in the last decade.\nAlthough several benchmarks are now available, especially on portals like\nCrisisLex, an important, practical problem that has not been addressed thus far\nis the rapid acquisition and benchmarking of data from free, publicly available\nstreams like the Twitter API. In this paper, we present ongoing work on a\npipeline for facilitating immediate post-crisis data collection, curation and\nrelevance filtering from the Twitter API. The pipeline is minimally supervised,\nalleviating the need for feature engineering by including a judicious mix of\ndata preprocessing and fast text embeddings, along with an active learning\nframework. We illustrate the utility of the pipeline by describing a recent\ncase study wherein it was used to collect and analyze millions of tweets in the\nimmediate aftermath of the Las Vegas shootings. \n\n"}
{"id": "1801.06510", "contents": "Title: Image Provenance Analysis at Scale Abstract: Prior art has shown it is possible to estimate, through image processing and\ncomputer vision techniques, the types and parameters of transformations that\nhave been applied to the content of individual images to obtain new images.\nGiven a large corpus of images and a query image, an interesting further step\nis to retrieve the set of original images whose content is present in the query\nimage, as well as the detailed sequences of transformations that yield the\nquery image given the original images. This is a problem that recently has\nreceived the name of image provenance analysis. In these times of public media\nmanipulation ( e.g., fake news and meme sharing), obtaining the history of\nimage transformations is relevant for fact checking and authorship\nverification, among many other applications. This article presents an\nend-to-end processing pipeline for image provenance analysis, which works at\nreal-world scale. It employs a cutting-edge image filtering solution that is\ncustom-tailored for the problem at hand, as well as novel techniques for\nobtaining the provenance graph that expresses how the images, as nodes, are\nancestrally connected. A comprehensive set of experiments for each stage of the\npipeline is provided, comparing the proposed solution with state-of-the-art\nresults, employing previously published datasets. In addition, this work\nintroduces a new dataset of real-world provenance cases from the social media\nsite Reddit, along with baseline results. \n\n"}
{"id": "1801.06637", "contents": "Title: Deep Hidden Physics Models: Deep Learning of Nonlinear Partial\n  Differential Equations Abstract: A long-standing problem at the interface of artificial intelligence and\napplied mathematics is to devise an algorithm capable of achieving human level\nor even superhuman proficiency in transforming observed data into predictive\nmathematical models of the physical world. In the current era of abundance of\ndata and advanced machine learning capabilities, the natural question arises:\nHow can we automatically uncover the underlying laws of physics from\nhigh-dimensional data generated from experiments? In this work, we put forth a\ndeep learning approach for discovering nonlinear partial differential equations\nfrom scattered and potentially noisy observations in space and time.\nSpecifically, we approximate the unknown solution as well as the nonlinear\ndynamics by two deep neural networks. The first network acts as a prior on the\nunknown solution and essentially enables us to avoid numerical differentiations\nwhich are inherently ill-conditioned and unstable. The second network\nrepresents the nonlinear dynamics and helps us distill the mechanisms that\ngovern the evolution of a given spatiotemporal data-set. We test the\neffectiveness of our approach for several benchmark problems spanning a number\nof scientific domains and demonstrate how the proposed framework can help us\naccurately learn the underlying dynamics and forecast future states of the\nsystem. In particular, we study the Burgers', Korteweg-de Vries (KdV),\nKuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations. \n\n"}
{"id": "1801.06664", "contents": "Title: Ontology-based Adaptive e-Textbook Platform for Student and Machine\n  Co-Learning Abstract: The use of electronic textbooks (e-book) has been heavily studied over the\nyears due to their flexibility, accessibility, interactivity and extensibility.\nYet current shortcomings of e-book, which is often just a digitized version of\nthe original book, does not encourage adoption. Consequently, this leads to a\nrethinking of e-book that should incorporate current technologies to augment\nits capabilities, where inclusion of information search and organization tools\nhave shown to be favorable. This paper is on a preliminary work to add\nintelligence into such tools in terms of information retrieval. Construction of\nknowledge graph for e-book material with little overhead is first introduced.\nInformation retrieval through typed similarity query is then performed via\nrandom walk. Case study demonstrate the applicability of the e-book platform,\nwith promising application and advancement in the area of electronic textbooks. \n\n"}
{"id": "1801.08702", "contents": "Title: Improving Bi-directional Generation between Different Modalities with\n  Variational Autoencoders Abstract: We investigate deep generative models that can exchange multiple modalities\nbi-directionally, e.g., generating images from corresponding texts and vice\nversa. A major approach to achieve this objective is to train a model that\nintegrates all the information of different modalities into a joint\nrepresentation and then to generate one modality from the corresponding other\nmodality via this joint representation. We simply applied this approach to\nvariational autoencoders (VAEs), which we call a joint multimodal variational\nautoencoder (JMVAE). However, we found that when this model attempts to\ngenerate a large dimensional modality missing at the input, the joint\nrepresentation collapses and this modality cannot be generated successfully.\nFurthermore, we confirmed that this difficulty cannot be resolved even using a\nknown solution. Therefore, in this study, we propose two models to prevent this\ndifficulty: JMVAE-kl and JMVAE-h. Results of our experiments demonstrate that\nthese methods can prevent the difficulty above and that they generate\nmodalities bi-directionally with equal or higher likelihood than conventional\nVAE methods, which generate in only one direction. Moreover, we confirm that\nthese methods can obtain the joint representation appropriately, so that they\ncan generate various variations of modality by moving over the joint\nrepresentation or changing the value of another modality. \n\n"}
{"id": "1801.08712", "contents": "Title: Classification of sparsely labeled spatio-temporal data through\n  semi-supervised adversarial learning Abstract: In recent years, Generative Adversarial Networks (GAN) have emerged as a\npowerful method for learning the mapping from noisy latent spaces to realistic\ndata samples in high-dimensional space. So far, the development and application\nof GANs have been predominantly focused on spatial data such as images. In this\nproject, we aim at modeling of spatio-temporal sensor data instead, i.e.\ndynamic data over time. The main goal is to encode temporal data into a global\nand low-dimensional latent vector that captures the dynamics of the\nspatio-temporal signal. To this end, we incorporate auto-regressive RNNs,\nWasserstein GAN loss, spectral norm weight constraints and a semi-supervised\nlearning scheme into InfoGAN, a method for retrieval of meaningful latents in\nadversarial learning. To demonstrate the modeling capability of our method, we\nencode full-body skeletal human motion from a large dataset representing 60\nclasses of daily activities, recorded in a multi-Kinect setup. Initial results\nindicate competitive classification performance of the learned latent\nrepresentations, compared to direct CNN/RNN inference. In future work, we plan\nto apply this method on a related problem in the medical domain, i.e. on\nrecovery of meaningful latents in gait analysis of patients with vertigo and\nbalance disorders. \n\n"}
{"id": "1801.09197", "contents": "Title: Algorithmic Linearly Constrained Gaussian Processes Abstract: We algorithmically construct multi-output Gaussian process priors which\nsatisfy linear differential equations. Our approach attempts to parametrize all\nsolutions of the equations using Gr\\\"obner bases. If successful, a push forward\nGaussian process along the paramerization is the desired prior. We consider\nseveral examples from physics, geomathematics and control, among them the full\ninhomogeneous system of Maxwell's equations. By bringing together stochastic\nlearning and computer algebra in a novel way, we combine noisy observations\nwith precise algebraic computations. \n\n"}
{"id": "1802.01737", "contents": "Title: Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent Abstract: Coherent uncertainty quantification is a key strength of Bayesian methods.\nBut modern algorithms for approximate Bayesian posterior inference often\nsacrifice accurate posterior uncertainty estimation in the pursuit of\nscalability. This work shows that previous Bayesian coreset construction\nalgorithms---which build a small, weighted subset of the data that approximates\nthe full dataset---are no exception. We demonstrate that these algorithms scale\nthe coreset log-likelihood suboptimally, resulting in underestimated posterior\nuncertainty. To address this shortcoming, we develop greedy iterative geodesic\nascent (GIGA), a novel algorithm for Bayesian coreset construction that scales\nthe coreset log-likelihood optimally. GIGA provides geometric decay in\nposterior approximation error as a function of coreset size, and maintains the\nfast running time of its predecessors. The paper concludes with validation of\nGIGA on both synthetic and real datasets, demonstrating that it reduces\nposterior approximation error by orders of magnitude compared with previous\ncoreset constructions. \n\n"}
{"id": "1802.02920", "contents": "Title: Spectral State Compression of Markov Processes Abstract: Model reduction of Markov processes is a basic problem in modeling\nstate-transition systems. Motivated by the state aggregation approach rooted in\ncontrol theory, we study the statistical state compression of a discrete-state\nMarkov chain from empirical trajectories. Through the lens of spectral\ndecomposition, we study the rank and features of Markov processes, as well as\nproperties like representability, aggregability, and lumpability. We develop\nspectral methods for estimating the transition matrix of a low-rank Markov\nmodel, estimating the leading subspace spanned by Markov features, and\nrecovering latent structures like state aggregation and lumpable partition of\nthe state space. We prove statistical upper bounds for the estimation errors\nand nearly matching minimax lower bounds. Numerical studies are performed on\nsynthetic data and a dataset of New York City taxi trips. \n\n"}
{"id": "1802.03203", "contents": "Title: Curve Registered Coupled Low Rank Factorization Abstract: We propose an extension of the canonical polyadic (CP) tensor model where one\nof the latent factors is allowed to vary through data slices in a constrained\nway. The components of the latent factors, which we want to retrieve from data,\ncan vary from one slice to another up to a diffeomorphism. We suppose that the\ndiffeomorphisms are also unknown, thus merging curve registration and tensor\ndecomposition in one model, which we call registered CP. We present an\nalgorithm to retrieve both the latent factors and the diffeomorphism, which is\nassumed to be in a parametrized form. At the end of the paper, we show\nsimulation results comparing registered CP with other models from the\nliterature. \n\n"}
{"id": "1802.03334", "contents": "Title: Learning Localized Spatio-Temporal Models From Streaming Data Abstract: We address the problem of predicting spatio-temporal processes with temporal\npatterns that vary across spatial regions, when data is obtained as a stream.\nThat is, when the training dataset is augmented sequentially. Specifically, we\ndevelop a localized spatio-temporal covariance model of the process that can\ncapture spatially varying temporal periodicities in the data. We then apply a\ncovariance-fitting methodology to learn the model parameters which yields a\npredictor that can be updated sequentially with each new data point. The\nproposed method is evaluated using both synthetic and real climate data which\ndemonstrate its ability to accurately predict data missing in spatial regions\nover time. \n\n"}
{"id": "1802.03676", "contents": "Title: Differentiable Dynamic Programming for Structured Prediction and\n  Attention Abstract: Dynamic programming (DP) solves a variety of structured combinatorial\nproblems by iteratively breaking them down into smaller subproblems. In spite\nof their versatility, DP algorithms are usually non-differentiable, which\nhampers their use as a layer in neural networks trained by backpropagation. To\naddress this issue, we propose to smooth the max operator in the dynamic\nprogramming recursion, using a strongly convex regularizer. This allows to\nrelax both the optimal value and solution of the original combinatorial\nproblem, and turns a broad class of DP algorithms into differentiable\noperators. Theoretically, we provide a new probabilistic perspective on\nbackpropagating through these DP operators, and relate them to inference in\ngraphical models. We derive two particular instantiations of our framework, a\nsmoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm\nfor time-series alignment. We showcase these instantiations on two structured\nprediction tasks and on structured and sparse attention for neural machine\ntranslation. \n\n"}
{"id": "1802.03692", "contents": "Title: Nearly Optimal Adaptive Procedure with Change Detection for\n  Piecewise-Stationary Bandit Abstract: Multi-armed bandit (MAB) is a class of online learning problems where a\nlearning agent aims to maximize its expected cumulative reward while repeatedly\nselecting to pull arms with unknown reward distributions. We consider a\nscenario where the reward distributions may change in a piecewise-stationary\nfashion at unknown time steps. We show that by incorporating a simple\nchange-detection component with classic UCB algorithms to detect and adapt to\nchanges, our so-called M-UCB algorithm can achieve nearly optimal regret bound\non the order of $O(\\sqrt{MKT\\log T})$, where $T$ is the number of time steps,\n$K$ is the number of arms, and $M$ is the number of stationary segments.\nComparison with the best available lower bound shows that our M-UCB is nearly\noptimal in $T$ up to a logarithmic factor. We also compare M-UCB with the\nstate-of-the-art algorithms in numerical experiments using a public Yahoo!\ndataset to demonstrate its superior performance. \n\n"}
{"id": "1802.03713", "contents": "Title: $\\mathcal{G}$-SGD: Optimizing ReLU Neural Networks in its Positively\n  Scale-Invariant Space Abstract: It is well known that neural networks with rectified linear units (ReLU)\nactivation functions are positively scale-invariant. Conventional algorithms\nlike stochastic gradient descent optimize the neural networks in the vector\nspace of weights, which is, however, not positively scale-invariant. This\nmismatch may lead to problems during the optimization process. Then, a natural\nquestion is: \\emph{can we construct a new vector space that is positively\nscale-invariant and sufficient to represent ReLU neural networks so as to\nbetter facilitate the optimization process }? In this paper, we provide our\npositive answer to this question. First, we conduct a formal study on the\npositive scaling operators which forms a transformation group, denoted as\n$\\mathcal{G}$. We show that the value of a path (i.e. the product of the\nweights along the path) in the neural network is invariant to positive scaling\nand prove that the value vector of all the paths is sufficient to represent the\nneural networks under mild conditions. Second, we show that one can identify\nsome basis paths out of all the paths and prove that the linear span of their\nvalue vectors (denoted as $\\mathcal{G}$-space) is an invariant space with lower\ndimension under the positive scaling group. Finally, we design stochastic\ngradient descent algorithm in $\\mathcal{G}$-space (abbreviated as\n$\\mathcal{G}$-SGD) to optimize the value vector of the basis paths of neural\nnetworks with little extra cost by leveraging back-propagation. Our experiments\nshow that $\\mathcal{G}$-SGD significantly outperforms the conventional SGD\nalgorithm in optimizing ReLU networks on benchmark datasets. \n\n"}
{"id": "1802.03793", "contents": "Title: Large-Scale Validation of Hypothesis Generation Systems via Candidate\n  Ranking Abstract: The first step of many research projects is to define and rank a short list\nof candidates for study. In the modern rapidity of scientific progress, some\nturn to automated hypothesis generation (HG) systems to aid this process. These\nsystems can identify implicit or overlooked connections within a large\nscientific corpus, and while their importance grows alongside the pace of\nscience, they lack thorough validation. Without any standard numerical\nevaluation method, many validate general-purpose HG systems by rediscovering a\nhandful of historical findings, and some wishing to be more thorough may run\nlaboratory experiments based on automatic suggestions. These methods are\nexpensive, time consuming, and cannot scale. Thus, we present a numerical\nevaluation framework for the purpose of validating HG systems that leverages\nthousands of validation hypotheses. This method evaluates a HG system by its\nability to rank hypotheses by plausibility; a process reminiscent of human\ncandidate selection. Because HG systems do not produce a ranking criteria,\nspecifically those that produce topic models, we additionally present novel\nmetrics to quantify the plausibility of hypotheses given topic model system\noutput. Finally, we demonstrate that our proposed validation method aligns with\nreal-world research goals by deploying our method within Moliere, our recent\ntopic-driven HG system, in order to automatically generate a set of candidate\ngenes related to HIV-associated neurodegenerative disease (HAND). By performing\nlaboratory experiments based on this candidate set, we discover a new\nconnection between HAND and Dead Box RNA Helicase 3 (DDX3). Reproducibility:\ncode, validation data, and results can be found at\nsybrandt.com/2018/validation. \n\n"}
{"id": "1802.04675", "contents": "Title: Attention based Sentence Extraction from Scientific Articles using\n  Pseudo-Labeled data Abstract: In this work, we present a weakly supervised sentence extraction technique\nfor identifying important sentences in scientific papers that are worthy of\ninclusion in the abstract. We propose a new attention based deep learning\narchitecture that jointly learns to identify important content, as well as the\ncue phrases that are indicative of summary worthy sentences. We propose a new\ncontext embedding technique for determining the focus of a given paper using\ntopic models and use it jointly with an LSTM based sequence encoder to learn\nattention weights across the sentence words. We use a collection of articles\npublicly available through ACL anthology for our experiments. Our system\nachieves a performance that is better, in terms of several ROUGE metrics, as\ncompared to several state of art extractive techniques. It also generates more\ncoherent summaries and preserves the overall structure of the document. \n\n"}
{"id": "1802.04868", "contents": "Title: SimplE Embedding for Link Prediction in Knowledge Graphs Abstract: Knowledge graphs contain knowledge about the world and provide a structured\nrepresentation of this knowledge. Current knowledge graphs contain only a small\nsubset of what is true in the world. Link prediction approaches aim at\npredicting new links for a knowledge graph given the existing links among the\nentities. Tensor factorization approaches have proved promising for such link\nprediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is\namong the first tensor factorization approaches. CP generally performs poorly\nfor link prediction as it learns two independent embedding vectors for each\nentity, whereas they are really tied. We present a simple enhancement of CP\n(which we call SimplE) to allow the two embeddings of each entity to be learned\ndependently. The complexity of SimplE grows linearly with the size of\nembeddings. The embeddings learned through SimplE are interpretable, and\ncertain types of background knowledge can be incorporated into these embeddings\nthrough weight tying. We prove SimplE is fully expressive and derive a bound on\nthe size of its embeddings for full expressivity. We show empirically that,\ndespite its simplicity, SimplE outperforms several state-of-the-art tensor\nfactorization techniques. SimplE's code is available on GitHub at\nhttps://github.com/Mehran-k/SimplE. \n\n"}
{"id": "1802.04911", "contents": "Title: Large-Scale Sparse Inverse Covariance Estimation via Thresholding and\n  Max-Det Matrix Completion Abstract: The sparse inverse covariance estimation problem is commonly solved using an\n$\\ell_{1}$-regularized Gaussian maximum likelihood estimator known as\n\"graphical lasso\", but its computational cost becomes prohibitive for large\ndata sets. A recent line of results showed--under mild assumptions--that the\ngraphical lasso estimator can be retrieved by soft-thresholding the sample\ncovariance matrix and solving a maximum determinant matrix completion (MDMC)\nproblem. This paper proves an extension of this result, and describes a\nNewton-CG algorithm to efficiently solve the MDMC problem. Assuming that the\nthresholded sample covariance matrix is sparse with a sparse Cholesky\nfactorization, we prove that the algorithm converges to an $\\epsilon$-accurate\nsolution in $O(n\\log(1/\\epsilon))$ time and $O(n)$ memory. The algorithm is\nhighly efficient in practice: we solve the associated MDMC problems with as\nmany as 200,000 variables to 7-9 digits of accuracy in less than an hour on a\nstandard laptop computer running MATLAB. \n\n"}
{"id": "1802.05811", "contents": "Title: Distributed Stochastic Optimization via Adaptive SGD Abstract: Stochastic convex optimization algorithms are the most popular way to train\nmachine learning models on large-scale data. Scaling up the training process of\nthese models is crucial, but the most popular algorithm, Stochastic Gradient\nDescent (SGD), is a serial method that is surprisingly hard to parallelize. In\nthis paper, we propose an efficient distributed stochastic optimization method\nby combining adaptivity with variance reduction techniques. Our analysis yields\na linear speedup in the number of machines, constant memory footprint, and only\na logarithmic number of communication rounds. Critically, our approach is a\nblack-box reduction that parallelizes any serial online learning algorithm,\nstreamlining prior analysis and allowing us to leverage the significant\nprogress that has been made in designing adaptive algorithms. In particular, we\nachieve optimal convergence rates without any prior knowledge of smoothness\nparameters, yielding a more robust algorithm that reduces the need for\nhyperparameter tuning. We implement our algorithm in the Spark distributed\nframework and exhibit dramatic performance gains on large-scale logistic\nregression problems. \n\n"}
{"id": "1802.05846", "contents": "Title: Train on Validation: Squeezing the Data Lemon Abstract: Model selection on validation data is an essential step in machine learning.\nWhile the mixing of data between training and validation is considered taboo,\npractitioners often violate it to increase performance. Here, we offer a\nsimple, practical method for using the validation set for training, which\nallows for a continuous, controlled trade-off between performance and\noverfitting of model selection. We define the notion of\non-average-validation-stable algorithms as one in which using small portions of\nvalidation data for training does not overfit the model selection process. We\nthen prove that stable algorithms are also validation stable. Finally, we\ndemonstrate our method on the MNIST and CIFAR-10 datasets using stable\nalgorithms as well as state-of-the-art neural networks. Our results show\nsignificant increase in test performance with a minor trade-off in bias\nadmitted to the model selection process. \n\n"}
{"id": "1802.06939", "contents": "Title: Estimator of Prediction Error Based on Approximate Message Passing for\n  Penalized Linear Regression Abstract: We propose an estimator of prediction error using an approximate message\npassing (AMP) algorithm that can be applied to a broad range of sparse\npenalties. Following Stein's lemma, the estimator of the generalized degrees of\nfreedom, which is a key quantity for the construction of the estimator of the\nprediction error, is calculated at the AMP fixed point. The resulting form of\nthe AMP-based estimator does not depend on the penalty function, and its value\ncan be further improved by considering the correlation between predictors. The\nproposed estimator is asymptotically unbiased when the components of the\npredictors and response variables are independently generated according to a\nGaussian distribution. We examine the behaviour of the estimator for real data\nunder nonconvex sparse penalties, where Akaike's information criterion does not\ncorrespond to an unbiased estimator of the prediction error. The model selected\nby the proposed estimator is close to that which minimizes the true prediction\nerror. \n\n"}
{"id": "1802.07295", "contents": "Title: Attack Strength vs. Detectability Dilemma in Adversarial Machine\n  Learning Abstract: As the prevalence and everyday use of machine learning algorithms, along with\nour reliance on these algorithms grow dramatically, so do the efforts to attack\nand undermine these algorithms with malicious intent, resulting in a growing\ninterest in adversarial machine learning. A number of approaches have been\ndeveloped that can render a machine learning algorithm ineffective through\npoisoning or other types of attacks. Most attack algorithms typically use\nsophisticated optimization approaches, whose objective function is designed to\ncause maximum damage with respect to accuracy and performance of the algorithm\nwith respect to some task. In this effort, we show that while such an objective\nfunction is indeed brutally effective in causing maximum damage on an embedded\nfeature selection task, it often results in an attack mechanism that can be\neasily detected with an embarrassingly simple novelty or outlier detection\nalgorithm. We then propose an equally simple yet elegant solution by adding a\nregularization term to the attacker's objective function that penalizes\noutlying attack points. \n\n"}
{"id": "1802.07528", "contents": "Title: Learning Integral Representations of Gaussian Processes Abstract: We propose a representation of Gaussian processes (GPs) based on powers of\nthe integral operator defined by a kernel function, we call these stochastic\nprocesses integral Gaussian processes (IGPs). Sample paths from IGPs are\nfunctions contained within the reproducing kernel Hilbert space (RKHS) defined\nby the kernel function, in contrast sample paths from the standard GP are not\nfunctions within the RKHS. We develop computationally efficient non-parametric\nregression models based on IGPs. The main innovation in our regression\nalgorithm is the construction of a low dimensional subspace that captures the\ninformation most relevant to explaining variation in the response. We use ideas\nfrom supervised dimension reduction to compute this subspace. The result of\nusing the construction we propose involves significant improvements in the\ncomputational complexity of estimating kernel hyper-parameters as well as\nreducing the prediction variance. \n\n"}
{"id": "1802.08301", "contents": "Title: Content-Based Citation Recommendation Abstract: We present a content-based method for recommending citations in an academic\npaper draft. We embed a given query document into a vector space, then use its\nnearest neighbors as candidates, and rerank the candidates using a\ndiscriminative model trained to distinguish between observed and unobserved\ncitations. Unlike previous work, our method does not require metadata such as\nauthor names which can be missing, e.g., during the peer review process.\nWithout using metadata, our method outperforms the best reported results on\nPubMed and DBLP datasets with relative improvements of over 18% in F1@20 and\nover 22% in MRR. We show empirically that, although adding metadata improves\nthe performance on standard metrics, it favors self-citations which are less\nuseful in a citation recommendation setup. We release an online portal\n(http://labs.semanticscholar.org/citeomatic/) for citation recommendation based\non our method, and a new dataset OpenCorpus of 7 million research articles to\nfacilitate future research on this task. \n\n"}
{"id": "1802.08401", "contents": "Title: Novel Approaches to Accelerating the Convergence Rate of Markov Decision\n  Process for Search Result Diversification Abstract: Recently, some studies have utilized the Markov Decision Process for\ndiversifying (MDP-DIV) the search results in information retrieval. Though\npromising performances can be delivered, MDP-DIV suffers from a very slow\nconvergence, which hinders its usability in real applications. In this paper,\nwe aim to promote the performance of MDP-DIV by speeding up the convergence\nrate without much accuracy sacrifice. The slow convergence is incurred by two\nmain reasons: the large action space and data scarcity. On the one hand, the\nsequential decision making at each position needs to evaluate the\nquery-document relevance for all the candidate set, which results in a huge\nsearching space for MDP; on the other hand, due to the data scarcity, the agent\nhas to proceed more \"trial and error\" interactions with the environment. To\ntackle this problem, we propose MDP-DIV-kNN and MDP-DIV-NTN methods. The\nMDP-DIV-kNN method adopts a $k$ nearest neighbor strategy, i.e., discarding the\n$k$ nearest neighbors of the recently-selected action (document), to reduce the\ndiversification searching space. The MDP-DIV-NTN employs a pre-trained\ndiversification neural tensor network (NTN-DIV) as the evaluation model, and\ncombines the results with MDP to produce the final ranking solution. The\nexperiment results demonstrate that the two proposed methods indeed accelerate\nthe convergence rate of the MDP-DIV, which is 3x faster, while the accuracies\nproduced barely degrade, or even are better. \n\n"}
{"id": "1802.08665", "contents": "Title: Learning Latent Permutations with Gumbel-Sinkhorn Networks Abstract: Permutations and matchings are core building blocks in a variety of latent\nvariable models, as they allow us to align, canonicalize, and sort data.\nLearning in such models is difficult, however, because exact marginalization\nover these combinatorial objects is intractable. In response, this paper\nintroduces a collection of new methods for end-to-end learning in such models\nthat approximate discrete maximum-weight matching using the continuous Sinkhorn\noperator. Sinkhorn iteration is attractive because it functions as a simple,\neasy-to-implement analog of the softmax operator. With this, we can define the\nGumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al.\n2016, Maddison2016 et al. 2016) to distributions over latent matchings. We\ndemonstrate the effectiveness of our method by outperforming competitive\nbaselines on a range of qualitatively different tasks: sorting numbers, solving\njigsaw puzzles, and identifying neural signals in worms. \n\n"}
{"id": "1802.08768", "contents": "Title: Is Generator Conditioning Causally Related to GAN Performance? Abstract: Recent work (Pennington et al, 2017) suggests that controlling the entire\ndistribution of Jacobian singular values is an important design consideration\nin deep learning. Motivated by this, we study the distribution of singular\nvalues of the Jacobian of the generator in Generative Adversarial Networks\n(GANs). We find that this Jacobian generally becomes ill-conditioned at the\nbeginning of training. Moreover, we find that the average (with z from p(z))\nconditioning of the generator is highly predictive of two other ad-hoc metrics\nfor measuring the 'quality' of trained GANs: the Inception Score and the\nFrechet Inception Distance (FID). We test the hypothesis that this relationship\nis causal by proposing a 'regularization' technique (called Jacobian Clamping)\nthat softly penalizes the condition number of the generator Jacobian. Jacobian\nClamping improves the mean Inception Score and the mean FID for GANs trained on\nseveral datasets. It also greatly reduces inter-run variance of the\naforementioned scores, addressing (at least partially) one of the main\ncriticisms of GANs. \n\n"}
{"id": "1802.09484", "contents": "Title: Disentangling the independently controllable factors of variation by\n  interacting with the world Abstract: It has been postulated that a good representation is one that disentangles\nthe underlying explanatory factors of variation. However, it remains an open\nquestion what kind of training framework could potentially achieve that.\nWhereas most previous work focuses on the static setting (e.g., with images),\nwe postulate that some of the causal factors could be discovered if the learner\nis allowed to interact with its environment. The agent can experiment with\ndifferent actions and observe their effects. More specifically, we hypothesize\nthat some of these factors correspond to aspects of the environment which are\nindependently controllable, i.e., that there exists a policy and a learnable\nfeature for each such aspect of the environment, such that this policy can\nyield changes in that feature with minimal changes to other features that\nexplain the statistical variations in the observed data. We propose a specific\nobjective function to find such factors, and verify experimentally that it can\nindeed disentangle independently controllable aspects of the environment\nwithout any extrinsic reward signal. \n\n"}
{"id": "1802.09979", "contents": "Title: The Emergence of Spectral Universality in Deep Networks Abstract: Recent work has shown that tight concentration of the entire spectrum of\nsingular values of a deep network's input-output Jacobian around one at\ninitialization can speed up learning by orders of magnitude. Therefore, to\nguide important design choices, it is important to build a full theoretical\nunderstanding of the spectra of Jacobians at initialization. To this end, we\nleverage powerful tools from free probability theory to provide a detailed\nanalytic understanding of how a deep network's Jacobian spectrum depends on\nvarious hyperparameters including the nonlinearity, the weight and bias\ndistributions, and the depth. For a variety of nonlinearities, our work reveals\nthe emergence of new universal limiting spectral distributions that remain\nconcentrated around one even as the depth goes to infinity. \n\n"}
{"id": "1802.10078", "contents": "Title: A Fast Deep Learning Model for Textual Relevance in Biomedical\n  Information Retrieval Abstract: Publications in the life sciences are characterized by a large technical\nvocabulary, with many lexical and semantic variations for expressing the same\nconcept. Towards addressing the problem of relevance in biomedical literature\nsearch, we introduce a deep learning model for the relevance of a document's\ntext to a keyword style query. Limited by a relatively small amount of training\ndata, the model uses pre-trained word embeddings. With these, the model first\ncomputes a variable-length Delta matrix between the query and document,\nrepresenting a difference between the two texts, which is then passed through a\ndeep convolution stage followed by a deep feed-forward network to compute a\nrelevance score. This results in a fast model suitable for use in an online\nsearch engine. The model is robust and outperforms comparable state-of-the-art\ndeep learning approaches. \n\n"}
{"id": "1802.10570", "contents": "Title: Statistical shape analysis in a Bayesian framework for shapes in two and\n  three dimensions Abstract: In this paper, we describe a novel shape classification method which is\nembedded in the Bayesian paradigm. We discuss the modelling and the resulting\nshape classification algorithm for two and three dimensional data shapes. We\nconclude by evaluating the efficiency and efficacy of the proposed algorithm on\nthe Kimia shape database for the two dimensional case. \n\n"}
{"id": "1803.00146", "contents": "Title: A Generic Top-N Recommendation Framework For Trading-off Accuracy,\n  Novelty, and Coverage Abstract: Standard collaborative filtering approaches for top-N recommendation are\nbiased toward popular items. As a result, they recommend items that users are\nlikely aware of and under-represent long-tail items. This is inadequate, both\nfor consumers who prefer novel items and because concentrating on popular items\npoorly covers the item space, whereas high item space coverage increases\nproviders' revenue.\n  We present an approach that relies on historical rating data to learn user\nlong-tail novelty preferences. We integrate these preferences into a generic\nre-ranking framework that customizes balance between accuracy and coverage. We\nempirically validate that our proposedframework increases the novelty of\nrecommendations. Furthermore, by promoting long-tail items to the right group\nof users, we significantly increase the system's coverage while scalably\nmaintaining accuracy. Our framework also enables personalization of existing\nnon-personalized algorithms, making them competitive with existing personalized\nalgorithms in key performance metrics, including accuracy and coverage. \n\n"}
{"id": "1803.00212", "contents": "Title: prDeep: Robust Phase Retrieval with a Flexible Deep Network Abstract: Phase retrieval algorithms have become an important component in many modern\ncomputational imaging systems. For instance, in the context of ptychography and\nspeckle correlation imaging, they enable imaging past the diffraction limit and\nthrough scattering media, respectively. Unfortunately, traditional phase\nretrieval algorithms struggle in the presence of noise. Progress has been made\nrecently on more robust algorithms using signal priors, but at the expense of\nlimiting the range of supported measurement models (e.g., to Gaussian or coded\ndiffraction patterns). In this work we leverage the regularization-by-denoising\nframework and a convolutional neural network denoiser to create prDeep, a new\nphase retrieval algorithm that is both robust and broadly applicable. We test\nand validate prDeep in simulation to demonstrate that it is robust to noise and\ncan handle a variety of system models.\n  A MatConvNet implementation of prDeep is available at\nhttps://github.com/ricedsp/prDeep. \n\n"}
{"id": "1803.00693", "contents": "Title: Accelerating E-Commerce Search Engine Ranking by Contextual Factor\n  Selection Abstract: In industrial large-scale search systems, such as Taobao.com search for\ncommodities, the quality of the ranking result is getting continually improved\nby introducing more factors from complex procedures, e.g., deep neural networks\nfor extracting image factors. Meanwhile, the increasing of the factors demands\nmore computation resource and raises the system response latency. It has been\nobserved that a search instance usually requires only a small set of effective\nfactors, instead of all factors. Therefore, removing ineffective factors\nsignificantly improves the system efficiency. This paper studies the\n\\emph{Contextual Factor Selection} (CFS), which selects only a subset of\neffective factors for every search instance, for a well balance between the\nsearch quality and the response latency. We inject CFS into the search engine\nranking score to accelerate the engine, considering both ranking effectiveness\nand efficiency. The learning of the CFS model involves a combinatorial\noptimization, which is transformed as a sequential decision-making problem.\nSolving the problem by reinforcement learning, we propose the RankCFS, which\nhas been assessed in an off-line environment as well as a real-world on-line\nenvironment (Taobao.com). The empirical results show that, the proposed CFS\napproach outperforms several existing supervised/unsupervised methods for\nfeature selection in the off-line environment, and also achieves significant\nreal-world performance improvement, in term of service latency, in daily test\nas well as Singles' Day Shopping Festival in $2017$. \n\n"}
{"id": "1803.00841", "contents": "Title: Gradient-based Sampling: An Adaptive Importance Sampling for\n  Least-squares Abstract: In modern data analysis, random sampling is an efficient and widely-used\nstrategy to overcome the computational difficulties brought by large sample\nsize. In previous studies, researchers conducted random sampling which is\naccording to the input data but independent on the response variable, however\nthe response variable may also be informative for sampling. In this paper we\npropose an adaptive sampling called the gradient-based sampling which is\ndependent on both the input data and the output for fast solving of\nleast-square (LS) problems. We draw the data points by random sampling from the\nfull data according to their gradient values. This sampling is computationally\nsaving, since the running time of computing the sampling probabilities is\nreduced to O(nd) where n is the full sample size and d is the dimension of the\ninput. Theoretically, we establish an error bound analysis of the general\nimportance sampling with respect to LS solution from full data. The result\nestablishes an improved performance of the use of our gradient- based sampling.\nSynthetic and real data sets are used to empirically argue that the\ngradient-based sampling has an obvious advantage over existing sampling methods\nfrom two aspects of statistical efficiency and computational saving. \n\n"}
{"id": "1803.01113", "contents": "Title: Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in\n  Distributed SGD Abstract: Distributed Stochastic Gradient Descent (SGD) when run in a synchronous\nmanner, suffers from delays in waiting for the slowest learners (stragglers).\nAsynchronous methods can alleviate stragglers, but cause gradient staleness\nthat can adversely affect convergence. In this work we present a novel\ntheoretical characterization of the speed-up offered by asynchronous methods by\nanalyzing the trade-off between the error in the trained model and the actual\ntraining runtime (wallclock time). The novelty in our work is that our runtime\nanalysis considers random straggler delays, which helps us design and compare\ndistributed SGD algorithms that strike a balance between stragglers and\nstaleness. We also present a new convergence analysis of asynchronous SGD\nvariants without bounded or exponential delay assumptions, and a novel learning\nrate schedule to compensate for gradient staleness. \n\n"}
{"id": "1803.01233", "contents": "Title: Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase\n  Procrustes Flow Abstract: We revisit the inductive matrix completion problem that aims to recover a\nrank-$r$ matrix with ambient dimension $d$ given $n$ features as the side prior\ninformation. The goal is to make use of the known $n$ features to reduce sample\nand computational complexities. We present and analyze a new gradient-based\nnon-convex optimization algorithm that converges to the true underlying matrix\nat a linear rate with sample complexity only linearly depending on $n$ and\nlogarithmically depending on $d$. To the best of our knowledge, all previous\nalgorithms either have a quadratic dependency on the number of features in\nsample complexity or a sub-linear computational convergence rate. In addition,\nwe provide experiments on both synthetic and real world data to demonstrate the\neffectiveness of our proposed algorithm. \n\n"}
{"id": "1803.01347", "contents": "Title: Greedy stochastic algorithms for entropy-regularized optimal transport\n  problems Abstract: Optimal transport (OT) distances are finding evermore applications in machine\nlearning and computer vision, but their wide spread use in larger-scale\nproblems is impeded by their high computational cost. In this work we develop a\nfamily of fast and practical stochastic algorithms for solving the optimal\ntransport problem with an entropic penalization. This work extends the recently\ndeveloped Greenkhorn algorithm, in the sense that, the Greenkhorn algorithm is\na limiting case of this family. We also provide a simple and general\nconvergence theorem for all algorithms in the class, with rates that match the\nbest known rates of Greenkorn and the Sinkhorn algorithm, and conclude with\nnumerical experiments that show under what regime of penalization the new\nstochastic methods are faster than the aforementioned methods. \n\n"}
{"id": "1803.01626", "contents": "Title: Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in\n  MDPs Abstract: The problem of reinforcement learning in an unknown and discrete Markov\nDecision Process (MDP) under the average-reward criterion is considered, when\nthe learner interacts with the system in a single stream of observations,\nstarting from an initial state without any reset. We revisit the minimax lower\nbound for that problem by making appear the local variance of the bias function\nin place of the diameter of the MDP. Furthermore, we provide a novel analysis\nof the KL-UCRL algorithm establishing a high-probability regret bound scaling\nas $\\widetilde {\\mathcal O}\\Bigl({\\textstyle \\sqrt{S\\sum_{s,a}{\\bf\nV}^\\star_{s,a}T}}\\Big)$ for this algorithm for ergodic MDPs, where $S$ denotes\nthe number of states and where ${\\bf V}^\\star_{s,a}$ is the variance of the\nbias function with respect to the next-state distribution following action $a$\nin state $s$. The resulting bound improves upon the best previously known\nregret bound $\\widetilde {\\mathcal O}(DS\\sqrt{AT})$ for that algorithm, where\n$A$ and $D$ respectively denote the maximum number of actions (per state) and\nthe diameter of MDP. We finally compare the leading terms of the two bounds in\nsome benchmark MDPs indicating that the derived bound can provide an order of\nmagnitude improvement in some cases. Our analysis leverages novel variations of\nthe transportation lemma combined with Kullback-Leibler concentration\ninequalities, that we believe to be of independent interest. \n\n"}
{"id": "1803.01833", "contents": "Title: Marginal Singularity, and the Benefits of Labels in Covariate-Shift Abstract: We present new minimax results that concisely capture the relative benefits\nof source and target labeled data, under covariate-shift. Namely, we show that\nthe benefits of target labels are controlled by a transfer-exponent $\\gamma$\nthat encodes how singular Q is locally w.r.t. P, and interestingly allows\nsituations where transfer did not seem possible under previous insights. In\nfact, our new minimax analysis - in terms of $\\gamma$ - reveals a continuum of\nregimes ranging from situations where target labels have little benefit, to\nregimes where target labels dramatically improve classification. We then show\nthat a recently proposed semi-supervised procedure can be extended to adapt to\nunknown $\\gamma$, and therefore requests labels only when beneficial, while\nachieving minimax transfer rates. \n\n"}
{"id": "1803.02349", "contents": "Title: Billion-scale Commodity Embedding for E-commerce Recommendation in\n  Alibaba Abstract: Recommender systems (RSs) have been the most important technology for\nincreasing the business in Taobao, the largest online consumer-to-consumer\n(C2C) platform in China. The billion-scale data in Taobao creates three major\nchallenges to Taobao's RS: scalability, sparsity and cold start. In this paper,\nwe present our technical solutions to address these three challenges. The\nmethods are based on the graph embedding framework. We first construct an item\ngraph from users' behavior history. Each item is then represented as a vector\nusing graph embedding. The item embeddings are employed to compute pairwise\nsimilarities between all items, which are then used in the recommendation\nprocess. To alleviate the sparsity and cold start problems, side information is\nincorporated into the embedding framework. We propose two aggregation methods\nto integrate the embeddings of items and the corresponding side information.\nExperimental results from offline experiments show that methods incorporating\nside information are superior to those that do not. Further, we describe the\nplatform upon which the embedding methods are deployed and the workflow to\nprocess the billion-scale data in Taobao. Using online A/B test, we show that\nthe online Click-Through-Rate (CTRs) are improved comparing to the previous\nrecommendation methods widely used in Taobao, further demonstrating the\neffectiveness and feasibility of our proposed methods in Taobao's live\nproduction environment. \n\n"}
{"id": "1803.02781", "contents": "Title: Fast Dawid-Skene: A Fast Vote Aggregation Scheme for Sentiment\n  Classification Abstract: Many real world problems can now be effectively solved using supervised\nmachine learning. A major roadblock is often the lack of an adequate quantity\nof labeled data for training. A possible solution is to assign the task of\nlabeling data to a crowd, and then infer the true label using aggregation\nmethods. A well-known approach for aggregation is the Dawid-Skene (DS)\nalgorithm, which is based on the principle of Expectation-Maximization (EM). We\npropose a new simple, yet effective, EM-based algorithm, which can be\ninterpreted as a `hard' version of DS, that allows much faster convergence\nwhile maintaining similar accuracy in aggregation. We show the use of this\nalgorithm as a quick and effective technique for online, real-time sentiment\nannotation. We also prove that our algorithm converges to the estimated labels\nat a linear rate. Our experiments on standard datasets show a significant\nspeedup in time taken for aggregation - upto $\\sim$8x over Dawid-Skene and\n$\\sim$6x over other fast EM methods, at competitive accuracy performance. The\ncode for the implementation of the algorithms can be found at\nhttps://github.com/GoodDeeds/Fast-Dawid-Skene \n\n"}
{"id": "1803.02922", "contents": "Title: Fast Convergence for Stochastic and Distributed Gradient Descent in the\n  Interpolation Limit Abstract: Modern supervised learning techniques, particularly those using deep nets,\ninvolve fitting high dimensional labelled data sets with functions containing\nvery large numbers of parameters. Much of this work is empirical. Interesting\nphenomena have been observed that require theoretical explanations; however the\nnon-convexity of the loss functions complicates the analysis. Recently it has\nbeen proposed that the success of these techniques rests partly in the\neffectiveness of the simple stochastic gradient descent algorithm in the so\ncalled interpolation limit in which all labels are fit perfectly. This analysis\nis made possible since the SGD algorithm reduces to a stochastic linear system\nnear the interpolating minimum of the loss function. Here we exploit this\ninsight by presenting and analyzing a new distributed algorithm for gradient\ndescent, also in the interpolating limit. The distributed SGD algorithm\npresented in the paper corresponds to gradient descent applied to a simple\npenalized distributed loss function, $L({\\bf w}_1,...,{\\bf w}_n) = \\Sigma_i\nl_i({\\bf w}_i) + \\mu \\sum_{<i,j>}|{\\bf w}_i-{\\bf w}_j|^2$. Here each node holds\nonly one sample, and its own parameter vector. The notation $<i,j>$ denotes\nedges of a connected graph defining the links between nodes. It is shown that\nthis distributed algorithm converges linearly (ie the error reduces\nexponentially with iteration number), with a rate\n$1-\\frac{\\eta}{n}\\lambda_{min}(H)<R<1$ where $\\lambda_{min}(H)$ is the smallest\nnonzero eigenvalue of the sample covariance or the Hessian H. In contrast with\nprevious usage of similar penalty functions to enforce consensus between nodes,\nin the interpolating limit it is not required to take the penalty parameter to\ninfinity for consensus to occur. The analysis further reinforces the utility of\nthe interpolation limit in the theoretical treatment of modern machine learning\nalgorithms. \n\n"}
{"id": "1803.03234", "contents": "Title: Improving Optimization for Models With Continuous Symmetry Breaking Abstract: Many loss functions in representation learning are invariant under a\ncontinuous symmetry transformation. For example, the loss function of word\nembeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate\nall word and context embedding vectors. We show that representation learning\nmodels for time series possess an approximate continuous symmetry that leads to\nslow convergence of gradient descent. We propose a new optimization algorithm\nthat speeds up convergence using ideas from gauge theory in physics. Our\nalgorithm leads to orders of magnitude faster convergence and to more\ninterpretable representations, as we show for dynamic extensions of matrix\nfactorization and word embedding models. We further present an example\napplication of our proposed algorithm that translates modern words into their\nhistoric equivalents. \n\n"}
{"id": "1803.03467", "contents": "Title: RippleNet: Propagating User Preferences on the Knowledge Graph for\n  Recommender Systems Abstract: To address the sparsity and cold start problem of collaborative filtering,\nresearchers usually make use of side information, such as social networks or\nitem attributes, to improve recommendation performance. This paper considers\nthe knowledge graph as the source of side information. To address the\nlimitations of existing embedding-based and path-based methods for\nknowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end\nframework that naturally incorporates the knowledge graph into recommender\nsystems. Similar to actual ripples propagating on the surface of water, Ripple\nNetwork stimulates the propagation of user preferences over the set of\nknowledge entities by automatically and iteratively extending a user's\npotential interests along links in the knowledge graph. The multiple \"ripples\"\nactivated by a user's historically clicked items are thus superposed to form\nthe preference distribution of the user with respect to a candidate item, which\ncould be used for predicting the final clicking probability. Through extensive\nexperiments on real-world datasets, we demonstrate that Ripple Network achieves\nsubstantial gains in a variety of scenarios, including movie, book and news\nrecommendation, over several state-of-the-art baselines. \n\n"}
{"id": "1803.03759", "contents": "Title: Speech Recognition: Keyword Spotting Through Image Recognition Abstract: The problem of identifying voice commands has always been a challenge due to\nthe presence of noise and variability in speed, pitch, etc. We will compare the\nefficacies of several neural network architectures for the speech recognition\nproblem. In particular, we will build a model to determine whether a one second\naudio clip contains a particular word (out of a set of 10), an unknown word, or\nsilence. The models to be implemented are a CNN recommended by the Tensorflow\nSpeech Recognition tutorial, a low-latency CNN, and an adversarially trained\nCNN. The result is a demonstration of how to convert a problem in audio\nrecognition to the better-studied domain of image classification, where the\npowerful techniques of convolutional neural networks are fully developed.\nAdditionally, we demonstrate the applicability of the technique of Virtual\nAdversarial Training (VAT) to this problem domain, functioning as a powerful\nregularizer with promising potential future applications. \n\n"}
{"id": "1803.03877", "contents": "Title: On dynamic ensemble selection and data preprocessing for multi-class\n  imbalance learning Abstract: Class-imbalance refers to classification problems in which many more\ninstances are available for certain classes than for others. Such imbalanced\ndatasets require special attention because traditional classifiers generally\nfavor the majority class which has a large number of instances. Ensemble of\nclassifiers have been reported to yield promising results. However, the\nmajority of ensemble methods applied too imbalanced learning are static ones.\nMoreover, they only deal with binary imbalanced problems. Hence, this paper\npresents an empirical analysis of dynamic selection techniques and data\npreprocessing methods for dealing with multi-class imbalanced problems. We\nconsidered five variations of preprocessing methods and four dynamic selection\nmethods. Our experiments conducted on 26 multi-class imbalanced problems show\nthat the dynamic ensemble improves the F-measure and the G-mean as compared to\nthe static ensemble. Moreover, data preprocessing plays an important role in\nsuch cases. \n\n"}
{"id": "1803.03919", "contents": "Title: Detecting Nonlinear Causality in Multivariate Time Series with Sparse\n  Additive Models Abstract: We propose a nonparametric method for detecting nonlinear causal relationship\nwithin a set of multidimensional discrete time series, by using sparse additive\nmodels (SpAMs). We show that, when the input to the SpAM is a $\\beta$-mixing\ntime series, the model can be fitted by first approximating each unknown\nfunction with a linear combination of a set of B-spline bases, and then solving\na group-lasso-type optimization problem with nonconvex regularization.\nTheoretically, we characterize the oracle statistical properties of the\nproposed sparse estimator in function estimation and model selection.\nNumerically, we propose an efficient pathwise iterative shrinkage thresholding\nalgorithm (PISTA), which tames the nonconvexity and guarantees linear\nconvergence towards the desired sparse estimator with high probability. \n\n"}
{"id": "1803.04371", "contents": "Title: Optimal Rates of Sketched-regularized Algorithms for Least-Squares\n  Regression over Hilbert Spaces Abstract: We investigate regularized algorithms combining with projection for\nleast-squares regression problem over a Hilbert space, covering nonparametric\nregression over a reproducing kernel Hilbert space. We prove convergence\nresults with respect to variants of norms, under a capacity assumption on the\nhypothesis space and a regularity condition on the target function. As a\nresult, we obtain optimal rates for regularized algorithms with randomized\nsketches, provided that the sketch dimension is proportional to the effective\ndimension up to a logarithmic factor. As a byproduct, we obtain similar results\nfor Nystr\\\"{o}m regularized algorithms. Our results are the first ones with\noptimal, distribution-dependent rates that do not have any saturation effect\nfor sketched/Nystr\\\"{o}m regularized algorithms, considering both the\nattainable and non-attainable cases. \n\n"}
{"id": "1803.04665", "contents": "Title: Pure Exploration in Infinitely-Armed Bandit Models with Fixed-Confidence Abstract: We consider the problem of near-optimal arm identification in the fixed\nconfidence setting of the infinitely armed bandit problem when nothing is known\nabout the arm reservoir distribution. We (1) introduce a PAC-like framework\nwithin which to derive and cast results; (2) derive a sample complexity lower\nbound for near-optimal arm identification; (3) propose an algorithm that\nidentifies a nearly-optimal arm with high probability and derive an upper bound\non its sample complexity which is within a log factor of our lower bound; and\n(4) discuss whether our log^2(1/delta) dependence is inescapable for\n\"two-phase\" (select arms first, identify the best later) algorithms in the\ninfinite setting. This work permits the application of bandit models to a\nbroader class of problems where fewer assumptions hold. \n\n"}
{"id": "1803.05170", "contents": "Title: xDeepFM: Combining Explicit and Implicit Feature Interactions for\n  Recommender Systems Abstract: Combinatorial features are essential for the success of many commercial\nmodels. Manually crafting these features usually comes with high cost due to\nthe variety, volume and velocity of raw data in web-scale systems.\nFactorization based models, which measure interactions in terms of vector\nproduct, can learn patterns of combinatorial features automatically and\ngeneralize to unseen features as well. With the great success of deep neural\nnetworks (DNNs) in various fields, recently researchers have proposed several\nDNN-based factorization model to learn both low- and high-order feature\ninteractions. Despite the powerful ability of learning an arbitrary function\nfrom data, plain DNNs generate feature interactions implicitly and at the\nbit-wise level. In this paper, we propose a novel Compressed Interaction\nNetwork (CIN), which aims to generate feature interactions in an explicit\nfashion and at the vector-wise level. We show that the CIN share some\nfunctionalities with convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs). We further combine a CIN and a classical DNN into one unified\nmodel, and named this new model eXtreme Deep Factorization Machine (xDeepFM).\nOn one hand, the xDeepFM is able to learn certain bounded-degree feature\ninteractions explicitly; on the other hand, it can learn arbitrary low- and\nhigh-order feature interactions implicitly. We conduct comprehensive\nexperiments on three real-world datasets. Our results demonstrate that xDeepFM\noutperforms state-of-the-art models. We have released the source code of\nxDeepFM at \\url{https://github.com/Leavingseason/xDeepFM}. \n\n"}
{"id": "1803.05401", "contents": "Title: Approximate Query Matching for Image Retrieval Abstract: Traditional image recognition involves identifying the key object in a\nportrait-type image with a single object focus (ILSVRC, AlexNet, and VGG). More\nrecent approaches consider dense image recognition - segmenting an image with\nappropriate bounding boxes and performing image recognition within these\nbounding boxes (Semantic segmentation). The Visual Genome dataset [5] is an\nattempt to bridge these various approaches to a cohesive dataset for each\nsubtask - bounding box generation, image recognition, captioning, and a new\noperation: scene graph generation. Our focus is on using such scene graphs to\nperform graph search on image databases to holistically retrieve images based\non a search criteria. We develop a method to store scene graphs and metadata in\ngraph databases (using Neo4J) and to perform fast approximate retrieval of\nimages based on a graph search query. We process more complex queries than\nsingle object search, e.g. \"girl eating cake\" retrieves images that contain the\nspecified relation as well as variations. \n\n"}
{"id": "1803.05457", "contents": "Title: Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\n  Challenge Abstract: We present a new question set, text corpus, and baselines assembled to\nencourage AI research in advanced question answering. Together, these\nconstitute the AI2 Reasoning Challenge (ARC), which requires far more powerful\nknowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC\nquestion set is partitioned into a Challenge Set and an Easy Set, where the\nChallenge Set contains only questions answered incorrectly by both a\nretrieval-based algorithm and a word co-occurence algorithm. The dataset\ncontains only natural, grade-school science questions (authored for human\ntests), and is the largest public-domain set of this kind (7,787 questions). We\ntest several baselines on the Challenge Set, including leading neural models\nfrom the SQuAD and SNLI tasks, and find that none are able to significantly\noutperform a random baseline, reflecting the difficult nature of this task. We\nare also releasing the ARC Corpus, a corpus of 14M science sentences relevant\nto the task, and implementations of the three neural baseline models tested.\nCan your model perform better? We pose ARC as a challenge to the community. \n\n"}
{"id": "1803.05776", "contents": "Title: Gaussian Processes Over Graphs Abstract: We propose Gaussian processes for signals over graphs (GPG) using the apriori\nknowledge that the target vectors lie over a graph. We incorporate this\ninformation using a graph- Laplacian based regularization which enforces the\ntarget vectors to have a specific profile in terms of graph Fourier transform\ncoeffcients, for example lowpass or bandpass graph signals. We discuss how the\nregularization affects the mean and the variance in the prediction output. In\nparticular, we prove that the predictive variance of the GPG is strictly\nsmaller than the conventional Gaussian process (GP) for any non-trivial graph.\nWe validate our concepts by application to various real-world graph signals.\nOur experiments show that the performance of the GPG is superior to GP for\nsmall training data sizes and under noisy training. \n\n"}
{"id": "1803.06518", "contents": "Title: Provable Convex Co-clustering of Tensors Abstract: Cluster analysis is a fundamental tool for pattern discovery of complex\nheterogeneous data. Prevalent clustering methods mainly focus on vector or\nmatrix-variate data and are not applicable to general-order tensors, which\narise frequently in modern scientific and business applications. Moreover,\nthere is a gap between statistical guarantees and computational efficiency for\nexisting tensor clustering solutions due to the nature of their non-convex\nformulations. In this work, we bridge this gap by developing a provable convex\nformulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator\nenjoys stability guarantees and its computational and storage costs are\npolynomial in the size of the data. We further establish a non-asymptotic error\nbound for the CoCo estimator, which reveals a surprising \"blessing of\ndimensionality\" phenomenon that does not exist in vector or matrix-variate\ncluster analysis. Our theoretical findings are supported by extensive simulated\nstudies. Finally, we apply the CoCo estimator to the cluster analysis of\nadvertisement click tensor data from a major online company. Our clustering\nresults provide meaningful business insights to improve advertising\neffectiveness. \n\n"}
{"id": "1803.07347", "contents": "Title: Optimizing Sponsored Search Ranking Strategy by Deep Reinforcement\n  Learning Abstract: Sponsored search is an indispensable business model and a major revenue\ncontributor of almost all the search engines. From the advertisers' side,\nparticipating in ranking the search results by paying for the sponsored search\nadvertisement to attract more awareness and purchase facilitates their\ncommercial goal. From the users' side, presenting personalized advertisement\nreflecting their propensity would make their online search experience more\nsatisfactory. Sponsored search platforms rank the advertisements by a ranking\nfunction to determine the list of advertisements to show and the charging price\nfor the advertisers. Hence, it is crucial to find a good ranking function which\ncan simultaneously satisfy the platform, the users and the advertisers.\nMoreover, advertisements showing positions under different queries from\ndifferent users may associate with advertisement candidates of different bid\nprice distributions and click probability distributions, which requires the\nranking functions to be optimized adaptively to the traffic characteristics. In\nthis work, we proposed a generic framework to optimize the ranking functions by\ndeep reinforcement learning methods. The framework is composed of two parts: an\noffline learning part which initializes the ranking functions by learning from\na simulated advertising environment, allowing adequate exploration of the\nranking function parameter space without hurting the performance of the\ncommercial platform. An online learning part which further optimizes the\nranking functions by adapting to the online data distribution. Experimental\nresults on a large-scale sponsored search platform confirm the effectiveness of\nthe proposed method. \n\n"}
{"id": "1803.07868", "contents": "Title: Scalable Generalized Dynamic Topic Models Abstract: Dynamic topic models (DTMs) model the evolution of prevalent themes in\nliterature, online media, and other forms of text over time. DTMs assume that\nword co-occurrence statistics change continuously and therefore impose\ncontinuous stochastic process priors on their model parameters. These dynamical\npriors make inference much harder than in regular topic models, and also limit\nscalability. In this paper, we present several new results around DTMs. First,\nwe extend the class of tractable priors from Wiener processes to the generic\nclass of Gaussian processes (GPs). This allows us to explore topics that\ndevelop smoothly over time, that have a long-term memory or are temporally\nconcentrated (for event detection). Second, we show how to perform scalable\napproximate inference in these models based on ideas around stochastic\nvariational inference and sparse Gaussian processes. This way we can train a\nrich family of DTMs to massive data. Our experiments on several large-scale\ndatasets show that our generalized model allows us to find interesting patterns\nthat were not accessible by previous approaches. \n\n"}
{"id": "1803.08000", "contents": "Title: Boosting Random Forests to Reduce Bias; One-Step Boosted Forest and its\n  Variance Estimate Abstract: In this paper we propose using the principle of boosting to reduce the bias\nof a random forest prediction in the regression setting. From the original\nrandom forest fit we extract the residuals and then fit another random forest\nto these residuals. We call the sum of these two random forests a\n\\textit{one-step boosted forest}. We show with simulated and real data that the\none-step boosted forest has a reduced bias compared to the original random\nforest. The paper also provides a variance estimate of the one-step boosted\nforest by an extension of the infinitesimal Jackknife estimator. Using this\nvariance estimate we can construct prediction intervals for the boosted forest\nand we show that they have good coverage probabilities. Combining the bias\nreduction and the variance estimate we show that the one-step boosted forest\nhas a significant reduction in predictive mean squared error and thus an\nimprovement in predictive performance. When applied on datasets from the UCI\ndatabase, one-step boosted forest performs better than random forest and\ngradient boosting machine algorithms. Theoretically we can also extend such a\nboosting process to more than one step and the same principles outlined in this\npaper can be used to find variance estimates for such predictors. Such boosting\nwill reduce bias even further but it risks over-fitting and also increases the\ncomputational burden. \n\n"}
{"id": "1803.08612", "contents": "Title: Evaluating How Developers Use General-Purpose Web-Search for Code\n  Retrieval Abstract: Search is an integral part of a software development process. Developers\noften use search engines to look for information during development, including\nreusable code snippets, API understanding, and reference examples. Developers\ntend to prefer general-purpose search engines like Google, which are often not\noptimized for code related documents and use search strategies and ranking\ntechniques that are more optimized for generic, non-code related information.\nIn this paper, we explore whether a general purpose search engine like Google\nis an optimal choice for code-related searches. In particular, we investigate\nwhether the performance of searching with Google varies for code vs. non-code\nrelated searches. To analyze this, we collect search logs from 310 developers\nthat contains nearly 150,000 search queries from Google and the associated\nresult clicks. To differentiate between code-related searches and non-code\nrelated searches, we build a model which identifies the code intent of queries.\nLeveraging this model, we build an automatic classifier that detects a code and\nnon-code related query. We confirm the effectiveness of the classifier on\nmanually annotated queries where the classifier achieves a precision of 87%, a\nrecall of 86%, and an F1-score of 87%. We apply this classifier to\nautomatically annotate all the queries in the dataset. Analyzing this dataset,\nwe observe that code related searching often requires more effort (e.g., time,\nresult clicks, and query modifications) than general non-code search, which\nindicates code search performance with a general search engine is less\neffective. \n\n"}
{"id": "1803.08882", "contents": "Title: Trace your sources in large-scale data: one ring to find them all Abstract: An important preprocessing step in most data analysis pipelines aims to\nextract a small set of sources that explain most of the data. Currently used\nalgorithms for blind source separation (BSS), however, often fail to extract\nthe desired sources and need extensive cross-validation. In contrast, their\nrarely used probabilistic counterparts can get away with little\ncross-validation and are more accurate and reliable but no simple and scalable\nimplementations are available. Here we present a novel probabilistic BSS\nframework (DECOMPOSE) that can be flexibly adjusted to the data, is extensible\nand easy to use, adapts to individual sources and handles large-scale data\nthrough algorithmic efficiency. DECOMPOSE encompasses and generalises many\ntraditional BSS algorithms such as PCA, ICA and NMF and we demonstrate\nsubstantial improvements in accuracy and robustness on artificial and real\ndata. \n\n"}
{"id": "1803.08988", "contents": "Title: Evaluating Sentence-Level Relevance Feedback for High-Recall Information\n  Retrieval Abstract: This study uses a novel simulation framework to evaluate whether the time and\neffort necessary to achieve high recall using active learning is reduced by\npresenting the reviewer with isolated sentences, as opposed to full documents,\nfor relevance feedback. Under the weak assumption that more time and effort is\nrequired to review an entire document than a single sentence, simulation\nresults indicate that the use of isolated sentences for relevance feedback can\nyield comparable accuracy and higher efficiency, relative to the\nstate-of-the-art Baseline Model Implementation (BMI) of the AutoTAR Continuous\nActive Learning (\"CAL\") method employed in the TREC 2015 and 2016 Total Recall\nTrack. \n\n"}
{"id": "1803.09010", "contents": "Title: Datasheets for Datasets Abstract: The machine learning community currently has no standardized process for\ndocumenting datasets, which can lead to severe consequences in high-stakes\ndomains. To address this gap, we propose datasheets for datasets. In the\nelectronics industry, every component, no matter how simple or complex, is\naccompanied with a datasheet that describes its operating characteristics, test\nresults, recommended uses, and other information. By analogy, we propose that\nevery dataset be accompanied with a datasheet that documents its motivation,\ncomposition, collection process, recommended uses, and so on. Datasheets for\ndatasets will facilitate better communication between dataset creators and\ndataset consumers, and encourage the machine learning community to prioritize\ntransparency and accountability. \n\n"}
{"id": "1803.09119", "contents": "Title: Gradient descent in Gaussian random fields as a toy model for\n  high-dimensional optimisation in deep learning Abstract: In this paper we model the loss function of high-dimensional optimization\nproblems by a Gaussian random field, or equivalently a Gaussian process. Our\naim is to study gradient descent in such loss functions or energy landscapes\nand compare it to results obtained from real high-dimensional optimization\nproblems such as encountered in deep learning. In particular, we analyze the\ndistribution of the improved loss function after a step of gradient descent,\nprovide analytic expressions for the moments as well as prove asymptotic\nnormality as the dimension of the parameter space becomes large. Moreover, we\ncompare this with the expectation of the global minimum of the landscape\nobtained by means of the Euler characteristic of excursion sets. Besides\ncomplementing our analytical findings with numerical results from simulated\nGaussian random fields, we also compare it to loss functions obtained from\noptimisation problems on synthetic and real data sets by proposing a \"black\nbox\" random field toy-model for a deep neural network loss function. \n\n"}
{"id": "1803.09383", "contents": "Title: Online Second Order Methods for Non-Convex Stochastic Optimizations Abstract: This paper proposes a family of online second order methods for possibly\nnon-convex stochastic optimizations based on the theory of preconditioned\nstochastic gradient descent (PSGD), which can be regarded as an enhance\nstochastic Newton method with the ability to handle gradient noise and\nnon-convexity simultaneously. We have improved the implementations of the\noriginal PSGD in several ways, e.g., new forms of preconditioners, more\naccurate Hessian vector product calculations, and better numerical stability\nwith vanishing or ill-conditioned Hessian, etc.. We also have unrevealed the\nrelationship between feature normalization and PSGD with Kronecker product\npreconditioners, which explains the excellent performance of Kronecker product\npreconditioners in deep neural network learning. A software package\n(https://github.com/lixilinx/psgd_tf) implemented in Tensorflow is provided to\ncompare variations of stochastic gradient descent (SGD) and PSGD with five\ndifferent preconditioners on a wide range of benchmark problems with commonly\nused neural network architectures, e.g., convolutional and recurrent neural\nnetworks. Experimental results clearly demonstrate the advantages of PSGD in\nterms of generalization performance and convergence speed. \n\n"}
{"id": "1803.09587", "contents": "Title: Evaluation of Session-based Recommendation Algorithms Abstract: Recommender systems help users find relevant items of interest, for example\non e-commerce or media streaming sites. Most academic research is concerned\nwith approaches that personalize the recommendations according to long-term\nuser profiles. In many real-world applications, however, such long-term\nprofiles often do not exist and recommendations therefore have to be made\nsolely based on the observed behavior of a user during an ongoing session.\nGiven the high practical relevance of the problem, an increased interest in\nthis problem can be observed in recent years, leading to a number of proposals\nfor session-based recommendation algorithms that typically aim to predict the\nuser's immediate next actions. In this work, we present the results of an\nin-depth performance comparison of a number of such algorithms, using a variety\nof datasets and evaluation measures. Our comparison includes the most recent\napproaches based on recurrent neural networks like GRU4REC, factorized Markov\nmodel approaches such as FISM or FOSSIL, as well as simpler methods based,\ne.g., on nearest neighbor schemes. Our experiments reveal that algorithms of\nthis latter class, despite their sometimes almost trivial nature, often perform\nequally well or significantly better than today's more complex approaches based\non deep neural networks. Our results therefore suggest that there is\nsubstantial room for improvement regarding the development of more\nsophisticated session-based recommendation algorithms. \n\n"}
{"id": "1803.10840", "contents": "Title: Defending against Adversarial Images using Basis Functions\n  Transformations Abstract: We study the effectiveness of various approaches that defend against\nadversarial attacks on deep networks via manipulations based on basis function\nrepresentations of images. Specifically, we experiment with low-pass filtering,\nPCA, JPEG compression, low resolution wavelet approximation, and\nsoft-thresholding. We evaluate these defense techniques using three types of\npopular attacks in black, gray and white-box settings. Our results show JPEG\ncompression tends to outperform the other tested defenses in most of the\nsettings considered, in addition to soft-thresholding, which performs well in\nspecific cases, and yields a more mild decrease in accuracy on benign examples.\nIn addition, we also mathematically derive a novel white-box attack in which\nthe adversarial perturbation is composed only of terms corresponding a to\npre-determined subset of the basis functions, of which a \"low frequency attack\"\nis a special case. \n\n"}
{"id": "1803.11266", "contents": "Title: Performance evaluation and hyperparameter tuning of statistical and\n  machine-learning models using spatial data Abstract: Machine-learning algorithms have gained popularity in recent years in the\nfield of ecological modeling due to their promising results in predictive\nperformance of classification problems. While the application of such\nalgorithms has been highly simplified in the last years due to their\nwell-documented integration in commonly used statistical programming languages\nsuch as R, there are several practical challenges in the field of ecological\nmodeling related to unbiased performance estimation, optimization of algorithms\nusing hyperparameter tuning and spatial autocorrelation. We address these\nissues in the comparison of several widely used machine-learning algorithms\nsuch as Boosted Regression Trees (BRT), k-Nearest Neighbor (WKNN), Random\nForest (RF) and Support Vector Machine (SVM) to traditional parametric\nalgorithms such as logistic regression (GLM) and semi-parametric ones like\ngeneralized additive models (GAM). Different nested cross-validation methods\nincluding hyperparameter tuning methods are used to evaluate model performances\nwith the aim to receive bias-reduced performance estimates. As a case study the\nspatial distribution of forest disease Diplodia sapinea in the Basque Country\nin Spain is investigated using common environmental variables such as\ntemperature, precipitation, soil or lithology as predictors. Results show that\nGAM and RF (mean AUROC estimates 0.708 and 0.699) outperform all other methods\nin predictive accuracy. The effect of hyperparameter tuning saturates at around\n50 iterations for this data set. The AUROC differences between the bias-reduced\n(spatial cross-validation) and overoptimistic (non-spatial cross-validation)\nperformance estimates of the GAM and RF are 0.167 (24%) and 0.213 (30%),\nrespectively. It is recommended to also use spatial partitioning for\ncross-validation hyperparameter tuning of spatial data. \n\n"}
{"id": "1804.00104", "contents": "Title: Learning Disentangled Joint Continuous and Discrete Representations Abstract: We present a framework for learning disentangled and interpretable jointly\ncontinuous and discrete representations in an unsupervised manner. By\naugmenting the continuous latent distribution of variational autoencoders with\na relaxed discrete distribution and controlling the amount of information\nencoded in each latent unit, we show how continuous and categorical factors of\nvariation can be discovered automatically from data. Experiments show that the\nframework disentangles continuous and discrete generative factors on various\ndatasets and outperforms current disentangling methods when a discrete\ngenerative factor is prominent. \n\n"}
{"id": "1804.00341", "contents": "Title: Sparse Principal Component Analysis via Variable Projection Abstract: Sparse principal component analysis (SPCA) has emerged as a powerful\ntechnique for modern data analysis, providing improved interpretation of\nlow-rank structures by identifying localized spatial structures in the data and\ndisambiguating between distinct time scales. We demonstrate a robust and\nscalable SPCA algorithm by formulating it as a value-function optimization\nproblem. This viewpoint leads to a flexible and computationally efficient\nalgorithm. Further, we can leverage randomized methods from linear algebra to\nextend the approach to the large-scale (big data) setting. Our proposed\ninnovation also allows for a robust SPCA formulation which obtains meaningful\nsparse principal components in spite of grossly corrupted input data. The\nproposed algorithms are demonstrated using both synthetic and real world data,\nand show exceptional computational efficiency and diagnostic performance. \n\n"}
{"id": "1804.00795", "contents": "Title: Estimation of Markov Chain via Rank-Constrained Likelihood Abstract: This paper studies the estimation of low-rank Markov chains from empirical\ntrajectories. We propose a non-convex estimator based on rank-constrained\nlikelihood maximization. Statistical upper bounds are provided for the\nKullback-Leiber divergence and the $\\ell_2$ risk between the estimator and the\ntrue transition matrix. The estimator reveals a compressed state space of the\nMarkov chain. We also develop a novel DC (difference of convex function)\nprogramming algorithm to tackle the rank-constrained non-smooth optimization\nproblem. Convergence results are established. Experiments show that the\nproposed estimator achieves better empirical performance than other popular\napproaches. \n\n"}
{"id": "1804.03273", "contents": "Title: On the Supermodularity of Active Graph-based Semi-supervised Learning\n  with Stieltjes Matrix Regularization Abstract: Active graph-based semi-supervised learning (AG-SSL) aims to select a small\nset of labeled examples and utilize their graph-based relation to other\nunlabeled examples to aid in machine learning tasks. It is also closely related\nto the sampling theory in graph signal processing. In this paper, we revisit\nthe original formulation of graph-based SSL and prove the supermodularity of an\nAG-SSL objective function under a broad class of regularization functions\nparameterized by Stieltjes matrices. Under this setting, supermodularity yields\na novel greedy label sampling algorithm with guaranteed performance relative to\nthe optimal sampling set. Compared to three state-of-the-art graph signal\nsampling and recovery methods on two real-life community detection datasets,\nthe proposed AG-SSL method attains superior classification accuracy given\nlimited sample budgets. \n\n"}
{"id": "1804.03608", "contents": "Title: Imagine This! Scripts to Compositions to Videos Abstract: Imagining a scene described in natural language with realistic layout and\nappearance of entities is the ultimate test of spatial, visual, and semantic\nworld knowledge. Towards this goal, we present the Composition, Retrieval, and\nFusion Network (CRAFT), a model capable of learning this knowledge from\nvideo-caption data and applying it while generating videos from novel captions.\nCRAFT explicitly predicts a temporal-layout of mentioned entities (characters\nand objects), retrieves spatio-temporal entity segments from a video database\nand fuses them to generate scene videos. Our contributions include sequential\ntraining of components of CRAFT while jointly modeling layout and appearances,\nand losses that encourage learning compositional representations for retrieval.\nWe evaluate CRAFT on semantic fidelity to caption, composition consistency, and\nvisual quality. CRAFT outperforms direct pixel generation approaches and\ngeneralizes well to unseen captions and to unseen video databases with no text\nannotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated\nvideo-caption dataset with over 25000 videos. For a glimpse of videos generated\nby CRAFT, see https://youtu.be/688Vv86n0z8. \n\n"}
{"id": "1804.04171", "contents": "Title: KS(conf ): A Light-Weight Test if a ConvNet Operates Outside of Its\n  Specifications Abstract: Computer vision systems for automatic image categorization have become\naccurate and reliable enough that they can run continuously for days or even\nyears as components of real-world commercial applications. A major open problem\nin this context, however, is quality control. Good classification performance\ncan only be expected if systems run under the specific conditions, in\nparticular data distributions, that they were trained for. Surprisingly, none\nof the currently used deep network architectures has a built-in functionality\nthat could detect if a network operates on data from a distribution that it was\nnot trained for and potentially trigger a warning to the human users. In this\nwork, we describe KS(conf), a procedure for detecting such outside of the\nspecifications operation. Building on statistical insights, its main step is\nthe applications of a classical Kolmogorov-Smirnov test to the distribution of\npredicted confidence values. We show by extensive experiments using ImageNet,\nAwA2 and DAVIS data on a variety of ConvNets architectures that KS(conf)\nreliably detects out-of-specs situations. It furthermore has a number of\nproperties that make it an excellent candidate for practical deployment: it is\neasy to implement, adds almost no overhead to the system, works with all\nnetworks, including pretrained ones, and requires no a priori knowledge about\nhow the data distribution could change. \n\n"}
{"id": "1804.04205", "contents": "Title: Learning Topics using Semantic Locality Abstract: The topic modeling discovers the latent topic probability of the given text\ndocuments. To generate the more meaningful topic that better represents the\ngiven document, we proposed a new feature extraction technique which can be\nused in the data preprocessing stage. The method consists of three steps.\nFirst, it generates the word/word-pair from every single document. Second, it\napplies a two-way TF-IDF algorithm to word/word-pair for semantic filtering.\nThird, it uses the K-means algorithm to merge the word pairs that have the\nsimilar semantic meaning.\n  Experiments are carried out on the Open Movie Database (OMDb), Reuters\nDataset and 20NewsGroup Dataset. The mean Average Precision score is used as\nthe evaluation metric. Comparing our results with other state-of-the-art topic\nmodels, such as Latent Dirichlet allocation and traditional Restricted\nBoltzmann Machines. Our proposed data preprocessing can improve the generated\ntopic accuracy by up to 12.99\\%. \n\n"}
{"id": "1804.04212", "contents": "Title: Word2Vec applied to Recommendation: Hyperparameters Matter Abstract: Skip-gram with negative sampling, a popular variant of Word2vec originally\ndesigned and tuned to create word embeddings for Natural Language Processing,\nhas been used to create item embeddings with successful applications in\nrecommendation. While these fields do not share the same type of data, neither\nevaluate on the same tasks, recommendation applications tend to use the same\nalready tuned hyperparameters values, even if optimal hyperparameters values\nare often known to be data and task dependent. We thus investigate the marginal\nimportance of each hyperparameter in a recommendation setting through large\nhyperparameter grid searches on various datasets. Results reveal that\noptimizing neglected hyperparameters, namely negative sampling distribution,\nnumber of epochs, subsampling parameter and window-size, significantly improves\nperformance on a recommendation task, and can increase it by an order of\nmagnitude. Importantly, we find that optimal hyperparameters configurations for\nNatural Language Processing tasks and Recommendation tasks are noticeably\ndifferent. \n\n"}
{"id": "1804.04266", "contents": "Title: A Capsule Network-based Embedding Model for Search Personalization Abstract: Search personalization aims to tailor search results to each specific user\nbased on the user's personal interests and preferences (i.e., the user\nprofile). Recent research approaches to search personalization by modelling the\npotential 3-way relationship between the submitted query, the user and the\nsearch results (i.e., documents). That relationship is then used to personalize\nthe search results to that user. In this paper, we introduce a novel embedding\nmodel based on capsule network, which recently is a breakthrough in deep\nlearning, to model the 3-way relationships for search personalization. In the\nmodel, each user (submitted query or returned document) is embedded by a vector\nin the same vector space. The 3-way relationship is described as a triple of\n(query, user, document) which is then modeled as a 3-column matrix containing\nthe three embedding vectors. After that, the 3-column matrix is fed into a deep\nlearning architecture to re-rank the search results returned by a basis ranker.\nExperimental results on query logs from a commercial web search engine show\nthat our model achieves better performances than the basis ranker as well as\nstrong search personalization baselines. \n\n"}
{"id": "1804.04440", "contents": "Title: Temporal Interpolation via Motion Field Prediction Abstract: Navigated 2D multi-slice dynamic Magnetic Resonance (MR) imaging enables high\ncontrast 4D MR imaging during free breathing and provides in-vivo observations\nfor treatment planning and guidance. Navigator slices are vital for\nretrospective stacking of 2D data slices in this method. However, they also\nprolong the acquisition sessions. Temporal interpolation of navigator slices an\nbe used to reduce the number of navigator acquisitions without degrading\nspecificity in stacking. In this work, we propose a convolutional neural\nnetwork (CNN) based method for temporal interpolation via motion field\nprediction. The proposed formulation incorporates the prior knowledge that a\nmotion field underlies changes in the image intensities over time. Previous\napproaches that interpolate directly in the intensity space are prone to\nproduce blurry images or even remove structures in the images. Our method\navoids such problems and faithfully preserves the information in the image.\nFurther, an important advantage of our formulation is that it provides an\nunsupervised estimation of bi-directional motion fields. We show that these\nmotion fields can be used to halve the number of registrations required during\n4D reconstruction, thus substantially reducing the reconstruction time. \n\n"}
{"id": "1804.04448", "contents": "Title: Adversarial Alignment of Class Prediction Uncertainties for Domain\n  Adaptation Abstract: We consider unsupervised domain adaptation: given labelled examples from a\nsource domain and unlabelled examples from a related target domain, the goal is\nto infer the labels of target examples. Under the assumption that features from\npre-trained deep neural networks are transferable across related domains,\ndomain adaptation reduces to aligning source and target domain at class\nprediction uncertainty level. We tackle this problem by introducing a method\nbased on adversarial learning which forces the label uncertainty predictions on\nthe target domain to be indistinguishable from those on the source domain.\nPre-trained deep neural networks are used to generate deep features having high\ntransferability across related domains. We perform an extensive experimental\nanalysis of the proposed method over a wide set of publicly available\npre-trained deep neural networks. Results of our experiments on domain\nadaptation tasks for image classification show that class prediction\nuncertainty alignment with features extracted from pre-trained deep neural\nnetworks provides an efficient, robust and effective method for domain\nadaptation. \n\n"}
{"id": "1804.04791", "contents": "Title: Fast, Parameter free Outlier Identification for Robust PCA Abstract: Robust PCA, the problem of PCA in the presence of outliers has been\nextensively investigated in the last few years. Here we focus on Robust PCA in\nthe column sparse outlier model. The existing methods for column sparse outlier\nmodel assumes either the knowledge of the dimension of the lower dimensional\nsubspace or the fraction of outliers in the system. However in many\napplications knowledge of these parameters is not available. Motivated by this\nwe propose a parameter free outlier identification method for robust PCA which\na) does not require the knowledge of outlier fraction, b) does not require the\nknowledge of the dimension of the underlying subspace, c) is computationally\nsimple and fast. Further, analytical guarantees are derived for outlier\nidentification and the performance of the algorithm is compared with the\nexisting state of the art methods. \n\n"}
{"id": "1804.05214", "contents": "Title: Fast Optimal Bandwidth Selection for RBF Kernel using Reproducing Kernel\n  Hilbert Space Operators for Kernel Based Classifiers Abstract: Kernel based methods have shown effective performance in many remote sensing\nclassification tasks. However their performance significantly depend on its\nhyper-parameters. The conventional technique to estimate the parameter comes\nwith high computational complexity. Thus, the objective of this letter is to\npropose an fast and efficient method to select the bandwidth parameter of the\nGaussian kernel in the kernel based classification methods. The proposed method\nis developed based on the operators in the reproducing kernel Hilbert space and\nit is evaluated on Support vector machines and PerTurbo classification method.\nExperiments conducted with hyperspectral datasets show that our proposed method\noutperforms the state-of-art method in terms in computational time and\nclassification performance. \n\n"}
{"id": "1804.05936", "contents": "Title: Learning a Deep Listwise Context Model for Ranking Refinement Abstract: Learning to rank has been intensively studied and widely applied in\ninformation retrieval. Typically, a global ranking function is learned from a\nset of labeled data, which can achieve good performance on average but may be\nsuboptimal for individual queries by ignoring the fact that relevant documents\nfor different queries may have different distributions in the feature space.\nInspired by the idea of pseudo relevance feedback where top ranked documents,\nwhich we refer as the \\textit{local ranking context}, can provide important\ninformation about the query's characteristics, we propose to use the inherent\nfeature distributions of the top results to learn a Deep Listwise Context Model\nthat helps us fine tune the initial ranked list. Specifically, we employ a\nrecurrent neural network to sequentially encode the top results using their\nfeature vectors, learn a local context model and use it to re-rank the top\nresults. There are three merits with our model: (1) Our model can capture the\nlocal ranking context based on the complex interactions between top results\nusing a deep neural network; (2) Our model can be built upon existing\nlearning-to-rank methods by directly using their extracted feature vectors; (3)\nOur model is trained with an attention-based loss function, which is more\neffective and efficient than many existing listwise methods. Experimental\nresults show that the proposed model can significantly improve the\nstate-of-the-art learning to rank methods on benchmark retrieval corpora. \n\n"}
{"id": "1804.06216", "contents": "Title: Learning Sparse Latent Representations with the Deep Copula Information\n  Bottleneck Abstract: Deep latent variable models are powerful tools for representation learning.\nIn this paper, we adopt the deep information bottleneck model, identify its\nshortcomings and propose a model that circumvents them. To this end, we apply a\ncopula transformation which, by restoring the invariance properties of the\ninformation bottleneck method, leads to disentanglement of the features in the\nlatent space. Building on that, we show how this transformation translates to\nsparsity of the latent space in the new model. We evaluate our method on\nartificial and real data. \n\n"}
{"id": "1804.06876", "contents": "Title: Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods Abstract: We introduce a new benchmark, WinoBias, for coreference resolution focused on\ngender bias. Our corpus contains Winograd-schema style sentences with entities\ncorresponding to people referred by their occupation (e.g. the nurse, the\ndoctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a\nneural coreference system all link gendered pronouns to pro-stereotypical\nentities with higher accuracy than anti-stereotypical entities, by an average\ndifference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation\napproach that, in combination with existing word-embedding debiasing\ntechniques, removes the bias demonstrated by these systems in WinoBias without\nsignificantly affecting their performance on existing coreference benchmark\ndatasets. Our dataset and code are available at http://winobias.org. \n\n"}
{"id": "1804.07000", "contents": "Title: Utilizing Neural Networks and Linguistic Metadata for Early Detection of\n  Depression Indications in Text Sequences Abstract: Depression is ranked as the largest contributor to global disability and is\nalso a major reason for suicide. Still, many individuals suffering from forms\nof depression are not treated for various reasons. Previous studies have shown\nthat depression also has an effect on language usage and that many depressed\nindividuals use social media platforms or the internet in general to get\ninformation or discuss their problems. This paper addresses the early detection\nof depression using machine learning models based on messages on a social\nplatform. In particular, a convolutional neural network based on different word\nembeddings is evaluated and compared to a classification based on user-level\nlinguistic metadata. An ensemble of both approaches is shown to achieve\nstate-of-the-art results in a current early detection task. Furthermore, the\ncurrently popular ERDE score as metric for early detection systems is examined\nin detail and its drawbacks in the context of shared tasks are illustrated. A\nslightly modified metric is proposed and compared to the original score.\nFinally, a new word embedding was trained on a large corpus of the same domain\nas the described task and is evaluated as well. \n\n"}
{"id": "1804.07010", "contents": "Title: Forward-Backward Stochastic Neural Networks: Deep Learning of\n  High-dimensional Partial Differential Equations Abstract: Classical numerical methods for solving partial differential equations suffer\nfrom the curse dimensionality mainly due to their reliance on meticulously\ngenerated spatio-temporal grids. Inspired by modern deep learning based\ntechniques for solving forward and inverse problems associated with partial\ndifferential equations, we circumvent the tyranny of numerical discretization\nby devising an algorithm that is scalable to high-dimensions. In particular, we\napproximate the unknown solution by a deep neural network which essentially\nenables us to benefit from the merits of automatic differentiation. To train\nthe aforementioned neural network we leverage the well-known connection between\nhigh-dimensional partial differential equations and forward-backward stochastic\ndifferential equations. In fact, independent realizations of a standard\nBrownian motion will act as training data. We test the effectiveness of our\napproach for a couple of benchmark problems spanning a number of scientific\ndomains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman\nequations, both in 100-dimensions. \n\n"}
{"id": "1804.07525", "contents": "Title: Benchmarking Top-K Keyword and Top-K Document Processing with\n  T${}^2$K${}^2$ and T${}^2$K${}^2$D${}^2$ Abstract: Top-k keyword and top-k document extraction are very popular text analysis\ntechniques. Top-k keywords and documents are often computed on-the-fly, but\nthey exploit weighted vocabularies that are costly to build. To compare\ncompeting weighting schemes and database implementations, benchmarking is\ncustomary. To the best of our knowledge, no benchmark currently addresses these\nproblems. Hence, in this paper, we present T${}^2$K${}^2$, a top-k keywords and\ndocuments benchmark, and its decision support-oriented evolution\nT${}^2$K${}^2$D${}^2$. Both benchmarks feature a real tweet dataset and queries\nwith various complexities and selectivities. They help evaluate weighting\nschemes and database implementations in terms of computing performance. To\nillustrate our bench-marks' relevance and genericity, we successfully ran\nperformance tests on the TF-IDF and Okapi BM25 weighting schemes, on one hand,\nand on different relational (Oracle, PostgreSQL) and document-oriented\n(MongoDB) database implementations, on the other hand. \n\n"}
{"id": "1804.08130", "contents": "Title: Sparse Travel Time Estimation from Streaming Data Abstract: We address two shortcomings in online travel time estimation methods for\ncongested urban traffic. The first shortcoming is related to the determination\nof the number of mixture modes, which can change dynamically, within day and\nfrom day to day. The second shortcoming is the wide-spread use of Gaussian\nprobability densities as mixture components. Gaussian densities fail to capture\nthe positive skew in travel time distributions and, consequently, large numbers\nof mixture components are needed for reasonable fitting accuracy when applied\nas mixture components. They also assign positive probabilities to negative\ntravel times. To address these issues, this paper derives a mixture\ndistribution with Gamma component densities, which are asymmetric and supported\non the positive numbers. We use sparse estimation techniques to ensure\nparsimonious models and propose a generalization of Gamma mixture densities\nusing Mittag-Leffler functions, which provides enhanced fitting flexibility and\nimproved parsimony. In order to accommodate within-day variability and allow\nfor online implementation of the proposed methodology (i.e., fast computations\non streaming travel time data), we introduce a recursive algorithm which\nefficiently updates the fitted distribution whenever new data become available.\nExperimental results using real-world travel time data illustrate the efficacy\nof the proposed methods. \n\n"}
{"id": "1804.08275", "contents": "Title: Deep Semantic Hashing with Generative Adversarial Networks Abstract: Hashing has been a widely-adopted technique for nearest neighbor search in\nlarge-scale image retrieval tasks. Recent research has shown that leveraging\nsupervised information can lead to high quality hashing. However, the cost of\nannotating data is often an obstacle when applying supervised hashing to a new\ndomain. Moreover, the results can suffer from the robustness problem as the\ndata at training and test stage could come from similar but different\ndistributions. This paper studies the exploration of generating synthetic data\nthrough semi-supervised generative adversarial networks (GANs), which leverages\nlargely unlabeled and limited labeled training data to produce highly\ncompelling data with intrinsic invariance and global coherence, for better\nunderstanding statistical structures of natural data. We demonstrate that the\nabove two limitations can be well mitigated by applying the synthetic data for\nhashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\npresented, which mainly consists of four components: a deep convolution neural\nnetworks (CNN) for learning image representations, an adversary stream to\ndistinguish synthetic images from real ones, a hash stream for encoding image\nrepresentations to hash codes and a classification stream. The whole\narchitecture is trained end-to-end by jointly optimizing three losses, i.e.,\nadversarial loss to correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the input\nreal-synthetic triplets and classification loss to classify each sample\naccurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\nbenchmarks validate the capability of exploiting synthetic images for hashing.\nOur framework also achieves superior results when compared to state-of-the-art\ndeep hash models. \n\n"}
{"id": "1804.08847", "contents": "Title: DeepEmo: Learning and Enriching Pattern-Based Emotion Representations Abstract: We propose a graph-based mechanism to extract rich-emotion bearing patterns,\nwhich fosters a deeper analysis of online emotional expressions, from a corpus.\nThe patterns are then enriched with word embeddings and evaluated through\nseveral emotion recognition tasks. Moreover, we conduct analysis on the\nemotion-oriented patterns to demonstrate its applicability and to explore its\nproperties. Our experimental results demonstrate that the proposed techniques\noutperform most state-of-the-art emotion recognition techniques. \n\n"}
{"id": "1804.09060", "contents": "Title: An Information-Theoretic View for Deep Learning Abstract: Deep learning has transformed computer vision, natural language processing,\nand speech recognition\\cite{badrinarayanan2017segnet, dong2016image,\nren2017faster, ji20133d}. However, two critical questions remain obscure: (1)\nwhy do deep neural networks generalize better than shallow networks; and (2)\ndoes it always hold that a deeper network leads to better performance?\nSpecifically, letting $L$ be the number of convolutional and pooling layers in\na deep neural network, and $n$ be the size of the training sample, we derive an\nupper bound on the expected generalization error for this network, i.e.,\n  \\begin{eqnarray*}\n  \\mathbb{E}[R(W)-R_S(W)] \\leq\n\\exp{\\left(-\\frac{L}{2}\\log{\\frac{1}{\\eta}}\\right)}\\sqrt{\\frac{2\\sigma^2}{n}I(S,W)\n}\n  \\end{eqnarray*} where $\\sigma >0$ is a constant depending on the loss\nfunction, $0<\\eta<1$ is a constant depending on the information loss for each\nconvolutional or pooling layer, and $I(S, W)$ is the mutual information between\nthe training sample $S$ and the output hypothesis $W$. This upper bound shows\nthat as the number of convolutional and pooling layers $L$ increases in the\nnetwork, the expected generalization error will decrease exponentially to zero.\nLayers with strict information loss, such as the convolutional layers, reduce\nthe generalization error for the whole network; this answers the first\nquestion. However, algorithms with zero expected generalization error does not\nimply a small test error or $\\mathbb{E}[R(W)]$. This is because\n$\\mathbb{E}[R_S(W)]$ is large when the information for fitting the data is lost\nas the number of layers increases. This suggests that the claim `the deeper the\nbetter' is conditioned on a small training error or $\\mathbb{E}[R_S(W)]$.\nFinally, we show that deep learning satisfies a weak notion of stability and\nthe sample complexity of deep neural networks will decrease as $L$ increases. \n\n"}
{"id": "1804.09217", "contents": "Title: On Learning Sparsely Used Dictionaries from Incomplete Samples Abstract: Most existing algorithms for dictionary learning assume that all entries of\nthe (high-dimensional) input data are fully observed. However, in several\npractical applications (such as hyper-spectral imaging or blood glucose\nmonitoring), only an incomplete fraction of the data entries may be available.\nFor incomplete settings, no provably correct and polynomial-time algorithm has\nbeen reported in the dictionary learning literature. In this paper, we provide\nprovable approaches for learning - from incomplete samples - a family of\ndictionaries whose atoms have sufficiently \"spread-out\" mass. First, we propose\na descent-style iterative algorithm that linearly converges to the true\ndictionary when provided a sufficiently coarse initial estimate. Second, we\npropose an initialization algorithm that utilizes a small number of extra fully\nobserved samples to produce such a coarse initial estimate. Finally, we\ntheoretically analyze their performance and provide asymptotic statistical and\ncomputational guarantees. \n\n"}
{"id": "1804.09301", "contents": "Title: Gender Bias in Coreference Resolution Abstract: We present an empirical study of gender bias in coreference resolution\nsystems. We first introduce a novel, Winograd schema-style set of minimal pair\nsentences that differ only by pronoun gender. With these \"Winogender schemas,\"\nwe evaluate and confirm systematic gender bias in three publicly-available\ncoreference resolution systems, and correlate this bias with real-world and\ntextual gender statistics. \n\n"}
{"id": "1804.09314", "contents": "Title: Deep Learning for Predicting Asset Returns Abstract: Deep learning searches for nonlinear factors for predicting asset returns.\nPredictability is achieved via multiple layers of composite factors as opposed\nto additive ones. Viewed in this way, asset pricing studies can be revisited\nusing multi-layer deep learners, such as rectified linear units (ReLU) or\nlong-short-term-memory (LSTM) for time-series effects. State-of-the-art\nalgorithms including stochastic gradient descent (SGD), TensorFlow and dropout\ndesign provide imple- mentation and efficient factor exploration. To illustrate\nour methodology, we revisit the equity market risk premium dataset of Welch and\nGoyal (2008). We find the existence of nonlinear factors which explain\npredictability of returns, in particular at the extremes of the characteristic\nspace. Finally, we conclude with directions for future research. \n\n"}
{"id": "1804.09943", "contents": "Title: System Description of CITlab's Recognition & Retrieval Engine for\n  ICDAR2017 Competition on Information Extraction in Historical Handwritten\n  Records Abstract: We present a recognition and retrieval system for the ICDAR2017 Competition\non Information Extraction in Historical Handwritten Records which successfully\ninfers person names and other data from marriage records. The system extracts\ninformation from the line images with a high accuracy and outperforms the\nbaseline. The optical model is based on Neural Networks. To infer the desired\ninformation, regular expressions are used to describe the set of feasible words\nsequences. \n\n"}
{"id": "1804.10299", "contents": "Title: Distributed Differentially-Private Algorithms for Matrix and Tensor\n  Factorization Abstract: In many signal processing and machine learning applications, datasets\ncontaining private information are held at different locations, requiring the\ndevelopment of distributed privacy-preserving algorithms. Tensor and matrix\nfactorizations are key components of many processing pipelines. In the\ndistributed setting, differentially private algorithms suffer because they\nintroduce noise to guarantee privacy. This paper designs new and improved\ndistributed and differentially private algorithms for two popular matrix and\ntensor factorization methods: principal component analysis (PCA) and orthogonal\ntensor decomposition (OTD). The new algorithms employ a correlated noise design\nscheme to alleviate the effects of noise and can achieve the same noise level\nas the centralized scenario. Experiments on synthetic and real data illustrate\nthe regimes in which the correlated noise allows performance matching with the\ncentralized setting, outperforming previous methods and demonstrating that\nmeaningful utility is possible while guaranteeing differential privacy. \n\n"}
{"id": "1804.10911", "contents": "Title: A Tree Search Algorithm for Sequence Labeling Abstract: In this paper we propose a novel reinforcement learning based model for\nsequence tagging, referred to as MM-Tag. Inspired by the success and\nmethodology of the AlphaGo Zero, MM-Tag formalizes the problem of sequence\ntagging with a Monte Carlo tree search (MCTS) enhanced Markov decision process\n(MDP) model, in which the time steps correspond to the positions of words in a\nsentence from left to right, and each action corresponds to assign a tag to a\nword. Two long short-term memory networks (LSTM) are used to summarize the past\ntag assignments and words in the sentence. Based on the outputs of LSTMs, the\npolicy for guiding the tag assignment and the value for predicting the whole\ntagging accuracy of the whole sentence are produced. The policy and value are\nthen strengthened with MCTS, which takes the produced raw policy and value as\ninputs, simulates and evaluates the possible tag assignments at the subsequent\npositions, and outputs a better search policy for assigning tags. A\nreinforcement learning algorithm is proposed to train the model parameters. Our\nwork is the first to apply the MCTS enhanced MDP model to the sequence tagging\ntask. We show that MM-Tag can accurately predict the tags thanks to the\nexploratory decision making mechanism introduced by MCTS. Experimental results\nshow based on a chunking benchmark showed that MM-Tag outperformed the\nstate-of-the-art sequence tagging baselines including CRF and CRF with LSTM. \n\n"}
{"id": "1805.00121", "contents": "Title: A Missing Information Loss function for implicit feedback datasets Abstract: Latent factor models for Recommender Systems with implicit feedback typically\ntreat unobserved user-item interactions (i.e. missing information) as negative\nfeedback. This is frequently done either through negative sampling (point--wise\nloss) or with a ranking loss function (pair-- or list--wise estimation). Since\na zero preference recommendation is a valid solution for most common objective\nfunctions, regarding unknown values as actual zeros results in users having a\nzero preference recommendation for most of the available items. In this paper\nwe propose a novel objective function, the \\emph{Missing Information Loss}\n(MIL), that explicitly forbids treating unobserved user-item interactions as\npositive or negative feedback. We apply this loss to both traditional Matrix\nFactorization and user--based Denoising Autoencoder, and compare it with other\nestablished objective functions such as cross-entropy (both point- and\npair-wise) or the recently proposed multinomial log-likelihood. MIL achieves\ncompetitive performance in ranking-aware metrics when applied to three\ndatasets. Furthermore, we show that such a relevance in the recommendation is\nobtained while displaying popular items less frequently (up to a $20 \\%$\ndecrease with respect to the best competing method). This debiasing from the\nrecommendation of popular items favours the appearance of infrequent items (up\nto a $50 \\%$ increase of long-tail recommendations), a valuable feature for\nRecommender Systems with a large catalogue of products. \n\n"}
{"id": "1805.00356", "contents": "Title: Deep Factorization Machines for Knowledge Tracing Abstract: This paper introduces our solution to the 2018 Duolingo Shared Task on Second\nLanguage Acquisition Modeling (SLAM). We used deep factorization machines, a\nwide and deep learning model of pairwise relationships between users, items,\nskills, and other entities considered. Our solution (AUC 0.815) hopefully\nmanaged to beat the logistic regression baseline (AUC 0.774) but not the top\nperforming model (AUC 0.861) and reveals interesting strategies to build upon\nitem response theory models. \n\n"}
{"id": "1805.01045", "contents": "Title: Alpha-Beta Divergence For Variational Inference Abstract: This paper introduces a variational approximation framework using direct\noptimization of what is known as the {\\it scale invariant Alpha-Beta\ndivergence} (sAB divergence). This new objective encompasses most variational\nobjectives that use the Kullback-Leibler, the R{\\'e}nyi or the gamma\ndivergences. It also gives access to objective functions never exploited before\nin the context of variational inference. This is achieved via two easy to\ninterpret control parameters, which allow for a smooth interpolation over the\ndivergence space while trading-off properties such as mass-covering of a target\ndistribution and robustness to outliers in the data. Furthermore, the sAB\nvariational objective can be optimized directly by repurposing existing methods\nfor Monte Carlo computation of complex variational objectives, leading to\nestimates of the divergence instead of variational lower bounds. We show the\nadvantages of this objective on Bayesian models for regression problems. \n\n"}
{"id": "1805.01138", "contents": "Title: Detecting Parkinson's Disease from interactions with a search engine: Is\n  expert knowledge sufficient? Abstract: Parkinson's disease (PD) is a slowly progressing neurodegenerative disease\nwith early manifestation of motor signs. Recently, there has been a growing\ninterest in developing automatic tools that can assess motor function in PD\npatients. Here we show that mouse tracking data collected during people's\ninteraction with a search engine can be used to distinguish PD patients from\nsimilar, non-diseased users and present a methodology developed for the\ndiagnosis of PD from these data. A main challenge we address is the extraction\nof informative features from raw mouse tracking data. We do so in two\ncomplementary ways: First, we manually construct expert-recommended informative\nfeatures, aiming to identify abnormalities in motor behaviors. Second, we use\nan unsupervised representation learning technique to map these raw data to\nhigh-level features. Using all the extracted features, a Random Forest\nclassifier is then used to distinguish PD patients from controls, achieving an\nAUC of 0.92, while results using only expert-generated or auto-generated\nfeatures are 0.87 and 0.83, respectively. Our results indicate that mouse\ntracking data can help in detecting users at early stages of the disease, and\nthat both expert-generated features and unsupervised techniques for feature\ngeneration are required to achieve the best possible performance \n\n"}
{"id": "1805.01334", "contents": "Title: Towards Better Text Understanding and Retrieval through Kernel Entity\n  Salience Modeling Abstract: This paper presents a Kernel Entity Salience Model (KESM) that improves text\nunderstanding and retrieval by better estimating entity salience (importance)\nin documents. KESM represents entities by knowledge enriched distributed\nrepresentations, models the interactions between entities and words by kernels,\nand combines the kernel scores to estimate entity salience. The whole model is\nlearned end-to-end using entity salience labels. The salience model also\nimproves ad hoc search accuracy, providing effective ranking features by\nmodeling the salience of query entities in candidate documents. Our experiments\non two entity salience corpora and two TREC ad hoc search datasets demonstrate\nthe effectiveness of KESM over frequency-based and feature-based methods. We\nalso provide examples showing how KESM conveys its text understanding ability\nlearned from entity salience to search. \n\n"}
{"id": "1805.01788", "contents": "Title: Equity of Attention: Amortizing Individual Fairness in Rankings Abstract: Rankings of people and items are at the heart of selection-making,\nmatch-making, and recommender systems, ranging from employment sites to sharing\neconomy platforms. As ranking positions influence the amount of attention the\nranked subjects receive, biases in rankings can lead to unfair distribution of\nopportunities and resources, such as jobs or income.\n  This paper proposes new measures and mechanisms to quantify and mitigate\nunfairness from a bias inherent to all rankings, namely, the position bias,\nwhich leads to disproportionately less attention being paid to low-ranked\nsubjects. Our approach differs from recent fair ranking approaches in two\nimportant ways. First, existing works measure unfairness at the level of\nsubject groups while our measures capture unfairness at the level of individual\nsubjects, and as such subsume group unfairness. Second, as no single ranking\ncan achieve individual attention fairness, we propose a novel mechanism that\nachieves amortized fairness, where attention accumulated across a series of\nrankings is proportional to accumulated relevance.\n  We formulate the challenge of achieving amortized individual fairness subject\nto constraints on ranking quality as an online optimization problem and show\nthat it can be solved as an integer linear program. Our experimental evaluation\nreveals that unfair attention distribution in rankings can be substantial, and\ndemonstrates that our method can improve individual fairness while retaining\nhigh ranking quality. \n\n"}
{"id": "1805.02161", "contents": "Title: Branching embedding: A heuristic dimensionality reduction algorithm\n  based on hierarchical clustering Abstract: This paper proposes a new dimensionality reduction algorithm named branching\nembedding (BE). It converts a dendrogram to a two-dimensional scatter plot, and\nvisualizes the inherent structures of the original high-dimensional data. Since\nthe conversion part is not computationally demanding, the BE algorithm would be\nbeneficial for the case where hierarchical clustering is already performed.\nNumerical experiments revealed that the outputs of the algorithm moderately\npreserve the original hierarchical structures. \n\n"}
{"id": "1805.03797", "contents": "Title: WikiPassageQA: A Benchmark Collection for Research on Non-factoid Answer\n  Passage Retrieval Abstract: With the rise in mobile and voice search, answer passage retrieval acts as a\ncritical component of an effective information retrieval system for open domain\nquestion answering. Currently, there are no comparable collections that address\nnon-factoid question answering within larger documents while simultaneously\nproviding enough examples sufficient to train a deep neural network. In this\npaper, we introduce a new Wikipedia based collection specific for non-factoid\nanswer passage retrieval containing thousands of questions with annotated\nanswers and show benchmark results on a variety of state of the art neural\narchitectures and retrieval models. The experimental results demonstrate the\nunique challenges presented by answer passage retrieval within topically\nrelevant documents for future research. \n\n"}
{"id": "1805.03911", "contents": "Title: Labelling as an unsupervised learning problem Abstract: Unravelling hidden patterns in datasets is a classical problem with many\npotential applications. In this paper, we present a challenge whose objective\nis to discover nonlinear relationships in noisy cloud of points. If a set of\npoint satisfies a nonlinear relationship that is unlikely to be due to\nrandomness, we will label the set with this relationship. Since points can\nsatisfy one, many or no such nonlinear relationships, cloud of points will\ntypically have one, multiple or no labels at all. This introduces the labelling\nproblem that will be studied in this paper.\n  The objective of this paper is to develop a framework for the labelling\nproblem. We introduce a precise notion of a label, and we propose an algorithm\nto discover such labels in a given dataset, which is then tested in synthetic\ndatasets. We also analyse, using tools from random matrix theory, the problem\nof discovering false labels in the dataset. \n\n"}
{"id": "1805.04104", "contents": "Title: The Capacity of Private Information Retrieval from Uncoded Storage\n  Constrained Databases Abstract: Private information retrieval (PIR) allows a user to retrieve a desired\nmessage from a set of databases without revealing the identity of the desired\nmessage. The replicated databases scenario was considered by Sun and Jafar,\n2016, where $N$ databases can store the same $K$ messages completely. A PIR\nscheme was developed to achieve the optimal download cost given by $\\left(1+\n\\frac{1}{N}+ \\frac{1}{N^{2}}+ \\cdots + \\frac{1}{N^{K-1}}\\right)$. In this work,\nwe consider the problem of PIR from storage constrained databases. Each\ndatabase has a storage capacity of $\\mu KL$ bits, where $L$ is the size of each\nmessage in bits, and $\\mu \\in [1/N, 1]$ is the normalized storage. On one\nextreme, $\\mu=1$ is the replicated databases case. On the other hand, when\n$\\mu= 1/N$, then in order to retrieve a message privately, the user has to\ndownload all the messages from the databases achieving a download cost of\n$1/K$. We aim to characterize the optimal download cost versus storage\ntrade-off for any storage capacity in the range $\\mu \\in [1/N, 1]$. For any\n$(N,K)$, we show that the optimal trade-off between storage, $\\mu$, and the\ndownload cost, $D(\\mu)$, is given by the lower convex hull of the $N$ pairs\n$\\left(\\mu= \\frac{t}{N},D(\\mu) = \\left(1+ \\frac{1}{t}+ \\frac{1}{t^{2}}+ \\cdots\n+ \\frac{1}{t^{K-1}}\\right)\\right)$ for $t=1,2,\\ldots, N$. To prove this result,\nwe first present the storage constrained PIR scheme for any $(N,K)$. We next\nobtain a general lower bound on the download cost for PIR, which is valid for\nthe following storage scenarios: replicated or storage constrained, coded or\nuncoded, and fixed or optimized. We then specialize this bound using the\nuncoded storage assumption to obtain lower bounds matching the achievable\ndownload cost of the storage constrained PIR scheme for any value of the\navailable storage. \n\n"}
{"id": "1805.04785", "contents": "Title: An Optimal Policy for Dynamic Assortment Planning Under Uncapacitated\n  Multinomial Logit Models Abstract: We study the dynamic assortment planning problem, where for each arriving\ncustomer, the seller offers an assortment of substitutable products and\ncustomer makes the purchase among offered products according to an\nuncapacitated multinomial logit (MNL) model. Since all the utility parameters\nof MNL are unknown, the seller needs to simultaneously learn customers' choice\nbehavior and make dynamic decisions on assortments based on the current\nknowledge. The goal of the seller is to maximize the expected revenue, or\nequivalently, to minimize the expected regret. Although dynamic assortment\nplanning problem has received an increasing attention in revenue management,\nmost existing policies require the estimation of mean utility for each product\nand the final regret usually involves the number of products $N$. The optimal\nregret of the dynamic assortment planning problem under the most basic and\npopular choice model---MNL model is still open. By carefully analyzing a\nrevenue potential function, we develop a trisection based policy combined with\nadaptive confidence bound construction, which achieves an {item-independent}\nregret bound of $O(\\sqrt{T})$, where $T$ is the length of selling horizon. We\nfurther establish the matching lower bound result to show the optimality of our\npolicy. There are two major advantages of the proposed policy. First, the\nregret of all our policies has no dependence on $N$. Second, our policies are\nalmost assumption free: there is no assumption on mean utility nor any\n\"separability\" condition on the expected revenues for different assortments.\nOur result also extends the unimodal bandit literature. \n\n"}
{"id": "1805.05479", "contents": "Title: Machine Readable Web APIs with Schema.org Action Annotations Abstract: The schema.org initiative led by the four major search engines curates a\nvocabulary for describing web content. The number of semantic annotations on\nthe web are increasing, mostly due to the industrial incentives provided by\nthose search engines. The annotations are not only consumed by search engines,\nbut also by other automated agents like intelligent personal assistants (IPAs).\nHowever, only annotating data is not enough for automated agents to reach their\nfull potential. Web APIs should be also annotated for automating service\nconsumption, so the IPAs can complete tasks like booking a hotel room or buying\na ticket for an event on the fly. Although there has been a vast amount of\neffort in the semantic web services field, the approaches did not gain too much\nadoption outside of academia, mainly due to lack of concrete incentives and\nsteep learning curves. In this paper, we suggest a lightweight, bottom-up\napproach based on schema.org actions to annotate Web APIs. We analyse\nschema.org vocabulary in the scope of lightweight semantic web services\nliterature and propose extensions where necessary. We show that schema.org\nactions could be a suitable vocabulary for Web API description. We demonstrate\nour work by annotating existing Web APIs of accommodation service providers.\nAdditionally, we briefly demonstrate how these APIs can be used dynamically,\nfor example, by a dialogue system. \n\n"}
{"id": "1805.06299", "contents": "Title: Change Detection in Graph Streams by Learning Graph Embeddings on\n  Constant-Curvature Manifolds Abstract: The space of graphs is often characterised by a non-trivial geometry, which\ncomplicates learning and inference in practical applications. A common approach\nis to use embedding techniques to represent graphs as points in a conventional\nEuclidean space, but non-Euclidean spaces have often been shown to be better\nsuited for embedding graphs. Among these, constant-curvature Riemannian\nmanifolds (CCMs) offer embedding spaces suitable for studying the statistical\nproperties of a graph distribution, as they provide ways to easily compute\nmetric geodesic distances. In this paper, we focus on the problem of detecting\nchanges in stationarity in a stream of attributed graphs. To this end, we\nintroduce a novel change detection framework based on neural networks and CCMs,\nthat takes into account the non-Euclidean nature of graphs. Our contribution in\nthis work is twofold. First, via a novel approach based on adversarial\nlearning, we compute graph embeddings by training an autoencoder to represent\ngraphs on CCMs. Second, we introduce two novel change detection tests operating\non CCMs. We perform experiments on synthetic data, as well as two real-world\napplication scenarios: the detection of epileptic seizures using functional\nconnectivity brain networks, and the detection of hostility between two\nsubjects, using human skeletal graphs. Results show that the proposed methods\nare able to detect even small changes in a graph-generating process,\nconsistently outperforming approaches based on Euclidean embeddings. \n\n"}
{"id": "1805.06370", "contents": "Title: Progress & Compress: A scalable framework for continual learning Abstract: We introduce a conceptually simple and scalable framework for continual\nlearning domains where tasks are learned sequentially. Our method is constant\nin the number of parameters and is designed to preserve performance on\npreviously encountered tasks while accelerating learning progress on subsequent\nproblems. This is achieved by training a network with two components: A\nknowledge base, capable of solving previously encountered problems, which is\nconnected to an active column that is employed to efficiently learn the current\ntask. After learning a new task, the active column is distilled into the\nknowledge base, taking care to protect any previously acquired skills. This\ncycle of active learning (progression) followed by consolidation (compression)\nrequires no architecture growth, no access to or storing of previous data or\ntasks, and no task-specific parameters. We demonstrate the progress & compress\napproach on sequential classification of handwritten alphabets as well as two\nreinforcement learning domains: Atari games and 3D maze navigation. \n\n"}
{"id": "1805.06576", "contents": "Title: Mad Max: Affine Spline Insights into Deep Learning Abstract: We build a rigorous bridge between deep networks (DNs) and approximation\ntheory via spline functions and operators. Our key result is that a large class\nof DNs can be written as a composition of max-affine spline operators (MASOs),\nwhich provide a powerful portal through which to view and analyze their inner\nworkings. For instance, conditioned on the input signal, the output of a MASO\nDN can be written as a simple affine transformation of the input. This implies\nthat a DN constructs a set of signal-dependent, class-specific templates\nagainst which the signal is compared via a simple inner product; we explore the\nlinks to the classical theory of optimal classification via matched filters and\nthe effects of data memorization. Going further, we propose a simple penalty\nterm that can be added to the cost function of any DN learning algorithm to\nforce the templates to be orthogonal with each other; this leads to\nsignificantly improved classification performance and reduced overfitting with\nno change to the DN architecture. The spline partition of the input signal\nspace that is implicitly induced by a MASO directly links DNs to the theory of\nvector quantization (VQ) and $K$-means clustering, which opens up new geometric\navenue to study how DNs organize signals in a hierarchical fashion. To validate\nthe utility of the VQ interpretation, we develop and validate a new distance\nmetric for signals and images that quantifies the difference between their VQ\nencodings. (This paper is a significantly expanded version of A Spline Theory\nof Deep Learning from ICML 2018.) \n\n"}
{"id": "1805.06627", "contents": "Title: Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures Abstract: Embedding methods which enforce a partial order or lattice structure over the\nconcept space, such as Order Embeddings (OE) (Vendrov et al., 2016), are a\nnatural way to model transitive relational data (e.g. entailment graphs).\nHowever, OE learns a deterministic knowledge base, limiting expressiveness of\nqueries and the ability to use uncertainty for both prediction and learning\n(e.g. learning from expectations). Probabilistic extensions of OE (Lai and\nHockenmaier, 2017) have provided the ability to somewhat calibrate these\ndenotational probabilities while retaining the consistency and inductive bias\nof ordered models, but lack the ability to model the negative correlations\nfound in real-world knowledge. In this work we show that a broad class of\nmodels that assign probability measures to OE can never capture negative\ncorrelation, which motivates our construction of a novel box lattice and\naccompanying probability measure to capture anticorrelation and even disjoint\nconcepts, while still providing the benefits of probabilistic modeling, such as\nthe ability to perform rich joint and conditional queries over arbitrary sets\nof concepts, and both learning from and predicting calibrated uncertainty. We\nshow improvements over previous approaches in modeling the Flickr and WordNet\nentailment graphs, and investigate the power of the model. \n\n"}
{"id": "1805.06826", "contents": "Title: The Blessings of Multiple Causes Abstract: Causal inference from observational data often assumes \"ignorability,\" that\nall confounders are observed. This assumption is standard yet untestable.\nHowever, many scientific studies involve multiple causes, different variables\nwhose effects are simultaneously of interest. We propose the deconfounder, an\nalgorithm that combines unsupervised machine learning and predictive model\nchecking to perform causal inference in multiple-cause settings. The\ndeconfounder infers a latent variable as a substitute for unobserved\nconfounders and then uses that substitute to perform causal inference. We\ndevelop theory for the deconfounder, and show that it requires weaker\nassumptions than classical causal inference. We analyze its performance in\nthree types of studies: semi-simulated data around smoking and lung cancer,\nsemi-simulated data around genome-wide association studies, and a real dataset\nabout actors and movie revenue. The deconfounder provides a checkable approach\nto estimating closer-to-truth causal effects. \n\n"}
{"id": "1805.07072", "contents": "Title: Optimizing for Generalization in Machine Learning with Cross-Validation\n  Gradients Abstract: Cross-validation is the workhorse of modern applied statistics and machine\nlearning, as it provides a principled framework for selecting the model that\nmaximizes generalization performance. In this paper, we show that the\ncross-validation risk is differentiable with respect to the hyperparameters and\ntraining data for many common machine learning algorithms, including logistic\nregression, elastic-net regression, and support vector machines. Leveraging\nthis property of differentiability, we propose a cross-validation gradient\nmethod (CVGM) for hyperparameter optimization. Our method enables efficient\noptimization in high-dimensional hyperparameter spaces of the cross-validation\nrisk, the best surrogate of the true generalization ability of our learning\nalgorithm. \n\n"}
{"id": "1805.07445", "contents": "Title: DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors Abstract: Boltzmann machines are powerful distributions that have been shown to be an\neffective prior over binary latent variables in variational autoencoders\n(VAEs). However, previous methods for training discrete VAEs have used the\nevidence lower bound and not the tighter importance-weighted bound. We propose\ntwo approaches for relaxing Boltzmann machines to continuous distributions that\npermit training with importance-weighted bounds. These relaxations are based on\ngeneralized overlapping transformations and the Gaussian integral trick.\nExperiments on the MNIST and OMNIGLOT datasets show that these relaxations\noutperform previous discrete VAEs with Boltzmann priors. An implementation\nwhich reproduces these results is available at\nhttps://github.com/QuadrantAI/dvae . \n\n"}
{"id": "1805.07479", "contents": "Title: Semisupervised Learning on Heterogeneous Graphs and its Applications to\n  Facebook News Feed Abstract: Graph-based semi-supervised learning is a fundamental machine learning\nproblem, and has been well studied. Most studies focus on homogeneous networks\n(e.g. citation network, friend network). In the present paper, we propose the\nHeterogeneous Embedding Label Propagation (HELP) algorithm, a graph-based\nsemi-supervised deep learning algorithm, for graphs that are characterized by\nheterogeneous node types. Empirically, we demonstrate the effectiveness of this\nmethod in domain classification tasks with Facebook user-domain interaction\ngraph, and compare the performance of the proposed HELP algorithm with the\nstate of the art algorithms. We show that the HELP algorithm improves the\npredictive performance across multiple tasks, together with semantically\nmeaningful embedding that are discriminative for downstream classification or\nregression tasks. \n\n"}
{"id": "1805.07633", "contents": "Title: Heterogeneous Multi-output Gaussian Process Prediction Abstract: We present a novel extension of multi-output Gaussian processes for handling\nheterogeneous outputs. We assume that each output has its own likelihood\nfunction and use a vector-valued Gaussian process prior to jointly model the\nparameters in all likelihoods as latent functions. Our multi-output Gaussian\nprocess uses a covariance function with a linear model of coregionalisation\nform. Assuming conditional independence across the underlying latent functions\ntogether with an inducing variable framework, we are able to obtain tractable\nvariational bounds amenable to stochastic variational inference. We illustrate\nthe performance of the model on synthetic data and two real datasets: a human\nbehavioral study and a demographic high-dimensional dataset. \n\n"}
{"id": "1805.07654", "contents": "Title: Sampling-Free Variational Inference of Bayesian Neural Networks by\n  Variance Backpropagation Abstract: We propose a new Bayesian Neural Net formulation that affords variational\ninference for which the evidence lower bound is analytically tractable subject\nto a tight approximation. We achieve this tractability by (i) decomposing ReLU\nnonlinearities into the product of an identity and a Heaviside step function,\n(ii) introducing a separate path that decomposes the neural net expectation\nfrom its variance. We demonstrate formally that introducing separate latent\nbinary variables to the activations allows representing the neural network\nlikelihood as a chain of linear operations. Performing variational inference on\nthis construction enables a sampling-free computation of the evidence lower\nbound which is a more effective approximation than the widely applied Monte\nCarlo sampling and CLT related techniques. We evaluate the model on a range of\nregression and classification tasks against BNN inference alternatives, showing\ncompetitive or improved performance over the current state-of-the-art. \n\n"}
{"id": "1805.07833", "contents": "Title: Wasserstein regularization for sparse multi-task regression Abstract: We focus in this paper on high-dimensional regression problems where each\nregressor can be associated to a location in a physical space, or more\ngenerally a generic geometric space. Such problems often employ sparse priors,\nwhich promote models using a small subset of regressors. To increase\nstatistical power, the so-called multi-task techniques were proposed, which\nconsist in the simultaneous estimation of several related models. Combined with\nsparsity assumptions, it lead to models enforcing the active regressors to be\nshared across models, thanks to, for instance L1 / Lq norms. We argue in this\npaper that these techniques fail to leverage the spatial information associated\nto regressors. Indeed, while sparse priors enforce that only a small subset of\nvariables is used, the assumption that these regressors overlap across all\ntasks is overly simplistic given the spatial variability observed in real data.\nIn this paper, we propose a convex regularizer for multi-task regression that\nencodes a more flexible geometry. Our regularizer is based on unbalanced\noptimal transport (OT) theory, and can take into account a prior geometric\nknowledge on the regressor variables, without necessarily requiring overlapping\nsupports. We derive an efficient algorithm based on a regularized formulation\nof OT, which iterates through applications of Sinkhorn's algorithm along with\ncoordinate descent iterations. The performance of our model is demonstrated on\nregular grids with both synthetic and real datasets as well as complex\ntriangulated geometries of the cortex with an application in neuroimaging. \n\n"}
{"id": "1805.08122", "contents": "Title: A General Family of Robust Stochastic Operators for Reinforcement\n  Learning Abstract: We consider a new family of operators for reinforcement learning with the\ngoal of alleviating the negative effects and becoming more robust to\napproximation or estimation errors. Various theoretical results are\nestablished, which include showing on a sample path basis that our family of\noperators preserve optimality and increase the action gap. Our empirical\nresults illustrate the strong benefits of our family of operators,\nsignificantly outperforming the classical Bellman operator and recently\nproposed operators. \n\n"}
{"id": "1805.08159", "contents": "Title: Multi-Perspective Relevance Matching with Hierarchical ConvNets for\n  Social Media Search Abstract: Despite substantial interest in applications of neural networks to\ninformation retrieval, neural ranking models have only been applied to standard\nad hoc retrieval tasks over web pages and newswire documents. This paper\nproposes MP-HCNN (Multi-Perspective Hierarchical Convolutional Neural Network)\na novel neural ranking model specifically designed for ranking short social\nmedia posts. We identify document length, informal language, and heterogeneous\nrelevance signals as features that distinguish documents in our domain, and\npresent a model specifically designed with these characteristics in mind. Our\nmodel uses hierarchical convolutional layers to learn latent semantic\nsoft-match relevance signals at the character, word, and phrase levels. A\npooling-based similarity measurement layer integrates evidence from multiple\ntypes of matches between the query, the social media post, as well as URLs\ncontained in the post. Extensive experiments using Twitter data from the TREC\nMicroblog Tracks 2011--2014 show that our model significantly outperforms prior\nfeature-based as well and existing neural ranking models. To our best\nknowledge, this paper presents the first substantial work tackling search over\nsocial media posts using neural ranking models. \n\n"}
{"id": "1805.08266", "contents": "Title: On the Selection of Initialization and Activation Function for Deep\n  Neural Networks Abstract: The weight initialization and the activation function of deep neural networks\nhave a crucial impact on the performance of the training procedure. An\ninappropriate selection can lead to the loss of information of the input during\nforward propagation and the exponential vanishing/exploding of gradients during\nback-propagation. Understanding the theoretical properties of untrained random\nnetworks is key to identifying which deep networks may be trained successfully\nas recently demonstrated by Schoenholz et al. (2017) who showed that for deep\nfeedforward neural networks only a specific choice of hyperparameters known as\nthe `edge of chaos' can lead to good performance. We complete this analysis by\nproviding quantitative results showing that, for a class of ReLU-like\nactivation functions, the information propagates indeed deeper for an\ninitialization at the edge of chaos. By further extending this analysis, we\nidentify a class of activation functions that improve the information\npropagation over ReLU-like functions. This class includes the Swish activation,\n$\\phi_{swish}(x) = x \\cdot \\text{sigmoid}(x)$, used in Hendrycks & Gimpel\n(2016), Elfwing et al. (2017) and Ramachandran et al. (2017). This provides a\ntheoretical grounding for the excellent empirical performance of $\\phi_{swish}$\nobserved in these contributions. We complement those previous results by\nillustrating the benefit of using a random initialization on the edge of chaos\nin this context. \n\n"}
{"id": "1805.08610", "contents": "Title: Optimization, fast and slow: optimally switching between local and\n  Bayesian optimization Abstract: We develop the first Bayesian Optimization algorithm, BLOSSOM, which selects\nbetween multiple alternative acquisition functions and traditional local\noptimization at each step. This is combined with a novel stopping condition\nbased on expected regret. This pairing allows us to obtain the best\ncharacteristics of both local and Bayesian optimization, making efficient use\nof function evaluations while yielding superior convergence to the global\nminimum on a selection of optimization problems, and also halting optimization\nonce a principled and intuitive stopping condition has been fulfilled. \n\n"}
{"id": "1805.08647", "contents": "Title: Multi-Statistic Approximate Bayesian Computation with Multi-Armed\n  Bandits Abstract: Approximate Bayesian computation is an established and popular method for\nlikelihood-free inference with applications in many disciplines. The\neffectiveness of the method depends critically on the availability of well\nperforming summary statistics. Summary statistic selection relies heavily on\ndomain knowledge and carefully engineered features, and can be a laborious time\nconsuming process. Since the method is sensitive to data dimensionality, the\nprocess of selecting summary statistics must balance the need to include\ninformative statistics and the dimensionality of the feature vector. This paper\nproposes to treat the problem of dynamically selecting an appropriate summary\nstatistic from a given pool of candidate summary statistics as a multi-armed\nbandit problem. This allows approximate Bayesian computation rejection sampling\nto dynamically focus on a distribution over well performing summary statistics\nas opposed to a fixed set of statistics. The proposed method is unique in that\nit does not require any pre-processing and is scalable to a large number of\ncandidate statistics. This enables efficient use of a large library of possible\ntime series summary statistics without prior feature engineering. The proposed\napproach is compared to state-of-the-art methods for summary statistics\nselection using a challenging test problem from the systems biology literature. \n\n"}
{"id": "1805.08671", "contents": "Title: Adding One Neuron Can Eliminate All Bad Local Minima Abstract: One of the main difficulties in analyzing neural networks is the\nnon-convexity of the loss function which may have many bad local minima.\n  In this paper, we study the landscape of neural networks for binary\nclassification tasks. Under mild assumptions, we prove that after adding one\nspecial neuron with a skip connection to the output, or one special neuron per\nlayer, every local minimum is a global minimum. \n\n"}
{"id": "1805.08719", "contents": "Title: Parsimonious Bayesian deep networks Abstract: Combining Bayesian nonparametrics and a forward model selection strategy, we\nconstruct parsimonious Bayesian deep networks (PBDNs) that infer\ncapacity-regularized network architectures from the data and require neither\ncross-validation nor fine-tuning when training the model. One of the two\nessential components of a PBDN is the development of a special infinite-wide\nsingle-hidden-layer neural network, whose number of active hidden units can be\ninferred from the data. The other one is the construction of a greedy\nlayer-wise learning algorithm that uses a forward model selection criterion to\ndetermine when to stop adding another hidden layer. We develop both Gibbs\nsampling and stochastic gradient descent based maximum a posteriori inference\nfor PBDNs, providing state-of-the-art classification accuracy and interpretable\ndata subtypes near the decision boundaries, while maintaining low computational\ncomplexity for out-of-sample prediction. \n\n"}
{"id": "1805.08916", "contents": "Title: Distribution Aware Active Learning Abstract: Discriminative learning machines often need a large set of labeled samples\nfor training. Active learning (AL) settings assume that the learner has the\nfreedom to ask an oracle to label its desired samples. Traditional AL\nalgorithms heuristically choose query samples about which the current learner\nis uncertain. This strategy does not make good use of the structure of the\ndataset at hand and is prone to be misguided by outliers. To alleviate this\nproblem, we propose to distill the structural information into a probabilistic\ngenerative model which acts as a \\emph{teacher} in our model. The active\n\\emph{learner} uses this information effectively at each cycle of active\nlearning. The proposed method is generic and does not depend on the type of\nlearner and teacher. We then suggest a query criterion for active learning that\nis aware of distribution of data and is more robust against outliers. Our\nmethod can be combined readily with several other query criteria for active\nlearning. We provide the formulation and empirically show our idea via toy and\nreal examples. \n\n"}
{"id": "1805.09091", "contents": "Title: Neural networks for post-processing ensemble weather forecasts Abstract: Ensemble weather predictions require statistical post-processing of\nsystematic errors to obtain reliable and accurate probabilistic forecasts.\nTraditionally, this is accomplished with distributional regression models in\nwhich the parameters of a predictive distribution are estimated from a training\nperiod. We propose a flexible alternative based on neural networks that can\nincorporate nonlinear relationships between arbitrary predictor variables and\nforecast distribution parameters that are automatically learned in a\ndata-driven way rather than requiring pre-specified link functions. In a case\nstudy of 2-meter temperature forecasts at surface stations in Germany, the\nneural network approach significantly outperforms benchmark post-processing\nmethods while being computationally more affordable. Key components to this\nimprovement are the use of auxiliary predictor variables and station-specific\ninformation with the help of embeddings. Furthermore, the trained neural\nnetwork can be used to gain insight into the importance of meteorological\nvariables thereby challenging the notion of neural networks as uninterpretable\nblack boxes. Our approach can easily be extended to other statistical\npost-processing and forecasting problems. We anticipate that recent advances in\ndeep learning combined with the ever-increasing amounts of model and\nobservation data will transform the post-processing of numerical weather\nforecasts in the coming decade. \n\n"}
{"id": "1805.09108", "contents": "Title: Deep Learning Estimation of Absorbed Dose for Nuclear Medicine\n  Diagnostics Abstract: The distribution of energy dose from Lu$^{177}$ radiotherapy can be estimated\nby convolving an image of a time-integrated activity distribution with a dose\nvoxel kernel (DVK) consisting of different types of tissues. This fast and\ninacurate approximation is inappropriate for personalized dosimetry as it\nneglects tissue heterogenity. The latter can be calculated using different\nimaging techniques such as CT and SPECT combined with a time consuming\nmonte-carlo simulation. The aim of this study is, for the first time, an\nestimation of DVKs from CT-derived density kernels (DK) via deep learning in\nconvolutional neural networks (CNNs). The proposed CNN achieved, on the test\nset, a mean intersection over union (IOU) of $= 0.86$ after $308$ epochs and a\ncorresponding mean squared error (MSE) $= 1.24 \\cdot 10^{-4}$. This\ngeneralization ability shows that the trained CNN can indeed learn the\ndifficult transfer function from DK to DVK. Future work will evaluate DVKs\nestimated by CNNs with full monte-carlo simulations of a whole body CT to\npredict patient specific voxel dose maps. \n\n"}
{"id": "1805.09114", "contents": "Title: Optimal Transport for structured data with application on graphs Abstract: This work considers the problem of computing distances between structured\nobjects such as undirected graphs, seen as probability distributions in a\nspecific metric space. We consider a new transportation distance (i.e. that\nminimizes a total cost of transporting probability masses) that unveils the\ngeometric nature of the structured objects space. Unlike Wasserstein or\nGromov-Wasserstein metrics that focus solely and respectively on features (by\nconsidering a metric in the feature space) or structure (by seeing structure as\na metric space), our new distance exploits jointly both information, and is\nconsequently called Fused Gromov-Wasserstein (FGW). After discussing its\nproperties and computational aspects, we show results on a graph classification\ntask, where our method outperforms both graph kernels and deep graph\nconvolutional networks. Exploiting further on the metric properties of FGW,\ninteresting geometric objects such as Fr\\'echet means or barycenters of graphs\nare illustrated and discussed in a clustering context. \n\n"}
{"id": "1805.09772", "contents": "Title: Auto-Detection of Safety Issues in Baby Products Abstract: Every year, thousands of people receive consumer product related injuries.\nResearch indicates that online customer reviews can be processed to\nautonomously identify product safety issues. Early identification of safety\nissues can lead to earlier recalls, and thus fewer injuries and deaths. A\ndataset of product reviews from Amazon.com was compiled, along with\n\\emph{SaferProducts.gov} complaints and recall descriptions from the Consumer\nProduct Safety Commission (CPSC) and European Commission Rapid Alert system. A\nsystem was built to clean the collected text and to extract relevant features.\nDimensionality reduction was performed by computing feature relevance through a\nRandom Forest and discarding features with low information gain. Various\nclassifiers were analyzed, including Logistic Regression, SVMs,\nNa{\\\"i}ve-Bayes, Random Forests, and an Ensemble classifier. Experimentation\nwith various features and classifier combinations resulted in a logistic\nregression model with 66\\% precision in the top 50 reviews surfaced. This\nclassifier outperforms all benchmarks set by related literature and consumer\nproduct safety professionals. \n\n"}
{"id": "1805.09909", "contents": "Title: Structure Learning from Time Series with False Discovery Control Abstract: We consider the Granger causal structure learning problem from time series\ndata. Granger causal algorithms predict a 'Granger causal effect' between two\nvariables by testing if prediction error of one decreases significantly in the\nabsence of the other variable among the predictor covariates. Almost all\nexisting Granger causal algorithms condition on a large number of variables\n(all but two variables) to test for effects between a pair of variables. We\npropose a new structure learning algorithm called MMPC-p inspired by the well\nknown MMHC algorithm for non-time series data. We show that under some\nassumptions, the algorithm provides false discovery rate control. The algorithm\nis sound and complete when given access to perfect directed information testing\noracles. We also outline a novel tester for the linear Gaussian case. We show\nthrough our extensive experiments that the MMPC-p algorithm scales to larger\nproblems and has improved statistical power compared to existing state of the\nart for large sparse graphs. We also apply our algorithm on a global\ndevelopment dataset and validate our findings with subject matter experts. \n\n"}
{"id": "1805.09921", "contents": "Title: Meta-Learning Probabilistic Inference For Prediction Abstract: This paper introduces a new framework for data efficient and versatile\nlearning. Specifically: 1) We develop ML-PIP, a general framework for\nMeta-Learning approximate Probabilistic Inference for Prediction. ML-PIP\nextends existing probabilistic interpretations of meta-learning to cover a\nbroad class of methods. 2) We introduce VERSA, an instance of the framework\nemploying a flexible and versatile amortization network that takes few-shot\nlearning datasets as inputs, with arbitrary numbers of shots, and outputs a\ndistribution over task-specific parameters in a single forward pass. VERSA\nsubstitutes optimization at test time with forward passes through inference\nnetworks, amortizing the cost of inference and relieving the need for second\nderivatives during training. 3) We evaluate VERSA on benchmark datasets where\nthe method sets new state-of-the-art results, handles arbitrary numbers of\nshots, and for classification, arbitrary numbers of classes at train and test\ntime. The power of the approach is then demonstrated through a challenging\nfew-shot ShapeNet view reconstruction task. \n\n"}
{"id": "1805.09978", "contents": "Title: Distributed Cartesian Power Graph Segmentation for Graphon Estimation Abstract: We study an extention of total variation denoising over images to over\nCartesian power graphs and its applications to estimating non-parametric\nnetwork models. The power graph fused lasso (PGFL) segments a matrix by\nexploiting a known graphical structure, $G$, over the rows and columns. Our\nmain results shows that for any connected graph, under subGaussian noise, the\nPGFL achieves the same mean-square error rate as 2D total variation denoising\nfor signals of bounded variation. We study the use of the PGFL for denoising an\nobserved network $H$, where we learn the graph $G$ as the $K$-nearest\nneighborhood graph of an estimated metric over the vertices. We provide\ntheoretical and empirical results for estimating graphons, a non-parametric\nexchangeable network model, and compare to the state of the art graphon\nestimation methods. \n\n"}
{"id": "1805.10050", "contents": "Title: Bayesian estimation for large scale multivariate Ornstein-Uhlenbeck\n  model of brain connectivity Abstract: Estimation of reliable whole-brain connectivity is a crucial step towards the\nuse of connectivity information in quantitative approaches to the study of\nneuropsychiatric disorders. When estimating brain connectivity a challenge is\nimposed by the paucity of time samples and the large dimensionality of the\nmeasurements. Bayesian estimation methods for network models offer a number of\nadvantages in this context but are not commonly employed. Here we compare three\ndifferent estimation methods for the multivariate Ornstein-Uhlenbeck model,\nthat has recently gained some popularity for characterizing whole-brain\nconnectivity. We first show that a Bayesian estimation of model parameters\nassuming uniform priors is equivalent to an application of the method of\nmoments. Then, using synthetic data, we show that the Bayesian estimate scales\npoorly with number of nodes in the network as compared to an iterative Lyapunov\noptimization. In particular when the network size is in the order of that used\nfor whole-brain studies (about 100 nodes) the Bayesian method needs about eight\ntimes more time samples than Lyapunov method in order to achieve similar\nestimation accuracy. We also show that the higher estimation accuracy of\nLyapunov method is reflected in a much better classification of individuals\nbased on the estimated connectivity from a real dataset of BOLD fMRI. Finally\nwe show that the poor accuracy of Bayesian method is due to numerical errors,\nwhen the imaginary part of the connectivity estimate gets large compared to its\nreal part. \n\n"}
{"id": "1805.10118", "contents": "Title: Analyzing high-dimensional time-series data using kernel transfer\n  operator eigenfunctions Abstract: Kernel transfer operators, which can be regarded as approximations of\ntransfer operators such as the Perron-Frobenius or Koopman operator in\nreproducing kernel Hilbert spaces, are defined in terms of covariance and\ncross-covariance operators and have been shown to be closely related to the\nconditional mean embedding framework developed by the machine learning\ncommunity. The goal of this paper is to show how the dominant eigenfunctions of\nthese operators in combination with gradient-based optimization techniques can\nbe used to detect long-lived coherent patterns in high-dimensional time-series\ndata. The results will be illustrated using video data and a fluid flow\nexample. \n\n"}
{"id": "1805.10348", "contents": "Title: Guaranteed Simultaneous Asymmetric Tensor Decomposition via\n  Orthogonalized Alternating Least Squares Abstract: Tensor CANDECOMP/PARAFAC (CP) decomposition is an important tool that solves\na wide class of machine learning problems. Existing popular approaches recover\ncomponents one by one, not necessarily in the order of larger components first.\nRecently developed simultaneous power method obtains only a high probability\nrecovery of top $r$ components even when the observed tensor is noiseless. We\npropose a Slicing Initialized Alternating Subspace Iteration (s-ASI) method\nthat is guaranteed to recover top $r$ components ($\\epsilon$-close)\nsimultaneously for (a)symmetric tensors almost surely under the noiseless case\n(with high probability for a bounded noise) using $O(\\log(\\log\n\\frac{1}{\\epsilon}))$ steps of tensor subspace iterations. Our s-ASI method\nintroduces a Slice-Based Initialization that runs\n$O(1/\\log(\\frac{\\lambda_r}{\\lambda_{r+1}}))$ steps of matrix subspace\niterations, where $\\lambda_r$ denotes the r-th top singular value of the\ntensor. We are the first to provide a theoretical guarantee on simultaneous\northogonal asymmetric tensor decomposition. Under the noiseless case, we are\nthe first to provide an \\emph{almost sure} theoretical guarantee on\nsimultaneous orthogonal tensor decomposition. When tensor is noisy, our\nalgorithm for asymmetric tensor is robust to noise smaller than\n$\\min\\{O(\\frac{(\\lambda_r - \\lambda_{r+1})\\epsilon}{\\sqrt{r}}),\nO(\\delta_0\\frac{\\lambda_r -\\lambda_{r+1}}{\\sqrt{d}})\\}$, where $\\delta_0$ is a\nsmall constant proportional to the probability of bad initializations in the\nnoisy setting. \n\n"}
{"id": "1805.10615", "contents": "Title: A Local Information Criterion for Dynamical Systems Abstract: Encoding a sequence of observations is an essential task with many\napplications. The encoding can become highly efficient when the observations\nare generated by a dynamical system. A dynamical system imposes regularities on\nthe observations that can be leveraged to achieve a more efficient code. We\npropose a method to encode a given or learned dynamical system. Apart from its\napplication for encoding a sequence of observations, we propose to use the\ncompression achieved by this encoding as a criterion for model selection. Given\na dataset, different learning algorithms result in different models. But not\nall learned models are equally good. We show that the proposed encoding\napproach can be used to choose the learned model which is closer to the true\nunderlying dynamics. We provide experiments for both encoding and model\nselection, and theoretical results that shed light on why the approach works. \n\n"}
{"id": "1805.10685", "contents": "Title: Legal Document Retrieval using Document Vector Embeddings and Deep\n  Learning Abstract: Domain specific information retrieval process has been a prominent and\nongoing research in the field of natural language processing. Many researchers\nhave incorporated different techniques to overcome the technical and domain\nspecificity and provide a mature model for various domains of interest. The\nmain bottleneck in these studies is the heavy coupling of domain experts, that\nmakes the entire process to be time consuming and cumbersome. In this study, we\nhave developed three novel models which are compared against a golden standard\ngenerated via the on line repositories provided, specifically for the legal\ndomain. The three different models incorporated vector space representations of\nthe legal domain, where document vector generation was done in two different\nmechanisms and as an ensemble of the above two. This study contains the\nresearch being carried out in the process of representing legal case documents\ninto different vector spaces, whilst incorporating semantic word measures and\nnatural language processing techniques. The ensemble model built in this study,\nshows a significantly higher accuracy level, which indeed proves the need for\nincorporation of domain specific semantic similarity measures into the\ninformation retrieval process. This study also shows, the impact of varying\ndistribution of the word similarity measures, against varying document vector\ndimensions, which can lead to improvements in the process of legal information\nretrieval. \n\n"}
{"id": "1805.10727", "contents": "Title: Perceive Your Users in Depth: Learning Universal User Representations\n  from Multiple E-commerce Tasks Abstract: Tasks such as search and recommendation have become increas- ingly important\nfor E-commerce to deal with the information over- load problem. To meet the\ndiverse needs of di erent users, person- alization plays an important role. In\nmany large portals such as Taobao and Amazon, there are a bunch of di erent\ntypes of search and recommendation tasks operating simultaneously for person-\nalization. However, most of current techniques address each task separately.\nThis is suboptimal as no information about users shared across di erent tasks.\nIn this work, we propose to learn universal user representations across\nmultiple tasks for more e ective personalization. In partic- ular, user\nbehavior sequences (e.g., click, bookmark or purchase of products) are modeled\nby LSTM and attention mechanism by integrating all the corresponding content,\nbehavior and temporal information. User representations are shared and learned\nin an end-to-end setting across multiple tasks. Bene ting from better\ninformation utilization of multiple tasks, the user representations are more e\nective to re ect their interests and are more general to be transferred to new\ntasks. We refer this work as Deep User Perception Network (DUPN) and conduct an\nextensive set of o ine and online experiments. Across all tested ve di erent\ntasks, our DUPN consistently achieves better results by giving more e ective\nuser representations. Moreover, we deploy DUPN in large scale operational tasks\nin Taobao. Detailed implementations, e.g., incre- mental model updating, are\nalso provided to address the practical issues for the real world applications. \n\n"}
{"id": "1805.10958", "contents": "Title: Discrete flow posteriors for variational inference in discrete dynamical\n  systems Abstract: Each training step for a variational autoencoder (VAE) requires us to sample\nfrom the approximate posterior, so we usually choose simple (e.g. factorised)\napproximate posteriors in which sampling is an efficient computation that fully\nexploits GPU parallelism. However, such simple approximate posteriors are often\ninsufficient, as they eliminate statistical dependencies in the posterior.\nWhile it is possible to use normalizing flow approximate posteriors for\ncontinuous latents, some problems have discrete latents and strong statistical\ndependencies. The most natural approach to model these dependencies is an\nautoregressive distribution, but sampling from such distributions is inherently\nsequential and thus slow. We develop a fast, parallel sampling procedure for\nautoregressive distributions based on fixed-point iterations which enables\nefficient and accurate variational inference in discrete state-space latent\nvariable dynamical systems. To optimize the variational bound, we considered\ntwo ways to evaluate probabilities: inserting the relaxed samples directly into\nthe pmf for the discrete distribution, or converting to continuous logistic\nlatent variables and interpreting the K-step fixed-point iterations as a\nnormalizing flow. We found that converting to continuous latent variables gave\nconsiderable additional scope for mismatch between the true and approximate\nposteriors, which resulted in biased inferences, we thus used the former\napproach. Using our fast sampling procedure, we were able to realize the\nbenefits of correlated posteriors, including accurate uncertainty estimates for\none cell, and accurate connectivity estimates for multiple cells, in an order\nof magnitude less time. \n\n"}
{"id": "1805.11494", "contents": "Title: Efficient Bayesian Inference for a Gaussian Process Density Model Abstract: We reconsider a nonparametric density model based on Gaussian processes. By\naugmenting the model with latent P\\'olya--Gamma random variables and a latent\nmarked Poisson process we obtain a new likelihood which is conjugate to the\nmodel's Gaussian process prior. The augmented posterior allows for efficient\ninference by Gibbs sampling and an approximate variational mean field approach.\nFor the latter we utilise sparse GP approximations to tackle the infinite\ndimensionality of the problem. The performance of both algorithms and\ncomparisons with other density estimators are demonstrated on artificial and\nreal datasets with up to several thousand data points. \n\n"}
{"id": "1805.11571", "contents": "Title: Human-in-the-Loop Interpretability Prior Abstract: We often desire our models to be interpretable as well as accurate. Prior\nwork on optimizing models for interpretability has relied on easy-to-quantify\nproxies for interpretability, such as sparsity or the number of operations\nrequired. In this work, we optimize for interpretability by directly including\nhumans in the optimization loop. We develop an algorithm that minimizes the\nnumber of user studies to find models that are both predictive and\ninterpretable and demonstrate our approach on several data sets. Our human\nsubjects results show trends towards different proxy notions of\ninterpretability on different datasets, which suggests that different proxies\nare preferred on different tasks. \n\n"}
{"id": "1805.11811", "contents": "Title: Stochastic Zeroth-order Optimization via Variance Reduction method Abstract: Derivative-free optimization has become an important technique used in\nmachine learning for optimizing black-box models. To conduct updates without\nexplicitly computing gradient, most current approaches iteratively sample a\nrandom search direction from Gaussian distribution and compute the estimated\ngradient along that direction. However, due to the variance in the search\ndirection, the convergence rates and query complexities of existing methods\nsuffer from a factor of $d$, where $d$ is the problem dimension. In this paper,\nwe introduce a novel Stochastic Zeroth-order method with Variance Reduction\nunder Gaussian smoothing (SZVR-G) and establish the complexity for optimizing\nnon-convex problems. With variance reduction on both sample space and search\nspace, the complexity of our algorithm is sublinear to $d$ and is strictly\nbetter than current approaches, in both smooth and non-smooth cases. Moreover,\nwe extend the proposed method to the mini-batch version. Our experimental\nresults demonstrate the superior performance of the proposed method over\nexisting derivative-free optimization techniques. Furthermore, we successfully\napply our method to conduct a universal black-box attack to deep neural\nnetworks and present some interesting results. \n\n"}
{"id": "1805.11917", "contents": "Title: The Dynamics of Learning: A Random Matrix Approach Abstract: Understanding the learning dynamics of neural networks is one of the key\nissues for the improvement of optimization algorithms as well as for the\ntheoretical comprehension of why deep neural nets work so well today. In this\npaper, we introduce a random matrix-based framework to analyze the learning\ndynamics of a single-layer linear network on a binary classification problem,\nfor data of simultaneously large dimension and size, trained by gradient\ndescent. Our results provide rich insights into common questions in neural\nnets, such as overfitting, early stopping and the initialization of training,\nthereby opening the door for future studies of more elaborate structures and\nmodels appearing in today's neural networks. \n\n"}
{"id": "1805.12505", "contents": "Title: The long-term impact of ranking algorithms in growing networks Abstract: When we search online for content, we are constantly exposed to rankings. For\nexample, web search results are presented as a ranking, and online bookstores\noften show us lists of best-selling books. While popularity-based ranking\nalgorithms (like Google's PageRank) have been extensively studied in previous\nworks, we still lack a clear understanding of their potential systemic\nconsequences. In this work, we fill this gap by introducing a new model of\nnetwork growth that allows us to compare the properties of the networks\ngenerated under the influence of different ranking algorithms. We show that by\ncorrecting for the omnipresent age bias of popularity-based ranking algorithms,\nthe resulting networks exhibit a significantly larger agreement between the\nnodes' inherent quality and their long-term popularity, and a less concentrated\npopularity distribution. To further promote popularity diversity, we introduce\nand validate a perturbation of the original rankings where a small number of\nrandomly-selected nodes are promoted to the top of the ranking. Our findings\nmove the first steps toward a model-based understanding of the long-term impact\nof popularity-based ranking algorithms, and could be used as an informative\ntool for the design of improved information filtering tools. \n\n"}
{"id": "1805.12547", "contents": "Title: Long-time predictive modeling of nonlinear dynamical systems using\n  neural networks Abstract: We study the use of feedforward neural networks (FNN) to develop models of\nnonlinear dynamical systems from data. Emphasis is placed on predictions at\nlong times, with limited data availability. Inspired by global stability\nanalysis, and the observation of the strong correlation between the local error\nand the maximum singular value of the Jacobian of the ANN, we introduce\nJacobian regularization in the loss function. This regularization suppresses\nthe sensitivity of the prediction to the local error and is shown to improve\naccuracy and robustness. Comparison between the proposed approach and sparse\npolynomial regression is presented in numerical examples ranging from simple\nODE systems to nonlinear PDE systems including vortex shedding behind a\ncylinder, and instability-driven buoyant mixing flow. Furthermore, limitations\nof feedforward neural networks are highlighted, especially when the training\ndata does not include a low dimensional attractor. Strategies of data\naugmentation are presented as remedies to address these issues to a certain\nextent. \n\n"}
{"id": "1806.00530", "contents": "Title: Efficient, Certifiably Optimal Clustering with Applications to Latent\n  Variable Graphical Models Abstract: Motivated by the task of clustering either $d$ variables or $d$ points into\n$K$ groups, we investigate efficient algorithms to solve the Peng-Wei (P-W)\n$K$-means semi-definite programming (SDP) relaxation. The P-W SDP has been\nshown in the literature to have good statistical properties in a variety of\nsettings, but remains intractable to solve in practice. To this end we propose\nFORCE, a new algorithm to solve this SDP relaxation. Compared to the naive\ninterior point method, our method reduces the computational complexity of\nsolving the SDP from $\\tilde{O}(d^7\\log\\epsilon^{-1})$ to\n$\\tilde{O}(d^{6}K^{-2}\\epsilon^{-1})$ arithmetic operations for an\n$\\epsilon$-optimal solution. Our method combines a primal first-order method\nwith a dual optimality certificate search, which when successful, allows for\nearly termination of the primal method. We show for certain variable clustering\nproblems that, with high probability, FORCE is guaranteed to find the optimal\nsolution to the SDP relaxation and provide a certificate of exact optimality.\nAs verified by our numerical experiments, this allows FORCE to solve the P-W\nSDP with dimensions in the hundreds in only tens of seconds. For a variation of\nthe P-W SDP where $K$ is not known a priori a slight modification of FORCE\nreduces the computational complexity of solving this problem as well: from\n$\\tilde{O}(d^7\\log\\epsilon^{-1})$ using a standard SDP solver to\n$\\tilde{O}(d^{4}\\epsilon^{-1})$. \n\n"}
{"id": "1806.00556", "contents": "Title: Intrinsic Isometric Manifold Learning with Application to Localization Abstract: Data living on manifolds commonly appear in many applications. Often this\nresults from an inherently latent low-dimensional system being observed through\nhigher dimensional measurements. We show that under certain conditions, it is\npossible to construct an intrinsic and isometric data representation, which\nrespects an underlying latent intrinsic geometry. Namely, we view the observed\ndata only as a proxy and learn the structure of a latent unobserved intrinsic\nmanifold, whereas common practice is to learn the manifold of the observed\ndata. For this purpose, we build a new metric and propose a method for its\nrobust estimation by assuming mild statistical priors and by using artificial\nneural networks as a mechanism for metric regularization and parametrization.\nWe show successful application to unsupervised indoor localization in ad-hoc\nsensor networks. Specifically, we show that our proposed method facilitates\naccurate localization of a moving agent from imaging data it collects.\nImportantly, our method is applied in the same way to two different imaging\nmodalities, thereby demonstrating its intrinsic and modality-invariant\ncapabilities. \n\n"}
{"id": "1806.00572", "contents": "Title: Autoencoders Learn Generative Linear Models Abstract: We provide a series of results for unsupervised learning with autoencoders.\nSpecifically, we study shallow two-layer autoencoder architectures with shared\nweights. We focus on three generative models for data that are common in\nstatistical machine learning: (i) the mixture-of-gaussians model, (ii) the\nsparse coding model, and (iii) the sparsity model with non-negative\ncoefficients. For each of these models, we prove that under suitable choices of\nhyperparameters, architectures, and initialization, autoencoders learned by\ngradient descent can successfully recover the parameters of the corresponding\nmodel. To our knowledge, this is the first result that rigorously studies the\ndynamics of gradient descent for weight-sharing autoencoders. Our analysis can\nbe viewed as theoretical evidence that shallow autoencoder modules indeed can\nbe used as feature learning mechanisms for a variety of data models, and may\nshed insight on how to train larger stacked architectures with autoencoders as\nbasic building blocks. \n\n"}
{"id": "1806.00811", "contents": "Title: Causal Inference with Noisy and Missing Covariates via Matrix\n  Factorization Abstract: Valid causal inference in observational studies often requires controlling\nfor confounders. However, in practice measurements of confounders may be noisy,\nand can lead to biased estimates of causal effects. We show that we can reduce\nthe bias caused by measurement noise using a large number of noisy measurements\nof the underlying confounders. We propose the use of matrix factorization to\ninfer the confounders from noisy covariates, a flexible and principled\nframework that adapts to missing values, accommodates a wide variety of data\ntypes, and can augment many causal inference methods. We bound the error for\nthe induced average treatment effect estimator and show it is consistent in a\nlinear regression setting, using Exponential Family Matrix Completion\npreprocessing. We demonstrate the effectiveness of the proposed procedure in\nnumerical experiments with both synthetic data and real clinical data. \n\n"}
{"id": "1806.00973", "contents": "Title: Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling Abstract: Learning the minimum/maximum mean among a finite set of distributions is a\nfundamental sub-task in planning, game tree search and reinforcement learning.\nWe formalize this learning task as the problem of sequentially testing how the\nminimum mean among a finite set of distributions compares to a given threshold.\nWe develop refined non-asymptotic lower bounds, which show that optimality\nmandates very different sampling behavior for a low vs high true minimum. We\nshow that Thompson Sampling and the intuitive Lower Confidence Bounds policy\neach nail only one of these cases. We develop a novel approach that we call\nMurphy Sampling. Even though it entertains exclusively low true minima, we\nprove that MS is optimal for both possibilities. We then design advanced\nself-normalized deviation inequalities, fueling more aggressive stopping rules.\nWe complement our theoretical guarantees by experiments showing that MS works\nbest in practice. \n\n"}
{"id": "1806.01264", "contents": "Title: OpenTag: Open Attribute Value Extraction from Product Profiles [Deep\n  Learning, Active Learning, Named Entity Recognition] Abstract: Extraction of missing attribute values is to find values describing an\nattribute of interest from a free text input. Most past related work on\nextraction of missing attribute values work with a closed world assumption with\nthe possible set of values known beforehand, or use dictionaries of values and\nhand-crafted features. How can we discover new attribute values that we have\nnever seen before? Can we do this with limited human annotation or supervision?\nWe study this problem in the context of product catalogs that often have\nmissing values for many attributes of interest.\n  In this work, we leverage product profile information such as titles and\ndescriptions to discover missing values of product attributes. We develop a\nnovel deep tagging model OpenTag for this extraction problem with the following\ncontributions: (1) we formalize the problem as a sequence tagging task, and\npropose a joint model exploiting recurrent neural networks (specifically,\nbidirectional LSTM) to capture context and semantics, and Conditional Random\nFields (CRF) to enforce tagging consistency, (2) we develop a novel attention\nmechanism to provide interpretable explanation for our model's decisions, (3)\nwe propose a novel sampling strategy exploring active learning to reduce the\nburden of human annotation. OpenTag does not use any dictionary or hand-crafted\nfeatures as in prior works. Extensive experiments in real-life datasets in\ndifferent domains show that OpenTag with our active learning strategy discovers\nnew attribute values from as few as 150 annotated samples (reduction in 3.3x\namount of annotation effort) with a high F-score of 83%, outperforming\nstate-of-the-art models. \n\n"}
{"id": "1806.01851", "contents": "Title: Pathwise Derivatives Beyond the Reparameterization Trick Abstract: We observe that gradients computed via the reparameterization trick are in\ndirect correspondence with solutions of the transport equation in the formalism\nof optimal transport. We use this perspective to compute (approximate) pathwise\ngradients for probability distributions not directly amenable to the\nreparameterization trick: Gamma, Beta, and Dirichlet. We further observe that\nwhen the reparameterization trick is applied to the Cholesky-factorized\nmultivariate Normal distribution, the resulting gradients are suboptimal in the\nsense of optimal transport. We derive the optimal gradients and show that they\nhave reduced variance in a Gaussian Process regression task. We demonstrate\nwith a variety of synthetic experiments and stochastic variational inference\ntasks that our pathwise gradients are competitive with other methods. \n\n"}
{"id": "1806.01918", "contents": "Title: A Framework for the construction of upper bounds on the number of affine\n  linear regions of ReLU feed-forward neural networks Abstract: We present a framework to derive upper bounds on the number of regions that\nfeed-forward neural networks with ReLU activation functions are affine linear\non. It is based on an inductive analysis that keeps track of the number of such\nregions per dimensionality of their images within the layers. More precisely,\nthe information about the number regions per dimensionality is pushed through\nthe layers starting with one region of the input dimension of the neural\nnetwork and using a recursion based on an analysis of how many regions per\noutput dimensionality a subsequent layer with a certain width can induce on an\ninput region with a given dimensionality. The final bound on the number of\nregions depends on the number and widths of the layers of the neural network\nand on some additional parameters that were used for the recursion. It is\nstated in terms of the $L1$-norm of the last column of a product of matrices\nand provides a unifying treatment of several previously known bounds: Depending\non the choice of the recursion parameters that determine these matrices, it is\npossible to obtain the bounds from Mont\\'{u}far (2014), (2017) and Serra et.\nal. (2017) as special cases. For the latter, which is the strongest of these\nbounds, the formulation in terms of matrices provides new insight. In\nparticular, by using explicit formulas for a Jordan-like decomposition of the\ninvolved matrices, we achieve new tighter results for the asymptotic setting,\nwhere the number of layers of the same fixed width tends to infinity. \n\n"}
{"id": "1806.02146", "contents": "Title: Adversarial Auto-encoders for Speech Based Emotion Recognition Abstract: Recently, generative adversarial networks and adversarial autoencoders have\ngained a lot of attention in machine learning community due to their\nexceptional performance in tasks such as digit classification and face\nrecognition. They map the autoencoder's bottleneck layer output (termed as code\nvectors) to different noise Probability Distribution Functions (PDFs), that can\nbe further regularized to cluster based on class information. In addition, they\nalso allow a generation of synthetic samples by sampling the code vectors from\nthe mapped PDFs. Inspired by these properties, we investigate the application\nof adversarial autoencoders to the domain of emotion recognition. Specifically,\nwe conduct experiments on the following two aspects: (i) their ability to\nencode high dimensional feature vector representations for emotional utterances\ninto a compressed space (with a minimal loss of emotion class discriminability\nin the compressed space), and (ii) their ability to regenerate synthetic\nsamples in the original feature space, to be later used for purposes such as\ntraining emotion recognition classifiers. We demonstrate the promise of\nadversarial autoencoders with regards to these aspects on the Interactive\nEmotional Dyadic Motion Capture (IEMOCAP) corpus and present our analysis. \n\n"}
{"id": "1806.02382", "contents": "Title: Variational Autoencoder with Arbitrary Conditioning Abstract: We propose a single neural probabilistic model based on variational\nautoencoder that can be conditioned on an arbitrary subset of observed features\nand then sample the remaining features in \"one shot\". The features may be both\nreal-valued and categorical. Training of the model is performed by stochastic\nvariational Bayes. The experimental evaluation on synthetic data, as well as\nfeature imputation and image inpainting problems, shows the effectiveness of\nthe proposed approach and diversity of the generated samples. \n\n"}
{"id": "1806.02557", "contents": "Title: Emoji-Powered Representation Learning for Cross-Lingual Sentiment\n  Classification Abstract: Sentiment classification typically relies on a large amount of labeled data.\nIn practice, the availability of labels is highly imbalanced among different\nlanguages, e.g., more English texts are labeled than texts in any other\nlanguages, which creates a considerable inequality in the quality of related\ninformation services received by users speaking different languages. To tackle\nthis problem, cross-lingual sentiment classification approaches aim to transfer\nknowledge learned from one language that has abundant labeled examples (i.e.,\nthe source language, usually English) to another language with fewer labels\n(i.e., the target language). The source and the target languages are usually\nbridged through off-the-shelf machine translation tools. Through such a\nchannel, cross-language sentiment patterns can be successfully learned from\nEnglish and transferred into the target languages. This approach, however,\noften fails to capture sentiment knowledge specific to the target language, and\nthus compromises the accuracy of the downstream classification task. In this\npaper, we employ emojis, which are widely available in many languages, as a new\nchannel to learn both the cross-language and the language-specific sentiment\npatterns. We propose a novel representation learning method that uses emoji\nprediction as an instrument to learn respective sentiment-aware representations\nfor each language. The learned representations are then integrated to\nfacilitate cross-lingual sentiment classification. The proposed method\ndemonstrates state-of-the-art performance on benchmark datasets, which is\nsustained even when sentiment labels are scarce. \n\n"}
{"id": "1806.02887", "contents": "Title: Residual Unfairness in Fair Machine Learning from Prejudiced Data Abstract: Recent work in fairness in machine learning has proposed adjusting for\nfairness by equalizing accuracy metrics across groups and has also studied how\ndatasets affected by historical prejudices may lead to unfair decision\npolicies. We connect these lines of work and study the residual unfairness that\narises when a fairness-adjusted predictor is not actually fair on the target\npopulation due to systematic censoring of training data by existing biased\npolicies. This scenario is particularly common in the same applications where\nfairness is a concern. We characterize theoretically the impact of such\ncensoring on standard fairness metrics for binary classifiers and provide\ncriteria for when residual unfairness may or may not appear. We prove that,\nunder certain conditions, fairness-adjusted classifiers will in fact induce\nresidual unfairness that perpetuates the same injustices, against the same\ngroups, that biased the data to begin with, thus showing that even\nstate-of-the-art fair machine learning can have a \"bias in, bias out\" property.\nWhen certain benchmark data is available, we show how sample reweighting can\nestimate and adjust fairness metrics while accounting for censoring. We use\nthis to study the case of Stop, Question, and Frisk (SQF) and demonstrate that\nattempting to adjust for fairness perpetuates the same injustices that the\npolicy is infamous for. \n\n"}
{"id": "1806.02924", "contents": "Title: Revisiting Adversarial Risk Abstract: Recent works on adversarial perturbations show that there is an inherent\ntrade-off between standard test accuracy and adversarial accuracy.\nSpecifically, they show that no classifier can simultaneously be robust to\nadversarial perturbations and achieve high standard test accuracy. However,\nthis is contrary to the standard notion that on tasks such as image\nclassification, humans are robust classifiers with low error rate. In this\nwork, we show that the main reason behind this confusion is the inexact\ndefinition of adversarial perturbation that is used in the literature. To fix\nthis issue, we propose a slight, yet important modification to the existing\ndefinition of adversarial perturbation. Based on the modified definition, we\nshow that there is no trade-off between adversarial and standard accuracies;\nthere exist classifiers that are robust and achieve high standard accuracy. We\nfurther study several properties of this new definition of adversarial risk and\nits relation to the existing definition. \n\n"}
{"id": "1806.03085", "contents": "Title: A Stein variational Newton method Abstract: Stein variational gradient descent (SVGD) was recently proposed as a general\npurpose nonparametric variational inference algorithm [Liu & Wang, NIPS 2016]:\nit minimizes the Kullback-Leibler divergence between the target distribution\nand its approximation by implementing a form of functional gradient descent on\na reproducing kernel Hilbert space. In this paper, we accelerate and generalize\nthe SVGD algorithm by including second-order information, thereby approximating\na Newton-like iteration in function space. We also show how second-order\ninformation can lead to more effective choices of kernel. We observe\nsignificant computational gains over the original SVGD algorithm in multiple\ntest cases. \n\n"}
{"id": "1806.03146", "contents": "Title: Neural Message Passing with Edge Updates for Predicting Properties of\n  Molecules and Materials Abstract: Neural message passing on molecular graphs is one of the most promising\nmethods for predicting formation energy and other properties of molecules and\nmaterials. In this work we extend the neural message passing model with an edge\nupdate network which allows the information exchanged between atoms to depend\non the hidden state of the receiving atom. We benchmark the proposed model on\nthree publicly available datasets (QM9, The Materials Project and OQMD) and\nshow that the proposed model yields superior prediction of formation energies\nand other properties on all three datasets in comparison with the best\npublished results. Furthermore we investigate different methods for\nconstructing the graph used to represent crystalline structures and we find\nthat using a graph based on K-nearest neighbors achieves better prediction\naccuracy than using maximum distance cutoff or the Voronoi tessellation graph. \n\n"}
{"id": "1806.03568", "contents": "Title: Explainable Recommendation via Multi-Task Learning in Opinionated Text\n  Data Abstract: Explaining automatically generated recommendations allows users to make more\ninformed and accurate decisions about which results to utilize, and therefore\nimproves their satisfaction. In this work, we develop a multi-task learning\nsolution for explainable recommendation. Two companion learning tasks of user\npreference modeling for recommendation} and \\textit{opinionated content\nmodeling for explanation are integrated via a joint tensor factorization. As a\nresult, the algorithm predicts not only a user's preference over a list of\nitems, i.e., recommendation, but also how the user would appreciate a\nparticular item at the feature level, i.e., opinionated textual explanation.\nExtensive experiments on two large collections of Amazon and Yelp reviews\nconfirmed the effectiveness of our solution in both recommendation and\nexplanation tasks, compared with several existing recommendation algorithms.\nAnd our extensive user study clearly demonstrates the practical value of the\nexplainable recommendations generated by our algorithm. \n\n"}
{"id": "1806.03583", "contents": "Title: IVUS-Net: An Intravascular Ultrasound Segmentation Network Abstract: IntraVascular UltraSound (IVUS) is one of the most effective imaging\nmodalities that provides assistance to experts in order to diagnose and treat\ncardiovascular diseases. We address a central problem in IVUS image analysis\nwith Fully Convolutional Network (FCN): automatically delineate the lumen and\nmedia-adventitia borders in IVUS images, which is crucial to shorten the\ndiagnosis process or benefits a faster and more accurate 3D reconstruction of\nthe artery. Particularly, we propose an FCN architecture, called IVUS-Net,\nfollowed by a post-processing contour extraction step, in order to\nautomatically segments the interior (lumen) and exterior (media-adventitia)\nregions of the human arteries. We evaluated our IVUS-Net on the test set of a\nstandard publicly available dataset containing 326 IVUS B-mode images with two\nmeasurements, namely Jaccard Measure (JM) and Hausdorff Distances (HD). The\nevaluation result shows that IVUS-Net outperforms the state-of-the-art lumen\nand media segmentation methods by 4% to 20% in terms of HD distance. IVUS-Net\nperforms well on images in the test set that contain a significant amount of\nmajor artifacts such as bifurcations, shadows, and side branches that are not\ncommon in the training set. Furthermore, using a modern GPU, IVUS-Net segments\neach IVUS frame only in 0.15 seconds. The proposed work, to the best of our\nknowledge, is the first deep learning based method for segmentation of both the\nlumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the\nbest results without any manual intervention. Code is available at\nhttps://github.com/Kulbear/ivus-segmentation-icsm2018 \n\n"}
{"id": "1806.03723", "contents": "Title: Smallify: Learning Network Size while Training Abstract: As neural networks become widely deployed in different applications and on\ndifferent hardware, it has become increasingly important to optimize inference\ntime and model size along with model accuracy. Most current techniques optimize\nmodel size, model accuracy and inference time in different stages, resulting in\nsuboptimal results and computational inefficiency. In this work, we propose a\nnew technique called Smallify that optimizes all three of these metrics at the\nsame time. Specifically we present a new method to simultaneously optimize\nnetwork size and model performance by neuron-level pruning during training.\nNeuron-level pruning not only produces much smaller networks but also produces\ndense weight matrices that are amenable to efficient inference. By applying our\ntechnique to convolutional as well as fully connected models, we show that\nSmallify can reduce network size by 35X with a 6X improvement in inference time\nwith similar accuracy as models found by traditional training techniques. \n\n"}
{"id": "1806.04398", "contents": "Title: Attentive cross-modal paratope prediction Abstract: Antibodies are a critical part of the immune system, having the function of\ndirectly neutralising or tagging undesirable objects (the antigens) for future\ndestruction. Being able to predict which amino acids belong to the paratope,\nthe region on the antibody which binds to the antigen, can facilitate antibody\ndesign and contribute to the development of personalised medicine. The\nsuitability of deep neural networks has recently been confirmed for this task,\nwith Parapred outperforming all prior physical models. Our contribution is\ntwofold: first, we significantly outperform the computational efficiency of\nParapred by leveraging \\`a trous convolutions and self-attention. Secondly, we\nimplement cross-modal attention by allowing the antibody residues to attend\nover antigen residues. This leads to new state-of-the-art results on this task,\nalong with insightful interpretations. \n\n"}
{"id": "1806.04411", "contents": "Title: Named Entity Recognition with Extremely Limited Data Abstract: Traditional information retrieval treats named entity recognition as a\npre-indexing corpus annotation task, allowing entity tags to be indexed and\nused during search. Named entity taggers themselves are typically trained on\nthousands or tens of thousands of examples labeled by humans.\n  However, there is a long tail of named entities classes, and for these cases,\nlabeled data may be impossible to find or justify financially. We propose\nexploring named entity recognition as a search task, where the named entity\nclass of interest is a query, and entities of that class are the relevant\n\"documents\". What should that query look like? Can we even perform NER-style\nlabeling with tens of labels? This study presents an exploration of CRF-based\nNER models with handcrafted features and of how we might transform them into\nsearch queries. \n\n"}
{"id": "1806.04522", "contents": "Title: Meta-Learning for Stochastic Gradient MCMC Abstract: Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become\nincreasingly popular for simulating posterior samples in large-scale Bayesian\nmodeling. However, existing SG-MCMC schemes are not tailored to any specific\nprobabilistic model, even a simple modification of the underlying dynamical\nsystem requires significant physical intuition. This paper presents the first\nmeta-learning algorithm that allows automated design for the underlying\ncontinuous dynamics of an SG-MCMC sampler. The learned sampler generalizes\nHamiltonian dynamics with state-dependent drift and diffusion, enabling fast\ntraversal and efficient exploration of neural network energy landscapes.\nExperiments validate the proposed approach on both Bayesian fully connected\nneural network and Bayesian recurrent neural network tasks, showing that the\nlearned sampler out-performs generic, hand-designed SG-MCMC algorithms, and\ngeneralizes to different datasets and larger architectures. \n\n"}
{"id": "1806.04613", "contents": "Title: Improving Regression Performance with Distributional Losses Abstract: There is growing evidence that converting targets to soft targets in\nsupervised learning can provide considerable gains in performance. Much of this\nwork has considered classification, converting hard zero-one values to soft\nlabels---such as by adding label noise, incorporating label ambiguity or using\ndistillation. In parallel, there is some evidence from a regression setting in\nreinforcement learning that learning distributions can improve performance. In\nthis work, we investigate the reasons for this improvement, in a regression\nsetting. We introduce a novel distributional regression loss, and similarly\nfind it significantly improves prediction accuracy. We investigate several\ncommon hypotheses, around reducing overfitting and improved representations. We\ninstead find evidence for an alternative hypothesis: this loss is easier to\noptimize, with better behaved gradients, resulting in improved generalization.\nWe provide theoretical support for this alternative hypothesis, by\ncharacterizing the norm of the gradients of this loss. \n\n"}
{"id": "1806.04910", "contents": "Title: Bilevel Programming for Hyperparameter Optimization and Meta-Learning Abstract: We introduce a framework based on bilevel programming that unifies\ngradient-based hyperparameter optimization and meta-learning. We show that an\napproximate version of the bilevel problem can be solved by taking into\nexplicit account the optimization dynamics for the inner objective. Depending\non the specific setting, the outer variables take either the meaning of\nhyperparameters in a supervised learning problem or parameters of a\nmeta-learner. We provide sufficient conditions under which solutions of the\napproximate problem converge to those of the exact problem. We instantiate our\napproach for meta-learning in the case of deep learning where representation\nlayers are treated as hyperparameters shared across a set of training episodes.\nIn experiments, we confirm our theoretical findings, present encouraging\nresults for few-shot learning and contrast the bilevel approach against\nclassical approaches for learning-to-learn. \n\n"}
{"id": "1806.04965", "contents": "Title: The streaming rollout of deep networks - towards fully model-parallel\n  execution Abstract: Deep neural networks, and in particular recurrent networks, are promising\ncandidates to control autonomous agents that interact in real-time with the\nphysical world. However, this requires a seamless integration of temporal\nfeatures into the network's architecture. For the training of and inference\nwith recurrent neural networks, they are usually rolled out over time, and\ndifferent rollouts exist. Conventionally during inference, the layers of a\nnetwork are computed in a sequential manner resulting in sparse temporal\nintegration of information and long response times. In this study, we present a\ntheoretical framework to describe rollouts, the level of model-parallelization\nthey induce, and demonstrate differences in solving specific tasks. We prove\nthat certain rollouts, also for networks with only skip and no recurrent\nconnections, enable earlier and more frequent responses, and show empirically\nthat these early responses have better performance. The streaming rollout\nmaximizes these properties and enables a fully parallel execution of the\nnetwork reducing runtime on massively parallel devices. Finally, we provide an\nopen-source toolbox to design, train, evaluate, and interact with streaming\nrollouts. \n\n"}
{"id": "1806.05139", "contents": "Title: High-Dimensional Inference for Cluster-Based Graphical Models Abstract: Motivated by modern applications in which one constructs graphical models\nbased on a very large number of features, this paper introduces a new class of\ncluster-based graphical models, in which variable clustering is applied as an\ninitial step for reducing the dimension of the feature space. We employ model\nassisted clustering, in which the clusters contain features that are similar to\nthe same unobserved latent variable. Two different cluster-based Gaussian\ngraphical models are considered: the latent variable graph, corresponding to\nthe graphical model associated with the unobserved latent variables, and the\ncluster-average graph, corresponding to the vector of features averaged over\nclusters. Our study reveals that likelihood based inference for the latent\ngraph, not analyzed previously, is analytically intractable. Our main\ncontribution is the development and analysis of alternative estimation and\ninference strategies, for the precision matrix of an unobservable latent vector\n$Z$. We replace the likelihood of the data by an appropriate class of empirical\nrisk functions, that can be specialized to the latent graphical model and to\nthe simpler, but under-analyzed, cluster-average graphical model. The\nestimators thus derived can be used for inference on the graph structure, for\ninstance on edge strength or pattern recovery. Inference is based on the\nasymptotic limits of the entry-wise estimates of the precision matrices\nassociated with the conditional independence graphs under consideration. While\ntaking the uncertainty induced by the clustering step into account, we\nestablish Berry-Esseen central limit theorems for the proposed estimators. It\nis noteworthy that, although the clusters are estimated adaptively from the\ndata, the central limit theorems regarding the entries of the estimated graphs\nare proved under the same conditions one would use if the clusters were\nknown.... \n\n"}
{"id": "1806.05355", "contents": "Title: Scalable Neural Network Compression and Pruning Using Hard Clustering\n  and L1 Regularization Abstract: We propose a simple and easy to implement neural network compression\nalgorithm that achieves results competitive with more complicated\nstate-of-the-art methods. The key idea is to modify the original optimization\nproblem by adding K independent Gaussian priors (corresponding to the k-means\nobjective) over the network parameters to achieve parameter quantization, as\nwell as an L1 penalty to achieve pruning. Unlike many existing\nquantization-based methods, our method uses hard clustering assignments of\nnetwork parameters, which adds minimal change or overhead to standard network\ntraining. We also demonstrate experimentally that tying neural network\nparameters provides less gain in generalization performance than changing\nnetwork architecture and connectivity patterns entirely. \n\n"}
{"id": "1806.05393", "contents": "Title: Dynamical Isometry and a Mean Field Theory of CNNs: How to Train\n  10,000-Layer Vanilla Convolutional Neural Networks Abstract: In recent years, state-of-the-art methods in computer vision have utilized\nincreasingly deep convolutional neural network architectures (CNNs), with some\nof the most successful models employing hundreds or even thousands of layers. A\nvariety of pathologies such as vanishing/exploding gradients make training such\ndeep networks challenging. While residual connections and batch normalization\ndo enable training at these depths, it has remained unclear whether such\nspecialized architecture designs are truly necessary to train deep CNNs. In\nthis work, we demonstrate that it is possible to train vanilla CNNs with ten\nthousand layers or more simply by using an appropriate initialization scheme.\nWe derive this initialization scheme theoretically by developing a mean field\ntheory for signal propagation and by characterizing the conditions for\ndynamical isometry, the equilibration of singular values of the input-output\nJacobian matrix. These conditions require that the convolution operator be an\northogonal transformation in the sense that it is norm-preserving. We present\nan algorithm for generating such random initial orthogonal convolution kernels\nand demonstrate empirically that they enable efficient training of extremely\ndeep architectures. \n\n"}
{"id": "1806.05394", "contents": "Title: Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables\n  Signal Propagation in Recurrent Neural Networks Abstract: Recurrent neural networks have gained widespread use in modeling sequence\ndata across various domains. While many successful recurrent architectures\nemploy a notion of gating, the exact mechanism that enables such remarkable\nperformance is not well understood. We develop a theory for signal propagation\nin recurrent networks after random initialization using a combination of mean\nfield theory and random matrix theory. To simplify our discussion, we introduce\na new RNN cell with a simple gating mechanism that we call the minimalRNN and\ncompare it with vanilla RNNs. Our theory allows us to define a maximum\ntimescale over which RNNs can remember an input. We show that this theory\npredicts trainability for both recurrent architectures. We show that gated\nrecurrent networks feature a much broader, more robust, trainable region than\nvanilla RNNs, which corroborates recent experimental findings. Finally, we\ndevelop a closed-form critical initialization scheme that achieves dynamical\nisometry in both vanilla RNNs and minimalRNNs. We show that this results in\nsignificantly improvement in training dynamics. Finally, we demonstrate that\nthe minimalRNN achieves comparable performance to its more complex\ncounterparts, such as LSTMs or GRUs, on a language modeling task. \n\n"}
{"id": "1806.05438", "contents": "Title: Stochastic Gradient Descent with Exponential Convergence Rates of\n  Expected Classification Errors Abstract: We consider stochastic gradient descent and its averaging variant for binary\nclassification problems in a reproducing kernel Hilbert space. In the\ntraditional analysis using a consistency property of loss functions, it is\nknown that the expected classification error converges more slowly than the\nexpected risk even when assuming a low-noise condition on the conditional label\nprobabilities. Consequently, the resulting rate is sublinear. Therefore, it is\nimportant to consider whether much faster convergence of the expected\nclassification error can be achieved. In recent research, an exponential\nconvergence rate for stochastic gradient descent was shown under a strong\nlow-noise condition but provided theoretical analysis was limited to the\nsquared loss function, which is somewhat inadequate for binary classification\ntasks. In this paper, we show an exponential convergence of the expected\nclassification error in the final phase of the stochastic gradient descent for\na wide class of differentiable convex loss functions under similar assumptions.\nAs for the averaged stochastic gradient descent, we show that the same\nconvergence rate holds from the early phase of training. In experiments, we\nverify our analyses on the $L_2$-regularized logistic regression. \n\n"}
{"id": "1806.05490", "contents": "Title: Inference in Deep Gaussian Processes using Stochastic Gradient\n  Hamiltonian Monte Carlo Abstract: Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian\nProcesses that combine well calibrated uncertainty estimates with the high\nflexibility of multilayer models. One of the biggest challenges with these\nmodels is that exact inference is intractable. The current state-of-the-art\ninference method, Variational Inference (VI), employs a Gaussian approximation\nto the posterior distribution. This can be a potentially poor unimodal\napproximation of the generally multimodal posterior. In this work, we provide\nevidence for the non-Gaussian nature of the posterior and we apply the\nStochastic Gradient Hamiltonian Monte Carlo method to generate samples. To\nefficiently optimize the hyperparameters, we introduce the Moving Window MCEM\nalgorithm. This results in significantly better predictions at a lower\ncomputational cost than its VI counterpart. Thus our method establishes a new\nstate-of-the-art for inference in DGPs. \n\n"}
{"id": "1806.05730", "contents": "Title: Learning Influence-Receptivity Network Structure with Guarantee Abstract: Traditional works on community detection from observations of information\ncascade assume that a single adjacency matrix parametrizes all the observed\ncascades. However, in reality the connection structure usually does not stay\nthe same across cascades. For example, different people have different topics\nof interest, therefore the connection structure depends on the\ninformation/topic content of the cascade. In this paper we consider the case\nwhere we observe a sequence of noisy adjacency matrices triggered by\ninformation/event with different topic distributions. We propose a novel latent\nmodel using the intuition that a connection is more likely to exist between two\nnodes if they are interested in similar topics, which are common with the\ninformation/event. Specifically, we endow each node with two node-topic\nvectors: an influence vector that measures how influential/authoritative they\nare on each topic; and a receptivity vector that measures how\nreceptive/susceptible they are to each topic. We show how these two node-topic\nstructures can be estimated from observed adjacency matrices with theoretical\nguarantee on estimation error, in cases where the topic distributions of the\ninformation/event are known, as well as when they are unknown. Experiments on\nsynthetic and real data demonstrate the effectiveness of our model and superior\nperformance compared to state-of-the-art methods. \n\n"}
{"id": "1806.05823", "contents": "Title: Primal-dual residual networks Abstract: In this work, we propose a deep neural network architecture motivated by\nprimal-dual splitting methods from convex optimization. We show theoretically\nthat there exists a close relation between the derived architecture and\nresidual networks, and further investigate this connection in numerical\nexperiments. Moreover, we demonstrate how our approach can be used to unroll\noptimization algorithms for certain problems with hard constraints. Using the\nexample of speech dequantization, we show that our method can outperform\nclassical splitting methods when both are applied to the same task. \n\n"}
{"id": "1806.05953", "contents": "Title: Controllable Semantic Image Inpainting Abstract: We develop a method for user-controllable semantic image inpainting: Given an\narbitrary set of observed pixels, the unobserved pixels can be imputed in a\nuser-controllable range of possibilities, each of which is semantically\ncoherent and locally consistent with the observed pixels. We achieve this using\na deep generative model bringing together: an encoder which can encode an\narbitrary set of observed pixels, latent variables which are trained to\nrepresent disentangled factors of variations, and a bidirectional PixelCNN\nmodel. We experimentally demonstrate that our method can generate plausible\ninpainting results matching the user-specified semantics, but is still coherent\nwith observed pixels. We justify our choices of architecture and training\nregime through more experiments. \n\n"}
{"id": "1806.06122", "contents": "Title: Fairness Under Composition Abstract: Algorithmic fairness, and in particular the fairness of scoring and\nclassification algorithms, has become a topic of increasing social concern and\nhas recently witnessed an explosion of research in theoretical computer\nscience, machine learning, statistics, the social sciences, and law. Much of\nthe literature considers the case of a single classifier (or scoring function)\nused once, in isolation. In this work, we initiate the study of the fairness\nproperties of systems composed of algorithms that are fair in isolation; that\nis, we study fairness under composition. We identify pitfalls of naive\ncomposition and give general constructions for fair composition, demonstrating\nboth that classifiers that are fair in isolation do not necessarily compose\ninto fair systems and also that seemingly unfair components may be carefully\ncombined to construct fair systems. We focus primarily on the individual\nfairness setting proposed in [Dwork, Hardt, Pitassi, Reingold, Zemel, 2011],\nbut also extend our results to a large class of group fairness definitions\npopular in the recent literature, exhibiting several cases in which group\nfairness definitions give misleading signals under composition. \n\n"}
{"id": "1806.06415", "contents": "Title: Feature Learning and Classification in Neuroimaging: Predicting\n  Cognitive Impairment from Magnetic Resonance Imaging Abstract: Due to the rapid innovation of technology and the desire to find and employ\nbiomarkers for neurodegenerative disease, high-dimensional data classification\nproblems are routinely encountered in neuroimaging studies. To avoid\nover-fitting and to explore relationships between disease and potential\nbiomarkers, feature learning and selection plays an important role in\nclassifier construction and is an important area in machine learning. In this\narticle, we review several important feature learning and selection techniques\nincluding lasso-based methods, PCA, the two-sample t-test, and stacked\nauto-encoders. We compare these approaches using a numerical study involving\nthe prediction of Alzheimer's disease from Magnetic Resonance Imaging. \n\n"}
{"id": "1806.06423", "contents": "Title: A Novel Hybrid Machine Learning Model for Auto-Classification of Retinal\n  Diseases Abstract: Automatic clinical diagnosis of retinal diseases has emerged as a promising\napproach to facilitate discovery in areas with limited access to specialists.\nWe propose a novel visual-assisted diagnosis hybrid model based on the support\nvector machine (SVM) and deep neural networks (DNNs). The model incorporates\ncomplementary strengths of DNNs and SVM. Furthermore, we present a new clinical\nretina label collection for ophthalmology incorporating 32 retina diseases\nclasses. Using EyeNet, our model achieves 89.73% diagnosis accuracy and the\nmodel performance is comparable to the professional ophthalmologists. \n\n"}
{"id": "1806.06535", "contents": "Title: Modeling Musical Taste Evolution with Recurrent Neural Networks Abstract: Finding the music of the moment can often be a challenging problem, even for\nwell-versed music listeners. Musical tastes are constantly in flux, and the\nproblem of developing computational models for musical taste dynamics presents\na rich and nebulous problem space. A variety of factors all play some role in\ndetermining preferences (e.g., popularity, musicological, social, geographical,\ngenerational), and these factors vary across different listeners and contexts.\nIn this paper, we leverage a massive dataset on internet radio station creation\nfrom a large music streaming company in order to develop computational models\nof listener taste evolution. We delve deep into the complexities of this\ndomain, identifying some of the unique challenges that it presents, and develop\na model utilizing recurrent neural networks. We apply our model to the problem\nof next station prediction and show that it not only outperforms several\nbaselines, but excels at long tail music personalization, particularly by\nlearning the long-term dependency structure of listener music preference\nevolution. \n\n"}
{"id": "1806.06583", "contents": "Title: Nonparametric Topic Modeling with Neural Inference Abstract: This work focuses on combining nonparametric topic models with Auto-Encoding\nVariational Bayes (AEVB). Specifically, we first propose iTM-VAE, where the\ntopics are treated as trainable parameters and the document-specific topic\nproportions are obtained by a stick-breaking construction. The inference of\niTM-VAE is modeled by neural networks such that it can be computed in a simple\nfeed-forward manner. We also describe how to introduce a hyper-prior into\niTM-VAE so as to model the uncertainty of the prior parameter. Actually, the\nhyper-prior technique is quite general and we show that it can be applied to\nother AEVB based models to alleviate the {\\it collapse-to-prior} problem\nelegantly. Moreover, we also propose HiTM-VAE, where the document-specific\ntopic distributions are generated in a hierarchical manner. HiTM-VAE is even\nmore flexible and can generate topic distributions with better variability.\nExperimental results on 20News and Reuters RCV1-V2 datasets show that the\nproposed models outperform the state-of-the-art baselines significantly. The\nadvantages of the hyper-prior technique and the hierarchical model construction\nare also confirmed by experiments. \n\n"}
{"id": "1806.06616", "contents": "Title: Comparison-Based Random Forests Abstract: Assume we are given a set of items from a general metric space, but we\nneither have access to the representation of the data nor to the distances\nbetween data points. Instead, suppose that we can actively choose a triplet of\nitems (A,B,C) and ask an oracle whether item A is closer to item B or to item\nC. In this paper, we propose a novel random forest algorithm for regression and\nclassification that relies only on such triplet comparisons. In the theory part\nof this paper, we establish sufficient conditions for the consistency of such a\nforest. In a set of comprehensive experiments, we then demonstrate that the\nproposed random forest is efficient both for classification and regression. In\nparticular, it is even competitive with other methods that have direct access\nto the metric representation of the data. \n\n"}
{"id": "1806.06676", "contents": "Title: Towards multi-instrument drum transcription Abstract: Automatic drum transcription, a subtask of the more general automatic music\ntranscription, deals with extracting drum instrument note onsets from an audio\nsource. Recently, progress in transcription performance has been made using\nnon-negative matrix factorization as well as deep learning methods. However,\nthese works primarily focus on transcribing three drum instruments only: snare\ndrum, bass drum, and hi-hat. Yet, for many applications, the ability to\ntranscribe more drum instruments which make up standard drum kits used in\nwestern popular music would be desirable. In this work, convolutional and\nconvolutional recurrent neural networks are trained to transcribe a wider range\nof drum instruments. First, the shortcomings of publicly available datasets in\nthis context are discussed. To overcome these limitations, a larger synthetic\ndataset is introduced. Then, methods to train models using the new dataset\nfocusing on generalization to real world data are investigated. Finally, the\ntrained models are evaluated on publicly available datasets and results are\ndiscussed. The contributions of this work comprise: (i.) a large-scale\nsynthetic dataset for drum transcription, (ii.) first steps towards an\nautomatic drum transcription system that supports a larger range of instruments\nby evaluating and discussing training setups and the impact of datasets in this\ncontext, and (iii.) a publicly available set of trained models for drum\ntranscription. Additional materials are available at\nhttp://ifs.tuwien.ac.at/~vogl/dafx2018 \n\n"}
{"id": "1806.07788", "contents": "Title: Random Feature Stein Discrepancies Abstract: Computable Stein discrepancies have been deployed for a variety of\napplications, ranging from sampler selection in posterior inference to\napproximate Bayesian inference to goodness-of-fit testing. Existing\nconvergence-determining Stein discrepancies admit strong theoretical guarantees\nbut suffer from a computational cost that grows quadratically in the sample\nsize. While linear-time Stein discrepancies have been proposed for\ngoodness-of-fit testing, they exhibit avoidable degradations in testing power\n-- even when power is explicitly optimized. To address these shortcomings, we\nintroduce feature Stein discrepancies ($\\Phi$SDs), a new family of quality\nmeasures that can be cheaply approximated using importance sampling. We show\nhow to construct $\\Phi$SDs that provably determine the convergence of a sample\nto its target and develop high-accuracy approximations -- random $\\Phi$SDs\n(R$\\Phi$SDs) -- which are computable in near-linear time. In our experiments\nwith sampler selection for approximate posterior inference and goodness-of-fit\ntesting, R$\\Phi$SDs perform as well or better than quadratic-time KSDs while\nbeing orders of magnitude faster to compute. \n\n"}
{"id": "1806.08301", "contents": "Title: The Online Saddle Point Problem and Online Convex Optimization with\n  Knapsacks Abstract: We study the online saddle point problem, an online learning problem where at\neach iteration a pair of actions need to be chosen without knowledge of the\ncurrent and future (convex-concave) payoff functions. The objective is to\nminimize the gap between the cumulative payoffs and the saddle point value of\nthe aggregate payoff function, which we measure using a metric called\n\"SP-Regret\". The problem generalizes the online convex optimization framework\nbut here we must ensure both players incur cumulative payoffs close to that of\nthe Nash equilibrium of the sum of the games. We propose an algorithm that\nachieves SP-Regret proportional to $\\sqrt{\\ln(T)T}$ in the general case, and\n$\\log(T)$ SP-Regret for the strongly convex-concave case. We also consider the\nspecial case where the payoff functions are bilinear and the decision sets are\nthe probability simplex. In this setting we are able to design algorithms that\nreduce the bounds on SP-Regret from a linear dependence in the dimension of the\nproblem to a \\textit{logarithmic} one. We also study the problem under bandit\nfeedback and provide an algorithm that achieves sublinear SP-Regret. We then\nconsider an online convex optimization with knapsacks problem motivated by a\nwide variety of applications such as: dynamic pricing, auctions, and\ncrowdsourcing. We relate this problem to the online saddle point problem and\nestablish $O(\\sqrt{T})$ regret using a primal-dual algorithm. \n\n"}
{"id": "1806.08541", "contents": "Title: Visualizing and Understanding Deep Neural Networks in CTR Prediction Abstract: Although deep learning techniques have been successfully applied to many\ntasks, interpreting deep neural network models is still a big challenge to us.\nRecently, many works have been done on visualizing and analyzing the mechanism\nof deep neural networks in the areas of image processing and natural language\nprocessing. In this paper, we present our approaches to visualize and\nunderstand deep neural networks for a very important commercial task--CTR\n(Click-through rate) prediction. We conduct experiments on the productive data\nfrom our online advertising system with daily varying distribution. To\nunderstand the mechanism and the performance of the model, we inspect the\nmodel's inner status at neuron level. Also, a probe approach is implemented to\nmeasure the layer-wise performance of the model. Moreover, to measure the\ninfluence from the input features, we calculate saliency scores based on the\nback-propagated gradients. Practical applications are also discussed, for\nexample, in understanding, monitoring, diagnosing and refining models and\nalgorithms. \n\n"}
{"id": "1806.08977", "contents": "Title: Explainable Outfit Recommendation with Joint Outfit Matching and Comment\n  Generation Abstract: Most previous work on outfit recommendation focuses on designing visual\nfeatures to enhance recommendations. Existing work neglects user comments of\nfashion items, which have been proved to be effective in generating\nexplanations along with better recommendation results. We propose a novel\nneural network framework, neural outfit recommendation (NOR), that\nsimultaneously provides outfit recommendations and generates abstractive\ncomments. NOR consists of two parts: outfit matching and comment generation.\nFor outfit matching, we propose a convolutional neural network with a mutual\nattention mechanism to extract visual features. The visual features are then\ndecoded into a rating score for the matching prediction. For abstractive\ncomment generation, we propose a gated recurrent neural network with a\ncross-modality attention mechanism to transform visual features into a concise\nsentence. The two parts are jointly trained based on a multi-task learning\nframework in an end-to-end back-propagation paradigm. Extensive experiments\nconducted on an existing dataset and a collected real-world dataset show NOR\nachieves significant improvements over state-of-the-art baselines for outfit\nrecommendation. Meanwhile, our generated comments achieve impressive ROUGE and\nBLEU scores in comparison to human-written comments. The generated comments can\nbe regarded as explanations for the recommendation results. We release the\ndataset and code to facilitate future research. \n\n"}
{"id": "1806.09708", "contents": "Title: Mimic and Classify : A meta-algorithm for Conditional Independence\n  Testing Abstract: Given independent samples generated from the joint distribution\n$p(\\mathbf{x},\\mathbf{y},\\mathbf{z})$, we study the problem of Conditional\nIndependence (CI-Testing), i.e., whether the joint equals the CI distribution\n$p^{CI}(\\mathbf{x},\\mathbf{y},\\mathbf{z})= p(\\mathbf{z})\np(\\mathbf{y}|\\mathbf{z})p(\\mathbf{x}|\\mathbf{z})$ or not. We cast this problem\nunder the purview of the proposed, provable meta-algorithm, \"Mimic and\nClassify\", which is realized in two-steps: (a) Mimic the CI distribution close\nenough to recover the support, and (b) Classify to distinguish the joint and\nthe CI distribution. Thus, as long as we have a good generative model and a\ngood classifier, we potentially have a sound CI Tester. With this modular\nparadigm, CI Testing becomes amiable to be handled by state-of-the-art, both\ngenerative and classification methods from the modern advances in Deep\nLearning, which in general can handle issues related to curse of dimensionality\nand operation in small sample regime. We show intensive numerical experiments\non synthetic and real datasets where new mimic methods such conditional GANs,\nRegression with Neural Nets, outperform the current best CI Testing performance\nin the literature. Our theoretical results provide analysis on the estimation\nof null distribution as well as allow for general measures, i.e., when either\nsome of the random variables are discrete and some are continuous or when one\nor more of them are discrete-continuous mixtures. \n\n"}
{"id": "1806.10234", "contents": "Title: Scalable Gaussian Process Inference with Finite-data Mean and Variance\n  Guarantees Abstract: Gaussian processes (GPs) offer a flexible class of priors for nonparametric\nBayesian regression, but popular GP posterior inference methods are typically\nprohibitively slow or lack desirable finite-data guarantees on quality. We\ndevelop an approach to scalable approximate GP regression with finite-data\nguarantees on the accuracy of pointwise posterior mean and variance estimates.\nOur main contribution is a novel objective for approximate inference in the\nnonparametric setting: the preconditioned Fisher (pF) divergence. We show that\nunlike the Kullback--Leibler divergence (used in variational inference), the pF\ndivergence bounds the 2-Wasserstein distance, which in turn provides tight\nbounds the pointwise difference of the mean and variance functions. We\ndemonstrate that, for sparse GP likelihood approximations, we can minimize the\npF divergence efficiently. Our experiments show that optimizing the pF\ndivergence has the same computational requirements as variational sparse GPs\nwhile providing comparable empirical performance--in addition to our novel\nfinite-data quality guarantees. \n\n"}
{"id": "1806.10701", "contents": "Title: Empirical Risk Minimization and Stochastic Gradient Descent for\n  Relational Data Abstract: Empirical risk minimization is the main tool for prediction problems, but its\nextension to relational data remains unsolved. We solve this problem using\nrecent ideas from graph sampling theory to (i) define an empirical risk for\nrelational data and (ii) obtain stochastic gradients for this empirical risk\nthat are automatically unbiased. This is achieved by considering the method by\nwhich data is sampled from a graph as an explicit component of model design. By\nintegrating fast implementations of graph sampling schemes with standard\nautomatic differentiation tools, we provide an efficient turnkey solver for the\nrisk minimization problem. We establish basic theoretical properties of the\nprocedure. Finally, we demonstrate relational ERM with application to two\nnon-standard problems: one-stage training for semi-supervised node\nclassification, and learning embedding vectors for vertex attributes.\nExperiments confirm that the turnkey inference procedure is effective in\npractice, and that the sampling scheme used for model specification has a\nstrong effect on model performance. Code is available at\nhttps://github.com/wooden-spoon/relational-ERM. \n\n"}
{"id": "1806.11326", "contents": "Title: Unsupervised Detection and Explanation of Latent-class Contextual\n  Anomalies Abstract: Detecting and explaining anomalies is a challenging effort. This holds\nespecially true when data exhibits strong dependencies and single measurements\nneed to be assessed and analyzed in their respective context. In this work, we\nconsider scenarios where measurements are non-i.i.d, i.e. where samples are\ndependent on corresponding discrete latent variables which are connected\nthrough some given dependency structure, the contextual information. Our\ncontribution is twofold: (i) Building atop of support vector data description\n(SVDD), we derive a method able to cope with latent-class dependency structure\nthat can still be optimized efficiently. We further show that our approach\nneatly generalizes vanilla SVDD as well as k-means and conditional random\nfields (CRF) and provide a corresponding probabilistic interpretation. (ii) In\nunsupervised scenarios where it is not possible to quantify the accuracy of an\nanomaly detector, having an human-interpretable solution is the key to success.\nBased on deep Taylor decomposition and a reformulation of our trained anomaly\ndetector as a neural network, we are able to backpropagate predictions to\npixel-domain and thus identify features and regions of high relevance. We\ndemonstrate the usefulness of our novel approach on toy data with known\nspatio-temporal structure and successfully validate on synthetic as well as\nreal world off-shore data from the oil industry. \n\n"}
{"id": "1806.11544", "contents": "Title: Nonparametric learning from Bayesian models with randomized objective\n  functions Abstract: Bayesian learning is built on an assumption that the model space contains a\ntrue reflection of the data generating mechanism. This assumption is\nproblematic, particularly in complex data environments. Here we present a\nBayesian nonparametric approach to learning that makes use of statistical\nmodels, but does not assume that the model is true. Our approach has provably\nbetter properties than using a parametric model and admits a Monte Carlo\nsampling scheme that can afford massive scalability on modern computer\narchitectures. The model-based aspect of learning is particularly attractive\nfor regularizing nonparametric inference when the sample size is small, and\nalso for correcting approximate approaches such as variational Bayes (VB). We\ndemonstrate the approach on a number of examples including VB classifiers and\nBayesian random forests. \n\n"}
{"id": "1807.01202", "contents": "Title: Generating Multi-Categorical Samples with Generative Adversarial\n  Networks Abstract: We propose a method to train generative adversarial networks on mutivariate\nfeature vectors representing multiple categorical values. In contrast to the\ncontinuous domain, where GAN-based methods have delivered considerable results,\nGANs struggle to perform equally well on discrete data. We propose and compare\nseveral architectures based on multiple (Gumbel) softmax output layers taking\ninto account the structure of the data. We evaluate the performance of our\narchitecture on datasets with different sparsity, number of features, ranges of\ncategorical values, and dependencies among the features. Our proposed\narchitecture and method outperforms existing models. \n\n"}
{"id": "1807.01422", "contents": "Title: Diagonal Discriminant Analysis with Feature Selection for High\n  Dimensional Data Abstract: We introduce a new method of performing high dimensional discriminant\nanalysis, which we call multiDA. We achieve this by constructing a hybrid model\nthat seamlessly integrates a multiclass diagonal discriminant analysis model\nand feature selection components. Our feature selection component naturally\nsimplifies to weights which are simple functions of likelihood ratio statistics\nallowing natural comparisons with traditional hypothesis testing methods. We\nprovide heuristic arguments suggesting desirable asymptotic properties of our\nalgorithm with regards to feature selection. We compare our method with several\nother approaches, showing marked improvements in regard to prediction accuracy,\ninterpretability of chosen features, and algorithm run time. We demonstrate\nsuch strengths of our model by showing strong classification performance on\npublicly available high dimensional datasets, as well as through multiple\nsimulation studies. We make an R package available implementing our approach. \n\n"}
{"id": "1807.02125", "contents": "Title: Scalable Gaussian Processes with Grid-Structured Eigenfunctions\n  (GP-GRIEF) Abstract: We introduce a kernel approximation strategy that enables computation of the\nGaussian process log marginal likelihood and all hyperparameter derivatives in\n$\\mathcal{O}(p)$ time. Our GRIEF kernel consists of $p$ eigenfunctions found\nusing a Nystrom approximation from a dense Cartesian product grid of inducing\npoints. By exploiting algebraic properties of Kronecker and Khatri-Rao tensor\nproducts, computational complexity of the training procedure can be practically\nindependent of the number of inducing points. This allows us to use arbitrarily\nmany inducing points to achieve a globally accurate kernel approximation, even\nin high-dimensional problems. The fast likelihood evaluation enables type-I or\nII Bayesian inference on large-scale datasets. We benchmark our algorithms on\nreal-world problems with up to two-million training points and $10^{33}$\ninducing points. \n\n"}
{"id": "1807.02150", "contents": "Title: Scalable Recommender Systems through Recursive Evidence Chains Abstract: Recommender systems can be formulated as a matrix completion problem,\npredicting ratings from user and item parameter vectors. Optimizing these\nparameters by subsampling data becomes difficult as the number of users and\nitems grows. We develop a novel approach to generate all latent variables on\ndemand from the ratings matrix itself and a fixed pool of parameters. We\nestimate missing ratings using chains of evidence that link them to a small set\nof prototypical users and items. Our model automatically addresses the\ncold-start and online learning problems by combining information across both\nusers and items. We investigate the scaling behavior of this model, and\ndemonstrate competitive results with respect to current matrix factorization\ntechniques in terms of accuracy and convergence speed. \n\n"}
{"id": "1807.02314", "contents": "Title: JUMPER: Learning When to Make Classification Decisions in Reading Abstract: In early years, text classification is typically accomplished by\nfeature-based machine learning models; recently, deep neural networks, as a\npowerful learning machine, make it possible to work with raw input as the text\nstands. However, exiting end-to-end neural networks lack explicit\ninterpretation of the prediction. In this paper, we propose a novel framework,\nJUMPER, inspired by the cognitive process of text reading, that models text\nclassification as a sequential decision process. Basically, JUMPER is a neural\nsystem that scans a piece of text sequentially and makes classification\ndecisions at the time it wishes. Both the classification result and when to\nmake the classification are part of the decision process, which is controlled\nby a policy network and trained with reinforcement learning. Experimental\nresults show that a properly trained JUMPER has the following properties: (1)\nIt can make decisions whenever the evidence is enough, therefore reducing total\ntext reading by 30-40% and often finding the key rationale of prediction. (2)\nIt achieves classification accuracy better than or comparable to\nstate-of-the-art models in several benchmark and industrial datasets. \n\n"}
{"id": "1807.02326", "contents": "Title: Cause-Effect Deep Information Bottleneck For Systematically Missing\n  Covariates Abstract: Estimating the causal effects of an intervention from high-dimensional\nobservational data is difficult due to the presence of confounding. The task is\noften complicated by the fact that we may have a systematic missingness in our\ndata at test time. Our approach uses the information bottleneck to perform a\nlow-dimensional compression of covariates by explicitly considering the\nrelevance of information. Based on the sufficiently reduced covariate, we\ntransfer the relevant information to cases where data is missing at test time,\nallowing us to reliably and accurately estimate the effects of an intervention,\neven where data is incomplete. Our results on causal inference benchmarks and a\nreal application for treating sepsis show that our method achieves state-of-the\nart performance, without sacrificing interpretability. \n\n"}
{"id": "1807.02374", "contents": "Title: A Structured Prediction Approach for Label Ranking Abstract: We propose to solve a label ranking problem as a structured output regression\ntask. We adopt a least square surrogate loss approach that solves a supervised\nlearning problem in two steps: the regression step in a well-chosen feature\nspace and the pre-image step. We use specific feature maps/embeddings for\nranking data, which convert any ranking/permutation into a vector\nrepresentation. These embeddings are all well-tailored for our approach, either\nby resulting in consistent estimators, or by solving trivially the pre-image\nproblem which is often the bottleneck in structured prediction. We also propose\ntheir natural extension to the case of partial rankings and prove their\nefficiency on real-world datasets. \n\n"}
{"id": "1807.02962", "contents": "Title: Learning to Index for Nearest Neighbor Search Abstract: In this study, we present a novel ranking model based on learning\nneighborhood relationships embedded in the index space. Given a query point,\nconventional approximate nearest neighbor search calculates the distances to\nthe cluster centroids, before ranking the clusters from near to far based on\nthe distances. The data indexed in the top-ranked clusters are retrieved and\ntreated as the nearest neighbor candidates for the query. However, the loss of\nquantization between the data and cluster centroids will inevitably harm the\nsearch accuracy. To address this problem, the proposed model ranks clusters\nbased on their nearest neighbor probabilities rather than the query-centroid\ndistances. The nearest neighbor probabilities are estimated by employing neural\nnetworks to characterize the neighborhood relationships, i.e., the density\nfunction of nearest neighbors with respect to the query. The proposed\nprobability-based ranking can replace the conventional distance-based ranking\nfor finding candidate clusters, and the predicted probability can be used to\ndetermine the data quantity to be retrieved from the candidate cluster. Our\nexperimental results demonstrated that the proposed ranking model could boost\nthe search performance effectively in billion-scale datasets. \n\n"}
{"id": "1807.03469", "contents": "Title: Pairwise Covariates-adjusted Block Model for Community Detection Abstract: One of the most fundamental problems in network study is community detection.\nThe stochastic block model (SBM) is a widely used model, for which various\nestimation methods have been developed with their community detection\nconsistency results unveiled. However, the SBM is restricted by the strong\nassumption that all nodes in the same community are stochastically equivalent,\nwhich may not be suitable for practical applications. We introduce a pairwise\ncovariates-adjusted stochastic block model (PCABM), a generalization of SBM\nthat incorporates pairwise covariate information. We study the maximum\nlikelihood estimates of the coefficients for the covariates as well as the\ncommunity assignments. It is shown that both the coefficient estimates of the\ncovariates and the community assignments are consistent under suitable sparsity\nconditions. Spectral clustering with adjustment (SCWA) is introduced to\nefficiently solve PCABM. Under certain conditions, we derive the error bound of\ncommunity detection under SCWA and show that it is community detection\nconsistent. In addition, we investigate model selection in terms of the number\nof communities and feature selection for the pairwise covariates, and propose\ntwo corresponding algorithms. PCABM compares favorably with the SBM or\ndegree-corrected stochastic block model (DCBM) under a wide range of simulated\nand real networks when covariate information is accessible. \n\n"}
{"id": "1807.03723", "contents": "Title: Understanding VAEs in Fisher-Shannon Plane Abstract: In information theory, Fisher information and Shannon information (entropy)\nare respectively used to quantify the uncertainty associated with the\ndistribution modeling and the uncertainty in specifying the outcome of given\nvariables. These two quantities are complementary and are jointly applied to\ninformation behavior analysis in most cases. The uncertainty property in\ninformation asserts a fundamental trade-off between Fisher information and\nShannon information, which enlightens us the relationship between the encoder\nand the decoder in variational auto-encoders (VAEs). In this paper, we\ninvestigate VAEs in the Fisher-Shannon plane and demonstrate that the\nrepresentation learning and the log-likelihood estimation are intrinsically\nrelated to these two information quantities. Through extensive qualitative and\nquantitative experiments, we provide with a better comprehension of VAEs in\ntasks such as high-resolution reconstruction, and representation learning in\nthe perspective of Fisher information and Shannon information. We further\npropose a variant of VAEs, termed as Fisher auto-encoder (FAE), for practical\nneeds to balance Fisher information and Shannon information. Our experimental\nresults have demonstrated its promise in improving the reconstruction accuracy\nand avoiding the non-informative latent code as occurred in previous works. \n\n"}
{"id": "1807.03905", "contents": "Title: Limits to Surprise in Recommender Systems Abstract: In this study, we address the challenge of measuring the ability of a\nrecommender system to make surprising recommendations. Although current\nevaluation methods make it possible to determine if two algorithms can make\nrecommendations with a significant difference in their average surprise\nmeasure, it could be of interest to our community to know how competent an\nalgorithm is at embedding surprise in its recommendations, without having to\nresort to making a direct comparison with another algorithm. We argue that a)\nsurprise is a finite resource in a recommender system, b) there is a limit to\nhow much surprise any algorithm can embed in a recommendation, and c) this\nlimit can provide us with a scale against which the performance of any\nalgorithm can be measured. By exploring these ideas, it is possible to define\nthe concepts of maximum and minimum potential surprise and design a surprise\nmetric called \"normalised surprise\" that employs these limits to potential\nsurprise. Two experiments were conducted to test the proposed metric. The aim\nof the first was to validate the quality of the estimates of minimum and\nmaximum potential surprise produced by a greedy algorithm. The purpose of the\nsecond experiment was to analyse the behaviour of the proposed metric using the\nMovieLens dataset. The results confirmed the behaviour that was expected, and\nshowed that the proposed surprise metric is both effective and consistent for\ndiffering choices of recommendation algorithms, data representations and\ndistance functions. \n\n"}
{"id": "1807.03929", "contents": "Title: Quantification under prior probability shift: the ratio estimator and\n  its extensions Abstract: The quantification problem consists of determining the prevalence of a given\nlabel in a target population. However, one often has access to the labels in a\nsample from the training population but not in the target population. A common\nassumption in this situation is that of prior probability shift, that is, once\nthe labels are known, the distribution of the features is the same in the\ntraining and target populations. In this paper, we derive a new lower bound for\nthe risk of the quantification problem under the prior shift assumption.\nComplementing this lower bound, we present a new approximately minimax class of\nestimators, ratio estimators, which generalize several previous proposals in\nthe literature. Using a weaker version of the prior shift assumption, which can\nbe tested, we show that ratio estimators can be used to build confidence\nintervals for the quantification problem. We also extend the ratio estimator so\nthat it can: (i) incorporate labels from the target population, when they are\navailable and (ii) estimate how the prevalence of positive labels varies\naccording to a function of certain covariates. \n\n"}
{"id": "1807.04183", "contents": "Title: Optimization over Continuous and Multi-dimensional Decisions with\n  Observational Data Abstract: We consider the optimization of an uncertain objective over continuous and\nmulti-dimensional decision spaces in problems in which we are only provided\nwith observational data. We propose a novel algorithmic framework that is\ntractable, asymptotically consistent, and superior to comparable methods on\nexample problems. Our approach leverages predictive machine learning methods\nand incorporates information on the uncertainty of the predicted outcomes for\nthe purpose of prescribing decisions. We demonstrate the efficacy of our method\non examples involving both synthetic and real data sets. \n\n"}
{"id": "1807.04833", "contents": "Title: DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate\n  Dependency Structures Abstract: We present a non-parametric Bayesian latent variable model capable of\nlearning dependency structures across dimensions in a multivariate setting. Our\napproach is based on flexible Gaussian process priors for the generative\nmappings and interchangeable Dirichlet process priors to learn the structure.\nThe introduction of the Dirichlet process as a specific structural prior allows\nour model to circumvent issues associated with previous Gaussian process latent\nvariable models. Inference is performed by deriving an efficient variational\nbound on the marginal log-likelihood on the model. \n\n"}
{"id": "1807.05540", "contents": "Title: A Survey on Expert Recommendation in Community Question Answering Abstract: Community question answering (CQA) represents the type of Web applications\nwhere people can exchange knowledge via asking and answering questions. One\nsignificant challenge of most real-world CQA systems is the lack of effective\nmatching between questions and the potential good answerers, which adversely\naffects the efficient knowledge acquisition and circulation. On the one hand, a\nrequester might experience many low-quality answers without receiving a quality\nresponse in a brief time, on the other hand, an answerer might face numerous\nnew questions without being able to identify their questions of interest\nquickly. Under this situation, expert recommendation emerges as a promising\ntechnique to address the above issues. Instead of passively waiting for users\nto browse and find their questions of interest, an expert recommendation method\nraises the attention of users to the appropriate questions actively and\npromptly. The past few years have witnessed considerable efforts that address\nthe expert recommendation problem from different perspectives. These methods\nall have their issues that need to be resolved before the advantages of expert\nrecommendation can be fully embraced. In this survey, we first present an\noverview of the research efforts and state-of-the-art techniques for the expert\nrecommendation in CQA. We next summarize and compare the existing methods\nconcerning their advantages and shortcomings, followed by discussing the open\nissues and future research directions. \n\n"}
{"id": "1807.05561", "contents": "Title: Spatio-Temporal Structured Sparse Regression with Hierarchical Gaussian\n  Process Priors Abstract: This paper introduces a new sparse spatio-temporal structured Gaussian\nprocess regression framework for online and offline Bayesian inference. This is\nthe first framework that gives a time-evolving representation of the\ninterdependencies between the components of the sparse signal of interest. A\nhierarchical Gaussian process describes such structure and the\ninterdependencies are represented via the covariance matrices of the prior\ndistributions. The inference is based on the expectation propagation method and\nthe theoretical derivation of the posterior distribution is provided in the\npaper. The inference framework is thoroughly evaluated over synthetic, real\nvideo and electroencephalography (EEG) data where the spatio-temporal evolving\npatterns need to be reconstructed with high accuracy. It is shown that it\nachieves 15% improvement of the F-measure compared with the alternating\ndirection method of multipliers, spatio-temporal sparse Bayesian learning\nmethod and one-level Gaussian process model. Additionally, the required memory\nfor the proposed algorithm is less than in the one-level Gaussian process\nmodel. This structured sparse regression framework is of broad applicability to\nsource localisation and object detection problems with sparse signals. \n\n"}
{"id": "1807.06160", "contents": "Title: Layer-wise Relevance Propagation for Explainable Recommendations Abstract: In this paper, we tackle the problem of explanations in a deep-learning based\nmodel for recommendations by leveraging the technique of layer-wise relevance\npropagation. We use a Deep Convolutional Neural Network to extract relevant\nfeatures from the input images before identifying similarity between the images\nin feature space. Relationships between the images are identified by the model\nand layer-wise relevance propagation is used to infer pixel-level details of\nthe images that may have significantly informed the model's choice. We evaluate\nour method on an Amazon products dataset and demonstrate the efficacy of our\napproach. \n\n"}
{"id": "1807.06214", "contents": "Title: Knockoffs for the mass: new feature importance statistics with false\n  discovery guarantees Abstract: An important problem in machine learning and statistics is to identify\nfeatures that causally affect the outcome. This is often impossible to do from\npurely observational data, and a natural relaxation is to identify features\nthat are correlated with the outcome even conditioned on all other observed\nfeatures. For example, we want to identify that smoking really is correlated\nwith cancer conditioned on demographics. The knockoff procedure is a recent\nbreakthrough in statistics that, in theory, can identify truly correlated\nfeatures while guaranteeing that the false discovery is limited. The idea is to\ncreate synthetic data -- knockoffs -- that captures correlations amongst the\nfeatures. However there are substantial computational and practical challenges\nto generating and using knockoffs. This paper makes several key advances that\nenable knockoff application to be more efficient and powerful. We develop an\nefficient algorithm to generate valid knockoffs from Bayesian Networks. Then we\nsystematically evaluate knockoff test statistics and develop new statistics\nwith improved power. The paper combines new mathematical guarantees with\nsystematic experiments on real and synthetic data. \n\n"}
{"id": "1807.06373", "contents": "Title: To Post or Not to Post: Using Online Trends to Predict Popularity of\n  Offline Content Abstract: Predicting the popularity of online content has attracted much attention in\nthe past few years. In news rooms, for instance, journalists and editors are\nkeen to know, as soon as possible, the articles that will bring the most\ntraffic into their website. The relevant literature includes a number of\napproaches and algorithms to perform this forecasting. Most of the proposed\nmethods require monitoring the popularity of content during some time after it\nis posted, before making any longer-term prediction. In this paper, we propose\na new approach for predicting the popularity of news articles before they go\nonline. Our approach complements existing content-based methods, and is based\non a number of observations regarding article similarity and topicality. First,\nthe popularity of a new article is correlated with the popularity of similar\narticles of recent publication. Second, the popularity of the new article is\nrelated to the recent historical popularity of its main topic. Based on these\nobservations, we use time series forecasting to predict the number of visits an\narticle will receive. Our experiments, conducted on a real data collection of\narticles in an international news website, demonstrate the effectiveness and\nefficiency of the proposed method. \n\n"}
{"id": "1807.06978", "contents": "Title: Improving Explainable Recommendations with Synthetic Reviews Abstract: An important task for a recommender system to provide interpretable\nexplanations for the user. This is important for the credibility of the system.\nCurrent interpretable recommender systems tend to focus on certain features\nknown to be important to the user and offer their explanations in a structured\nform. It is well known that user generated reviews and feedback from reviewers\nhave strong leverage over the users' decisions. On the other hand, recent text\ngeneration works have been shown to generate text of similar quality to human\nwritten text, and we aim to show that generated text can be successfully used\nto explain recommendations.\n  In this paper, we propose a framework consisting of popular review-oriented\ngeneration models aiming to create personalised explanations for\nrecommendations. The interpretations are generated at both character and word\nlevels. We build a dataset containing reviewers' feedback from the Amazon books\nreview dataset. Our cross-domain experiments are designed to bridge from\nnatural language processing to the recommender system domain. Besides language\nmodel evaluation methods, we employ DeepCoNN, a novel review-oriented\nrecommender system using a deep neural network, to evaluate the recommendation\nperformance of generated reviews by root mean square error (RMSE). We\ndemonstrate that the synthetic personalised reviews have better recommendation\nperformance than human written reviews. To our knowledge, this presents the\nfirst machine-generated natural language explanations for rating prediction. \n\n"}
{"id": "1807.07987", "contents": "Title: Deep Learning Abstract: Deep learning (DL) is a high dimensional data reduction technique for\nconstructing high-dimensional predictors in input-output models. DL is a form\nof machine learning that uses hierarchical layers of latent features. In this\narticle, we review the state-of-the-art of deep learning from a modeling and\nalgorithmic perspective. We provide a list of successful areas of applications\nin Artificial Intelligence (AI), Image Processing, Robotics and Automation.\nDeep learning is predictive in its nature rather then inferential and can be\nviewed as a black-box methodology for high-dimensional function estimation. \n\n"}
{"id": "1807.09089", "contents": "Title: Decision Variance in Online Learning Abstract: Online learning has traditionally focused on the expected rewards. In this\npaper, a risk-averse online learning problem under the performance measure of\nthe mean-variance of the rewards is studied. Both the bandit and full\ninformation settings are considered. The performance of several existing\npolicies is analyzed, and new fundamental limitations on risk-averse learning\nis established. In particular, it is shown that although a logarithmic\ndistribution-dependent regret in time $T$ is achievable (similar to the\nrisk-neutral problem), the worst-case (i.e. minimax) regret is lower bounded by\n$\\Omega(T)$ (in contrast to the $\\Omega(\\sqrt{T})$ lower bound in the\nrisk-neutral problem). This sharp difference from the risk-neutral counterpart\nis caused by the the variance in the player's decisions, which, while absent in\nthe regret under the expected reward criterion, contributes to excess\nmean-variance due to the non-linearity of this risk measure. The role of the\ndecision variance in regret performance reflects a risk-averse player's desire\nfor robust decisions and outcomes. \n\n"}
{"id": "1807.09097", "contents": "Title: Algorithm Selection for Collaborative Filtering: the influence of graph\n  metafeatures and multicriteria metatargets Abstract: To select the best algorithm for a new problem is an expensive and difficult\ntask. However, there are automatic solutions to address this problem: using\nMetalearning, which takes advantage of problem characteristics (i.e.\nmetafeatures), one is able to predict the relative performance of algorithms.\nIn the Collaborative Filtering scope, recent works have proposed diverse\nmetafeatures describing several dimensions of this problem. Despite interesting\nand effective findings, it is still unknown whether these are the most\neffective metafeatures. Hence, this work proposes a new set of graph\nmetafeatures, which approach the Collaborative Filtering problem from a Graph\nTheory perspective. Furthermore, in order to understand whether metafeatures\nfrom multiple dimensions are a better fit, we investigate the effects of\ncomprehensive metafeatures. These metafeatures are a selection of the best\nmetafeatures from all existing Collaborative Filtering metafeatures. The impact\nof the most representative metafeatures is investigated in a controlled\nexperimental setup. Another contribution we present is the use of a\nPareto-Efficient ranking procedure to create multicriteria metatargets. These\nnew rankings of algorithms, which take into account multiple evaluation\nmeasures, allow to explore the algorithm selection problem in a fairer and more\ndetailed way. According to the experimental results, the graph metafeatures are\na good alternative to related work metafeatures. However, the results have\nshown that the feature selection procedure used to create the comprehensive\nmetafeatures is is not effective, since there is no gain in predictive\nperformance. Finally, an extensive metaknowledge analysis was conducted to\nidentify the most influential metafeatures. \n\n"}
{"id": "1807.09289", "contents": "Title: Noise Contrastive Priors for Functional Uncertainty Abstract: Obtaining reliable uncertainty estimates of neural network predictions is a\nlong standing challenge. Bayesian neural networks have been proposed as a\nsolution, but it remains open how to specify their prior. In particular, the\ncommon practice of an independent normal prior in weight space imposes\nrelatively weak constraints on the function posterior, allowing it to\ngeneralize in unforeseen ways on inputs outside of the training distribution.\nWe propose noise contrastive priors (NCPs) to obtain reliable uncertainty\nestimates. The key idea is to train the model to output high uncertainty for\ndata points outside of the training distribution. NCPs do so using an input\nprior, which adds noise to the inputs of the current mini batch, and an output\nprior, which is a wide distribution given these inputs. NCPs are compatible\nwith any model that can output uncertainty estimates, are easy to scale, and\nyield reliable uncertainty estimates throughout training. Empirically, we show\nthat NCPs prevent overfitting outside of the training distribution and result\nin uncertainty estimates that are useful for active learning. We demonstrate\nthe scalability of our method on the flight delays data set, where we\nsignificantly improve upon previously published results. \n\n"}
{"id": "1807.09751", "contents": "Title: Multi-Perspective Neural Architecture for Recommendation System Abstract: Currently, there starts a research trend to leverage neural architecture for\nrecommendation systems. Though several deep recommender models are proposed,\nmost methods are too simple to characterize users' complex preference. In this\npaper, for a fine-grain analysis, users' ratings are explained from multiple\nperspectives, based on which, we propose our neural architecture. Specifically,\nour model employs several sequential stages to encode the user and item into\nhidden representations. In one stage, the user and item are represented from\nmultiple perspectives and in each perspective, the representations of user and\nitem put attentions to each other. Last, we metric the output representations\nof final stage to approach the users' rating. Extensive experiments demonstrate\nthat our method achieves substantial improvements against baselines. \n\n"}
{"id": "1807.09842", "contents": "Title: Understanding and representing the semantics of large structured\n  documents Abstract: Understanding large, structured documents like scholarly articles, requests\nfor proposals or business reports is a complex and difficult task. It involves\ndiscovering a document's overall purpose and subject(s), understanding the\nfunction and meaning of its sections and subsections, and extracting low level\nentities and facts about them. In this research, we present a deep learning\nbased document ontology to capture the general purpose semantic structure and\ndomain specific semantic concepts from a large number of academic articles and\nbusiness documents. The ontology is able to describe different functional parts\nof a document, which can be used to enhance semantic indexing for a better\nunderstanding by human beings and machines. We evaluate our models through\nextensive experiments on datasets of scholarly articles from arXiv and Request\nfor Proposal documents. \n\n"}
{"id": "1807.11374", "contents": "Title: Weakly-Supervised Deep Learning of Heat Transport via Physics Informed\n  Loss Abstract: In typical machine learning tasks and applications, it is necessary to obtain\nor create large labeled datasets in order to to achieve high performance.\nUnfortunately, large labeled datasets are not always available and can be\nexpensive to source, creating a bottleneck towards more widely applicable\nmachine learning. The paradigm of weak supervision offers an alternative that\nallows for integration of domain-specific knowledge by enforcing constraints\nthat a correct solution to the learning problem will obey over the output\nspace. In this work, we explore the application of this paradigm to 2-D\nphysical systems governed by non-linear differential equations. We demonstrate\nthat knowledge of the partial differential equations governing a system can be\nencoded into the loss function of a neural network via an appropriately chosen\nconvolutional kernel. We demonstrate this by showing that the steady-state\nsolution to the 2-D heat equation can be learned directly from initial\nconditions by a convolutional neural network, in the absence of labeled\ntraining data. We also extend recent work in the progressive growing of fully\nconvolutional networks to achieve high accuracy (< 1.5% error) at multiple\nscales of the heat-flow problem, including at the very large scale (1024x1024).\nFinally, we demonstrate that this method can be used to speed up exact\ncalculation of the solution to the differential equations via finite\ndifference. \n\n"}
{"id": "1808.00004", "contents": "Title: Graph-Based Recommendation System Abstract: In this work, we study recommendation systems modelled as contextual\nmulti-armed bandit (MAB) problems. We propose a graph-based recommendation\nsystem that learns and exploits the geometry of the user space to create\nmeaningful clusters in the user domain. This reduces the dimensionality of the\nrecommendation problem while preserving the accuracy of MAB. We then study the\neffect of graph sparsity and clusters size on the MAB performance and provide\nexhaustive simulation results both in synthetic and in real-case datasets.\nSimulation results show improvements with respect to state-of-the-art MAB\nalgorithms. \n\n"}
{"id": "1808.00957", "contents": "Title: SWDE : A Sub-Word And Document Embedding Based Engine for Clickbait\n  Detection Abstract: In order to expand their reach and increase website ad revenue, media outlets\nhave started using clickbait techniques to lure readers to click on articles on\ntheir digital platform. Having successfully enticed the user to open the\narticle, the article fails to satiate his curiosity serving only to boost\nclick-through rates. Initial methods for this task were dependent on feature\nengineering, which varies with each dataset. Industry systems have relied on an\nexhaustive set of rules to get the job done. Neural networks have barely been\nexplored to perform this task. We propose a novel approach considering\ndifferent textual embeddings of a news headline and the related article. We\ngenerate sub-word level embeddings of the title using Convolutional Neural\nNetworks and use them to train a bidirectional LSTM architecture. An attention\nlayer allows for calculation of significance of each term towards the nature of\nthe post. We also generate Doc2Vec embeddings of the title and article text and\nmodel how they interact, following which it is concatenated with the output of\nthe previous component. Finally, this representation is passed through a neural\nnetwork to obtain a score for the headline. We test our model over 2538 posts\n(having trained it on 17000 records) and achieve an accuracy of 83.49%\noutscoring previous state-of-the-art approaches. \n\n"}
{"id": "1808.00973", "contents": "Title: Likelihood-free inference with an improved cross-entropy estimator Abstract: We extend recent work (Brehmer, et. al., 2018) that use neural networks as\nsurrogate models for likelihood-free inference. As in the previous work, we\nexploit the fact that the joint likelihood ratio and joint score, conditioned\non both observed and latent variables, can often be extracted from an implicit\ngenerative model or simulator to augment the training data for these surrogate\nmodels. We show how this augmented training data can be used to provide a new\ncross-entropy estimator, which provides improved sample efficiency compared to\nprevious loss functions exploiting this augmented training data. \n\n"}
{"id": "1808.02013", "contents": "Title: Automated Extraction of Personal Knowledge from Smartphone Push\n  Notifications Abstract: Personalized services are in need of a rich and powerful personal knowledge\nbase, i.e. a knowledge base containing information about the user. This paper\nproposes an approach to extracting personal knowledge from smartphone push\nnotifications, which are used by mobile systems and apps to inform users of a\nrich range of information. Our solution is based on the insight that most\nnotifications are formatted using templates, while knowledge entities can be\nusually found within the parameters to the templates. As defining all the\nnotification templates and their semantic rules are impractical due to the huge\nnumber of notification templates used by potentially millions of apps, we\npropose an automated approach for personal knowledge extraction from push\nnotifications. We first discover notification templates through pattern mining,\nthen use machine learning to understand the template semantics. Based on the\ntemplates and their semantics, we are able to translate notification text into\nknowledge facts automatically. Users' privacy is preserved as we only need to\nupload the templates to the server for model training, which do not contain any\npersonal information. According to our experiments with about 120 million push\nnotifications from 100,000 smartphone users, our system is able to extract\npersonal knowledge accurately and efficiently. \n\n"}
{"id": "1808.02078", "contents": "Title: Unbiased Implicit Variational Inference Abstract: We develop unbiased implicit variational inference (UIVI), a method that\nexpands the applicability of variational inference by defining an expressive\nvariational family. UIVI considers an implicit variational distribution\nobtained in a hierarchical manner using a simple reparameterizable distribution\nwhose variational parameters are defined by arbitrarily flexible deep neural\nnetworks. Unlike previous works, UIVI directly optimizes the evidence lower\nbound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on\nseveral models, including Bayesian multinomial logistic regression and\nvariational autoencoders, and show that UIVI achieves both tighter ELBO and\nbetter predictive performance than existing approaches at a similar\ncomputational cost. \n\n"}
{"id": "1808.02433", "contents": "Title: Robust Implicit Backpropagation Abstract: Arguably the biggest challenge in applying neural networks is tuning the\nhyperparameters, in particular the learning rate. The sensitivity to the\nlearning rate is due to the reliance on backpropagation to train the network.\nIn this paper we present the first application of Implicit Stochastic Gradient\nDescent (ISGD) to train neural networks, a method known in convex optimization\nto be unconditionally stable and robust to the learning rate. Our key\ncontribution is a novel layer-wise approximation of ISGD which makes its\nupdates tractable for neural networks. Experiments show that our method is more\nrobust to high learning rates and generally outperforms standard\nbackpropagation on a variety of tasks. \n\n"}
{"id": "1808.02911", "contents": "Title: A Case Study on the Impact of Similarity Measure on Information\n  Retrieval based Software Engineering Tasks Abstract: Information Retrieval (IR) plays a pivotal role in diverse Software\nEngineering (SE) tasks, e.g., bug localization and triaging, code retrieval,\nrequirements analysis, etc. The choice of similarity measure is the core\ncomponent of an IR technique. The performance of any IR method critically\ndepends on selecting an appropriate similarity measure for the given\napplication domain. Since different SE tasks operate on different document\ntypes like bug reports, software descriptions, source code, etc. that often\ncontain non-standard domain-specific vocabulary, it is essential to understand\nwhich similarity measures work best for different SE documents.\n  This paper presents two case studies on the effect of different similarity\nmeasure on various SE documents w.r.t. two tasks: (i) project recommendation:\nfinding similar GitHub projects and (ii) bug localization: retrieving buggy\nsource file(s) correspond to a bug report. These tasks contain a diverse\ncombination of textual (i.e. description, readme) and code (i.e. source code,\nAPI, import package) artifacts. We observe that the performance of IR models\nvaries when applied to different artifact types. We find that, in general, the\ncontext-aware models achieve better performance on textual artifacts. In\ncontrast, simple keyword-based bag-of-words models perform better on code\nartifacts. On the other hand, the probabilistic ranking model BM25 performs\nbetter on a mixture of text and code artifacts.\n  We further investigate how such an informed choice of similarity measure\nimpacts the performance of SE tools. In particular, we analyze two previously\nproposed tools for project recommendation and bug localization tasks, which\nleverage diverse software artifacts, and observe that an informed choice of\nsimilarity measure indeed leads to improved performance of the existing SE\ntools. \n\n"}
{"id": "1808.02933", "contents": "Title: Sequential Monte Carlo Bandits Abstract: We extend Bayesian multi-armed bandit (MAB) algorithms beyond their original\nsetting by making use of sequential Monte Carlo (SMC) methods.\n  A MAB is a sequential decision making problem where the goal is to learn a\npolicy that maximizes long term payoff, where only the reward of the executed\naction is observed. In the stochastic MAB, the reward for each action is\ngenerated from an unknown distribution, often assumed to be stationary. To\ndecide which action to take next, a MAB agent must learn the characteristics of\nthe unknown reward distribution, e.g., compute its sufficient statistics.\nHowever, closed-form expressions for these statistics are analytically\nintractable except for simple, stationary cases.\n  We here utilize SMC for estimation of the statistics Bayesian MAB agents\ncompute, and devise flexible policies that can address a rich class of bandit\nproblems: i.e., MABs with nonlinear, stateless- and context-dependent reward\ndistributions that evolve over time. We showcase how non-stationary bandits,\nwhere time dynamics are modeled via linear dynamical systems, can be\nsuccessfully addressed by SMC-based Bayesian bandit agents. We empirically\ndemonstrate good regret performance of the proposed SMC-based bandit policies\nin several MAB scenarios that have remained elusive, i.e., in non-stationary\nbandits with nonlinear rewards. \n\n"}
{"id": "1808.03027", "contents": "Title: Sentimental Content Analysis and Knowledge Extraction from News Articles Abstract: In web era, since technology has revolutionized mankind life, plenty of data\nand information are published on the Internet each day. For instance, news\nagencies publish news on their websites all over the world. These raw data\ncould be an important resource for knowledge extraction. These shared data\ncontain emotions (i.e., positive, neutral or negative) toward various topics;\ntherefore, sentimental content extraction could be a beneficial task in many\naspects. Extracting the sentiment of news illustrates highly valuable\ninformation about the events over a period of time, the viewpoint of a media or\nnews agency to these events. In this paper an attempt is made to propose an\napproach for news analysis and extracting useful knowledge from them. Firstly,\nwe attempt to extract a noise robust sentiment of news documents; therefore,\nthe news associated to six countries: United State, United Kingdom, Germany,\nCanada, France and Australia in 5 different news categories: Politics, Sports,\nBusiness, Entertainment and Technology are downloaded. In this paper we compare\nthe condition of different countries in each 5 news topics based on the\nextracted sentiments and emotional contents in news documents. Moreover, we\npropose an approach to reduce the bulky news data to extract the hottest topics\nand news titles as a knowledge. Eventually, we generate a word model to map\neach word to a fixed-size vector by Word2Vec in order to understand the\nrelations between words in our collected news database. \n\n"}
{"id": "1808.03216", "contents": "Title: Data-driven polynomial chaos expansion for machine learning regression Abstract: We present a regression technique for data-driven problems based on\npolynomial chaos expansion (PCE). PCE is a popular technique in the field of\nuncertainty quantification (UQ), where it is typically used to replace a\nrunnable but expensive computational model subject to random inputs with an\ninexpensive-to-evaluate polynomial function. The metamodel obtained enables a\nreliable estimation of the statistics of the output, provided that a suitable\nprobabilistic model of the input is available. Machine learning (ML) regression\nis a research field that focuses on providing purely data-driven input-output\nmaps, with the focus on pointwise prediction accuracy. We show that a PCE\nmetamodel purely trained on data can yield pointwise predictions whose accuracy\nis comparable to that of other ML regression models, such as neural networks\nand support vector machines. The comparisons are performed on benchmark\ndatasets available from the literature. The methodology also enables the\nquantification of the output uncertainties, and is robust to noise.\nFurthermore, it enjoys additional desirable properties, such as good\nperformance for small training sets and simplicity of construction, with only\nlittle parameter tuning required. \n\n"}
{"id": "1808.03737", "contents": "Title: Learning Multi-touch Conversion Attribution with Dual-attention\n  Mechanisms for Online Advertising Abstract: In online advertising, the Internet users may be exposed to a sequence of\ndifferent ad campaigns, i.e., display ads, search, or referrals from multiple\nchannels, before led up to any final sales conversion and transaction. For both\ncampaigners and publishers, it is fundamentally critical to estimate the\ncontribution from ad campaign touch-points during the customer journey\n(conversion funnel) and assign the right credit to the right ad exposure\naccordingly. However, the existing research on the multi-touch attribution\nproblem lacks a principled way of utilizing the users' pre-conversion actions\n(i.e., clicks), and quite often fails to model the sequential patterns among\nthe touch points from a user's behavior data. To make it worse, the current\nindustry practice is merely employing a set of arbitrary rules as the\nattribution model, e.g., the popular last-touch model assigns 100% credit to\nthe final touch-point regardless of actual attributions. In this paper, we\npropose a Dual-attention Recurrent Neural Network (DARNN) for the multi-touch\nattribution problem. It learns the attribution values through an attention\nmechanism directly from the conversion estimation objective. To achieve this,\nwe utilize sequence-to-sequence prediction for user clicks, and combine both\npost-view and post-click attribution patterns together for the final conversion\nestimation. To quantitatively benchmark attribution models, we also propose a\nnovel yet practical attribution evaluation scheme through the proxy of budget\nallocation (under the estimated attributions) over ad channels. The\nexperimental results on two real datasets demonstrate the significant\nperformance gains of our attribution model against the state of the art. \n\n"}
{"id": "1808.03967", "contents": "Title: Augmenting word2vec with latent Dirichlet allocation within a clinical\n  application Abstract: This paper presents three hybrid models that directly combine latent\nDirichlet allocation and word embedding for distinguishing between speakers\nwith and without Alzheimer's disease from transcripts of picture descriptions.\nTwo of our models get F-scores over the current state-of-the-art using\nautomatic methods on the DementiaBank dataset. \n\n"}
{"id": "1808.04888", "contents": "Title: Skill Rating for Generative Models Abstract: We explore a new way to evaluate generative models using insights from\nevaluation of competitive games between human players. We show experimentally\nthat tournaments between generators and discriminators provide an effective way\nto evaluate generative models. We introduce two methods for summarizing\ntournament outcomes: tournament win rate and skill rating. Evaluations are\nuseful in different contexts, including monitoring the progress of a single\nmodel as it learns during the training process, and comparing the capabilities\nof two different fully trained models. We show that a tournament consisting of\na single model playing against past and future versions of itself produces a\nuseful measure of training progress. A tournament containing multiple separate\nmodels (using different seeds, hyperparameters, and architectures) provides a\nuseful relative comparison between different trained GANs. Tournament-based\nrating methods are conceptually distinct from numerous previous categories of\napproaches to evaluation of generative models, and have complementary\nadvantages and disadvantages. \n\n"}
{"id": "1808.05587", "contents": "Title: Deep Convolutional Networks as shallow Gaussian Processes Abstract: We show that the output of a (residual) convolutional neural network (CNN)\nwith an appropriate prior over the weights and biases is a Gaussian process\n(GP) in the limit of infinitely many convolutional filters, extending similar\nresults for dense networks. For a CNN, the equivalent kernel can be computed\nexactly and, unlike \"deep kernels\", has very few parameters: only the\nhyperparameters of the original CNN. Further, we show that this kernel has two\nproperties that allow it to be computed efficiently; the cost of evaluating the\nkernel for a pair of images is similar to a single forward pass through the\noriginal CNN with only one filter per layer. The kernel equivalent to a\n32-layer ResNet obtains 0.84% classification error on MNIST, a new record for\nGPs with a comparable number of parameters. \n\n"}
{"id": "1808.05636", "contents": "Title: IceBreaker: Solving Cold Start Problem for Video Recommendation Engines Abstract: Internet has brought about a tremendous increase in content of all forms and,\nin that, video content constitutes the major backbone of the total content\nbeing published as well as watched. Thus it becomes imperative for video\nrecommendation engines such as Hulu to look for novel and innovative ways to\nrecommend the newly added videos to their users. However, the problem with new\nvideos is that they lack any sort of metadata and user interaction so as to be\nable to rate the videos for the consumers. To this effect, this paper\nintroduces the several techniques we develop for the Content Based Video\nRelevance Prediction (CBVRP) Challenge being hosted by Hulu for the ACM\nMultimedia Conference 2018. We employ different architectures on the CBVRP\ndataset to make use of the provided frame and video level features and generate\npredictions of videos that are similar to the other videos. We also implement\nseveral ensemble strategies to explore complementarity between both the types\nof provided features. The obtained results are encouraging and will impel the\nboundaries of research for multimedia based video recommendation systems. \n\n"}
{"id": "1808.05726", "contents": "Title: An N Time-Slice Dynamic Chain Event Graph Abstract: The Dynamic Chain Event Graph (DCEG) is able to depict many classes of\ndiscrete random processes exhibiting asymmetries in their developments and\ncontext-specific conditional probabilities structures. However, paradoxically,\nthis very generality has so far frustrated its wide application. So in this\npaper we develop an object-oriented method to fully analyse a particularly\nuseful and feasibly implementable new subclass of these graphical models called\nthe N Time-Slice DCEG (NT-DCEG). After demonstrating a close relationship\nbetween an NT-DCEG and a specific class of Markov processes, we discuss how\ngraphical modellers can exploit this connection to gain a deep understanding of\ntheir processes. We also show how to read from the topology of this graph\ncontext-specific independence statements that can then be checked by domain\nexperts. Our methods are illustrated throughout using examples of dynamic\nmultivariate processes describing inmate radicalisation in a prison. \n\n"}
{"id": "1808.05784", "contents": "Title: Multiview Boosting by Controlling the Diversity and the Accuracy of\n  View-specific Voters Abstract: In this paper we propose a boosting based multiview learning algorithm,\nreferred to as PB-MVBoost, which iteratively learns i) weights over\nview-specific voters capturing view-specific information; and ii) weights over\nviews by optimizing a PAC-Bayes multiview C-Bound that takes into account the\naccuracy of view-specific classifiers and the diversity between the views. We\nderive a generalization bound for this strategy following the PAC-Bayes theory\nwhich is a suitable tool to deal with models expressed as weighted combination\nover a set of voters. Different experiments on three publicly available\ndatasets show the efficiency of the proposed approach with respect to\nstate-of-art models. \n\n"}
{"id": "1808.06670", "contents": "Title: Learning deep representations by mutual information estimation and\n  maximization Abstract: In this work, we perform unsupervised learning of representations by\nmaximizing mutual information between an input and the output of a deep neural\nnetwork encoder. Importantly, we show that structure matters: incorporating\nknowledge about locality of the input to the objective can greatly influence a\nrepresentation's suitability for downstream tasks. We further control\ncharacteristics of the representation by matching to a prior distribution\nadversarially. Our method, which we call Deep InfoMax (DIM), outperforms a\nnumber of popular unsupervised learning methods and competes with\nfully-supervised learning on several classification tasks. DIM opens new\navenues for unsupervised learning of representations and is an important step\ntowards flexible formulations of representation-learning objectives for\nspecific end-goals. \n\n"}
{"id": "1808.06791", "contents": "Title: LRMM: Learning to Recommend with Missing Modalities Abstract: Multimodal learning has shown promising performance in content-based\nrecommendation due to the auxiliary user and item information of multiple\nmodalities such as text and images. However, the problem of incomplete and\nmissing modality is rarely explored and most existing methods fail in learning\na recommendation model with missing or corrupted modalities. In this paper, we\npropose LRMM, a novel framework that mitigates not only the problem of missing\nmodalities but also more generally the cold-start problem of recommender\nsystems. We propose modality dropout (m-drop) and a multimodal sequential\nautoencoder (m-auto) to learn multimodal representations for complementing and\nimputing missing modalities. Extensive experiments on real-world Amazon data\nshow that LRMM achieves state-of-the-art performance on rating prediction\ntasks. More importantly, LRMM is more robust to previous methods in alleviating\ndata-sparsity and the cold-start problem. \n\n"}
{"id": "1808.07510", "contents": "Title: XPCA: Extending PCA for a Combination of Discrete and Continuous\n  Variables Abstract: Principal component analysis (PCA) is arguably the most popular tool in\nmultivariate exploratory data analysis. In this paper, we consider the question\nof how to handle heterogeneous variables that include continuous, binary, and\nordinal. In the probabilistic interpretation of low-rank PCA, the data has a\nnormal multivariate distribution and, therefore, normal marginal distributions\nfor each column. If some marginals are continuous but not normal, the\nsemiparametric copula-based principal component analysis (COCA) method is an\nalternative to PCA that combines a Gaussian copula with nonparametric\nmarginals. If some marginals are discrete or semi-continuous, we propose a new\nextended PCA (XPCA) method that also uses a Gaussian copula and nonparametric\nmarginals and accounts for discrete variables in the likelihood calculation by\nintegrating over appropriate intervals. Like PCA, the factors produced by XPCA\ncan be used to find latent structure in data, build predictive models, and\nperform dimensionality reduction. We present the new model, its induced\nlikelihood function, and a fitting algorithm which can be applied in the\npresence of missing data. We demonstrate how to use XPCA to produce an\nestimated full conditional distribution for each data point, and use this to\nproduce to provide estimates for missing data that are automatically range\nrespecting. We compare the methods as applied to simulated and real-world data\nsets that have a mixture of discrete and continuous variables. \n\n"}
{"id": "1808.07801", "contents": "Title: On a 'Two Truths' Phenomenon in Spectral Graph Clustering Abstract: Clustering is concerned with coherently grouping observations without any\nexplicit concept of true groupings. Spectral graph clustering - clustering the\nvertices of a graph based on their spectral embedding - is commonly approached\nvia K-means (or, more generally, Gaussian mixture model) clustering composed\nwith either Laplacian or Adjacency spectral embedding (LSE or ASE). Recent\ntheoretical results provide new understanding of the problem and solutions, and\nlead us to a 'Two Truths' LSE vs. ASE spectral graph clustering phenomenon\nconvincingly illustrated here via a diffusion MRI connectome data set: the\ndifferent embedding methods yield different clustering results, with LSE\ncapturing left hemisphere/right hemisphere affinity structure and ASE capturing\ngray matter/white matter core-periphery structure. \n\n"}
{"id": "1808.08558", "contents": "Title: Spectral Pruning: Compressing Deep Neural Networks via Spectral Analysis\n  and its Generalization Error Abstract: Compression techniques for deep neural network models are becoming very\nimportant for the efficient execution of high-performance deep learning systems\non edge-computing devices. The concept of model compression is also important\nfor analyzing the generalization error of deep learning, known as the\ncompression-based error bound. However, there is still huge gap between a\npractically effective compression method and its rigorous background of\nstatistical learning theory. To resolve this issue, we develop a new\ntheoretical framework for model compression and propose a new pruning method\ncalled {\\it spectral pruning} based on this framework. We define the ``degrees\nof freedom'' to quantify the intrinsic dimensionality of a model by using the\neigenvalue distribution of the covariance matrix across the internal nodes and\nshow that the compression ability is essentially controlled by this quantity.\nMoreover, we present a sharp generalization error bound of the compressed model\nand characterize the bias--variance tradeoff induced by the compression\nprocedure. We apply our method to several datasets to justify our theoretical\nanalyses and show the superiority of the the proposed method. \n\n"}
{"id": "1808.08811", "contents": "Title: Exponential inequalities for nonstationary Markov Chains Abstract: Exponential inequalities are main tools in machine learning theory. To prove\nexponential inequalities for non i.i.d random variables allows to extend many\nlearning techniques to these variables. Indeed, much work has been done both on\ninequalities and learning theory for time series, in the past 15 years.\nHowever, for the non independent case, almost all the results concern\nstationary time series. This excludes many important applications: for example\nany series with a periodic behavior is non-stationary. In this paper, we extend\nthe basic tools of Dedecker and Fan (2015) to nonstationary Markov chains. As\nan application, we provide a Bernstein-type inequality, and we deduce risk\nbounds for the prediction of periodic autoregressive processes with an unknown\nperiod. \n\n"}
{"id": "1808.09123", "contents": "Title: Investigating Human + Machine Complementarity for Recidivism Predictions Abstract: When might human input help (or not) when assessing risk in fairness domains?\nDressel and Farid (2018) asked Mechanical Turk workers to evaluate a subset of\ndefendants in the ProPublica COMPAS data for risk of recidivism, and concluded\nthat COMPAS predictions were no more accurate or fair than predictions made by\nhumans. We delve deeper into this claim to explore differences in human and\nalgorithmic decision making. We construct a Human Risk Score based on the\npredictions made by multiple Turk workers, characterize the features that\ndetermine agreement and disagreement between COMPAS and Human Scores, and\nconstruct hybrid Human+Machine models to predict recidivism. Our key finding is\nthat on this data set, Human and COMPAS decision making differed, but not in\nways that could be leveraged to significantly improve ground-truth prediction.\nWe present the results of our analyses and suggestions for data collection best\npractices to leverage complementary strengths of human and machines in the\nfairness domain. \n\n"}
{"id": "1808.09270", "contents": "Title: Models for Predicting Community-Specific Interest in News Articles Abstract: In this work, we ask two questions: 1. Can we predict the type of community\ninterested in a news article using only features from the article content? and\n2. How well do these models generalize over time? To answer these questions, we\ncompute well-studied content-based features on over 60K news articles from 4\ncommunities on reddit.com. We train and test models over three different time\nperiods between 2015 and 2017 to demonstrate which features degrade in\nperformance the most due to concept drift. Our models can classify news\narticles into communities with high accuracy, ranging from 0.81 ROC AUC to 1.0\nROC AUC. However, while we can predict the community-specific popularity of\nnews articles with high accuracy, practitioners should approach these models\ncarefully. Predictions are both community-pair dependent and feature group\ndependent. Moreover, these feature groups generalize over time differently,\nwith some only degrading slightly over time, but others degrading greatly.\nTherefore, we recommend that community-interest predictions are done in a\nhierarchical structure, where multiple binary classifiers can be used to\nseparate community pairs, rather than a traditional multi-class model. Second,\nthese models should be retrained over time based on accuracy goals and the\navailability of training data. \n\n"}
{"id": "1808.09407", "contents": "Title: Implementation Notes for the Soft Cosine Measure Abstract: The standard bag-of-words vector space model (VSM) is efficient, and\nubiquitous in information retrieval, but it underestimates the similarity of\ndocuments with the same meaning, but different terminology. To overcome this\nlimitation, Sidorov et al. proposed the Soft Cosine Measure (SCM) that\nincorporates term similarity relations. Charlet and Damnati showed that the SCM\nis highly effective in question answering (QA) systems. However, the\northonormalization algorithm proposed by Sidorov et al. has an impractical time\ncomplexity of $\\mathcal O(n^4)$, where n is the size of the vocabulary.\n  In this paper, we prove a tighter lower worst-case time complexity bound of\n$\\mathcal O(n^3)$. We also present an algorithm for computing the similarity\nbetween documents and we show that its worst-case time complexity is $\\mathcal\nO(1)$ given realistic conditions. Lastly, we describe implementation in\ngeneral-purpose vector databases such as Annoy, and Faiss and in the inverted\nindices of text search engines such as Apache Lucene, and ElasticSearch. Our\nresults enable the deployment of the SCM in real-world information retrieval\nsystems. \n\n"}
{"id": "1808.09489", "contents": "Title: Convergence Rate of Krasulina Estimator Abstract: Principal component analysis (PCA) is one of the most commonly used\nstatistical procedures with a wide range of applications. Consider the points\n$X_1, X_2,..., X_n$ are vectors drawn i.i.d. from a distribution with mean zero\nand covariance $\\Sigma$, where $\\Sigma$ is unknown. Let $A_n = X_nX_n^T$, then\n$E[A_n] = \\Sigma$. This paper consider the problem of finding the least\neigenvalue and eigenvector of matrix $\\Sigma$. A classical such estimator are\ndue to Krasulina\\cite{krasulina_method_1969}. We are going to state the\nconvergence proof of Krasulina for the least eigenvalue and corresponding\neigenvector, and then find their convergence rate. \n\n"}
{"id": "1808.09802", "contents": "Title: Modelling Irregular Spatial Patterns using Graph Convolutional Neural\n  Networks Abstract: The understanding of geographical reality is a process of data representation\nand pattern discovery. Former studies mainly adopted continuous-field models to\nrepresent spatial variables and to investigate the underlying spatial\ncontinuity/heterogeneity in the regular spatial domain. In this article, we\nintroduce a more generalized model based on graph convolutional neural networks\n(GCNs) that can capture the complex parameters of spatial patterns underlying\ngraph-structured spatial data, which generally contain both Euclidean spatial\ninformation and non-Euclidean feature information. A trainable semi-supervised\nprediction framework is proposed to model the spatial distribution patterns of\nintra-urban points of interest(POI) check-ins. This work demonstrates the\nfeasibility of GCNs in complex geographic decision problems and provides a\npromising tool to analyze irregular spatial data. \n\n"}
{"id": "1808.10523", "contents": "Title: Spectral Collaborative Filtering Abstract: Despite the popularity of Collaborative Filtering (CF), CF-based methods are\nhaunted by the \\textit{cold-start} problem, which has a significantly negative\nimpact on users' experiences with Recommender Systems (RS). In this paper, to\novercome the aforementioned drawback, we first formulate the relationships\nbetween users and items as a bipartite graph. Then, we propose a new spectral\nconvolution operation directly performing in the \\textit{spectral domain},\nwhere not only the proximity information of a graph but also the connectivity\ninformation hidden in the graph are revealed. With the proposed spectral\nconvolution operation, we build a deep recommendation model called Spectral\nCollaborative Filtering (SpectralCF). Benefiting from the rich information of\nconnectivity existing in the \\textit{spectral domain}, SpectralCF is capable of\ndiscovering deep connections between users and items and therefore, alleviates\nthe \\textit{cold-start} problem for CF. To the best of our knowledge,\nSpectralCF is the first CF-based method directly learning from the\n\\textit{spectral domains} of user-item bipartite graphs. We apply our method on\nseveral standard datasets. It is shown that SpectralCF significantly\noutperforms state-of-the-art models. Code and data are available at\n\\url{https://github.com/lzheng21/SpectralCF}. \n\n"}
{"id": "1809.00082", "contents": "Title: NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation Abstract: Effective feature representation is key to the predictive performance of any\nalgorithm. This paper introduces a meta-procedure, called Non-Euclidean\nUpgrading (NEU), which learns feature maps that are expressive enough to embed\nthe universal approximation property (UAP) into most model classes while only\noutputting feature maps that preserve any model class's UAP. We show that NEU\ncan learn any feature map with these two properties if that feature map is\nasymptotically deformable into the identity. We also find that the\nfeature-representations learned by NEU are always submanifolds of the feature\nspace. NEU's properties are derived from a new deep neural model that is\nuniversal amongst all orientation-preserving homeomorphisms on the input space.\nWe derive qualitative and quantitative approximation guarantees for this\narchitecture. We quantify the number of parameters required for this new\narchitecture to memorize any set of input-output pairs while simultaneously\nfixing every point of the input space lying outside some compact set, and we\nquantify the size of this set as a function of our model's depth. Moreover, we\nshow that no deep feed-forward network with commonly used activation function\nhas all these properties. NEU's performance is evaluated against competing\nmachine learning methods on various regression and dimension reduction tasks\nboth with financial and simulated data. \n\n"}
{"id": "1809.00345", "contents": "Title: IntentsKB: A Knowledge Base of Entity-Oriented Search Intents Abstract: We address the problem of constructing a knowledge base of entity-oriented\nsearch intents. Search intents are defined on the level of entity types, each\ncomprising of a high-level intent category (property, website, service, or\nother), along with a cluster of query terms used to express that intent. These\nmachine-readable statements can be leveraged in various applications, e.g., for\ngenerating entity cards or query recommendations. By structuring\nservice-oriented search intents, we take one step towards making entities\nactionable. The main contribution of this paper is a pipeline of components we\ndevelop to construct a knowledge base of entity intents. We evaluate\nperformance both component-wise and end-to-end, and demonstrate that our\napproach is able to generate high-quality data. \n\n"}
{"id": "1809.00540", "contents": "Title: Multilingual Clustering of Streaming News Abstract: Clustering news across languages enables efficient media monitoring by\naggregating articles from multilingual sources into coherent stories. Doing so\nin an online setting allows scalable processing of massive news streams. To\nthis end, we describe a novel method for clustering an incoming stream of\nmultilingual documents into monolingual and crosslingual story clusters. Unlike\ntypical clustering approaches that consider a small and known number of labels,\nwe tackle the problem of discovering an ever growing number of cluster labels\nin an online fashion, using real news datasets in multiple languages. Our\nmethod is simple to implement, computationally efficient and produces\nstate-of-the-art results on datasets in German, English and Spanish. \n\n"}
{"id": "1809.00979", "contents": "Title: Regularizing Matrix Factorization with User and Item Embeddings for\n  Recommendation Abstract: Following recent successes in exploiting both latent factor and word\nembedding models in recommendation, we propose a novel Regularized\nMulti-Embedding (RME) based recommendation model that simultaneously\nencapsulates the following ideas via decomposition: (1) which items a user\nlikes, (2) which two users co-like the same items, (3) which two items users\noften co-liked, and (4) which two items users often co-disliked. In\nexperimental validation, the RME outperforms competing state-of-the-art models\nin both explicit and implicit feedback datasets, significantly improving\nRecall@5 by 5.9~7.0%, NDCG@20 by 4.3~5.6%, and MAP@10 by 7.9~8.9%. In addition,\nunder the cold-start scenario for users with the lowest number of interactions,\nagainst the competing models, the RME outperforms NDCG@5 by 20.2% and 29.4% in\nMovieLens-10M and MovieLens-20M datasets, respectively. Our datasets and source\ncode are available at: https://github.com/thanhdtran/RME.git. \n\n"}
{"id": "1809.01477", "contents": "Title: A Supervised Learning Approach For Heading Detection Abstract: As the Portable Document Format (PDF) file format increases in popularity,\nresearch in analysing its structure for text extraction and analysis is\nnecessary. Detecting headings can be a crucial component of classifying and\nextracting meaningful data. This research involves training a supervised\nlearning model to detect headings with features carefully selected through\nrecursive feature elimination. The best performing classifier had an accuracy\nof 96.95%, sensitivity of 0.986 and a specificity of 0.953. This research into\nheading detection contributes to the field of PDF based text extraction and can\nbe applied to the automation of large scale PDF text analysis in a variety of\nprofessional and policy based contexts. \n\n"}
{"id": "1809.01478", "contents": "Title: Weakly-Supervised Neural Text Classification Abstract: Deep neural networks are gaining increasing popularity for the classic text\nclassification task, due to their strong expressive power and less requirement\nfor feature engineering. Despite such attractiveness, neural text\nclassification models suffer from the lack of training data in many real-world\napplications. Although many semi-supervised and weakly-supervised text\nclassification models exist, they cannot be easily applied to deep neural\nmodels and meanwhile support limited supervision types. In this paper, we\npropose a weakly-supervised method that addresses the lack of training data in\nneural text classification. Our method consists of two modules: (1) a\npseudo-document generator that leverages seed information to generate\npseudo-labeled documents for model pre-training, and (2) a self-training module\nthat bootstraps on real unlabeled data for model refinement. Our method has the\nflexibility to handle different types of weak supervision and can be easily\nintegrated into existing deep neural models for text classification. We have\nperformed extensive experiments on three real-world datasets from different\ndomains. The results demonstrate that our proposed method achieves inspiring\nperformance without requiring excessive training data and outperforms baseline\nmethods significantly. \n\n"}
{"id": "1809.01479", "contents": "Title: UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification Abstract: The Fact Extraction and VERification (FEVER) shared task was launched to\nsupport the development of systems able to verify claims by extracting\nsupporting or refuting facts from raw text. The shared task organizers provide\na large-scale dataset for the consecutive steps involved in claim verification,\nin particular, document retrieval, fact extraction, and claim classification.\nIn this paper, we present our claim verification pipeline approach, which,\naccording to the preliminary results, scored third in the shared task, out of\n23 competing systems. For the document retrieval, we implemented a new entity\nlinking approach. In order to be able to rank candidate facts and classify a\nclaim on the basis of several selected facts, we introduce two extensions to\nthe Enhanced LSTM (ESIM). \n\n"}
{"id": "1809.01740", "contents": "Title: Predicting Smoking Events with a Time-Varying Semi-Parametric Hawkes\n  Process Model Abstract: Health risks from cigarette smoking -- the leading cause of preventable death\nin the United States -- can be substantially reduced by quitting. Although most\nsmokers are motivated to quit, the majority of quit attempts fail. A number of\nstudies have explored the role of self-reported symptoms, physiologic\nmeasurements, and environmental context on smoking risk, but less work has\nfocused on the temporal dynamics of smoking events, including daily patterns\nand related nicotine effects. In this work, we examine these dynamics and\nimprove risk prediction by modeling smoking as a self-triggering process, in\nwhich previous smoking events modify current risk. Specifically, we fit smoking\nevents self-reported by 42 smokers to a time-varying semi-parametric Hawkes\nprocess (TV-SPHP) developed for this purpose. Results show that the TV-SPHP\nachieves superior prediction performance compared to related and existing\nmodels, with the incorporation of time-varying predictors having greatest\nbenefit over longer prediction windows. Moreover, the impact function\nillustrates previously unknown temporal dynamics of smoking, with possible\nconnections to nicotine metabolism to be explored in future work through a\nrandomized study design. By more effectively predicting smoking events and\nexploring a self-triggering component of smoking risk, this work supports\ndevelopment of novel or improved cessation interventions that aim to reduce\ndeath from smoking. \n\n"}
{"id": "1809.02235", "contents": "Title: A Bandit Approach to Multiple Testing with False Discovery Control Abstract: We propose an adaptive sampling approach for multiple testing which aims to\nmaximize statistical power while ensuring anytime false discovery control. We\nconsider $n$ distributions whose means are partitioned by whether they are\nbelow or equal to a baseline (nulls), versus above the baseline (actual\npositives). In addition, each distribution can be sequentially and repeatedly\nsampled. Inspired by the multi-armed bandit literature, we provide an algorithm\nthat takes as few samples as possible to exceed a target true positive\nproportion (i.e. proportion of actual positives discovered) while giving\nanytime control of the false discovery proportion (nulls predicted as actual\npositives). Our sample complexity results match known information theoretic\nlower bounds and through simulations we show a substantial performance\nimprovement over uniform sampling and an adaptive elimination style algorithm.\nGiven the simplicity of the approach, and its sample efficiency, the method has\npromise for wide adoption in the biological sciences, clinical testing for drug\ndiscovery, and online A/B/n testing problems. \n\n"}
{"id": "1809.02352", "contents": "Title: Multi-Target Prediction: A Unifying View on Problems and Methods Abstract: Multi-target prediction (MTP) is concerned with the simultaneous prediction\nof multiple target variables of diverse type. Due to its enormous application\npotential, it has developed into an active and rapidly expanding research field\nthat combines several subfields of machine learning, including multivariate\nregression, multi-label classification, multi-task learning, dyadic prediction,\nzero-shot learning, network inference, and matrix completion. In this paper, we\npresent a unifying view on MTP problems and methods. First, we formally discuss\ncommonalities and differences between existing MTP problems. To this end, we\nintroduce a general framework that covers the above subfields as special cases.\nAs a second contribution, we provide a structured overview of MTP methods. This\nis accomplished by identifying a number of key properties, which distinguish\nsuch methods and determine their suitability for different types of problems.\nFinally, we also discuss a few challenges for future research. \n\n"}
{"id": "1809.02921", "contents": "Title: Personalizing Fairness-aware Re-ranking Abstract: Personalized recommendation brings about novel challenges in ensuring\nfairness, especially in scenarios in which users are not the only stakeholders\ninvolved in the recommender system. For example, the system may want to ensure\nthat items from different providers have a fair chance of being recommended. To\nsolve this problem, we propose a Fairness-Aware Re-ranking algorithm (FAR) to\nbalance the ranking quality and provider-side fairness. We iteratively generate\nthe ranking list by trading off between accuracy and the coverage of the\nproviders. Although fair treatment of providers is desirable, users may differ\nin their receptivity to the addition of this type of diversity. Therefore,\npersonalized user tolerance towards provider diversification is incorporated.\nExperiments are conducted on both synthetic and real-world data. The results\nshow that our proposed re-ranking algorithm can significantly promote fairness\nwith a slight sacrifice in accuracy and can do so while being attentive to\nindividual user differences. \n\n"}
{"id": "1809.03186", "contents": "Title: Off-line vs. On-line Evaluation of Recommender Systems in Small\n  E-commerce Abstract: In this paper, we present our work towards comparing on-line and off-line\nevaluation metrics in the context of small e-commerce recommender systems.\nRecommending on small e-commerce enterprises is rather challenging due to the\nlower volume of interactions and low user loyalty, rarely extending beyond a\nsingle session. On the other hand, we usually have to deal with lower volumes\nof objects, which are easier to discover by users through various\nbrowsing/searching GUIs.\n  The main goal of this paper is to determine applicability of off-line\nevaluation metrics in learning true usability of recommender systems (evaluated\non-line in A/B testing). In total 800 variants of recommending algorithms were\nevaluated off-line w.r.t. 18 metrics covering rating-based, ranking-based,\nnovelty and diversity evaluation. The off-line results were afterwards compared\nwith on-line evaluation of 12 selected recommender variants and based on the\nresults, we tried to learn and utilize an off-line to on-line results\nprediction model.\n  Off-line results shown a great variance in performance w.r.t. different\nmetrics with the Pareto front covering 68\\% of the approaches. Furthermore, we\nobserved that on-line results are considerably affected by the novelty of\nusers. On-line metrics correlates positively with ranking-based metrics (AUC,\nMRR, nDCG) for novice users, while too high values of diversity and novelty had\na negative impact on the on-line results for them. For users with more visited\nitems, however, the diversity became more important, while ranking-based\nmetrics relevance gradually decrease. \n\n"}
{"id": "1809.03497", "contents": "Title: A Correlation Maximization Approach for Cross Domain Co-Embeddings Abstract: Although modern recommendation systems can exploit the structure in users'\nitem feedback, most are powerless in the face of new users who provide no\nstructure for them to exploit. In this paper we introduce ImplicitCE, an\nalgorithm for recommending items to new users during their sign-up flow.\nImplicitCE works by transforming users' implicit feedback towards auxiliary\ndomain items into an embedding in the target domain item embedding space.\nImplicitCE learns these embedding spaces and transformation function in an\nend-to-end fashion and can co-embed users and items with any differentiable\nsimilarity function. To train ImplicitCE we explore methods for maximizing the\ncorrelations between model predictions and users' affinities and introduce\nSample Correlation Update, a novel and extremely simple training strategy.\nFinally, we show that ImplicitCE trained with Sample Correlation Update\noutperforms a variety of state of the art algorithms and loss functions on both\na large scale Twitter dataset and the DBLP dataset. \n\n"}
{"id": "1809.03577", "contents": "Title: Using Image Fairness Representations in Diversity-Based Re-ranking for\n  Recommendations Abstract: The trade-off between relevance and fairness in personalized recommendations\nhas been explored in recent works, with the goal of minimizing learned\ndiscrimination towards certain demographics while still producing relevant\nresults.\n  We present a fairness-aware variation of the Maximal Marginal Relevance (MMR)\nre-ranking method which uses representations of demographic groups computed\nusing a labeled dataset. This method is intended to incorporate fairness with\nrespect to these demographic groups.\n  We perform an experiment on a stock photo dataset and examine the trade-off\nbetween relevance and fairness against a well known baseline, MMR, by using\nhuman judgment to examine the results of the re-ranking when using different\nfractions of a labeled dataset, and by performing a quantitative analysis on\nthe ranked results of a set of query images. We show that our proposed method\ncan incorporate fairness in the ranked results while obtaining higher precision\nthan the baseline, while our case study shows that even a limited amount of\nlabeled data can be used to compute the representations to obtain fairness.\nThis method can be used as a post-processing step for recommender systems and\nsearch. \n\n"}
{"id": "1809.03901", "contents": "Title: Mitigating Confirmation Bias on Twitter by Recommending Opposing Views Abstract: In this work, we propose a content-based recommendation approach to increase\nexposure to opposing beliefs and opinions. Our aim is to help provide users\nwith more diverse viewpoints on issues, which are discussed in partisan groups\nfrom different perspectives. Since due to the backfire effect, people's\noriginal beliefs tend to strengthen when challenged with counter evidence, we\nneed to expose them to opposing viewpoints at the right time. The preliminary\nwork presented here describes our first step into this direction. As\nillustrative showcase, we take the political debate on Twitter around the\npresidency of Donald Trump. \n\n"}
{"id": "1809.04019", "contents": "Title: Training and Prediction Data Discrepancies: Challenges of Text\n  Classification with Noisy, Historical Data Abstract: Industry datasets used for text classification are rarely created for that\npurpose. In most cases, the data and target predictions are a by-product of\naccumulated historical data, typically fraught with noise, present in both the\ntext-based document, as well as in the targeted labels. In this work, we\naddress the question of how well performance metrics computed on noisy,\nhistorical data reflect the performance on the intended future machine learning\nmodel input. The results demonstrate the utility of dirty training datasets\nused to build prediction models for cleaner (and different) prediction inputs. \n\n"}
{"id": "1809.04094", "contents": "Title: FIVR: Fine-grained Incident Video Retrieval Abstract: This paper introduces the problem of Fine-grained Incident Video Retrieval\n(FIVR). Given a query video, the objective is to retrieve all associated\nvideos, considering several types of associations that range from duplicate\nvideos to videos from the same incident. FIVR offers a single framework that\ncontains several retrieval tasks as special cases. To address the benchmarking\nneeds of all such tasks, we construct and present a large-scale annotated video\ndataset, which we call FIVR-200K, and it comprises 225,960 videos. To create\nthe dataset, we devise a process for the collection of YouTube videos based on\nmajor news events from recent years crawled from Wikipedia and deploy a\nretrieval pipeline for the automatic selection of query videos based on their\nestimated suitability as benchmarks. We also devise a protocol for the\nannotation of the dataset with respect to the four types of video associations\ndefined by FIVR. Finally, we report the results of an experimental study on the\ndataset comparing five state-of-the-art methods developed based on a variety of\nvisual descriptors, highlighting the challenges of the current problem. \n\n"}
{"id": "1809.05274", "contents": "Title: Dueling Bandits with Qualitative Feedback Abstract: We formulate and study a novel multi-armed bandit problem called the\nqualitative dueling bandit (QDB) problem, where an agent observes not numeric\nbut qualitative feedback by pulling each arm. We employ the same regret as the\ndueling bandit (DB) problem where the duel is carried out by comparing the\nqualitative feedback. Although we can naively use classic DB algorithms for\nsolving the QDB problem, this reduction significantly worsens the\nperformance---actually, in the QDB problem, the probability that one arm wins\nthe duel over another arm can be directly estimated without carrying out actual\nduels. In this paper, we propose such direct algorithms for the QDB problem.\nOur theoretical analysis shows that the proposed algorithms significantly\noutperform DB algorithms by incorporating the qualitative feedback, and\nexperimental results also demonstrate vast improvement over the existing DB\nalgorithms. \n\n"}
{"id": "1809.06120", "contents": "Title: cf2vec: Collaborative Filtering algorithm selection using graph\n  distributed representations Abstract: Algorithm selection using Metalearning aims to find mappings between problem\ncharacteristics (i.e. metafeatures) with relative algorithm performance to\npredict the best algorithm(s) for new datasets. Therefore, it is of the utmost\nimportance that the metafeatures used are informative. In Collaborative\nFiltering, recent research has created an extensive collection of such\nmetafeatures. However, since these are created based on the practitioner's\nunderstanding of the problem, they may not capture the most relevant aspects\nnecessary to properly characterize the problem. We propose to overcome this\nproblem by taking advantage of Representation Learning, which is able to create\nan alternative problem characterizations by having the data guide the design of\nthe representation instead of the practitioner's opinion. Our hypothesis states\nthat such alternative representations can be used to replace standard\nmetafeatures, hence hence leading to a more robust approach to Metalearning. We\npropose a novel procedure specially designed for Collaborative Filtering\nalgorithm selection. The procedure models Collaborative Filtering as graphs and\nextracts distributed representations using graph2vec. Experimental results show\nthat the proposed procedure creates representations that are competitive with\nstate-of-the-art metafeatures, while requiring significantly less data and\nwithout virtually any human input. \n\n"}
{"id": "1809.06491", "contents": "Title: Triad-based Neural Network for Coreference Resolution Abstract: We propose a triad-based neural network system that generates affinity scores\nbetween entity mentions for coreference resolution. The system simultaneously\naccepts three mentions as input, taking mutual dependency and logical\nconstraints of all three mentions into account, and thus makes more accurate\npredictions than the traditional pairwise approach. Depending on system\nchoices, the affinity scores can be further used in clustering or mention\nranking. Our experiments show that a standard hierarchical clustering using the\nscores produces state-of-art results with gold mentions on the English portion\nof CoNLL 2012 Shared Task. The model does not rely on many handcrafted features\nand is easy to train and use. The triads can also be easily extended to polyads\nof higher orders. To our knowledge, this is the first neural network system to\nmodel mutual dependency of more than two members at mention level. \n\n"}
{"id": "1809.07011", "contents": "Title: Positive-Unlabeled Classification under Class Prior Shift and Asymmetric\n  Error Abstract: Bottlenecks of binary classification from positive and unlabeled data (PU\nclassification) are the requirements that given unlabeled patterns are drawn\nfrom the test marginal distribution, and the penalty of the false positive\nerror is identical to the false negative error. However, such requirements are\noften not fulfilled in practice. In this paper, we generalize PU classification\nto the class prior shift and asymmetric error scenarios. Based on the analysis\nof the Bayes optimal classifier, we show that given a test class prior, PU\nclassification under class prior shift is equivalent to PU classification with\nasymmetric error. Then, we propose two different frameworks to handle these\nproblems, namely, a risk minimization framework and density ratio estimation\nframework. Finally, we demonstrate the effectiveness of the proposed frameworks\nand compare both frameworks through experiments using benchmark datasets. \n\n"}
{"id": "1809.07276", "contents": "Title: Music Mood Detection Based On Audio And Lyrics With Deep Neural Net Abstract: We consider the task of multimodal music mood prediction based on the audio\nsignal and the lyrics of a track. We reproduce the implementation of\ntraditional feature engineering based approaches and propose a new model based\non deep learning. We compare the performance of both approaches on a database\ncontaining 18,000 tracks with associated valence and arousal values and show\nthat our approach outperforms classical models on the arousal detection task,\nand that both approaches perform equally on the valence prediction task. We\nalso compare the a posteriori fusion with fusion of modalities optimized\nsimultaneously with each unimodal model, and observe a significant improvement\nof valence prediction. We release part of our database for comparison purposes. \n\n"}
{"id": "1809.07754", "contents": "Title: PriPeARL: A Framework for Privacy-Preserving Analytics and Reporting at\n  LinkedIn Abstract: Preserving privacy of users is a key requirement of web-scale analytics and\nreporting applications, and has witnessed a renewed focus in light of recent\ndata breaches and new regulations such as GDPR. We focus on the problem of\ncomputing robust, reliable analytics in a privacy-preserving manner, while\nsatisfying product requirements. We present PriPeARL, a framework for\nprivacy-preserving analytics and reporting, inspired by differential privacy.\nWe describe the overall design and architecture, and the key modeling\ncomponents, focusing on the unique challenges associated with privacy,\ncoverage, utility, and consistency. We perform an experimental study in the\ncontext of ads analytics and reporting at LinkedIn, thereby demonstrating the\ntradeoffs between privacy and utility needs, and the applicability of\nprivacy-preserving mechanisms to real-world data. We also highlight the lessons\nlearned from the production deployment of our system at LinkedIn. \n\n"}
{"id": "1809.08415", "contents": "Title: Differentiable Unbiased Online Learning to Rank Abstract: Online Learning to Rank (OLTR) methods optimize rankers based on user\ninteractions. State-of-the-art OLTR methods are built specifically for linear\nmodels. Their approaches do not extend well to non-linear models such as neural\nnetworks. We introduce an entirely novel approach to OLTR that constructs a\nweighted differentiable pairwise loss after each interaction: Pairwise\nDifferentiable Gradient Descent (PDGD). PDGD breaks away from the traditional\napproach that relies on interleaving or multileaving and extensive sampling of\nmodels to estimate gradients. Instead, its gradient is based on inferring\npreferences between document pairs from user clicks and can optimize any\ndifferentiable model. We prove that the gradient of PDGD is unbiased w.r.t.\nuser document pair preferences. Our experiments on the largest publicly\navailable Learning to Rank (LTR) datasets show considerable and significant\nimprovements under all levels of interaction noise. PDGD outperforms existing\nOLTR methods both in terms of learning speed as well as final convergence.\nFurthermore, unlike previous OLTR methods, PDGD also allows for non-linear\nmodels to be optimized effectively. Our results show that using a neural\nnetwork leads to even better performance at convergence than a linear model. In\nsummary, PDGD is an efficient and unbiased OLTR approach that provides a better\nuser experience than previously possible. \n\n"}
{"id": "1809.08706", "contents": "Title: Is Ordered Weighted $\\ell_1$ Regularized Regression Robust to\n  Adversarial Perturbation? A Case Study on OSCAR Abstract: Many state-of-the-art machine learning models such as deep neural networks\nhave recently shown to be vulnerable to adversarial perturbations, especially\nin classification tasks. Motivated by adversarial machine learning, in this\npaper we investigate the robustness of sparse regression models with strongly\ncorrelated covariates to adversarially designed measurement noises.\nSpecifically, we consider the family of ordered weighted $\\ell_1$ (OWL)\nregularized regression methods and study the case of OSCAR (octagonal shrinkage\nclustering algorithm for regression) in the adversarial setting. Under a\nnorm-bounded threat model, we formulate the process of finding a maximally\ndisruptive noise for OWL-regularized regression as an optimization problem and\nillustrate the steps towards finding such a noise in the case of OSCAR.\nExperimental results demonstrate that the regression performance of grouping\nstrongly correlated features can be severely degraded under our adversarial\nsetting, even when the noise budget is significantly smaller than the\nground-truth signals. \n\n"}
{"id": "1809.08717", "contents": "Title: Unified recurrent neural network for many feature types Abstract: There are time series that are amenable to recurrent neural network (RNN)\nsolutions when treated as sequences, but some series, e.g. asynchronous time\nseries, provide a richer variation of feature types than current RNN cells take\ninto account. In order to address such situations, we introduce a unified RNN\nthat handles five different feature types, each in a different manner. Our RNN\nframework separates sequential features into two groups dependent on their\nfrequency, which we call sparse and dense features, and which affect cell\nupdates differently. Further, we also incorporate time features at the\nsequential level that relate to the time between specified events in the\nsequence and are used to modify the cell's memory state. We also include two\ntypes of static (whole sequence level) features, one related to time and one\nnot, which are combined with the encoder output. The experiments show that the\nmodeling framework proposed does increase performance compared to standard\ncells. \n\n"}
{"id": "1809.08771", "contents": "Title: Modeling longitudinal data using matrix completion Abstract: In clinical practice and biomedical research, measurements are often\ncollected sparsely and irregularly in time while the data acquisition is\nexpensive and inconvenient. Examples include measurements of spine bone mineral\ndensity, cancer growth through mammography or biopsy, a progression of\ndefective vision, or assessment of gait in patients with neurological\ndisorders. Since the data collection is often costly and inconvenient,\nestimation of progression from sparse observations is of great interest for\npractitioners.\n  From the statistical standpoint, such data is often analyzed in the context\nof a mixed-effect model where time is treated as both a fixed-effect\n(population progression curve) and a random-effect (individual variability).\nAlternatively, researchers analyze Gaussian processes or functional data where\nobservations are assumed to be drawn from a certain distribution of processes.\nThese models are flexible but rely on probabilistic assumptions, require very\ncareful implementation, specific to the given problem, and tend to be slow in\npractice.\n  In this study, we propose an alternative elementary framework for analyzing\nlongitudinal data, relying on matrix completion. Our method yields estimates of\nprogression curves by iterative application of the Singular Value\nDecomposition. Our framework covers multivariate longitudinal data, regression,\nand can be easily extended to other settings. As it relies on existing tools\nfor matrix algebra it is efficient and easy to implement.\n  We apply our methods to understand trends of progression of motor impairment\nin children with Cerebral Palsy. Our model approximates individual progression\ncurves and explains 30% of the variability. Low-rank representation of\nprogression trends enables identification of different progression trends in\nsubtypes of Cerebral Palsy. \n\n"}
{"id": "1809.09096", "contents": "Title: Text Summarization as Tree Transduction by Top-Down TreeLSTM Abstract: Extractive compression is a challenging natural language processing problem.\nThis work contributes by formulating neural extractive compression as a parse\ntree transduction problem, rather than a sequence transduction task. Motivated\nby this, we introduce a deep neural model for learning\nstructure-to-substructure tree transductions by extending the standard Long\nShort-Term Memory, considering the parent-child relationships in the structural\nrecursion. The proposed model can achieve state of the art performance on\nsentence compression benchmarks, both in terms of accuracy and compression\nrate. \n\n"}
{"id": "1809.10044", "contents": "Title: No One is Perfect: Analysing the Performance of Question Answering\n  Components over the DBpedia Knowledge Graph Abstract: Question answering (QA) over knowledge graphs has gained significant momentum\nover the past five years due to the increasing availability of large knowledge\ngraphs and the rising importance of question answering for user interaction.\nDBpedia has been the most prominently used knowledge graph in this setting and\nmost approaches currently use a pipeline of processing steps connecting a\nsequence of components. In this article, we analyse and micro evaluate the\nbehaviour of 29 available QA components for DBpedia knowledge graph that were\nreleased by the research community since 2010. As a result, we provide a\nperspective on collective failure cases, suggest characteristics of QA\ncomponents that prevent them from performing better and provide future\nchallenges and research directions for the field. \n\n"}
{"id": "1809.10330", "contents": "Title: Variance reduction properties of the reparameterization trick Abstract: The reparameterization trick is widely used in variational inference as it\nyields more accurate estimates of the gradient of the variational objective\nthan alternative approaches such as the score function method. Although there\nis overwhelming empirical evidence in the literature showing its success, there\nis relatively little research exploring why the reparameterization trick is so\neffective. We explore this under the idealized assumptions that the variational\napproximation is a mean-field Gaussian density and that the log of the joint\ndensity of the model parameters and the data is a quadratic function that\ndepends on the variational mean. From this, we show that the marginal variances\nof the reparameterization gradient estimator are smaller than those of the\nscore function gradient estimator. We apply the result of our idealized\nanalysis to real-world examples. \n\n"}
{"id": "1809.10745", "contents": "Title: A Short Survey of Topological Data Analysis in Time Series and Systems\n  Analysis Abstract: Topological Data Analysis (TDA) is the collection of mathematical tools that\ncapture the structure of shapes in data. Despite computational topology and\ncomputational geometry, the utilization of TDA in time series and signal\nprocessing is relatively new. In some recent contributions, TDA has been\nutilized as an alternative to the conventional signal processing methods.\nSpecifically, TDA is been considered to deal with noisy signals and time\nseries. In these applications, TDA is used to find the shapes in data as the\nmain properties, while the other properties are assumed much less informative.\nIn this paper, we will review recent developments and contributions where\ntopological data analysis especially persistent homology has been applied to\ntime series analysis, dynamical systems and signal processing. We will cover\nproblem statements such as stability determination, risk analysis, systems\nbehaviour, and predicting critical transitions in financial markets. \n\n"}
{"id": "1810.00113", "contents": "Title: Predicting the Generalization Gap in Deep Networks with Margin\n  Distributions Abstract: As shown in recent research, deep neural networks can perfectly fit randomly\nlabeled data, but with very poor accuracy on held out data. This phenomenon\nindicates that loss functions such as cross-entropy are not a reliable\nindicator of generalization. This leads to the crucial question of how\ngeneralization gap should be predicted from the training data and network\nparameters. In this paper, we propose such a measure, and conduct extensive\nempirical studies on how well it can predict the generalization gap. Our\nmeasure is based on the concept of margin distribution, which are the distances\nof training points to the decision boundary. We find that it is necessary to\nuse margin distributions at multiple layers of a deep network. On the CIFAR-10\nand the CIFAR-100 datasets, our proposed measure correlates very strongly with\nthe generalization gap. In addition, we find the following other factors to be\nof importance: normalizing margin values for scale independence, using\ncharacterizations of margin distribution rather than just the margin (closest\ndistance to decision boundary), and working in log space instead of linear\nspace (effectively using a product of margins rather than a sum). Our measure\ncan be easily applied to feedforward deep networks with any architecture and\nmay point towards new training loss functions that could enable better\ngeneralization. \n\n"}
{"id": "1810.00223", "contents": "Title: Generalized Multichannel Variational Autoencoder for Underdetermined\n  Source Separation Abstract: This paper deals with a multichannel audio source separation problem under\nunderdetermined conditions. Multichannel Non-negative Matrix Factorization\n(MNMF) is one of powerful approaches, which adopts the NMF concept for source\npower spectrogram modeling. This concept is also employed in Independent\nLow-Rank Matrix Analysis (ILRMA), a special class of the MNMF framework\nformulated under determined conditions. While these methods work reasonably\nwell for particular types of sound sources, one limitation is that they can\nfail to work for sources with spectrograms that do not comply with the NMF\nmodel. To address this limitation, an extension of ILRMA called the\nMultichannel Variational Autoencoder (MVAE) method was recently proposed, where\na Conditional VAE (CVAE) is used instead of the NMF model for source power\nspectrogram modeling. This approach has shown to perform impressively in\ndetermined source separation tasks thanks to the representation power of DNNs.\nWhile the original MVAE method was formulated under determined mixing\nconditions, this paper generalizes it so that it can also deal with\nunderdetermined cases. We call the proposed framework the Generalized MVAE\n(GMVAE). The proposed method was evaluated on a underdetermined source\nseparation task of separating out three sources from two microphone inputs.\nExperimental results revealed that the GMVAE method achieved better performance\nthan the MNMF method. \n\n"}
{"id": "1810.00368", "contents": "Title: Deep Quality-Value (DQV) Learning Abstract: We introduce a novel Deep Reinforcement Learning (DRL) algorithm called Deep\nQuality-Value (DQV) Learning. DQV uses temporal-difference learning to train a\nValue neural network and uses this network for training a second Quality-value\nnetwork that learns to estimate state-action values. We first test DQV's update\nrules with Multilayer Perceptrons as function approximators on two classic RL\nproblems, and then extend DQV with the use of Deep Convolutional Neural\nNetworks, `Experience Replay' and `Target Neural Networks' for tackling four\ngames of the Atari Arcade Learning environment. Our results show that DQV\nlearns significantly faster and better than Deep Q-Learning and Double Deep\nQ-Learning, suggesting that our algorithm can potentially be a better\nperforming synchronous temporal difference algorithm than what is currently\npresent in DRL. \n\n"}
{"id": "1810.01392", "contents": "Title: WAIC, but Why? Generative Ensembles for Robust Anomaly Detection Abstract: Machine learning models encounter Out-of-Distribution (OoD) errors when the\ndata seen at test time are generated from a different stochastic generator than\nthe one used to generate the training data. One proposal to scale OoD detection\nto high-dimensional data is to learn a tractable likelihood approximation of\nthe training distribution, and use it to reject unlikely inputs. However,\nlikelihood models on natural data are themselves susceptible to OoD errors, and\neven assign large likelihoods to samples from other datasets. To mitigate this\nproblem, we propose Generative Ensembles, which robustify density-based OoD\ndetection by way of estimating epistemic uncertainty of the likelihood model.\nWe present a puzzling observation in need of an explanation -- although\nlikelihood measures cannot account for the typical set of a distribution, and\ntherefore should not be suitable on their own for OoD detection, WAIC performs\nsurprisingly well in practice. \n\n"}
{"id": "1810.01539", "contents": "Title: Automated learning with a probabilistic programming language: Birch Abstract: This work offers a broad perspective on probabilistic modeling and inference\nin light of recent advances in probabilistic programming, in which models are\nformally expressed in Turing-complete programming languages. We consider a\ntypical workflow and how probabilistic programming languages can help to\nautomate this workflow, especially in the matching of models with inference\nmethods. We focus on two properties of a model that are critical in this\nmatching: its structure---the conditional dependencies between random\nvariables---and its form---the precise mathematical definition of those\ndependencies. While the structure and form of a probabilistic model are often\nfixed a priori, it is a curiosity of probabilistic programming that they need\nnot be, and may instead vary according to random choices made during program\nexecution. We introduce a formal description of models expressed as programs,\nand discuss some of the ways in which probabilistic programming languages can\nreveal the structure and form of these, in order to tailor inference methods.\nWe demonstrate the ideas with a new probabilistic programming language called\nBirch, with a multiple object tracking example. \n\n"}
{"id": "1810.01588", "contents": "Title: Interpreting Layered Neural Networks via Hierarchical Modular\n  Representation Abstract: Interpreting the prediction mechanism of complex models is currently one of\nthe most important tasks in the machine learning field, especially with layered\nneural networks, which have achieved high predictive performance with various\npractical data sets. To reveal the global structure of a trained neural network\nin an interpretable way, a series of clustering methods have been proposed,\nwhich decompose the units into clusters according to the similarity of their\ninference roles. The main problems in these studies were that (1) we have no\nprior knowledge about the optimal resolution for the decomposition, or the\nappropriate number of clusters, and (2) there was no method with which to\nacquire knowledge about whether the outputs of each cluster have a positive or\nnegative correlation with the input and output dimension values. In this paper,\nto solve these problems, we propose a method for obtaining a hierarchical\nmodular representation of a layered neural network. The application of a\nhierarchical clustering method to a trained network reveals a tree-structured\nrelationship among hidden layer units, based on their feature vectors defined\nby their correlation with the input and output dimension values. \n\n"}
{"id": "1810.01778", "contents": "Title: A Bayesian model for sparse graphs with flexible degree distribution and\n  overlapping community structure Abstract: We consider a non-projective class of inhomogeneous random graph models with\ninterpretable parameters and a number of interesting asymptotic properties.\nUsing the results of Bollob\\'as et al. [2007], we show that i) the class of\nmodels is sparse and ii) depending on the choice of the parameters, the model\nis either scale-free, with power-law exponent greater than 2, or with an\nasymptotic degree distribution which is power-law with exponential cut-off. We\npropose an extension of the model that can accommodate an overlapping community\nstructure. Scalable posterior inference can be performed due to the specific\nchoice of the link probability. We present experiments on five different\nreal-world networks with up to 100,000 nodes and edges, showing that the model\ncan provide a good fit to the degree distribution and recovers well the latent\ncommunity structure. \n\n"}
{"id": "1810.01807", "contents": "Title: Disambiguating Music Artists at Scale with Audio Metric Learning Abstract: We address the problem of disambiguating large scale catalogs through the\ndefinition of an unknown artist clustering task. We explore the use of metric\nlearning techniques to learn artist embeddings directly from audio, and using a\ndedicated homonym artists dataset, we compare our method with a recent approach\nthat learn similar embeddings using artist classifiers. While both systems have\nthe ability to disambiguate unknown artists relying exclusively on audio, we\nshow that our system is more suitable in the case when enough audio data is\navailable for each artist in the train dataset. We also propose a new negative\nsampling method for metric learning that takes advantage of side information\nsuch as music genre during the learning phase and shows promising results for\nthe artist clustering task. \n\n"}
{"id": "1810.02030", "contents": "Title: Robust Estimation and Generative Adversarial Nets Abstract: Robust estimation under Huber's $\\epsilon$-contamination model has become an\nimportant topic in statistics and theoretical computer science. Statistically\noptimal procedures such as Tukey's median and other estimators based on depth\nfunctions are impractical because of their computational intractability. In\nthis paper, we establish an intriguing connection between $f$-GANs and various\ndepth functions through the lens of $f$-Learning. Similar to the derivation of\n$f$-GANs, we show that these depth functions that lead to statistically optimal\nrobust estimators can all be viewed as variational lower bounds of the total\nvariation distance in the framework of $f$-Learning. This connection opens the\ndoor of computing robust estimators using tools developed for training GANs. In\nparticular, we show in both theory and experiments that some appropriate\nstructures of discriminator networks with hidden layers in GANs lead to\nstatistically optimal robust location estimators for both Gaussian distribution\nand general elliptical distributions where first moment may not exist. \n\n"}
{"id": "1810.02263", "contents": "Title: Convergence and Dynamical Behavior of the ADAM Algorithm for Non-Convex\n  Stochastic Optimization Abstract: Adam is a popular variant of stochastic gradient descent for finding a local\nminimizer of a function. In the constant stepsize regime, assuming that the\nobjective function is differentiable and non-convex, we establish the\nconvergence in the long run of the iterates to a stationary point under a\nstability condition. The key ingredient is the introduction of a\ncontinuous-time version of Adam, under the form of a non-autonomous ordinary\ndifferential equation. This continuous-time system is a relevant approximation\nof the Adam iterates, in the sense that the interpolated Adam process converges\nweakly towards the solution to the ODE. The existence and the uniqueness of the\nsolution are established. We further show the convergence of the solution\ntowards the critical points of the objective function and quantify its\nconvergence rate under a Lojasiewicz assumption. Then, we introduce a novel\ndecreasing stepsize version of Adam. Under mild assumptions, it is shown that\nthe iterates are almost surely bounded and converge almost surely to critical\npoints of the objective function. Finally, we analyze the fluctuations of the\nalgorithm by means of a conditional central limit theorem. \n\n"}
{"id": "1810.02321", "contents": "Title: Optimal Learning with Anisotropic Gaussian SVMs Abstract: This paper investigates the nonparametric regression problem using SVMs with\nanisotropic Gaussian RBF kernels. Under the assumption that the target\nfunctions are resided in certain anisotropic Besov spaces, we establish the\nalmost optimal learning rates, more precisely, optimal up to some logarithmic\nfactor, presented by the effective smoothness. By taking the effective\nsmoothness into consideration, our almost optimal learning rates are faster\nthan those obtained with the underlying RKHSs being certain anisotropic Sobolev\nspaces. Moreover, if the target function depends only on fewer dimensions,\nfaster learning rates can be further achieved. \n\n"}
{"id": "1810.02406", "contents": "Title: Projective Inference in High-dimensional Problems: Prediction and\n  Feature Selection Abstract: This paper discusses predictive inference and feature selection for\ngeneralized linear models with scarce but high-dimensional data. We argue that\nin many cases one can benefit from a decision theoretically justified two-stage\napproach: first, construct a possibly non-sparse model that predicts well, and\nthen find a minimal subset of features that characterize the predictions. The\nmodel built in the first step is referred to as the \\emph{reference model} and\nthe operation during the latter step as predictive \\emph{projection}. The key\ncharacteristic of this approach is that it finds an excellent tradeoff between\nsparsity and predictive accuracy, and the gain comes from utilizing all\navailable information including prior and that coming from the left out\nfeatures. We review several methods that follow this principle and provide\nnovel methodological contributions. We present a new projection technique that\nunifies two existing techniques and is both accurate and fast to compute. We\nalso propose a way of evaluating the feature selection process using fast\nleave-one-out cross-validation that allows for easy and intuitive model size\nselection. Furthermore, we prove a theorem that helps to understand the\nconditions under which the projective approach could be beneficial. The\nbenefits are illustrated via several simulated and real world examples. \n\n"}
{"id": "1810.02789", "contents": "Title: Doubly Semi-Implicit Variational Inference Abstract: We extend the existing framework of semi-implicit variational inference\n(SIVI) and introduce doubly semi-implicit variational inference (DSIVI), a way\nto perform variational inference and learning when both the approximate\nposterior and the prior distribution are semi-implicit. In other words, DSIVI\nperforms inference in models where the prior and the posterior can be expressed\nas an intractable infinite mixture of some analytic density with a highly\nflexible implicit mixing distribution. We provide a sandwich bound on the\nevidence lower bound (ELBO) objective that can be made arbitrarily tight.\nUnlike discriminator-based and kernel-based approaches to implicit variational\ninference, DSIVI optimizes a proper lower bound on ELBO that is asymptotically\nexact. We evaluate DSIVI on a set of problems that benefit from implicit\npriors. In particular, we show that DSIVI gives rise to a simple modification\nof VampPrior, the current state-of-the-art prior for variational autoencoders,\nwhich improves its performance. \n\n"}
{"id": "1810.02814", "contents": "Title: Statistical Optimality of Interpolated Nearest Neighbor Algorithms Abstract: In the era of deep learning, understanding over-fitting phenomenon becomes\nincreasingly important. It is observed that carefully designed deep neural\nnetworks achieve small testing error even when the training error is close to\nzero. One possible explanation is that for many modern machine learning\nalgorithms, over-fitting can greatly reduce the estimation bias, while not\nincreasing the estimation variance too much. To illustrate the above idea, we\nprove that the proposed interpolated nearest neighbor algorithm achieves the\nminimax optimal rate in both regression and classification regimes, and observe\nthat they are empirically better than the traditional $k$ nearest neighbor\nmethod in some cases. \n\n"}
{"id": "1810.02840", "contents": "Title: Training Complex Models with Multi-Task Weak Supervision Abstract: As machine learning models continue to increase in complexity, collecting\nlarge hand-labeled training sets has become one of the biggest roadblocks in\npractice. Instead, weaker forms of supervision that provide noisier but cheaper\nlabels are often used. However, these weak supervision sources have diverse and\nunknown accuracies, may output correlated labels, and may label different tasks\nor apply at different levels of granularity. We propose a framework for\nintegrating and modeling such weak supervision sources by viewing them as\nlabeling different related sub-tasks of a problem, which we refer to as the\nmulti-task weak supervision setting. We show that by solving a matrix\ncompletion-style problem, we can recover the accuracies of these multi-task\nsources given their dependency structure, but without any labeled data, leading\nto higher-quality supervision for training an end model. Theoretically, we show\nthat the generalization error of models trained with this approach improves\nwith the number of unlabeled data points, and characterize the scaling with\nrespect to the task and dependency structures. On three fine-grained\nclassification problems, we show that our approach leads to average gains of\n20.2 points in accuracy over a traditional supervised approach, 6.8 points over\na majority vote baseline, and 4.1 points over a previously proposed weak\nsupervision method that models tasks separately. \n\n"}
{"id": "1810.02938", "contents": "Title: Co-Stack Residual Affinity Networks with Multi-level Attention\n  Refinement for Matching Text Sequences Abstract: Learning a matching function between two text sequences is a long standing\nproblem in NLP research. This task enables many potential applications such as\nquestion answering and paraphrase identification. This paper proposes Co-Stack\nResidual Affinity Networks (CSRAN), a new and universal neural architecture for\nthis problem. CSRAN is a deep architecture, involving stacked (multi-layered)\nrecurrent encoders. Stacked/Deep architectures are traditionally difficult to\ntrain, due to the inherent weaknesses such as difficulty with feature\npropagation and vanishing gradients. CSRAN incorporates two novel components to\ntake advantage of the stacked architecture. Firstly, it introduces a new\nbidirectional alignment mechanism that learns affinity weights by fusing\nsequence pairs across stacked hierarchies. Secondly, it leverages a multi-level\nattention refinement component between stacked recurrent layers. The key\nintuition is that, by leveraging information across all network hierarchies, we\ncan not only improve gradient flow but also improve overall performance. We\nconduct extensive experiments on six well-studied text sequence matching\ndatasets, achieving state-of-the-art performance on all. \n\n"}
{"id": "1810.03067", "contents": "Title: Geocoding Without Geotags: A Text-based Approach for reddit Abstract: In this paper, we introduce the first geolocation inference approach for\nreddit, a social media platform where user pseudonymity has thus far made\nsupervised demographic inference difficult to implement and validate. In\nparticular, we design a text-based heuristic schema to generate ground truth\nlocation labels for reddit users in the absence of explicitly geotagged data.\nAfter evaluating the accuracy of our labeling procedure, we train and test\nseveral geolocation inference models across our reddit data set and three\nbenchmark Twitter geolocation data sets. Ultimately, we show that geolocation\nmodels trained and applied on the same domain substantially outperform models\nattempting to transfer training data across domains, even more so on reddit\nwhere platform-specific interest-group metadata can be used to improve\ninferences. \n\n"}
{"id": "1810.03235", "contents": "Title: Entity-Relationship Search over the Web Abstract: Entity-Relationship (E-R) Search is a complex case of Entity Search where the\ngoal is to search for multiple unknown entities and relationships connecting\nthem. We assume that a E-R query can be decomposed as a sequence of sub-queries\neach containing keywords related to a specific entity or relationship. We adopt\na probabilistic formulation of the E-R search problem. When creating specific\nrepresentations for entities (e.g. context terms) and for pairs of entities\n(i.e. relationships) it is possible to create a graph of probabilistic\ndependencies between sub-queries and entity plus relationship representations.\nTo the best of our knowledge this represents the first probabilistic model of\nE-R search. We propose and develop a novel supervised Early Fusion-based model\nfor E-R search, the Entity-Relationship Dependence Model (ERDM). It uses Markov\nRandom Field to model term dependencies of E-R sub-queries and\nentity/relationship documents. We performed experiments with more than 800M\nentities and relationships extractions from ClueWeb-09-B with FACC1 entity\nlinking. We obtained promising results using 3 different query collections\ncomprising 469 E-R queries, with results showing that it is possible to perform\nE-R search without using fix and pre-defined entity and relationship types,\nenabling a wide range of queries to be addressed. \n\n"}
{"id": "1810.03463", "contents": "Title: Graph Embedding with Shifted Inner Product Similarity and Its Improved\n  Approximation Capability Abstract: We propose shifted inner-product similarity (SIPS), which is a novel yet very\nsimple extension of the ordinary inner-product similarity (IPS) for\nneural-network based graph embedding (GE). In contrast to IPS, that is limited\nto approximating positive-definite (PD) similarities, SIPS goes beyond the\nlimitation by introducing bias terms in IPS; we theoretically prove that SIPS\nis capable of approximating not only PD but also conditionally PD (CPD)\nsimilarities with many examples such as cosine similarity, negative Poincare\ndistance and negative Wasserstein distance. Since SIPS with sufficiently large\nneural networks learns a variety of similarities, SIPS alleviates the need for\nconfiguring the similarity function of GE. Approximation error rate is also\nevaluated, and experiments on two real-world datasets demonstrate that graph\nembedding using SIPS indeed outperforms existing methods. \n\n"}
{"id": "1810.03545", "contents": "Title: Stein Neural Sampler Abstract: We propose two novel samplers to generate high-quality samples from a given\n(un-normalized) probability density. Motivated by the success of generative\nadversarial networks, we construct our samplers using deep neural networks that\ntransform a reference distribution to the target distribution. Training schemes\nare developed to minimize two variations of the Stein discrepancy, which is\ndesigned to work with un-normalized densities. Once trained, our samplers are\nable to generate samples instantaneously. We show that the proposed methods are\ntheoretically sound and experience fewer convergence issues compared with\ntraditional sampling approaches according to our empirical studies. \n\n"}
{"id": "1810.03743", "contents": "Title: JOBS: Joint-Sparse Optimization from Bootstrap Samples Abstract: Classical signal recovery based on $\\ell_1$ minimization solves the least\nsquares problem with all available measurements via sparsity-promoting\nregularization. In practice, it is often the case that not all measurements are\navailable or required for recovery. Measurements might be corrupted/missing or\nthey arrive sequentially in streaming fashion. In this paper, we propose a\nglobal sparse recovery strategy based on subsets of measurements, named JOBS,\nin which multiple measurements vectors are generated from the original pool of\nmeasurements via bootstrapping, and then a joint-sparse constraint is enforced\nto ensure support consistency among multiple predictors. The final estimate is\nobtained by averaging over the $K$ predictors. The performance limits\nassociated with different choices of number of bootstrap samples $L$ and number\nof estimates $K$ is analyzed theoretically. Simulation results validate some of\nthe theoretical analysis, and show that the proposed method yields\nstate-of-the-art recovery performance, outperforming $\\ell_1$ minimization and\na few other existing bootstrap-based techniques in the challenging case of low\nlevels of measurements and is preferable over other bagging-based methods in\nthe streaming setting since it performs better with small $K$ and $L$ for\ndata-sets with large sizes. \n\n"}
{"id": "1810.04040", "contents": "Title: Person-Job Fit: Adapting the Right Talent for the Right Job with Joint\n  Representation Learning Abstract: Person-Job Fit is the process of matching the right talent for the right job\nby identifying talent competencies that are required for the job. While many\nqualitative efforts have been made in related fields, it still lacks of\nquantitative ways of measuring talent competencies as well as the job's talent\nrequirements. To this end, in this paper, we propose a novel end-to-end\ndata-driven model based on Convolutional Neural Network (CNN), namely\nPerson-Job Fit Neural Network (PJFNN), for matching a talent qualification to\nthe requirements of a job. To be specific, PJFNN is a bipartite neural network\nwhich can effectively learn the joint representation of Person-Job fitness from\nhistorical job applications. In particular, due to the design of a hierarchical\nrepresentation structure, PJFNN can not only estimate whether a candidate fits\na job, but also identify which specific requirement items in the job posting\nare satisfied by the candidate by measuring the distances between corresponding\nlatent representations. Finally, the extensive experiments on a large-scale\nreal-world dataset clearly validate the performance of PJFNN in terms of\nPerson-Job Fit prediction. Also, we provide effective data visualization to\nshow some job and talent benchmark insights obtained by PJFNN. \n\n"}
{"id": "1810.04249", "contents": "Title: Data-dependent compression of random features for large-scale kernel\n  approximation Abstract: Kernel methods offer the flexibility to learn complex relationships in\nmodern, large data sets while enjoying strong theoretical guarantees on\nquality. Unfortunately, these methods typically require cubic running time in\nthe data set size, a prohibitive cost in the large-data setting. Random feature\nmaps (RFMs) and the Nystrom method both consider low-rank approximations to the\nkernel matrix as a potential solution. But, in order to achieve desirable\ntheoretical guarantees, the former may require a prohibitively large number of\nfeatures J+, and the latter may be prohibitively expensive for high-dimensional\nproblems. We propose to combine the simplicity and generality of RFMs with a\ndata-dependent feature selection scheme to achieve desirable theoretical\napproximation properties of Nystrom with just O(log J+) features. Our key\ninsight is to begin with a large set of random features, then reduce them to a\nsmall number of weighted features in a data-dependent, computationally\nefficient way, while preserving the statistical guarantees of using the\noriginal large set of features. We demonstrate the efficacy of our method with\ntheory and experiments--including on a data set with over 50 million\nobservations. In particular, we show that our method achieves small kernel\nmatrix approximation error and better test set accuracy with provably fewer\nrandom features than state-of-the-art methods. \n\n"}
{"id": "1810.04401", "contents": "Title: V3C - a Research Video Collection Abstract: With the widespread use of smartphones as recording devices and the massive\ngrowth in bandwidth, the number and volume of video collections has increased\nsignificantly in the last years. This poses novel challenges to the management\nof these large-scale video data and especially to the analysis of and retrieval\nfrom such video collections. At the same time, existing video datasets used for\nresearch and experimentation are either not large enough to represent current\ncollections or do not reflect the properties of video commonly found on the\nInternet in terms of content, length, or resolution. In this paper, we\nintroduce the Vimeo Creative Commons Collection, in short V3C, a collection of\n28'450 videos (with overall length of about 3'800 hours) published under\ncreative commons license on Vimeo. V3C comes with a shot segmentation for each\nvideo, together with the resulting keyframes in original as well as reduced\nresolution and additional metadata. It is intended to be used from 2019 at the\nInternational large-scale TREC Video Retrieval Evaluation campaign (TRECVid). \n\n"}
{"id": "1810.04502", "contents": "Title: Is your Statement Purposeless? Predicting Computer Science Graduation\n  Admission Acceptance based on Statement Of Purpose Abstract: We present a quantitative, data-driven machine learning approach to mitigate\nthe problem of unpredictability of Computer Science Graduate School Admissions.\nIn this paper, we discuss the possibility of a system which may help\nprospective applicants evaluate their Statement of Purpose (SOP) based on our\nsystem output. We, then, identify feature sets which can be used to train a\npredictive model. We train a model over fifty manually verified SOPs for which\nit uses an SVM classifier and achieves the highest accuracy of 92% with 10-fold\ncross-validation. We also perform experiments to establish that Word Embedding\nbased features and Document Similarity-based features outperform other\nidentified feature combinations. We plan to deploy our application as a web\nservice and release it as a FOSS service. \n\n"}
{"id": "1810.04652", "contents": "Title: Learning Embeddings for Product Visual Search with Triplet Loss and\n  Online Sampling Abstract: In this paper, we propose learning an embedding function for content-based\nimage retrieval within the e-commerce domain using the triplet loss and an\nonline sampling method that constructs triplets from within a minibatch. We\ncompare our method to several strong baselines as well as recent works on the\nDeepFashion and Stanford Online Product datasets. Our approach significantly\noutperforms the state-of-the-art on the DeepFashion dataset. With a\nmodification to favor sampling minibatches from a single product category, the\nsame approach demonstrates competitive results when compared to the\nstate-of-the-art for the Stanford Online Products dataset. \n\n"}
{"id": "1810.04778", "contents": "Title: Offline Multi-Action Policy Learning: Generalization and Optimization Abstract: In many settings, a decision-maker wishes to learn a rule, or policy, that\nmaps from observable characteristics of an individual to an action. Examples\ninclude selecting offers, prices, advertisements, or emails to send to\nconsumers, as well as the problem of determining which medication to prescribe\nto a patient. While there is a growing body of literature devoted to this\nproblem, most existing results are focused on the case where data comes from a\nrandomized experiment, and further, there are only two possible actions, such\nas giving a drug to a patient or not. In this paper, we study the offline\nmulti-action policy learning problem with observational data and where the\npolicy may need to respect budget constraints or belong to a restricted policy\nclass such as decision trees. We build on the theory of efficient\nsemi-parametric inference in order to propose and implement a policy learning\nalgorithm that achieves asymptotically minimax-optimal regret. To the best of\nour knowledge, this is the first result of this type in the multi-action setup,\nand it provides a substantial performance improvement over the existing\nlearning algorithms. We then consider additional computational challenges that\narise in implementing our method for the case where the policy is restricted to\ntake the form of a decision tree. We propose two different approaches, one\nusing a mixed integer program formulation and the other using a tree-search\nbased algorithm. \n\n"}
{"id": "1810.04793", "contents": "Title: Patient2Vec: A Personalized Interpretable Deep Representation of the\n  Longitudinal Electronic Health Record Abstract: The wide implementation of electronic health record (EHR) systems facilitates\nthe collection of large-scale health data from real clinical settings. Despite\nthe significant increase in adoption of EHR systems, this data remains largely\nunexplored, but presents a rich data source for knowledge discovery from\npatient health histories in tasks such as understanding disease correlations\nand predicting health outcomes. However, the heterogeneity, sparsity, noise,\nand bias in this data present many complex challenges. This complexity makes it\ndifficult to translate potentially relevant information into machine learning\nalgorithms. In this paper, we propose a computational framework, Patient2Vec,\nto learn an interpretable deep representation of longitudinal EHR data which is\npersonalized for each patient. To evaluate this approach, we apply it to the\nprediction of future hospitalizations using real EHR data and compare its\npredictive performance with baseline methods. Patient2Vec produces a vector\nspace with meaningful structure and it achieves an AUC around 0.799\noutperforming baseline methods. In the end, the learned feature importance can\nbe visualized and interpreted at both the individual and population levels to\nbring clinical insights. \n\n"}
{"id": "1810.04920", "contents": "Title: Pairwise Augmented GANs with Adversarial Reconstruction Loss Abstract: We propose a novel autoencoding model called Pairwise Augmented GANs. We\ntrain a generator and an encoder jointly and in an adversarial manner. The\ngenerator network learns to sample realistic objects. In turn, the encoder\nnetwork at the same time is trained to map the true data distribution to the\nprior in latent space. To ensure good reconstructions, we introduce an\naugmented adversarial reconstruction loss. Here we train a discriminator to\ndistinguish two types of pairs: an object with its augmentation and the one\nwith its reconstruction. We show that such adversarial loss compares objects\nbased on the content rather than on the exact match. We experimentally\ndemonstrate that our model generates samples and reconstructions of quality\ncompetitive with state-of-the-art on datasets MNIST, CIFAR10, CelebA and\nachieves good quantitative results on CIFAR10. \n\n"}
{"id": "1810.04957", "contents": "Title: A Distributed and Accountable Approach to Offline Recommender Systems\n  Evaluation Abstract: Different software tools have been developed with the purpose of performing\noffline evaluations of recommender systems. However, the results obtained with\nthese tools may be not directly comparable because of subtle differences in the\nexperimental protocols and metrics. Furthermore, it is difficult to analyze in\nthe same experimental conditions several algorithms without disclosing their\nimplementation details. For these reasons, we introduce RecLab, an open source\nsoftware for evaluating recommender systems in a distributed fashion. By\nrelying on consolidated web protocols, we created RESTful APIs for training and\nquerying recommenders remotely. In this way, it is possible to easily integrate\ninto the same toolkit algorithms realized with different technologies. In\ndetails, the experimenter can perform an evaluation by simply visiting a web\ninterface provided by RecLab. The framework will then interact with all the\nselected recommenders and it will compute and display a comprehensive set of\nmeasures, each representing a different metric. The results of all experiments\nare permanently stored and publicly available in order to support\naccountability and comparative analyses. \n\n"}
{"id": "1810.05193", "contents": "Title: Understanding Priors in Bayesian Neural Networks at the Unit Level Abstract: We investigate deep Bayesian neural networks with Gaussian weight priors and\na class of ReLU-like nonlinearities. Bayesian neural networks with Gaussian\npriors are well known to induce an L2, \"weight decay\", regularization. Our\nresults characterize a more intricate regularization effect at the level of the\nunit activations. Our main result establishes that the induced prior\ndistribution on the units before and after activation becomes increasingly\nheavy-tailed with the depth of the layer. We show that first layer units are\nGaussian, second layer units are sub-exponential, and units in deeper layers\nare characterized by sub-Weibull distributions. Our results provide new\ntheoretical insight on deep Bayesian neural networks, which we corroborate with\nsimulation experiments. \n\n"}
{"id": "1810.05471", "contents": "Title: Safe Grid Search with Optimal Complexity Abstract: Popular machine learning estimators involve regularization parameters that\ncan be challenging to tune, and standard strategies rely on grid search for\nthis task. In this paper, we revisit the techniques of approximating the\nregularization path up to predefined tolerance $\\epsilon$ in a unified\nframework and show that its complexity is $O(1/\\sqrt[d]{\\epsilon})$ for\nuniformly convex loss of order $d \\geq 2$ and $O(1/\\sqrt{\\epsilon})$ for\nGeneralized Self-Concordant functions. This framework encompasses least-squares\nbut also logistic regression, a case that as far as we know was not handled as\nprecisely in previous works. We leverage our technique to provide refined\nbounds on the validation error as well as a practical algorithm for\nhyperparameter tuning. The latter has global convergence guarantee when\ntargeting a prescribed accuracy on the validation set. Last but not least, our\napproach helps relieving the practitioner from the (often neglected) task of\nselecting a stopping criterion when optimizing over the training set: our\nmethod automatically calibrates this criterion based on the targeted accuracy\non the validation set. \n\n"}
{"id": "1810.05558", "contents": "Title: Variational Bayesian Monte Carlo Abstract: Many probabilistic models of interest in scientific computing and machine\nlearning have expensive, black-box likelihoods that prevent the application of\nstandard techniques for Bayesian inference, such as MCMC, which would require\naccess to the gradient or a large number of likelihood evaluations. We\nintroduce here a novel sample-efficient inference framework, Variational\nBayesian Monte Carlo (VBMC). VBMC combines variational inference with\nGaussian-process based, active-sampling Bayesian quadrature, using the latter\nto efficiently approximate the intractable integral in the variational\nobjective. Our method produces both a nonparametric approximation of the\nposterior distribution and an approximate lower bound of the model evidence,\nuseful for model selection. We demonstrate VBMC both on several synthetic\nlikelihoods and on a neuronal model with data from real neurons. Across all\ntested problems and dimensions (up to $D = 10$), VBMC performs consistently\nwell in reconstructing the posterior and the model evidence with a limited\nbudget of likelihood evaluations, unlike other methods that work only in very\nlow dimensions. Our framework shows great promise as a novel tool for posterior\nand model inference with expensive, black-box likelihoods. \n\n"}
{"id": "1810.06758", "contents": "Title: Discriminator Rejection Sampling Abstract: We propose a rejection sampling scheme using the discriminator of a GAN to\napproximately correct errors in the GAN generator distribution. We show that\nunder quite strict assumptions, this will allow us to recover the data\ndistribution exactly. We then examine where those strict assumptions break down\nand design a practical algorithm - called Discriminator Rejection Sampling\n(DRS) - that can be used on real data-sets. Finally, we demonstrate the\nefficacy of DRS on a mixture of Gaussians and on the SAGAN model,\nstate-of-the-art in the image generation task at the time of developing this\nwork. On ImageNet, we train an improved baseline that increases the Inception\nScore from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65\nto 14.79. We then use DRS to further improve on this baseline, improving the\nInception Score to 76.08 and the FID to 13.75. \n\n"}
{"id": "1810.06943", "contents": "Title: The Deep Weight Prior Abstract: Bayesian inference is known to provide a general framework for incorporating\nprior knowledge or specific properties into machine learning models via\ncarefully choosing a prior distribution. In this work, we propose a new type of\nprior distributions for convolutional neural networks, deep weight prior (DWP),\nthat exploit generative models to encourage a specific structure of trained\nconvolutional filters e.g., spatial correlations of weights. We define DWP in\nthe form of an implicit distribution and propose a method for variational\ninference with such type of implicit priors. In experiments, we show that DWP\nimproves the performance of Bayesian neural networks when training data are\nlimited, and initialization of weights with samples from DWP accelerates\ntraining of conventional convolutional neural networks. \n\n"}
{"id": "1810.07158", "contents": "Title: Data Association with Gaussian Processes Abstract: The data association problem is concerned with separating data coming from\ndifferent generating processes, for example when data come from different data\nsources, contain significant noise, or exhibit multimodality. We present a\nfully Bayesian approach to this problem. Our model is capable of simultaneously\nsolving the data association problem and the induced supervised learning\nproblems. Underpinning our approach is the use of Gaussian process priors to\nencode the structure of both the data and the data associations. We present an\nefficient learning scheme based on doubly stochastic variational inference and\ndiscuss how it can be applied to deep Gaussian process priors. \n\n"}
{"id": "1810.07287", "contents": "Title: Signed iterative random forests to identify enhancer-associated\n  transcription factor binding Abstract: Standard ChIP-seq peak calling pipelines seek to differentiate biochemically\nreproducible signals of individual genomic elements from background noise.\nHowever, reproducibility alone does not imply functional regulation (e.g.,\nenhancer activation, alternative splicing). Here we present a general-purpose,\ninterpretable machine learning method: signed iterative random forests (siRF),\nwhich we use to infer regulatory interactions among transcription factors and\nfunctional binding signatures surrounding enhancer elements in Drosophila\nmelanogaster. \n\n"}
{"id": "1810.07382", "contents": "Title: Analysis of Railway Accidents' Narratives Using Deep Learning Abstract: Automatic understanding of domain specific texts in order to extract useful\nrelationships for later use is a non-trivial task. One such relationship would\nbe between railroad accidents' causes and their correspondent descriptions in\nreports. From 2001 to 2016 rail accidents in the U.S. cost more than $4.6B.\nRailroads involved in accidents are required to submit an accident report to\nthe Federal Railroad Administration (FRA). These reports contain a variety of\nfixed field entries including primary cause of the accidents (a coded variable\nwith 389 values) as well as a narrative field which is a short text description\nof the accident. Although these narratives provide more information than a\nfixed field entry, the terminologies used in these reports are not easy to\nunderstand by a non-expert reader. Therefore, providing an assisting method to\nfill in the primary cause from such domain specific texts(narratives) would\nhelp to label the accidents with more accuracy. Another important question for\ntransportation safety is whether the reported accident cause is consistent with\nnarrative description. To address these questions, we applied deep learning\nmethods together with powerful word embeddings such as Word2Vec and GloVe to\nclassify accident cause values for the primary cause field using the text in\nthe narratives. The results show that such approaches can both accurately\nclassify accident causes based on report narratives and find important\ninconsistencies in accident reporting. \n\n"}
{"id": "1810.08010", "contents": "Title: Variational Noise-Contrastive Estimation Abstract: Unnormalised latent variable models are a broad and flexible class of\nstatistical models. However, learning their parameters from data is\nintractable, and few estimation techniques are currently available for such\nmodels. To increase the number of techniques in our arsenal, we propose\nvariational noise-contrastive estimation (VNCE), building on NCE which is a\nmethod that only applies to unnormalised models. The core idea is to use a\nvariational lower bound to the NCE objective function, which can be optimised\nin the same fashion as the evidence lower bound (ELBO) in standard variational\ninference (VI). We prove that VNCE can be used for both parameter estimation of\nunnormalised models and posterior inference of latent variables. The developed\ntheory shows that VNCE has the same level of generality as standard VI, meaning\nthat advances made there can be directly imported to the unnormalised setting.\nWe validate VNCE on toy models and apply it to a realistic problem of\nestimating an undirected graphical model from incomplete data. \n\n"}
{"id": "1810.08033", "contents": "Title: Adaptivity of deep ReLU network for learning in Besov and mixed smooth\n  Besov spaces: optimal rate and curse of dimensionality Abstract: Deep learning has shown high performances in various types of tasks from\nvisual recognition to natural language processing, which indicates superior\nflexibility and adaptivity of deep learning. To understand this phenomenon\ntheoretically, we develop a new approximation and estimation error analysis of\ndeep learning with the ReLU activation for functions in a Besov space and its\nvariant with mixed smoothness. The Besov space is a considerably general\nfunction space including the Holder space and Sobolev space, and especially can\ncapture spatial inhomogeneity of smoothness. Through the analysis in the Besov\nspace, it is shown that deep learning can achieve the minimax optimal rate and\noutperform any non-adaptive (linear) estimator such as kernel ridge regression,\nwhich shows that deep learning has higher adaptivity to the spatial\ninhomogeneity of the target function than other estimators such as linear ones.\nIn addition to this, it is shown that deep learning can avoid the curse of\ndimensionality if the target function is in a mixed smooth Besov space. We also\nshow that the dependency of the convergence rate on the dimensionality is tight\ndue to its minimax optimality. These results support high adaptivity of deep\nlearning and its superior ability as a feature extractor. \n\n"}
{"id": "1810.08189", "contents": "Title: Convolutional Collaborative Filter Network for Video Based\n  Recommendation Systems Abstract: This analysis explores the temporal sequencing of objects in a movie trailer.\nTemporal sequencing of objects in a movie trailer (e.g., a long shot of an\nobject vs intermittent short shots) can convey information about the type of\nmovie, plot of the movie, role of the main characters, and the filmmakers\ncinematographic choices. When combined with historical customer data,\nsequencing analysis can be used to improve predictions of customer behavior.\nE.g., a customer buys tickets to a new movie and maybe the customer has seen\nmovies in the past that contained similar sequences. To explore object\nsequencing in movie trailers, we propose a video convolutional network to\ncapture actions and scenes that are predictive of customers' preferences. The\nmodel learns the specific nature of sequences for different types of objects\n(e.g., cars vs faces), and the role of sequences in predicting customer future\nbehavior. We show how such a temporal-aware model outperforms simple feature\npooling methods proposed in our previous works and, importantly, demonstrate\nthe additional model explain-ability allowed by such a model. \n\n"}
{"id": "1810.08765", "contents": "Title: Attribute-aware Collaborative Filtering: Survey and Classification Abstract: Attribute-aware CF models aims at rating prediction given not only the\nhistorical rating from users to items, but also the information associated with\nusers (e.g. age), items (e.g. price), or even ratings (e.g. rating time). This\npaper surveys works in the past decade developing attribute-aware CF systems,\nand discovered that mathematically they can be classified into four different\ncategories. We provide the readers not only the high level mathematical\ninterpretation of the existing works in this area but also the mathematical\ninsight for each category of models. Finally we provide in-depth experiment\nresults comparing the effectiveness of the major works in each category. \n\n"}
{"id": "1810.09079", "contents": "Title: Sparsemax and Relaxed Wasserstein for Topic Sparsity Abstract: Topic sparsity refers to the observation that individual documents usually\nfocus on several salient topics instead of covering a wide variety of topics,\nand a real topic adopts a narrow range of terms instead of a wide coverage of\nthe vocabulary. Understanding this topic sparsity is especially important for\nanalyzing user-generated web content and social media, which are featured in\nthe form of extremely short posts and discussions. As topic sparsity of\nindividual documents in online social media increases, so does the difficulty\nof analyzing the online text sources using traditional methods.\n  In this paper, we propose two novel neural models by providing sparse\nposterior distributions over topics based on the Gaussian sparsemax\nconstruction, enabling efficient training by stochastic backpropagation. We\nconstruct an inference network conditioned on the input data and infer the\nvariational distribution with the relaxed Wasserstein (RW) divergence. Unlike\nexisting works based on Gaussian softmax construction and Kullback-Leibler (KL)\ndivergence, our approaches can identify latent topic sparsity with training\nstability, predictive performance, and topic coherence. Experiments on\ndifferent genres of large text corpora have demonstrated the effectiveness of\nour models as they outperform both probabilistic and neural methods. \n\n"}
{"id": "1810.09098", "contents": "Title: Stochastic Gradient MCMC for State Space Models Abstract: State space models (SSMs) are a flexible approach to modeling complex time\nseries. However, inference in SSMs is often computationally prohibitive for\nlong time series. Stochastic gradient MCMC (SGMCMC) is a popular method for\nscalable Bayesian inference for large independent data. Unfortunately when\napplied to dependent data, such as in SSMs, SGMCMC's stochastic gradient\nestimates are biased as they break crucial temporal dependencies. To alleviate\nthis, we propose stochastic gradient estimators that control this bias by\nperforming additional computation in a `buffer' to reduce breaking\ndependencies. Furthermore, we derive error bounds for this bias and show a\ngeometric decay under mild conditions. Using these estimators, we develop novel\nSGMCMC samplers for discrete, continuous and mixed-type SSMs with analytic\nmessage passing. Our experiments on real and synthetic data demonstrate the\neffectiveness of our SGMCMC algorithms compared to batch MCMC, allowing us to\nscale inference to long time series with millions of time points. \n\n"}
{"id": "1810.09147", "contents": "Title: Summarizing User-generated Textual Content: Motivation and Methods for\n  Fairness in Algorithmic Summaries Abstract: As the amount of user-generated textual content grows rapidly, text\nsummarization algorithms are increasingly being used to provide users a quick\noverview of the information content. Traditionally, summarization algorithms\nhave been evaluated only based on how well they match human-written summaries\n(e.g. as measured by ROUGE scores). In this work, we propose to evaluate\nsummarization algorithms from a completely new perspective that is important\nwhen the user-generated data to be summarized comes from different socially\nsalient user groups, e.g. men or women, Caucasians or African-Americans, or\ndifferent political groups (Republicans or Democrats). In such cases, we check\nwhether the generated summaries fairly represent these different social groups.\nSpecifically, considering that an extractive summarization algorithm selects a\nsubset of the textual units (e.g. microblogs) in the original data for\ninclusion in the summary, we investigate whether this selection is fair or not.\nOur experiments over real-world microblog datasets show that existing\nsummarization algorithms often represent the socially salient user-groups very\ndifferently compared to their distributions in the original data. More\nimportantly, some groups are frequently under-represented in the generated\nsummaries, and hence get far less exposure than what they would have obtained\nin the original data. To reduce such adverse impacts, we propose novel\nfairness-preserving summarization algorithms which produce high-quality\nsummaries while ensuring fairness among various groups. To our knowledge, this\nis the first attempt to produce fair text summarization, and is likely to open\nup an interesting research direction. \n\n"}
{"id": "1810.09433", "contents": "Title: Bayesian multi-domain learning for cancer subtype discovery from\n  next-generation sequencing count data Abstract: Precision medicine aims for personalized prognosis and therapeutics by\nutilizing recent genome-scale high-throughput profiling techniques, including\nnext-generation sequencing (NGS). However, translating NGS data faces several\nchallenges. First, NGS count data are often overdispersed, requiring\nappropriate modeling. Second, compared to the number of involved molecules and\nsystem complexity, the number of available samples for studying complex\ndisease, such as cancer, is often limited, especially considering disease\nheterogeneity. The key question is whether we may integrate available data from\nall different sources or domains to achieve reproducible disease prognosis\nbased on NGS count data. In this paper, we develop a Bayesian Multi-Domain\nLearning (BMDL) model that derives domain-dependent latent representations of\noverdispersed count data based on hierarchical negative binomial factorization\nfor accurate cancer subtyping even if the number of samples for a specific\ncancer type is small. Experimental results from both our simulated and NGS\ndatasets from The Cancer Genome Atlas (TCGA) demonstrate the promising\npotential of BMDL for effective multi-domain learning without \"negative\ntransfer\" effects often seen in existing multi-task learning and transfer\nlearning methods. \n\n"}
{"id": "1810.09591", "contents": "Title: Applying Deep Learning To Airbnb Search Abstract: The application to search ranking is one of the biggest machine learning\nsuccess stories at Airbnb. Much of the initial gains were driven by a gradient\nboosted decision tree model. The gains, however, plateaued over time. This\npaper discusses the work done in applying neural networks in an attempt to\nbreak out of that plateau. We present our perspective not with the intention of\npushing the frontier of new modeling techniques. Instead, ours is a story of\nthe elements we found useful in applying neural networks to a real life\nproduct. Deep learning was steep learning for us. To other teams embarking on\nsimilar journeys, we hope an account of our struggles and triumphs will provide\nsome useful pointers. Bon voyage! \n\n"}
{"id": "1810.10004", "contents": "Title: Time-Aware and Corpus-Specific Entity Relatedness Abstract: Entity relatedness has emerged as an important feature in a plethora of\napplications such as information retrieval, entity recommendation and entity\nlinking. Given an entity, for instance a person or an organization, entity\nrelatedness measures can be exploited for generating a list of highly-related\nentities. However, the relation of an entity to some other entity depends on\nseveral factors, with time and context being two of the most important ones\n(where, in our case, context is determined by a particular corpus). For\nexample, the entities related to the International Monetary Fund are different\nnow compared to some years ago, while these entities also may highly differ in\nthe context of a USA news portal compared to a Greek news portal. In this\npaper, we propose a simple but flexible model for entity relatedness which\nconsiders time and entity aware word embeddings by exploiting the underlying\ncorpus. The proposed model does not require external knowledge and is language\nindependent, which makes it widely useful in a variety of applications. \n\n"}
{"id": "1810.10065", "contents": "Title: Statistical mechanics of low-rank tensor decomposition Abstract: Often, large, high dimensional datasets collected across multiple modalities\ncan be organized as a higher order tensor. Low-rank tensor decomposition then\narises as a powerful and widely used tool to discover simple low dimensional\nstructures underlying such data. However, we currently lack a theoretical\nunderstanding of the algorithmic behavior of low-rank tensor decompositions. We\nderive Bayesian approximate message passing (AMP) algorithms for recovering\narbitrarily shaped low-rank tensors buried within noise, and we employ dynamic\nmean field theory to precisely characterize their performance. Our theory\nreveals the existence of phase transitions between easy, hard and impossible\ninference regimes, and displays an excellent match with simulations. Moreover,\nit reveals several qualitative surprises compared to the behavior of symmetric,\ncubic tensor decomposition. Finally, we compare our AMP algorithm to the most\ncommonly used algorithm, alternating least squares (ALS), and demonstrate that\nAMP significantly outperforms ALS in the presence of noise. \n\n"}
{"id": "1810.10460", "contents": "Title: Distilling with Performance Enhanced Students Abstract: The task of accelerating large neural networks on general purpose hardware\nhas, in recent years, prompted the use of channel pruning to reduce network\nsize. However, the efficacy of pruning based approaches has since been called\ninto question. In this paper, we turn to distillation for model\ncompression---specifically, attention transfer---and develop a simple method\nfor discovering performance enhanced student networks. We combine channel\nsaliency metrics with empirical observations of runtime performance to design\nmore accurate networks for a given latency budget. We apply our methodology to\nresidual and densely-connected networks, and show that we are able to find\nresource-efficient student networks on different hardware platforms while\nmaintaining very high accuracy. These performance-enhanced student networks\nachieve up to 10% boosts in top-1 ImageNet accuracy over their channel-pruned\ncounterparts for the same inference time. \n\n"}
{"id": "1810.11347", "contents": "Title: Generating equilibrium molecules with deep neural networks Abstract: Discovery of atomistic systems with desirable properties is a major challenge\nin chemistry and material science. Here we introduce a novel, autoregressive,\nconvolutional deep neural network architecture that generates molecular\nequilibrium structures by sequentially placing atoms in three-dimensional\nspace. The model estimates the joint probability over molecular configurations\nwith tractable conditional probabilities which only depend on distances between\natoms and their nuclear charges. It combines concepts from state-of-the-art\natomistic neural networks with auto-regressive generative models for images and\nspeech. We demonstrate that the architecture is capable of generating molecules\nclose to equilibrium for constitutional isomers of C$_7$O$_2$H$_{10}$. \n\n"}
{"id": "1810.11630", "contents": "Title: Informative Features for Model Comparison Abstract: Given two candidate models, and a set of target observations, we address the\nproblem of measuring the relative goodness of fit of the two models. We propose\ntwo new statistical tests which are nonparametric, computationally efficient\n(runtime complexity is linear in the sample size), and interpretable. As a\nunique advantage, our tests can produce a set of examples (informative\nfeatures) indicating the regions in the data domain where one model fits\nsignificantly better than the other. In a real-world problem of comparing GAN\nmodels, the test power of our new test matches that of the state-of-the-art\ntest of relative goodness of fit, while being one order of magnitude faster. \n\n"}
{"id": "1810.11646", "contents": "Title: Removing Hidden Confounding by Experimental Grounding Abstract: Observational data is increasingly used as a means for making\nindividual-level causal predictions and intervention recommendations. The\nforemost challenge of causal inference from observational data is hidden\nconfounding, whose presence cannot be tested in data and can invalidate any\ncausal conclusion. Experimental data does not suffer from confounding but is\nusually limited in both scope and scale. We introduce a novel method of using\nlimited experimental data to correct the hidden confounding in causal effect\nmodels trained on larger observational data, even if the observational data\ndoes not fully overlap with the experimental data. Our method makes strictly\nweaker assumptions than existing approaches, and we prove conditions under\nwhich it yields a consistent estimator. We demonstrate our method's efficacy\nusing real-world data from a large educational experiment. \n\n"}
{"id": "1810.12027", "contents": "Title: Deep Reinforcement Learning based Recommendation with Explicit User-Item\n  Interactions Modeling Abstract: Recommendation is crucial in both academia and industry, and various\ntechniques are proposed such as content-based collaborative filtering, matrix\nfactorization, logistic regression, factorization machines, neural networks and\nmulti-armed bandits. However, most of the previous studies suffer from two\nlimitations: (1) considering the recommendation as a static procedure and\nignoring the dynamic interactive nature between users and the recommender\nsystems, (2) focusing on the immediate feedback of recommended items and\nneglecting the long-term rewards. To address the two limitations, in this paper\nwe propose a novel recommendation framework based on deep reinforcement\nlearning, called DRR. The DRR framework treats recommendation as a sequential\ndecision making procedure and adopts an \"Actor-Critic\" reinforcement learning\nscheme to model the interactions between the users and recommender systems,\nwhich can consider both the dynamic adaptation and long-term rewards.\nFurthermore, a state representation module is incorporated into DRR, which can\nexplicitly capture the interactions between items and users. Three\ninstantiation structures are developed. Extensive experiments on four\nreal-world datasets are conducted under both the offline and online evaluation\nsettings. The experimental results demonstrate the proposed DRR method indeed\noutperforms the state-of-the-art competitors. \n\n"}
{"id": "1811.00410", "contents": "Title: Dilated DenseNets for Relational Reasoning Abstract: Despite their impressive performance in many tasks, deep neural networks\noften struggle at relational reasoning. This has recently been remedied with\nthe introduction of a plug-in relational module that considers relations\nbetween pairs of objects. Unfortunately, this is combinatorially expensive. In\nthis extended abstract, we show that a DenseNet incorporating dilated\nconvolutions excels at relational reasoning on the Sort-of-CLEVR dataset,\nallowing us to forgo this relational module and its associated expense. \n\n"}
{"id": "1811.00542", "contents": "Title: Pymc-learn: Practical Probabilistic Machine Learning in Python Abstract: $\\textit{Pymc-learn}$ is a Python package providing a variety of\nstate-of-the-art probabilistic models for supervised and unsupervised machine\nlearning. It is inspired by $\\textit{scikit-learn}$ and focuses on bringing\nprobabilistic machine learning to non-specialists. It uses a general-purpose\nhigh-level language that mimics $\\textit{scikit-learn}$. Emphasis is put on\nease of use, productivity, flexibility, performance, documentation, and an API\nconsistent with $\\textit{scikit-learn}$. It depends on $\\textit{scikit-learn}$\nand $\\textit{pymc3}$ and is distributed under the new BSD-3 license,\nencouraging its use in both academia and industry. Source code, binaries, and\ndocumentation are available on http://github.com/pymc-learn/pymc-learn. \n\n"}
{"id": "1811.01159", "contents": "Title: Understanding and Comparing Scalable Gaussian Process Regression for Big\n  Data Abstract: As a non-parametric Bayesian model which produces informative predictive\ndistribution, Gaussian process (GP) has been widely used in various fields,\nlike regression, classification and optimization. The cubic complexity of\nstandard GP however leads to poor scalability, which poses challenges in the\nera of big data. Hence, various scalable GPs have been developed in the\nliterature in order to improve the scalability while retaining desirable\nprediction accuracy. This paper devotes to investigating the methodological\ncharacteristics and performance of representative global and local scalable GPs\nincluding sparse approximations and local aggregations from four main\nperspectives: scalability, capability, controllability and robustness. The\nnumerical experiments on two toy examples and five real-world datasets with up\nto 250K points offer the following findings. In terms of scalability, most of\nthe scalable GPs own a time complexity that is linear to the training size. In\nterms of capability, the sparse approximations capture the long-term spatial\ncorrelations, the local aggregations capture the local patterns but suffer from\nover-fitting in some scenarios. In terms of controllability, we could improve\nthe performance of sparse approximations by simply increasing the inducing\nsize. But this is not the case for local aggregations. In terms of robustness,\nlocal aggregations are robust to various initializations of hyperparameters due\nto the local attention mechanism. Finally, we highlight that the proper hybrid\nof global and local scalable GPs may be a promising way to improve both the\nmodel capability and scalability for big data. \n\n"}
{"id": "1811.01179", "contents": "Title: Large-scale Heteroscedastic Regression via Gaussian Process Abstract: Heteroscedastic regression considering the varying noises among observations\nhas many applications in the fields like machine learning and statistics. Here\nwe focus on the heteroscedastic Gaussian process (HGP) regression which\nintegrates the latent function and the noise function together in a unified\nnon-parametric Bayesian framework. Though showing remarkable performance, HGP\nsuffers from the cubic time complexity, which strictly limits its application\nto big data. To improve the scalability, we first develop a variational sparse\ninference algorithm, named VSHGP, to handle large-scale datasets. Furthermore,\ntwo variants are developed to improve the scalability and capability of VSHGP.\nThe first is stochastic VSHGP (SVSHGP) which derives a factorized evidence\nlower bound, thus enhancing efficient stochastic variational inference. The\nsecond is distributed VSHGP (DVSHGP) which (i) follows the Bayesian committee\nmachine formalism to distribute computations over multiple local VSHGP experts\nwith many inducing points; and (ii) adopts hybrid parameters for experts to\nguard against over-fitting and capture local variety. The superiority of DVSHGP\nand SVSHGP as compared to existing scalable heteroscedastic/homoscedastic GPs\nis then extensively verified on various datasets. \n\n"}
{"id": "1811.02033", "contents": "Title: Physics-Informed Generative Adversarial Networks for Stochastic\n  Differential Equations Abstract: We developed a new class of physics-informed generative adversarial networks\n(PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic\nproblems based on a limited number of scattered measurements. Unlike standard\nGANs relying only on data for training, here we encoded into the architecture\nof GANs the governing physical laws in the form of stochastic differential\nequations (SDEs) using automatic differentiation. In particular, we applied\nWasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability\ncompared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian\nprocesses of different correlation lengths based on data realizations collected\nfrom simultaneous reads at sparsely placed sensors. We obtained good\napproximation of the generated stochastic processes to the target ones even for\na mismatch between the input noise dimensionality and the effective\ndimensionality of the target stochastic processes. We also studied the\noverfitting issue for both the discriminator and generator, and we found that\noverfitting occurs also in the generator in addition to the discriminator as\npreviously reported. Subsequently, we considered the solution of elliptic SDEs\nrequiring approximations of three stochastic processes, namely the solution,\nthe forcing, and the diffusion coefficient. We used three generators for the\nPI-GANs, two of them were feed forward deep neural networks (DNNs) while the\nother one was the neural network induced by the SDE. Depending on the data, we\nemployed one or multiple feed forward DNNs as the discriminators in PI-GANs.\nHere, we have demonstrated the accuracy and effectiveness of PI-GANs in solving\nSDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high\ndimensional problems given more sensor data with low-polynomial growth in\ncomputational cost. \n\n"}
{"id": "1811.02459", "contents": "Title: Nonlinear Evolution via Spatially-Dependent Linear Dynamics for\n  Electrophysiology and Calcium Data Abstract: Latent variable models have been widely applied for the analysis of time\nseries resulting from experimental neuroscience techniques. In these datasets,\nobservations are relatively smooth and possibly nonlinear. We present\nVariational Inference for Nonlinear Dynamics (VIND), a variational inference\nframework that is able to uncover nonlinear, smooth latent dynamics from\nsequential data. The framework is a direct extension of PfLDS; including a\nstructured approximate posterior describing spatially-dependent linear\ndynamics, as well as an algorithm that relies on the fixed-point iteration\nmethod to achieve convergence. We apply VIND to electrophysiology, single-cell\nvoltage and widefield imaging datasets with state-of-the-art results in\nreconstruction error. In single-cell voltage data, VIND finds a 5D latent\nspace, with variables akin to those of Hodgkin-Huxley-like models. VIND's\nlearned dynamics are further quantified by predicting future neural activity.\nVIND excels in this task, in some cases substantially outperforming current\nmethods. \n\n"}
{"id": "1811.02815", "contents": "Title: SocialGCN: An Efficient Graph Convolutional Network based Model for\n  Social Recommendation Abstract: Collaborative Filtering (CF) is one of the most successful approaches for\nrecommender systems. With the emergence of online social networks, social\nrecommendation has become a popular research direction. Most of these social\nrecommendation models utilized each user's local neighbors' preferences to\nalleviate the data sparsity issue in CF. However, they only considered the\nlocal neighbors of each user and neglected the process that users' preferences\nare influenced as information diffuses in the social network. Recently, Graph\nConvolutional Networks~(GCN) have shown promising results by modeling the\ninformation diffusion process in graphs that leverage both graph structure and\nnode feature information. To this end, in this paper, we propose an effective\ngraph convolutional neural network based model for social recommendation. Based\non a classical CF model, the key idea of our proposed model is that we borrow\nthe strengths of GCNs to capture how users' preferences are influenced by the\nsocial diffusion process in social networks. The diffusion of users'\npreferences is built on a layer-wise diffusion manner, with the initial user\nembedding as a function of the current user's features and a free base user\nlatent vector that is not contained in the user feature. Similarly, each item's\nlatent vector is also a combination of the item's free latent vector, as well\nas its feature representation. Furthermore, we show that our proposed model is\nflexible when user and item features are not available. Finally, extensive\nexperimental results on two real-world datasets clearly show the effectiveness\nof our proposed model. \n\n"}
{"id": "1811.02834", "contents": "Title: Fused Gromov-Wasserstein distance for structured objects: theoretical\n  foundations and mathematical properties Abstract: Optimal transport theory has recently found many applications in machine\nlearning thanks to its capacity for comparing various machine learning objects\nconsidered as distributions. The Kantorovitch formulation, leading to the\nWasserstein distance, focuses on the features of the elements of the objects\nbut treat them independently, whereas the Gromov-Wasserstein distance focuses\nonly on the relations between the elements, depicting the structure of the\nobject, yet discarding its features.\n  In this paper we propose to extend these distances in order to encode\nsimultaneously both the feature and structure informations, resulting in the\nFused Gromov-Wasserstein distance. We develop the mathematical framework for\nthis novel distance, prove its metric and interpolation properties and provide\na concentration result for the convergence of finite samples. We also\nillustrate and interpret its use in various contexts where structured objects\nare involved. \n\n"}
{"id": "1811.03154", "contents": "Title: Poisson Multi-Bernoulli Mapping Using Gibbs Sampling Abstract: This paper addresses the mapping problem. Using a conjugate prior form, we\nderive the exact theoretical batch multi-object posterior density of the map\ngiven a set of measurements. The landmarks in the map are modeled as extended\nobjects, and the measurements are described as a Poisson process, conditioned\non the map. We use a Poisson process prior on the map and prove that the\nposterior distribution is a hybrid Poisson, multi-Bernoulli mixture\ndistribution. We devise a Gibbs sampling algorithm to sample from the batch\nmulti-object posterior. The proposed method can handle uncertainties in the\ndata associations and the cardinality of the set of landmarks, and is\nparallelizable, making it suitable for large-scale problems. The performance of\nthe proposed method is evaluated on synthetic data and is shown to outperform a\nstate-of-the-art method. \n\n"}
{"id": "1811.03862", "contents": "Title: Targeting Solutions in Bayesian Multi-Objective Optimization: Sequential\n  and Batch Versions Abstract: Multi-objective optimization aims at finding trade-off solutions to\nconflicting objectives. These constitute the Pareto optimal set. In the context\nof expensive-to-evaluate functions, it is impossible and often non-informative\nto look for the entire set. As an end-user would typically prefer a certain\npart of the objective space, we modify the Bayesian multi-objective\noptimization algorithm which uses Gaussian Processes to maximize the Expected\nHypervolume Improvement, to focus the search in the preferred region. The\ncumulated effects of the Gaussian Processes and the targeting strategy lead to\na particularly efficient convergence to the desired part of the Pareto set. To\ntake advantage of parallel computing, a multi-point extension of the targeting\ncriterion is proposed and analyzed. \n\n"}
{"id": "1811.04210", "contents": "Title: Densely Connected Attention Propagation for Reading Comprehension Abstract: We propose DecaProp (Densely Connected Attention Propagation), a new densely\nconnected neural architecture for reading comprehension (RC). There are two\ndistinct characteristics of our model. Firstly, our model densely connects all\npairwise layers of the network, modeling relationships between passage and\nquery across all hierarchical levels. Secondly, the dense connectors in our\nnetwork are learned via attention instead of standard residual skip-connectors.\nTo this end, we propose novel Bidirectional Attention Connectors (BAC) for\nefficiently forging connections throughout the network. We conduct extensive\nexperiments on four challenging RC benchmarks. Our proposed approach achieves\nstate-of-the-art results on all four, outperforming existing baselines by up to\n$2.6\\%-14.2\\%$ in absolute F1 score. \n\n"}
{"id": "1811.04288", "contents": "Title: IP Geolocation through Reverse DNS Abstract: IP Geolocation databases are widely used in online services to map end user\nIP addresses to their geographical locations. However, they use proprietary\ngeolocation methods and in some cases they have poor accuracy. We propose a\nsystematic approach to use publicly accessible reverse DNS hostnames for\ngeolocating IP addresses. Our method is designed to be combined with other\ngeolocation data sources. We cast the task as a machine learning problem where\nfor a given hostname, we generate and rank a list of potential location\ncandidates. We evaluate our approach against three state of the art academic\nbaselines and two state of the art commercial IP geolocation databases. We show\nthat our work significantly outperforms the academic baselines, and is\ncomplementary and competitive with commercial databases. To aid\nreproducibility, we open source our entire approach. \n\n"}
{"id": "1811.04380", "contents": "Title: ReSet: Learning Recurrent Dynamic Routing in ResNet-like Neural Networks Abstract: Neural Network is a powerful Machine Learning tool that shows outstanding\nperformance in Computer Vision, Natural Language Processing, and Artificial\nIntelligence. In particular, recently proposed ResNet architecture and its\nmodifications produce state-of-the-art results in image classification\nproblems. ResNet and most of the previously proposed architectures have a fixed\nstructure and apply the same transformation to all input images. In this work,\nwe develop a ResNet-based model that dynamically selects Computational Units\n(CU) for each input object from a learned set of transformations. Dynamic\nselection allows the network to learn a sequence of useful transformations and\napply only required units to predict the image label. We compare our model to\nResNet-38 architecture and achieve better results than the original ResNet on\nCIFAR-10.1 test set. While examining the produced paths, we discovered that the\nnetwork learned different routes for images from different classes and similar\nroutes for similar images. \n\n"}
{"id": "1811.04392", "contents": "Title: Deep Item-based Collaborative Filtering for Top-N Recommendation Abstract: Item-based Collaborative Filtering(short for ICF) has been widely adopted in\nrecommender systems in industry, owing to its strength in user interest\nmodeling and ease in online personalization. By constructing a user's profile\nwith the items that the user has consumed, ICF recommends items that are\nsimilar to the user's profile. With the prevalence of machine learning in\nrecent years, significant processes have been made for ICF by learning item\nsimilarity (or representation) from data. Nevertheless, we argue that most\nexisting works have only considered linear and shallow relationship between\nitems, which are insufficient to capture the complicated decision-making\nprocess of users.\n  In this work, we propose a more expressive ICF solution by accounting for the\nnonlinear and higher-order relationship among items. Going beyond modeling only\nthe second-order interaction (e.g. similarity) between two items, we\nadditionally consider the interaction among all interacted item pairs by using\nnonlinear neural networks. Through this way, we can effectively model the\nhigher-order relationship among items, capturing more complicated effects in\nuser decision-making. For example, it can differentiate which historical\nitemsets in a user's profile are more important in affecting the user to make a\npurchase decision on an item. We treat this solution as a deep variant of ICF,\nthus term it as DeepICF. To justify our proposal, we perform empirical studies\non two public datasets from MovieLens and Pinterest. Extensive experiments\nverify the highly positive effect of higher-order item interaction modeling\nwith nonlinear neural networks. Moreover, we demonstrate that by more\nfine-grained second-order interaction modeling with attention network, the\nperformance of our DeepICF method can be further improved. \n\n"}
{"id": "1811.04852", "contents": "Title: Quantum-inspired sublinear classical algorithms for solving low-rank\n  linear systems Abstract: We present classical sublinear-time algorithms for solving low-rank linear\nsystems of equations. Our algorithms are inspired by the HHL quantum algorithm\nfor solving linear systems and the recent breakthrough by Tang of dequantizing\nthe quantum algorithm for recommendation systems. Let $A \\in \\mathbb{C}^{m\n\\times n}$ be a rank-$k$ matrix, and $b \\in \\mathbb{C}^m$ be a vector. We\npresent two algorithms: a \"sampling\" algorithm that provides a sample from\n$A^{-1}b$ and a \"query\" algorithm that outputs an estimate of an entry of\n$A^{-1}b$, where $A^{-1}$ denotes the Moore-Penrose pseudo-inverse. Both of our\nalgorithms have query and time complexity $O(\\mathrm{poly}(k, \\kappa, \\|A\\|_F,\n1/\\epsilon)\\,\\mathrm{polylog}(m, n))$, where $\\kappa$ is the condition number\nof $A$ and $\\epsilon$ is the precision parameter. Note that the algorithms we\nconsider are sublinear time, so they cannot write and read the whole matrix or\nvectors. In this paper, we assume that $A$ and $b$ come with well-known\nlow-overhead data structures such that entries of $A$ and $b$ can be sampled\naccording to some natural probability distributions. Alternatively, when $A$ is\npositive semidefinite, our algorithms can be adapted so that the sampling\nassumption on $b$ is not required. \n\n"}
{"id": "1811.05402", "contents": "Title: Embedding Electronic Health Records for Clinical Information Retrieval Abstract: Neural network representation learning frameworks have recently shown to be\nhighly effective at a wide range of tasks ranging from radiography\ninterpretation via data-driven diagnostics to clinical decision support. This\noften superior performance comes at the price of dramatically increased\ntraining data requirements that cannot be satisfied in every given institution\nor scenario. As a means of countering such data sparsity effects, distant\nsupervision alleviates the need for scarce in-domain data by relying on a\nrelated, resource-rich, task for training.\n  This study presents an end-to-end neural clinical decision support system\nthat recommends relevant literature for individual patients (few available\nresources) via distant supervision on the well-known MIMIC-III collection\n(abundant resource). Our experiments show significant improvements in retrieval\neffectiveness over traditional statistical as well as purely locally supervised\nretrieval models. \n\n"}
{"id": "1811.06687", "contents": "Title: Deep Knockoffs Abstract: This paper introduces a machine for sampling approximate model-X knockoffs\nfor arbitrary and unspecified data distributions using deep generative models.\nThe main idea is to iteratively refine a knockoff sampling mechanism until a\ncriterion measuring the validity of the produced knockoffs is optimized; this\ncriterion is inspired by the popular maximum mean discrepancy in machine\nlearning and can be thought of as measuring the distance to pairwise\nexchangeability between original and knockoff features. By building upon the\nexisting model-X framework, we thus obtain a flexible and model-free\nstatistical tool to perform controlled variable selection. Extensive numerical\nexperiments and quantitative tests confirm the generality, effectiveness, and\npower of our deep knockoff machines. Finally, we apply this new method to a\nreal study of mutations linked to changes in drug resistance in the human\nimmunodeficiency virus. \n\n"}
{"id": "1811.06773", "contents": "Title: A Novel Approach to Sparse Inverse Covariance Estimation Using Transform\n  Domain Updates and Exponentially Adaptive Thresholding Abstract: Sparse Inverse Covariance Estimation (SICE) is useful in many practical data\nanalyses. Recovering the connectivity, non-connectivity graph of covariates is\nclassified amongst the most important data mining and learning problems. In\nthis paper, we introduce a novel SICE approach using adaptive thresholding. Our\nmethod is based on updates in a transformed domain of the desired matrix and\nexponentially decaying adaptive thresholding in the main domain (Inverse\nCovariance matrix domain). In addition to the proposed algorithm, the\nconvergence analysis is also provided. In the Numerical Experiments Section, we\nshow that the proposed method outperforms state-of-the-art methods in terms of\naccuracy. \n\n"}
{"id": "1811.07174", "contents": "Title: Link Prediction in Dynamic Graphs for Recommendation Abstract: Recent advances in employing neural networks on graph domains helped push the\nstate of the art in link prediction tasks, particularly in recommendation\nservices. However, the use of temporal contextual information, often modeled as\ndynamic graphs that encode the evolution of user-item relationships over time,\nhas been overlooked in link prediction problems. In this paper, we consider the\nhypothesis that leveraging such information enables models to make better\npredictions, proposing a new neural network approach for this. Our experiments,\nperformed on the widely used ML-100k and ML-1M datasets, show that our approach\nproduces better predictions in scenarios where the pattern of user-item\nrelationships change over time. In addition, they suggest that existing\napproaches are significantly impacted by those changes. \n\n"}
{"id": "1811.07209", "contents": "Title: A Statistical Approach to Assessing Neural Network Robustness Abstract: We present a new approach to assessing the robustness of neural networks\nbased on estimating the proportion of inputs for which a property is violated.\nSpecifically, we estimate the probability of the event that the property is\nviolated under an input model. Our approach critically varies from the formal\nverification framework in that when the property can be violated, it provides\nan informative notion of how robust the network is, rather than just the\nconventional assertion that the network is not verifiable. Furthermore, it\nprovides an ability to scale to larger networks than formal verification\napproaches. Though the framework still provides a formal guarantee of\nsatisfiability whenever it successfully finds one or more violations, these\nadvantages do come at the cost of only providing a statistical estimate of\nunsatisfiability whenever no violation is found. Key to the practical success\nof our approach is an adaptation of multi-level splitting, a Monte Carlo\napproach for estimating the probability of rare events, to our statistical\nrobustness framework. We demonstrate that our approach is able to emulate\nformal verification procedures on benchmark problems, while scaling to larger\nnetworks and providing reliable additional information in the form of accurate\nestimates of the violation probability. \n\n"}
{"id": "1811.09975", "contents": "Title: Sequential Variational Autoencoders for Collaborative Filtering Abstract: Variational autoencoders were proven successful in domains such as computer\nvision and speech processing. Their adoption for modeling user preferences is\nstill unexplored, although recently it is starting to gain attention in the\ncurrent literature. In this work, we propose a model which extends variational\nautoencoders by exploiting the rich information present in the past preference\nhistory. We introduce a recurrent version of the VAE, where instead of passing\na subset of the whole history regardless of temporal dependencies, we rather\npass the consumption sequence subset through a recurrent neural network. At\neach time-step of the RNN, the sequence is fed through a series of\nfully-connected layers, the output of which models the probability distribution\nof the most likely future preferences. We show that handling temporal\ninformation is crucial for improving the accuracy of the VAE: In fact, our\nmodel beats the current state-of-the-art by valuable margins because of its\nability to capture temporal dependencies among the user-consumption sequence\nusing the recurrent encoder still keeping the fundamentals of variational\nautoencoders intact. \n\n"}
{"id": "1811.10686", "contents": "Title: Beyond \"How may I help you?\": Assisting Customer Service Agents with\n  Proactive Responses Abstract: We study the problem of providing recommended responses to customer service\nagents in live-chat dialogue systems. Smart-reply systems have been widely\napplied in real-world applications (e.g. Gmail, LinkedIn Messaging), where most\nof them can successfully recommend reactive responses. However, we observe a\nmajor limitation of current methods is that they generally have difficulties in\nsuggesting proactive investigation act (e.g. \"Do you perhaps have another\naccount with us?\") due to the lack of long-term context information, which\nindeed act as critical steps for customer service agents to collect information\nand resolve customers' issues. Thus in this work, we propose an end-to-end\nmethod with special focus on suggesting proactive investigative questions to\ncustomer agents in Airbnb's customer service live-chat system. Effectiveness of\nour proposed method can be validated through qualitative and quantitative\nresults. \n\n"}
{"id": "1811.10810", "contents": "Title: A Scalable Optimization Mechanism for Pairwise based Discrete Hashing Abstract: Maintaining the pair similarity relationship among originally\nhigh-dimensional data into a low-dimensional binary space is a popular strategy\nto learn binary codes. One simiple and intutive method is to utilize two\nidentical code matrices produced by hash functions to approximate a pairwise\nreal label matrix. However, the resulting quartic problem is difficult to\ndirectly solve due to the non-convex and non-smooth nature of the objective. In\nthis paper, unlike previous optimization methods using various relaxation\nstrategies, we aim to directly solve the original quartic problem using a novel\nalternative optimization mechanism to linearize the quartic problem by\nintroducing a linear regression model. Additionally, we find that gradually\nlearning each batch of binary codes in a sequential mode, i.e. batch by batch,\nis greatly beneficial to the convergence of binary code learning. Based on this\nsignificant discovery and the proposed strategy, we introduce a scalable\nsymmetric discrete hashing algorithm that gradually and smoothly updates each\nbatch of binary codes. To further improve the smoothness, we also propose a\ngreedy symmetric discrete hashing algorithm to update each bit of batch binary\ncodes. Moreover, we extend the proposed optimization mechanism to solve the\nnon-convex optimization problems for binary code learning in many other\npairwise based hashing algorithms. Extensive experiments on benchmark\nsingle-label and multi-label databases demonstrate the superior performance of\nthe proposed mechanism over recent state-of-the-art methods. \n\n"}
{"id": "1811.10947", "contents": "Title: Reliable Semi-Supervised Learning when Labels are Missing at Random Abstract: Semi-supervised learning methods are motivated by the availability of large\ndatasets with unlabeled features in addition to labeled data. Unlabeled data\nis, however, not guaranteed to improve classification performance and has in\nfact been reported to impair the performance in certain cases. A fundamental\nsource of error arises from restrictive assumptions about the unlabeled\nfeatures, which result in unreliable classifiers that underestimate their\nprediction error probabilities. In this paper, we develop a semi-supervised\nlearning approach that relaxes such assumptions and is capable of providing\nclassifiers that reliably quantify the label uncertainty. The approach is\napplicable using any generative model with a supervised learning algorithm. We\nillustrate the approach using both handwritten digit and cloth classification\ndata where the labels are missing at random. \n\n"}
{"id": "1811.11133", "contents": "Title: A Concept-Centered Hypertext Approach to Case-Based Retrieval Abstract: The goal of case-based retrieval is to assist physicians in the clinical\ndecision making process, by finding relevant medical literature in large\narchives. We propose a research that aims at improving the effectiveness of\ncase-based retrieval systems through the use of automatically created\ndocument-level semantic networks. The proposed research tackles different\naspects of information systems and leverages the recent advancements in\ninformation extraction and relational learning to revisit and advance the core\nideas of concept-centered hypertext models. We propose a two-step methodology\nthat in the first step addresses the automatic creation of document-level\nsemantic networks, then in the second step it designs methods that exploit such\ndocument representations to retrieve relevant cases from medical literature.\nFor the automatic creation of documents' semantic networks, we design a\ncombination of information extraction techniques and relational learning\nmodels. Mining concepts and relations from text, information extraction\ntechniques represent the core of the document-level semantic networks' building\nprocess. On the other hand, relational learning models have the task of\nenriching the graph with additional connections that have not been detected by\ninformation extraction algorithms and strengthening the confidence score of\nextracted relations. For the retrieval of relevant medical literature, we\ninvestigate methods that are capable of comparing the documents' semantic\nnetworks in terms of structure and semantics. The automatic extraction of\nsemantic relations from documents, and their centrality in the creation of the\ndocuments' semantic networks, represent our attempt to go one step further than\nprevious graph-based approaches. \n\n"}
{"id": "1811.11523", "contents": "Title: Sequence Learning with RNNs for Medical Concept Normalization in\n  User-Generated Texts Abstract: In this work, we consider the medical concept normalization problem, i.e.,\nthe problem of mapping a disease mention in free-form text to a concept in a\ncontrolled vocabulary, usually to the standard thesaurus in the Unified Medical\nLanguage System (UMLS). This task is challenging since medical terminology is\nvery different when coming from health care professionals or from the general\npublic in the form of social media texts. We approach it as a sequence learning\nproblem, with recurrent neural networks trained to obtain semantic\nrepresentations of one- and multi-word expressions. We develop end-to-end\nneural architectures tailored specifically to medical concept normalization,\nincluding bidirectional LSTM and GRU with an attention mechanism and additional\nsemantic similarity features based on UMLS. Our evaluation over a standard\nbenchmark shows that our model improves over a state of the art baseline for\nclassification based on CNNs. \n\n"}
{"id": "1811.12050", "contents": "Title: Recurrent Deep Divergence-based Clustering for simultaneous feature\n  learning and clustering of variable length time series Abstract: The task of clustering unlabeled time series and sequences entails a\nparticular set of challenges, namely to adequately model temporal relations and\nvariable sequence lengths. If these challenges are not properly handled, the\nresulting clusters might be of suboptimal quality. As a key solution, we\npresent a joint clustering and feature learning framework for time series based\non deep learning. For a given set of time series, we train a recurrent network\nto represent, or embed, each time series in a vector space such that a\ndivergence-based clustering loss function can discover the underlying cluster\nstructure in an end-to-end manner. Unlike previous approaches, our model\ninherently handles multivariate time series of variable lengths and does not\nrequire specification of a distance-measure in the input space. On a diverse\nset of benchmark datasets we illustrate that our proposed Recurrent Deep\nDivergence-based Clustering approach outperforms, or performs comparable to,\nprevious approaches. \n\n"}
{"id": "1811.12386", "contents": "Title: Tree-Structured Recurrent Switching Linear Dynamical Systems for\n  Multi-Scale Modeling Abstract: Many real-world systems studied are governed by complex, nonlinear dynamics.\nBy modeling these dynamics, we can gain insight into how these systems work,\nmake predictions about how they will behave, and develop strategies for\ncontrolling them. While there are many methods for modeling nonlinear dynamical\nsystems, existing techniques face a trade off between offering interpretable\ndescriptions and making accurate predictions. Here, we develop a class of\nmodels that aims to achieve both simultaneously, smoothly interpolating between\nsimple descriptions and more complex, yet also more accurate models. Our\nprobabilistic model achieves this multi-scale property through a hierarchy of\nlocally linear dynamics that jointly approximate global nonlinear dynamics. We\ncall it the tree-structured recurrent switching linear dynamical system. To fit\nthis model, we present a fully-Bayesian sampling procedure using Polya-Gamma\ndata augmentation to allow for fast and conjugate Gibbs sampling. Through a\nvariety of synthetic and real examples, we show how these models outperform\nexisting methods in both interpretability and predictive capability. \n\n"}
{"id": "1811.12500", "contents": "Title: Sequential Embedding Induced Text Clustering, a Non-parametric Bayesian\n  Approach Abstract: Current state-of-the-art nonparametric Bayesian text clustering methods model\ndocuments through multinomial distribution on bags of words. Although these\nmethods can effectively utilize the word burstiness representation of documents\nand achieve decent performance, they do not explore the sequential information\nof text and relationships among synonyms. In this paper, the documents are\nmodeled as the joint of bags of words, sequential features and word embeddings.\nWe proposed Sequential Embedding induced Dirichlet Process Mixture Model\n(SiDPMM) to effectively exploit this joint document representation in text\nclustering. The sequential features are extracted by the encoder-decoder\ncomponent. Word embeddings produced by the continuous-bag-of-words (CBOW) model\nare introduced to handle synonyms. Experimental results demonstrate the\nbenefits of our model in two major aspects: 1) improved performance across\nmultiple diverse text datasets in terms of the normalized mutual information\n(NMI); 2) more accurate inference of ground truth cluster numbers with\nregularization effect on tiny outlier clusters. \n\n"}
{"id": "1812.00158", "contents": "Title: Approximating Categorical Similarity in Sponsored Search Relevance Abstract: Sponsored Search is a major source of revenue for web search engines. Since\nsponsored search follows a pay-per-click model, showing relevant ads for\nreceiving clicks is crucial. Matching categories of a query and its ad\ncandidates have been explored in modeling relevance of query-ad pairs. The\napproach involves matching cached categories of queries seen in the past to\ncategories of candidate ads. Since queries have a heavy tail distribution, the\napproach has limited coverage. In this work, we propose approximating\ncategorical similarity of a query-ad pairs using neural networks, particularly\nCLSM. Embedding of a query (or document) is generated using its tri-letter\nrepresentation which allows coverage of tail queries. Offline experiments of\nincorporating this feature as opposed to using the categories directly show a\n5.23% improvement in AUC ROC. A/B testing results show an improvement of 8.2%\nin relevance. \n\n"}
{"id": "1812.00239", "contents": "Title: Building robust classifiers through generation of confident out of\n  distribution examples Abstract: Deep learning models are known to be overconfident in their predictions on\nout of distribution inputs. There have been several pieces of work to address\nthis issue, including a number of approaches for building Bayesian neural\nnetworks, as well as closely related work on detection of out of distribution\nsamples. Recently, there has been work on building classifiers that are robust\nto out of distribution samples by adding a regularization term that maximizes\nthe entropy of the classifier output on out of distribution data. To\napproximate out of distribution samples (which are not known apriori), a GAN\nwas used for generation of samples at the edges of the training distribution.\nIn this paper, we introduce an alternative GAN based approach for building a\nrobust classifier, where the idea is to use the GAN to explicitly generate out\nof distribution samples that the classifier is confident on (low entropy), and\nhave the classifier maximize the entropy for these samples. We showcase the\neffectiveness of our approach relative to state-of-the-art on hand-written\ncharacters as well as on a variety of natural image datasets. \n\n"}
{"id": "1812.01190", "contents": "Title: EENMF: An End-to-End Neural Matching Framework for E-Commerce Sponsored\n  Search Abstract: E-commerce sponsored search contributes an important part of revenue for the\ne-commerce company. In consideration of effectiveness and efficiency, a\nlarge-scale sponsored search system commonly adopts a multi-stage architecture.\nWe name these stages as ad retrieval, ad pre-ranking and ad ranking. Ad\nretrieval and ad pre-ranking are collectively referred to as ad matching in\nthis paper. We propose an end-to-end neural matching framework (EENMF) to model\ntwo tasks---vector-based ad retrieval and neural networks based ad pre-ranking.\nUnder the deep matching framework, vector-based ad retrieval harnesses user\nrecent behavior sequence to retrieve relevant ad candidates without the\nconstraint of keyword bidding. Simultaneously, the deep model is employed to\nperform the global pre-ranking of ad candidates from multiple retrieval paths\neffectively and efficiently. Besides, the proposed model tries to optimize the\npointwise cross-entropy loss which is consistent with the objective of predict\nmodels in the ranking stage. We conduct extensive evaluation to validate the\nperformance of the proposed framework. In the real traffic of a large-scale\ne-commerce sponsored search, the proposed approach significantly outperforms\nthe baseline. \n\n"}
{"id": "1812.01353", "contents": "Title: Structured Semantic Model supported Deep Neural Network for\n  Click-Through Rate Prediction Abstract: With the rapid development of online advertising and recommendation systems,\nclick-through rate prediction is expected to play an increasingly important\nrole.Recently many DNN-based models which follow a similar Embedding&MLP\nparadigm have been proposed, and have achieved good result in image/voice and\nnlp fields. In these methods the Wide&Deep model announced by Google plays a\nkey role.Most models first map large scale sparse input features into\nlow-dimensional vectors which are transformed to fixed-length vectors, then\nconcatenated together before being fed into a multilayer perceptron (MLP) to\nlearn non-linear relations among input features. The number of trainable\nvariables normally grow dramatically the number of feature fields and the\nembedding dimension grow. It is a big challenge to get state-of-the-art result\nthrough training deep neural network and embedding together, which falls into\nlocal optimal or overfitting easily. In this paper, we propose an Structured\nSemantic Model (SSM) to tackles this challenge by designing a orthogonal base\nconvolution and pooling model which adaptively learn the multi-scale base\nsemantic representation between features supervised by the click label.The\noutput of SSM are then used in the Wide&Deep for CTR prediction.Experiments on\ntwo public datasets as well as real Weibo production dataset with over 1\nbillion samples have demonstrated the effectiveness of our proposed approach\nwith superior performance comparing to state-of-the-art methods. \n\n"}
{"id": "1812.01483", "contents": "Title: CompILE: Compositional Imitation Learning and Execution Abstract: We introduce Compositional Imitation Learning and Execution (CompILE): a\nframework for learning reusable, variable-length segments of\nhierarchically-structured behavior from demonstration data. CompILE uses a\nnovel unsupervised, fully-differentiable sequence segmentation module to learn\nlatent encodings of sequential data that can be re-composed and executed to\nperform new tasks. Once trained, our model generalizes to sequences of longer\nlength and from environment instances not seen during training. We evaluate\nCompILE in a challenging 2D multi-task environment and a continuous control\ntask, and show that it can find correct task boundaries and event encodings in\nan unsupervised manner. Latent codes and associated behavior policies\ndiscovered by CompILE can be used by a hierarchical agent, where the high-level\npolicy selects actions in the latent code space, and the low-level,\ntask-specific policies are simply the learned decoders. We found that our\nCompILE-based agent could learn given only sparse rewards, where agents without\ntask-specific policies struggle. \n\n"}
{"id": "1812.01495", "contents": "Title: Expanding search in the space of empirical ML Abstract: As researchers and practitioners of applied machine learning, we are given a\nset of requirements on the problem to be solved, the plausibly obtainable data,\nand the computational resources available. We aim to find (within those bounds)\nreliably useful combinations of problem, data, and algorithm. An emphasis on\nalgorithmic or technical novelty in ML conference publications leads to\nexploration of one dimension of this space. Data collection and ML deployment\nat scale in industry settings offers an environment for exploring the others.\nOur conferences and reviewing criteria can better support empirical ML by\nsoliciting and incentivizing experimentation and synthesis independent of\nalgorithmic innovation. \n\n"}
{"id": "1812.01504", "contents": "Title: Fighting Fire with Fire: Using Antidote Data to Improve Polarization and\n  Fairness of Recommender Systems Abstract: The increasing role of recommender systems in many aspects of society makes\nit essential to consider how such systems may impact social good. Various\nmodifications to recommendation algorithms have been proposed to improve their\nperformance for specific socially relevant measures. However, previous\nproposals are often not easily adapted to different measures, and they\ngenerally require the ability to modify either existing system inputs, the\nsystem's algorithm, or the system's outputs. As an alternative, in this paper\nwe introduce the idea of improving the social desirability of recommender\nsystem outputs by adding more data to the input, an approach we view as\nproviding `antidote' data to the system. We formalize the antidote data\nproblem, and develop optimization-based solutions. We take as our model system\nthe matrix factorization approach to recommendation, and we propose a set of\nmeasures to capture the polarization or fairness of recommendations. We then\nshow how to generate antidote data for each measure, pointing out a number of\ncomputational efficiencies, and discuss the impact on overall system accuracy.\nOur experiments show that a modest budget for antidote data can lead to\nsignificant improvements in the polarization or fairness of recommendations. \n\n"}
{"id": "1812.02108", "contents": "Title: Relative concentration bounds for the spectrum of kernel matrices Abstract: In this paper we study the concentration properties for the eigenvalues of\nkernel matrices, which are central objects in a wide range of kernel methods\nand, more recently, in network analysis. We present a set of concentration\ninequalities tailored for each individual eigenvalue of the kernel matrix with\nrespect to its known asymptotic limit. The inequalities presented here are of\nrelative type, meaning that they scale with the eigenvalue in consideration,\nwhich results in convergence rates that vary across the spectrum. The rates we\nobtain here are faster than the typical $\\O(\\frac{1}{\\sqrt n})$ and are often\nexponential, depending on regularity assumptions of Sobolev type. One key\nfeature of our results is that they apply to non positive kernels, which is\nfundamental in the context of network analysis. We show how our results are\nwell suited for the study of dot product kernels, which are related to random\ngeometric graphs on the sphere, via the graphon formalism. We illustrate our\nresults by applying them to a variety of dot product kernels on the sphere and\nto the one dimensional Gaussian kernel. \n\n"}
{"id": "1812.02353", "contents": "Title: Top-K Off-Policy Correction for a REINFORCE Recommender System Abstract: Industrial recommender systems deal with extremely large action spaces --\nmany millions of items to recommend. Moreover, they need to serve billions of\nusers, who are unique at any point in time, making a complex user state space.\nLuckily, huge quantities of logged implicit feedback (e.g., user clicks, dwell\ntime) are available for learning. Learning from the logged feedback is however\nsubject to biases caused by only observing feedback on recommendations selected\nby the previous versions of the recommender. In this work, we present a general\nrecipe of addressing such biases in a production top-K recommender system at\nYoutube, built with a policy-gradient-based algorithm, i.e. REINFORCE. The\ncontributions of the paper are: (1) scaling REINFORCE to a production\nrecommender system with an action space on the orders of millions; (2) applying\noff-policy correction to address data biases in learning from logged feedback\ncollected from multiple behavior policies; (3) proposing a novel top-K\noff-policy correction to account for our policy recommending multiple items at\na time; (4) showcasing the value of exploration. We demonstrate the efficacy of\nour approaches through a series of simulations and multiple live experiments on\nYoutube. \n\n"}
{"id": "1812.02833", "contents": "Title: Disentangling Disentanglement in Variational Autoencoders Abstract: We develop a generalisation of disentanglement in VAEs---decomposition of the\nlatent representation---characterising it as the fulfilment of two factors: a)\nthe latent encodings of the data having an appropriate level of overlap, and b)\nthe aggregate encoding of the data conforming to a desired structure,\nrepresented through the prior. Decomposition permits disentanglement, i.e.\nexplicit independence between latents, as a special case, but also allows for a\nmuch richer class of properties to be imposed on the learnt representation,\nsuch as sparsity, clustering, independent subspaces, or even intricate\nhierarchical dependency relationships. We show that the $\\beta$-VAE varies from\nthe standard VAE predominantly in its control of latent overlap and that for\nthe standard choice of an isotropic Gaussian prior, its objective is invariant\nto rotations of the latent representation. Viewed from the decomposition\nperspective, breaking this invariance with simple manipulations of the prior\ncan yield better disentanglement with little or no detriment to\nreconstructions. We further demonstrate how other choices of prior can assist\nin producing different decompositions and introduce an alternative training\nobjective that allows the control of both decomposition factors in a principled\nmanner. \n\n"}
{"id": "1812.02890", "contents": "Title: Three Tools for Practical Differential Privacy Abstract: Differentially private learning on real-world data poses challenges for\nstandard machine learning practice: privacy guarantees are difficult to\ninterpret, hyperparameter tuning on private data reduces the privacy budget,\nand ad-hoc privacy attacks are often required to test model privacy. We\nintroduce three tools to make differentially private machine learning more\npractical: (1) simple sanity checks which can be carried out in a centralized\nmanner before training, (2) an adaptive clipping bound to reduce the effective\nnumber of tuneable privacy parameters, and (3) we show that large-batch\ntraining improves model performance. \n\n"}
{"id": "1812.02919", "contents": "Title: When Bifidelity Meets CoKriging: An Efficient Physics-Informed\n  Multifidelity Method Abstract: In this work, we propose a framework that combines the\napproximation-theory-based multifidelity method and\nGaussian-process-regression-based multifidelity method to achieve data-model\nconvergence when stochastic simulation models and sparse accurate observation\ndata are available. Specifically, the two types of multifidelity methods we use\nare the bifidelity and CoKriging methods. The new approach uses the bifidelity\nmethod to efficiently estimate the empirical mean and covariance of the\nstochastic simulation outputs, then it uses these statistics to construct a\nGaussian process (GP) representing low-fidelity in CoKriging. We also combine\nthe bifidelity method with Kriging, where the approximated empirical statistics\nare used to construct the GP as well. We prove that the resulting posterior\nmean by the new physics-informed approach preserves linear physical constraints\nup to an error bound. By using this method, we can obtain an accurate\nconstruction of a state of interest based on a partially correct physical model\nand a few accurate observations. We present numerical examples to demonstrate\nperformance of the method. \n\n"}
{"id": "1812.03781", "contents": "Title: Inflo: News Categorization and Keyphrase Extraction for Implementation\n  in an Aggregation System Abstract: The work herein describes a system for automatic news category and keyphrase\nlabeling, presented in the context of our motivation to improve the speed at\nwhich a user can find relevant and interesting content within an aggregation\nplatform. A set of 12 discrete categories were applied to over 500,000 news\narticles for training a neural network, to be used to facilitate the more\nin-depth task of extracting the most significant keyphrases. The latter was\ndone using three methods: statistical, graphical and numerical, using the\npre-identified category label to improve relevance of extracted phrases. The\nresults are presented in a demo in which the articles are pre-populated via\nNews API, and upon being selected, the category and keyphrase labels will be\ncomputed via the methods explained herein. \n\n"}
{"id": "1812.03835", "contents": "Title: Graph Embedding for Citation Recommendation Abstract: As science advances, the academic community has published millions of\nresearch papers. Researchers devote time and effort to search relevant\nmanuscripts when writing a paper or simply to keep up with current research. In\nthis paper, we consider the problem of citation recommendation on graph and\npropose a task-specific neighborhood construction strategy to learn the\ndistributed representations of papers. In addition, given the learned\nrepresentations, we investigate various schemes to rank the candidate papers\nfor citation recommendation. The experimental results show our proposed\nneighborhood construction strategy outperforms the widely-used random walks\nbased sampling strategy on all ranking schemes, and the model based ranking\nscheme outperforms embedding based rankings for both neighborhood construction\nstrategies. We also demonstrated that graph embedding is a robust approach for\ncitation recommendation when hidden ratio changes, while the performance of\nclassic methods drop significantly when the set of seed papers is becoming\nsmall. \n\n"}
{"id": "1812.03962", "contents": "Title: Disentangled Dynamic Representations from Unordered Data Abstract: We present a deep generative model that learns disentangled static and\ndynamic representations of data from unordered input. Our approach exploits\nregularities in sequential data that exist regardless of the order in which the\ndata is viewed. The result of our factorized graphical model is a\nwell-organized and coherent latent space for data dynamics. We demonstrate our\nmethod on several synthetic dynamic datasets and real video data featuring\nvarious facial expressions and head poses. \n\n"}
{"id": "1812.04370", "contents": "Title: Sparse component separation from Poisson measurements Abstract: Blind source separation (BSS) aims at recovering signals from mixtures. This\nproblem has been extensively studied in cases where the mixtures are\ncontaminated with additive Gaussian noise. However, it is not well suited to\ndescribe data that are corrupted with Poisson measurements such as in low\nphoton count optics or in high-energy astronomical imaging (e.g. observations\nfrom the Chandra or Fermi telescopes). To that purpose, we propose a novel BSS\nalgorithm coined pGMCA that specifically tackles the blind separation of sparse\nsources from Poisson measurements. \n\n"}
{"id": "1812.05945", "contents": "Title: EEG-based Communication with a Predictive Text Algorithm Abstract: Several changes occur in the brain in response to voluntary and involuntary\nactivities performed by a person. The ability to retrieve data from the brain\nwithin a time space provides a basis for in-depth analyses that offer insight\non what changes occur in the brain during its decision-making processes. In\nthis work, we present the technical description and software implementation of\nan electroencephalographic (EEG) based communication system. We read EEG data\nin real-time with which we compute the likelihood that a voluntary eye blink\nhas been made by a person and use the decision to trigger buttons on a user\ninterface in order to produce text. Relevant texts are suggested using a\nmodification of the T9 algorithm. Our results indicate that EEG-based\ntechnology can be effectively applied in facilitating speech for people with\nsevere speech and muscular disabilities, providing a foundation for future work\nin the area. \n\n"}
{"id": "1812.06866", "contents": "Title: Bayesian Mean-parameterized Nonnegative Binary Matrix Factorization Abstract: Binary data matrices can represent many types of data such as social\nnetworks, votes, or gene expression. In some cases, the analysis of binary\nmatrices can be tackled with nonnegative matrix factorization (NMF), where the\nobserved data matrix is approximated by the product of two smaller nonnegative\nmatrices. In this context, probabilistic NMF assumes a generative model where\nthe data is usually Bernoulli-distributed. Often, a link function is used to\nmap the factorization to the $[0,1]$ range, ensuring a valid Bernoulli mean\nparameter. However, link functions have the potential disadvantage to lead to\nuninterpretable models. Mean-parameterized NMF, on the contrary, overcomes this\nproblem. We propose a unified framework for Bayesian mean-parameterized\nnonnegative binary matrix factorization models (NBMF). We analyze three models\nwhich correspond to three possible constraints that respect the\nmean-parametrization without the need for link functions. Furthermore, we\nderive a novel collapsed Gibbs sampler and a collapsed variational algorithm to\ninfer the posterior distribution of the factors. Next, we extend the proposed\nmodels to a nonparametric setting where the number of used latent dimensions is\nautomatically driven by the observed data. We analyze the performance of our\nNBMF methods in multiple datasets for different tasks such as dictionary\nlearning and prediction of missing data. Experiments show that our methods\nprovide similar or superior results than the state of the art, while\nautomatically detecting the number of relevant components. \n\n"}
{"id": "1812.07813", "contents": "Title: Matrix Completion under Low-Rank Missing Mechanism Abstract: Matrix completion is a modern missing data problem where both the missing\nstructure and the underlying parameter are high dimensional. Although missing\nstructure is a key component to any missing data problems, existing matrix\ncompletion methods often assume a simple uniform missing mechanism. In this\nwork, we study matrix completion from corrupted data under a novel low-rank\nmissing mechanism. The probability matrix of observation is estimated via a\nhigh dimensional low-rank matrix estimation procedure, and further used to\ncomplete the target matrix via inverse probabilities weighting. Due to both\nhigh dimensional and extreme (i.e., very small) nature of the true probability\nmatrix, the effect of inverse probability weighting requires careful study. We\nderive optimal asymptotic convergence rates of the proposed estimators for both\nthe observation probabilities and the target matrix. \n\n"}
{"id": "1812.08254", "contents": "Title: Factorization Machines for Data with Implicit Feedback Abstract: In this work, we propose FM-Pair, an adaptation of Factorization Machines\nwith a pairwise loss function, making them effective for datasets with implicit\nfeedback. The optimization model in FM-Pair is based on the BPR (Bayesian\nPersonalized Ranking) criterion, which is a well-established pairwise\noptimization model. FM-Pair retains the advantages of FMs on generality,\nexpressiveness and performance and yet it can be used for datasets with\nimplicit feedback. We also propose how to apply FM-Pair effectively on two\ncollaborative filtering problems, namely, context-aware recommendation and\ncross-domain collaborative filtering. By performing experiments on different\ndatasets with explicit or implicit feedback we empirically show that in most of\nthe tested datasets, FM-Pair beats state-of-the-art learning-to-rank methods\nsuch as BPR-MF (BPR with Matrix Factorization model). We also show that FM-Pair\nis significantly more effective for ranking, compared to the standard FMs\nmodel. Moreover, we show that FM-Pair can utilize context or cross-domain\ninformation effectively as the accuracy of recommendations would always improve\nwith the right auxiliary features. Finally we show that FM-Pair has a linear\ntime complexity and scales linearly by exploiting additional features. \n\n"}
{"id": "1812.08808", "contents": "Title: Reducing Sampling Ratios Improves Bagging in Sparse Regression Abstract: Bagging, a powerful ensemble method from machine learning, improves the\nperformance of unstable predictors. Although the power of Bagging has been\nshown mostly in classification problems, we demonstrate the success of\nemploying Bagging in sparse regression over the baseline method (L1\nminimization). The framework employs the generalized version of the original\nBagging with various bootstrap ratios. The performance limits associated with\ndifferent choices of bootstrap sampling ratio L/m and number of estimates K is\nanalyzed theoretically. Simulation shows that the proposed method yields\nstate-of-the-art recovery performance, outperforming L1 minimization and\nBolasso in the challenging case of low levels of measurements. A lower L/m\nratio (60% - 90%) leads to better performance, especially with a small number\nof measurements. With the reduced sampling rate, SNR improves over the original\nBagging by up to 24%. With a properly chosen sampling ratio, a reasonably small\nnumber of estimates K = 30 gives satisfying result, even though increasing K is\ndiscovered to always improve or at least maintain the performance. \n\n"}
{"id": "1812.08870", "contents": "Title: Iterative Relevance Feedback for Answer Passage Retrieval with\n  Passage-level Semantic Match Abstract: Relevance feedback techniques assume that users provide relevance judgments\nfor the top k (usually 10) documents and then re-rank using a new query model\nbased on those judgments. Even though this is effective, there has been little\nresearch recently on this topic because requiring users to provide substantial\nfeedback on a result list is impractical in a typical web search scenario. In\nnew environments such as voice-based search with smart home devices, however,\nfeedback about result quality can potentially be obtained during users'\ninteractions with the system. Since there are severe limitations on the length\nand number of results that can be presented in a single interaction in this\nenvironment, the focus should move from browsing result lists to iterative\nretrieval and from retrieving documents to retrieving answers. In this paper,\nwe study iterative relevance feedback techniques with a focus on retrieving\nanswer passages. We first show that iterative feedback is more effective than\nthe top-k approach for answer retrieval. Then we propose an iterative feedback\nmodel based on passage-level semantic match and show that it can produce\nsignificant improvements compared to both word-based iterative feedback models\nand those based on term-level semantic similarity. \n\n"}
{"id": "1812.09424", "contents": "Title: Distributed sequential method for analyzing massive data Abstract: To analyse a very large data set containing lengthy variables, we adopt a\nsequential estimation idea and propose a parallel divide-and-conquer method. We\nconduct several conventional sequential estimation procedures separately, and\nproperly integrate their results while maintaining the desired statistical\nproperties. Additionally, using a criterion from the statistical experiment\ndesign, we adopt an adaptive sample selection, together with an adaptive\nshrinkage estimation method, to simultaneously accelerate the estimation\nprocedure and identify the effective variables. We confirm the cogency of our\nmethods through theoretical justifications and numerical results derived from\nsynthesized data sets. We then apply the proposed method to three real data\nsets, including those pertaining to appliance energy use and particulate matter\nconcentration. \n\n"}
{"id": "1812.09444", "contents": "Title: Deep autoregressive neural networks for high-dimensional inverse\n  problems in groundwater contaminant source identification Abstract: Identification of a groundwater contaminant source simultaneously with the\nhydraulic conductivity in highly-heterogeneous media often results in a\nhigh-dimensional inverse problem. In this study, a deep autoregressive neural\nnetwork-based surrogate method is developed for the forward model to allow us\nto solve efficiently such high-dimensional inverse problems. The surrogate is\ntrained using limited evaluations of the forward model. Since the relationship\nbetween the time-varying inputs and outputs of the forward transport model is\ncomplex, we propose an autoregressive strategy, which treats the output at the\nprevious time step as input to the network for predicting the output at the\ncurrent time step. We employ a dense convolutional encoder-decoder network\narchitecture in which the high-dimensional input and output fields of the model\nare treated as images to leverage the robust capability of convolutional\nnetworks in image-like data processing. An iterative local updating ensemble\nsmoother (ILUES) algorithm is used as the inversion framework. The proposed\nmethod is evaluated using a synthetic contaminant source identification problem\nwith 686 uncertain input parameters. Results indicate that, with relatively\nlimited training data, the deep autoregressive neural network consisting of 27\nconvolutional layers is capable of providing an accurate approximation for the\nhigh-dimensional model input-output relationship. The autoregressive strategy\nsubstantially improves the network's accuracy and computational efficiency. The\napplication of the surrogate-based ILUES in solving the inverse problem shows\nthat it can achieve accurate inversion results and predictive uncertainty\nestimates. \n\n"}
{"id": "1812.09541", "contents": "Title: TEST: A Terminology Extraction System for Technology Related Terms Abstract: Tracking developments in the highly dynamic data-technology landscape are\nvital to keeping up with novel technologies and tools, in the various areas of\nArtificial Intelligence (AI). However, It is difficult to keep track of all the\nrelevant technology keywords. In this paper, we propose a novel system that\naddresses this problem. This tool is used to automatically detect the existence\nof new technologies and tools in text, and extract terms used to describe these\nnew technologies. The extracted new terms can be logged as new AI technologies\nas they are found on-the-fly in the web. It can be subsequently classified into\nthe relevant semantic labels and AI domains. Our proposed tool is based on a\ntwo-stage cascading model -- the first stage classifies if the sentence\ncontains a technology term or not; and the second stage identifies the\ntechnology keyword in the sentence. We obtain a competitive accuracy for both\ntasks of sentence classification and text identification. \n\n"}
{"id": "1812.10847", "contents": "Title: The Clickbait Challenge 2017: Towards a Regression Model for Clickbait\n  Strength Abstract: Clickbait has grown to become a nuisance to social media users and social\nmedia operators alike. Malicious content publishers misuse social media to\nmanipulate as many users as possible to visit their websites using clickbait\nmessages. Machine learning technology may help to handle this problem, giving\nrise to automatic clickbait detection. To accelerate progress in this\ndirection, we organized the Clickbait Challenge 2017, a shared task inviting\nthe submission of clickbait detectors for a comparative evaluation. A total of\n13 detectors have been submitted, achieving significant improvements over the\nprevious state of the art in terms of detection performance. Also, many of the\nsubmitted approaches have been published open source, rendering them\nreproducible, and a good starting point for newcomers. While the 2017 challenge\nhas passed, we maintain the evaluation system and answer to new registrations\nin support of the ongoing research on better clickbait detectors. \n\n"}
{"id": "1812.11444", "contents": "Title: Multivariate Arrival Times with Recurrent Neural Networks for\n  Personalized Demand Forecasting Abstract: Access to a large variety of data across a massive population has made it\npossible to predict customer purchase patterns and responses to marketing\ncampaigns. In particular, accurate demand forecasts for popular products with\nfrequent repeat purchases are essential since these products are one of the\nmain drivers of profits. However, buyer purchase patterns are extremely diverse\nand sparse on a per-product level due to population heterogeneity as well as\ndependence in purchase patterns across product categories. Traditional methods\nin survival analysis have proven effective in dealing with censored data by\nassuming parametric distributions on inter-arrival times. Distributional\nparameters are then fitted, typically in a regression framework. On the other\nhand, neural-network based models take a non-parametric approach to learn\nrelations from a larger functional class. However, the lack of distributional\nassumptions make it difficult to model partially observed data. In this paper,\nwe model directly the inter-arrival times as well as the partially observed\ninformation at each time step in a survival-based approach using Recurrent\nNeural Networks (RNN) to model purchase times jointly over several products.\nInstead of predicting a point estimate for inter-arrival times, the RNN outputs\nparameters that define a distributional estimate. The loss function is the\nnegative log-likelihood of these parameters given partially observed data. This\napproach allows one to leverage both fully observed data as well as partial\ninformation. By externalizing the censoring problem through a log-likelihood\nloss function, we show that substantial improvements over state-of-the-art\nmachine learning methods can be achieved. We present experimental results based\non two open datasets as well as a study on a real dataset from a large\nretailer. \n\n"}
{"id": "1812.11740", "contents": "Title: A Neural Network Based Explainable Recommender System Abstract: Recommendation system could help the companies to persuade users to visit or\nconsume at a particular place, which was based on many traditional methods such\nas the set of collaborative filtering algorithms. Most research discusses the\nmodel design or feature engineering methods to minimize the root mean square\nerror (RMSE) of rating prediction, but lacks exploring the ways to generate the\nreasons for recommendations. This paper proposed an integrated neural network\nbased model which integrates rating scores prediction and explainable words\ngeneration. Based on the experimental results, this model presented lower RMSE\ncompared with traditional methods, and generate the explanation of\nrecommendation to convince customers to visit the recommended place. \n\n"}
{"id": "1812.11755", "contents": "Title: Approximate Inference for Multiplicative Latent Force Models Abstract: Latent force models are a class of hybrid models for dynamic systems,\ncombining simple mechanistic models with flexible Gaussian process (GP)\nperturbations. An extension of this framework to include multiplicative\ninteractions between the state and GP terms allows strong a priori control of\nthe model geometry at the expense of tractable inference. In this paper we\nconsider two methods of carrying out inference within this broader class of\nmodels. The first is based on an adaptive gradient matching approximation, and\nthe second is constructed around mixtures of local approximations to the\nsolution. We compare the performance of both methods on simulated data, and\nalso demonstrate an application of the multiplicative latent force model on\nmotion capture data. \n\n"}
{"id": "1901.00630", "contents": "Title: Projecting \"better than randomly\": How to reduce the dimensionality of\n  very large datasets in a way that outperforms random projections Abstract: For very large datasets, random projections (RP) have become the tool of\nchoice for dimensionality reduction. This is due to the computational\ncomplexity of principal component analysis. However, the recent development of\nrandomized principal component analysis (RPCA) has opened up the possibility of\nobtaining approximate principal components on very large datasets. In this\npaper, we compare the performance of RPCA and RP in dimensionality reduction\nfor supervised learning. In Experiment 1, study a malware classification task\non a dataset with over 10 million samples, almost 100,000 features, and over 25\nbillion non-zero values, with the goal of reducing the dimensionality to a\ncompressed representation of 5,000 features. In order to apply RPCA to this\ndataset, we develop a new algorithm called large sample RPCA (LS-RPCA), which\nextends the RPCA algorithm to work on datasets with arbitrarily many samples.\nWe find that classification performance is much higher when using LS-RPCA for\ndimensionality reduction than when using random projections. In particular,\nacross a range of target dimensionalities, we find that using LS-RPCA reduces\nclassification error by between 37% and 54%. Experiment 2 generalizes the\nphenomenon to multiple datasets, feature representations, and classifiers.\nThese findings have implications for a large number of research projects in\nwhich random projections were used as a preprocessing step for dimensionality\nreduction. As long as accuracy is at a premium and the target dimensionality is\nsufficiently less than the numeric rank of the dataset, randomized PCA may be a\nsuperior choice. Moreover, if the dataset has a large number of samples, then\nLS-RPCA will provide a method for obtaining the approximate principal\ncomponents. \n\n"}
{"id": "1901.01477", "contents": "Title: Dynamic Visualization and Fast Computation for Convex Clustering via\n  Algorithmic Regularization Abstract: Convex clustering is a promising new approach to the classical problem of\nclustering, combining strong performance in empirical studies with rigorous\ntheoretical foundations. Despite these advantages, convex clustering has not\nbeen widely adopted, due to its computationally intensive nature and its lack\nof compelling visualizations. To address these impediments, we introduce\nAlgorithmic Regularization, an innovative technique for obtaining high-quality\nestimates of regularization paths using an iterative one-step approximation\nscheme. We justify our approach with a novel theoretical result, guaranteeing\nglobal convergence of the approximate path to the exact solution under\neasily-checked non-data-dependent assumptions. The application of algorithmic\nregularization to convex clustering yields the Convex Clustering via\nAlgorithmic Regularization Paths (CARP) algorithm for computing the clustering\nsolution path. On example data sets from genomics and text analysis, CARP\ndelivers over a 100-fold speed-up over existing methods, while attaining a\nfiner approximation grid than standard methods. Furthermore, CARP enables\nimproved visualization of clustering solutions: the fine solution grid returned\nby CARP can be used to construct a convex clustering-based dendrogram, as well\nas forming the basis of a dynamic path-wise visualization based on modern web\ntechnologies. Our methods are implemented in the open-source R package\nclustRviz, available at https://github.com/DataSlingers/clustRviz. \n\n"}
{"id": "1901.01696", "contents": "Title: Semi-supervised learning in unbalanced and heterogeneous networks Abstract: Community detection was a hot topic on network analysis, where the main aim\nis to perform unsupervised learning or clustering in networks. Recently,\nsemi-supervised learning has received increasing attention among researchers.\nIn this paper, we propose a new algorithm, called weighted inverse Laplacian\n(WIL), for predicting labels in partially labeled networks. The idea comes from\nthe first hitting time in random walk, and it also has nice explanations both\nin information propagation and the regularization framework. We propose a\npartially labeled degree-corrected block model (pDCBM) to describe the\ngeneration of partially labeled networks. We show that WIL ensures the\nmisclassification rate is of order $O(\\frac{1}{d})$ for the pDCBM with average\ndegree $d=\\Omega(\\log n),$ and that it can handle situations with greater\nunbalanced than traditional Laplacian methods. WIL outperforms other\nstate-of-the-art methods in most of our simulations and real datasets,\nespecially in unbalanced networks and heterogeneous networks. \n\n"}
{"id": "1901.02928", "contents": "Title: Beyond the EM Algorithm: Constrained Optimization Methods for Latent\n  Class Model Abstract: Latent class model (LCM), which is a finite mixture of different categorical\ndistributions, is one of the most widely used models in statistics and machine\nlearning fields. Because of its non-continuous nature and the flexibility in\nshape, researchers in practice areas such as marketing and social sciences also\nfrequently use LCM to gain insights from their data. One likelihood-based\nmethod, the Expectation-Maximization (EM) algorithm, is often used to obtain\nthe model estimators. However, the EM algorithm is well-known for its\nnotoriously slow convergence. In this research, we explore alternative\nlikelihood-based methods that can potential remedy the slow convergence of the\nEM algorithm. More specifically, we regard likelihood-based approach as a\nconstrained nonlinear optimization problem, and apply quasi-Newton type methods\nto solve them. We examine two different constrained optimization methods to\nmaximize the log likelihood function. We present simulation study results to\nshow that the proposed methods not only converge in less iterations than the EM\nalgorithm but also produce more accurate model estimators. \n\n"}
{"id": "1901.03298", "contents": "Title: Automatic detection of passable roads after floods in remote sensed and\n  social media data Abstract: This paper addresses the problem of floods classification and floods\naftermath detection utilizing both social media and satellite imagery.\nAutomatic detection of disasters such as floods is still a very challenging\ntask. The focus lies on identifying passable routes or roads during floods. Two\nnovel solutions are presented, which were developed for two corresponding tasks\nat the MediaEval 2018 benchmarking challenge. The tasks are (i) identification\nof images providing evidence for road passability and (ii) differentiation and\ndetection of passable and non-passable roads in images from two complementary\nsources of information. For the first challenge, we mainly rely on object and\nscene-level features extracted through multiple deep models pre-trained on the\nImageNet and Places datasets. The object and scene-level features are then\ncombined using early, late and double fusion techniques. To identify whether or\nnot it is possible for a vehicle to pass a road in satellite images, we rely on\nConvolutional Neural Networks and a transfer learning-based classification\napproach. The evaluation of the proposed methods are carried out on the\nlarge-scale datasets provided for the benchmark competition. The results\ndemonstrate significant improvement in the performance over the recent\nstate-of-art approaches. \n\n"}
{"id": "1901.03357", "contents": "Title: No-Regret Bayesian Optimization with Unknown Hyperparameters Abstract: Bayesian optimization (BO) based on Gaussian process models is a powerful\nparadigm to optimize black-box functions that are expensive to evaluate. While\nseveral BO algorithms provably converge to the global optimum of the unknown\nfunction, they assume that the hyperparameters of the kernel are known in\nadvance. This is not the case in practice and misspecification often causes\nthese algorithms to converge to poor local optima. In this paper, we present\nthe first BO algorithm that is provably no-regret and converges to the optimum\nwithout knowledge of the hyperparameters. During optimization we slowly adapt\nthe hyperparameters of stationary kernels and thereby expand the associated\nfunction class over time, so that the BO algorithm considers more complex\nfunction candidates. Based on the theoretical insights, we propose several\npractical algorithms that achieve the empirical sample efficiency of BO with\nonline hyperparameter estimation, but retain theoretical convergence\nguarantees. We evaluate our method on several benchmark problems. \n\n"}
{"id": "1901.04277", "contents": "Title: Natural Disasters Detection in Social Media and Satellite imagery: a\n  survey Abstract: The analysis of natural disaster-related multimedia content got great\nattention in recent years. Being one of the most important sources of\ninformation, social media have been crawled over the years to collect and\nanalyze disaster-related multimedia content. Satellite imagery has also been\nwidely explored for disasters analysis. In this paper, we survey the existing\nliterature on disaster detection and analysis of the retrieved information from\nsocial media and satellites. Literature on disaster detection and analysis of\nrelated multimedia content on the basis of the nature of the content can be\ncategorized into three groups, namely (i) disaster detection in text; (ii)\nanalysis of disaster-related visual content from social media; and (iii)\ndisaster detection in satellite imagery. We extensively review different\napproaches proposed in these three domains. Furthermore, we also review\nbenchmarking datasets available for the evaluation of disaster detection\nframeworks. Moreover, we provide a detailed discussion on the insights obtained\nfrom the literature review, and identify future trends and challenges, which\nwill provide an important starting point for the researchers in the field. \n\n"}
{"id": "1901.04321", "contents": "Title: Large-scale Collaborative Filtering with Product Embeddings Abstract: The application of machine learning techniques to large-scale personalized\nrecommendation problems is a challenging task. Such systems must make sense of\nenormous amounts of implicit feedback in order to understand user preferences\nacross numerous product categories. This paper presents a deep learning based\nsolution to this problem within the collaborative filtering with implicit\nfeedback framework. Our approach combines neural attention mechanisms, which\nallow for context dependent weighting of past behavioral signals, with\nrepresentation learning techniques to produce models which obtain extremely\nhigh coverage, can easily incorporate new information as it becomes available,\nand are computationally efficient. Offline experiments demonstrate significant\nperformance improvements when compared to several alternative methods from the\nliterature. Results from an online setting show that the approach compares\nfavorably with current production techniques used to produce personalized\nproduct recommendations. \n\n"}
{"id": "1901.04704", "contents": "Title: DeepCF: A Unified Framework of Representation Learning and Matching\n  Function Learning in Recommender System Abstract: In general, recommendation can be viewed as a matching problem, i.e., match\nproper items for proper users. However, due to the huge semantic gap between\nusers and items, it's almost impossible to directly match users and items in\ntheir initial representation spaces. To solve this problem, many methods have\nbeen studied, which can be generally categorized into two types, i.e.,\nrepresentation learning-based CF methods and matching function learning-based\nCF methods. Representation learning-based CF methods try to map users and items\ninto a common representation space. In this case, the higher similarity between\na user and an item in that space implies they match better. Matching function\nlearning-based CF methods try to directly learn the complex matching function\nthat maps user-item pairs to matching scores. Although both methods are well\ndeveloped, they suffer from two fundamental flaws, i.e., the limited\nexpressiveness of dot product and the weakness in capturing low-rank relations\nrespectively. To this end, we propose a general framework named DeepCF, short\nfor Deep Collaborative Filtering, to combine the strengths of the two types of\nmethods and overcome such flaws. Extensive experiments on four publicly\navailable datasets demonstrate the effectiveness of the proposed DeepCF\nframework. \n\n"}
{"id": "1901.04791", "contents": "Title: Mixed Variational Inference Abstract: The Laplace approximation has been one of the workhorses of Bayesian\ninference. It often delivers good approximations in practice despite the fact\nthat it does not strictly take into account where the volume of posterior\ndensity lies. Variational approaches avoid this issue by explicitly minimising\nthe Kullback-Leibler divergence DKL between a postulated posterior and the true\n(unnormalised) logarithmic posterior. However, they rely on a closed form DKL\nin order to update the variational parameters. To address this, stochastic\nversions of variational inference have been devised that approximate the\nintractable DKL with a Monte Carlo average. This approximation allows\ncalculating gradients with respect to the variational parameters. However,\nvariational methods often postulate a factorised Gaussian approximating\nposterior. In doing so, they sacrifice a-posteriori correlations. In this work,\nwe propose a method that combines the Laplace approximation with the\nvariational approach. The advantages are that we maintain: applicability on\nnon-conjugate models, posterior correlations and a reduced number of free\nvariational parameters. Numerical experiments demonstrate improvement over the\nLaplace approximation and variational inference with factorised Gaussian\nposteriors. \n\n"}
{"id": "1901.04816", "contents": "Title: Learning Direct and Inverse Transmission Matrices Abstract: Linear problems appear in a variety of disciplines and their application for\nthe transmission matrix recovery is one of the most stimulating challenges in\nbiomedical imaging. Its knowledge turns any random media into an optical tool\nthat can focus or transmit an image through disorder. Here, converting an\ninput-output problem into a statistical mechanical formulation, we investigate\nhow inference protocols can learn the transmission couplings by\npseudolikelihood maximization. Bridging linear regression and thermodynamics\nlet us propose an innovative framework to pursue the solution of the\nscattering-riddle. \n\n"}
{"id": "1901.07229", "contents": "Title: Fast and Robust Shortest Paths on Manifolds Learned from Data Abstract: We propose a fast, simple and robust algorithm for computing shortest paths\nand distances on Riemannian manifolds learned from data. This amounts to\nsolving a system of ordinary differential equations (ODEs) subject to boundary\nconditions. Here standard solvers perform poorly because they require\nwell-behaved Jacobians of the ODE, and usually, manifolds learned from data\nimply unstable and ill-conditioned Jacobians. Instead, we propose a fixed-point\niteration scheme for solving the ODE that avoids Jacobians. This enhances the\nstability of the solver, while reduces the computational cost. In experiments\ninvolving both Riemannian metric learning and deep generative models we\ndemonstrate significant improvements in speed and stability over both\ngeneral-purpose state-of-the-art solvers as well as over specialized solvers. \n\n"}
{"id": "1901.07555", "contents": "Title: Managing Popularity Bias in Recommender Systems with Personalized\n  Re-ranking Abstract: Many recommender systems suffer from popularity bias: popular items are\nrecommended frequently while less popular, niche products, are recommended\nrarely or not at all. However, recommending the ignored products in the `long\ntail' is critical for businesses as they are less likely to be discovered. In\nthis paper, we introduce a personalized diversification re-ranking approach to\nincrease the representation of less popular items in recommendations while\nmaintaining acceptable recommendation accuracy. Our approach is a\npost-processing step that can be applied to the output of any recommender\nsystem. We show that our approach is capable of managing popularity bias more\neffectively, compared with an existing method based on regularization. We also\nexamine both new and existing metrics to measure the coverage of long-tail\nitems in the recommendation. \n\n"}
{"id": "1901.07710", "contents": "Title: Unified estimation framework for unnormalized models with statistical\n  efficiency Abstract: The parameter estimation of unnormalized models is a challenging problem. The\nmaximum likelihood estimation (MLE) is computationally infeasible for these\nmodels since normalizing constants are not explicitly calculated. Although some\nconsistent estimators have been proposed earlier, the problem of statistical\nefficiency remains. In this study, we propose a unified, statistically\nefficient estimation framework for unnormalized models and several efficient\nestimators, whose asymptotic variance is the same as the MLE. The computational\ncost of these estimators is also reasonable and they can be employed whether\nthe sample space is discrete or continuous. The loss functions of the proposed\nestimators are derived by combining the following two methods: (1)\ndensity-ratio matching using Bregman divergence, and (2) plugging-in\nnonparametric estimators. We also analyze the properties of the proposed\nestimators when the unnormalized models are misspecified. The experimental\nresults demonstrate the advantages of our method over existing approaches. \n\n"}
{"id": "1901.08275", "contents": "Title: Multi-fidelity Bayesian Optimization with Max-value Entropy Search and\n  its parallelization Abstract: In a standard setting of Bayesian optimization (BO), the objective function\nevaluation is assumed to be highly expensive. Multi-fidelity Bayesian\noptimization (MFBO) accelerates BO by incorporating lower fidelity observations\navailable with a lower sampling cost. In this paper, we focus on the\ninformation-based approach, which is a popular and empirically successful\napproach in BO. For MFBO, however, existing information-based methods are\nplagued by difficulty in estimating the information gain. We propose an\napproach based on max-value entropy search (MES), which greatly facilitates\ncomputations by considering the entropy of the optimal function value instead\nof the optimal input point. We show that, in our multi-fidelity MES (MF-MES),\nmost of additional computations, compared with usual MES, is reduced to\nanalytical computations. Although an additional numerical integration is\nnecessary for the information across different fidelities, this is only in one\ndimensional space, which can be performed efficiently and accurately. Further,\nwe also propose parallelization of MF-MES. Since there exist a variety of\ndifferent sampling costs, queries typically occur asynchronously in MFBO. We\nshow that similar simple computations can be derived for asynchronous parallel\nMFBO. We demonstrate effectiveness of our approach by using benchmark datasets\nand a real-world application to materials science data. \n\n"}
{"id": "1901.08770", "contents": "Title: Robust estimation of tree structured Gaussian Graphical Model Abstract: Consider jointly Gaussian random variables whose conditional independence\nstructure is specified by a graphical model. If we observe realizations of the\nvariables, we can compute the covariance matrix, and it is well known that the\nsupport of the inverse covariance matrix corresponds to the edges of the\ngraphical model. Instead, suppose we only have noisy observations. If the noise\nat each node is independent, we can compute the sum of the covariance matrix\nand an unknown diagonal. The inverse of this sum is (in general) dense. We ask:\ncan the original independence structure be recovered? We address this question\nfor tree structured graphical models. We prove that this problem is\nunidentifiable, but show that this unidentifiability is limited to a small\nclass of candidate trees. We further present additional constraints under which\nthe problem is identifiable. Finally, we provide an O(n^3) algorithm to find\nthis equivalence class of trees. \n\n"}
{"id": "1901.09415", "contents": "Title: Disentangling and Learning Robust Representations with Natural\n  Clustering Abstract: Learning representations that disentangle the underlying factors of\nvariability in data is an intuitive way to achieve generalization in deep\nmodels. In this work, we address the scenario where generative factors present\na multimodal distribution due to the existence of class distinction in the\ndata. We propose N-VAE, a model which is capable of separating factors of\nvariation which are exclusive to certain classes from factors that are shared\namong classes. This model implements an explicitly compositional latent\nvariable structure by defining a class-conditioned latent space and a shared\nlatent space. We show its usefulness for detecting and disentangling\nclass-dependent generative factors as well as its capacity to generate\nartificial samples which contain characteristics unseen in the training data. \n\n"}
{"id": "1901.09451", "contents": "Title: Bias in Bios: A Case Study of Semantic Representation Bias in a\n  High-Stakes Setting Abstract: We present a large-scale study of gender bias in occupation classification, a\ntask where the use of machine learning may lead to negative outcomes on\npeoples' lives. We analyze the potential allocation harms that can result from\nsemantic representation bias. To do so, we study the impact on occupation\nclassification of including explicit gender indicators---such as first names\nand pronouns---in different semantic representations of online biographies.\nAdditionally, we quantify the bias that remains when these indicators are\n\"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit\ngender indicators. As we demonstrate, differences in true positive rates\nbetween genders are correlated with existing gender imbalances in occupations,\nwhich may compound these imbalances. \n\n"}
{"id": "1901.09888", "contents": "Title: Federated Collaborative Filtering for Privacy-Preserving Personalized\n  Recommendation System Abstract: The increasing interest in user privacy is leading to new privacy preserving\nmachine learning paradigms. In the Federated Learning paradigm, a master\nmachine learning model is distributed to user clients, the clients use their\nlocally stored data and model for both inference and calculating model updates.\nThe model updates are sent back and aggregated on the server to update the\nmaster model then redistributed to the clients. In this paradigm, the user data\nnever leaves the client, greatly enhancing the user' privacy, in contrast to\nthe traditional paradigm of collecting, storing and processing user data on a\nbackend server beyond the user's control. In this paper we introduce, as far as\nwe are aware, the first federated implementation of a Collaborative Filter. The\nfederated updates to the model are based on a stochastic gradient approach. As\na classical case study in machine learning, we explore a personalized\nrecommendation system based on users' implicit feedback and demonstrate the\nmethod's applicability to both the MovieLens and an in-house dataset. Empirical\nvalidation confirms a collaborative filter can be federated without a loss of\naccuracy compared to a standard implementation, hence enhancing the user's\nprivacy in a widely used recommender application while maintaining recommender\nperformance. \n\n"}
{"id": "1901.09981", "contents": "Title: Improving Adversarial Robustness of Ensembles with Diversity Training Abstract: Deep Neural Networks are vulnerable to adversarial attacks even in settings\nwhere the attacker has no direct access to the model being attacked. Such\nattacks usually rely on the principle of transferability, whereby an attack\ncrafted on a surrogate model tends to transfer to the target model. We show\nthat an ensemble of models with misaligned loss gradients can provide an\neffective defense against transfer-based attacks. Our key insight is that an\nadversarial example is less likely to fool multiple models in the ensemble if\ntheir loss functions do not increase in a correlated fashion. To this end, we\npropose Diversity Training, a novel method to train an ensemble of models with\nuncorrelated loss functions. We show that our method significantly improves the\nadversarial robustness of ensembles and can also be combined with existing\nmethods to create a stronger defense. \n\n"}
{"id": "1901.10568", "contents": "Title: Stochastic Gradient MCMC for Nonlinear State Space Models Abstract: State space models (SSMs) provide a flexible framework for modeling complex\ntime series via a latent stochastic process. Inference for nonlinear,\nnon-Gaussian SSMs is often tackled with particle methods that do not scale well\nto long time series. The challenge is two-fold: not only do computations scale\nlinearly with time, as in the linear case, but particle filters additionally\nsuffer from increasing particle degeneracy with longer series. Stochastic\ngradient MCMC methods have been developed to scale Bayesian inference for\nfinite-state hidden Markov models and linear SSMs using buffered stochastic\ngradient estimates to account for temporal dependencies. We extend these\nstochastic gradient estimators to nonlinear SSMs using particle methods. We\npresent error bounds that account for both buffering error and particle error\nin the case of nonlinear SSMs that are log-concave in the latent process. We\nevaluate our proposed particle buffered stochastic gradient using stochastic\ngradient MCMC for inference on both long sequential synthetic and\nminute-resolution financial returns data, demonstrating the importance of this\nclass of methods. \n\n"}
{"id": "1901.10655", "contents": "Title: On the Calibration of Multiclass Classification with Rejection Abstract: We investigate the problem of multiclass classification with rejection, where\na classifier can choose not to make a prediction to avoid critical\nmisclassification. First, we consider an approach based on simultaneous\ntraining of a classifier and a rejector, which achieves the state-of-the-art\nperformance in the binary case. We analyze this approach for the multiclass\ncase and derive a general condition for calibration to the Bayes-optimal\nsolution, which suggests that calibration is hard to achieve by general loss\nfunctions unlike the binary case. Next, we consider another traditional\napproach based on confidence scores, in which the existing work focuses on a\nspecific class of losses. We propose rejection criteria for more general losses\nfor this approach and guarantee calibration to the Bayes-optimal solution.\nFinally, we conduct experiments to validate the relevance of our theoretical\nfindings. \n\n"}
{"id": "1901.11512", "contents": "Title: Minimizing Negative Transfer of Knowledge in Multivariate Gaussian\n  Processes: A Scalable and Regularized Approach Abstract: Recently there has been an increasing interest in the multivariate Gaussian\nprocess (MGP) which extends the Gaussian process (GP) to deal with multiple\noutputs. One approach to construct the MGP and account for non-trivial\ncommonalities amongst outputs employs a convolution process (CP). The CP is\nbased on the idea of sharing latent functions across several convolutions.\nDespite the elegance of the CP construction, it provides new challenges that\nneed yet to be tackled. First, even with a moderate number of outputs, model\nbuilding is extremely prohibitive due to the huge increase in computational\ndemands and number of parameters to be estimated. Second, the negative transfer\nof knowledge may occur when some outputs do not share commonalities. In this\npaper we address these issues. We propose a regularized pairwise modeling\napproach for the MGP established using CP. The key feature of our approach is\nto distribute the estimation of the full multivariate model into a group of\nbivariate GPs which are individually built. Interestingly pairwise modeling\nturns out to possess unique characteristics, which allows us to tackle the\nchallenge of negative transfer through penalizing the latent function that\nfacilitates information sharing in each bivariate model. Predictions are then\nmade through combining predictions from the bivariate models within a Bayesian\nframework. The proposed method has excellent scalability when the number of\noutputs is large and minimizes the negative transfer of knowledge between\nuncorrelated outputs. Statistical guarantees for the proposed method are\nstudied and its advantageous features are demonstrated through numerical\nstudies. \n\n"}
{"id": "cs/0312018", "contents": "Title: Mapping Subsets of Scholarly Information Abstract: We illustrate the use of machine learning techniques to analyze, structure,\nmaintain, and evolve a large online corpus of academic literature. An emerging\nfield of research can be identified as part of an existing corpus, permitting\nthe implementation of a more coherent community structure for its\npractitioners. \n\n"}
{"id": "cs/0610011", "contents": "Title: Creation and use of Citations in the ADS Abstract: With over 20 million records, the ADS citation database is regularly used by\nresearchers and librarians to measure the scientific impact of individuals,\ngroups, and institutions. In addition to the traditional sources of citations,\nthe ADS has recently added references extracted from the arXiv e-prints on a\nnightly basis. We review the procedures used to harvest and identify the\nreference data used in the creation of citations, the policies and procedures\nthat we follow to avoid double-counting and to eliminate contributions which\nmay not be scholarly in nature. Finally, we describe how users and institutions\ncan easily obtain quantitative citation data from the ADS, both interactively\nand via web-based programming tools.\n  The ADS is available at http://ads.harvard.edu. \n\n"}
{"id": "cs/9904002", "contents": "Title: A geometric framework for modelling similarity search Abstract: The aim of this paper is to propose a geometric framework for modelling\nsimilarity search in large and multidimensional data spaces of general nature,\nwhich seems to be flexible enough to address such issues as analysis of\ncomplexity, indexability, and the `curse of dimensionality.' Such a framework\nis provided by the concept of the so-called similarity workload, which is a\nprobability metric space $\\Omega$ (query domain) with a distinguished finite\nsubspace $X$ (dataset), together with an assembly of concepts, techniques, and\nresults from metric geometry. They include such notions as metric transform,\n$\\e$-entropy, and the phenomenon of concentration of measure on\nhigh-dimensional structures. In particular, we discuss the relevance of the\nlatter to understanding the curse of dimensionality. As some of those concepts\nand techniques are being currently reinvented by the database community, it\nseems desirable to try and bridge the gap between database research and the\nrelevant work already done in geometry and analysis. \n\n"}
{"id": "math/0607507", "contents": "Title: In-Degree and PageRank of Web pages: Why do they follow similar power\n  laws? Abstract: The PageRank is a popularity measure designed by Google to rank Web pages.\nExperiments confirm that the PageRank obeys a `power law' with the same\nexponent as the In-Degree. This paper presents a novel mathematical model that\nexplains this phenomenon. The relation between the PageRank and In-Degree is\nmodelled through a stochastic equation, which is inspired by the original\ndefinition of the PageRank, and is analogous to the well-known distributional\nidentity for the busy period in the M/G/1 queue. Further, we employ the theory\nof regular variation and Tauberian theorems to analytically prove that the tail\nbehavior of the PageRank and the In-Degree differ only by a multiplicative\nfactor, for which we derive a closed-form expression. Our analytical results\nare in good agreement with experimental data. \n\n"}
