{"id": "0802.1258", "contents": "Title: Bayesian Nonlinear Principal Component Analysis Using Random Fields Abstract: We propose a novel model for nonlinear dimension reduction motivated by the\nprobabilistic formulation of principal component analysis. Nonlinearity is\nachieved by specifying different transformation matrices at different locations\nof the latent space and smoothing the transformation using a Markov random\nfield type prior. The computation is made feasible by the recent advances in\nsampling from von Mises-Fisher distributions. \n\n"}
{"id": "1107.2021", "contents": "Title: Multi-Instance Learning with Any Hypothesis Class Abstract: In the supervised learning setting termed Multiple-Instance Learning (MIL),\nthe examples are bags of instances, and the bag label is a function of the\nlabels of its instances. Typically, this function is the Boolean OR. The\nlearner observes a sample of bags and the bag labels, but not the instance\nlabels that determine the bag labels. The learner is then required to emit a\nclassification rule for bags based on the sample. MIL has numerous\napplications, and many heuristic algorithms have been used successfully on this\nproblem, each adapted to specific settings or applications. In this work we\nprovide a unified theoretical analysis for MIL, which holds for any underlying\nhypothesis class, regardless of a specific application or problem domain. We\nshow that the sample complexity of MIL is only poly-logarithmically dependent\non the size of the bag, for any underlying hypothesis class. In addition, we\nintroduce a new PAC-learning algorithm for MIL, which uses a regular supervised\nlearning algorithm as an oracle. We prove that efficient PAC-learning for MIL\ncan be generated from any efficient non-MIL supervised learning algorithm that\nhandles one-sided error. The computational complexity of the resulting\nalgorithm is only polynomially dependent on the bag size. \n\n"}
{"id": "1203.0076", "contents": "Title: Using Barriers to Reduce the Sensitivity to Edge Miscalculations of\n  Casting-Based Object Projection Feature Estimation Abstract: 3D motion tracking is a critical task in many computer vision applications.\nUnsupervised markerless 3D motion tracking systems determine the most relevant\nobject in the screen and then track it by continuously estimating its\nprojection features (center and area) from the edge image and a point inside\nthe relevant object projection (namely, inner point), until the tracking fails.\nExisting reliable object projection feature estimation techniques are based on\nray-casting or grid-filling from the inner point. These techniques assume the\nedge image to be accurate. However, in real case scenarios, edge\nmiscalculations may arise from low contrast between the target object and its\nsurroundings or motion blur caused by low frame rates or fast moving target\nobjects. In this paper, we propose a barrier extension to casting-based\ntechniques that mitigates the effect of edge miscalculations. \n\n"}
{"id": "1203.1513", "contents": "Title: Invariant Scattering Convolution Networks Abstract: A wavelet scattering network computes a translation invariant image\nrepresentation, which is stable to deformations and preserves high frequency\ninformation for classification. It cascades wavelet transform convolutions with\nnon-linear modulus and averaging operators. The first network layer outputs\nSIFT-type descriptors whereas the next layers provide complementary invariant\ninformation which improves classification. The mathematical analysis of wavelet\nscattering networks explains important properties of deep convolution networks\nfor classification.\n  A scattering representation of stationary processes incorporates higher order\nmoments and can thus discriminate textures having the same Fourier power\nspectrum. State of the art classification results are obtained for handwritten\ndigits and texture discrimination, using a Gaussian kernel SVM and a generative\nPCA classifier. \n\n"}
{"id": "1205.5075", "contents": "Title: Efficient Sparse Group Feature Selection via Nonconvex Optimization Abstract: Sparse feature selection has been demonstrated to be effective in handling\nhigh-dimensional data. While promising, most of the existing works use convex\nmethods, which may be suboptimal in terms of the accuracy of feature selection\nand parameter estimation. In this paper, we expand a nonconvex paradigm to\nsparse group feature selection, which is motivated by applications that require\nidentifying the underlying group structure and performing feature selection\nsimultaneously. The main contributions of this article are twofold: (1)\nstatistically, we introduce a nonconvex sparse group feature selection model\nwhich can reconstruct the oracle estimator. Therefore, consistent feature\nselection and parameter estimation can be achieved; (2) computationally, we\npropose an efficient algorithm that is applicable to large-scale problems.\nNumerical results suggest that the proposed nonconvex method compares favorably\nagainst its competitors on synthetic data and real-world applications, thus\nachieving desired goal of delivering high performance. \n\n"}
{"id": "1206.6381", "contents": "Title: Shortest path distance in random k-nearest neighbor graphs Abstract: Consider a weighted or unweighted k-nearest neighbor graph that has been\nbuilt on n data points drawn randomly according to some density p on R^d. We\nstudy the convergence of the shortest path distance in such graphs as the\nsample size tends to infinity. We prove that for unweighted kNN graphs, this\ndistance converges to an unpleasant distance function on the underlying space\nwhose properties are detrimental to machine learning. We also study the\nbehavior of the shortest path distance in weighted kNN graphs. \n\n"}
{"id": "1208.5365", "contents": "Title: A Missing and Found Recognition System for Hajj and Umrah Abstract: This note describes an integrated recognition system for identifying missing\nand found objects as well as missing, dead, and found people during Hajj and\nUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of\nSaudi Arabia. It is assumed that the total estimated number of pilgrims will\nreach 20 millions during the next decade. The ultimate goal of this system is\nto integrate facial recognition and object identification solutions into the\nHajj and Umrah rituals. The missing and found computerized system is part of\nthe CrowdSensing system for Hajj and Umrah crowd estimation, management and\nsafety. \n\n"}
{"id": "1209.0654", "contents": "Title: Compressive Optical Deflectometric Tomography: A Constrained\n  Total-Variation Minimization Approach Abstract: Optical Deflectometric Tomography (ODT) provides an accurate characterization\nof transparent materials whose complex surfaces present a real challenge for\nmanufacture and control. In ODT, the refractive index map (RIM) of a\ntransparent object is reconstructed by measuring light deflection under\nmultiple orientations. We show that this imaging modality can be made\n\"compressive\", i.e., a correct RIM reconstruction is achievable with far less\nobservations than required by traditional Filtered Back Projection (FBP)\nmethods. Assuming a cartoon-shape RIM model, this reconstruction is driven by\nminimizing the map Total-Variation under a fidelity constraint with the\navailable observations. Moreover, two other realistic assumptions are added to\nimprove the stability of our approach: the map positivity and a frontier\ncondition. Numerically, our method relies on an accurate ODT sensing model and\non a primal-dual minimization scheme, including easily the sensing operator and\nthe proposed RIM constraints. We conclude this paper by demonstrating the power\nof our method on synthetic and experimental data under various compressive\nscenarios. In particular, the compressiveness of the stabilized ODT problem is\ndemonstrated by observing a typical gain of 20 dB compared to FBP at only 5% of\n360 incident light angles for moderately noisy sensing. \n\n"}
{"id": "1209.1688", "contents": "Title: Rank Centrality: Ranking from Pair-wise Comparisons Abstract: The question of aggregating pair-wise comparisons to obtain a global ranking\nover a collection of objects has been of interest for a very long time: be it\nranking of online gamers (e.g. MSR's TrueSkill system) and chess players,\naggregating social opinions, or deciding which product to sell based on\ntransactions. In most settings, in addition to obtaining a ranking, finding\n`scores' for each object (e.g. player's rating) is of interest for\nunderstanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation\nalgorithm for discovering scores for objects (or items) from pair-wise\ncomparisons. The algorithm has a natural random walk interpretation over the\ngraph of objects with an edge present between a pair of objects if they are\ncompared; the score, which we call Rank Centrality, of an object turns out to\nbe its stationary probability under this random walk. To study the efficacy of\nthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model\n(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which\neach object has an associated score which determines the probabilistic outcomes\nof pair-wise comparisons between objects. In terms of the pair-wise marginal\nprobabilities, which is the main subject of this paper, the MNL model and the\nBTL model are identical. We bound the finite sample error rates between the\nscores assumed by the BTL model and those estimated by our algorithm. In\nparticular, the number of samples required to learn the score well with high\nprobability depends on the structure of the comparison graph. When the\nLaplacian of the comparison graph has a strictly positive spectral gap, e.g.\neach item is compared to a subset of randomly chosen items, this leads to\ndependence on the number of samples that is nearly order-optimal. \n\n"}
{"id": "1209.2139", "contents": "Title: Fused Multiple Graphical Lasso Abstract: In this paper, we consider the problem of estimating multiple graphical\nmodels simultaneously using the fused lasso penalty, which encourages adjacent\ngraphs to share similar structures. A motivating example is the analysis of\nbrain networks of Alzheimer's disease using neuroimaging data. Specifically, we\nmay wish to estimate a brain network for the normal controls (NC), a brain\nnetwork for the patients with mild cognitive impairment (MCI), and a brain\nnetwork for Alzheimer's patients (AD). We expect the two brain networks for NC\nand MCI to share common structures but not to be identical to each other;\nsimilarly for the two brain networks for MCI and AD. The proposed formulation\ncan be solved using a second-order method. Our key technical contribution is to\nestablish the necessary and sufficient condition for the graphs to be\ndecomposable. Based on this key property, a simple screening rule is presented,\nwhich decomposes the large graphs into small subgraphs and allows an efficient\nestimation of multiple independent (small) subgraphs, dramatically reducing the\ncomputational cost. We perform experiments on both synthetic and real data; our\nresults demonstrate the effectiveness and efficiency of the proposed approach. \n\n"}
{"id": "1211.1043", "contents": "Title: Soft (Gaussian CDE) regression models and loss functions Abstract: Regression, unlike classification, has lacked a comprehensive and effective\napproach to deal with cost-sensitive problems by the reuse (and not a\nre-training) of general regression models. In this paper, a wide variety of\ncost-sensitive problems in regression (such as bids, asymmetric losses and\nrejection rules) can be solved effectively by a lightweight but powerful\napproach, consisting of: (1) the conversion of any traditional one-parameter\ncrisp regression model into a two-parameter soft regression model, seen as a\nnormal conditional density estimator, by the use of newly-introduced enrichment\nmethods; and (2) the reframing of an enriched soft regression model to new\ncontexts by an instance-dependent optimisation of the expected loss derived\nfrom the conditional normal distribution. \n\n"}
{"id": "1211.7180", "contents": "Title: Multislice Modularity Optimization in Community Detection and Image\n  Segmentation Abstract: Because networks can be used to represent many complex systems, they have\nattracted considerable attention in physics, computer science, sociology, and\nmany other disciplines. One of the most important areas of network science is\nthe algorithmic detection of cohesive groups (i.e., \"communities\") of nodes. In\nthis paper, we algorithmically detect communities in social networks and image\ndata by optimizing multislice modularity. A key advantage of modularity\noptimization is that it does not require prior knowledge of the number or sizes\nof communities, and it is capable of finding network partitions that are\ncomposed of communities of different sizes. By optimizing multislice modularity\nand subsequently calculating diagnostics on the resulting network partitions,\nit is thereby possible to obtain information about network structure across\nmultiple system scales. We illustrate this method on data from both social\nnetworks and images, and we find that optimization of multislice modularity\nperforms well on these two tasks without the need for extensive\nproblem-specific adaptation. However, improving the computational speed of this\nmethod remains a challenging open problem. \n\n"}
{"id": "1212.3530", "contents": "Title: A Multi-Orientation Analysis Approach to Retinal Vessel Tracking Abstract: This paper presents a method for retinal vasculature extraction based on\nbiologically inspired multi-orientation analysis. We apply multi-orientation\nanalysis via so-called invertible orientation scores, modeling the cortical\ncolumns in the visual system of higher mammals. This allows us to generically\ndeal with many hitherto complex problems inherent to vessel tracking, such as\ncrossings, bifurcations, parallel vessels, vessels of varying widths and\nvessels with high curvature. Our approach applies tracking in invertible\norientation scores via a novel geometrical principle for curve optimization in\nthe Euclidean motion group SE(2). The method runs fully automatically and\nprovides a detailed model of the retinal vasculature, which is crucial as a\nsound basis for further quantitative analysis of the retina, especially in\nscreening applications. \n\n"}
{"id": "1212.4527", "contents": "Title: GMM-Based Hidden Markov Random Field for Color Image and 3D Volume\n  Segmentation Abstract: In this project, we first study the Gaussian-based hidden Markov random field\n(HMRF) model and its expectation-maximization (EM) algorithm. Then we\ngeneralize it to Gaussian mixture model-based hidden Markov random field. The\nalgorithm is implemented in MATLAB. We also apply this algorithm to color image\nsegmentation problems and 3D volume segmentation problems. \n\n"}
{"id": "1212.5156", "contents": "Title: Nonparametric ridge estimation Abstract: We study the problem of estimating the ridges of a density function. Ridge\nestimation is an extension of mode finding and is useful for understanding the\nstructure of a density. It can also be used to find hidden structure in point\ncloud data. We show that, under mild regularity conditions, the ridges of the\nkernel density estimator consistently estimate the ridges of the true density.\nWhen the data are noisy measurements of a manifold, we show that the ridges are\nclose and topologically similar to the hidden manifold. To find the estimated\nridges in practice, we adapt the modified mean-shift algorithm proposed by\nOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical\nexperiments verify that the algorithm is accurate. \n\n"}
{"id": "1302.2576", "contents": "Title: The trace norm constrained matrix-variate Gaussian process for multitask\n  bipartite ranking Abstract: We propose a novel hierarchical model for multitask bipartite ranking. The\nproposed approach combines a matrix-variate Gaussian process with a generative\nmodel for task-wise bipartite ranking. In addition, we employ a novel trace\nconstrained variational inference approach to impose low rank structure on the\nposterior matrix-variate Gaussian process. The resulting posterior covariance\nfunction is derived in closed form, and the posterior mean function is the\nsolution to a matrix-variate regression with a novel spectral elastic net\nregularizer. Further, we show that variational inference for the trace\nconstrained matrix-variate Gaussian process combined with maximum likelihood\nparameter estimation for the bipartite ranking model is jointly convex. Our\nmotivating application is the prioritization of candidate disease genes. The\ngoal of this task is to aid the identification of unobserved associations\nbetween human genes and diseases using a small set of observed associations as\nwell as kernels induced by gene-gene interaction networks and disease\nontologies. Our experimental results illustrate the performance of the proposed\nmodel on real world datasets. Moreover, we find that the resulting low rank\nsolution improves the computational scalability of training and testing as\ncompared to baseline models. \n\n"}
{"id": "1304.5583", "contents": "Title: Distributed Low-rank Subspace Segmentation Abstract: Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups. \n\n"}
{"id": "1306.3003", "contents": "Title: Non-parametric Power-law Data Clustering Abstract: It has always been a great challenge for clustering algorithms to\nautomatically determine the cluster numbers according to the distribution of\ndatasets. Several approaches have been proposed to address this issue,\nincluding the recent promising work which incorporate Bayesian Nonparametrics\ninto the $k$-means clustering procedure. This approach shows simplicity in\nimplementation and solidity in theory, while it also provides a feasible way to\ninference in large scale datasets. However, several problems remains unsolved\nin this pioneering work, including the power-law data applicability, mechanism\nto merge centers to avoid the over-fitting problem, clustering order problem,\ne.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely\n\\emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-Yor\nProcess, \\emph{pyp-means} treats clusters differently by dynamically and\nadaptively changing the threshold to guarantee the generation of power-law\nclustering results. Also, one center agglomeration procedure is integrated into\nthe implementation to be able to merge small but close clusters and then\nadaptively determine the cluster number. With more discussion on the clustering\norder, the convergence proof, complexity analysis and extension to spectral\nclustering, our approach is compared with traditional clustering algorithm and\nvariational inference methods. The advantages and properties of pyp-means are\nvalidated by experiments on both synthetic datasets and real world datasets. \n\n"}
{"id": "1306.3476", "contents": "Title: Hyperparameter Optimization and Boosting for Classifying Facial\n  Expressions: How good can a \"Null\" Model be? Abstract: One of the goals of the ICML workshop on representation and learning is to\nestablish benchmark scores for a new data set of labeled facial expressions.\nThis paper presents the performance of a \"Null\" model consisting of\nconvolutions with random weights, PCA, pooling, normalization, and a linear\nreadout. Our approach focused on hyperparameter optimization rather than novel\nmodel components. On the Facial Expression Recognition Challenge held by the\nKaggle website, our hyperparameter optimization approach achieved a score of\n60% accuracy on the test data. This paper also introduces a new ensemble\nconstruction variant that combines hyperparameter optimization with the\nconstruction of ensembles. This algorithm constructed an ensemble of four\nmodels that scored 65.5% accuracy. These scores rank 12th and 5th respectively\namong the 56 challenge participants. It is worth noting that our approach was\ndeveloped prior to the release of the data set, and applied without\nmodification; our strong competition performance suggests that the TPE\nhyperparameter optimization algorithm and domain expertise encoded in our Null\nmodel can generalize to new image classification data sets. \n\n"}
{"id": "1307.6549", "contents": "Title: Making Laplacians commute Abstract: In this paper, we construct multimodal spectral geometry by finding a pair of\nclosest commuting operators (CCO) to a given pair of Laplacians. The CCOs are\njointly diagonalizable and hence have the same eigenbasis. Our construction\nnaturally extends classical data analysis tools based on spectral geometry,\nsuch as diffusion maps and spectral clustering. We provide several synthetic\nand real examples of applications in dimensionality reduction, shape analysis,\nand clustering, demonstrating that our method better captures the inherent\nstructure of multi-modal data. \n\n"}
{"id": "1309.4151", "contents": "Title: A Non-Local Means Filter for Removing the Poisson Noise Abstract: A new image denoising algorithm to deal with the Poisson noise model is\ngiven, which is based on the idea of Non-Local Mean. By using the \"Oracle\"\nconcept, we establish a theorem to show that the Non-Local Means Filter can\neffectively deal with Poisson noise with some modification. Under the\ntheoretical result, we construct our new algorithm called Non-Local Means\nPoisson Filter and demonstrate in theory that the filter converges at the usual\noptimal rate. The filter is as simple as the classic Non-Local Means and the\nsimulation results show that our filter is very competitive. \n\n"}
{"id": "1310.1533", "contents": "Title: CAM: Causal additive models, high-dimensional order search and penalized\n  regression Abstract: We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data. \n\n"}
{"id": "1310.7443", "contents": "Title: On Convergent Finite Difference Schemes for Variational - PDE Based\n  Image Processing Abstract: We study an adaptive anisotropic Huber functional based image restoration\nscheme. By using a combination of L2-L1 regularization functions, an adaptive\nHuber functional based energy minimization model provides denoising with edge\npreservation in noisy digital images. We study a convergent finite difference\nscheme based on continuous piecewise linear functions and use a variable\nsplitting scheme, namely the Split Bregman, to obtain the discrete minimizer.\nExperimental results are given in image denoising and comparison with additive\noperator splitting, dual fixed point, and projected gradient schemes illustrate\nthat the best convergence rates are obtained for our algorithm. \n\n"}
{"id": "1312.3061", "contents": "Title: Fast Approximate $K$-Means via Cluster Closures Abstract: $K$-means, a simple and effective clustering algorithm, is one of the most\nwidely used algorithms in multimedia and computer vision community. Traditional\n$k$-means is an iterative algorithm---in each iteration new cluster centers are\ncomputed and each data point is re-assigned to its nearest center. The cluster\nre-assignment step becomes prohibitively expensive when the number of data\npoints and cluster centers are large.\n  In this paper, we propose a novel approximate $k$-means algorithm to greatly\nreduce the computational complexity in the assignment step. Our approach is\nmotivated by the observation that most active points changing their cluster\nassignments at each iteration are located on or near cluster boundaries. The\nidea is to efficiently identify those active points by pre-assembling the data\ninto groups of neighboring points using multiple random spatial partition\ntrees, and to use the neighborhood information to construct a closure for each\ncluster, in such a way only a small number of cluster candidates need to be\nconsidered when assigning a data point to its nearest cluster. Using complexity\nanalysis, image data clustering, and applications to image retrieval, we show\nthat our approach out-performs state-of-the-art approximate $k$-means\nalgorithms in terms of clustering quality and efficiency. \n\n"}
{"id": "1312.6199", "contents": "Title: Intriguing properties of neural networks Abstract: Deep neural networks are highly expressive models that have recently achieved\nstate of the art performance on speech and visual recognition tasks. While\ntheir expressiveness is the reason they succeed, it also causes them to learn\nuninterpretable solutions that could have counter-intuitive properties. In this\npaper we report two such properties.\n  First, we find that there is no distinction between individual high level\nunits and random linear combinations of high level units, according to various\nmethods of unit analysis. It suggests that it is the space, rather than the\nindividual units, that contains of the semantic information in the high layers\nof neural networks.\n  Second, we find that deep neural networks learn input-output mappings that\nare fairly discontinuous to a significant extend. We can cause the network to\nmisclassify an image by applying a certain imperceptible perturbation, which is\nfound by maximizing the network's prediction error. In addition, the specific\nnature of these perturbations is not a random artifact of learning: the same\nperturbation can cause a different network, that was trained on a different\nsubset of the dataset, to misclassify the same input. \n\n"}
{"id": "1312.6826", "contents": "Title: 3D Interest Point Detection via Discriminative Learning Abstract: The task of detecting the interest points in 3D meshes has typically been\nhandled by geometric methods. These methods, while greatly describing human\npreference, can be ill-equipped for handling the variety and subjectivity in\nhuman responses. Different tasks have different requirements for interest point\ndetection; some tasks may necessitate high precision while other tasks may\nrequire high recall. Sometimes points with high curvature may be desirable,\nwhile in other cases high curvature may be an indication of noise. Geometric\nmethods lack the required flexibility to adapt to such changes. As a\nconsequence, interest point detection seems to be well suited for machine\nlearning methods that can be trained to match the criteria applied on the\nannotated training data. In this paper, we formulate interest point detection\nas a supervised binary classification problem using a random forest as our\nclassifier. Among other challenges, we are faced with an imbalanced learning\nproblem due to the substantial difference in the priors between interest and\nnon-interest points. We address this by re-sampling the training set. We\nvalidate the accuracy of our method and compare our results to those of five\nstate of the art methods on a new, standard benchmark. \n\n"}
{"id": "1401.3409", "contents": "Title: Low-Rank Modeling and Its Applications in Image Analysis Abstract: Low-rank modeling generally refers to a class of methods that solve problems\nby representing variables of interest as low-rank matrices. It has achieved\ngreat success in various fields including computer vision, data mining, signal\nprocessing and bioinformatics. Recently, much progress has been made in\ntheories, algorithms and applications of low-rank modeling, such as exact\nlow-rank matrix recovery via convex programming and matrix completion applied\nto collaborative filtering. These advances have brought more and more\nattentions to this topic. In this paper, we review the recent advance of\nlow-rank modeling, the state-of-the-art algorithms, and related applications in\nimage analysis. We first give an overview to the concept of low-rank modeling\nand challenging problems in this area. Then, we summarize the models and\nalgorithms for low-rank matrix recovery and illustrate their advantages and\nlimitations with numerical experiments. Next, we introduce a few applications\nof low-rank modeling in the context of image analysis. Finally, we conclude\nthis paper with some discussions. \n\n"}
{"id": "1402.1801", "contents": "Title: Efficient Low Dose X-ray CT Reconstruction through Sparsity-Based MAP\n  Modeling Abstract: Ultra low radiation dose in X-ray Computed Tomography (CT) is an important\nclinical objective in order to minimize the risk of carcinogenesis. Compressed\nSensing (CS) enables significant reductions in radiation dose to be achieved by\nproducing diagnostic images from a limited number of CT projections. However,\nthe excessive computation time that conventional CS-based CT reconstruction\ntypically requires has limited clinical implementation. In this paper, we first\ndemonstrate that a thorough analysis of CT reconstruction through a Maximum a\nPosteriori objective function results in a weighted compressive sensing\nproblem. This analysis enables us to formulate a low dose fan beam and helical\ncone beam CT reconstruction. Subsequently, we provide an efficient solution to\nthe formulated CS problem based on a Fast Composite Splitting Algorithm-Latent\nExpected Maximization (FCSA-LEM) algorithm. In the proposed method we use\npseudo polar Fourier transform as the measurement matrix in order to decrease\nthe computational complexity; and rebinning of the projections to parallel rays\nin order to extend its application to fan beam and helical cone beam scans. The\nweight involved in the proposed weighted CS model, denoted by Error Adaptation\nWeight (EAW), is calculated based on the statistical characteristics of CT\nreconstruction and is a function of Poisson measurement noise and rebinning\ninterpolation error. Simulation results show that low computational complexity\nof the proposed method made the fast recovery of the CT images possible and\nusing EAW reduces the reconstruction error by one order of magnitude. Recovery\nof a high quality 512$\\times$ 512 image was achieved in less than 20 sec on a\ndesktop computer without numerical optimizations. \n\n"}
{"id": "1402.2031", "contents": "Title: Deeply Coupled Auto-encoder Networks for Cross-view Classification Abstract: The comparison of heterogeneous samples extensively exists in many\napplications, especially in the task of image classification. In this paper, we\npropose a simple but effective coupled neural network, called Deeply Coupled\nAutoencoder Networks (DCAN), which seeks to build two deep neural networks,\ncoupled with each other in every corresponding layers. In DCAN, each deep\nstructure is developed via stacking multiple discriminative coupled\nauto-encoders, a denoising auto-encoder trained with maximum margin criterion\nconsisting of intra-class compactness and inter-class penalty. This single\nlayer component makes our model simultaneously preserve the local consistency\nand enhance its discriminative capability. With increasing number of layers,\nthe coupled networks can gradually narrow the gap between the two views.\nExtensive experiments on cross-view image classification tasks demonstrate the\nsuperiority of our method over state-of-the-art methods. \n\n"}
{"id": "1403.0700", "contents": "Title: Random Projections on Manifolds of Symmetric Positive Definite Matrices\n  for Image Classification Abstract: Recent advances suggest that encoding images through Symmetric Positive\nDefinite (SPD) matrices and then interpreting such matrices as points on\nRiemannian manifolds can lead to increased classification performance. Taking\ninto account manifold geometry is typically done via (1) embedding the\nmanifolds in tangent spaces, or (2) embedding into Reproducing Kernel Hilbert\nSpaces (RKHS). While embedding into tangent spaces allows the use of existing\nEuclidean-based learning algorithms, manifold shape is only approximated which\ncan cause loss of discriminatory information. The RKHS approach retains more of\nthe manifold structure, but may require non-trivial effort to kernelise\nEuclidean-based learning algorithms. In contrast to the above approaches, in\nthis paper we offer a novel solution that allows SPD matrices to be used with\nunmodified Euclidean-based learning algorithms, with the true manifold shape\nwell-preserved. Specifically, we propose to project SPD matrices using a set of\nrandom projection hyperplanes over RKHS into a random projection space, which\nleads to representing each matrix as a vector of projection coefficients.\nExperiments on face recognition, person re-identification and texture\nclassification show that the proposed approach outperforms several recent\nmethods, such as Tensor Sparse Coding, Histogram Plus Epitome, Riemannian\nLocality Preserving Projection and Relational Divergence Classification. \n\n"}
{"id": "1403.2330", "contents": "Title: Subspace clustering using a symmetric low-rank representation Abstract: In this paper, we propose a low-rank representation with symmetric constraint\n(LRRSC) method for robust subspace clustering. Given a collection of data\npoints approximately drawn from multiple subspaces, the proposed technique can\nsimultaneously recover the dimension and members of each subspace. LRRSC\nextends the original low-rank representation algorithm by integrating a\nsymmetric constraint into the low-rankness property of high-dimensional data\nrepresentation. The symmetric low-rank representation, which preserves the\nsubspace structures of high-dimensional data, guarantees weight consistency for\neach pair of data points so that highly correlated data points of subspaces are\nrepresented together. Moreover, it can be efficiently calculated by solving a\nconvex optimization problem. We provide a rigorous proof for minimizing the\nnuclear-norm regularized least square problem with a symmetric constraint. The\naffinity matrix for spectral clustering can be obtained by further exploiting\nthe angular information of the principal directions of the symmetric low-rank\nrepresentation. This is a critical step towards evaluating the memberships\nbetween data points. Experimental results on benchmark databases demonstrate\nthe effectiveness and robustness of LRRSC compared with several\nstate-of-the-art subspace clustering algorithms. \n\n"}
{"id": "1403.6706", "contents": "Title: Beyond L2-Loss Functions for Learning Sparse Models Abstract: Incorporating sparsity priors in learning tasks can give rise to simple, and\ninterpretable models for complex high dimensional data. Sparse models have\nfound widespread use in structure discovery, recovering data from corruptions,\nand a variety of large scale unsupervised and supervised learning problems.\nAssuming the availability of sufficient data, these methods infer dictionaries\nfor sparse representations by optimizing for high-fidelity reconstruction. In\nmost scenarios, the reconstruction quality is measured using the squared\nEuclidean distance, and efficient algorithms have been developed for both batch\nand online learning cases. However, new application domains motivate looking\nbeyond conventional loss functions. For example, robust loss functions such as\n$\\ell_1$ and Huber are useful in learning outlier-resilient models, and the\nquantile loss is beneficial in discovering structures that are the\nrepresentative of a particular quantile. These new applications motivate our\nwork in generalizing sparse learning to a broad class of convex loss functions.\nIn particular, we consider the class of piecewise linear quadratic (PLQ) cost\nfunctions that includes Huber, as well as $\\ell_1$, quantile, Vapnik, hinge\nloss, and smoothed variants of these penalties. We propose an algorithm to\nlearn dictionaries and obtain sparse codes when the data reconstruction\nfidelity is measured using any smooth PLQ cost function. We provide convergence\nguarantees for the proposed algorithm, and demonstrate the convergence behavior\nusing empirical experiments. Furthermore, we present three case studies that\nrequire the use of PLQ cost functions: (i) robust image modeling, (ii) tag\nrefinement for image annotation and retrieval and (iii) computing empirical\nconfidence limits for subspace clustering. \n\n"}
{"id": "1404.0334", "contents": "Title: Active Deformable Part Models Abstract: This paper presents an active approach for part-based object detection, which\noptimizes the order of part filter evaluations and the time at which to stop\nand make a prediction. Statistics, describing the part responses, are learned\nfrom training data and are used to formalize the part scheduling problem as an\noffline optimization. Dynamic programming is applied to obtain a policy, which\nbalances the number of part evaluations with the classification accuracy.\nDuring inference, the policy is used as a look-up table to choose the part\norder and the stopping time based on the observed filter responses. The method\nis faster than cascade detection with deformable part models (which does not\noptimize the part order) with negligible loss in accuracy when evaluated on the\nPASCAL VOC 2007 and 2010 datasets. \n\n"}
{"id": "1406.0132", "contents": "Title: Seeing the Big Picture: Deep Embedding with Contextual Evidences Abstract: In the Bag-of-Words (BoW) model based image retrieval task, the precision of\nvisual matching plays a critical role in improving retrieval performance.\nConventionally, local cues of a keypoint are employed. However, such strategy\ndoes not consider the contextual evidences of a keypoint, a problem which would\nlead to the prevalence of false matches. To address this problem, this paper\ndefines \"true match\" as a pair of keypoints which are similar on three levels,\ni.e., local, regional, and global. Then, a principled probabilistic framework\nis established, which is capable of implicitly integrating discriminative cues\nfrom all these feature levels.\n  Specifically, the Convolutional Neural Network (CNN) is employed to extract\nfeatures from regional and global patches, leading to the so-called \"Deep\nEmbedding\" framework. CNN has been shown to produce excellent performance on a\ndozen computer vision tasks such as image classification and detection, but few\nworks have been done on BoW based image retrieval. In this paper, firstly we\nshow that proper pre-processing techniques are necessary for effective usage of\nCNN feature. Then, in the attempt to fit it into our model, a novel indexing\nstructure called \"Deep Indexing\" is introduced, which dramatically reduces\nmemory usage.\n  Extensive experiments on three benchmark datasets demonstrate that, the\nproposed Deep Embedding method greatly promotes the retrieval accuracy when CNN\nfeature is integrated. We show that our method is efficient in terms of both\nmemory and time cost, and compares favorably with the state-of-the-art methods. \n\n"}
{"id": "1406.2602", "contents": "Title: Graph Approximation and Clustering on a Budget Abstract: We consider the problem of learning from a similarity matrix (such as\nspectral clustering and lowd imensional embedding), when computing pairwise\nsimilarities are costly, and only a limited number of entries can be observed.\nWe provide a theoretical analysis using standard notions of graph\napproximation, significantly generalizing previous results (which focused on\nspectral clustering with two clusters). We also propose a new algorithmic\napproach based on adaptive sampling, which experimentally matches or improves\non previous methods, while being considerably more general and computationally\ncheaper. \n\n"}
{"id": "1407.4764", "contents": "Title: Efficient On-the-fly Category Retrieval using ConvNets and GPUs Abstract: We investigate the gains in precision and speed, that can be obtained by\nusing Convolutional Networks (ConvNets) for on-the-fly retrieval - where\nclassifiers are learnt at run time for a textual query from downloaded images,\nand used to rank large image or video datasets.\n  We make three contributions: (i) we present an evaluation of state-of-the-art\nimage representations for object category retrieval over standard benchmark\ndatasets containing 1M+ images; (ii) we show that ConvNets can be used to\nobtain features which are incredibly performant, and yet much lower dimensional\nthan previous state-of-the-art image representations, and that their\ndimensionality can be reduced further without loss in performance by\ncompression using product quantization or binarization. Consequently, features\nwith the state-of-the-art performance on large-scale datasets of millions of\nimages can fit in the memory of even a commodity GPU card; (iii) we show that\nan SVM classifier can be learnt within a ConvNet framework on a GPU in parallel\nwith downloading the new training images, allowing for a continuous refinement\nof the model as more images become available, and simultaneous training and\nranking. The outcome is an on-the-fly system that significantly outperforms its\npredecessors in terms of: precision of retrieval, memory requirements, and\nspeed, facilitating accurate on-the-fly learning and ranking in under a second\non a single GPU. \n\n"}
{"id": "1408.3467", "contents": "Title: Evaluating Visual Properties via Robust HodgeRank Abstract: Nowadays, how to effectively evaluate visual properties has become a popular\ntopic for fine-grained visual comprehension. In this paper we study the problem\nof how to estimate such visual properties from a ranking perspective with the\nhelp of the annotators from online crowdsourcing platforms. The main challenges\nof our task are two-fold. On one hand, the annotations often contain\ncontaminated information, where a small fraction of label flips might ruin the\nglobal ranking of the whole dataset. On the other hand, considering the large\ndata capacity, the annotations are often far from being complete. What is\nworse, there might even exist imbalanced annotations where a small subset of\nsamples are frequently annotated. Facing such challenges, we propose a robust\nranking framework based on the principle of Hodge decomposition of imbalanced\nand incomplete ranking data. According to the HodgeRank theory, we find that\nthe major source of the contamination comes from the cyclic ranking component\nof the Hodge decomposition. This leads us to an outlier detection formulation\nas sparse approximations of the cyclic ranking projection. Taking a step\nfurther, it facilitates a novel outlier detection model as Huber's LASSO in\nrobust statistics. Moreover, simple yet scalable algorithms are developed based\non Linearized Bregman Iteration to achieve an even less biased estimator.\nStatistical consistency of outlier detection is established in both cases under\nnearly the same conditions. Our studies are supported by experiments with both\nsimulated examples and real-world data. The proposed framework provides us a\npromising tool for robust ranking with large scale crowdsourcing data arising\nfrom computer vision. \n\n"}
{"id": "1409.7963", "contents": "Title: MoDeep: A Deep Learning Framework Using Motion Features for Human Pose\n  Estimation Abstract: In this work, we propose a novel and efficient method for articulated human\npose estimation in videos using a convolutional network architecture, which\nincorporates both color and motion features. We propose a new human body pose\ndataset, FLIC-motion, that extends the FLIC dataset with additional motion\nfeatures. We apply our architecture to this dataset and report significantly\nbetter performance than current state-of-the-art pose detection systems. \n\n"}
{"id": "1410.0311", "contents": "Title: $\\ell_1$-K-SVD: A Robust Dictionary Learning Algorithm With Simultaneous\n  Update Abstract: We develop a dictionary learning algorithm by minimizing the $\\ell_1$\ndistortion metric on the data term, which is known to be robust for\nnon-Gaussian noise contamination. The proposed algorithm exploits the idea of\niterative minimization of weighted $\\ell_2$ error. We refer to this algorithm\nas $\\ell_1$-K-SVD, where the dictionary atoms and the corresponding sparse\ncoefficients are simultaneously updated to minimize the $\\ell_1$ objective,\nresulting in noise-robustness. We demonstrate through experiments that the\n$\\ell_1$-K-SVD algorithm results in higher atom recovery rate compared with the\nK-SVD and the robust dictionary learning (RDL) algorithm proposed by Lu et al.,\nboth in Gaussian and non-Gaussian noise conditions. We also show that, for\nfixed values of sparsity, number of dictionary atoms, and data-dimension, the\n$\\ell_1$-K-SVD algorithm outperforms the K-SVD and RDL algorithms when the\ntraining set available is small. We apply the proposed algorithm for denoising\nnatural images corrupted by additive Gaussian and Laplacian noise. The images\ndenoised using $\\ell_1$-K-SVD are observed to have slightly higher peak\nsignal-to-noise ratio (PSNR) over K-SVD for Laplacian noise, but the\nimprovement in structural similarity index (SSIM) is significant (approximately\n$0.1$) for lower values of input PSNR, indicating the efficacy of the $\\ell_1$\nmetric. \n\n"}
{"id": "1411.1091", "contents": "Title: Do Convnets Learn Correspondence? Abstract: Convolutional neural nets (convnets) trained from massive labeled datasets\nhave substantially improved the state-of-the-art in image classification and\nobject detection. However, visual understanding requires establishing\ncorrespondence on a finer level than object category. Given their large pooling\nregions and training from whole-image labels, it is not clear that convnets\nderive their success from an accurate correspondence model which could be used\nfor precise localization. In this paper, we study the effectiveness of convnet\nactivation features for tasks requiring correspondence. We present evidence\nthat convnet features localize at a much finer scale than their receptive field\nsizes, that they can be used to perform intraclass alignment as well as\nconventional hand-engineered features, and that they outperform conventional\nfeatures in keypoint prediction on objects from PASCAL VOC 2011. \n\n"}
{"id": "1411.1971", "contents": "Title: Power-Law Graph Cuts Abstract: Algorithms based on spectral graph cut objectives such as normalized cuts,\nratio cuts and ratio association have become popular in recent years because\nthey are widely applicable and simple to implement via standard eigenvector\ncomputations. Despite strong performance for a number of clustering tasks,\nspectral graph cut algorithms still suffer from several limitations: first,\nthey require the number of clusters to be known in advance, but this\ninformation is often unknown a priori; second, they tend to produce clusters\nwith uniform sizes. In some cases, the true clusters exhibit a known size\ndistribution; in image segmentation, for instance, human-segmented images tend\nto yield segment sizes that follow a power-law distribution. In this paper, we\npropose a general framework of power-law graph cut algorithms that produce\nclusters whose sizes are power-law distributed, and also does not fix the\nnumber of clusters upfront. To achieve our goals, we treat the Pitman-Yor\nexchangeable partition probability function (EPPF) as a regularizer to graph\ncut objectives. Because the resulting objectives cannot be solved by relaxing\nvia eigenvectors, we derive a simple iterative algorithm to locally optimize\nthe objectives. Moreover, we show that our proposed algorithm can be viewed as\nperforming MAP inference on a particular Pitman-Yor mixture model. Our\nexperiments on various data sets show the effectiveness of our algorithms. \n\n"}
{"id": "1411.4952", "contents": "Title: From Captions to Visual Concepts and Back Abstract: This paper presents a novel approach for automatically generating image\ndescriptions: visual detectors, language models, and multimodal similarity\nmodels learnt directly from a dataset of image captions. We use multiple\ninstance learning to train visual detectors for words that commonly occur in\ncaptions, including many different parts of speech such as nouns, verbs, and\nadjectives. The word detector outputs serve as conditional inputs to a\nmaximum-entropy language model. The language model learns from a set of over\n400,000 image descriptions to capture the statistics of word usage. We capture\nglobal semantics by re-ranking caption candidates using sentence-level features\nand a deep multimodal similarity model. Our system is state-of-the-art on the\nofficial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When\nhuman judges compare the system captions to ones written by other people on our\nheld-out test set, the system captions have equal or better quality 34% of the\ntime. \n\n"}
{"id": "1411.6067", "contents": "Title: Viewpoints and Keypoints Abstract: We characterize the problem of pose estimation for rigid objects in terms of\ndetermining viewpoint to explain coarse pose and keypoint prediction to capture\nthe finer details. We address both these tasks in two different settings - the\nconstrained setting with known bounding boxes and the more challenging\ndetection setting where the aim is to simultaneously detect and correctly\nestimate pose of objects. We present Convolutional Neural Network based\narchitectures for these and demonstrate that leveraging viewpoint estimates can\nsubstantially improve local appearance based keypoint predictions. In addition\nto achieving significant improvements over state-of-the-art in the above tasks,\nwe analyze the error modes and effect of object characteristics on performance\nto guide future efforts towards this goal. \n\n"}
{"id": "1411.6447", "contents": "Title: The Application of Two-level Attention Models in Deep Convolutional\n  Neural Network for Fine-grained Image Classification Abstract: Fine-grained classification is challenging because categories can only be\ndiscriminated by subtle and local differences. Variances in the pose, scale or\nrotation usually make the problem more difficult. Most fine-grained\nclassification systems follow the pipeline of finding foreground object or\nobject parts (where) to extract discriminative features (what).\n  In this paper, we propose to apply visual attention to fine-grained\nclassification task using deep neural network. Our pipeline integrates three\ntypes of attention: the bottom-up attention that propose candidate patches, the\nobject-level top-down attention that selects relevant patches to a certain\nobject, and the part-level top-down attention that localizes discriminative\nparts. We combine these attentions to train domain-specific deep nets, then use\nit to improve both the what and where aspects. Importantly, we avoid using\nexpensive annotations like bounding box or part information from end-to-end.\nThe weak supervision constraint makes our work easier to generalize.\n  We have verified the effectiveness of the method on the subsets of ILSVRC2012\ndataset and CUB200_2011 dataset. Our pipeline delivered significant\nimprovements and achieved the best accuracy under the weakest supervision\ncondition. The performance is competitive against other methods that rely on\nadditional annotations. \n\n"}
{"id": "1412.0680", "contents": "Title: Fast Sublinear Sparse Representation using Shallow Tree Matching Pursuit Abstract: Sparse approximations using highly over-complete dictionaries is a\nstate-of-the-art tool for many imaging applications including denoising,\nsuper-resolution, compressive sensing, light-field analysis, and object\nrecognition. Unfortunately, the applicability of such methods is severely\nhampered by the computational burden of sparse approximation: these algorithms\nare linear or super-linear in both the data dimensionality and size of the\ndictionary. We propose a framework for learning the hierarchical structure of\nover-complete dictionaries that enables fast computation of sparse\nrepresentations. Our method builds on tree-based strategies for nearest\nneighbor matching, and presents domain-specific enhancements that are highly\nefficient for the analysis of image patches. Contrary to most popular methods\nfor building spatial data structures, out methods rely on shallow, balanced\ntrees with relatively few layers. We show an extensive array of experiments on\nseveral applications such as image denoising/superresolution, compressive\nvideo/light-field sensing where we practically achieve 100-1000x speedup (with\na less than 1dB loss in accuracy). \n\n"}
{"id": "1412.1897", "contents": "Title: Deep Neural Networks are Easily Fooled: High Confidence Predictions for\n  Unrecognizable Images Abstract: Deep neural networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of pattern-recognition tasks, most notably visual\nclassification problems. Given that DNNs are now able to classify objects in\nimages with near-human-level performance, questions naturally arise as to what\ndifferences remain between computer and human vision. A recent study revealed\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\nlion a library). Here we show a related result: it is easy to produce images\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\ncertainty that white noise static is a lion). Specifically, we take\nconvolutional neural networks trained to perform well on either the ImageNet or\nMNIST datasets and then find images with evolutionary algorithms or gradient\nascent that DNNs label with high confidence as belonging to each dataset class.\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\nbelieve with near certainty are familiar objects, which we call \"fooling\nimages\" (more generally, fooling examples). Our results shed light on\ninteresting differences between human vision and current DNNs, and raise\nquestions about the generality of DNN computer vision. \n\n"}
{"id": "1412.4729", "contents": "Title: Translating Videos to Natural Language Using Deep Recurrent Neural\n  Networks Abstract: Solving the visual symbol grounding problem has long been a goal of\nartificial intelligence. The field appears to be advancing closer to this goal\nwith recent breakthroughs in deep learning for natural language grounding in\nstatic images. In this paper, we propose to translate videos directly to\nsentences using a unified deep neural network with both convolutional and\nrecurrent structure. Described video datasets are scarce, and most existing\nmethods have been applied to toy domains with a small vocabulary of possible\nwords. By transferring knowledge from 1.2M+ images with category labels and\n100,000+ images with captions, our method is able to create sentence\ndescriptions of open-domain videos with large vocabularies. We compare our\napproach with recent work using language generation metrics, subject, verb, and\nobject prediction accuracy, and a human evaluation. \n\n"}
{"id": "1412.5068", "contents": "Title: Towards Deep Neural Network Architectures Robust to Adversarial Examples Abstract: Recent work has shown deep neural networks (DNNs) to be highly susceptible to\nwell-designed, small perturbations at the input layer, or so-called adversarial\nexamples. Taking images as an example, such distortions are often\nimperceptible, but can result in 100% mis-classification for a state of the art\nDNN. We study the structure of adversarial examples and explore network\ntopology, pre-processing and training strategies to improve the robustness of\nDNNs. We perform various experiments to assess the removability of adversarial\nexamples by corrupting with additional noise and pre-processing with denoising\nautoencoders (DAEs). We find that DAEs can remove substantial amounts of the\nadversarial noise. How- ever, when stacking the DAE with the original DNN, the\nresulting network can again be attacked by new adversarial examples with even\nsmaller distortion. As a solution, we propose Deep Contractive Network, a model\nwith a new end-to-end training procedure that includes a smoothness penalty\ninspired by the contractive autoencoder (CAE). This increases the network\nrobustness to adversarial examples, without a significant performance penalty. \n\n"}
{"id": "1412.5808", "contents": "Title: Minimizing the Number of Matching Queries for Object Retrieval Abstract: To increase the computational efficiency of interest-point based object\nretrieval, researchers have put remarkable research efforts into improving the\nefficiency of kNN-based feature matching, pursuing to match thousands of\nfeatures against a database within fractions of a second. However, due to the\nhigh-dimensional nature of image features that reduces the effectivity of index\nstructures (curse of dimensionality), due to the vast amount of features stored\nin image databases (images are often represented by up to several thousand\nfeatures), this ultimate goal demanded to trade query runtimes for query\nprecision. In this paper we address an approach complementary to indexing in\norder to improve the runtimes of retrieval by querying only the most promising\nkeypoint descriptors, as this affects matching runtimes linearly and can\ntherefore lead to increased efficiency. As this reduction of kNN queries\nreduces the number of tentative correspondences, a loss of query precision is\nminimized by an additional image-level correspondence generation stage with a\ncomputational performance independent of the underlying indexing structure. We\nevaluate such an adaption of the standard recognition pipeline on a variety of\ndatasets using both SIFT and state-of-the-art binary descriptors. Our results\nsuggest that decreasing the number of queried descriptors does not necessarily\nimply a reduction in the result quality as long as alternative ways of\nincreasing query recall (by thoroughly selecting k) and MAP (using image-level\ncorrespondence generation) are considered. \n\n"}
{"id": "1412.6115", "contents": "Title: Compressing Deep Convolutional Networks using Vector Quantization Abstract: Deep convolutional neural networks (CNN) has become the most promising method\nfor object recognition, repeatedly demonstrating record breaking results for\nimage classification and object detection in recent years. However, a very deep\nCNN generally involves many layers with millions of parameters, making the\nstorage of the network model to be extremely large. This prohibits the usage of\ndeep CNNs on resource limited hardware, especially cell phones or other\nembedded devices. In this paper, we tackle this model storage issue by\ninvestigating information theoretical vector quantization methods for\ncompressing the parameters of CNNs. In particular, we have found in terms of\ncompressing the most storage demanding dense connected layers, vector\nquantization methods have a clear gain over existing matrix factorization\nmethods. Simply applying k-means clustering to the weights or conducting\nproduct quantization can lead to a very good balance between model size and\nrecognition accuracy. For the 1000-category classification task in the ImageNet\nchallenge, we are able to achieve 16-24 times compression of the network with\nonly 1% loss of classification accuracy using the state-of-the-art CNN. \n\n"}
{"id": "1412.6553", "contents": "Title: Speeding-up Convolutional Neural Networks Using Fine-tuned\n  CP-Decomposition Abstract: We propose a simple two-step approach for speeding up convolution layers\nwithin large convolutional neural networks based on tensor decomposition and\ndiscriminative fine-tuning. Given a layer, we use non-linear least squares to\ncompute a low-rank CP-decomposition of the 4D convolution kernel tensor into a\nsum of a small number of rank-one tensors. At the second step, this\ndecomposition is used to replace the original convolutional layer with a\nsequence of four convolutional layers with small kernels. After such\nreplacement, the entire network is fine-tuned on the training data using\nstandard backpropagation process.\n  We evaluate this approach on two CNNs and show that it is competitive with\nprevious approaches, leading to higher obtained CPU speedups at the cost of\nlower accuracy drops for the smaller of the two networks. Thus, for the\n36-class character classification CNN, our approach obtains a 8.5x CPU speedup\nof the whole network with only minor accuracy drop (1% from 91% to 90%). For\nthe standard ImageNet architecture (AlexNet), the approach speeds up the second\nconvolution layer by a factor of 4x at the cost of $1\\%$ increase of the\noverall top-5 classification error. \n\n"}
{"id": "1412.6596", "contents": "Title: Training Deep Neural Networks on Noisy Labels with Bootstrapping Abstract: Current state-of-the-art deep learning systems for visual object recognition\nand detection use purely supervised training with regularization such as\ndropout to avoid overfitting. The performance depends critically on the amount\nof labeled examples, and in current practice the labels are assumed to be\nunambiguous and accurate. However, this assumption often does not hold; e.g. in\nrecognition, class labels may be missing; in detection, objects in the image\nmay not be localized; and in general, the labeling may be subjective. In this\nwork we propose a generic way to handle noisy and incomplete labeling by\naugmenting the prediction objective with a notion of consistency. We consider a\nprediction consistent if the same prediction is made given similar percepts,\nwhere the notion of similarity is between deep network features computed from\nthe input data. In experiments we demonstrate that our approach yields\nsubstantial robustness to label noise on several datasets. On MNIST handwritten\ndigits, we show that our model is robust to label corruption. On the Toronto\nFace Database, we show that our model handles well the case of subjective\nlabels in emotion recognition, achieving state-of-the- art results, and can\nalso benefit from unlabeled face images with no modification to our method. On\nthe ILSVRC2014 detection challenge data, we show that our approach extends to\nvery deep networks, high resolution images and structured outputs, and results\nin improved scalable detection. \n\n"}
{"id": "1501.03755", "contents": "Title: Screen Content Image Segmentation Using Least Absolute Deviation Fitting Abstract: We propose an algorithm for separating the foreground (mainly text and line\ngraphics) from the smoothly varying background in screen content images. The\nproposed method is designed based on the assumption that the background part of\nthe image is smoothly varying and can be represented by a linear combination of\na few smoothly varying basis functions, while the foreground text and graphics\ncreate sharp discontinuity and cannot be modeled by this smooth representation.\nThe algorithm separates the background and foreground using a least absolute\ndeviation method to fit the smooth model to the image pixels. This algorithm\nhas been tested on several images from HEVC standard test sequences for screen\ncontent coding, and is shown to have superior performance over other popular\nmethods, such as k-means clustering based segmentation in DjVu and shape\nprimitive extraction and coding (SPEC) algorithm. Such background/foreground\nsegmentation are important pre-processing steps for text extraction and\nseparate coding of background and foreground for compression of screen content\nimages. \n\n"}
{"id": "1501.06297", "contents": "Title: Geodesic convolutional neural networks on Riemannian manifolds Abstract: Feature descriptors play a crucial role in a wide range of geometry analysis\nand processing applications, including shape correspondence, retrieval, and\nsegmentation. In this paper, we introduce Geodesic Convolutional Neural\nNetworks (GCNN), a generalization of the convolutional networks (CNN) paradigm\nto non-Euclidean manifolds. Our construction is based on a local geodesic\nsystem of polar coordinates to extract \"patches\", which are then passed through\na cascade of filters and linear and non-linear operators. The coefficients of\nthe filters and linear combination weights are optimization variables that are\nlearned to minimize a task-specific cost function. We use GCNN to learn\ninvariant shape features, allowing to achieve state-of-the-art performance in\nproblems such as shape description, retrieval, and correspondence. \n\n"}
{"id": "1501.07492", "contents": "Title: Weakly Supervised Learning for Salient Object Detection Abstract: Recent advances in supervised salient object detection has resulted in\nsignificant performance on benchmark datasets. Training such models, however,\nrequires expensive pixel-wise annotations of salient objects. Moreover, many\nexisting salient object detection models assume that at least one salient\nobject exists in the input image. Such an assumption often leads to less\nappealing saliency maps on the background images, which contain no salient\nobject at all. To avoid the requirement of expensive pixel-wise salient region\nannotations, in this paper, we study weakly supervised learning approaches for\nsalient object detection. Given a set of background images and salient object\nimages, we propose a solution toward jointly addressing the salient object\nexistence and detection tasks. We adopt the latent SVM framework and formulate\nthe two problems together in a single integrated objective function: saliency\nlabels of superpixels are modeled as hidden variables and involved in a\nclassification term conditioned to the salient object existence variable, which\nin turn depends on both global image and regional saliency features and\nsaliency label assignment. Experimental results on benchmark datasets validate\nthe effectiveness of our proposed approach. \n\n"}
{"id": "1502.02367", "contents": "Title: Gated Feedback Recurrent Neural Networks Abstract: In this work, we propose a novel recurrent neural network (RNN) architecture.\nThe proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of\nstacking multiple recurrent layers by allowing and controlling signals flowing\nfrom upper recurrent layers to lower layers using a global gating unit for each\npair of layers. The recurrent signals exchanged between layers are gated\nadaptively based on the previous hidden states and the current input. We\nevaluated the proposed GF-RNN with different types of recurrent units, such as\ntanh, long short-term memory and gated recurrent units, on the tasks of\ncharacter-level language modeling and Python program evaluation. Our empirical\nevaluation of different RNN units, revealed that in both tasks, the GF-RNN\noutperforms the conventional approaches to build deep stacked RNNs. We suggest\nthat the improvement arises because the GF-RNN can adaptively assign different\nlayers to different timescales and layer-to-layer interactions (including the\ntop-down ones which are not usually present in a stacked RNN) by learning to\ngate these interactions. \n\n"}
{"id": "1502.07816", "contents": "Title: Puzzle Imaging: Using Large-scale Dimensionality Reduction Algorithms\n  for Localization Abstract: Current high-resolution imaging techniques require an intact sample that\npreserves spatial relationships. We here present a novel approach, \"puzzle\nimaging,\" that allows imaging a spatially scrambled sample. This technique\ntakes many spatially disordered samples, and then pieces them back together\nusing local properties embedded within the sample. We show that puzzle imaging\ncan efficiently produce high-resolution images using dimensionality reduction\nalgorithms. We demonstrate the theoretical capabilities of puzzle imaging in\nthree biological scenarios, showing that (1) relatively precise 3-dimensional\nbrain imaging is possible; (2) the physical structure of a neural network can\noften be recovered based only on the neural connectivity matrix; and (3) a\nchemical map could be reproduced using bacteria with chemosensitive DNA and\nconjugative transfer. The ability to reconstruct scrambled images promises to\nenable imaging based on DNA sequencing of homogenized tissue samples. \n\n"}
{"id": "1503.01531", "contents": "Title: Spectral Clustering by Ellipsoid and Its Connection to Separable\n  Nonnegative Matrix Factorization Abstract: This paper proposes a variant of the normalized cut algorithm for spectral\nclustering. Although the normalized cut algorithm applies the K-means algorithm\nto the eigenvectors of a normalized graph Laplacian for finding clusters, our\nalgorithm instead uses a minimum volume enclosing ellipsoid for them. We show\nthat the algorithm shares similarity with the ellipsoidal rounding algorithm\nfor separable nonnegative matrix factorization. Our theoretical insight implies\nthat the algorithm can serve as a bridge between spectral clustering and\nseparable NMF. The K-means algorithm has the issues in that the choice of\ninitial points affects the construction of clusters and certain choices result\nin poor clustering performance. The normalized cut algorithm inherits these\nissues since K-means is incorporated in it, whereas the algorithm proposed here\ndoes not. An empirical study is presented to examine the performance of the\nalgorithm. \n\n"}
{"id": "1503.02351", "contents": "Title: Fully Connected Deep Structured Networks Abstract: Convolutional neural networks with many layers have recently been shown to\nachieve excellent results on many high-level tasks such as image\nclassification, object detection and more recently also semantic segmentation.\nParticularly for semantic segmentation, a two-stage procedure is often\nemployed. Hereby, convolutional networks are trained to provide good local\npixel-wise features for the second step being traditionally a more global\ngraphical model. In this work we unify this two-stage process into a single\njoint training algorithm. We demonstrate our method on the semantic image\nsegmentation task and show encouraging results on the challenging PASCAL VOC\n2012 dataset. \n\n"}
{"id": "1503.04400", "contents": "Title: Separable and non-separable data representation for pattern\n  discrimination Abstract: We provide a complete work-flow, based on the language of quantum information\ntheory, suitable for processing data for the purpose of pattern recognition.\nThe main advantage of the introduced scheme is that it can be easily\nimplemented and applied to process real-world data using modest computation\nresources. At the same time it can be used to investigate the difference in the\npattern recognition resulting from the utilization of the tensor product\nstructure of the space of quantum states. We illustrate this difference by\nproviding a simple example based on the classification of 2D data. \n\n"}
{"id": "1503.07077", "contents": "Title: Rotation-invariant convolutional neural networks for galaxy morphology\n  prediction Abstract: Measuring the morphological parameters of galaxies is a key requirement for\nstudying their formation and evolution. Surveys such as the Sloan Digital Sky\nSurvey (SDSS) have resulted in the availability of very large collections of\nimages, which have permitted population-wide analyses of galaxy morphology.\nMorphological analysis has traditionally been carried out mostly via visual\ninspection by trained experts, which is time-consuming and does not scale to\nlarge ($\\gtrsim10^4$) numbers of images.\n  Although attempts have been made to build automated classification systems,\nthese have not been able to achieve the desired level of accuracy. The Galaxy\nZoo project successfully applied a crowdsourcing strategy, inviting online\nusers to classify images by answering a series of questions. Unfortunately,\neven this approach does not scale well enough to keep up with the increasing\navailability of galaxy images.\n  We present a deep neural network model for galaxy morphology classification\nwhich exploits translational and rotational symmetry. It was developed in the\ncontext of the Galaxy Challenge, an international competition to build the best\nmodel for morphology classification based on annotated images from the Galaxy\nZoo project.\n  For images with high agreement among the Galaxy Zoo participants, our model\nis able to reproduce their consensus with near-perfect accuracy ($> 99\\%$) for\nmost questions. Confident model predictions are highly accurate, which makes\nthe model suitable for filtering large collections of images and forwarding\nchallenging images to experts for manual annotation. This approach greatly\nreduces the experts' workload without affecting accuracy. The application of\nthese algorithms to larger sets of training data will be critical for analysing\nresults from future surveys such as the LSST. \n\n"}
{"id": "1504.01515", "contents": "Title: Simultaneously sparse and low-rank abundance matrix estimation for\n  hyperspectral image unmixing Abstract: In a plethora of applications dealing with inverse problems, e.g. in image\nprocessing, social networks, compressive sensing, biological data processing\netc., the signal of interest is known to be structured in several ways at the\nsame time. This premise has recently guided the research to the innovative and\nmeaningful idea of imposing multiple constraints on the parameters involved in\nthe problem under study. For instance, when dealing with problems whose\nparameters form sparse and low-rank matrices, the adoption of suitably combined\nconstraints imposing sparsity and low-rankness, is expected to yield\nsubstantially enhanced estimation results. In this paper, we address the\nspectral unmixing problem in hyperspectral images. Specifically, two novel\nunmixing algorithms are introduced, in an attempt to exploit both spatial\ncorrelation and sparse representation of pixels lying in homogeneous regions of\nhyperspectral images. To this end, a novel convex mixed penalty term is first\ndefined consisting of the sum of the weighted $\\ell_1$ and the weighted nuclear\nnorm of the abundance matrix corresponding to a small area of the image\ndetermined by a sliding square window. This penalty term is then used to\nregularize a conventional quadratic cost function and impose simultaneously\nsparsity and row-rankness on the abundance matrix. The resulting regularized\ncost function is minimized by a) an incremental proximal sparse and low-rank\nunmixing algorithm and b) an algorithm based on the alternating minimization\nmethod of multipliers (ADMM). The effectiveness of the proposed algorithms is\nillustrated in experiments conducted both on simulated and real data. \n\n"}
{"id": "1504.01989", "contents": "Title: Pixel-wise Deep Learning for Contour Detection Abstract: We address the problem of contour detection via per-pixel classifications of\nedge point. To facilitate the process, the proposed approach leverages with\nDenseNet, an efficient implementation of multiscale convolutional neural\nnetworks (CNNs), to extract an informative feature vector for each pixel and\nuses an SVM classifier to accomplish contour detection. In the experiment of\ncontour detection, we look into the effectiveness of combining per-pixel\nfeatures from different CNN layers and verify their performance on BSDS500. \n\n"}
{"id": "1504.05035", "contents": "Title: F-SVM: Combination of Feature Transformation and SVM Learning via Convex\n  Relaxation Abstract: The generalization error bound of support vector machine (SVM) depends on the\nratio of radius and margin, while standard SVM only considers the maximization\nof the margin but ignores the minimization of the radius. Several approaches\nhave been proposed to integrate radius and margin for joint learning of feature\ntransformation and SVM classifier. However, most of them either require the\nform of the transformation matrix to be diagonal, or are non-convex and\ncomputationally expensive. In this paper, we suggest a novel approximation for\nthe radius of minimum enclosing ball (MEB) in feature space, and then propose a\nconvex radius-margin based SVM model for joint learning of feature\ntransformation and SVM classifier, i.e., F-SVM. An alternating minimization\nmethod is adopted to solve the F-SVM model, where the feature transformation is\nupdatedvia gradient descent and the classifier is updated by employing the\nexisting SVM solver. By incorporating with kernel principal component analysis,\nF-SVM is further extended for joint learning of nonlinear transformation and\nclassifier. Experimental results on the UCI machine learning datasets and the\nLFW face datasets show that F-SVM outperforms the standard SVM and the existing\nradius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\\mu}. \n\n"}
{"id": "1504.06785", "contents": "Title: Complete Dictionary Recovery over the Sphere Abstract: We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb R^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to the theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals, and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$. In contrast, prior\nresults based on efficient algorithms provide recovery guarantees when $\\mathbf\nX_0$ has only $O(n^{1-\\delta})$ nonzeros per column for any constant $\\delta\n\\in (0, 1)$.\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint, and hence is naturally\nphrased in the language of manifold optimization. To show this apparently hard\nproblem is tractable, we first provide a geometric characterization of the\nhigh-dimensional objective landscape, which shows that with high probability\nthere are no \"spurious\" local minima. This particular geometric structure\nallows us to design a Riemannian trust region algorithm over the sphere that\nprovably converges to one local minimizer with an arbitrary initialization,\ndespite the presence of saddle points. The geometric approach we develop here\nmay also shed light on other problems arising from nonconvex recovery of\nstructured signals. \n\n"}
{"id": "1504.07918", "contents": "Title: Robust hyperspectral image classification with rejection fields Abstract: In this paper we present a novel method for robust hyperspectral image\nclassification using context and rejection. Hyperspectral image classification\nis generally an ill-posed image problem where pixels may belong to unknown\nclasses, and obtaining representative and complete training sets is costly.\nFurthermore, the need for high classification accuracies is frequently greater\nthan the need to classify the entire image.\n  We approach this problem with a robust classification method that combines\nclassification with context with classification with rejection. A rejection\nfield that will guide the rejection is derived from the classification with\ncontextual information obtained by using the SegSALSA algorithm. We validate\nour method in real hyperspectral data and show that the performance gains\nobtained from the rejection fields are equivalent to an increase the dimension\nof the training sets. \n\n"}
{"id": "1504.08291", "contents": "Title: Deep Neural Networks with Random Gaussian Weights: A Universal\n  Classification Strategy? Abstract: Three important properties of a classification machinery are: (i) the system\npreserves the core information of the input data; (ii) the training examples\nconvey information about unseen data; and (iii) the system is able to treat\ndifferently points from different classes. In this work we show that these\nfundamental properties are satisfied by the architecture of deep neural\nnetworks. We formally prove that these networks with random Gaussian weights\nperform a distance-preserving embedding of the data, with a special treatment\nfor in-class and out-of-class data. Similar points at the input of the network\nare likely to have a similar output. The theoretical analysis of deep networks\nhere presented exploits tools used in the compressed sensing and dictionary\nlearning literature, thereby making a formal connection between these important\ntopics. The derived results allow drawing conclusions on the metric learning\nproperties of the network and their relation to its structure, as well as\nproviding bounds on the required size of the training set such that the\ntraining examples would represent faithfully the unseen data. The results are\nvalidated with state-of-the-art trained networks. \n\n"}
{"id": "1505.00468", "contents": "Title: VQA: Visual Question Answering Abstract: We propose the task of free-form and open-ended Visual Question Answering\n(VQA). Given an image and a natural language question about the image, the task\nis to provide an accurate natural language answer. Mirroring real-world\nscenarios, such as helping the visually impaired, both the questions and\nanswers are open-ended. Visual questions selectively target different areas of\nan image, including background details and underlying context. As a result, a\nsystem that succeeds at VQA typically needs a more detailed understanding of\nthe image and complex reasoning than a system producing generic image captions.\nMoreover, VQA is amenable to automatic evaluation, since many open-ended\nanswers contain only a few words or a closed set of answers that can be\nprovided in a multiple-choice format. We provide a dataset containing ~0.25M\nimages, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the\ninformation it provides. Numerous baselines and methods for VQA are provided\nand compared with human performance. Our VQA demo is available on CloudCV\n(http://cloudcv.org/vqa). \n\n"}
{"id": "1505.00670", "contents": "Title: Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database\n  for Automated Image Interpretation Abstract: Despite tremendous progress in computer vision, there has not been an attempt\nfor machine learning on very large-scale medical image databases. We present an\ninterleaved text/image deep learning system to extract and mine the semantic\ninteractions of radiology images and reports from a national research\nhospital's Picture Archiving and Communication System. With natural language\nprocessing, we mine a collection of representative ~216K two-dimensional key\nimages selected by clinicians for diagnostic reference, and match the images\nwith their descriptions in an automated manner. Our system interleaves between\nunsupervised learning and supervised learning on document- and sentence-level\ntext collections, to generate semantic labels and to predict them given an\nimage. Given an image of a patient scan, semantic topics in radiology levels\nare predicted, and associated key-words are generated. Also, a number of\nfrequent disease types are detected as present or absent, to provide more\nspecific interpretation of a patient scan. This shows the potential of\nlarge-scale learning and prediction in electronic patient records available in\nmost modern clinical institutions. \n\n"}
{"id": "1505.02108", "contents": "Title: MegaFace: A Million Faces for Recognition at Scale Abstract: Recent face recognition experiments on the LFW benchmark show that face\nrecognition is performing stunningly well, surpassing human recognition rates.\nIn this paper, we study face recognition at scale. Specifically, we have\ncollected from Flickr a \\textbf{Million} faces and evaluated state of the art\nface recognition algorithms on this dataset. We found that the performance of\nalgorithms varies--while all perform great on LFW, once evaluated at scale\nrecognition rates drop drastically for most algorithms. Interestingly, deep\nlearning based approach by \\cite{schroff2015facenet} performs much better, but\nstill gets less robust at scale. We consider both verification and\nidentification problems, and evaluate how pose affects recognition at scale.\nMoreover, we ran an extensive human study on Mechanical Turk to evaluate human\nrecognition at scale, and report results. All the photos are creative commons\nphotos and is released at \\small{\\url{http://megaface.cs.washington.edu/}} for\nresearch and further experiments. \n\n"}
{"id": "1505.02438", "contents": "Title: Deep Learning for Semantic Part Segmentation with High-Level Guidance Abstract: In this work we address the task of segmenting an object into its parts, or\nsemantic part segmentation. We start by adapting a state-of-the-art semantic\nsegmentation system to this task, and show that a combination of a\nfully-convolutional Deep CNN system coupled with Dense CRF labelling provides\nexcellent results for a broad range of object categories. Still, this approach\nremains agnostic to high-level constraints between object parts. We introduce\nsuch prior information by means of the Restricted Boltzmann Machine, adapted to\nour task and train our model in an discriminative fashion, as a hidden CRF,\ndemonstrating that prior information can yield additional improvements. We also\ninvestigate the performance of our approach ``in the wild'', without\ninformation concerning the objects' bounding boxes, using an object detector to\nguide a multi-scale segmentation scheme. We evaluate the performance of our\napproach on the Penn-Fudan and LFW datasets for the tasks of pedestrian parsing\nand face labelling respectively. We show superior performance with respect to\ncompetitive methods that have been extensively engineered on these benchmarks,\nas well as realistic qualitative results on part segmentation, even for\noccluded or deformable objects. We also provide quantitative and extensive\nqualitative results on three classes from the PASCAL Parts dataset. Finally, we\nshow that our multi-scale segmentation scheme can boost accuracy, recovering\nsegmentations for finer parts. \n\n"}
{"id": "1505.03046", "contents": "Title: Improving Computer-aided Detection using Convolutional Neural Networks\n  and Random View Aggregation Abstract: Automated computer-aided detection (CADe) in medical imaging has been an\nimportant tool in clinical practice and research. State-of-the-art methods\noften show high sensitivities but at the cost of high false-positives (FP) per\npatient rates. We design a two-tiered coarse-to-fine cascade framework that\nfirst operates a candidate generation system at sensitivities of $\\sim$100% but\nat high FP levels. By leveraging existing CAD systems, coordinates of regions\nor volumes of interest (ROI or VOI) for lesion candidates are generated in this\nstep and function as input for a second tier, which is our focus in this study.\nIn this second stage, we generate $N$ 2D (two-dimensional) or 2.5D views via\nsampling through scale transformations, random translations and rotations with\nrespect to each ROI's centroid coordinates. These random views are used to\ntrain deep convolutional neural network (ConvNet) classifiers. In testing, the\ntrained ConvNets are employed to assign class (e.g., lesion, pathology)\nprobabilities for a new set of $N$ random views that are then averaged at each\nROI to compute a final per-candidate classification probability. This second\ntier behaves as a highly selective process to reject difficult false positives\nwhile preserving high sensitivities. The methods are evaluated on three\ndifferent data sets with different numbers of patients: 59 patients for\nsclerotic metastases detection, 176 patients for lymph node detection, and\n1,186 patients for colonic polyp detection. Experimental results show the\nability of ConvNets to generalize well to different medical imaging CADe\napplications and scale elegantly to various data sets. Our proposed methods\nimprove CADe performance markedly in all cases. CADe sensitivities improved\nfrom 57% to 70%, from 43% to 77% and from 58% to 75% at 3 FPs per patient for\nsclerotic metastases, lymph nodes and colonic polyps, respectively. \n\n"}
{"id": "1505.04424", "contents": "Title: Improved Microaneurysm Detection using Deep Neural Networks Abstract: In this work, we propose a novel microaneurysm (MA) detection for early\ndiabetic retinopathy screening using color fundus images. Since MA usually the\nfirst lesions to appear as an indicator of diabetic retinopathy, accurate\ndetection of MA is necessary for treatment. Each pixel of the image is\nclassified as either MA or non-MA using a deep neural network with dropout\ntraining procedure using maxout activation function. No preprocessing step or\nmanual feature extraction is required. Substantial improvements over standard\nMA detection method based on the pipeline of preprocessing, feature extraction,\nclassification followed by post processing is achieved. The presented method is\nevaluated in publicly available Retinopathy Online Challenge (ROC) and\nDiaretdb1v2 database and achieved state-of-the-art accuracy. \n\n"}
{"id": "1505.05424", "contents": "Title: Weight Uncertainty in Neural Networks Abstract: We introduce a new, efficient, principled and backpropagation-compatible\nalgorithm for learning a probability distribution on the weights of a neural\nnetwork, called Bayes by Backprop. It regularises the weights by minimising a\ncompression cost, known as the variational free energy or the expected lower\nbound on the marginal likelihood. We show that this principled kind of\nregularisation yields comparable performance to dropout on MNIST\nclassification. We then demonstrate how the learnt uncertainty in the weights\ncan be used to improve generalisation in non-linear regression problems, and\nhow this weight uncertainty can be used to drive the exploration-exploitation\ntrade-off in reinforcement learning. \n\n"}
{"id": "1505.05643", "contents": "Title: Object Modelling with a Handheld RGB-D Camera Abstract: This work presents a flexible system to reconstruct 3D models of objects\ncaptured with an RGB-D sensor. A major advantage of the method is that our\nreconstruction pipeline allows the user to acquire a full 3D model of the\nobject. This is achieved by acquiring several partial 3D models in different\nsessions that are automatically merged together to reconstruct a full model. In\naddition, the 3D models acquired by our system can be directly used by\nstate-of-the-art object instance recognition and object tracking modules,\nproviding object-perception capabilities for different applications, such as\nhuman-object interaction analysis or robot grasping. The system does not impose\nconstraints in the appearance of objects (textured, untextured) nor in the\nmodelling setup (moving camera with static object or a turn-table setup). The\nproposed reconstruction system has been used to model a large number of objects\nresulting in metrically accurate and visually appealing 3D models. \n\n"}
{"id": "1505.06237", "contents": "Title: Tunnel Surface 3D Reconstruction from Unoriented Image Sequences Abstract: The 3D documentation of the tunnel surface during construction requires fast\nand robust measurement systems. In the solution proposed in this paper, during\ntunnel advance a single camera is taking pictures of the tunnel surface from\nseveral positions. The recorded images are automatically processed to gain a 3D\ntunnel surface model. Image acquisition is realized by the\ntunneling/advance/driving personnel close to the tunnel face (= the front end\nof the advance). Based on the following fully automatic analysis/evaluation, a\ndecision on the quality of the outbreak can be made within a few minutes. This\npaper describes the image recording system and conditions as well as the\nstereo-photogrammetry based workflow for the continuously merged dense 3D\nreconstruction of the entire advance region. Geo-reference is realized by means\nof signalized targets that are automatically detected in the images. We report\non the results of recent testing under real construction conditions, and\nconclude with prospects for further development in terms of on-site\nperformance. \n\n"}
{"id": "1505.06798", "contents": "Title: Accelerating Very Deep Convolutional Networks for Classification and\n  Detection Abstract: This paper aims to accelerate the test-time computation of convolutional\nneural networks (CNNs), especially very deep CNNs that have substantially\nimpacted the computer vision community. Unlike previous methods that are\ndesigned for approximating linear filters or linear responses, our method takes\nthe nonlinear units into account. We develop an effective solution to the\nresulting nonlinear optimization problem without the need of stochastic\ngradient descent (SGD). More importantly, while previous methods mainly focus\non optimizing one or two layers, our nonlinear method enables an asymmetric\nreconstruction that reduces the rapidly accumulated error when multiple (e.g.,\n>=10) layers are approximated. For the widely used very deep VGG-16 model, our\nmethod achieves a whole-model speedup of 4x with merely a 0.3% increase of\ntop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also\nshows a graceful accuracy degradation for object detection when plugged into\nthe Fast R-CNN detector. \n\n"}
{"id": "1505.08082", "contents": "Title: Learning to count with deep object features Abstract: Learning to count is a learning strategy that has been recently proposed in\nthe literature for dealing with problems where estimating the number of object\ninstances in a scene is the final objective. In this framework, the task of\nlearning to detect and localize individual object instances is seen as a harder\ntask that can be evaded by casting the problem as that of computing a\nregression value from hand-crafted image features. In this paper we explore the\nfeatures that are learned when training a counting convolutional neural network\nin order to understand their underlying representation. To this end we define a\ncounting problem for MNIST data and show that the internal representation of\nthe network is able to classify digits in spite of the fact that no direct\nsupervision was provided for them during training. We also present preliminary\nresults about a deep network that is able to count the number of pedestrians in\na scene. \n\n"}
{"id": "1506.00333", "contents": "Title: Learning to Answer Questions From Image Using Convolutional Neural\n  Network Abstract: In this paper, we propose to employ the convolutional neural network (CNN)\nfor the image question answering (QA). Our proposed CNN provides an end-to-end\nframework with convolutional architectures for learning not only the image and\nquestion representations, but also their inter-modal interactions to produce\nthe answer. More specifically, our model consists of three CNNs: one image CNN\nto encode the image content, one sentence CNN to compose the words of the\nquestion, and one multimodal convolution layer to learn their joint\nrepresentation for the classification in the space of candidate answer words.\nWe demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA\ndatasets, which are two benchmark datasets for the image QA, with the\nperformances significantly outperforming the state-of-the-art. \n\n"}
{"id": "1506.02158", "contents": "Title: Bayesian Convolutional Neural Networks with Bernoulli Approximate\n  Variational Inference Abstract: Convolutional neural networks (CNNs) work well on large datasets. But\nlabelled data is hard to collect, and in some applications larger amounts of\ndata are not available. The problem then is how to use CNNs with small data --\nas CNNs overfit quickly. We present an efficient Bayesian CNN, offering better\nrobustness to over-fitting on small data than traditional approaches. This is\nby placing a probability distribution over the CNN's kernels. We approximate\nour model's intractable posterior with Bernoulli variational distributions,\nrequiring no additional model parameters.\n  On the theoretical side, we cast dropout network training as approximate\ninference in Bayesian neural networks. This allows us to implement our model\nusing existing tools in deep learning with no increase in time complexity,\nwhile highlighting a negative result in the field. We show a considerable\nimprovement in classification accuracy compared to standard techniques and\nimprove on published state-of-the-art results for CIFAR-10. \n\n"}
{"id": "1506.02753", "contents": "Title: Inverting Visual Representations with Convolutional Networks Abstract: Feature representations, both hand-designed and learned ones, are often hard\nto analyze and interpret, even when they are extracted from visual data. We\npropose a new approach to study image representations by inverting them with an\nup-convolutional neural network. We apply the method to shallow representations\n(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our\napproach provides significantly better reconstructions than existing methods,\nrevealing that there is surprisingly rich information contained in these\nfeatures. Inverting a deep network trained on ImageNet provides several\ninsights into the properties of the feature representation learned by the\nnetwork. Most strikingly, the colors and the rough contours of an image can be\nreconstructed from activations in higher network layers and even from the\npredicted class probabilities. \n\n"}
{"id": "1506.06272", "contents": "Title: Aligning where to see and what to tell: image caption with region-based\n  attention and scene factorization Abstract: Recent progress on automatic generation of image captions has shown that it\nis possible to describe the most salient information conveyed by images with\naccurate and meaningful sentences. In this paper, we propose an image caption\nsystem that exploits the parallel structures between images and sentences. In\nour model, the process of generating the next word, given the previously\ngenerated ones, is aligned with the visual perception experience where the\nattention shifting among the visual regions imposes a thread of visual\nordering. This alignment characterizes the flow of \"abstract meaning\", encoding\nwhat is semantically shared by both the visual scene and the text description.\nOur system also makes another novel modeling contribution by introducing\nscene-specific contexts that capture higher-level semantic information encoded\nin an image. The contexts adapt language models for word generation to specific\nscene types. We benchmark our system and contrast to published results on\nseveral popular datasets. We show that using either region-based attention or\nscene-specific contexts improves systems without those components. Furthermore,\ncombining these two modeling ingredients attains the state-of-the-art\nperformance. \n\n"}
{"id": "1506.07704", "contents": "Title: AttentionNet: Aggregating Weak Directions for Accurate Object Detection Abstract: We present a novel detection method using a deep convolutional neural network\n(CNN), named AttentionNet. We cast an object detection problem as an iterative\nclassification problem, which is the most suitable form of a CNN. AttentionNet\nprovides quantized weak directions pointing a target object and the ensemble of\niterative predictions from AttentionNet converges to an accurate object\nboundary box. Since AttentionNet is a unified network for object detection, it\ndetects objects without any separated models from the object proposal to the\npost bounding-box regression. We evaluate AttentionNet by a human detection\ntask and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC\n2007/2012 with an 8-layered architecture only. \n\n"}
{"id": "1506.08006", "contents": "Title: Spectral Collaborative Representation based Classification for Hand\n  Gestures recognition on Electromyography Signals Abstract: In this study, we introduce a novel variant and application of the\nCollaborative Representation based Classification in spectral domain for\nrecognition of the hand gestures using the raw surface Electromyography\nsignals. The intuitive use of spectral features are explained via circulant\nmatrices. The proposed Spectral Collaborative Representation based\nClassification (SCRC) is able to recognize gestures with higher levels of\naccuracy for a fairly rich gesture set. The worst recognition result which is\nthe best in the literature is obtained as 97.3\\% among the four sets of the\nexperiments for each hand gestures. The recognition results are reported with a\nsubstantial number of experiments and labeling computation. \n\n"}
{"id": "1507.01578", "contents": "Title: Beyond Semantic Image Segmentation : Exploring Efficient Inference in\n  Video Abstract: We explore the efficiency of the CRF inference module beyond image level\nsemantic segmentation. The key idea is to combine the best of two worlds of\nsemantic co-labeling and exploiting more expressive models. Similar to\n[Alvarez14] our formulation enables us perform inference over ten thousand\nimages within seconds. On the other hand, it can handle higher-order clique\npotentials similar to [vineet2014] in terms of region-level label consistency\nand context in terms of co-occurrences. We follow the mean-field updates for\nhigher order potentials similar to [vineet2014] and extend the spatial\nsmoothness and appearance kernels [DenseCRF13] to address video data inspired\nby [Alvarez14]; thus making the system amenable to perform video semantic\nsegmentation most effectively. \n\n"}
{"id": "1507.02189", "contents": "Title: Intersecting Faces: Non-negative Matrix Factorization With New\n  Guarantees Abstract: Non-negative matrix factorization (NMF) is a natural model of admixture and\nis widely used in science and engineering. A plethora of algorithms have been\ndeveloped to tackle NMF, but due to the non-convex nature of the problem, there\nis little guarantee on how well these methods work. Recently a surge of\nresearch have focused on a very restricted class of NMFs, called separable NMF,\nwhere provably correct algorithms have been developed. In this paper, we\npropose the notion of subset-separable NMF, which substantially generalizes the\nproperty of separability. We show that subset-separability is a natural\nnecessary condition for the factorization to be unique or to have minimum\nvolume. We developed the Face-Intersect algorithm which provably and\nefficiently solves subset-separable NMF under natural conditions, and we prove\nthat our algorithm is robust to small noise. We explored the performance of\nFace-Intersect on simulations and discuss settings where it empirically\noutperformed the state-of-art methods. Our work is a step towards finding\nprovably correct algorithms that solve large classes of NMF problems. \n\n"}
{"id": "1507.04125", "contents": "Title: Untangling AdaBoost-based Cost-Sensitive Classification. Part I:\n  Theoretical Perspective Abstract: Boosting algorithms have been widely used to tackle a plethora of problems.\nIn the last few years, a lot of approaches have been proposed to provide\nstandard AdaBoost with cost-sensitive capabilities, each with a different\nfocus. However, for the researcher, these algorithms shape a tangled set with\ndiffuse differences and properties, lacking a unifying analysis to jointly\ncompare, classify, evaluate and discuss those approaches on a common basis. In\nthis series of two papers we aim to revisit the various proposals, both from\ntheoretical (Part I) and practical (Part II) perspectives, in order to analyze\ntheir specific properties and behavior, with the final goal of identifying the\nalgorithm providing the best and soundest results. \n\n"}
{"id": "1507.05348", "contents": "Title: Learning Complexity-Aware Cascades for Deep Pedestrian Detection Abstract: The design of complexity-aware cascaded detectors, combining features of very\ndifferent complexities, is considered. A new cascade design procedure is\nintroduced, by formulating cascade learning as the Lagrangian optimization of a\nrisk that accounts for both accuracy and complexity. A boosting algorithm,\ndenoted as complexity aware cascade training (CompACT), is then derived to\nsolve this optimization. CompACT cascades are shown to seek an optimal\ntrade-off between accuracy and complexity by pushing features of higher\ncomplexity to the later cascade stages, where only a few difficult candidate\npatches remain to be classified. This enables the use of features of vastly\ndifferent complexities in a single detector. In result, the feature pool can be\nexpanded to features previously impractical for cascade design, such as the\nresponses of a deep convolutional neural network (CNN). This is demonstrated\nthrough the design of a pedestrian detector with a pool of features whose\ncomplexities span orders of magnitude. The resulting cascade generalizes the\ncombination of a CNN with an object proposal mechanism: rather than a\npre-processing stage, CompACT cascades seamlessly integrate CNNs in their\nstages. This enables state of the art performance on the Caltech and KITTI\ndatasets, at fairly fast speeds. \n\n"}
{"id": "1507.05699", "contents": "Title: Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians Abstract: Convolutional neural nets (CNNs) have demonstrated remarkable performance in\nrecent history. Such approaches tend to work in a unidirectional bottom-up\nfeed-forward fashion. However, practical experience and biological evidence\ntells us that feedback plays a crucial role, particularly for detailed spatial\nunderstanding tasks. This work explores bidirectional architectures that also\nreason with top-down feedback: neural units are influenced by both lower and\nhigher-level units.\n  We do so by treating units as rectified latent variables in a quadratic\nenergy function, which can be seen as a hierarchical Rectified Gaussian model\n(RGs). We show that RGs can be optimized with a quadratic program (QP), that\ncan in turn be optimized with a recurrent neural network (with rectified linear\nunits). This allows RGs to be trained with GPU-optimized gradient descent. From\na theoretical perspective, RGs help establish a connection between CNNs and\nhierarchical probabilistic models. From a practical perspective, RGs are well\nsuited for detailed spatial tasks that can benefit from top-down reasoning. We\nillustrate them on the challenging task of keypoint localization under\nocclusions, where local bottom-up evidence may be misleading. We demonstrate\nstate-of-the-art results on challenging benchmarks. \n\n"}
{"id": "1507.06149", "contents": "Title: Data-free parameter pruning for Deep Neural Networks Abstract: Deep Neural nets (NNs) with millions of parameters are at the heart of many\nstate-of-the-art computer vision systems today. However, recent works have\nshown that much smaller models can achieve similar levels of performance. In\nthis work, we address the problem of pruning parameters in a trained NN model.\nInstead of removing individual weights one at a time as done in previous works,\nwe remove one neuron at a time. We show how similar neurons are redundant, and\npropose a systematic way to remove them. Our experiments in pruning the densely\nconnected layers show that we can remove upto 85\\% of the total parameters in\nan MNIST-trained network, and about 35\\% for AlexNet without significantly\naffecting performance. Our method can be applied on top of most networks with a\nfully connected layer to give a smaller network. \n\n"}
{"id": "1507.06535", "contents": "Title: Manitest: Are classifiers really invariant? Abstract: Invariance to geometric transformations is a highly desirable property of\nautomatic classifiers in many image recognition tasks. Nevertheless, it is\nunclear to which extent state-of-the-art classifiers are invariant to basic\ntransformations such as rotations and translations. This is mainly due to the\nlack of general methods that properly measure such an invariance. In this\npaper, we propose a rigorous and systematic approach for quantifying the\ninvariance to geometric transformations of any classifier. Our key idea is to\ncast the problem of assessing a classifier's invariance as the computation of\ngeodesics along the manifold of transformed images. We propose the Manitest\nmethod, built on the efficient Fast Marching algorithm to compute the\ninvariance of classifiers. Our new method quantifies in particular the\nimportance of data augmentation for learning invariance from data, and the\nincreased invariance of convolutional neural networks with depth. We foresee\nthat the proposed generic tool for measuring invariance to a large class of\ngeometric transformations and arbitrary classifiers will have many applications\nfor evaluating and comparing classifiers based on their invariance, and help\nimproving the invariance of existing classifiers. \n\n"}
{"id": "1508.00330", "contents": "Title: On the Importance of Normalisation Layers in Deep Learning with\n  Piecewise Linear Activation Units Abstract: Deep feedforward neural networks with piecewise linear activations are\ncurrently producing the state-of-the-art results in several public datasets.\nThe combination of deep learning models and piecewise linear activation\nfunctions allows for the estimation of exponentially complex functions with the\nuse of a large number of subnetworks specialized in the classification of\nsimilar input examples. During the training process, these subnetworks avoid\noverfitting with an implicit regularization scheme based on the fact that they\nmust share their parameters with other subnetworks. Using this framework, we\nhave made an empirical observation that can improve even more the performance\nof such models. We notice that these models assume a balanced initial\ndistribution of data points with respect to the domain of the piecewise linear\nactivation function. If that assumption is violated, then the piecewise linear\nactivation units can degenerate into purely linear activation units, which can\nresult in a significant reduction of their capacity to learn complex functions.\nFurthermore, as the number of model layers increases, this unbalanced initial\ndistribution makes the model ill-conditioned. Therefore, we propose the\nintroduction of batch normalisation units into deep feedforward neural networks\nwith piecewise linear activations, which drives a more balanced use of these\nactivation units, where each region of the activation function is trained with\na relatively large proportion of training samples. Also, this batch\nnormalisation promotes the pre-conditioning of very deep learning models. We\nshow that by introducing maxout and batch normalisation units to the network in\nnetwork model results in a model that produces classification results that are\nbetter than or comparable to the current state of the art in CIFAR-10,\nCIFAR-100, MNIST, and SVHN datasets. \n\n"}
{"id": "1508.01746", "contents": "Title: Using Deep Learning for Detecting Spoofing Attacks on Speech Signals Abstract: It is well known that speaker verification systems are subject to spoofing\nattacks. The Automatic Speaker Verification Spoofing and Countermeasures\nChallenge -- ASVSpoof2015 -- provides a standard spoofing database, containing\nattacks based on synthetic speech, along with a protocol for experiments. This\npaper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based\non deep neural networks, working both as a classifier and as a feature\nextraction module for a GMM and a SVM classifier. Results show the validity of\nthis approach, achieving less than 0.5\\% EER for known attacks. \n\n"}
{"id": "1508.02171", "contents": "Title: Automatic Extraction of the Passing Strategies of Soccer Teams Abstract: Technology offers new ways to measure the locations of the players and of the\nball in sports. This translates to the trajectories the ball takes on the field\nas a result of the tactics the team applies. The challenge professionals in\nsoccer are facing is to take the reverse path: given the trajectories of the\nball is it possible to infer the underlying strategy/tactic of a team? We\npropose a method based on Dynamic Time Warping to reveal the tactics of a team\nthrough the analysis of repeating series of events. Based on the analysis of an\nentire season, we derive insights such as passing strategies for maintaining\nball possession or counter attacks, and passing styles with a focus on the team\nor on the capabilities of the individual players. \n\n"}
{"id": "1508.03276", "contents": "Title: Talking about the Moving Image: A Declarative Model for Image Schema\n  Based Embodied Perception Grounding and Language Generation Abstract: We present a general theory and corresponding declarative model for the\nembodied grounding and natural language based analytical summarisation of\ndynamic visuo-spatial imagery. The declarative model ---ecompassing\nspatio-linguistic abstractions, image schemas, and a spatio-temporal feature\nbased language generator--- is modularly implemented within Constraint Logic\nProgramming (CLP). The implemented model is such that primitives of the theory,\ne.g., pertaining to space and motion, image schemata, are available as\nfirst-class objects with `deep semantics' suited for inference and query. We\ndemonstrate the model with select examples broadly motivated by areas such as\nfilm, design, geography, smart environments where analytical natural language\nbased externalisations of the moving image are central from the viewpoint of\nhuman interaction, evidence-based qualitative analysis, and sensemaking.\n  Keywords: moving image, visual semantics and embodiment, visuo-spatial\ncognition and computation, cognitive vision, computational models of narrative,\ndeclarative spatial reasoning \n\n"}
{"id": "1508.05581", "contents": "Title: Learning Sampling Distributions for Efficient Object Detection Abstract: Object detection is an important task in computer vision and learning\nsystems. Multistage particle windows (MPW), proposed by Gualdi et al., is an\nalgorithm of fast and accurate object detection. By sampling particle windows\nfrom a proposal distribution (PD), MPW avoids exhaustively scanning the image.\nDespite its success, it is unknown how to determine the number of stages and\nthe number of particle windows in each stage. Moreover, it has to generate too\nmany particle windows in the initialization step and it redraws unnecessary too\nmany particle windows around object-like regions. In this paper, we attempt to\nsolve the problems of MPW. An important fact we used is that there is large\nprobability for a randomly generated particle window not to contain the object\nbecause the object is a sparse event relevant to the huge number of candidate\nwindows. Therefore, we design the proposal distribution so as to efficiently\nreject the huge number of non-object windows. Specifically, we propose the\nconcepts of rejection, acceptance, and ambiguity windows and regions. This\ncontrasts to MPW which utilizes only on region of support. The PD of MPW is\nacceptance-oriented whereas the PD of our method (called iPW) is\nrejection-oriented. Experimental results on human and face detection\ndemonstrate the efficiency and effectiveness of the iPW algorithm. The source\ncode is publicly accessible. \n\n"}
{"id": "1509.01329", "contents": "Title: Semantic Amodal Segmentation Abstract: Common visual recognition tasks such as classification, object detection, and\nsemantic segmentation are rapidly reaching maturity, and given the recent rate\nof progress, it is not unreasonable to conjecture that techniques for many of\nthese problems will approach human levels of performance in the next few years.\nIn this paper we look to the future: what is the next frontier in visual\nrecognition?\n  We offer one possible answer to this question. We propose a detailed image\nannotation that captures information beyond the visible pixels and requires\ncomplex reasoning about full scene structure. Specifically, we create an amodal\nsegmentation of each image: the full extent of each region is marked, not just\nthe visible pixels. Annotators outline and name all salient regions in the\nimage and specify a partial depth order. The result is a rich scene structure,\nincluding visible and occluded portions of each region, figure-ground edge\ninformation, semantic labels, and object overlap.\n  We create two datasets for semantic amodal segmentation. First, we label 500\nimages in the BSDS dataset with multiple annotators per image, allowing us to\nstudy the statistics of human annotations. We show that the proposed full scene\nannotation is surprisingly consistent between annotators, including for regions\nand edges. Second, we annotate 5000 images from COCO. This larger dataset\nallows us to explore a number of algorithmic ideas for amodal segmentation and\ndepth ordering. We introduce novel metrics for these tasks, and along with our\nstrong baselines, define concrete new challenges for the community. \n\n"}
{"id": "1509.02470", "contents": "Title: Deep Attributes from Context-Aware Regional Neural Codes Abstract: Recently, many researches employ middle-layer output of convolutional neural\nnetwork models (CNN) as features for different visual recognition tasks.\nAlthough promising results have been achieved in some empirical studies, such\ntype of representations still suffer from the well-known issue of semantic gap.\nThis paper proposes so-called deep attribute framework to alleviate this issue\nfrom three aspects. First, we introduce object region proposals as intermedia\nto represent target images, and extract features from region proposals. Second,\nwe study aggregating features from different CNN layers for all region\nproposals. The aggregation yields a holistic yet compact representation of\ninput images. Results show that cross-region max-pooling of soft-max layer\noutput outperform all other layers. As soft-max layer directly corresponds to\nsemantic concepts, this representation is named \"deep attributes\". Third, we\nobserve that only a small portion of generated regions by object proposals\nalgorithm are correlated to classification target. Therefore, we introduce\ncontext-aware region refining algorithm to pick out contextual regions and\nbuild context-aware classifiers.\n  We apply the proposed deep attributes framework for various vision tasks.\nExtensive experiments are conducted on standard benchmarks for three visual\nrecognition tasks, i.e., image classification, fine-grained recognition and\nvisual instance retrieval. Results show that deep attribute approaches achieve\nstate-of-the-art results, and outperforms existing peer methods with a\nsignificant margin, even though some benchmarks have little overlap of concepts\nwith the pre-trained CNN models. \n\n"}
{"id": "1509.09089", "contents": "Title: Moving Object Detection in Video Using Saliency Map and Subspace\n  Learning Abstract: Moving object detection is a key to intelligent video analysis. On the one\nhand, what moves is not only interesting objects but also noise and cluttered\nbackground. On the other hand, moving objects without rich texture are prone\nnot to be detected. So there are undesirable false alarms and missed alarms in\nmany algorithms of moving object detection. To reduce the false alarms and\nmissed alarms, in this paper, we propose to incorporate a saliency map into an\nincremental subspace analysis framework where the saliency map makes estimated\nbackground has less chance than foreground (i.e., moving objects) to contain\nsalient objects. The proposed objective function systematically takes account\ninto the properties of sparsity, low-rank, connectivity, and saliency. An\nalternative minimization algorithm is proposed to seek the optimal solutions.\nExperimental results on the Perception Test Images Sequences demonstrate that\nthe proposed method is effective in reducing false alarms and missed alarms. \n\n"}
{"id": "1509.09114", "contents": "Title: Online Object Tracking with Proposal Selection Abstract: Tracking-by-detection approaches are some of the most successful object\ntrackers in recent years. Their success is largely determined by the detector\nmodel they learn initially and then update over time. However, under\nchallenging conditions where an object can undergo transformations, e.g.,\nsevere rotation, these methods are found to be lacking. In this paper, we\naddress this problem by formulating it as a proposal selection task and making\ntwo contributions. The first one is introducing novel proposals estimated from\nthe geometric transformations undergone by the object, and building a rich\ncandidate set for predicting the object location. The second one is devising a\nnovel selection strategy using multiple cues, i.e., detection score and\nedgeness score computed from state-of-the-art object edges and motion\nboundaries. We extensively evaluate our approach on the visual object tracking\n2014 challenge and online tracking benchmark datasets, and show the best\nperformance. \n\n"}
{"id": "1510.00149", "contents": "Title: Deep Compression: Compressing Deep Neural Networks with Pruning, Trained\n  Quantization and Huffman Coding Abstract: Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency. \n\n"}
{"id": "1510.02781", "contents": "Title: Where Is My Puppy? Retrieving Lost Dogs by Facial Features Abstract: A pet that goes missing is among many people's worst fears: a moment of\ndistraction is enough for a dog or a cat wandering off from home. Some measures\nhelp matching lost animals to their owners; but automated visual recognition is\none that - although convenient, highly available, and low-cost - is\nsurprisingly overlooked. In this paper, we inaugurate that promising avenue by\npursuing face recognition for dogs. We contrast four ready-to-use human facial\nrecognizers (EigenFaces, FisherFaces, LBPH, and a Sparse method) to two\noriginal solutions based upon convolutional neural networks: BARK (inspired in\narchitecture-optimized networks employed for human facial recognition) and WOOF\n(based upon off-the-shelf OverFeat features). Human facial recognizers perform\npoorly for dogs (up to 60.5% accuracy), showing that dog facial recognition is\nnot a trivial extension of human facial recognition. The convolutional network\nsolutions work much better, with BARK attaining up to 81.1% accuracy, and WOOF,\n89.4%. The tests were conducted in two datasets: Flickr-dog, with 42 dogs of\ntwo breeds (pugs and huskies); and Snoopybook, with 18 mongrel dogs. \n\n"}
{"id": "1510.05484", "contents": "Title: DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object\n  Detection Abstract: A key problem in salient object detection is how to effectively model the\nsemantic properties of salient objects in a data-driven manner. In this paper,\nwe propose a multi-task deep saliency model based on a fully convolutional\nneural network (FCNN) with global input (whole raw images) and global output\n(whole saliency maps). In principle, the proposed saliency model takes a\ndata-driven strategy for encoding the underlying saliency prior information,\nand then sets up a multi-task learning scheme for exploring the intrinsic\ncorrelations between saliency detection and semantic image segmentation.\nThrough collaborative feature learning from such two correlated tasks, the\nshared fully convolutional layers produce effective features for object\nperception. Moreover, it is capable of capturing the semantic information on\nsalient objects across different levels using the fully convolutional layers,\nwhich investigate the feature-sharing properties of salient object detection\nwith great feature redundancy reduction. Finally, we present a graph Laplacian\nregularized nonlinear regression model for saliency refinement. Experimental\nresults demonstrate the effectiveness of our approach in comparison with the\nstate-of-the-art approaches. \n\n"}
{"id": "1510.06767", "contents": "Title: Order-Fractal transition in abstract paintings Abstract: We report the degree of order of twenty-two Jackson Pollock's paintings using\n\\emph{Hausdorff-Besicovitch fractal dimension}. Through the maximum value of\neach multi-fractal spectrum, the artworks are classify by the year in which\nthey were painted. It has been reported that Pollock's paintings are fractal\nand it increased on his latest works. However our results show that fractal\ndimension of the paintings are on a range of fractal dimension with values\nclose to two. We identify this behavior as a fractal-order transition. Based on\nthe study of disorder-order transition in physical systems, we interpreted the\nfractal-order transition through its dark paint strokes in Pollocks' paintings,\nas structured lines following a power law measured by fractal dimension. We\nobtain self-similarity in some specific Pollock's paintings, that reveal an\nimportant dependence on the scale of observation. We also characterize by its\nfractal spectrum, the called \\emph{Teri's Find}. We obtained similar spectrums\nbetween \\emph{Teri's Find} and \\emph{Number 5} from Pollock, suggesting that\nfractal dimension cannot be completely rejected as a quantitative parameter to\nauthenticate this kind of artworks. \n\n"}
{"id": "1510.07740", "contents": "Title: The Wilson Machine for Image Modeling Abstract: Learning the distribution of natural images is one of the hardest and most\nimportant problems in machine learning. The problem remains open, because the\nenormous complexity of the structures in natural images spans all length\nscales. We break down the complexity of the problem and show that the hierarchy\nof structures in natural images fuels a new class of learning algorithms based\non the theory of critical phenomena and stochastic processes. We approach this\nproblem from the perspective of the theory of critical phenomena, which was\ndeveloped in condensed matter physics to address problems with infinite\nlength-scale fluctuations, and build a framework to integrate the criticality\nof natural images into a learning algorithm. The problem is broken down by\nmapping images into a hierarchy of binary images, called bitplanes. In this\nrepresentation, the top bitplane is critical, having fluctuations in structures\nover a vast range of scales. The bitplanes below go through a gradual\nstochastic heating process to disorder. We turn this representation into a\ndirected probabilistic graphical model, transforming the learning problem into\nthe unsupervised learning of the distribution of the critical bitplane and the\nsupervised learning of the conditional distributions for the remaining\nbitplanes. We learnt the conditional distributions by logistic regression in a\nconvolutional architecture. Conditioned on the critical binary image, this\nsimple architecture can generate large, natural-looking images, with many\nshades of gray, without the use of hidden units, unprecedented in the studies\nof natural images. The framework presented here is a major step in bringing\ncriticality and stochastic processes to machine learning and in studying\nnatural image statistics. \n\n"}
{"id": "1511.02683", "contents": "Title: A Light CNN for Deep Face Representation with Noisy Labels Abstract: The volume of convolutional neural network (CNN) models proposed for face\nrecognition has been continuously growing larger to better fit large amount of\ntraining data. When training data are obtained from internet, the labels are\nlikely to be ambiguous and inaccurate. This paper presents a Light CNN\nframework to learn a compact embedding on the large-scale face data with\nmassive noisy labels. First, we introduce a variation of maxout activation,\ncalled Max-Feature-Map (MFM), into each convolutional layer of CNN. Different\nfrom maxout activation that uses many feature maps to linearly approximate an\narbitrary convex activation function, MFM does so via a competitive\nrelationship. MFM can not only separate noisy and informative signals but also\nplay the role of feature selection between two feature maps. Second, three\nnetworks are carefully designed to obtain better performance meanwhile reducing\nthe number of parameters and computational costs. Lastly, a semantic\nbootstrapping method is proposed to make the prediction of the networks more\nconsistent with noisy labels. Experimental results show that the proposed\nframework can utilize large-scale noisy data to learn a Light model that is\nefficient in computational costs and storage spaces. The learned single network\nwith a 256-D representation achieves state-of-the-art results on various face\nbenchmarks without fine-tuning. The code is released on\nhttps://github.com/AlfredXiangWu/LightCNN. \n\n"}
{"id": "1511.02799", "contents": "Title: Neural Module Networks Abstract: Visual question answering is fundamentally compositional in nature---a\nquestion like \"where is the dog?\" shares substructure with questions like \"what\ncolor is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously\nexploit the representational capacity of deep networks and the compositional\nlinguistic structure of questions. We describe a procedure for constructing and\nlearning *neural module networks*, which compose collections of jointly-trained\nneural \"modules\" into deep networks for question answering. Our approach\ndecomposes questions into their linguistic substructures, and uses these\nstructures to dynamically instantiate modular networks (with reusable\ncomponents for recognizing dogs, classifying colors, etc.). The resulting\ncompound networks are jointly trained. We evaluate our approach on two\nchallenging datasets for visual question answering, achieving state-of-the-art\nresults on both the VQA natural image dataset and a new dataset of complex\nquestions about abstract shapes. \n\n"}
{"id": "1511.03034", "contents": "Title: Learning with a Strong Adversary Abstract: The robustness of neural networks to intended perturbations has recently\nattracted significant attention. In this paper, we propose a new method,\n\\emph{learning with a strong adversary}, that learns robust classifiers from\nsupervised data. The proposed method takes finding adversarial examples as an\nintermediate step. A new and simple way of finding adversarial examples is\npresented and experimentally shown to be efficient. Experimental results\ndemonstrate that resulting learning method greatly improves the robustness of\nthe classification models produced. \n\n"}
{"id": "1511.03416", "contents": "Title: Visual7W: Grounded Question Answering in Images Abstract: We have seen great progress in basic perceptual tasks such as object\nrecognition and detection. However, AI models still fail to match humans in\nhigh-level vision tasks due to the lack of capacities for deeper reasoning.\nRecently the new task of visual question answering (QA) has been proposed to\nevaluate a model's capacity for deep image understanding. Previous works have\nestablished a loose, global association between QA sentences and images.\nHowever, many questions and answers, in practice, relate to local regions in\nthe images. We establish a semantic link between textual descriptions and image\nregions by object-level grounding. It enables a new type of QA with visual\nanswers, in addition to textual answers used in previous work. We study the\nvisual QA tasks in a grounded setting with a large collection of 7W\nmultiple-choice QA pairs. Furthermore, we evaluate human performance and\nseveral baseline models on the QA tasks. Finally, we propose a novel LSTM model\nwith spatial attention to tackle the 7W QA tasks. \n\n"}
{"id": "1511.04103", "contents": "Title: Basic Level Categorization Facilitates Visual Object Recognition Abstract: Recent advances in deep learning have led to significant progress in the\ncomputer vision field, especially for visual object recognition tasks. The\nfeatures useful for object classification are learned by feed-forward deep\nconvolutional neural networks (CNNs) automatically, and they are shown to be\nable to predict and decode neural representations in the ventral visual pathway\nof humans and monkeys. However, despite the huge amount of work on optimizing\nCNNs, there has not been much research focused on linking CNNs with guiding\nprinciples from the human visual cortex. In this work, we propose a network\noptimization strategy inspired by both of the developmental trajectory of\nchildren's visual object recognition capabilities, and Bar (2003), who\nhypothesized that basic level information is carried in the fast magnocellular\npathway through the prefrontal cortex (PFC) and then projected back to inferior\ntemporal cortex (IT), where subordinate level categorization is achieved. We\ninstantiate this idea by training a deep CNN to perform basic level object\ncategorization first, and then train it on subordinate level categorization. We\napply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC\n2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%,\ndemonstrating the effectiveness of the method. We also show that subsequent\ntransfer learning on smaller datasets gives superior results. \n\n"}
{"id": "1511.04166", "contents": "Title: Unsupervised Learning of Edges Abstract: Data-driven approaches for edge detection have proven effective and achieve\ntop results on modern benchmarks. However, all current data-driven edge\ndetectors require manual supervision for training in the form of hand-labeled\nregion segments or object boundaries. Specifically, human annotators mark\nsemantically meaningful edges which are subsequently used for training. Is this\nform of strong, high-level supervision actually necessary to learn to\naccurately detect edges? In this work we present a simple yet effective\napproach for training edge detectors without human supervision. To this end we\nutilize motion, and more specifically, the only input to our method is noisy\nsemi-dense matches between frames. We begin with only a rudimentary knowledge\nof edges (in the form of image gradients), and alternate between improving\nmotion estimation and edge detection in turn. Using a large corpus of video\ndata, we show that edge detectors trained using our unsupervised scheme\napproach the performance of the same methods trained with full supervision\n(within 3-5%). Finally, we show that when using a deep network for the edge\ndetector, our approach provides a novel pre-training scheme for object\ndetection. \n\n"}
{"id": "1511.04401", "contents": "Title: Symbol Grounding Association in Multimodal Sequences with Missing\n  Elements Abstract: In this paper, we extend a symbolic association framework for being able to\nhandle missing elements in multimodal sequences. The general scope of the work\nis the symbolic associations of object-word mappings as it happens in language\ndevelopment in infants. In other words, two different representations of the\nsame abstract concepts can associate in both directions. This scenario has been\nlong interested in Artificial Intelligence, Psychology, and Neuroscience. In\nthis work, we extend a recent approach for multimodal sequences (visual and\naudio) to also cope with missing elements in one or both modalities. Our method\nuses two parallel Long Short-Term Memories (LSTMs) with a learning rule based\non EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We\npropose to include an extra step for the combination with the max operation for\nexploiting the common elements between both sequences. The motivation behind is\nthat the combination acts as a condition selector for choosing the best\nrepresentation from both LSTMs. We evaluated the proposed extension in the\nfollowing scenarios: missing elements in one modality (visual or audio) and\nmissing elements in both modalities (visual and sound). The performance of our\nextension reaches better results than the original model and similar results to\nindividual LSTM trained in each modality. \n\n"}
{"id": "1511.04404", "contents": "Title: Robust Face Alignment Using a Mixture of Invariant Experts Abstract: Face alignment, which is the task of finding the locations of a set of facial\nlandmark points in an image of a face, is useful in widespread application\nareas. Face alignment is particularly challenging when there are large\nvariations in pose (in-plane and out-of-plane rotations) and facial expression.\nTo address this issue, we propose a cascade in which each stage consists of a\nmixture of regression experts. Each expert learns a customized regression model\nthat is specialized to a different subset of the joint space of pose and\nexpressions. The system is invariant to a predefined class of transformations\n(e.g., affine), because the input is transformed to match each expert's\nprototype shape before the regression is applied. We also present a method to\ninclude deformation constraints within the discriminative alignment framework,\nwhich makes our algorithm more robust. Our algorithm significantly outperforms\nprevious methods on publicly available face alignment datasets. \n\n"}
{"id": "1511.04508", "contents": "Title: Distillation as a Defense to Adversarial Perturbations against Deep\n  Neural Networks Abstract: Deep learning algorithms have been shown to perform extremely well on many\nclassical machine learning problems. However, recent studies have shown that\ndeep learning, like other machine learning techniques, is vulnerable to\nadversarial samples: inputs crafted to force a deep neural network (DNN) to\nprovide adversary-selected outputs. Such attacks can seriously undermine the\nsecurity of the system supported by the DNN, sometimes with devastating\nconsequences. For example, autonomous vehicles can be crashed, illicit or\nillegal content can bypass content filters, or biometric authentication systems\ncan be manipulated to allow improper access. In this work, we introduce a\ndefensive mechanism called defensive distillation to reduce the effectiveness\nof adversarial samples on DNNs. We analytically investigate the\ngeneralizability and robustness properties granted by the use of defensive\ndistillation when training DNNs. We also empirically study the effectiveness of\nour defense mechanisms on two DNNs placed in adversarial settings. The study\nshows that defensive distillation can reduce effectiveness of sample creation\nfrom 95% to less than 0.5% on a studied DNN. Such dramatic gains can be\nexplained by the fact that distillation leads gradients used in adversarial\nsample creation to be reduced by a factor of 10^30. We also find that\ndistillation increases the average minimum number of features that need to be\nmodified to create adversarial samples by about 800% on one of the DNNs we\ntested. \n\n"}
{"id": "1511.04524", "contents": "Title: Efficient Training of Very Deep Neural Networks for Supervised Hashing Abstract: In this paper, we propose training very deep neural networks (DNNs) for\nsupervised learning of hash codes. Existing methods in this context train\nrelatively \"shallow\" networks limited by the issues arising in back propagation\n(e.e. vanishing gradients) as well as computational efficiency. We propose a\nnovel and efficient training algorithm inspired by alternating direction method\nof multipliers (ADMM) that overcomes some of these limitations. Our method\ndecomposes the training process into independent layer-wise local updates\nthrough auxiliary variables. Empirically we observe that our training algorithm\nalways converges and its computational complexity is linearly proportional to\nthe number of edges in the networks. Empirically we manage to train DNNs with\n64 hidden layers and 1024 nodes per layer for supervised hashing in about 3\nhours using a single GPU. Our proposed very deep supervised hashing (VDSH)\nmethod significantly outperforms the state-of-the-art on several benchmark\ndatasets. \n\n"}
{"id": "1511.05099", "contents": "Title: Yin and Yang: Balancing and Answering Binary Visual Questions Abstract: The complex compositional structure of language makes problems at the\nintersection of vision and language challenging. But language also provides a\nstrong prior that can result in good superficial performance, without the\nunderlying models truly understanding the visual content. This can hinder\nprogress in pushing state of art in the computer vision aspects of multi-modal\nAI. In this paper, we address binary Visual Question Answering (VQA) on\nabstract scenes. We formulate this problem as visual verification of concepts\ninquired in the questions. Specifically, we convert the question to a tuple\nthat concisely summarizes the visual concept to be detected in the image. If\nthe concept can be found in the image, the answer to the question is \"yes\", and\notherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on\nthe high-level semantics of the VQA task as opposed to the low-level\nrecognition problems, and perhaps more importantly, (2) They provide us the\nmodality to balance the dataset such that language priors are controlled, and\nthe role of vision is essential. In particular, we collect fine-grained pairs\nof scenes for every question, such that the answer to the question is \"yes\" for\none scene, and \"no\" for the other for the exact same question. Indeed, language\npriors alone do not perform better than chance on our balanced dataset.\nMoreover, our proposed approach matches the performance of a state-of-the-art\nVQA approach on the unbalanced dataset, and outperforms it on the balanced\ndataset. \n\n"}
{"id": "1511.05497", "contents": "Title: Learning Neural Network Architectures using Backpropagation Abstract: Deep neural networks with millions of parameters are at the heart of many\nstate of the art machine learning models today. However, recent works have\nshown that models with much smaller number of parameters can also perform just\nas well. In this work, we introduce the problem of architecture-learning, i.e;\nlearning the architecture of a neural network along with weights. We introduce\na new trainable parameter called tri-state ReLU, which helps in eliminating\nunnecessary neurons. We also propose a smooth regularizer which encourages the\ntotal number of neurons after elimination to be small. The resulting objective\nis differentiable and simple to optimize. We experimentally validate our method\non both small and large networks, and show that it can learn models with a\nconsiderably small number of parameters without affecting prediction accuracy. \n\n"}
{"id": "1511.06065", "contents": "Title: Deep Learning for Tactile Understanding From Visual and Haptic Data Abstract: Robots which interact with the physical world will benefit from a\nfine-grained tactile understanding of objects and surfaces. Additionally, for\ncertain tasks, robots may need to know the haptic properties of an object\nbefore touching it. To enable better tactile understanding for robots, we\npropose a method of classifying surfaces with haptic adjectives (e.g.,\ncompressible or smooth) from both visual and physical interaction data. Humans\ntypically combine visual predictions and feedback from physical interactions to\naccurately predict haptic properties and interact with the world. Inspired by\nthis cognitive pattern, we propose and explore a purely visual haptic\nprediction model. Purely visual models enable a robot to \"feel\" without\nphysical interaction. Furthermore, we demonstrate that using both visual and\nphysical interaction signals together yields more accurate haptic\nclassification. Our models take advantage of recent advances in deep neural\nnetworks by employing a unified approach to learning features for physical\ninteraction and visual observations. Even though we employ little domain\nspecific knowledge, our model still achieves better results than methods based\non hand-designed features. \n\n"}
{"id": "1511.06147", "contents": "Title: Coreset-Based Adaptive Tracking Abstract: We propose a method for learning from streaming visual data using a compact,\nconstant size representation of all the data that was seen until a given\nmoment. Specifically, we construct a 'coreset' representation of streaming data\nusing a parallelized algorithm, which is an approximation of a set with\nrelation to the squared distances between this set and all other points in its\nambient space. We learn an adaptive object appearance model from the coreset\ntree in constant time and logarithmic space and use it for object tracking by\ndetection. Our method obtains excellent results for object tracking on three\nstandard datasets over more than 100 videos. The ability to summarize data\nefficiently makes our method ideally suited for tracking in long videos in\npresence of space and time constraints. We demonstrate this ability by\noutperforming a variety of algorithms on the TLD dataset with 2685 frames on\naverage. This coreset based learning approach can be applied for both real-time\nlearning of small, varied data and fast learning of big data. \n\n"}
{"id": "1511.06233", "contents": "Title: Towards Open Set Deep Networks Abstract: Deep networks have produced significant gains for various visual recognition\nproblems, leading to high impact academic and commercial applications. Recent\nwork in deep networks highlighted that it is easy to generate images that\nhumans would never classify as a particular object class, yet networks classify\nsuch images high confidence as that given class - deep network are easily\nfooled with images humans do not consider meaningful. The closed set nature of\ndeep networks forces them to choose from one of the known classes leading to\nsuch artifacts. Recognition in the real world is open set, i.e. the recognition\nsystem should reject unknown/unseen classes at test time. We present a\nmethodology to adapt deep networks for open set recognition, by introducing a\nnew model layer, OpenMax, which estimates the probability of an input being\nfrom an unknown class. A key element of estimating the unknown probability is\nadapting Meta-Recognition concepts to the activation patterns in the\npenultimate layer of the network. OpenMax allows rejection of \"fooling\" and\nunrelated open set images presented to the system; OpenMax greatly reduces the\nnumber of obvious errors made by a deep network. We prove that the OpenMax\nconcept provides bounded open space risk, thereby formally providing an open\nset recognition solution. We evaluate the resulting open set deep networks\nusing pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation\ndata, and thousands of fooling and open set images. The proposed OpenMax model\nsignificantly outperforms open set recognition accuracy of basic deep networks\nas well as deep networks with thresholding of SoftMax probabilities. \n\n"}
{"id": "1511.06241", "contents": "Title: Convolutional Clustering for Unsupervised Learning Abstract: The task of labeling data for training deep neural networks is daunting and\ntedious, requiring millions of labels to achieve the current state-of-the-art\nresults. Such reliance on large amounts of labeled data can be relaxed by\nexploiting hierarchical features via unsupervised learning techniques. In this\nwork, we propose to train a deep convolutional network based on an enhanced\nversion of the k-means clustering algorithm, which reduces the number of\ncorrelated parameters in the form of similar filters, and thus increases test\ncategorization accuracy. We call our algorithm convolutional k-means\nclustering. We further show that learning the connection between the layers of\na deep convolutional neural network improves its ability to be trained on a\nsmaller amount of labeled data. Our experiments show that the proposed\nalgorithm outperforms other techniques that learn filters unsupervised.\nSpecifically, we obtained a test accuracy of 74.1% on STL-10 and a test error\nof 0.5% on MNIST. \n\n"}
{"id": "1511.06381", "contents": "Title: Manifold Regularized Deep Neural Networks using Adversarial Examples Abstract: Learning meaningful representations using deep neural networks involves\ndesigning efficient training schemes and well-structured networks. Currently,\nthe method of stochastic gradient descent that has a momentum with dropout is\none of the most popular training protocols. Based on that, more advanced\nmethods (i.e., Maxout and Batch Normalization) have been proposed in recent\nyears, but most still suffer from performance degradation caused by small\nperturbations, also known as adversarial examples. To address this issue, we\npropose manifold regularized networks (MRnet) that utilize a novel training\nobjective function that minimizes the difference between multi-layer embedding\nresults of samples and those adversarial. Our experimental results demonstrated\nthat MRnet is more resilient to adversarial examples and helps us to generalize\nrepresentations on manifolds. Furthermore, combining MRnet and dropout allowed\nus to achieve competitive classification performances for three well-known\nbenchmarks: MNIST, CIFAR-10, and SVHN. \n\n"}
{"id": "1511.06452", "contents": "Title: Deep Metric Learning via Lifted Structured Feature Embedding Abstract: Learning the distance metric between pairs of examples is of great importance\nfor learning and visual recognition. With the remarkable success from the state\nof the art convolutional neural networks, recent works have shown promising\nresults on discriminatively training the networks to learn semantic feature\nembeddings where similar examples are mapped close to each other and dissimilar\nexamples are mapped farther apart. In this paper, we describe an algorithm for\ntaking full advantage of the training batches in the neural network training by\nlifting the vector of pairwise distances within the batch to the matrix of\npairwise distances. This step enables the algorithm to learn the state of the\nart feature embedding by optimizing a novel structured prediction objective on\nthe lifted problem. Additionally, we collected Online Products dataset: 120k\nimages of 23k classes of online products for metric learning. Our experiments\non the CUB-200-2011, CARS196, and Online Products datasets demonstrate\nsignificant improvement over existing deep feature embedding methods on all\nexperimented embedding sizes with the GoogLeNet network. \n\n"}
{"id": "1511.06457", "contents": "Title: DOC: Deep OCclusion Estimation From a Single Image Abstract: Recovering the occlusion relationships between objects is a fundamental human\nvisual ability which yields important information about the 3D world. In this\npaper we propose a deep network architecture, called DOC, which acts on a\nsingle image, detects object boundaries and estimates the border ownership\n(i.e. which side of the boundary is foreground and which is background). We\nrepresent occlusion relations by a binary edge map, to indicate the object\nboundary, and an occlusion orientation variable which is tangential to the\nboundary and whose direction specifies border ownership by a left-hand rule. We\ntrain two related deep convolutional neural networks, called DOC, which exploit\nlocal and non-local image cues to estimate this representation and hence\nrecover occlusion relations. In order to train and test DOC we construct a\nlarge-scale instance occlusion boundary dataset using PASCAL VOC images, which\nwe call the PASCAL instance occlusion dataset (PIOD). This contains 10,000\nimages and hence is two orders of magnitude larger than existing occlusion\ndatasets for outdoor images. We test two variants of DOC on PIOD and on the\nBSDS occlusion dataset and show they outperform state-of-the-art methods.\nFinally, we perform numerous experiments investigating multiple settings of DOC\nand transfer between BSDS and PIOD, which provides more insights for further\nstudy of occlusion estimation. \n\n"}
{"id": "1511.06654", "contents": "Title: Tracklet Association by Online Target-Specific Metric Learning and\n  Coherent Dynamics Estimation Abstract: In this paper, we present a novel method based on online target-specific\nmetric learning and coherent dynamics estimation for tracklet (track fragment)\nassociation by network flow optimization in long-term multi-person tracking.\nOur proposed framework aims to exploit appearance and motion cues to prevent\nidentity switches during tracking and to recover missed detections.\nFurthermore, target-specific metrics (appearance cue) and motion dynamics\n(motion cue) are proposed to be learned and estimated online, i.e. during the\ntracking process. Our approach is effective even when such cues fail to\nidentify or follow the target due to occlusions or object-to-object\ninteractions. We also propose to learn the weights of these two tracking cues\nto handle the difficult situations, such as severe occlusions and\nobject-to-object interactions effectively. Our method has been validated on\nseveral public datasets and the experimental results show that it outperforms\nseveral state-of-the-art tracking methods. \n\n"}
{"id": "1511.07122", "contents": "Title: Multi-Scale Context Aggregation by Dilated Convolutions Abstract: State-of-the-art models for semantic segmentation are based on adaptations of\nconvolutional networks that had originally been designed for image\nclassification. However, dense prediction and image classification are\nstructurally different. In this work, we develop a new convolutional network\nmodule that is specifically designed for dense prediction. The presented module\nuses dilated convolutions to systematically aggregate multi-scale contextual\ninformation without losing resolution. The architecture is based on the fact\nthat dilated convolutions support exponential expansion of the receptive field\nwithout loss of resolution or coverage. We show that the presented context\nmodule increases the accuracy of state-of-the-art semantic segmentation\nsystems. In addition, we examine the adaptation of image classification\nnetworks to dense prediction and show that simplifying the adapted network can\nincrease accuracy. \n\n"}
{"id": "1511.07299", "contents": "Title: Rendering refraction and reflection of eyeglasses for synthetic eye\n  tracker images Abstract: While for the evaluation of robustness of eye tracking algorithms the use of\nreal-world data is essential, there are many applications where simulated,\nsynthetic eye images are of advantage. They can generate labelled ground-truth\ndata for appearance based gaze estimation algorithms or enable the development\nof model based gaze estimation techniques by showing the influence on gaze\nestimation error of different model factors that can then be simplified or\nextended. We extend the generation of synthetic eye images by a simulation of\nrefraction and reflection for eyeglasses. On the one hand this allows for the\ntesting of pupil and glint detection algorithms under different illumination\nand reflection conditions, on the other hand the error of gaze estimation\nroutines can be estimated in conjunction with different eyeglasses. We show how\na polynomial function fitting calibration performs equally well with and\nwithout eyeglasses, and how a geometrical eye model behaves when exposed to\nglasses. \n\n"}
{"id": "1511.07845", "contents": "Title: Shape and Symmetry Induction for 3D Objects Abstract: Actions as simple as grasping an object or navigating around it require a\nrich understanding of that object's 3D shape from a given viewpoint. In this\npaper we repurpose powerful learning machinery, originally developed for object\nclassification, to discover image cues relevant for recovering the 3D shape of\npotentially unfamiliar objects. We cast the problem as one of local prediction\nof surface normals and global detection of 3D reflection symmetry planes, which\nopen the door for extrapolating occluded surfaces from visible ones. We\ndemonstrate that our method is able to recover accurate 3D shape information\nfor classes of objects it was not trained on, in both synthetic and real\nimages. \n\n"}
{"id": "1511.07917", "contents": "Title: Context-aware CNNs for person head detection Abstract: Person detection is a key problem for many computer vision tasks. While face\ndetection has reached maturity, detecting people under a full variation of\ncamera view-points, human poses, lighting conditions and occlusions is still a\ndifficult challenge. In this work we focus on detecting human heads in natural\nscenes. Starting from the recent local R-CNN object detector, we extend it with\ntwo types of contextual cues. First, we leverage person-scene relations and\npropose a Global CNN model trained to predict positions and scales of heads\ndirectly from the full image. Second, we explicitly model pairwise relations\namong objects and train a Pairwise CNN model using a structured-output\nsurrogate loss. The Local, Global and Pairwise models are combined into a joint\nCNN framework. To train and test our full model, we introduce a large dataset\ncomposed of 369,846 human heads annotated in 224,740 movie frames. We evaluate\nour method and demonstrate improvements of person head detection against\nseveral recent baselines in three datasets. We also show improvements of the\ndetection speed provided by our model. \n\n"}
{"id": "1511.08032", "contents": "Title: Learning to detect video events from zero or very few video examples Abstract: In this work we deal with the problem of high-level event detection in video.\nSpecifically, we study the challenging problems of i) learning to detect video\nevents from solely a textual description of the event, without using any\npositive video examples, and ii) additionally exploiting very few positive\ntraining samples together with a small number of ``related'' videos. For\nlearning only from an event's textual description, we first identify a general\nlearning framework and then study the impact of different design choices for\nvarious stages of this framework. For additionally learning from example\nvideos, when true positive training samples are scarce, we employ an extension\nof the Support Vector Machine that allows us to exploit ``related'' event\nvideos by automatically introducing different weights for subsets of the videos\nin the overall training set. Experimental evaluations performed on the\nlarge-scale TRECVID MED 2014 video dataset provide insight on the effectiveness\nof the proposed methods. \n\n"}
{"id": "1512.02134", "contents": "Title: A Large Dataset to Train Convolutional Networks for Disparity, Optical\n  Flow, and Scene Flow Estimation Abstract: Recent work has shown that optical flow estimation can be formulated as a\nsupervised learning task and can be successfully solved with convolutional\nnetworks. Training of the so-called FlowNet was enabled by a large\nsynthetically generated dataset. The present paper extends the concept of\noptical flow estimation via convolutional networks to disparity and scene flow\nestimation. To this end, we propose three synthetic stereo video datasets with\nsufficient realism, variation, and size to successfully train large networks.\nOur datasets are the first large-scale datasets to enable training and\nevaluating scene flow methods. Besides the datasets, we present a convolutional\nnetwork for real-time disparity estimation that provides state-of-the-art\nresults. By combining a flow and disparity estimation network and training it\njointly, we demonstrate the first scene flow estimation with a convolutional\nnetwork. \n\n"}
{"id": "1512.02736", "contents": "Title: Window-Object Relationship Guided Representation Learning for Generic\n  Object Detections Abstract: In existing works that learn representation for object detection, the\nrelationship between a candidate window and the ground truth bounding box of an\nobject is simplified by thresholding their overlap. This paper shows\ninformation loss in this simplification and picks up the relative location/size\ninformation discarded by thresholding. We propose a representation learning\npipeline to use the relationship as supervision for improving the learned\nrepresentation in object detection. Such relationship is not limited to object\nof the target category, but also includes surrounding objects of other\ncategories. We show that image regions with multiple contexts and multiple\nrotations are effective in capturing such relationship during the\nrepresentation learning process and in handling the semantic and visual\nvariation caused by different window-object configurations. Experimental\nresults show that the representation learned by our approach can improve the\nobject detection accuracy by 6.4% in mean average precision (mAP) on\nILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved\nby our single model and it is the best among published results. On PASCAL VOC,\nit outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute\nmAP. \n\n"}
{"id": "1512.03460", "contents": "Title: Neural Self Talk: Image Understanding via Continuous Questioning and\n  Answering Abstract: In this paper we consider the problem of continuously discovering image\ncontents by actively asking image based questions and subsequently answering\nthe questions being asked. The key components include a Visual Question\nGeneration (VQG) module and a Visual Question Answering module, in which\nRecurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are\nused. Given a dataset that contains images, questions and their answers, both\nmodules are trained at the same time, with the difference being VQG uses the\nimages as input and the corresponding questions as output, while VQA uses\nimages and questions as input and the corresponding answers as output. We\nevaluate the self talk process subjectively using Amazon Mechanical Turk, which\nshow effectiveness of the proposed method. \n\n"}
{"id": "1512.05227", "contents": "Title: Fine-grained Categorization and Dataset Bootstrapping using Deep Metric\n  Learning with Humans in the Loop Abstract: Existing fine-grained visual categorization methods often suffer from three\nchallenges: lack of training data, large number of fine-grained categories, and\nhigh intraclass vs. low inter-class variance. In this work we propose a generic\niterative framework for fine-grained categorization and dataset bootstrapping\nthat handles these three challenges. Using deep metric learning with humans in\nthe loop, we learn a low dimensional feature embedding with anchor points on\nmanifolds for each category. These anchor points capture intra-class variances\nand remain discriminative between classes. In each round, images with high\nconfidence scores from our model are sent to humans for labeling. By comparing\nwith exemplar images, labelers mark each candidate image as either a \"true\npositive\" or a \"false positive\". True positives are added into our current\ndataset and false positives are regarded as \"hard negatives\" for our metric\nlearning model. Then the model is retrained with an expanded dataset and hard\nnegatives for the next round. To demonstrate the effectiveness of the proposed\nframework, we bootstrap a fine-grained flower dataset with 620 categories from\nInstagram images. The proposed deep metric learning scheme is evaluated on both\nour dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show\nsignificant performance gain using dataset bootstrapping and demonstrate\nstate-of-the-art results achieved by the proposed deep metric learning methods. \n\n"}
{"id": "1512.05246", "contents": "Title: Blockout: Dynamic Model Selection for Hierarchical Deep Networks Abstract: Most deep architectures for image classification--even those that are trained\nto classify a large number of diverse categories--learn shared image\nrepresentations with a single model. Intuitively, however, categories that are\nmore similar should share more information than those that are very different.\nWhile hierarchical deep networks address this problem by learning separate\nfeatures for subsets of related categories, current implementations require\nsimplified models using fixed architectures specified via heuristic clustering\nmethods. Instead, we propose Blockout, a method for regularization and model\nselection that simultaneously learns both the model architecture and\nparameters. A generalization of Dropout, our approach gives a novel\nparametrization of hierarchical architectures that allows for structure\nlearning via back-propagation. To demonstrate its utility, we evaluate Blockout\non the CIFAR and ImageNet datasets, demonstrating improved classification\naccuracy, better regularization performance, faster training, and the clear\nemergence of hierarchical network structures. \n\n"}
{"id": "1512.06658", "contents": "Title: Deep Learning for Surface Material Classification Using Haptic And\n  Visual Information Abstract: When a user scratches a hand-held rigid tool across an object surface, an\nacceleration signal can be captured, which carries relevant information about\nthe surface. More importantly, such a haptic signal is complementary to the\nvisual appearance of the surface, which suggests the combination of both\nmodalities for the recognition of the surface material. In this paper, we\npresent a novel deep learning method dealing with the surface material\nclassification problem based on a Fully Convolutional Network (FCN), which\ntakes as input the aforementioned acceleration signal and a corresponding image\nof the surface texture. Compared to previous surface material classification\nsolutions, which rely on a careful design of hand-crafted domain-specific\nfeatures, our method automatically extracts discriminative features utilizing\nthe advanced deep learning methodologies. Experiments performed on the TUM\nsurface material database demonstrate that our method achieves state-of-the-art\nclassification accuracy robustly and efficiently. \n\n"}
{"id": "1512.07928", "contents": "Title: Learning Transferrable Knowledge for Semantic Segmentation with Deep\n  Convolutional Neural Network Abstract: We propose a novel weakly-supervised semantic segmentation algorithm based on\nDeep Convolutional Neural Network (DCNN). Contrary to existing\nweakly-supervised approaches, our algorithm exploits auxiliary segmentation\nannotations available for different categories to guide segmentations on images\nwith only image-level class labels. To make the segmentation knowledge\ntransferrable across categories, we design a decoupled encoder-decoder\narchitecture with attention model. In this architecture, the model generates\nspatial highlights of each category presented in an image using an attention\nmodel, and subsequently generates foreground segmentation for each highlighted\nregion using decoder. Combining attention model, we show that the decoder\ntrained with segmentation annotations in different categories can boost the\nperformance of weakly-supervised semantic segmentation. The proposed algorithm\ndemonstrates substantially improved performance compared to the\nstate-of-the-art weakly-supervised techniques in challenging PASCAL VOC 2012\ndataset when our model is trained with the annotations in 60 exclusive\ncategories in Microsoft COCO dataset. \n\n"}
{"id": "1601.02919", "contents": "Title: Using Filter Banks in Convolutional Neural Networks for Texture\n  Classification Abstract: Deep learning has established many new state of the art solutions in the last\ndecade in areas such as object, scene and speech recognition. In particular\nConvolutional Neural Network (CNN) is a category of deep learning which obtains\nexcellent results in object detection and recognition tasks. Its architecture\nis indeed well suited to object analysis by learning and classifying complex\n(deep) features that represent parts of an object or the object itself.\nHowever, some of its features are very similar to texture analysis methods. CNN\nlayers can be thought of as filter banks of complexity increasing with the\ndepth. Filter banks are powerful tools to extract texture features and have\nbeen widely used in texture analysis. In this paper we develop a simple network\narchitecture named Texture CNN (T-CNN) which explores this observation. It is\nbuilt on the idea that the overall shape information extracted by the fully\nconnected layers of a classic CNN is of minor importance in texture analysis.\nTherefore, we pool an energy measure from the last convolution layer which we\nconnect to a fully connected layer. We show that our approach can improve the\nperformance of a network while greatly reducing the memory usage and\ncomputation. \n\n"}
{"id": "1601.06759", "contents": "Title: Pixel Recurrent Neural Networks Abstract: Modeling the distribution of natural images is a landmark problem in\nunsupervised learning. This task requires an image model that is at once\nexpressive, tractable and scalable. We present a deep neural network that\nsequentially predicts the pixels in an image along the two spatial dimensions.\nOur method models the discrete probability of the raw pixel values and encodes\nthe complete set of dependencies in the image. Architectural novelties include\nfast two-dimensional recurrent layers and an effective use of residual\nconnections in deep recurrent networks. We achieve log-likelihood scores on\nnatural images that are considerably better than the previous state of the art.\nOur main results also provide benchmarks on the diverse ImageNet dataset.\nSamples generated from the model appear crisp, varied and globally coherent. \n\n"}
{"id": "1602.04489", "contents": "Title: Convolutional Tables Ensemble: classification in microseconds Abstract: We study classifiers operating under severe classification time constraints,\ncorresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble\n(CTE), an inherently fast architecture for object category recognition. The\narchitecture is based on convolutionally-applied sparse feature extraction,\nusing trees or ferns, and a linear voting layer. Several structure and\noptimization variants are considered, including novel decision functions, tree\nlearning algorithm, and distillation from CNN to CTE architecture. Accuracy\nimprovements of 24-45% over related art of similar speed are demonstrated on\nstandard object recognition benchmarks. Using Pareto speed-accuracy curves, we\nshow that CTE can provide better accuracy than Convolutional Neural Networks\n(CNN) for a certain range of classification time constraints, or alternatively\nprovide similar error rates with 5-200X speedup. \n\n"}
{"id": "1602.04951", "contents": "Title: Q($\\lambda$) with Off-Policy Corrections Abstract: We propose and analyze an alternate approach to off-policy multi-step\ntemporal difference learning, in which off-policy returns are corrected with\nthe current Q-function in terms of rewards, rather than with the target policy\nin terms of transition probabilities. We prove that such approximate\ncorrections are sufficient for off-policy convergence both in policy evaluation\nand control, provided certain conditions. These conditions relate the distance\nbetween the target and behavior policies, the eligibility trace parameter and\nthe discount factor, and formalize an underlying tradeoff in off-policy\nTD($\\lambda$). We illustrate this theoretical relationship empirically on a\ncontinuous-state control task. \n\n"}
{"id": "1602.07017", "contents": "Title: A survey of sparse representation: algorithms and applications Abstract: Sparse representation has attracted much attention from researchers in fields\nof signal processing, image processing, computer vision and pattern\nrecognition. Sparse representation also has a good reputation in both\ntheoretical research and practical applications. Many different algorithms have\nbeen proposed for sparse representation. The main purpose of this article is to\nprovide a comprehensive study and an updated review on sparse representation\nand to supply a guidance for researchers. The taxonomy of sparse representation\nmethods can be studied from various viewpoints. For example, in terms of\ndifferent norm minimizations used in sparsity constraints, the methods can be\nroughly categorized into five groups: sparse representation with $l_0$-norm\nminimization, sparse representation with $l_p$-norm (0$<$p$<$1) minimization,\nsparse representation with $l_1$-norm minimization and sparse representation\nwith $l_{2,1}$-norm minimization. In this paper, a comprehensive overview of\nsparse representation is provided. The available sparse representation\nalgorithms can also be empirically categorized into four groups: greedy\nstrategy approximation, constrained optimization, proximity algorithm-based\noptimization, and homotopy algorithm-based sparse representation. The\nrationales of different algorithms in each category are analyzed and a wide\nrange of sparse representation applications are summarized, which could\nsufficiently reveal the potential nature of the sparse representation theory.\nSpecifically, an experimentally comparative study of these sparse\nrepresentation algorithms was presented. The Matlab code used in this paper can\nbe available at: http://www.yongxu.org/lunwen.html. \n\n"}
{"id": "1602.07383", "contents": "Title: Automatic Moth Detection from Trap Images for Pest Management Abstract: Monitoring the number of insect pests is a crucial component in\npheromone-based pest management systems. In this paper, we propose an automatic\ndetection pipeline based on deep learning for identifying and counting pests in\nimages taken inside field traps. Applied to a commercial codling moth dataset,\nour method shows promising performance both qualitatively and quantitatively.\nCompared to previous attempts at pest detection, our approach uses no\npest-specific engineering which enables it to adapt to other species and\nenvironments with minimal human effort. It is amenable to implementation on\nparallel hardware and therefore capable of deployment in settings where\nreal-time performance is required. \n\n"}
{"id": "1603.00370", "contents": "Title: Scalable Metric Learning via Weighted Approximate Rank Component\n  Analysis Abstract: We are interested in the large-scale learning of Mahalanobis distances, with\na particular focus on person re-identification.\n  We propose a metric learning formulation called Weighted Approximate Rank\nComponent Analysis (WARCA). WARCA optimizes the precision at top ranks by\ncombining the WARP loss with a regularizer that favors orthonormal linear\nmappings, and avoids rank-deficient embeddings. Using this new regularizer\nallows us to adapt the large-scale WSABIE procedure and to leverage the Adam\nstochastic optimization algorithm, which results in an algorithm that scales\ngracefully to very large data-sets. Also, we derive a kernelized version which\nallows to take advantage of state-of-the-art features for re-identification\nwhen data-set size permits kernel computation.\n  Benchmarks on recent and standard re-identification data-sets show that our\nmethod beats existing state-of-the-art techniques both in term of accuracy and\nspeed. We also provide experimental analysis to shade lights on the properties\nof the regularizer we use, and how it improves performance. \n\n"}
{"id": "1603.01292", "contents": "Title: Modular Decomposition and Analysis of Registration based Trackers Abstract: This paper presents a new way to study registration based trackers by\ndecomposing them into three constituent sub modules: appearance model, state\nspace model and search method. It is often the case that when a new tracker is\nintroduced in literature, it only contributes to one or two of these sub\nmodules while using existing methods for the rest. Since these are often\nselected arbitrarily by the authors, they may not be optimal for the new\nmethod. In such cases, our breakdown can help to experimentally find the best\ncombination of methods for these sub modules while also providing a framework\nwithin which the contributions of the new tracker can be clearly demarcated and\nthus studied better. We show how existing trackers can be broken down using the\nsuggested methodology and compare the performance of the default configuration\nchosen by the authors against other possible combinations to demonstrate the\nnew insights that can be gained by such an approach. We also present an open\nsource system that provides a convenient interface to plug in a new method for\nany sub module and test it against all possible combinations of methods for the\nother two sub modules while also serving as a fast and efficient solution for\npractical tracking requirements. \n\n"}
{"id": "1603.01801", "contents": "Title: Variational methods for Conditional Multimodal Deep Learning Abstract: In this paper, we address the problem of conditional modality learning,\nwhereby one is interested in generating one modality given the other. While it\nis straightforward to learn a joint distribution over multiple modalities using\na deep multimodal architecture, we observe that such models aren't very\neffective at conditional generation. Hence, we address the problem by learning\nconditional distributions between the modalities. We use variational methods\nfor maximizing the corresponding conditional log-likelihood. The resultant deep\nmodel, which we refer to as conditional multimodal autoencoder (CMMA), forces\nthe latent representation obtained from a single modality alone to be `close'\nto the joint representation obtained from multiple modalities. We use the\nproposed model to generate faces from attributes. We show that the faces\ngenerated from attributes using the proposed model, are qualitatively and\nquantitatively more representative of the attributes from which they were\ngenerated, than those obtained by other deep generative models. We also propose\na secondary task, whereby the existing faces are modified by modifying the\ncorresponding attributes. We observe that the modifications in face introduced\nby the proposed model are representative of the corresponding modifications in\nattributes. \n\n"}
{"id": "1603.01842", "contents": "Title: Proximal groupoid patterns In digital images Abstract: The focus of this article is on the detection and classification of patterns\nbased on groupoids. The approach hinges on descriptive proximity of points in a\nset based on the neighborliness property. This approach lends support to image\nanalysis and understanding and in studying nearness of image segments. A\npractical application of the approach is in terms of the analysis of natural\nimages for pattern identification and classification. \n\n"}
{"id": "1603.02003", "contents": "Title: From A to Z: Supervised Transfer of Style and Content Using Deep Neural\n  Network Generators Abstract: We propose a new neural network architecture for solving single-image\nanalogies - the generation of an entire set of stylistically similar images\nfrom just a single input image. Solving this problem requires separating image\nstyle from content. Our network is a modified variational autoencoder (VAE)\nthat supports supervised training of single-image analogies and in-network\nevaluation of outputs with a structured similarity objective that captures\npixel covariances. On the challenging task of generating a 62-letter font from\na single example letter we produce images with 22.4% lower dissimilarity to the\nground truth than state-of-the-art. \n\n"}
{"id": "1603.02199", "contents": "Title: Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning\n  and Large-Scale Data Collection Abstract: We describe a learning-based approach to hand-eye coordination for robotic\ngrasping from monocular images. To learn hand-eye coordination for grasping, we\ntrained a large convolutional neural network to predict the probability that\ntask-space motion of the gripper will result in successful grasps, using only\nmonocular camera images and independently of camera calibration or the current\nrobot pose. This requires the network to observe the spatial relationship\nbetween the gripper and objects in the scene, thus learning hand-eye\ncoordination. We then use this network to servo the gripper in real time to\nachieve successful grasps. To train our network, we collected over 800,000\ngrasp attempts over the course of two months, using between 6 and 14 robotic\nmanipulators at any given time, with differences in camera placement and\nhardware. Our experimental evaluation demonstrates that our method achieves\neffective real-time control, can successfully grasp novel objects, and corrects\nmistakes by continuous servoing. \n\n"}
{"id": "1603.02644", "contents": "Title: Online but Accurate Inference for Latent Variable Models with Local\n  Gibbs Sampling Abstract: We study parameter inference in large-scale latent variable models. We first\npropose an unified treatment of online inference for latent variable models\nfrom a non-canonical exponential family, and draw explicit links between\nseveral previously proposed frequentist or Bayesian methods. We then propose a\nnovel inference method for the frequentist estimation of parameters, that\nadapts MCMC methods to online inference of latent variable models with the\nproper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we\nprovide an extensive set of experiments and comparisons with existing work,\nwhere our new approach outperforms all previously proposed methods. In\nparticular, using Gibbs sampling for latent variable inference is superior to\nvariational inference in terms of test log-likelihoods. Moreover, Bayesian\ninference through variational methods perform poorly, sometimes leading to\nworse fits with latent variables of higher dimensionality. \n\n"}
{"id": "1603.02814", "contents": "Title: Image Captioning and Visual Question Answering Based on Attributes and\n  External Knowledge Abstract: Much recent progress in Vision-to-Language problems has been achieved through\na combination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs). This approach does not explicitly represent high-level\nsemantic concepts, but rather seeks to progress directly from image features to\ntext. In this paper we first propose a method of incorporating high-level\nconcepts into the successful CNN-RNN approach, and show that it achieves a\nsignificant improvement on the state-of-the-art in both image captioning and\nvisual question answering. We further show that the same mechanism can be used\nto incorporate external knowledge, which is critically important for answering\nhigh level visual questions. Specifically, we design a visual question\nanswering model that combines an internal representation of the content of an\nimage with information extracted from a general knowledge base to answer a\nbroad range of image-based questions. It particularly allows questions to be\nasked about the contents of an image, even when the image itself does not\ncontain a complete answer. Our final model achieves the best reported results\non both image captioning and visual question answering on several benchmark\ndatasets. \n\n"}
{"id": "1603.03183", "contents": "Title: Exploring Context with Deep Structured models for Semantic Segmentation Abstract: State-of-the-art semantic image segmentation methods are mostly based on\ntraining deep convolutional neural networks (CNNs). In this work, we proffer to\nimprove semantic segmentation with the use of contextual information. In\nparticular, we explore `patch-patch' context and `patch-background' context in\ndeep CNNs. We formulate deep structured models by combining CNNs and\nConditional Random Fields (CRFs) for learning the patch-patch context between\nimage regions. Specifically, we formulate CNN-based pairwise potential\nfunctions to capture semantic correlations between neighboring patches.\nEfficient piecewise training of the proposed deep structured model is then\napplied in order to avoid repeated expensive CRF inference during the course of\nback propagation. For capturing the patch-background context, we show that a\nnetwork design with traditional multi-scale image inputs and sliding pyramid\npooling is very effective for improving performance. We perform comprehensive\nevaluation of the proposed method. We achieve new state-of-the-art performance\non a number of challenging semantic segmentation datasets including $NYUDv2$,\n$PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$,\n$SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report an\nintersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset. \n\n"}
{"id": "1603.04619", "contents": "Title: Image Co-localization by Mimicking a Good Detector's Confidence Score\n  Distribution Abstract: Given a set of images containing objects from the same category, the task of\nimage co-localization is to identify and localize each instance. This paper\nshows that this problem can be solved by a simple but intriguing idea, that is,\na common object detector can be learnt by making its detection confidence\nscores distributed like those of a strongly supervised detector. More\nspecifically, we observe that given a set of object proposals extracted from an\nimage that contains the object of interest, an accurate strongly supervised\nobject detector should give high scores to only a small minority of proposals,\nand low scores to most of them. Thus, we devise an entropy-based objective\nfunction to enforce the above property when learning the common object\ndetector. Once the detector is learnt, we resort to a segmentation approach to\nrefine the localization. We show that despite its simplicity, our approach\noutperforms state-of-the-art methods. \n\n"}
{"id": "1603.05027", "contents": "Title: Identity Mappings in Deep Residual Networks Abstract: Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers \n\n"}
{"id": "1603.05279", "contents": "Title: XNOR-Net: ImageNet Classification Using Binary Convolutional Neural\n  Networks Abstract: We propose two efficient approximations to standard convolutional neural\nnetworks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,\nthe filters are approximated with binary values resulting in 32x memory saving.\nIn XNOR-Networks, both the filters and the input to convolutional layers are\nbinary. XNOR-Networks approximate convolutions using primarily binary\noperations. This results in 58x faster convolutional operations and 32x memory\nsavings. XNOR-Nets offer the possibility of running state-of-the-art networks\non CPUs (rather than GPUs) in real-time. Our binary networks are simple,\naccurate, efficient, and work on challenging visual tasks. We evaluate our\napproach on the ImageNet classification task. The classification accuracy with\na Binary-Weight-Network version of AlexNet is only 2.9% less than the\nfull-precision AlexNet (in top-1 measure). We compare our method with recent\nnetwork binarization methods, BinaryConnect and BinaryNets, and outperform\nthese methods by large margins on ImageNet, more than 16% in top-1 accuracy. \n\n"}
{"id": "1603.05474", "contents": "Title: Neural Aggregation Network for Video Face Recognition Abstract: This paper presents a Neural Aggregation Network (NAN) for video face\nrecognition. The network takes a face video or face image set of a person with\na variable number of face images as its input, and produces a compact,\nfixed-dimension feature representation for recognition. The whole network is\ncomposed of two modules. The feature embedding module is a deep Convolutional\nNeural Network (CNN) which maps each face image to a feature vector. The\naggregation module consists of two attention blocks which adaptively aggregate\nthe feature vectors to form a single feature inside the convex hull spanned by\nthem. Due to the attention mechanism, the aggregation is invariant to the image\norder. Our NAN is trained with a standard classification or verification loss\nwithout any extra supervision signal, and we found that it automatically learns\nto advocate high-quality face images while repelling low-quality ones such as\nblurred, occluded and improperly exposed faces. The experiments on IJB-A,\nYouTube Face, Celebrity-1000 video face recognition benchmarks show that it\nconsistently outperforms naive aggregation methods and achieves the\nstate-of-the-art accuracy. \n\n"}
{"id": "1603.07415", "contents": "Title: Attentive Contexts for Object Detection Abstract: Modern deep neural network based object detection methods typically classify\ncandidate proposals using their interior features. However, global and local\nsurrounding contexts that are believed to be valuable for object detection are\nnot fully exploited by existing methods yet. In this work, we take a step\ntowards understanding what is a robust practice to extract and utilize\ncontextual information to facilitate object detection in practice.\nSpecifically, we consider the following two questions: \"how to identify useful\nglobal contextual information for detecting a certain object?\" and \"how to\nexploit local context surrounding a proposal for better inferring its\ncontents?\". We provide preliminary answers to these questions through\ndeveloping a novel Attention to Context Convolution Neural Network (AC-CNN)\nbased object detection model. AC-CNN effectively incorporates global and local\ncontextual information into the region-based CNN (e.g. Fast RCNN) detection\nmodel and provides better object detection performance. It consists of one\nattention-based global contextualized (AGC) sub-network and one multi-scale\nlocal contextualized (MLC) sub-network. To capture global context, the AGC\nsub-network recurrently generates an attention map for an input image to\nhighlight useful global contextual locations, through multiple stacked Long\nShort-Term Memory (LSTM) layers. For capturing surrounding local context, the\nMLC sub-network exploits both the inside and outside contextual information of\neach specific proposal at multiple scales. The global and local context are\nthen fused together for making the final decision for detection. Extensive\nexperiments on PASCAL VOC 2007 and VOC 2012 well demonstrate the superiority of\nthe proposed AC-CNN over well-established baselines. In particular, AC-CNN\noutperforms the popular Fast-RCNN by 2.0% and 2.2% on VOC 2007 and VOC 2012 in\nterms of mAP, respectively. \n\n"}
{"id": "1603.07475", "contents": "Title: Fine-scale Surface Normal Estimation using a Single NIR Image Abstract: We present surface normal estimation using a single near infrared (NIR)\nimage. We are focusing on fine-scale surface geometry captured with an\nuncalibrated light source. To tackle this ill-posed problem, we adopt a\ngenerative adversarial network which is effective in recovering a sharp output,\nwhich is also essential for fine-scale surface normal estimation. We\nincorporate angular error and integrability constraint into the objective\nfunction of the network to make estimated normals physically meaningful. We\ntrain and validate our network on a recent NIR dataset, and also evaluate the\ngenerality of our trained model by using new external datasets which are\ncaptured with a different camera under different environment. \n\n"}
{"id": "1603.08155", "contents": "Title: Perceptual Losses for Real-Time Style Transfer and Super-Resolution Abstract: We consider image transformation problems, where an input image is\ntransformed into an output image. Recent methods for such problems typically\ntrain feed-forward convolutional neural networks using a \\emph{per-pixel} loss\nbetween the output and ground-truth images. Parallel work has shown that\nhigh-quality images can be generated by defining and optimizing\n\\emph{perceptual} loss functions based on high-level features extracted from\npretrained networks. We combine the benefits of both approaches, and propose\nthe use of perceptual loss functions for training feed-forward networks for\nimage transformation tasks. We show results on image style transfer, where a\nfeed-forward network is trained to solve the optimization problem proposed by\nGatys et al in real-time. Compared to the optimization-based method, our\nnetwork gives similar qualitative results but is three orders of magnitude\nfaster. We also experiment with single-image super-resolution, where replacing\na per-pixel loss with a perceptual loss gives visually pleasing results. \n\n"}
{"id": "1603.08233", "contents": "Title: Evolution of active categorical image classification via saccadic eye\n  movement Abstract: Pattern recognition and classification is a central concern for modern\ninformation processing systems. In particular, one key challenge to image and\nvideo classification has been that the computational cost of image processing\nscales linearly with the number of pixels in the image or video. Here we\npresent an intelligent machine (the \"active categorical classifier,\" or ACC)\nthat is inspired by the saccadic movements of the eye, and is capable of\nclassifying images by selectively scanning only a portion of the image. We\nharness evolutionary computation to optimize the ACC on the MNIST hand-written\ndigit classification task, and provide a proof-of-concept that the ACC works on\nnoisy multi-class data. We further analyze the ACC and demonstrate its ability\nto classify images after viewing only a fraction of the pixels, and provide\ninsight on future research paths to further improve upon the ACC presented\nhere. \n\n"}
{"id": "1603.08358", "contents": "Title: Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation\n  with Deep Gaussian CRFs Abstract: In this work we propose a structured prediction technique that combines the\nvirtues of Gaussian Conditional Random Fields (G-CRF) with Deep Learning: (a)\nour structured prediction task has a unique global optimum that is obtained\nexactly from the solution of a linear system (b) the gradients of our model\nparameters are analytically computed using closed form expressions, in contrast\nto the memory-demanding contemporary deep structured prediction approaches that\nrely on back-propagation-through-time, (c) our pairwise terms do not have to be\nsimple hand-crafted expressions, as in the line of works building on the\nDenseCRF, but can rather be `discovered' from data through deep architectures,\nand (d) out system can trained in an end-to-end manner. Building on standard\ntools from numerical analysis we develop very efficient algorithms for\ninference and learning, as well as a customized technique adapted to the\nsemantic segmentation task. This efficiency allows us to explore more\nsophisticated architectures for structured prediction in deep learning: we\nintroduce multi-resolution architectures to couple information across scales in\na joint optimization framework, yielding systematic improvements. We\ndemonstrate the utility of our approach on the challenging VOC PASCAL 2012\nimage segmentation benchmark, showing substantial improvements over strong\nbaselines. We make all of our code and experiments available at\n{https://github.com/siddharthachandra/gcrf} \n\n"}
{"id": "1603.08988", "contents": "Title: Towards Practical Bayesian Parameter and State Estimation Abstract: Joint state and parameter estimation is a core problem for dynamic Bayesian\nnetworks. Although modern probabilistic inference toolkits make it relatively\neasy to specify large and practically relevant probabilistic models, the silver\nbullet---an efficient and general online inference algorithm for such\nproblems---remains elusive, forcing users to write special-purpose code for\neach application. We propose a novel blackbox algorithm -- a hybrid of particle\nfiltering for state variables and assumed density filtering for parameter\nvariables. It has following advantages: (a) it is efficient due to its online\nnature, and (b) it is applicable to both discrete and continuous parameter\nspaces . On a variety of toy and real models, our system is able to generate\nmore accurate results within a fixed computation budget. This preliminary\nevidence indicates that the proposed approach is likely to be of practical use. \n\n"}
{"id": "1604.01325", "contents": "Title: Deep Image Retrieval: Learning global representations for image search Abstract: We propose a novel approach for instance-level image retrieval. It produces a\nglobal and compact fixed-length representation for each image by aggregating\nmany region-wise descriptors. In contrast to previous works employing\npre-trained deep networks as a black box to produce features, our method\nleverages a deep architecture trained for the specific task of image retrieval.\nOur contribution is twofold: (i) we leverage a ranking framework to learn\nconvolution and projection weights that are used to build the region features;\nand (ii) we employ a region proposal network to learn which regions should be\npooled to form the final global descriptor. We show that using clean training\ndata is key to the success of our approach. To that aim, we use a large scale\nbut noisy landmark dataset and develop an automatic cleaning approach. The\nproposed architecture produces a global image representation in a single\nforward pass. Our approach significantly outperforms previous approaches based\non global descriptors on standard datasets. It even surpasses most prior works\nbased on costly local descriptor indexing and spatial verification. Additional\nmaterial is available at www.xrce.xerox.com/Deep-Image-Retrieval. \n\n"}
{"id": "1604.03073", "contents": "Title: Reservoir computing for spatiotemporal signal classification without\n  trained output weights Abstract: Reservoir computing is a recently introduced machine learning paradigm that\nhas been shown to be well-suited for the processing of spatiotemporal data.\nRather than training the network node connections and weights via\nbackpropagation in traditional recurrent neural networks, reservoirs instead\nhave fixed connections and weights among the `hidden layer' nodes, and\ntraditionally only the weights to the output layer of neurons are trained using\nlinear regression. We claim that for signal classification tasks one may forgo\nthe weight training step entirely and instead use a simple supervised\nclustering method based upon principal components of norms of reservoir states.\nThe proposed method is mathematically analyzed and explored through numerical\nexperiments on real-world data. The examples demonstrate that the proposed may\noutperform the traditional trained output weight approach in terms of\nclassification accuracy and sensitivity to reservoir parameters. \n\n"}
{"id": "1604.03227", "contents": "Title: Recurrent Attentional Networks for Saliency Detection Abstract: Convolutional-deconvolution networks can be adopted to perform end-to-end\nsaliency detection. But, they do not work well with objects of multiple scales.\nTo overcome such a limitation, in this work, we propose a recurrent attentional\nconvolutional-deconvolution network (RACDNN). Using spatial transformer and\nrecurrent network units, RACDNN is able to iteratively attend to selected image\nsub-regions to perform saliency refinement progressively. Besides tackling the\nscale problem, RACDNN can also learn context-aware features from past\niterations to enhance saliency refinement in future iterations. Experiments on\nseveral challenging saliency detection datasets validate the effectiveness of\nRACDNN, and show that RACDNN outperforms state-of-the-art saliency detection\nmethods. \n\n"}
{"id": "1604.03915", "contents": "Title: Removing Clouds and Recovering Ground Observations in Satellite Image\n  Sequences via Temporally Contiguous Robust Matrix Completion Abstract: We consider the problem of removing and replacing clouds in satellite image\nsequences, which has a wide range of applications in remote sensing. Our\napproach first detects and removes the cloud-contaminated part of the image\nsequences. It then recovers the missing scenes from the clean parts using the\nproposed \"TECROMAC\" (TEmporally Contiguous RObust MAtrix Completion) objective.\nThe objective function balances temporal smoothness with a low rank solution\nwhile staying close to the original observations. The matrix whose the rows are\npixels and columnsare days corresponding to the image, has low-rank because the\npixels reflect land-types such as vegetation, roads and lakes and there are\nrelatively few variations as a result. We provide efficient optimization\nalgorithms for TECROMAC, so we can exploit images containing millions of\npixels. Empirical results on real satellite image sequences, as well as\nsimulated data, demonstrate that our approach is able to recover underlying\nimages from heavily cloud-contaminated observations. \n\n"}
{"id": "1604.06480", "contents": "Title: LOH and behold: Web-scale visual search, recommendation and clustering\n  using Locally Optimized Hashing Abstract: We propose a novel hashing-based matching scheme, called Locally Optimized\nHashing (LOH), based on a state-of-the-art quantization algorithm that can be\nused for efficient, large-scale search, recommendation, clustering, and\ndeduplication. We show that matching with LOH only requires set intersections\nand summations to compute and so is easily implemented in generic distributed\ncomputing systems. We further show application of LOH to: a) large-scale search\ntasks where performance is on par with other state-of-the-art hashing\napproaches; b) large-scale recommendation where queries consisting of thousands\nof images can be used to generate accurate recommendations from collections of\nhundreds of millions of images; and c) efficient clustering with a graph-based\nalgorithm that can be scaled to massive collections in a distributed\nenvironment or can be used for deduplication for small collections, like search\nresults, performing better than traditional hashing approaches while only\nrequiring a few milliseconds to run. In this paper we experiment on datasets of\nup to 100 million images, but in practice our system can scale to larger\ncollections and can be used for other types of data that have a vector\nrepresentation in a Euclidean space. \n\n"}
{"id": "1605.01141", "contents": "Title: Texture Synthesis Through Convolutional Neural Networks and Spectrum\n  Constraints Abstract: This paper presents a significant improvement for the synthesis of texture\nimages using convolutional neural networks (CNNs), making use of constraints on\nthe Fourier spectrum of the results. More precisely, the texture synthesis is\nregarded as a constrained optimization problem, with constraints conditioning\nboth the Fourier spectrum and statistical features learned by CNNs. In contrast\nwith existing methods, the presented method inherits from previous CNN\napproaches the ability to depict local structures and fine scale details, and\nat the same time yields coherent large scale structures, even in the case of\nquasi-periodic images. This is done at no extra computational cost. Synthesis\nexperiments on various images show a clear improvement compared to a recent\nstate-of-the art method relying on CNN constraints only. \n\n"}
{"id": "1605.01156", "contents": "Title: Application of Deep Convolutional Neural Networks for Detecting Extreme\n  Weather in Climate Datasets Abstract: Detecting extreme events in large datasets is a major challenge in climate\nscience research. Current algorithms for extreme event detection are build upon\nhuman expertise in defining events based on subjective thresholds of relevant\nphysical variables. Often, multiple competing methods produce vastly different\nresults on the same dataset. Accurate characterization of extreme events in\nclimate simulations and observational data archives is critical for\nunderstanding the trends and potential impacts of such events in a climate\nchange content. This study presents the first application of Deep Learning\ntechniques as alternative methodology for climate extreme events detection.\nDeep neural networks are able to learn high-level representations of a broad\nclass of patterns from labeled data. In this work, we developed deep\nConvolutional Neural Network (CNN) classification system and demonstrated the\nusefulness of Deep Learning technique for tackling climate pattern detection\nproblems. Coupled with Bayesian based hyper-parameter optimization scheme, our\ndeep CNN system achieves 89\\%-99\\% of accuracy in detecting extreme events\n(Tropical Cyclones, Atmospheric Rivers and Weather Fronts \n\n"}
{"id": "1605.01710", "contents": "Title: Plug-and-Play ADMM for Image Restoration: Fixed Point Convergence and\n  Applications Abstract: Alternating direction method of multiplier (ADMM) is a widely used algorithm\nfor solving constrained optimization problems in image restoration. Among many\nuseful features, one critical feature of the ADMM algorithm is its modular\nstructure which allows one to plug in any off-the-shelf image denoising\nalgorithm for a subproblem in the ADMM algorithm. Because of the plug-in\nnature, this type of ADMM algorithms is coined the name \"Plug-and-Play ADMM\".\nPlug-and-Play ADMM has demonstrated promising empirical results in a number of\nrecent papers. However, it is unclear under what conditions and by using what\ndenoising algorithms would it guarantee convergence. Also, since Plug-and-Play\nADMM uses a specific way to split the variables, it is unclear if fast\nimplementation can be made for common Gaussian and Poissonian image restoration\nproblems.\n  In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixed\npoint convergence. We show that for any denoising algorithm satisfying an\nasymptotic criteria, called bounded denoisers, Plug-and-Play ADMM converges to\na fixed point under a continuation scheme. We also present fast implementations\nfor two image restoration problems on super-resolution and single-photon\nimaging. We compare Plug-and-Play ADMM with state-of-the-art algorithms in each\nproblem type, and demonstrate promising experimental results of the algorithm. \n\n"}
{"id": "1605.02686", "contents": "Title: Unconstrained Still/Video-Based Face Verification with Deep\n  Convolutional Neural Networks Abstract: Over the last five years, methods based on Deep Convolutional Neural Networks\n(DCNNs) have shown impressive performance improvements for object detection and\nrecognition problems. This has been made possible due to the availability of\nlarge annotated datasets, a better understanding of the non-linear mapping\nbetween input images and class labels as well as the affordability of GPUs. In\nthis paper, we present the design details of a deep learning system for\nunconstrained face recognition, including modules for face detection,\nassociation, alignment and face verification. The quantitative performance\nevaluation is conducted using the IARPA Janus Benchmark A (IJB-A), the JANUS\nChallenge Set 2 (JANUS CS2), and the LFW dataset. The IJB-A dataset includes\nreal-world unconstrained faces of 500 subjects with significant pose and\nillumination variations which are much harder than the Labeled Faces in the\nWild (LFW) and Youtube Face (YTF) datasets. JANUS CS2 is the extended version\nof IJB-A which contains not only all the images/frames of IJB-A but also\nincludes the original videos for evaluating the video-based face verification\nsystem. Some open issues regarding DCNNs for face verification problems are\nthen discussed. \n\n"}
{"id": "1605.07133", "contents": "Title: Towards Multi-Agent Communication-Based Language Learning Abstract: We propose an interactive multimodal framework for language learning. Instead\nof being passively exposed to large amounts of natural text, our learners\n(implemented as feed-forward neural networks) engage in cooperative referential\ngames starting from a tabula rasa setup, and thus develop their own language\nfrom the need to communicate in order to succeed at the game. Preliminary\nexperiments provide promising results, but also suggest that it is important to\nensure that agents trained in this way do not develop an adhoc communication\ncode only effective for the game they are playing \n\n"}
{"id": "1605.07139", "contents": "Title: Fairness in Learning: Classic and Contextual Bandits Abstract: We introduce the study of fairness in multi-armed bandit problems. Our\nfairness definition can be interpreted as demanding that given a pool of\napplicants (say, for college admission or mortgages), a worse applicant is\nnever favored over a better one, despite a learning algorithm's uncertainty\nover the true payoffs. We prove results of two types.\n  First, in the important special case of the classic stochastic bandits\nproblem (i.e., in which there are no contexts), we provide a provably fair\nalgorithm based on \"chained\" confidence intervals, and provide a cumulative\nregret bound with a cubic dependence on the number of arms. We further show\nthat any fair algorithm must have such a dependence. When combined with regret\nbounds for standard non-fair algorithms such as UCB, this proves a strong\nseparation between fair and unfair learning, which extends to the general\ncontextual case.\n  In the general contextual case, we prove a tight connection between fairness\nand the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class\nof functions can be transformed into a provably fair contextual bandit\nalgorithm, and conversely any fair contextual bandit algorithm can be\ntransformed into a KWIK learning algorithm. This tight connection allows us to\nprovide a provably fair algorithm for the linear contextual bandit problem with\na polynomial dependence on the dimension, and to show (for a different class of\nfunctions) a worst-case exponential gap in regret between fair and non-fair\nlearning algorithms \n\n"}
{"id": "1605.08233", "contents": "Title: Stochastic Variance Reduced Riemannian Eigensolver Abstract: We study the stochastic Riemannian gradient algorithm for matrix\neigen-decomposition. The state-of-the-art stochastic Riemannian algorithm\nrequires the learning rate to decay to zero and thus suffers from slow\nconvergence and sub-optimal solutions. In this paper, we address this issue by\ndeploying the variance reduction (VR) technique of stochastic gradient descent\n(SGD). The technique was originally developed to solve convex problems in the\nEuclidean space. We generalize it to Riemannian manifolds and realize it to\nsolve the non-convex eigen-decomposition problem. We are the first to propose\nand analyze the generalization of SVRG to Riemannian manifolds. Specifically,\nwe propose the general variance reduction form, SVRRG, in the framework of the\nstochastic Riemannian gradient optimization. It's then specialized to the\nproblem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide a\nnovel and elegant theoretical analysis on this algorithm. The theory shows that\na fixed learning rate can be used in the Riemannian setting with an exponential\nglobal convergence rate guaranteed. The theoretical results make a significant\nimprovement over existing studies, with the effectiveness empirically verified. \n\n"}
{"id": "1605.08247", "contents": "Title: cvpaper.challenge in 2015 - A review of CVPR2015 and DeepSurvey Abstract: The \"cvpaper.challenge\" is a group composed of members from AIST, Tokyo Denki\nUniv. (TDU), and Univ. of Tsukuba that aims to systematically summarize papers\non computer vision, pattern recognition, and related fields. For this\nparticular review, we focused on reading the ALL 602 conference papers\npresented at the CVPR2015, the premier annual computer vision event held in\nJune 2015, in order to grasp the trends in the field. Further, we are proposing\n\"DeepSurvey\" as a mechanism embodying the entire process from the reading\nthrough all the papers, the generation of ideas, and to the writing of paper. \n\n"}
{"id": "1605.08831", "contents": "Title: Weighted Residuals for Very Deep Networks Abstract: Deep residual networks have recently shown appealing performance on many\nchallenging computer vision tasks. However, the original residual structure\nstill has some defects making it difficult to converge on very deep networks.\nIn this paper, we introduce a weighted residual network to address the\nincompatibility between \\texttt{ReLU} and element-wise addition and the deep\nnetwork initialization problem. The weighted residual network is able to learn\nto combine residuals from different layers effectively and efficiently. The\nproposed models enjoy a consistent improvement over accuracy and convergence\nwith increasing depths from 100+ layers to 1000+ layers. Besides, the weighted\nresidual networks have little more computation and GPU memory burden than the\noriginal residual networks. The networks are optimized by projected stochastic\ngradient descent. Experiments on CIFAR-10 have shown that our algorithm has a\n\\emph{faster convergence speed} than the original residual networks and reaches\na \\emph{high accuracy} at 95.3\\% with a 1192-layer model. \n\n"}
{"id": "1605.08912", "contents": "Title: A Riemannian Framework for Statistical Analysis of Topological\n  Persistence Diagrams Abstract: Topological data analysis is becoming a popular way to study high dimensional\nfeature spaces without any contextual clues or assumptions. This paper concerns\nitself with one popular topological feature, which is the number of\n$d-$dimensional holes in the dataset, also known as the Betti$-d$ number. The\npersistence of the Betti numbers over various scales is encoded into a\npersistence diagram (PD), which indicates the birth and death times of these\nholes as scale varies. A common way to compare PDs is by a point-to-point\nmatching, which is given by the $n$-Wasserstein metric. However, a big drawback\nof this approach is the need to solve correspondence between points before\ncomputing the distance; for $n$ points, the complexity grows according to\n$\\mathcal{O}($n$^3)$. Instead, we propose to use an entirely new framework\nbuilt on Riemannian geometry, that models PDs as 2D probability density\nfunctions that are represented in the square-root framework on a Hilbert\nSphere. The resulting space is much more intuitive with closed form expressions\nfor common operations. The distance metric is 1) correspondence-free and also\n2) independent of the number of points in the dataset. The complexity of\ncomputing distance between PDs now grows according to $\\mathcal{O}(K^2)$, for a\n$K \\times K$ discretization of $[0,1]^2$. This also enables the use of existing\nmachinery in differential geometry towards statistical analysis of PDs such as\ncomputing the mean, geodesics, classification etc. We report competitive\nresults with the Wasserstein metric, at a much lower computational load,\nindicating the favorable properties of the proposed approach. \n\n"}
{"id": "1605.09046", "contents": "Title: TripleSpin - a generic compact paradigm for fast machine learning\n  computations Abstract: We present a generic compact computational framework relying on structured\nrandom matrices that can be applied to speed up several machine learning\nalgorithms with almost no loss of accuracy. The applications include new fast\nLSH-based algorithms, efficient kernel computations via random feature maps,\nconvex optimization algorithms, quantization techniques and many more. Certain\nmodels of the presented paradigm are even more compressible since they apply\nonly bit matrices. This makes them suitable for deploying on mobile devices.\nAll our findings come with strong theoretical guarantees. In particular, as a\nbyproduct of the presented techniques and by using relatively new\nBerry-Esseen-type CLT for random vectors, we give the first theoretical\nguarantees for one of the most efficient existing LSH algorithms based on the\n$\\textbf{HD}_{3}\\textbf{HD}_{2}\\textbf{HD}_{1}$ structured matrix (\"Practical\nand Optimal LSH for Angular Distance\"). These guarantees as well as theoretical\nresults for other aforementioned applications follow from the same general\ntheoretical principle that we present in the paper. Our structured family\ncontains as special cases all previously considered structured schemes,\nincluding the recently introduced $P$-model. Experimental evaluation confirms\nthe accuracy and efficiency of TripleSpin matrices. \n\n"}
{"id": "1605.09673", "contents": "Title: Dynamic Filter Networks Abstract: In a traditional convolutional layer, the learned filters stay fixed after\ntraining. In contrast, we introduce a new framework, the Dynamic Filter\nNetwork, where filters are generated dynamically conditioned on an input. We\nshow that this architecture is a powerful one, with increased flexibility\nthanks to its adaptive nature, yet without an excessive increase in the number\nof model parameters. A wide variety of filtering operations can be learned this\nway, including local spatial transformations, but also others like selective\n(de)blurring or adaptive feature extraction. Moreover, multiple such layers can\nbe combined, e.g. in a recurrent architecture. We demonstrate the effectiveness\nof the dynamic filter network on the tasks of video and stereo prediction, and\nreach state-of-the-art performance on the moving MNIST dataset with a much\nsmaller model. By visualizing the learned filters, we illustrate that the\nnetwork has picked up flow information by only looking at unlabelled training\ndata. This suggests that the network can be used to pretrain networks for\nvarious supervised tasks in an unsupervised way, like optical flow and depth\nestimation. \n\n"}
{"id": "1605.09674", "contents": "Title: VIME: Variational Information Maximizing Exploration Abstract: Scalable and effective exploration remains a key challenge in reinforcement\nlearning (RL). While there are methods with optimality guarantees in the\nsetting of discrete state and action spaces, these methods cannot be applied in\nhigh-dimensional deep RL scenarios. As such, most contemporary RL relies on\nsimple heuristics such as epsilon-greedy exploration or adding Gaussian noise\nto the controls. This paper introduces Variational Information Maximizing\nExploration (VIME), an exploration strategy based on maximization of\ninformation gain about the agent's belief of environment dynamics. We propose a\npractical implementation, using variational inference in Bayesian neural\nnetworks which efficiently handles continuous state and action spaces. VIME\nmodifies the MDP reward function, and can be applied with several different\nunderlying RL algorithms. We demonstrate that VIME achieves significantly\nbetter performance compared to heuristic exploration methods across a variety\nof continuous control tasks and algorithms, including tasks with very sparse\nrewards. \n\n"}
{"id": "1606.00800", "contents": "Title: Multi-View Treelet Transform Abstract: Current multi-view factorization methods make assumptions that are not\nacceptable for many kinds of data, and in particular, for graphical data with\nhierarchical structure. At the same time, current hierarchical methods work\nonly in the single-view setting. We generalize the Treelet Transform to the\nMulti-View Treelet Transform (MVTT) to allow for the capture of hierarchical\nstructure when multiple views are available. Further, we show how this\ngeneralization is consistent with the existing theory and how it might be used\nin denoising empirical networks and in computing the shared response of\nfunctional brain data. \n\n"}
{"id": "1606.02894", "contents": "Title: A Comprehensive Analysis of Deep Learning Based Representation for Face\n  Recognition Abstract: Deep learning based approaches have been dominating the face recognition\nfield due to the significant performance improvement they have provided on the\nchallenging wild datasets. These approaches have been extensively tested on\nsuch unconstrained datasets, on the Labeled Faces in the Wild and YouTube\nFaces, to name a few. However, their capability to handle individual appearance\nvariations caused by factors such as head pose, illumination, occlusion, and\nmisalignment has not been thoroughly assessed till now. In this paper, we\npresent a comprehensive study to evaluate the performance of deep learning\nbased face representation under several conditions including the varying head\npose angles, upper and lower face occlusion, changing illumination of different\nstrengths, and misalignment due to erroneous facial feature localization. Two\nsuccessful and publicly available deep learning models, namely VGG-Face and\nLightened CNN have been utilized to extract face representations. The obtained\nresults show that although deep learning provides a powerful representation for\nface recognition, it can still benefit from preprocessing, for example, for\npose and illumination normalization in order to achieve better performance\nunder various conditions. Particularly, if these variations are not included in\nthe dataset used to train the deep learning model, the role of preprocessing\nbecomes more crucial. Experimental results also show that deep learning based\nrepresentation is robust to misalignment and can tolerate facial feature\nlocalization errors up to 10% of the interocular distance. \n\n"}
{"id": "1606.03657", "contents": "Title: InfoGAN: Interpretable Representation Learning by Information Maximizing\n  Generative Adversarial Nets Abstract: This paper describes InfoGAN, an information-theoretic extension to the\nGenerative Adversarial Network that is able to learn disentangled\nrepresentations in a completely unsupervised manner. InfoGAN is a generative\nadversarial network that also maximizes the mutual information between a small\nsubset of the latent variables and the observation. We derive a lower bound to\nthe mutual information objective that can be optimized efficiently, and show\nthat our training procedure can be interpreted as a variation of the Wake-Sleep\nalgorithm. Specifically, InfoGAN successfully disentangles writing styles from\ndigit shapes on the MNIST dataset, pose from lighting of 3D rendered images,\nand background digits from the central digit on the SVHN dataset. It also\ndiscovers visual concepts that include hair styles, presence/absence of\neyeglasses, and emotions on the CelebA face dataset. Experiments show that\nInfoGAN learns interpretable representations that are competitive with\nrepresentations learned by existing fully supervised methods. \n\n"}
{"id": "1606.04189", "contents": "Title: Inverting face embeddings with convolutional neural networks Abstract: Deep neural networks have dramatically advanced the state of the art for many\nareas of machine learning. Recently they have been shown to have a remarkable\nability to generate highly complex visual artifacts such as images and text\nrather than simply recognize them.\n  In this work we use neural networks to effectively invert low-dimensional\nface embeddings while producing realistically looking consistent images. Our\ncontribution is twofold, first we show that a gradient ascent style approaches\ncan be used to reproduce consistent images, with a help of a guiding image.\nSecond, we demonstrate that we can train a separate neural network to\neffectively solve the minimization problem in one pass, and generate images in\nreal-time. We then evaluate the loss imposed by using a neural network instead\nof the gradient descent by comparing the final values of the minimized loss\nfunction. \n\n"}
{"id": "1606.04450", "contents": "Title: Multiple Human Tracking in RGB-D Data: A Survey Abstract: Multiple human tracking (MHT) is a fundamental task in many computer vision\napplications. Appearance-based approaches, primarily formulated on RGB data,\nare constrained and affected by problems arising from occlusions and/or\nillumination variations. In recent years, the arrival of cheap RGB-Depth\n(RGB-D) devices has {led} to many new approaches to MHT, and many of these\nintegrate color and depth cues to improve each and every stage of the process.\nIn this survey, we present the common processing pipeline of these methods and\nreview their methodology based (a) on how they implement this pipeline and (b)\non what role depth plays within each stage of it. We identify and introduce\nexisting, publicly available, benchmark datasets and software resources that\nfuse color and depth data for MHT. Finally, we present a brief comparative\nevaluation of the performance of those works that have applied their methods to\nthese datasets. \n\n"}
{"id": "1606.04801", "contents": "Title: A Powerful Generative Model Using Random Weights for the Deep Image\n  Representation Abstract: To what extent is the success of deep visualization due to the training?\nCould we do deep visualization using untrained, random weight networks? To\naddress this issue, we explore new and powerful generative models for three\npopular deep visualization tasks using untrained, random weight convolutional\nneural networks. First we invert representations in feature spaces and\nreconstruct images from white noise inputs. The reconstruction quality is\nstatistically higher than that of the same method applied on well trained\nnetworks with the same architecture. Next we synthesize textures using scaled\ncorrelations of representations in multiple layers and our results are almost\nindistinguishable with the original natural texture and the synthesized\ntextures based on the trained network. Third, by recasting the content of an\nimage in the style of various artworks, we create artistic images with high\nperceptual quality, highly competitive to the prior work of Gatys et al. on\npretrained networks. To our knowledge this is the first demonstration of image\nrepresentations using untrained deep neural networks. Our work provides a new\nand fascinating tool to study the representation of deep network architecture\nand sheds light on new understandings on deep visualization. \n\n"}
{"id": "1606.06108", "contents": "Title: DualNet: Domain-Invariant Network for Visual Question Answering Abstract: Visual question answering (VQA) task not only bridges the gap between images\nand language, but also requires that specific contents within the image are\nunderstood as indicated by linguistic context of the question, in order to\ngenerate the accurate answers. Thus, it is critical to build an efficient\nembedding of images and texts. We implement DualNet, which fully takes\nadvantage of discriminative power of both image and textual features by\nseparately performing two operations. Building an ensemble of DualNet further\nboosts the performance. Contrary to common belief, our method proved effective\nin both real images and abstract scenes, in spite of significantly different\nproperties of respective domain. Our method was able to outperform previous\nstate-of-the-art methods in real images category even without explicitly\nemploying attention mechanism, and also outperformed our own state-of-the-art\nmethod in abstract scenes category, which recently won the first place in VQA\nChallenge 2016. \n\n"}
{"id": "1606.07230", "contents": "Title: Deep Learning Markov Random Field for Semantic Segmentation Abstract: Semantic segmentation tasks can be well modeled by Markov Random Field (MRF).\nThis paper addresses semantic segmentation by incorporating high-order\nrelations and mixture of label contexts into MRF. Unlike previous works that\noptimized MRFs using iterative algorithm, we solve MRF by proposing a\nConvolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which\nenables deterministic end-to-end computation in a single forward pass.\nSpecifically, DPN extends a contemporary CNN to model unary terms and\nadditional layers are devised to approximate the mean field (MF) algorithm for\npairwise terms. It has several appealing properties. First, different from the\nrecent works that required many iterations of MF during back-propagation, DPN\nis able to achieve high performance by approximating one iteration of MF.\nSecond, DPN represents various types of pairwise terms, making many existing\nmodels as its special cases. Furthermore, pairwise terms in DPN provide a\nunified framework to encode rich contextual information in high-dimensional\ndata, such as images and videos. Third, DPN makes MF easier to be parallelized\nand speeded up, thus enabling efficient inference. DPN is thoroughly evaluated\non standard semantic image/video segmentation benchmarks, where a single DPN\nmodel yields state-of-the-art segmentation accuracies on PASCAL VOC 2012,\nCityscapes dataset and CamVid dataset. \n\n"}
{"id": "1606.07575", "contents": "Title: Multipartite Ranking-Selection of Low-Dimensional Instances by\n  Supervised Projection to High-Dimensional Space Abstract: Pruning of redundant or irrelevant instances of data is a key to every\nsuccessful solution for pattern recognition. In this paper, we present a novel\nranking-selection framework for low-length but highly correlated instances.\nInstead of working in the low-dimensional instance space, we learn a supervised\nprojection to high-dimensional space spanned by the number of classes in the\ndataset under study. Imposing higher distinctions via exposing the notion of\nlabels to the instances, lets to deploy one versus all ranking for each\nindividual classes and selecting quality instances via adaptive thresholding of\nthe overall scores. To prove the efficiency of our paradigm, we employ it for\nthe purpose of texture understanding which is a hard recognition challenge due\nto high similarity of texture pixels and low dimensionality of their color\nfeatures. Our experiments show considerable improvements in recognition\nperformance over other local descriptors on several publicly available\ndatasets. \n\n"}
{"id": "1607.01059", "contents": "Title: Improving Sparse Representation-Based Classification Using Local\n  Principal Component Analysis Abstract: Sparse representation-based classification (SRC), proposed by Wright et al.,\nseeks the sparsest decomposition of a test sample over the dictionary of\ntraining samples, with classification to the most-contributing class. Because\nit assumes test samples can be written as linear combinations of their\nsame-class training samples, the success of SRC depends on the size and\nrepresentativeness of the training set. Our proposed classification algorithm\nenlarges the training set by using local principal component analysis to\napproximate the basis vectors of the tangent hyperplane of the class manifold\nat each training sample. The dictionary in SRC is replaced by a local\ndictionary that adapts to the test sample and includes training samples and\ntheir corresponding tangent basis vectors. We use a synthetic data set and\nthree face databases to demonstrate that this method can achieve higher\nclassification accuracy than SRC in cases of sparse sampling, nonlinear class\nmanifolds, and stringent dimension reduction. \n\n"}
{"id": "1607.01719", "contents": "Title: Deep CORAL: Correlation Alignment for Deep Domain Adaptation Abstract: Deep neural networks are able to learn powerful representations from large\nquantities of labeled input data, however they cannot always generalize well\nacross changes in input distributions. Domain adaptation algorithms have been\nproposed to compensate for the degradation in performance due to domain shift.\nIn this paper, we address the case when the target domain is unlabeled,\nrequiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised\ndomain adaptation method that aligns the second-order statistics of the source\nand target distributions with a linear transformation. Here, we extend CORAL to\nlearn a nonlinear transformation that aligns correlations of layer activations\nin deep neural networks (Deep CORAL). Experiments on standard benchmark\ndatasets show state-of-the-art performance. \n\n"}
{"id": "1607.01855", "contents": "Title: Iterative Multi-domain Regularized Deep Learning for Anatomical\n  Structure Detection and Segmentation from Ultrasound Images Abstract: Accurate detection and segmentation of anatomical structures from ultrasound\nimages are crucial for clinical diagnosis and biometric measurements. Although\nultrasound imaging has been widely used with superiorities such as low cost and\nportability, the fuzzy border definition and existence of abounding artifacts\npose great challenges for automatically detecting and segmenting the complex\nanatomical structures. In this paper, we propose a multi-domain regularized\ndeep learning method to address this challenging problem. By leveraging the\ntransfer learning from cross domains, the feature representations are\neffectively enhanced. The results are further improved by the iterative\nrefinement. Moreover, our method is quite efficient by taking advantage of a\nfully convolutional network, which is formulated as an end-to-end learning\nframework of detection and segmentation. Extensive experimental results on a\nlarge-scale database corroborated that our method achieved a superior detection\nand segmentation accuracy, outperforming other methods by a significant margin\nand demonstrating competitive capability even compared to human performance. \n\n"}
{"id": "1607.03682", "contents": "Title: Hierarchical learning for DNN-based acoustic scene classification Abstract: In this paper, we present a deep neural network (DNN)-based acoustic scene\nclassification framework. Two hierarchical learning methods are proposed to\nimprove the DNN baseline performance by incorporating the hierarchical taxonomy\ninformation of environmental sounds. Firstly, the parameters of the DNN are\ninitialized by the proposed hierarchical pre-training. Multi-level objective\nfunction is then adopted to add more constraint on the cross-entropy based loss\nfunction. A series of experiments were conducted on the Task1 of the Detection\nand Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The\nfinal DNN-based system achieved a 22.9% relative improvement on average scene\nclassification error as compared with the Gaussian Mixture Model (GMM)-based\nbenchmark system across four standard folds. \n\n"}
{"id": "1607.03827", "contents": "Title: The KIT Motion-Language Dataset Abstract: Linking human motion and natural language is of great interest for the\ngeneration of semantic representations of human activities as well as for the\ngeneration of robot activities based on natural language input. However, while\nthere have been years of research in this area, no standardized and openly\navailable dataset exists to support the development and evaluation of such\nsystems. We therefore propose the KIT Motion-Language Dataset, which is large,\nopen, and extensible. We aggregate data from multiple motion capture databases\nand include them in our dataset using a unified representation that is\nindependent of the capture system or marker set, making it easy to work with\nthe data regardless of its origin. To obtain motion annotations in natural\nlanguage, we apply a crowd-sourcing approach and a web-based tool that was\nspecifically build for this purpose, the Motion Annotation Tool. We thoroughly\ndocument the annotation process itself and discuss gamification methods that we\nused to keep annotators motivated. We further propose a novel method,\nperplexity-based selection, which systematically selects motions for further\nannotation that are either under-represented in our dataset or that have\nerroneous annotations. We show that our method mitigates the two aforementioned\nproblems and ensures a systematic annotation process. We provide an in-depth\nanalysis of the structure and contents of our resulting dataset, which, as of\nOctober 10, 2016, contains 3911 motions with a total duration of 11.23 hours\nand 6278 annotations in natural language that contain 52,903 words. We believe\nthis makes our dataset an excellent choice that enables more transparent and\ncomparable research in this important area. \n\n"}
{"id": "1607.05691", "contents": "Title: Information-theoretical label embeddings for large-scale image\n  classification Abstract: We present a method for training multi-label, massively multi-class image\nclassification models, that is faster and more accurate than supervision via a\nsigmoid cross-entropy loss (logistic regression). Our method consists in\nembedding high-dimensional sparse labels onto a lower-dimensional dense sphere\nof unit-normed vectors, and treating the classification problem as a cosine\nproximity regression problem on this sphere. We test our method on a dataset of\n300 million high-resolution images with 17,000 labels, where it yields\nconsiderably faster convergence, as well as a 7% higher mean average precision\ncompared to logistic regression. \n\n"}
{"id": "1607.05910", "contents": "Title: Visual Question Answering: A Survey of Methods and Datasets Abstract: Visual Question Answering (VQA) is a challenging task that has received\nincreasing attention from both the computer vision and the natural language\nprocessing communities. Given an image and a question in natural language, it\nrequires reasoning over visual elements of the image and general knowledge to\ninfer the correct answer. In the first part of this survey, we examine the\nstate of the art by comparing modern approaches to the problem. We classify\nmethods by their mechanism to connect the visual and textual modalities. In\nparticular, we examine the common approach of combining convolutional and\nrecurrent neural networks to map images and questions to a common feature\nspace. We also discuss memory-augmented and modular architectures that\ninterface with structured knowledge bases. In the second part of this survey,\nwe review the datasets available for training and evaluating VQA systems. The\nvarious datatsets contain questions at different levels of complexity, which\nrequire different capabilities and types of reasoning. We examine in depth the\nquestion/answer pairs from the Visual Genome project, and evaluate the\nrelevance of the structured annotations of images with scene graphs for VQA.\nFinally, we discuss promising future directions for the field, in particular\nthe connection to structured knowledge bases and the use of natural language\nprocessing models. \n\n"}
{"id": "1607.07032", "contents": "Title: Is Faster R-CNN Doing Well for Pedestrian Detection? Abstract: Detecting pedestrian has been arguably addressed as a special topic beyond\ngeneral object detection. Although recent deep learning object detectors such\nas Fast/Faster R-CNN [1, 2] have shown excellent performance for general object\ndetection, they have limited success for detecting pedestrian, and previous\nleading pedestrian detectors were in general hybrid methods combining\nhand-crafted and deep convolutional features. In this paper, we investigate\nissues involving Faster R-CNN [2] for pedestrian detection. We discover that\nthe Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a\nstand-alone pedestrian detector, but surprisingly, the downstream classifier\ndegrades the results. We argue that two reasons account for the unsatisfactory\naccuracy: (i) insufficient resolution of feature maps for handling small\ninstances, and (ii) lack of any bootstrapping strategy for mining hard negative\nexamples. Driven by these observations, we propose a very simple but effective\nbaseline for pedestrian detection, using an RPN followed by boosted forests on\nshared, high-resolution convolutional feature maps. We comprehensively evaluate\nthis method on several benchmarks (Caltech, INRIA, ETH, and KITTI), presenting\ncompetitive accuracy and good speed. Code will be made publicly available. \n\n"}
{"id": "1607.07215", "contents": "Title: DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation Abstract: In this work, we consider the task of generating highly-realistic images of a\ngiven face with a redirected gaze. We treat this problem as a specific instance\nof conditional image generation and suggest a new deep architecture that can\nhandle this task very well as revealed by numerical comparison with prior art\nand a user study. Our deep architecture performs coarse-to-fine warping with an\nadditional intensity correction of individual pixels. All these operations are\nperformed in a feed-forward manner, and the parameters associated with\ndifferent operations are learned jointly in the end-to-end fashion. After\nlearning, the resulting neural network can synthesize images with manipulated\ngaze, while the redirection angle can be selected arbitrarily from a certain\nrange and provided as an input to the network. \n\n"}
{"id": "1608.00797", "contents": "Title: CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016 Abstract: This paper presents the method that underlies our submission to the untrimmed\nvideo classification task of ActivityNet Challenge 2016. We follow the basic\npipeline of temporal segment networks and further raise the performance via a\nnumber of other techniques. Specifically, we use the latest deep model\narchitecture, e.g., ResNet and Inception V3, and introduce new aggregation\nschemes (top-k and attention-weighted pooling). Additionally, we incorporate\nthe audio as a complementary channel, extracting relevant information via a CNN\napplied to the spectrograms. With these techniques, we derive an ensemble of\ndeep models, which, together, attains a high classification accuracy (mAP\n$93.23\\%$) on the testing set and secured the first place in the challenge. \n\n"}
{"id": "1608.00905", "contents": "Title: PicHunt: Social Media Image Retrieval for Improved Law Enforcement Abstract: First responders are increasingly using social media to identify and reduce\ncrime for well-being and safety of the society. Images shared on social media\nhurting religious, political, communal and other sentiments of people, often\ninstigate violence and create law & order situations in society. This results\nin the need for first responders to inspect the spread of such images and users\npropagating them on social media. In this paper, we present a comparison\nbetween different hand-crafted features and a Convolutional Neural Network\n(CNN) model to retrieve similar images, which outperforms state-of-art\nhand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time\nimage search system, robust to retrieve modified images that allows first\nresponders to analyze the current spread of images, sentiments floating and\ndetails of users propagating such content. The system also aids officials to\nsave time of manually analyzing the content by reducing the search space on an\naverage by 67%. \n\n"}
{"id": "1608.01230", "contents": "Title: Learning a Driving Simulator Abstract: Comma.ai's approach to Artificial Intelligence for self-driving cars is based\non an agent that learns to clone driver behaviors and plans maneuvers by\nsimulating future events in the road. This paper illustrates one of our\nresearch approaches for driving simulation. One where we learn to simulate.\nHere we investigate variational autoencoders with classical and learned cost\nfunctions using generative adversarial networks for embedding road frames.\nAfterwards, we learn a transition model in the embedded space using action\nconditioned Recurrent Neural Networks. We show that our approach can keep\npredicting realistic looking video for several frames despite the transition\nmodel being optimized without a cost function in the pixel space. \n\n"}
{"id": "1608.01471", "contents": "Title: UnitBox: An Advanced Object Detection Network Abstract: In present object detection systems, the deep convolutional neural networks\n(CNNs) are utilized to predict bounding boxes of object candidates, and have\ngained performance advantages over the traditional region proposal methods.\nHowever, existing deep CNN methods assume the object bounds to be four\nindependent variables, which could be regressed by the $\\ell_2$ loss\nseparately. Such an oversimplified assumption is contrary to the well-received\nobservation, that those variables are correlated, resulting to less accurate\nlocalization. To address the issue, we firstly introduce a novel Intersection\nover Union ($IoU$) loss function for bounding box prediction, which regresses\nthe four bounds of a predicted box as a whole unit. By taking the advantages of\n$IoU$ loss and deep fully convolutional networks, the UnitBox is introduced,\nwhich performs accurate and efficient localization, shows robust to objects of\nvaried shapes and scales, and converges fast. We apply UnitBox on face\ndetection task and achieve the best performance among all published methods on\nthe FDDB benchmark. \n\n"}
{"id": "1608.02052", "contents": "Title: Multi-Model Hypothesize-and-Verify Approach for Incremental Loop Closure\n  Verification Abstract: Loop closure detection, which is the task of identifying locations revisited\nby a robot in a sequence of odometry and perceptual observations, is typically\nformulated as a visual place recognition (VPR) task. However, even\nstate-of-the-art VPR techniques generate a considerable number of false\npositives as a result of confusing visual features and perceptual aliasing. In\nthis paper, we propose a robust incremental framework for loop closure\ndetection, termed incremental loop closure verification. Our approach\nreformulates the problem of loop closure detection as an instance of a\nmulti-model hypothesize-and-verify framework, in which multiple loop closure\nhypotheses are generated and verified in terms of the consistency between loop\nclosure hypotheses and VPR constraints at multiple viewpoints along the robot's\ntrajectory. Furthermore, we consider the general incremental setting of loop\nclosure detection, in which the system must update both the set of VPR\nconstraints and that of loop closure hypotheses when new constraints or\nhypotheses arrive during robot navigation. Experimental results using a stereo\nSLAM system and DCNN features and visual odometry validate effectiveness of the\nproposed approach. \n\n"}
{"id": "1608.02128", "contents": "Title: Spoofing 2D Face Detection: Machines See People Who Aren't There Abstract: Machine learning is increasingly used to make sense of the physical world yet\nmay suffer from adversarial manipulation. We examine the Viola-Jones 2D face\ndetection algorithm to study whether images can be created that humans do not\nnotice as faces yet the algorithm detects as faces. We show that it is possible\nto construct images that Viola-Jones recognizes as containing faces yet no\nhuman would consider a face. Moreover, we show that it is possible to construct\nimages that fool facial detection even when they are printed and then\nphotographed. \n\n"}
{"id": "1608.02693", "contents": "Title: Deeply Semantic Inductive Spatio-Temporal Learning Abstract: We present an inductive spatio-temporal learning framework rooted in\ninductive logic programming. With an emphasis on visuo-spatial language, logic,\nand cognition, the framework supports learning with relational spatio-temporal\nfeatures identifiable in a range of domains involving the processing and\ninterpretation of dynamic visuo-spatial imagery. We present a prototypical\nsystem, and an example application in the domain of computing for visual arts\nand computational cognitive science. \n\n"}
{"id": "1608.03667", "contents": "Title: Reasoning and Algorithm Selection Augmented Symbolic Segmentation Abstract: In this paper we present an alternative method to symbolic segmentation: we\napproach symbolic segmentation as an algorithm selection problem. That is, let\nthere be a set A of available algorithms for symbolic segmentation, a set of\ninput features $F$, a set of image attribute $\\mathbb{A}$ and a selection\nmechanism $S(F,\\mathbb{A},A)$ that selects on a case by case basis the best\nalgorithm. The semantic segmentation is then an optimization process that\ncombines best component segments from multiple results into a single optimal\nresult. The experiments compare three different algorithm selection mechanisms\nusing three selected semantic segmentation algorithms. The results show that\nusing the current state of art algorithms and relatively low accuracy of\nalgorithm selection the accuracy of the semantic segmentation can be improved\nby 2\\%. \n\n"}
{"id": "1608.04493", "contents": "Title: Dynamic Network Surgery for Efficient DNNs Abstract: Deep learning has become a ubiquitous technology to improve machine\nintelligence. However, most of the existing deep models are structurally very\ncomplex, making them difficult to be deployed on the mobile platforms with\nlimited computational power. In this paper, we propose a novel network\ncompression method called dynamic network surgery, which can remarkably reduce\nthe network complexity by making on-the-fly connection pruning. Unlike the\nprevious methods which accomplish this task in a greedy way, we properly\nincorporate connection splicing into the whole process to avoid incorrect\npruning and make it as a continual network maintenance. The effectiveness of\nour method is proved with experiments. Without any accuracy loss, our method\ncan efficiently compress the number of parameters in LeNet-5 and AlexNet by a\nfactor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it\noutperforms the recent pruning method by considerable margins. Code and some\nmodels are available at https://github.com/yiwenguo/Dynamic-Network-Surgery. \n\n"}
{"id": "1608.04644", "contents": "Title: Towards Evaluating the Robustness of Neural Networks Abstract: Neural networks provide state-of-the-art results for most machine learning\ntasks. Unfortunately, neural networks are vulnerable to adversarial examples:\ngiven an input $x$ and any target classification $t$, it is possible to find a\nnew input $x'$ that is similar to $x$ but classified as $t$. This makes it\ndifficult to apply neural networks in security-critical areas. Defensive\ndistillation is a recently proposed approach that can take an arbitrary neural\nnetwork, and increase its robustness, reducing the success rate of current\nattacks' ability to find adversarial examples from $95\\%$ to $0.5\\%$.\n  In this paper, we demonstrate that defensive distillation does not\nsignificantly increase the robustness of neural networks by introducing three\nnew attack algorithms that are successful on both distilled and undistilled\nneural networks with $100\\%$ probability. Our attacks are tailored to three\ndistance metrics used previously in the literature, and when compared to\nprevious adversarial example generation algorithms, our attacks are often much\nmore effective (and never worse). Furthermore, we propose using high-confidence\nadversarial examples in a simple transferability test we show can also be used\nto break defensive distillation. We hope our attacks will be used as a\nbenchmark in future defense attempts to create neural networks that resist\nadversarial examples. \n\n"}
{"id": "1608.05081", "contents": "Title: BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for\n  Task-Oriented Dialogue Systems Abstract: We present a new algorithm that significantly improves the efficiency of\nexploration for deep Q-learning agents in dialogue systems. Our agents explore\nvia Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop\nneural network. Our algorithm learns much faster than common exploration\nstrategies such as $\\epsilon$-greedy, Boltzmann, bootstrapping, and\nintrinsic-reward-based ones. Additionally, we show that spiking the replay\nbuffer with experiences from just a few successful episodes can make Q-learning\nfeasible when it might otherwise fail. \n\n"}
{"id": "1608.05275", "contents": "Title: A Tight Convex Upper Bound on the Likelihood of a Finite Mixture Abstract: The likelihood function of a finite mixture model is a non-convex function\nwith multiple local maxima and commonly used iterative algorithms such as EM\nwill converge to different solutions depending on initial conditions. In this\npaper we ask: is it possible to assess how far we are from the global maximum\nof the likelihood? Since the likelihood of a finite mixture model can grow\nunboundedly by centering a Gaussian on a single datapoint and shrinking the\ncovariance, we constrain the problem by assuming that the parameters of the\nindividual models are members of a large discrete set (e.g. estimating a\nmixture of two Gaussians where the means and variances of both Gaussians are\nmembers of a set of a million possible means and variances). For this setting\nwe show that a simple upper bound on the likelihood can be computed using\nconvex optimization and we analyze conditions under which the bound is\nguaranteed to be tight. This bound can then be used to assess the quality of\nsolutions found by EM (where the final result is projected on the discrete set)\nor any other mixture estimation algorithm. For any dataset our method allows us\nto find a finite mixture model together with a dataset-specific bound on how\nfar the likelihood of this mixture is from the global optimum of the likelihood \n\n"}
{"id": "1608.05889", "contents": "Title: Online Feature Selection with Group Structure Analysis Abstract: Online selection of dynamic features has attracted intensive interest in\nrecent years. However, existing online feature selection methods evaluate\nfeatures individually and ignore the underlying structure of feature stream.\nFor instance, in image analysis, features are generated in groups which\nrepresent color, texture and other visual information. Simply breaking the\ngroup structure in feature selection may degrade performance. Motivated by this\nfact, we formulate the problem as an online group feature selection. The\nproblem assumes that features are generated individually but there are group\nstructure in the feature stream. To the best of our knowledge, this is the\nfirst time that the correlation among feature stream has been considered in the\nonline feature selection process. To solve this problem, we develop a novel\nonline group feature selection method named OGFS. Our proposed approach\nconsists of two stages: online intra-group selection and online inter-group\nselection. In the intra-group selection, we design a criterion based on\nspectral analysis to select discriminative features in each group. In the\ninter-group selection, we utilize a linear regression model to select an\noptimal subset. This two-stage procedure continues until there are no more\nfeatures arriving or some predefined stopping conditions are met. %Our method\nhas been applied Finally, we apply our method to multiple tasks including image\nclassification %, face verification and face verification. Extensive empirical\nstudies performed on real-world and benchmark data sets demonstrate that our\nmethod outperforms other state-of-the-art online feature selection %method\nmethods. \n\n"}
{"id": "1608.06010", "contents": "Title: Feedback-Controlled Sequential Lasso Screening Abstract: One way to solve lasso problems when the dictionary does not fit into\navailable memory is to first screen the dictionary to remove unneeded features.\nPrior research has shown that sequential screening methods offer the greatest\npromise in this endeavor. Most existing work on sequential screening targets\nthe context of tuning parameter selection, where one screens and solves a\nsequence of $N$ lasso problems with a fixed grid of geometrically spaced\nregularization parameters. In contrast, we focus on the scenario where a target\nregularization parameter has already been chosen via cross-validated model\nselection, and we then need to solve many lasso instances using this fixed\nvalue. In this context, we propose and explore a feedback controlled sequential\nscreening scheme. Feedback is used at each iteration to select the next problem\nto be solved. This allows the sequence of problems to be adapted to the\ninstance presented and the number of intermediate problems to be automatically\nselected. We demonstrate our feedback scheme using several datasets including a\ndictionary of approximate size 100,000 by 300,000. \n\n"}
{"id": "1608.06197", "contents": "Title: CrowdNet: A Deep Convolutional Network for Dense Crowd Counting Abstract: Our work proposes a novel deep learning framework for estimating crowd\ndensity from static images of highly dense crowds. We use a combination of deep\nand shallow, fully convolutional networks to predict the density map for a\ngiven crowd image. Such a combination is used for effectively capturing both\nthe high-level semantic information (face/body detectors) and the low-level\nfeatures (blob detectors), that are necessary for crowd counting under large\nscale variations. As most crowd datasets have limited training samples (<100\nimages) and deep learning based approaches require large amounts of training\ndata, we perform multi-scale data augmentation. Augmenting the training samples\nin such a manner helps in guiding the CNN to learn scale invariant\nrepresentations. Our method is tested on the challenging UCF_CC_50 dataset, and\nshown to outperform the state of the art methods. \n\n"}
{"id": "1608.06993", "contents": "Title: Densely Connected Convolutional Networks Abstract: Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet . \n\n"}
{"id": "1608.08334", "contents": "Title: Egocentric Meets Top-view Abstract: Thanks to the availability and increasing popularity of Egocentric cameras\nsuch as GoPro cameras, glasses, and etc. we have been provided with a plethora\nof videos captured from the first person perspective. Surveillance cameras and\nUnmanned Aerial Vehicles(also known as drones) also offer tremendous amount of\nvideos, mostly with top-down or oblique view-point. Egocentric vision and\ntop-view surveillance videos have been studied extensively in the past in the\ncomputer vision community. However, the relationship between the two has yet to\nbe explored thoroughly. In this effort, we attempt to explore this relationship\nby approaching two questions. First, having a set of egocentric videos and a\ntop-view video, can we verify if the top-view video contains all, or some of\nthe egocentric viewers present in the egocentric set? And second, can we\nidentify the egocentric viewers in the content of the top-view video? In other\nwords, can we find the cameramen in the surveillance videos? These problems can\nbecome more challenging when the videos are not time-synchronous. Thus we\nformalize the problem in a way which handles and also estimates the unknown\nrelative time-delays between the egocentric videos and the top-view video. We\nformulate the problem as a spectral graph matching instance, and jointly seek\nthe optimal assignments and relative time-delays of the videos. As a result, we\nspatiotemporally localize the egocentric observers in the top-view video. We\nmodel each view (egocentric or top) using a graph, and compute the assignment\nand time-delays in an iterative-alternative fashion. \n\n"}
{"id": "1608.08716", "contents": "Title: Measuring Machine Intelligence Through Visual Question Answering Abstract: As machines have become more intelligent, there has been a renewed interest\nin methods for measuring their intelligence. A common approach is to propose\ntasks for which a human excels, but one which machines find difficult. However,\nan ideal task should also be easy to evaluate and not be easily gameable. We\nbegin with a case study exploring the recently popular task of image captioning\nand its limitations as a task for measuring machine intelligence. An\nalternative and more promising task is Visual Question Answering that tests a\nmachine's ability to reason about language and vision. We describe a dataset\nunprecedented in size created for the task that contains over 760,000 human\ngenerated questions about images. Using around 10 million human generated\nanswers, machines may be easily evaluated. \n\n"}
{"id": "1609.01775", "contents": "Title: Performance Measures and a Data Set for Multi-Target, Multi-Camera\n  Tracking Abstract: To help accelerate progress in multi-target, multi-camera tracking systems,\nwe present (i) a new pair of precision-recall measures of performance that\ntreats errors of all types uniformly and emphasizes correct identification over\nsources of error; (ii) the largest fully-annotated and calibrated data set to\ndate with more than 2 million frames of 1080p, 60fps video taken by 8 cameras\nobserving more than 2,700 identities over 85 minutes; and (iii) a reference\nsoftware system as a comparison baseline. We show that (i) our measures\nproperly account for bottom-line identity match performance in the multi-camera\nsetting; (ii) our data set poses realistic challenges to current trackers; and\n(iii) the performance of our system is comparable to the state of the art. \n\n"}
{"id": "1609.03894", "contents": "Title: Crafting a multi-task CNN for viewpoint estimation Abstract: Convolutional Neural Networks (CNNs) were recently shown to provide\nstate-of-the-art results for object category viewpoint estimation. However\ndifferent ways of formulating this problem have been proposed and the competing\napproaches have been explored with very different design choices. This paper\npresents a comparison of these approaches in a unified setting as well as a\ndetailed analysis of the key factors that impact performance. Followingly, we\npresent a new joint training method with the detection task and demonstrate its\nbenefit. We also highlight the superiority of classification approaches over\nregression approaches, quantify the benefits of deeper architectures and\nextended training data, and demonstrate that synthetic data is beneficial even\nwhen using ImageNet training data. By combining all these elements, we\ndemonstrate an improvement of approximately 5% mAVP over previous\nstate-of-the-art results on the Pascal3D+ dataset. In particular for their most\nchallenging 24 view classification task we improve the results from 31.1% to\n36.1% mAVP. \n\n"}
{"id": "1609.04453", "contents": "Title: A Large Contextual Dataset for Classification, Detection and Counting of\n  Cars with Deep Learning Abstract: We have created a large diverse set of cars from overhead images, which are\nuseful for training a deep learner to binary classify, detect and count them.\nThe dataset and all related material will be made publically available. The set\ncontains contextual matter to aid in identification of difficult targets. We\ndemonstrate classification and detection on this dataset using a neural network\nwe call ResCeption. This network combines residual learning with\nInception-style layers and is used to count cars in one look. This is a new way\nto count objects rather than by localization or density estimation. It is\nfairly accurate, fast and easy to implement. Additionally, the counting method\nis not car or scene specific. It would be easy to train this method to count\nother kinds of objects and counting over new scenes requires no extra set up or\nassumptions about object locations. \n\n"}
{"id": "1609.04938", "contents": "Title: Image-to-Markup Generation with Coarse-to-Fine Attention Abstract: We present a neural encoder-decoder model to convert images into\npresentational markup based on a scalable coarse-to-fine attention mechanism.\nOur method is evaluated in the context of image-to-LaTeX generation, and we\nintroduce a new dataset of real-world rendered mathematical expressions paired\nwith LaTeX markup. We show that unlike neural OCR techniques using CTC-based\nmodels, attention-based approaches can tackle this non-standard OCR task. Our\napproach outperforms classical mathematical OCR systems by a large margin on\nin-domain rendered data, and, with pretraining, also performs well on\nout-of-domain handwritten data. To reduce the inference complexity associated\nwith the attention-based approaches, we introduce a new coarse-to-fine\nattention layer that selects a support region before applying attention. \n\n"}
{"id": "1609.05158", "contents": "Title: Real-Time Single Image and Video Super-Resolution Using an Efficient\n  Sub-Pixel Convolutional Neural Network Abstract: Recently, several models based on deep neural networks have achieved great\nsuccess in terms of both reconstruction accuracy and computational performance\nfor single image super-resolution. In these methods, the low resolution (LR)\ninput image is upscaled to the high resolution (HR) space using a single\nfilter, commonly bicubic interpolation, before reconstruction. This means that\nthe super-resolution (SR) operation is performed in HR space. We demonstrate\nthat this is sub-optimal and adds computational complexity. In this paper, we\npresent the first convolutional neural network (CNN) capable of real-time SR of\n1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN\narchitecture where the feature maps are extracted in the LR space. In addition,\nwe introduce an efficient sub-pixel convolution layer which learns an array of\nupscaling filters to upscale the final LR feature maps into the HR output. By\ndoing so, we effectively replace the handcrafted bicubic filter in the SR\npipeline with more complex upscaling filters specifically trained for each\nfeature map, whilst also reducing the computational complexity of the overall\nSR operation. We evaluate the proposed approach using images and videos from\npublicly available datasets and show that it performs significantly better\n(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster\nthan previous CNN-based methods. \n\n"}
{"id": "1609.05600", "contents": "Title: Graph-Structured Representations for Visual Question Answering Abstract: This paper proposes to improve visual question answering (VQA) with\nstructured representations of both scene contents and questions. A key\nchallenge in VQA is to require joint reasoning over the visual and text\ndomains. The predominant CNN/LSTM-based approach to VQA is limited by\nmonolithic vector representations that largely ignore structure in the scene\nand in the form of the question. CNN feature vectors cannot effectively capture\nsituations as simple as multiple object instances, and LSTMs process questions\nas series of words, which does not reflect the true complexity of language\nstructure. We instead propose to build graphs over the scene objects and over\nthe question words, and we describe a deep neural network that exploits the\nstructure in these representations. This shows significant benefit over the\nsequential processing of LSTMs. The overall efficacy of our approach is\ndemonstrated by significant improvements over the state-of-the-art, from 71.2%\nto 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and\nfrom 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images\nwith fine-grained differences and opposite yes/no answers to a same question. \n\n"}
{"id": "1609.05672", "contents": "Title: Multi-Residual Networks: Improving the Speed and Accuracy of Residual\n  Networks Abstract: In this article, we take one step toward understanding the learning behavior\nof deep residual networks, and supporting the observation that deep residual\nnetworks behave like ensembles. We propose a new convolutional neural network\narchitecture which builds upon the success of residual networks by explicitly\nexploiting the interpretation of very deep networks as an ensemble. The\nproposed multi-residual network increases the number of residual functions in\nthe residual blocks. Our architecture generates models that are wider, rather\nthan deeper, which significantly improves accuracy. We show that our model\nachieves an error rate of 3.73% and 19.45% on CIFAR-10 and CIFAR-100\nrespectively, that outperforms almost all of the existing models. We also\ndemonstrate that our model outperforms very deep residual networks by 0.22%\n(top-1 error) on the full ImageNet 2012 classification dataset. Additionally,\ninspired by the parallel structure of multi-residual networks, a model\nparallelism technique has been investigated. The model parallelism method\ndistributes the computation of residual blocks among the processors, yielding\nup to 15% computational complexity improvement. \n\n"}
{"id": "1609.06647", "contents": "Title: Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning\n  Challenge Abstract: Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. Finally, given the recent\nsurge of interest in this task, a competition was organized in 2015 using the\nnewly released COCO dataset. We describe and analyze the various improvements\nwe applied to our own baseline and show the resulting performance in the\ncompetition, which we won ex-aequo with a team from Microsoft Research, and\nprovide an open source implementation in TensorFlow. \n\n"}
{"id": "1609.06831", "contents": "Title: Hawkes Processes with Stochastic Excitations Abstract: We propose an extension to Hawkes processes by treating the levels of\nself-excitation as a stochastic differential equation. Our new point process\nallows better approximation in application domains where events and intensities\naccelerate each other with correlated levels of contagion. We generalize a\nrecent algorithm for simulating draws from Hawkes processes whose levels of\nexcitation are stochastic processes, and propose a hybrid Markov chain Monte\nCarlo approach for model fitting. Our sampling procedure scales linearly with\nthe number of required events and does not require stationarity of the point\nprocess. A modular inference procedure consisting of a combination between\nGibbs and Metropolis Hastings steps is put forward. We recover expectation\nmaximization as a special case. Our general approach is illustrated for\ncontagion following geometric Brownian motion and exponential Langevin\ndynamics. \n\n"}
{"id": "1609.06870", "contents": "Title: Distributed Training of Deep Neural Networks: Theoretical and Practical\n  Limits of Parallel Scalability Abstract: This paper presents a theoretical analysis and practical evaluation of the\nmain bottlenecks towards a scalable distributed solution for the training of\nDeep Neuronal Networks (DNNs). The presented results show, that the current\nstate of the art approach, using data-parallelized Stochastic Gradient Descent\n(SGD), is quickly turning into a vastly communication bound problem. In\naddition, we present simple but fixed theoretic constraints, preventing\neffective scaling of DNN training beyond only a few dozen nodes. This leads to\npoor scalability of DNN training in most practical scenarios. \n\n"}
{"id": "1609.07495", "contents": "Title: A Rotation Invariant Latent Factor Model for Moveme Discovery from\n  Static Poses Abstract: We tackle the problem of learning a rotation invariant latent factor model\nwhen the training data is comprised of lower-dimensional projections of the\noriginal feature space. The main goal is the discovery of a set of 3-D bases\nposes that can characterize the manifold of primitive human motions, or\nmovemes, from a training set of 2-D projected poses obtained from still images\ntaken at various camera angles. The proposed technique for basis discovery is\ndata-driven rather than hand-designed. The learned representation is rotation\ninvariant, and can reconstruct any training instance from multiple viewing\nangles. We apply our method to modeling human poses in sports (via the Leeds\nSports Dataset), and demonstrate the effectiveness of the learned bases in a\nrange of applications such as activity classification, inference of dynamics\nfrom a single frame, and synthetic representation of movements. \n\n"}
{"id": "1609.08221", "contents": "Title: Simultaneous Low-rank Component and Graph Estimation for\n  High-dimensional Graph Signals: Application to Brain Imaging Abstract: We propose an algorithm to uncover the intrinsic low-rank component of a\nhigh-dimensional, graph-smooth and grossly-corrupted dataset, under the\nsituations that the underlying graph is unknown. Based on a model with a\nlow-rank component plus a sparse perturbation, and an initial graph estimation,\nour proposed algorithm simultaneously learns the low-rank component and refines\nthe graph. Our evaluations using synthetic and real brain imaging data in\nunsupervised and supervised classification tasks demonstrate encouraging\nperformance. \n\n"}
{"id": "1609.09025", "contents": "Title: Learning to Push by Grasping: Using multiple tasks for effective\n  learning Abstract: Recently, end-to-end learning frameworks are gaining prevalence in the field\nof robot control. These frameworks input states/images and directly predict the\ntorques or the action parameters. However, these approaches are often critiqued\ndue to their huge data requirements for learning a task. The argument of the\ndifficulty in scalability to multiple tasks is well founded, since training\nthese tasks often require hundreds or thousands of examples. But do end-to-end\napproaches need to learn a unique model for every task? Intuitively, it seems\nthat sharing across tasks should help since all tasks require some common\nunderstanding of the environment. In this paper, we attempt to take the next\nstep in data-driven end-to-end learning frameworks: move from the realm of\ntask-specific models to joint learning of multiple robot tasks. In an\nastonishing result we show that models with multi-task learning tend to perform\nbetter than task-specific models trained with same amounts of data. For\nexample, a deep-network learned with 2.5K grasp and 2.5K push examples performs\nbetter on grasping than a network trained on 5K grasp examples. \n\n"}
{"id": "1610.00527", "contents": "Title: Video Pixel Networks Abstract: We propose a probabilistic video model, the Video Pixel Network (VPN), that\nestimates the discrete joint distribution of the raw pixel values in a video.\nThe model and the neural architecture reflect the time, space and color\nstructure of video tensors and encode it as a four-dimensional dependency\nchain. The VPN approaches the best possible performance on the Moving MNIST\nbenchmark, a leap over the previous state of the art, and the generated videos\nshow only minor deviations from the ground truth. The VPN also produces\ndetailed samples on the action-conditional Robotic Pushing benchmark and\ngeneralizes to the motion of novel objects. \n\n"}
{"id": "1610.00768", "contents": "Title: Technical Report on the CleverHans v2.1.0 Adversarial Examples Library Abstract: CleverHans is a software library that provides standardized reference\nimplementations of adversarial example construction techniques and adversarial\ntraining. The library may be used to develop more robust machine learning\nmodels and to provide standardized benchmarks of models' performance in the\nadversarial setting. Benchmarks constructed without a standardized\nimplementation of adversarial example construction are not comparable to each\nother, because a good result may indicate a robust model or it may merely\nindicate a weak implementation of the adversarial example construction\nprocedure.\n  This technical report is structured as follows. Section 1 provides an\noverview of adversarial examples in machine learning and of the CleverHans\nsoftware. Section 2 presents the core functionalities of the library: namely\nthe attacks based on adversarial examples and defenses to improve the\nrobustness of machine learning models to these attacks. Section 3 describes how\nto report benchmark results using the library. Section 4 describes the\nversioning system. \n\n"}
{"id": "1610.01685", "contents": "Title: Supervision via Competition: Robot Adversaries for Learning Tasks Abstract: There has been a recent paradigm shift in robotics to data-driven learning\nfor planning and control. Due to large number of experiences required for\ntraining, most of these approaches use a self-supervised paradigm: using\nsensors to measure success/failure. However, in most cases, these sensors\nprovide weak supervision at best. In this work, we propose an adversarial\nlearning framework that pits an adversary against the robot learning the task.\nIn an effort to defeat the adversary, the original robot learns to perform the\ntask with more robustness leading to overall improved performance. We show that\nthis adversarial framework forces the the robot to learn a better grasping\nmodel in order to overcome the adversary. By grasping 82% of presented novel\nobjects compared to 68% without an adversary, we demonstrate the utility of\ncreating adversaries. We also demonstrate via experiments that having robots in\nadversarial setting might be a better learning strategy as compared to having\ncollaborative multiple robots. \n\n"}
{"id": "1610.01854", "contents": "Title: Do They All Look the Same? Deciphering Chinese, Japanese and Koreans by\n  Fine-Grained Deep Learning Abstract: We study to what extend Chinese, Japanese and Korean faces can be classified\nand which facial attributes offer the most important cues. First, we propose a\nnovel way of obtaining large numbers of facial images with nationality labels.\nThen we train state-of-the-art neural networks with these labeled images. We\nare able to achieve an accuracy of 75.03% in the classification task, with\nchances being 33.33% and human accuracy 38.89% . Further, we train multiple\nfacial attribute classifiers to identify the most distinctive features for each\ngroup. We find that Chinese, Japanese and Koreans do exhibit substantial\ndifferences in certain attributes, such as bangs, smiling, and bushy eyebrows.\nAlong the way, we uncover several gender-related cross-country patterns as\nwell. Our work, which complements existing APIs such as Microsoft Cognitive\nServices and Face++, could find potential applications in tourism, e-commerce,\nsocial media marketing, criminal justice and even counter-terrorism. \n\n"}
{"id": "1610.01983", "contents": "Title: Driving in the Matrix: Can Virtual Worlds Replace Human-Generated\n  Annotations for Real World Tasks? Abstract: Deep learning has rapidly transformed the state of the art algorithms used to\naddress a variety of problems in computer vision and robotics. These\nbreakthroughs have relied upon massive amounts of human annotated training\ndata. This time consuming process has begun impeding the progress of these deep\nlearning efforts. This paper describes a method to incorporate photo-realistic\ncomputer images from a simulation engine to rapidly generate annotated data\nthat can be used for the training of machine learning algorithms. We\ndemonstrate that a state of the art architecture, which is trained only using\nthese synthetic annotations, performs better than the identical architecture\ntrained on human annotated real-world data, when tested on the KITTI data set\nfor vehicle detection. By training machine learning algorithms on a rich\nvirtual world, real objects in real scenes can be learned and classified using\nsynthetic data. This approach offers the possibility of accelerating deep\nlearning's application to sensor-based classification problems like those that\nappear in self-driving cars. The source code and data to train and validate the\nnetworks described in this paper are made available for researchers. \n\n"}
{"id": "1610.02714", "contents": "Title: Egocentric Height Estimation Abstract: Egocentric, or first-person vision which became popular in recent years with\nan emerge in wearable technology, is different than exocentric (third-person)\nvision in some distinguishable ways, one of which being that the camera wearer\nis generally not visible in the video frames. Recent work has been done on\naction and object recognition in egocentric videos, as well as work on\nbiometric extraction from first-person videos. Height estimation can be a\nuseful feature for both soft-biometrics and object tracking. Here, we propose a\nmethod of estimating the height of an egocentric camera without any calibration\nor reference points. We used both traditional computer vision approaches and\ndeep learning in order to determine the visual cues that results in best height\nestimation. Here, we introduce a framework inspired by two stream networks\ncomprising of two Convolutional Neural Networks, one based on spatial\ninformation, and one based on information given by optical flow in a frame.\nGiven an egocentric video as an input to the framework, our model yields a\nheight estimate as an output. We also incorporate late fusion to learn a\ncombination of temporal and spatial cues. Comparing our model with other\nmethods we used as baselines, we achieve height estimates for videos with a\nMean Average Error of 14.04 cm over a range of 103 cm of data, and\nclassification accuracy for relative height (tall, medium or short) up to\n93.75% where chance level is 33%. \n\n"}
{"id": "1610.04057", "contents": "Title: Stroke Sequence-Dependent Deep Convolutional Neural Network for Online\n  Handwritten Chinese Character Recognition Abstract: In this paper, we propose a novel model, named Stroke Sequence-dependent Deep\nConvolutional Neural Network (SSDCNN), using the stroke sequence information\nand eight-directional features for Online Handwritten Chinese Character\nRecognition (OLHCCR). On one hand, SSDCNN can learn the representation of\nOnline Handwritten Chinese Character (OLHCC) by incorporating the natural\nsequence information of the strokes. On the other hand, SSDCNN can incorporate\neight-directional features in a natural way. In order to train SSDCNN, we\ndivide the process of training into two stages: 1) The training data is used to\npre-train the whole architecture until the performance tends to converge. 2)\nFully-connected neural network which is used to combine the stroke\nsequence-dependent representation with eight-directional features and softmax\nlayer are further trained. Experiments were conducted on the OLHCCR competition\ntasks of ICDAR 2013. Results show that, SSDCNN can reduce the recognition error\nby 50\\% (5.13\\% vs 2.56\\%) compared to the model which only use\neight-directional features. The proposed SSDCNN achieves 97.44\\% accuracy which\nreduces the recognition error by about 1.9\\% compared with the best submitted\nsystem on ICDAR2013 competition. These results indicate that SSDCNN can exploit\nthe stroke sequence information to learn high-quality representation of OLHCC.\nIt also shows that the learnt representation and the classical\neight-directional features complement each other within the SSDCNN\narchitecture. \n\n"}
{"id": "1610.04583", "contents": "Title: Message-passing algorithms for synchronization problems over compact\n  groups Abstract: Various alignment problems arising in cryo-electron microscopy, community\ndetection, time synchronization, computer vision, and other fields fall into a\ncommon framework of synchronization problems over compact groups such as Z/L,\nU(1), or SO(3). The goal of such problems is to estimate an unknown vector of\ngroup elements given noisy relative observations. We present an efficient\niterative algorithm to solve a large class of these problems, allowing for any\ncompact group, with measurements on multiple 'frequency channels' (Fourier\nmodes, or more generally, irreducible representations of the group). Our\nalgorithm is a highly efficient iterative method following the blueprint of\napproximate message passing (AMP), which has recently arisen as a central\ntechnique for inference problems such as structured low-rank estimation and\ncompressed sensing. We augment the standard ideas of AMP with ideas from\nrepresentation theory so that the algorithm can work with distributions over\ncompact groups. Using standard but non-rigorous methods from statistical\nphysics we analyze the behavior of our algorithm on a Gaussian noise model,\nidentifying phases where the problem is easy, (computationally) hard, and\n(statistically) impossible. In particular, such evidence predicts that our\nalgorithm is information-theoretically optimal in many cases, and that the\nremaining cases show evidence of statistical-to-computational gaps. \n\n"}
{"id": "1610.05083", "contents": "Title: Efficient Metric Learning for the Analysis of Motion Data Abstract: We investigate metric learning in the context of dynamic time warping (DTW),\nthe by far most popular dissimilarity measure used for the comparison and\nanalysis of motion capture data. While metric learning enables a\nproblem-adapted representation of data, the majority of methods has been\nproposed for vectorial data only. In this contribution, we extend the popular\nprinciple offered by the large margin nearest neighbors learner (LMNN) to DTW\nby treating the resulting component-wise dissimilarity values as features. We\ndemonstrate that this principle greatly enhances the classification accuracy in\nseveral benchmarks. Further, we show that recent auxiliary concepts such as\nmetric regularization can be transferred from the vectorial case to\ncomponent-wise DTW in a similar way. We illustrate that metric regularization\nconstitutes a crucial prerequisite for the interpretation of the resulting\nrelevance profiles. \n\n"}
{"id": "1610.05712", "contents": "Title: Fast L1-NMF for Multiple Parametric Model Estimation Abstract: In this work we introduce a comprehensive algorithmic pipeline for multiple\nparametric model estimation. The proposed approach analyzes the information\nproduced by a random sampling algorithm (e.g., RANSAC) from a machine\nlearning/optimization perspective, using a \\textit{parameterless} biclustering\nalgorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed\nframework exploits consistent patterns that naturally arise during the RANSAC\nexecution, while explicitly avoiding spurious inconsistencies. Contrarily to\nthe main trends in the literature, the proposed technique does not impose\nnon-intersecting parametric models. A new accelerated algorithm to compute\nL1-NMFs allows to handle medium-sized problems faster while also extending the\nusability of the algorithm to much larger datasets. This accelerated algorithm\nhas applications in any other context where an L1-NMF is needed, beyond the\nbiclustering approach to parameter estimation here addressed. We accompany the\nalgorithmic presentation with theoretical foundations and numerous and diverse\nexamples. \n\n"}
{"id": "1610.06756", "contents": "Title: Fine-grained Recognition in the Noisy Wild: Sensitivity Analysis of\n  Convolutional Neural Networks Approaches Abstract: In this paper, we study the sensitivity of CNN outputs with respect to image\ntransformations and noise in the area of fine-grained recognition. In\nparticular, we answer the following questions (1) how sensitive are CNNs with\nrespect to image transformations encountered during wild image capture?; (2)\nhow can we predict CNN sensitivity?; and (3) can we increase the robustness of\nCNNs with respect to image degradations? To answer the first question, we\nprovide an extensive empirical sensitivity analysis of commonly used CNN\narchitectures (AlexNet, VGG19, GoogleNet) across various types of image\ndegradations. This allows for predicting CNN performance for new domains\ncomprised by images of lower quality or captured from a different viewpoint. We\nalso show how the sensitivity of CNN outputs can be predicted for single\nimages. Furthermore, we demonstrate that input layer dropout or pre-filtering\nduring test time only reduces CNN sensitivity for high levels of degradation.\n  Experiments for fine-grained recognition tasks reveal that VGG19 is more\nrobust to severe image degradations than AlexNet and GoogleNet. However, small\nintensity noise can lead to dramatic changes in CNN performance even for VGG19. \n\n"}
{"id": "1610.07008", "contents": "Title: Optimization on Submanifolds of Convolution Kernels in CNNs Abstract: Kernel normalization methods have been employed to improve robustness of\noptimization methods to reparametrization of convolution kernels, covariate\nshift, and to accelerate training of Convolutional Neural Networks (CNNs).\nHowever, our understanding of theoretical properties of these methods has\nlagged behind their success in applications. We develop a geometric framework\nto elucidate underlying mechanisms of a diverse range of kernel normalization\nmethods. Our framework enables us to expound and identify geometry of space of\nnormalized kernels. We analyze and delineate how state-of-the-art kernel\nnormalization methods affect the geometry of search spaces of the stochastic\ngradient descent (SGD) algorithms in CNNs. Following our theoretical results,\nwe propose a SGD algorithm with assurance of almost sure convergence of the\nmethods to a solution at single minimum of classification loss of CNNs.\nExperimental results show that the proposed method achieves state-of-the-art\nperformance for major image classification benchmarks with CNNs. \n\n"}
{"id": "1610.08120", "contents": "Title: Image Segmentation for Fruit Detection and Yield Estimation in Apple\n  Orchards Abstract: Ground vehicles equipped with monocular vision systems are a valuable source\nof high resolution image data for precision agriculture applications in\norchards. This paper presents an image processing framework for fruit detection\nand counting using orchard image data. A general purpose image segmentation\napproach is used, including two feature learning algorithms; multi-scale\nMulti-Layered Perceptrons (MLP) and Convolutional Neural Networks (CNN). These\nnetworks were extended by including contextual information about how the image\ndata was captured (metadata), which correlates with some of the appearance\nvariations and/or class distributions observed in the data. The pixel-wise\nfruit segmentation output is processed using the Watershed Segmentation (WS)\nand Circular Hough Transform (CHT) algorithms to detect and count individual\nfruits. Experiments were conducted in a commercial apple orchard near\nMelbourne, Australia. The results show an improvement in fruit segmentation\nperformance with the inclusion of metadata on the previously benchmarked MLP\nnetwork. We extend this work with CNNs, bringing agrovision closer to the\nstate-of-the-art in computer vision, where although metadata had negligible\ninfluence, the best pixel-wise F1-score of $0.791$ was achieved. The WS\nalgorithm produced the best apple detection and counting results, with a\ndetection F1-score of $0.858$. As a final step, image fruit counts were\naccumulated over multiple rows at the orchard and compared against the\npost-harvest fruit counts that were obtained from a grading and counting\nmachine. The count estimates using CNN and WS resulted in the best performance\nfor this dataset, with a squared correlation coefficient of $r^2=0.826$. \n\n"}
{"id": "1610.08851", "contents": "Title: Single- and Multi-Task Architectures for Tool Presence Detection\n  Challenge at M2CAI 2016 Abstract: The tool presence detection challenge at M2CAI 2016 consists of identifying\nthe presence/absence of seven surgical tools in the images of cholecystectomy\nvideos. Here, we propose to use deep architectures that are based on our\nprevious work where we presented several architectures to perform multiple\nrecognition tasks on laparoscopic videos. In this technical report, we present\nthe tool presence detection results using two architectures: (1) a single-task\narchitecture designed to perform solely the tool presence detection task and\n(2) a multi-task architecture designed to perform jointly phase recognition and\ntool presence detection. The results show that the multi-task network only\nslightly improves the tool presence detection results. In constrast, a\nsignificant improvement is obtained when there are more data available to train\nthe networks. This significant improvement can be regarded as a call for action\nfor other institutions to start working toward publishing more datasets into\nthe community, so that better models could be generated to perform the task. \n\n"}
{"id": "1610.08904", "contents": "Title: Local Similarity-Aware Deep Feature Embedding Abstract: Existing deep embedding methods in vision tasks are capable of learning a\ncompact Euclidean space from images, where Euclidean distances correspond to a\nsimilarity metric. To make learning more effective and efficient, hard sample\nmining is usually employed, with samples identified through computing the\nEuclidean feature distance. However, the global Euclidean distance cannot\nfaithfully characterize the true feature similarity in a complex visual feature\nspace, where the intraclass distance in a high-density region may be larger\nthan the interclass distance in low-density regions. In this paper, we\nintroduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of\nlearning a similarity metric adaptive to local feature structure. The metric\ncan be used to select genuinely hard samples in a local neighborhood to guide\nthe deep embedding learning in an online and robust manner. The new layer is\nappealing in that it is pluggable to any convolutional networks and is trained\nend-to-end. Our local similarity-aware feature embedding not only demonstrates\nfaster convergence and boosted performance on two complex image retrieval\ndatasets, its large margin nature also leads to superior generalization results\nunder the large and open set scenarios of transfer learning and zero-shot\nlearning on ImageNet 2010 and ImageNet-10K datasets. \n\n"}
{"id": "1611.00094", "contents": "Title: Learning recurrent representations for hierarchical behavior modeling Abstract: We propose a framework for detecting action patterns from motion sequences\nand modeling the sensory-motor relationship of animals, using a generative\nrecurrent neural network. The network has a discriminative part (classifying\nactions) and a generative part (predicting motion), whose recurrent cells are\nlaterally connected, allowing higher levels of the network to represent high\nlevel phenomena. We test our framework on two types of data, fruit fly behavior\nand online handwriting. Our results show that 1) taking advantage of unlabeled\nsequences, by predicting future motion, significantly improves action detection\nperformance when training labels are scarce, 2) the network learns to represent\nhigh level phenomena such as writer identity and fly gender, without\nsupervision, and 3) simulated motion trajectories, generated by treating motion\nprediction as input to the network, look realistic and may be used to\nqualitatively evaluate whether the model has learnt generative control rules. \n\n"}
{"id": "1611.00471", "contents": "Title: Dual Attention Networks for Multimodal Reasoning and Matching Abstract: We propose Dual Attention Networks (DANs) which jointly leverage visual and\ntextual attention mechanisms to capture fine-grained interplay between vision\nand language. DANs attend to specific regions in images and words in text\nthrough multiple steps and gather essential information from both modalities.\nBased on this framework, we introduce two types of DANs for multimodal\nreasoning and matching, respectively. The reasoning model allows visual and\ntextual attentions to steer each other during collaborative inference, which is\nuseful for tasks such as Visual Question Answering (VQA). In addition, the\nmatching model exploits the two attention mechanisms to estimate the similarity\nbetween images and sentences by focusing on their shared semantics. Our\nextensive experiments validate the effectiveness of DANs in combining vision\nand language, achieving the state-of-the-art performance on public benchmarks\nfor VQA and image-text matching. \n\n"}
{"id": "1611.01236", "contents": "Title: Adversarial Machine Learning at Scale Abstract: Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess. \n\n"}
{"id": "1611.01331", "contents": "Title: RenderGAN: Generating Realistic Labeled Data Abstract: Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable\nperformance on many computer vision tasks. Due to their large parameter space,\nthey require many labeled samples when trained in a supervised setting. The\ncosts of annotating data manually can render the use of DCNNs infeasible. We\npresent a novel framework called RenderGAN that can generate large amounts of\nrealistic, labeled images by combining a 3D model and the Generative\nAdversarial Network framework. In our approach, image augmentations (e.g.\nlighting, background, and detail) are learned from unlabeled data such that the\ngenerated images are strikingly realistic while preserving the labels known\nfrom the 3D model. We apply the RenderGAN framework to generate images of\nbarcode-like markers that are attached to honeybees. Training a DCNN on data\ngenerated by the RenderGAN yields considerably better performance than training\nit on various baselines. \n\n"}
{"id": "1611.01731", "contents": "Title: Deep Label Distribution Learning with Label Ambiguity Abstract: Convolutional Neural Networks (ConvNets) have achieved excellent recognition\nperformance in various visual recognition tasks. A large labeled training set\nis one of the most important factors for its success. However, it is difficult\nto collect sufficient training images with precise labels in some domains such\nas apparent age estimation, head pose estimation, multi-label classification\nand semantic segmentation. Fortunately, there is ambiguous information among\nlabels, which makes these tasks different from traditional classification.\nBased on this observation, we convert the label of each image into a discrete\nlabel distribution, and learn the label distribution by minimizing a\nKullback-Leibler divergence between the predicted and ground-truth label\ndistributions using deep ConvNets. The proposed DLDL (Deep Label Distribution\nLearning) method effectively utilizes the label ambiguity in both feature\nlearning and classifier learning, which help prevent the network from\nover-fitting even when the training set is small. Experimental results show\nthat the proposed approach produces significantly better results than\nstate-of-the-art methods for age estimation and head pose estimation. At the\nsame time, it also improves recognition performance for multi-label\nclassification and semantic segmentation tasks. \n\n"}
{"id": "1611.02261", "contents": "Title: Memory-augmented Attention Modelling for Videos Abstract: We present a method to improve video description generation by modeling\nhigher-order interactions between video frames and described concepts. By\nstoring past visual attention in the video associated to previously generated\nwords, the system is able to decide what to look at and describe in light of\nwhat it has already looked at and described. This enables not only more\neffective local attention, but tractable consideration of the video sequence\nwhile generating each word. Evaluation on the challenging and popular MSVD and\nCharades datasets demonstrates that the proposed architecture outperforms\nprevious video description approaches without requiring external temporal video\nfeatures. \n\n"}
{"id": "1611.02320", "contents": "Title: Adversarial Ladder Networks Abstract: The use of unsupervised data in addition to supervised data in training\ndiscriminative neural networks has improved the performance of this clas-\nsification scheme. However, the best results were achieved with a training\nprocess that is divided in two parts: first an unsupervised pre-training step\nis done for initializing the weights of the network and after these weights are\nrefined with the use of supervised data. On the other hand adversarial noise\nhas improved the results of clas- sical supervised learning. Recently, a new\nneural network topology called Ladder Network, where the key idea is based in\nsome properties of hierar- chichal latent variable models, has been proposed as\na technique to train a neural network using supervised and unsupervised data at\nthe same time with what is called semi-supervised learning. This technique has\nreached state of the art classification. In this work we add adversarial noise\nto the ladder network and get state of the art classification, with several\nimportant conclusions on how adversarial noise can help in addition with new\npossible lines of investi- gation. We also propose an alternative to add\nadversarial noise to unsu- pervised data. \n\n"}
{"id": "1611.03679", "contents": "Title: Deep Convolutional Neural Network for Inverse Problems in Imaging Abstract: In this paper, we propose a novel deep convolutional neural network\n(CNN)-based algorithm for solving ill-posed inverse problems. Regularized\niterative algorithms have emerged as the standard approach to ill-posed inverse\nproblems in the past few decades. These methods produce excellent results, but\ncan be challenging to deploy in practice due to factors including the high\ncomputational cost of the forward and adjoint operators and the difficulty of\nhyper parameter selection. The starting point of our work is the observation\nthat unrolled iterative methods have the form of a CNN (filtering followed by\npoint-wise non-linearity) when the normal operator (H*H, the adjoint of H times\nH) of the forward model is a convolution. Based on this observation, we propose\nusing direct inversion followed by a CNN to solve normal-convolutional inverse\nproblems. The direct inversion encapsulates the physical model of the system,\nbut leads to artifacts when the problem is ill-posed; the CNN combines\nmultiresolution decomposition and residual learning in order to learn to remove\nthese artifacts while preserving image structure. We demonstrate the\nperformance of the proposed network in sparse-view reconstruction (down to 50\nviews) on parallel beam X-ray computed tomography in synthetic phantoms as well\nas in real experimental sinograms. The proposed network outperforms total\nvariation-regularized iterative reconstruction for the more realistic phantoms\nand requires less than a second to reconstruct a 512 x 512 image on GPU. \n\n"}
{"id": "1611.03999", "contents": "Title: Optimized clothes segmentation to boost gender classification in\n  unconstrained scenarios Abstract: Several applications require demographic information of ordinary people in\nunconstrained scenarios. This is not a trivial task due to significant human\nappearance variations. In this work, we introduce trixels for clustering image\nregions, enumerating their advantages compared to superpixels. The classical\nGrabCut algorithm is later modified to segment trixels instead of pixels in an\nunsupervised context. Combining with face detection lead us to a clothes\nsegmentation approach close to real time. The study uses the challenging Pascal\nVOC dataset for segmentation evaluation experiments. A final experiment\nanalyzes the fusion of clothes features with state-of-the-art gender\nclassifiers in ClothesDB, revealing a significant performance improvement in\ngender classification. \n\n"}
{"id": "1611.05546", "contents": "Title: Zero-Shot Visual Question Answering Abstract: Part of the appeal of Visual Question Answering (VQA) is its promise to\nanswer new questions about previously unseen images. Most current methods\ndemand training questions that illustrate every possible concept, and will\ntherefore never achieve this capability, since the volume of required training\ndata would be prohibitive. Answering general questions about images requires\nmethods capable of Zero-Shot VQA, that is, methods able to answer questions\nbeyond the scope of the training questions. We propose a new evaluation\nprotocol for VQA methods which measures their ability to perform Zero-Shot VQA,\nand in doing so highlights significant practical deficiencies of current\napproaches, some of which are masked by the biases in current datasets. We\npropose and evaluate several strategies for achieving Zero-Shot VQA, including\nmethods based on pretrained word embeddings, object classifiers with semantic\nembeddings, and test-time retrieval of example images. Our extensive\nexperiments are intended to serve as baselines for Zero-Shot VQA, and they also\nachieve state-of-the-art performance in the standard VQA evaluation setting. \n\n"}
{"id": "1611.06284", "contents": "Title: Understanding Anatomy Classification Through Attentive Response Maps Abstract: One of the main challenges for broad adoption of deep learning based models\nsuch as convolutional neural networks (CNN), is the lack of understanding of\ntheir decisions. In many applications, a simpler, less capable model that can\nbe easily understood is favorable to a black-box model that has superior\nperformance. In this paper, we present an approach for designing CNNs based on\nvisualization of the internal activations of the model. We visualize the\nmodel's response through attentive response maps obtained using a fractional\nstride convolution technique and compare the results with known imaging\nlandmarks from the medical literature. We show that sufficiently deep and\ncapable models can be successfully trained to use the same medical landmarks a\nhuman expert would use. Our approach allows for communicating the model\ndecision process well, but also offers insight towards detecting biases. \n\n"}
{"id": "1611.06475", "contents": "Title: Dealing with Range Anxiety in Mean Estimation via Statistical Queries Abstract: We give algorithms for estimating the expectation of a given real-valued\nfunction $\\phi:X\\to {\\bf R}$ on a sample drawn randomly from some unknown\ndistribution $D$ over domain $X$, namely ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf\nx})]$. Our algorithms work in two well-studied models of restricted access to\ndata samples. The first one is the statistical query (SQ) model in which an\nalgorithm has access to an SQ oracle for the input distribution $D$ over $X$\ninstead of i.i.d. samples from $D$. Given a query function $\\phi:X \\to [0,1]$,\nthe oracle returns an estimate of ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf x})]$\nwithin some tolerance $\\tau$. The second, is a model in which only a single bit\nis communicated from each sample. In both of these models the error obtained\nusing a naive implementation would scale polynomially with the range of the\nrandom variable $\\phi({\\bf x})$ (which might even be infinite). In contrast,\nwithout restrictions on access to data the expected error scales with the\nstandard deviation of $\\phi({\\bf x})$. Here we give a simple algorithm whose\nerror scales linearly in standard deviation of $\\phi({\\bf x})$ and\nlogarithmically with an upper bound on the second moment of $\\phi({\\bf x})$.\n  As corollaries, we obtain algorithms for high dimensional mean estimation and\nstochastic convex optimization in these models that work in more general\nsettings than previously known solutions. \n\n"}
{"id": "1611.06973", "contents": "Title: RhoanaNet Pipeline: Dense Automatic Neural Annotation Abstract: Reconstructing a synaptic wiring diagram, or connectome, from electron\nmicroscopy (EM) images of brain tissue currently requires many hours of manual\nannotation or proofreading (Kasthuri and Lichtman, 2010; Lichtman and Sanes,\n2008; Seung, 2009). The desire to reconstruct ever larger and more complex\nnetworks has pushed the collection of ever larger EM datasets. A cubic\nmillimeter of raw imaging data would take up 1 PB of storage and present an\nannotation project that would be impractical without relying heavily on\nautomatic segmentation methods. The RhoanaNet image processing pipeline was\ndeveloped to automatically segment large volumes of EM data and ease the burden\nof manual proofreading and annotation. Based on (Kaynig et al., 2015), we\nupdated every stage of the software pipeline to provide better throughput\nperformance and higher quality segmentation results. We used state of the art\ndeep learning techniques to generate improved membrane probability maps, and\nGala (Nunez-Iglesias et al., 2014) was used to agglomerate 2D segments into 3D\nobjects.\n  We applied the RhoanaNet pipeline to four densely annotated EM datasets, two\nfrom mouse cortex, one from cerebellum and one from mouse lateral geniculate\nnucleus (LGN). All training and test data is made available for benchmark\ncomparisons. The best segmentation results obtained gave\n$V^\\text{Info}_\\text{F-score}$ scores of 0.9054 and 09182 for the cortex\ndatasets, 0.9438 for LGN, and 0.9150 for Cerebellum.\n  The RhoanaNet pipeline is open source software. All source code, training\ndata, test data, and annotations for all four benchmark datasets are available\nat www.rhoana.org. \n\n"}
{"id": "1611.07450", "contents": "Title: Grad-CAM: Why did you say that? Abstract: We propose a technique for making Convolutional Neural Network (CNN)-based\nmodels more transparent by visualizing input regions that are 'important' for\npredictions -- or visual explanations. Our approach, called Gradient-weighted\nClass Activation Mapping (Grad-CAM), uses class-specific gradient information\nto localize important regions. These localizations are combined with existing\npixel-space visualizations to create a novel high-resolution and\nclass-discriminative visualization called Guided Grad-CAM. These methods help\nbetter understand CNN-based models, including image captioning and visual\nquestion answering (VQA) models. We evaluate our visual explanations by\nmeasuring their ability to discriminate between classes, to inspire trust in\nhumans, and their correlation with occlusion maps. Grad-CAM provides a new way\nto understand CNN-based models.\n  We have released code, an online demo hosted on CloudCV, and a full version\nof this extended abstract. \n\n"}
{"id": "1611.07492", "contents": "Title: Inducing Interpretable Representations with Variational Autoencoders Abstract: We develop a framework for incorporating structured graphical models in the\n\\emph{encoders} of variational autoencoders (VAEs) that allows us to induce\ninterpretable representations through approximate variational inference. This\nallows us to both perform reasoning (e.g. classification) under the structural\nconstraints of a given graphical model, and use deep generative models to deal\nwith messy, high-dimensional domains where it is often difficult to model all\nthe variation. Learning in this framework is carried out end-to-end with a\nvariational objective, applying to both unsupervised and semi-supervised\nschemes. \n\n"}
{"id": "1611.07837", "contents": "Title: Adaptive Feature Abstraction for Translating Video to Text Abstract: Previous models for video captioning often use the output from a specific\nlayer of a Convolutional Neural Network (CNN) as video features. However, the\nvariable context-dependent semantics in the video may make it more appropriate\nto adaptively select features from the multiple CNN layers. We propose a new\napproach for generating adaptive spatiotemporal representations of videos for\nthe captioning task. A novel attention mechanism is developed, that adaptively\nand sequentially focuses on different layers of CNN features (levels of feature\n\"abstraction\"), as well as local spatiotemporal regions of the feature maps at\neach layer. The proposed approach is evaluated on three benchmark datasets:\nYouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the\nmodel works, these experiments quantitatively demonstrate the effectiveness of\nthe proposed adaptive spatiotemporal feature abstraction for translating videos\nto sentences with rich semantics. \n\n"}
{"id": "1611.07865", "contents": "Title: Controlling Perceptual Factors in Neural Style Transfer Abstract: Neural Style Transfer has shown very exciting results enabling new forms of\nimage manipulation. Here we extend the existing method to introduce control\nover spatial location, colour information and across spatial scale. We\ndemonstrate how this enhances the method by allowing high-resolution controlled\nstylisation and helps to alleviate common failure cases such as applying ground\ntextures to sky regions. Furthermore, by decomposing style into these\nperceptual factors we enable the combination of style information from multiple\nsources to generate new, perceptually appealing styles from existing ones. We\nalso describe how these methods can be used to more efficiently produce large\nsize, high-quality stylisation. Finally we show how the introduced control\nmeasures can be applied in recent methods for Fast Neural Style Transfer. \n\n"}
{"id": "1611.08036", "contents": "Title: Robotic Grasp Detection using Deep Convolutional Neural Networks Abstract: Deep learning has significantly advanced computer vision and natural language\nprocessing. While there have been some successes in robotics using deep\nlearning, it has not been widely adopted. In this paper, we present a novel\nrobotic grasp detection system that predicts the best grasping pose of a\nparallel-plate robotic gripper for novel objects using the RGB-D image of the\nscene. The proposed model uses a deep convolutional neural network to extract\nfeatures from the scene and then uses a shallow convolutional neural network to\npredict the grasp configuration for the object of interest. Our multi-modal\nmodel achieved an accuracy of 89.21% on the standard Cornell Grasp Dataset and\nruns at real-time speeds. This redefines the state-of-the-art for robotic grasp\ndetection. \n\n"}
{"id": "1611.08481", "contents": "Title: GuessWhat?! Visual object discovery through multi-modal dialogue Abstract: We introduce GuessWhat?!, a two-player guessing game as a testbed for\nresearch on the interplay of computer vision and dialogue systems. The goal of\nthe game is to locate an unknown object in a rich image scene by asking a\nsequence of questions. Higher-level image understanding, like spatial reasoning\nand language grounding, is required to solve the proposed task. Our key\ncontribution is the collection of a large-scale dataset consisting of 150K\nhuman-played games with a total of 800K visual question-answer pairs on 66K\nimages. We explain our design decisions in collecting the dataset and introduce\nthe oracle and questioner tasks that are associated with the two players of the\ngame. We prototyped deep learning models to establish initial baselines of the\nintroduced tasks. \n\n"}
{"id": "1611.08657", "contents": "Title: Convolutional Experts Constrained Local Model for Facial Landmark\n  Detection Abstract: Constrained Local Models (CLMs) are a well-established family of methods for\nfacial landmark detection. However, they have recently fallen out of favor to\ncascaded regression-based approaches. This is in part due to the inability of\nexisting CLM local detectors to model the very complex individual landmark\nappearance that is affected by expression, illumination, facial hair, makeup,\nand accessories. In our work, we present a novel local detector --\nConvolutional Experts Network (CEN) -- that brings together the advantages of\nneural architectures and mixtures of experts in an end-to-end framework. We\nfurther propose a Convolutional Experts Constrained Local Model (CE-CLM)\nalgorithm that uses CEN as local detectors. We demonstrate that our proposed\nCE-CLM algorithm outperforms competitive state-of-the-art baselines for facial\nlandmark detection by a large margin on four publicly-available datasets. Our\napproach is especially accurate and robust on challenging profile images. \n\n"}
{"id": "1612.00542", "contents": "Title: Breast Mass Classification from Mammograms using Deep Convolutional\n  Neural Networks Abstract: Mammography is the most widely used method to screen breast cancer. Because\nof its mostly manual nature, variability in mass appearance, and low\nsignal-to-noise ratio, a significant number of breast masses are missed or\nmisdiagnosed. In this work, we present how Convolutional Neural Networks can be\nused to directly classify pre-segmented breast masses in mammograms as benign\nor malignant, using a combination of transfer learning, careful pre-processing\nand data augmentation to overcome limited training data. We achieve\nstate-of-the-art results on the DDSM dataset, surpassing human performance, and\nshow interpretability of our model. \n\n"}
{"id": "1612.00563", "contents": "Title: Self-critical Sequence Training for Image Captioning Abstract: Recently it has been shown that policy-gradient methods for reinforcement\nlearning can be utilized to train deep end-to-end systems directly on\nnon-differentiable metrics for the task at hand. In this paper we consider the\nproblem of optimizing image captioning systems using reinforcement learning,\nand show that by carefully optimizing our systems using the test metrics of the\nMSCOCO task, significant gains in performance can be realized. Our systems are\nbuilt using a new optimization approach that we call self-critical sequence\ntraining (SCST). SCST is a form of the popular REINFORCE algorithm that, rather\nthan estimating a \"baseline\" to normalize the rewards and reduce variance,\nutilizes the output of its own test-time inference algorithm to normalize the\nrewards it experiences. Using this approach, estimating the reward signal (as\nactor-critic methods must do) and estimating normalization (as REINFORCE\nalgorithms typically do) is avoided, while at the same time harmonizing the\nmodel with respect to its test-time inference procedure. Empirically we find\nthat directly optimizing the CIDEr metric with SCST and greedy decoding at\ntest-time is highly effective. Our results on the MSCOCO evaluation sever\nestablish a new state-of-the-art on the task, improving the best result in\nterms of CIDEr from 104.9 to 114.7. \n\n"}
{"id": "1612.01543", "contents": "Title: Towards the Limit of Network Quantization Abstract: Network quantization is one of network compression techniques to reduce the\nredundancy of deep neural networks. It reduces the number of distinct network\nparameter values by quantization in order to save the storage for them. In this\npaper, we design network quantization schemes that minimize the performance\nloss due to quantization given a compression ratio constraint. We analyze the\nquantitative relation of quantization errors to the neural network loss\nfunction and identify that the Hessian-weighted distortion measure is locally\nthe right objective function for the optimization of network quantization. As a\nresult, Hessian-weighted k-means clustering is proposed for clustering network\nparameters to quantize. When optimal variable-length binary codes, e.g.,\nHuffman codes, are employed for further compression, we derive that the network\nquantization problem can be related to the entropy-constrained scalar\nquantization (ECSQ) problem in information theory and consequently propose two\nsolutions of ECSQ for network quantization, i.e., uniform quantization and an\niterative solution similar to Lloyd's algorithm. Finally, using the simple\nuniform quantization followed by Huffman coding, we show from our experiments\nthat the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet,\n32-layer ResNet and AlexNet, respectively. \n\n"}
{"id": "1612.01928", "contents": "Title: Invariant Representations for Noisy Speech Recognition Abstract: Modern automatic speech recognition (ASR) systems need to be robust under\nacoustic variability arising from environmental, speaker, channel, and\nrecording conditions. Ensuring such robustness to variability is a challenge in\nmodern day neural network-based ASR systems, especially when all types of\nvariability are not seen during training. We attempt to address this problem by\nencouraging the neural network acoustic model to learn invariant feature\nrepresentations. We use ideas from recent research on image generation using\nGenerative Adversarial Networks and domain adaptation ideas extending\nadversarial gradient-based training. A recent work from Ganin et al. proposes\nto use adversarial training for image domain adaptation by using an\nintermediate representation from the main target classification network to\ndeteriorate the domain classifier performance through a separate neural\nnetwork. Our work focuses on investigating neural architectures which produce\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\nshow that our method generalizes better than the standard multi-condition\ntraining especially when only a few noise categories are seen during training. \n\n"}
{"id": "1612.01981", "contents": "Title: Core Sampling Framework for Pixel Classification Abstract: The intermediate map responses of a Convolutional Neural Network (CNN)\ncontain information about an image that can be used to extract contextual\nknowledge about it. In this paper, we present a core sampling framework that is\nable to use these activation maps from several layers as features to another\nneural network using transfer learning to provide an understanding of an input\nimage. Our framework creates a representation that combines features from the\ntest data and the contextual knowledge gained from the responses of a\npretrained network, processes it and feeds it to a separate Deep Belief\nNetwork. We use this representation to extract more information from an image\nat the pixel level, hence gaining understanding of the whole image. We\nexperimentally demonstrate the usefulness of our framework using a pretrained\nVGG-16 model to perform segmentation on the BAERI dataset of Synthetic Aperture\nRadar(SAR) imagery and the CAMVID dataset. \n\n"}
{"id": "1612.01988", "contents": "Title: Local Group Invariant Representations via Orbit Embeddings Abstract: Invariance to nuisance transformations is one of the desirable properties of\neffective representations. We consider transformations that form a \\emph{group}\nand propose an approach based on kernel methods to derive local group invariant\nrepresentations. Locality is achieved by defining a suitable probability\ndistribution over the group which in turn induces distributions in the input\nfeature space. We learn a decision function over these distributions by\nappealing to the powerful framework of kernel methods and generate local\ninvariant random feature maps via kernel approximations. We show uniform\nconvergence bounds for kernel approximation and provide excess risk bounds for\nlearning with these features. We evaluate our method on three real datasets,\nincluding Rotated MNIST and CIFAR-10, and observe that it outperforms competing\nkernel based approaches. The proposed method also outperforms deep CNN on\nRotated-MNIST and performs comparably to the recently proposed\ngroup-equivariant CNN. \n\n"}
{"id": "1612.02095", "contents": "Title: ExtremeWeather: A large-scale climate dataset for semi-supervised\n  detection, localization, and understanding of extreme weather events Abstract: Then detection and identification of extreme weather events in large-scale\nclimate simulations is an important problem for risk management, informing\ngovernmental policy decisions and advancing our basic understanding of the\nclimate system. Recent work has shown that fully supervised convolutional\nneural networks (CNNs) can yield acceptable accuracy for classifying well-known\ntypes of extreme weather events when large amounts of labeled data are\navailable. However, many different types of spatially localized climate\npatterns are of interest including hurricanes, extra-tropical cyclones, weather\nfronts, and blocking events among others. Existing labeled data for these\npatterns can be incomplete in various ways, such as covering only certain years\nor geographic areas and having false negatives. This type of climate data\ntherefore poses a number of interesting machine learning challenges. We present\na multichannel spatiotemporal CNN architecture for semi-supervised bounding box\nprediction and exploratory data analysis. We demonstrate that our approach is\nable to leverage temporal information and unlabeled data to improve the\nlocalization of extreme weather events. Further, we explore the representations\nlearned by our model in order to better understand this important data. We\npresent a dataset, ExtremeWeather, to encourage machine learning research in\nthis area and to help facilitate further work in understanding and mitigating\nthe effects of climate change. The dataset is available at\nextremeweatherdataset.github.io and the code is available at\nhttps://github.com/eracah/hur-detect. \n\n"}
{"id": "1612.03350", "contents": "Title: Non-negative Factorization of the Occurrence Tensor from Financial\n  Contracts Abstract: We propose an algorithm for the non-negative factorization of an occurrence\ntensor built from heterogeneous networks. We use l0 norm to model sparse errors\nover discrete values (occurrences), and use decomposed factors to model the\nembedded groups of nodes. An efficient splitting method is developed to\noptimize the nonconvex and nonsmooth objective. We study both synthetic\nproblems and a new dataset built from financial documents, resMBS. \n\n"}
{"id": "1612.04844", "contents": "Title: The More You Know: Using Knowledge Graphs for Image Classification Abstract: One characteristic that sets humans apart from modern learning-based computer\nvision algorithms is the ability to acquire knowledge about the world and use\nthat knowledge to reason about the visual world. Humans can learn about the\ncharacteristics of objects and the relationships that occur between them to\nlearn a large variety of visual concepts, often with few examples. This paper\ninvestigates the use of structured prior knowledge in the form of knowledge\ngraphs and shows that using this knowledge improves performance on image\nclassification. We build on recent work on end-to-end learning on graphs,\nintroducing the Graph Search Neural Network as a way of efficiently\nincorporating large knowledge graphs into a vision classification pipeline. We\nshow in a number of experiments that our method outperforms standard neural\nnetwork baselines for multi-label classification. \n\n"}
{"id": "1612.05086", "contents": "Title: Coupling Adaptive Batch Sizes with Learning Rates Abstract: Mini-batch stochastic gradient descent and variants thereof have become\nstandard for large-scale empirical risk minimization like the training of\nneural networks. These methods are usually used with a constant batch size\nchosen by simple empirical inspection. The batch size significantly influences\nthe behavior of the stochastic optimization algorithm, though, since it\ndetermines the variance of the gradient estimates. This variance also changes\nover the optimization process; when using a constant batch size, stability and\nconvergence is thus often enforced by means of a (manually tuned) decreasing\nlearning rate schedule.\n  We propose a practical method for dynamic batch size adaptation. It estimates\nthe variance of the stochastic gradients and adapts the batch size to decrease\nthe variance proportionally to the value of the objective function, removing\nthe need for the aforementioned learning rate decrease. In contrast to recent\nrelated work, our algorithm couples the batch size to the learning rate,\ndirectly reflecting the known relationship between the two. On popular image\nclassification benchmarks, our batch size adaptation yields faster optimization\nconvergence, while simultaneously simplifying learning rate tuning. A\nTensorFlow implementation is available. \n\n"}
{"id": "1612.06851", "contents": "Title: Beyond Skip Connections: Top-Down Modulation for Object Detection Abstract: In recent years, we have seen tremendous progress in the field of object\ndetection. Most of the recent improvements have been achieved by targeting\ndeeper feedforward networks. However, many hard object categories such as\nbottle, remote, etc. require representation of fine details and not just\ncoarse, semantic representations. But most of these fine details are lost in\nthe early convolutional layers. What we need is a way to incorporate finer\ndetails from lower layers into the detection architecture. Skip connections\nhave been proposed to combine high-level and low-level features, but we argue\nthat selecting the right features from low-level requires top-down contextual\ninformation. Inspired by the human visual pathway, in this paper we propose\ntop-down modulations as a way to incorporate fine details into the detection\nframework. Our approach supplements the standard bottom-up, feedforward ConvNet\nwith a top-down modulation (TDM) network, connected using lateral connections.\nThese connections are responsible for the modulation of lower layer filters,\nand the top-down network handles the selection and integration of contextual\ninformation and low-level features. The proposed TDM architecture provides a\nsignificant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,\n35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any\nbells and whistles (e.g., multi-scale, iterative box refinement, etc.). \n\n"}
{"id": "1612.07310", "contents": "Title: Beyond Holistic Object Recognition: Enriching Image Understanding with\n  Part States Abstract: Important high-level vision tasks such as human-object interaction, image\ncaptioning and robotic manipulation require rich semantic descriptions of\nobjects at part level. Based upon previous work on part localization, in this\npaper, we address the problem of inferring rich semantics imparted by an object\npart in still images. We propose to tokenize the semantic space as a discrete\nset of part states. Our modeling of part state is spatially localized,\ntherefore, we formulate the part state inference problem as a pixel-wise\nannotation problem. An iterative part-state inference neural network is\nspecifically designed for this task, which is efficient in time and accurate in\nperformance. Extensive experiments demonstrate that the proposed method can\neffectively predict the semantic states of parts and simultaneously correct\nlocalization errors, thus benefiting a few visual understanding applications.\nThe other contribution of this paper is our part state dataset which contains\nrich part-level semantic annotations. \n\n"}
{"id": "1701.00458", "contents": "Title: Deep-HiTS: Rotation Invariant Convolutional Neural Network for Transient\n  Detection Abstract: We introduce Deep-HiTS, a rotation invariant convolutional neural network\n(CNN) model for classifying images of transients candidates into artifacts or\nreal sources for the High cadence Transient Survey (HiTS). CNNs have the\nadvantage of learning the features automatically from the data while achieving\nhigh performance. We compare our CNN model against a feature engineering\napproach using random forests (RF). We show that our CNN significantly\noutperforms the RF model reducing the error by almost half. Furthermore, for a\nfixed number of approximately 2,000 allowed false transient candidates per\nnight we are able to reduce the miss-classified real transients by\napproximately 1/5. To the best of our knowledge, this is the first time CNNs\nhave been used to detect astronomical transient events. Our approach will be\nvery useful when processing images from next generation instruments such as the\nLarge Synoptic Survey Telescope (LSST). We have made all our code and data\navailable to the community for the sake of allowing further developments and\ncomparisons at https://github.com/guille-c/Deep-HiTS. \n\n"}
{"id": "1701.03151", "contents": "Title: Guaranteed Parameter Estimation for Discrete Energy Minimization Abstract: Structural learning, a method to estimate the parameters for discrete energy\nminimization, has been proven to be effective in solving computer vision\nproblems, especially in 3D scene parsing. As the complexity of the models\nincreases, structural learning algorithms turn to approximate inference to\nretain tractability. Unfortunately, such methods often fail because the\napproximation can be arbitrarily poor. In this work, we propose a method to\novercome this limitation through exploiting the properties of the joint problem\nof training time inference and learning. With the help of the learning\nframework, we transform the inapproximable inference problem into a polynomial\ntime solvable one, thereby enabling tractable exact inference while still\nallowing an arbitrary graph structure and full potential interactions. Our\nlearning algorithm is guaranteed to return a solution with a bounded error to\nthe global optimal within the feasible parameter space. We demonstrate the\neffectiveness of this method on two point cloud scene parsing datasets. Our\napproach runs much faster and solves a problem that is intractable for\nprevious, well-known approaches. \n\n"}
{"id": "1701.03439", "contents": "Title: Comprehension-guided referring expressions Abstract: We consider generation and comprehension of natural language referring\nexpression for objects in an image. Unlike generic \"image captioning\" which\nlacks natural standard evaluation criteria, quality of a referring expression\nmay be measured by the receiver's ability to correctly infer which object is\nbeing described. Following this intuition, we propose two approaches to utilize\nmodels trained for comprehension task to generate better expressions. First, we\nuse a comprehension module trained on human-generated expressions, as a\n\"critic\" of referring expression generator. The comprehension module serves as\na differentiable proxy of human evaluation, providing training signal to the\ngeneration module. Second, we use the comprehension module in a\ngenerate-and-rerank pipeline, which chooses from candidate expressions\ngenerated by a model according to their performance on the comprehension task.\nWe show that both approaches lead to improved referring expression generation\non multiple benchmark datasets. \n\n"}
{"id": "1701.03555", "contents": "Title: Active Self-Paced Learning for Cost-Effective and Progressive Face\n  Identification Abstract: This paper aims to develop a novel cost-effective framework for face\nidentification, which progressively maintains a batch of classifiers with the\nincreasing face images of different individuals. By naturally combining two\nrecently rising techniques: active learning (AL) and self-paced learning (SPL),\nour framework is capable of automatically annotating new instances and\nincorporating them into training under weak expert re-certification. We first\ninitialize the classifier using a few annotated samples for each individual,\nand extract image features using the convolutional neural nets. Then, a number\nof candidates are selected from the unannotated samples for classifier\nupdating, in which we apply the current classifiers ranking the samples by the\nprediction confidence. In particular, our approach utilizes the high-confidence\nand low-confidence samples in the self-paced and the active user-query way,\nrespectively. The neural nets are later fine-tuned based on the updated\nclassifiers. Such heuristic implementation is formulated as solving a concise\nactive SPL optimization problem, which also advances the SPL development by\nsupplementing a rational dynamic curriculum constraint. The new model finely\naccords with the \"instructor-student-collaborative\" learning mode in human\neducation. The advantages of this proposed framework are two-folds: i) The\nrequired number of annotated samples is significantly decreased while the\ncomparable performance is guaranteed. A dramatic reduction of user effort is\nalso achieved over other state-of-the-art active learning techniques. ii) The\nmixture of SPL and AL effectively improves not only the classifier accuracy\ncompared to existing AL/SPL methods but also the robustness against noisy data.\nWe evaluate our framework on two challenging datasets, and demonstrate very\npromising results. (http://hcp.sysu.edu.cn/projects/aspl/) \n\n"}
{"id": "1701.03779", "contents": "Title: Tumour Ellipsification in Ultrasound Images for Treatment Prediction in\n  Breast Cancer Abstract: Recent advances in using quantitative ultrasound (QUS) methods have provided\na promising framework to non-invasively and inexpensively monitor or predict\nthe effectiveness of therapeutic cancer responses. One of the earliest steps in\nusing QUS methods is contouring a region of interest (ROI) inside the tumour in\nultrasound B-mode images. While manual segmentation is a very time-consuming\nand tedious task for human experts, auto-contouring is also an extremely\ndifficult task for computers due to the poor quality of ultrasound B-mode\nimages. However, for the purpose of cancer response prediction, a rough\nboundary of the tumour as an ROI is only needed. In this research, a\nsemi-automated tumour localization approach is proposed for ROI estimation in\nultrasound B-mode images acquired from patients with locally advanced breast\ncancer (LABC). The proposed approach comprised several modules, including 1)\nfeature extraction using keypoint descriptors, 2) augmenting the feature\ndescriptors with the distance of the keypoints to the user-input pixel as the\ncentre of the tumour, 3) supervised learning using a support vector machine\n(SVM) to classify keypoints as \"tumour\" or \"non-tumour\", and 4) computation of\nan ellipse as an outline of the ROI representing the tumour. Experiments with\n33 B-mode images from 10 LABC patients yielded promising results with an\naccuracy of 76.7% based on the Dice coefficient performance measure. The\nresults demonstrated that the proposed method can potentially be used as the\nfirst stage in a computer-assisted cancer response prediction system for\nsemi-automated contouring of breast tumours. \n\n"}
{"id": "1701.04928", "contents": "Title: Bringing Impressionism to Life with Neural Style Transfer in Come Swim Abstract: Neural Style Transfer is a striking, recently-developed technique that uses\nneural networks to artistically redraw an image in the style of a source style\nimage. This paper explores the use of this technique in a production setting,\napplying Neural Style Transfer to redraw key scenes in 'Come Swim' in the style\nof the impressionistic painting that inspired the film. We document how the\ntechnique can be driven within the framework of an iterative creative process\nto achieve a desired look, and propose a mapping of the broad parameter space\nto a key set of creative controls. We hope that this mapping can provide\ninsights into priorities for future research. \n\n"}
{"id": "1701.05369", "contents": "Title: Variational Dropout Sparsifies Deep Neural Networks Abstract: We explore a recently proposed Variational Dropout technique that provided an\nelegant Bayesian interpretation to Gaussian Dropout. We extend Variational\nDropout to the case when dropout rates are unbounded, propose a way to reduce\nthe variance of the gradient estimator and report first experimental results\nwith individual dropout rates per weight. Interestingly, it leads to extremely\nsparse solutions both in fully-connected and convolutional layers. This effect\nis similar to automatic relevance determination effect in empirical Bayes but\nhas a number of advantages. We reduce the number of parameters up to 280 times\non LeNet architectures and up to 68 times on VGG-like networks with a\nnegligible decrease of accuracy. \n\n"}
{"id": "1701.06106", "contents": "Title: Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a\n  Changing World Abstract: In this paper, we focus on online representation learning in non-stationary\nenvironments which may require continuous adaptation of model architecture. We\npropose a novel online dictionary-learning (sparse-coding) framework which\nincorporates the addition and deletion of hidden units (dictionary elements),\nand is inspired by the adult neurogenesis phenomenon in the dentate gyrus of\nthe hippocampus, known to be associated with improved cognitive function and\nadaptation to new environments. In the online learning setting, where new input\ninstances arrive sequentially in batches, the neuronal-birth is implemented by\nadding new units with random initial weights (random dictionary elements); the\nnumber of new units is determined by the current performance (representation\nerror) of the dictionary, higher error causing an increase in the birth rate.\nNeuronal-death is implemented by imposing l1/l2-regularization (group sparsity)\non the dictionary within the block-coordinate descent optimization at each\niteration of our online alternating minimization scheme, which iterates between\nthe code and dictionary updates. Finally, hidden unit connectivity adaptation\nis facilitated by introducing sparsity in dictionary elements. Our empirical\nevaluation on several real-life datasets (images and language) as well as on\nsynthetic data demonstrates that the proposed approach can considerably\noutperform the state-of-art fixed-size (nonadaptive) online sparse coding of\nMairal et al. (2009) in the presence of nonstationary data. Moreover, we\nidentify certain properties of the data (e.g., sparse inputs with nearly\nnon-overlapping supports) and of the model (e.g., dictionary sparsity)\nassociated with such improvements. \n\n"}
{"id": "1701.06452", "contents": "Title: Learning what to look in chest X-rays with a recurrent visual attention\n  model Abstract: X-rays are commonly performed imaging tests that use small amounts of\nradiation to produce pictures of the organs, tissues, and bones of the body.\nX-rays of the chest are used to detect abnormalities or diseases of the\nairways, blood vessels, bones, heart, and lungs. In this work we present a\nstochastic attention-based model that is capable of learning what regions\nwithin a chest X-ray scan should be visually explored in order to conclude that\nthe scan contains a specific radiological abnormality. The proposed model is a\nrecurrent neural network (RNN) that learns to sequentially sample the entire\nX-ray and focus only on informative areas that are likely to contain the\nrelevant information. We report on experiments carried out with more than\n$100,000$ X-rays containing enlarged hearts or medical devices. The model has\nbeen trained using reinforcement learning methods to learn task-specific\npolicies. \n\n"}
{"id": "1701.06772", "contents": "Title: Training Group Orthogonal Neural Networks with Privileged Information Abstract: Learning rich and diverse representations is critical for the performance of\ndeep convolutional neural networks (CNNs). In this paper, we consider how to\nuse privileged information to promote inherent diversity of a single CNN model\nsuch that the model can learn better representations and offer stronger\ngeneralization ability. To this end, we propose a novel group orthogonal\nconvolutional neural network (GoCNN) that learns untangled representations\nwithin each layer by exploiting provided privileged information and enhances\nrepresentation diversity effectively. We take image classification as an\nexample where image segmentation annotations are used as privileged information\nduring the training process. Experiments on two benchmark datasets -- ImageNet\nand PASCAL VOC -- clearly demonstrate the strong generalization ability of our\nproposed GoCNN model. On the ImageNet dataset, GoCNN improves the performance\nof state-of-the-art ResNet-152 model by absolute value of 1.2% while only uses\nprivileged information of 10% of the training images, confirming effectiveness\nof GoCNN on utilizing available privileged knowledge to train better CNNs. \n\n"}
{"id": "1702.00338", "contents": "Title: Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval Abstract: This paper addresses the problem of large scale image retrieval, with the aim\nof accurately ranking the similarity of a large number of images to a given\nquery image. To achieve this, we propose a novel Siamese network. This network\nconsists of two computational strands, each comprising of a CNN component\nfollowed by a Fisher vector component. The CNN component produces dense, deep\nconvolutional descriptors that are then aggregated by the Fisher Vector method.\nCrucially, we propose to simultaneously learn both the CNN filter weights and\nFisher Vector model parameters. This allows us to account for the evolving\ndistribution of deep descriptors over the course of the learning process. We\nshow that the proposed approach gives significant improvements over the\nstate-of-the-art methods on the Oxford and Paris image retrieval datasets.\nAdditionally, we provide a baseline performance measure for both these datasets\nwith the inclusion of 1 million distractors. \n\n"}
{"id": "1702.01636", "contents": "Title: Slice-to-volume medical image registration: a survey Abstract: During the last decades, the research community of medical imaging has\nwitnessed continuous advances in image registration methods, which pushed the\nlimits of the state-of-the-art and enabled the development of novel medical\nprocedures. A particular type of image registration problem, known as\nslice-to-volume registration, played a fundamental role in areas like image\nguided surgeries and volumetric image reconstruction. However, to date, and\ndespite the extensive literature available on this topic, no survey has been\nwritten to discuss this challenging problem. This paper introduces the first\ncomprehensive survey of the literature about slice-to-volume registration,\npresenting a categorical study of the algorithms according to an ad-hoc\ntaxonomy and analyzing advantages and disadvantages of every category. We draw\nsome general conclusions from this analysis and present our perspectives on the\nfuture of the field. \n\n"}
{"id": "1702.02284", "contents": "Title: Adversarial Attacks on Neural Network Policies Abstract: Machine learning classifiers are known to be vulnerable to inputs maliciously\nconstructed by adversaries to force misclassification. Such adversarial\nexamples have been extensively studied in the context of computer vision\napplications. In this work, we show adversarial attacks are also effective when\ntargeting neural network policies in reinforcement learning. Specifically, we\nshow existing adversarial example crafting techniques can be used to\nsignificantly degrade test-time performance of trained policies. Our threat\nmodel considers adversaries capable of introducing small perturbations to the\nraw input of the policy. We characterize the degree of vulnerability across\ntasks and training algorithms, for a subclass of adversarial-example attacks in\nwhite-box and black-box settings. Regardless of the learned task or training\nalgorithm, we observe a significant drop in performance, even with small\nadversarial perturbations that do not interfere with human perception. Videos\nare available at http://rll.berkeley.edu/adversarial. \n\n"}
{"id": "1702.03044", "contents": "Title: Incremental Network Quantization: Towards Lossless CNNs with\n  Low-Precision Weights Abstract: This paper presents incremental network quantization (INQ), a novel method,\ntargeting to efficiently convert any pre-trained full-precision convolutional\nneural network (CNN) model into a low-precision version whose weights are\nconstrained to be either powers of two or zero. Unlike existing methods which\nare struggled in noticeable accuracy loss, our INQ has the potential to resolve\nthis issue, as benefiting from two innovations. On one hand, we introduce three\ninterdependent operations, namely weight partition, group-wise quantization and\nre-training. A well-proven measure is employed to divide the weights in each\nlayer of a pre-trained CNN model into two disjoint groups. The weights in the\nfirst group are responsible to form a low-precision base, thus they are\nquantized by a variable-length encoding method. The weights in the other group\nare responsible to compensate for the accuracy loss from the quantization, thus\nthey are the ones to be re-trained. On the other hand, these three operations\nare repeated on the latest re-trained group in an iterative manner until all\nthe weights are converted into low-precision ones, acting as an incremental\nnetwork quantization and accuracy enhancement procedure. Extensive experiments\non the ImageNet classification task using almost all known deep CNN\narchitectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the\nefficacy of the proposed method. Specifically, at 5-bit quantization, our\nmodels have improved accuracy than the 32-bit floating-point references. Taking\nResNet-18 as an example, we further show that our quantized models with 4-bit,\n3-bit and 2-bit ternary weights have improved or very similar accuracy against\nits 32-bit floating-point baseline. Besides, impressive results with the\ncombination of network pruning and INQ are also reported. The code is available\nat https://github.com/Zhouaojun/Incremental-Network-Quantization. \n\n"}
{"id": "1702.04114", "contents": "Title: Graph Based Over-Segmentation Methods for 3D Point Clouds Abstract: Over-segmentation, or super-pixel generation, is a common preliminary stage\nfor many computer vision applications. New acquisition technologies enable the\ncapturing of 3D point clouds that contain color and geometrical information.\nThis 3D information introduces a new conceptual change that can be utilized to\nimprove the results of over-segmentation, which uses mainly color information,\nand to generate clusters of points we call super-points. We consider a variety\nof possible 3D extensions of the Local Variation (LV) graph based\nover-segmentation algorithms, and compare them thoroughly. We consider\ndifferent alternatives for constructing the connectivity graph, for assigning\nthe edge weights, and for defining the merge criterion, which must now account\nfor the geometric information and not only color. Following this evaluation, we\nderive a new generic algorithm for over-segmentation of 3D point clouds. We\ncall this new algorithm Point Cloud Local Variation (PCLV). The advantages of\nthe new over-segmentation algorithm are demonstrated on both outdoor and\ncluttered indoor scenes. Performance analysis of the proposed approach compared\nto state-of-the-art 2D and 3D over-segmentation algorithms shows significant\nimprovement according to the common performance measures. \n\n"}
{"id": "1702.04267", "contents": "Title: On Detecting Adversarial Perturbations Abstract: Machine learning and deep learning in particular has advanced tremendously on\nperceptual tasks in recent years. However, it remains vulnerable against\nadversarial perturbations of the input that have been crafted specifically to\nfool the system while being quasi-imperceptible to a human. In this work, we\npropose to augment deep neural networks with a small \"detector\" subnetwork\nwhich is trained on the binary classification task of distinguishing genuine\ndata from data containing adversarial perturbations. Our method is orthogonal\nto prior work on addressing adversarial perturbations, which has mostly focused\non making the classification network itself more robust. We show empirically\nthat adversarial perturbations can be detected surprisingly well even though\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\ntrained to detect only a specific adversary, they generalize to similar and\nweaker adversaries. In addition, we propose an adversarial attack that fools\nboth the classifier and the detector and a novel training procedure for the\ndetector that counteracts this attack. \n\n"}
{"id": "1702.05464", "contents": "Title: Adversarial Discriminative Domain Adaptation Abstract: Adversarial learning methods are a promising approach to training robust deep\nnetworks, and can generate complex samples across diverse domains. They also\ncan improve recognition despite the presence of domain shift or dataset bias:\nseveral adversarial approaches to unsupervised domain adaptation have recently\nbeen introduced, which reduce the difference between the training and test\ndomain distributions and thus improve generalization performance. Prior\ngenerative approaches show compelling visualizations, but are not optimal on\ndiscriminative tasks and can be limited to smaller shifts. Prior discriminative\napproaches could handle larger domain shifts, but imposed tied weights on the\nmodel and did not exploit a GAN-based loss. We first outline a novel\ngeneralized framework for adversarial adaptation, which subsumes recent\nstate-of-the-art approaches as special cases, and we use this generalized view\nto better relate the prior approaches. We propose a previously unexplored\ninstance of our general framework which combines discriminative modeling,\nuntied weight sharing, and a GAN loss, which we call Adversarial Discriminative\nDomain Adaptation (ADDA). We show that ADDA is more effective yet considerably\nsimpler than competing domain-adversarial methods, and demonstrate the promise\nof our approach by exceeding state-of-the-art unsupervised adaptation results\non standard cross-domain digit classification tasks and a new more difficult\ncross-modality object classification task. \n\n"}
{"id": "1702.05993", "contents": "Title: An Extended Framework for Marginalized Domain Adaptation Abstract: We propose an extended framework for marginalized domain adaptation, aimed at\naddressing unsupervised, supervised and semi-supervised scenarios. We argue\nthat the denoising principle should be extended to explicitly promote\ndomain-invariant features as well as help the classification task. Therefore we\npropose to jointly learn the data auto-encoders and the target classifiers.\nFirst, in order to make the denoised features domain-invariant, we propose a\ndomain regularization that may be either a domain prediction loss or a maximum\nmean discrepancy between the source and target data. The noise marginalization\nin this case is reduced to solving the linear matrix system $AX=B$ which has a\nclosed-form solution. Second, in order to help the classification, we include a\nclass regularization term. Adding this component reduces the learning problem\nto solving a Sylvester linear matrix equation $AX+BX=C$, for which an efficient\niterative procedure exists as well. We did an extensive study to assess how\nthese regularization terms improve the baseline performance in the three domain\nadaptation scenarios and present experimental results on two image and one text\nbenchmark datasets, conventionally used for validating domain adaptation\nmethods. We report our findings and comparison with state-of-the-art methods. \n\n"}
{"id": "1702.06086", "contents": "Title: Label Distribution Learning Forests Abstract: Label distribution learning (LDL) is a general learning framework, which\nassigns to an instance a distribution over a set of labels rather than a single\nlabel or multiple labels. Current LDL methods have either restricted\nassumptions on the expression form of the label distribution or limitations in\nrepresentation learning, e.g., to learn deep features in an end-to-end manner.\nThis paper presents label distribution learning forests (LDLFs) - a novel label\ndistribution learning algorithm based on differentiable decision trees, which\nhave several advantages: 1) Decision trees have the potential to model any\ngeneral form of label distributions by a mixture of leaf node predictions. 2)\nThe learning of differentiable decision trees can be combined with\nrepresentation learning. We define a distribution-based loss function for a\nforest, enabling all the trees to be learned jointly, and show that an update\nfunction for leaf node predictions, which guarantees a strict decrease of the\nloss function, can be derived by variational bounding. The effectiveness of the\nproposed LDLFs is verified on several LDL tasks and a computer vision\napplication, showing significant improvements to the state-of-the-art LDL\nmethods. \n\n"}
{"id": "1702.06257", "contents": "Title: The Power of Sparsity in Convolutional Neural Networks Abstract: Deep convolutional networks are well-known for their high computational and\nmemory demands. Given limited resources, how does one design a network that\nbalances its size, training time, and prediction accuracy? A surprisingly\neffective approach to trade accuracy for size and speed is to simply reduce the\nnumber of channels in each convolutional layer by a fixed fraction and retrain\nthe network. In many cases this leads to significantly smaller networks with\nonly minimal changes to accuracy. In this paper, we take a step further by\nempirically examining a strategy for deactivating connections between filters\nin convolutional layers in a way that allows us to harvest savings both in\nrun-time and memory for many network architectures. More specifically, we\ngeneralize 2D convolution to use a channel-wise sparse connection structure and\nshow that this leads to significantly better results than the baseline approach\nfor large networks including VGG and Inception V3. \n\n"}
{"id": "1702.06318", "contents": "Title: Is Saki #delicious? The Food Perception Gap on Instagram and Its\n  Relation to Health Abstract: Food is an integral part of our life and what and how much we eat crucially\naffects our health. Our food choices largely depend on how we perceive certain\ncharacteristics of food, such as whether it is healthy, delicious or if it\nqualifies as a salad. But these perceptions differ from person to person and\none person's \"single lettuce leaf\" might be another person's \"side salad\".\nStudying how food is perceived in relation to what it actually is typically\ninvolves a laboratory setup. Here we propose to use recent advances in image\nrecognition to tackle this problem. Concretely, we use data for 1.9 million\nimages from Instagram from the US to look at systematic differences in how a\nmachine would objectively label an image compared to how a human subjectively\ndoes. We show that this difference, which we call the \"perception gap\", relates\nto a number of health outcomes observed at the county level. To the best of our\nknowledge, this is the first time that image recognition is being used to study\nthe \"misalignment\" of how people describe food images vs. what they actually\ndepict. \n\n"}
{"id": "1702.07025", "contents": "Title: Convolutional Neural Network Committees for Melanoma Classification with\n  Classical And Expert Knowledge Based Image Transforms Data Augmentation Abstract: Skin cancer is a major public health problem, as is the most common type of\ncancer and represents more than half of cancer diagnoses worldwide. Early\ndetection influences the outcome of the disease and motivates our work. We\ninvestigate the composition of CNN committees and data augmentation for the the\nISBI 2017 Melanoma Classification Challenge (named Skin Lesion Analysis towards\nMelanoma Detection) facing the peculiarities of dealing with such a small,\nunbalanced, biological database. For that, we explore committees of\nConvolutional Neural Networks trained over the ISBI challenge training dataset\nartificially augmented by both classical image processing transforms and image\nwarping guided by specialist knowledge about the lesion axis and improve the\nfinal classifier invariance to common melanoma variations. \n\n"}
{"id": "1702.08530", "contents": "Title: Semi-parametric Network Structure Discovery Models Abstract: We propose a network structure discovery model for continuous observations\nthat generalizes linear causal models by incorporating a Gaussian process (GP)\nprior on a network-independent component, and random sparsity and weight\nmatrices as the network-dependent parameters. This approach provides flexible\nmodeling of network-independent trends in the observations as well as\nuncertainty quantification around the discovered network structure. We\nestablish a connection between our model and multi-task GPs and develop an\nefficient stochastic variational inference algorithm for it. Furthermore, we\nformally show that our approach is numerically stable and in fact numerically\neasy to carry out almost everywhere on the support of the random variables\ninvolved. Finally, we evaluate our model on three applications, showing that it\noutperforms previous approaches. We provide a qualitative and quantitative\nanalysis of the structures discovered for domains such as the study of the full\ngenome regulation of the yeast Saccharomyces cerevisiae. \n\n"}
{"id": "1702.08840", "contents": "Title: Iterative Bayesian Learning for Crowdsourced Regression Abstract: Crowdsourcing platforms emerged as popular venues for purchasing human\nintelligence at low cost for large volume of tasks. As many low-paid workers\nare prone to give noisy answers, a common practice is to add redundancy by\nassigning multiple workers to each task and then simply average out these\nanswers. However, to fully harness the wisdom of the crowd, one needs to learn\nthe heterogeneous quality of each worker. We resolve this fundamental challenge\nin crowdsourced regression tasks, i.e., the answer takes continuous labels,\nwhere identifying good or bad workers becomes much more non-trivial compared to\na classification setting of discrete labels. In particular, we introduce a\nBayesian iterative scheme and show that it provably achieves the optimal mean\nsquared error. Our evaluations on synthetic and real-world datasets support our\ntheoretical results and show the superiority of the proposed scheme. \n\n"}
{"id": "1703.00837", "contents": "Title: Meta Networks Abstract: Neural networks have been successfully applied in applications with a large\namount of labeled data. However, the task of rapid generalization on new\nconcepts with small training data while preserving performances on previously\nlearned ones still presents a significant challenge to neural network models.\nIn this work, we introduce a novel meta learning method, Meta Networks\n(MetaNet), that learns a meta-level knowledge across tasks and shifts its\ninductive biases via fast parameterization for rapid generalization. When\nevaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve\na near human-level performance and outperform the baseline approaches by up to\n6% accuracy. We demonstrate several appealing properties of MetaNet relating to\ngeneralization and continual learning. \n\n"}
{"id": "1703.00862", "contents": "Title: Binarized Convolutional Landmark Localizers for Human Pose Estimation\n  and Face Alignment with Limited Resources Abstract: Our goal is to design architectures that retain the groundbreaking\nperformance of CNNs for landmark localization and at the same time are\nlightweight, compact and suitable for applications with limited computational\nresources. To this end, we make the following contributions: (a) we are the\nfirst to study the effect of neural network binarization on localization tasks,\nnamely human pose estimation and face alignment. We exhaustively evaluate\nvarious design choices, identify performance bottlenecks, and more importantly\npropose multiple orthogonal ways to boost performance. (b) Based on our\nanalysis, we propose a novel hierarchical, parallel and multi-scale residual\narchitecture that yields large performance improvement over the standard\nbottleneck block while having the same number of parameters, thus bridging the\ngap between the original network and its binarized counterpart. (c) We perform\na large number of ablation studies that shed light on the properties and the\nperformance of the proposed block. (d) We present results for experiments on\nthe most challenging datasets for human pose estimation and face alignment,\nreporting in many cases state-of-the-art performance. Code can be downloaded\nfrom https://www.adrianbulat.com/binary-cnn-landmarks \n\n"}
{"id": "1703.01127", "contents": "Title: On the Behavior of Convolutional Nets for Feature Extraction Abstract: Deep neural networks are representation learning techniques. During training,\na deep net is capable of generating a descriptive language of unprecedented\nsize and detail in machine learning. Extracting the descriptive language coded\nwithin a trained CNN model (in the case of image data), and reusing it for\nother purposes is a field of interest, as it provides access to the visual\ndescriptors previously learnt by the CNN after processing millions of images,\nwithout requiring an expensive training phase. Contributions to this field\n(commonly known as feature representation transfer or transfer learning) have\nbeen purely empirical so far, extracting all CNN features from a single layer\nclose to the output and testing their performance by feeding them to a\nclassifier. This approach has provided consistent results, although its\nrelevance is limited to classification tasks. In a completely different\napproach, in this paper we statistically measure the discriminative power of\nevery single feature found within a deep CNN, when used for characterizing\nevery class of 11 datasets. We seek to provide new insights into the behavior\nof CNN features, particularly the ones from convolutional layers, as this can\nbe relevant for their application to knowledge representation and reasoning.\nOur results confirm that low and middle level features may behave differently\nto high level features, but only under certain conditions. We find that all CNN\nfeatures can be used for knowledge representation purposes both by their\npresence or by their absence, doubling the information a single CNN feature may\nprovide. We also study how much noise these features may include, and propose a\nthresholding approach to discard most of it. All these insights have a direct\napplication to the generation of CNN embedding spaces. \n\n"}
{"id": "1703.01560", "contents": "Title: LR-GAN: Layered Recursive Generative Adversarial Networks for Image\n  Generation Abstract: We present LR-GAN: an adversarial image generation model which takes scene\nstructure and context into account. Unlike previous generative adversarial\nnetworks (GANs), the proposed GAN learns to generate image background and\nforegrounds separately and recursively, and stitch the foregrounds on the\nbackground in a contextually relevant manner to produce a complete natural\nimage. For each foreground, the model learns to generate its appearance, shape\nand pose. The whole model is unsupervised, and is trained in an end-to-end\nmanner with gradient descent methods. The experiments demonstrate that LR-GAN\ncan generate more natural images with objects that are more human recognizable\nthan DCGAN. \n\n"}
{"id": "1703.03492", "contents": "Title: A New Representation of Skeleton Sequences for 3D Action Recognition Abstract: This paper presents a new method for 3D action recognition with skeleton\nsequences (i.e., 3D trajectories of human skeleton joints). The proposed method\nfirst transforms each skeleton sequence into three clips each consisting of\nseveral frames for spatial temporal feature learning using deep neural\nnetworks. Each clip is generated from one channel of the cylindrical\ncoordinates of the skeleton sequence. Each frame of the generated clips\nrepresents the temporal information of the entire skeleton sequence, and\nincorporates one particular spatial relationship between the joints. The entire\nclips include multiple frames with different spatial relationships, which\nprovide useful spatial structural information of the human skeleton. We propose\nto use deep convolutional neural networks to learn long-term temporal\ninformation of the skeleton sequence from the frames of the generated clips,\nand then use a Multi-Task Learning Network (MTLN) to jointly process all frames\nof the generated clips in parallel to incorporate spatial structural\ninformation for action recognition. Experimental results clearly show the\neffectiveness of the proposed new representation and feature learning method\nfor 3D action recognition. \n\n"}
{"id": "1703.04730", "contents": "Title: Understanding Black-box Predictions via Influence Functions Abstract: How can we explain the predictions of a black-box model? In this paper, we\nuse influence functions -- a classic technique from robust statistics -- to\ntrace a model's prediction through the learning algorithm and back to its\ntraining data, thereby identifying training points most responsible for a given\nprediction. To scale up influence functions to modern machine learning\nsettings, we develop a simple, efficient implementation that requires only\noracle access to gradients and Hessian-vector products. We show that even on\nnon-convex and non-differentiable models where the theory breaks down,\napproximations to influence functions can still provide valuable information.\nOn linear models and convolutional neural networks, we demonstrate that\ninfluence functions are useful for multiple purposes: understanding model\nbehavior, debugging models, detecting dataset errors, and even creating\nvisually-indistinguishable training-set attacks. \n\n"}
{"id": "1703.04775", "contents": "Title: Discriminate-and-Rectify Encoders: Learning from Image Transformation\n  Sets Abstract: The complexity of a learning task is increased by transformations in the\ninput space that preserve class identity. Visual object recognition for example\nis affected by changes in viewpoint, scale, illumination or planar\ntransformations. While drastically altering the visual appearance, these\nchanges are orthogonal to recognition and should not be reflected in the\nrepresentation or feature encoding used for learning. We introduce a framework\nfor weakly supervised learning of image embeddings that are robust to\ntransformations and selective to the class distribution, using sets of\ntransforming examples (orbit sets), deep parametrizations and a novel\norbit-based loss. The proposed loss combines a discriminative, contrastive part\nfor orbits with a reconstruction error that learns to rectify orbit\ntransformations. The learned embeddings are evaluated in distance metric-based\ntasks, such as one-shot classification under geometric transformations, as well\nas face verification and retrieval under more realistic visual variability. Our\nresults suggest that orbit sets, suitably computed or observed, can be used for\nefficient, weakly-supervised learning of semantically relevant image\nembeddings. \n\n"}
{"id": "1703.07047", "contents": "Title: High-Resolution Breast Cancer Screening with Multi-View Deep\n  Convolutional Neural Networks Abstract: Advances in deep learning for natural images have prompted a surge of\ninterest in applying similar techniques to medical images. The majority of the\ninitial attempts focused on replacing the input of a deep convolutional neural\nnetwork with a medical image, which does not take into consideration the\nfundamental differences between these two types of images. Specifically, fine\ndetails are necessary for detection in medical images, unlike in natural images\nwhere coarse structures matter most. This difference makes it inadequate to use\nthe existing network architectures developed for natural images, because they\nwork on heavily downscaled images to reduce the memory requirements. This hides\ndetails necessary to make accurate predictions. Additionally, a single exam in\nmedical imaging often comes with a set of views which must be fused in order to\nreach a correct conclusion. In our work, we propose to use a multi-view deep\nconvolutional neural network that handles a set of high-resolution medical\nimages. We evaluate it on large-scale mammography-based breast cancer screening\n(BI-RADS prediction) using 886,000 images. We focus on investigating the impact\nof the training set size and image size on the prediction accuracy. Our results\nhighlight that performance increases with the size of training set, and that\nthe best performance can only be achieved using the original resolution. In the\nreader study, performed on a random subset of the test set, we confirmed the\nefficacy of our model, which achieved performance comparable to a committee of\nradiologists when presented with the same data. \n\n"}
{"id": "1703.08378", "contents": "Title: Feature Fusion using Extended Jaccard Graph and Stochastic Gradient\n  Descent for Robot Abstract: Robot vision is a fundamental device for human-robot interaction and robot\ncomplex tasks. In this paper, we use Kinect and propose a feature graph fusion\n(FGF) for robot recognition. Our feature fusion utilizes RGB and depth\ninformation to construct fused feature from Kinect. FGF involves multi-Jaccard\nsimilarity to compute a robust graph and utilize word embedding method to\nenhance the recognition results. We also collect DUT RGB-D face dataset and a\nbenchmark datset to evaluate the effectiveness and efficiency of our method.\nThe experimental results illustrate FGF is robust and effective to face and\nobject datasets in robot applications. \n\n"}
{"id": "1703.08388", "contents": "Title: DeepVisage: Making face recognition simple yet with powerful\n  generalization skills Abstract: Face recognition (FR) methods report significant performance by adopting the\nconvolutional neural network (CNN) based learning methods. Although CNNs are\nmostly trained by optimizing the softmax loss, the recent trend shows an\nimprovement of accuracy with different strategies, such as task-specific CNN\nlearning with different loss functions, fine-tuning on target dataset, metric\nlearning and concatenating features from multiple CNNs. Incorporating these\ntasks obviously requires additional efforts. Moreover, it demotivates the\ndiscovery of efficient CNN models for FR which are trained only with identity\nlabels. We focus on this fact and propose an easily trainable and single CNN\nbased FR method. Our CNN model exploits the residual learning framework.\nAdditionally, it uses normalized features to compute the loss. Our extensive\nexperiments show excellent generalization on different datasets. We obtain very\ncompetitive and state-of-the-art results on the LFW, IJB-A, YouTube faces and\nCACD datasets. \n\n"}
{"id": "1703.09529", "contents": "Title: Objects as context for detecting their semantic parts Abstract: We present a semantic part detection approach that effectively leverages\nobject information.We use the object appearance and its class as indicators of\nwhat parts to expect. We also model the expected relative location of parts\ninside the objects based on their appearance. We achieve this with a new\nnetwork module, called OffsetNet, that efficiently predicts a variable number\nof part locations within a given object. Our model incorporates all these cues\nto detect parts in the context of their objects. This leads to considerably\nhigher performance for the challenging task of part detection compared to using\npart appearance alone (+5 mAP on the PASCAL-Part dataset). We also compare to\nother part detection methods on both PASCAL-Part and CUB200-2011 datasets. \n\n"}
{"id": "1703.09891", "contents": "Title: LabelBank: Revisiting Global Perspectives for Semantic Segmentation Abstract: Semantic segmentation requires a detailed labeling of image pixels by object\ncategory. Information derived from local image patches is necessary to describe\nthe detailed shape of individual objects. However, this information is\nambiguous and can result in noisy labels. Global inference of image content can\ninstead capture the general semantic concepts present. We advocate that\nholistic inference of image concepts provides valuable information for detailed\npixel labeling. We propose a generic framework to leverage holistic information\nin the form of a LabelBank for pixel-level segmentation.\n  We show the ability of our framework to improve semantic segmentation\nperformance in a variety of settings. We learn models for extracting a holistic\nLabelBank from visual cues, attributes, and/or textual descriptions. We\ndemonstrate improvements in semantic segmentation accuracy on standard datasets\nacross a range of state-of-the-art segmentation architectures and holistic\ninference approaches. \n\n"}
{"id": "1703.10889", "contents": "Title: Single Image Super Resolution - When Model Adaptation Matters Abstract: In the recent years impressive advances were made for single image\nsuper-resolution. Deep learning is behind a big part of this success. Deep(er)\narchitecture design and external priors modeling are the key ingredients. The\ninternal contents of the low resolution input image is neglected with deep\nmodeling despite the earlier works showing the power of using such internal\npriors. In this paper we propose a novel deep convolutional neural network\ncarefully designed for robustness and efficiency at both learning and testing.\nMoreover, we propose a couple of model adaptation strategies to the internal\ncontents of the low resolution input image and analyze their strong points and\nweaknesses. By trading runtime and using internal priors we achieve 0.1 up to\n0.3dB PSNR improvements over best reported results on standard datasets. Our\nadaptation especially favors images with repetitive structures or under large\nresolutions. Moreover, it can be combined with other simple techniques, such as\nback-projection or enhanced prediction, for further improvements. \n\n"}
{"id": "1704.00028", "contents": "Title: Improved Training of Wasserstein GANs Abstract: Generative Adversarial Networks (GANs) are powerful generative models, but\nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN)\nmakes progress toward stable training of GANs, but sometimes can still generate\nonly low-quality samples or fail to converge. We find that these problems are\noften due to the use of weight clipping in WGAN to enforce a Lipschitz\nconstraint on the critic, which can lead to undesired behavior. We propose an\nalternative to clipping weights: penalize the norm of gradient of the critic\nwith respect to its input. Our proposed method performs better than standard\nWGAN and enables stable training of a wide variety of GAN architectures with\nalmost no hyperparameter tuning, including 101-layer ResNets and language\nmodels over discrete data. We also achieve high quality generations on CIFAR-10\nand LSUN bedrooms. \n\n"}
{"id": "1704.00103", "contents": "Title: SafetyNet: Detecting and Rejecting Adversarial Examples Robustly Abstract: We describe a method to produce a network where current methods such as\nDeepFool have great difficulty producing adversarial samples. Our construction\nsuggests some insights into how deep networks work. We provide a reasonable\nanalyses that our construction is difficult to defeat, and show experimentally\nthat our method is hard to defeat with both Type I and Type II attacks using\nseveral standard networks and datasets. This SafetyNet architecture is used to\nan important and novel application SceneProof, which can reliably detect\nwhether an image is a picture of a real scene or not. SceneProof applies to\nimages captured with depth maps (RGBD images) and checks if a pair of image and\ndepth map is consistent. It relies on the relative difficulty of producing\nnaturalistic depth maps for images in post processing. We demonstrate that our\nSafetyNet is robust to adversarial examples built from currently known\nattacking approaches. \n\n"}
{"id": "1704.00112", "contents": "Title: Configurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel\n  Ground Truth using Stochastic Grammars Abstract: We propose a systematic learning-based approach to the generation of massive\nquantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D\nimages thereof, with associated ground truth information, for the purposes of\ntraining, benchmarking, and diagnosing learning-based computer vision and\nrobotics algorithms. In particular, we devise a learning-based pipeline of\nalgorithms capable of automatically generating and rendering a potentially\ninfinite variety of indoor scenes by using a stochastic grammar, represented as\nan attributed Spatial And-Or Graph, in conjunction with state-of-the-art\nphysics-based rendering. Our pipeline is capable of synthesizing scene layouts\nwith high diversity, and it is configurable inasmuch as it enables the precise\ncustomization and control of important attributes of the generated scenes. It\nrenders photorealistic RGB images of the generated scenes while automatically\nsynthesizing detailed, per-pixel ground truth data, including visible surface\ndepth and normal, object identity, and material information (detailed to object\nparts), as well as environments (e.g., illuminations and camera viewpoints). We\ndemonstrate the value of our synthesized dataset, by improving performance in\ncertain machine-learning-based scene understanding tasks--depth and surface\nnormal prediction, semantic segmentation, reconstruction, etc.--and by\nproviding benchmarks for and diagnostics of trained models by modifying object\nattributes and scene properties in a controllable manner. \n\n"}
{"id": "1704.00280", "contents": "Title: The Stixel world: A medium-level representation of traffic scenes Abstract: Recent progress in advanced driver assistance systems and the race towards\nautonomous vehicles is mainly driven by two factors: (1) increasingly\nsophisticated algorithms that interpret the environment around the vehicle and\nreact accordingly, and (2) the continuous improvements of sensor technology\nitself. In terms of cameras, these improvements typically include higher\nspatial resolution, which as a consequence requires more data to be processed.\nThe trend to add multiple cameras to cover the entire surrounding of the\nvehicle is not conducive in that matter. At the same time, an increasing number\nof special purpose algorithms need access to the sensor input data to correctly\ninterpret the various complex situations that can occur, particularly in urban\ntraffic.\n  By observing those trends, it becomes clear that a key challenge for vision\narchitectures in intelligent vehicles is to share computational resources. We\nbelieve this challenge should be faced by introducing a representation of the\nsensory data that provides compressed and structured access to all relevant\nvisual content of the scene. The Stixel World discussed in this paper is such a\nrepresentation. It is a medium-level model of the environment that is\nspecifically designed to compress information about obstacles by leveraging the\ntypical layout of outdoor traffic scenes. It has proven useful for a multitude\nof automotive vision applications, including object detection, tracking,\nsegmentation, and mapping.\n  In this paper, we summarize the ideas behind the model and generalize it to\ntake into account multiple dense input streams: the image itself, stereo depth\nmaps, and semantic class probability maps that can be generated, e.g., by CNNs.\nOur generalization is embedded into a novel mathematical formulation for the\nStixel model. We further sketch how the free parameters of the model can be\nlearned using structured SVMs. \n\n"}
{"id": "1704.00763", "contents": "Title: AMC: Attention guided Multi-modal Correlation Learning for Image Search Abstract: Given a user's query, traditional image search systems rank images according\nto its relevance to a single modality (e.g., image content or surrounding\ntext). Nowadays, an increasing number of images on the Internet are available\nwith associated meta data in rich modalities (e.g., titles, keywords, tags,\netc.), which can be exploited for better similarity measure with queries. In\nthis paper, we leverage visual and textual modalities for image search by\nlearning their correlation with input query. According to the intent of query,\nattention mechanism can be introduced to adaptively balance the importance of\ndifferent modalities. We propose a novel Attention guided Multi-modal\nCorrelation (AMC) learning method which consists of a jointly learned hierarchy\nof intra and inter-attention networks. Conditioned on query's intent,\nintra-attention networks (i.e., visual intra-attention network and language\nintra-attention network) attend on informative parts within each modality; a\nmulti-modal inter-attention network promotes the importance of the most\nquery-relevant modalities. In experiments, we evaluate AMC models on the search\nlogs from two real world image search engines and show a significant boost on\nthe ranking of user-clicked images in search results. Additionally, we extend\nAMC models to caption ranking task on COCO dataset and achieve competitive\nresults compared with recent state-of-the-arts. \n\n"}
{"id": "1704.01085", "contents": "Title: Deep Depth From Focus Abstract: Depth from focus (DFF) is one of the classical ill-posed inverse problems in\ncomputer vision. Most approaches recover the depth at each pixel based on the\nfocal setting which exhibits maximal sharpness. Yet, it is not obvious how to\nreliably estimate the sharpness level, particularly in low-textured areas. In\nthis paper, we propose `Deep Depth From Focus (DDFF)' as the first end-to-end\nlearning approach to this problem. One of the main challenges we face is the\nhunger for data of deep neural networks. In order to obtain a significant\namount of focal stacks with corresponding groundtruth depth, we propose to\nleverage a light-field camera with a co-calibrated RGB-D sensor. This allows us\nto digitally create focal stacks of varying sizes. Compared to existing\nbenchmarks our dataset is 25 times larger, enabling the use of machine learning\nfor this inverse problem. We compare our results with state-of-the-art DFF\nmethods and we also analyze the effect of several key deep architectural\ncomponents. These experiments show that our proposed method `DDFFNet' achieves\nstate-of-the-art performance in all scenes, reducing depth error by more than\n75% compared to the classical DFF methods. \n\n"}
{"id": "1704.01344", "contents": "Title: Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via\n  Deep Layer Cascade Abstract: We propose a novel deep layer cascade (LC) method to improve the accuracy and\nspeed of semantic segmentation. Unlike the conventional model cascade (MC) that\nis composed of multiple independent models, LC treats a single deep model as a\ncascade of several sub-models. Earlier sub-models are trained to handle easy\nand confident regions, and they progressively feed-forward harder regions to\nthe next sub-model for processing. Convolutions are only calculated on these\nregions to reduce computations. The proposed method possesses several\nadvantages. First, LC classifies most of the easy regions in the shallow stage\nand makes deeper stage focuses on a few hard regions. Such an adaptive and\n'difficulty-aware' learning improves segmentation performance. Second, LC\naccelerates both training and testing of deep network thanks to early decisions\nin the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable\nframework, allowing joint learning of all sub-models. We evaluate our method on\nPASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and\nfast speed. \n\n"}
{"id": "1704.01358", "contents": "Title: Incremental Tube Construction for Human Action Detection Abstract: Current state-of-the-art action detection systems are tailored for offline\nbatch-processing applications. However, for online applications like\nhuman-robot interaction, current systems fall short, either because they only\ndetect one action per video, or because they assume that the entire video is\navailable ahead of time. In this work, we introduce a real-time and online\njoint-labelling and association algorithm for action detection that can\nincrementally construct space-time action tubes on the most challenging action\nvideos in which different action categories occur concurrently. In contrast to\nprevious methods, we solve the detection-window association and action\nlabelling problems jointly in a single pass. We demonstrate superior online\nassociation accuracy and speed (2.2ms per frame) as compared to the current\nstate-of-the-art offline systems. We further demonstrate that the entire action\ndetection pipeline can easily be made to work effectively in real-time using\nour action tube construction algorithm. \n\n"}
{"id": "1704.01474", "contents": "Title: Convolutional Neural Networks for Page Segmentation of Historical\n  Document Images Abstract: This paper presents a Convolutional Neural Network (CNN) based page\nsegmentation method for handwritten historical document images. We consider\npage segmentation as a pixel labeling problem, i.e., each pixel is classified\nas one of the predefined classes. Traditional methods in this area rely on\ncarefully hand-crafted features or large amounts of prior knowledge. In\ncontrast, we propose to learn features from raw image pixels using a CNN. While\nmany researchers focus on developing deep CNN architectures to solve different\nproblems, we train a simple CNN with only one convolution layer. We show that\nthe simple architecture achieves competitive results against other deep\narchitectures on different public datasets. Experiments also demonstrate the\neffectiveness and superiority of the proposed method compared to previous\nmethods. \n\n"}
{"id": "1704.02203", "contents": "Title: Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic\n  Encryption Abstract: We propose a privacy-preserving framework for learning visual classifiers by\nleveraging distributed private image data. This framework is designed to\naggregate multiple classifiers updated locally using private data and to ensure\nthat no private information about the data is exposed during and after its\nlearning procedure. We utilize a homomorphic cryptosystem that can aggregate\nthe local classifiers while they are encrypted and thus kept secret. To\novercome the high computational cost of homomorphic encryption of\nhigh-dimensional classifiers, we (1) impose sparsity constraints on local\nclassifier updates and (2) propose a novel efficient encryption scheme named\ndoubly-permuted homomorphic encryption (DPHE) which is tailored to sparse\nhigh-dimensional data. DPHE (i) decomposes sparse data into its constituent\nnon-zero values and their corresponding support indices, (ii) applies\nhomomorphic encryption only to the non-zero values, and (iii) employs double\npermutations on the support indices to make them secret. Our experimental\nevaluation on several public datasets shows that the proposed approach achieves\ncomparable performance against state-of-the-art visual recognition methods\nwhile preserving privacy and significantly outperforms other privacy-preserving\nmethods. \n\n"}
{"id": "1704.02304", "contents": "Title: It Takes (Only) Two: Adversarial Generator-Encoder Networks Abstract: We present a new autoencoder-type architecture that is trainable in an\nunsupervised mode, sustains both generation and inference, and has the quality\nof conditional and unconditional samples boosted by adversarial learning.\nUnlike previous hybrids of autoencoders and adversarial networks, the\nadversarial game in our approach is set up directly between the encoder and the\ngenerator, and no external mappings are trained in the process of learning. The\ngame objective compares the divergences of each of the real and the generated\ndata distributions with the prior distribution in the latent space. We show\nthat direct generator-vs-encoder game leads to a tight coupling of the two\ncomponents, resulting in samples and reconstructions of a comparable quality to\nsome recently-proposed more complex architectures. \n\n"}
{"id": "1704.02685", "contents": "Title: Learning Important Features Through Propagating Activation Differences Abstract: The purported \"black box\" nature of neural networks is a barrier to adoption\nin applications where interpretability is essential. Here we present DeepLIFT\n(Deep Learning Important FeaTures), a method for decomposing the output\nprediction of a neural network on a specific input by backpropagating the\ncontributions of all neurons in the network to every feature of the input.\nDeepLIFT compares the activation of each neuron to its 'reference activation'\nand assigns contribution scores according to the difference. By optionally\ngiving separate consideration to positive and negative contributions, DeepLIFT\ncan also reveal dependencies which are missed by other approaches. Scores can\nbe computed efficiently in a single backward pass. We apply DeepLIFT to models\ntrained on MNIST and simulated genomic data, and show significant advantages\nover gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides:\nbit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code:\nhttp://goo.gl/RM8jvH. \n\n"}
{"id": "1704.02798", "contents": "Title: Bayesian Recurrent Neural Networks Abstract: In this work we explore a straightforward variational Bayes scheme for\nRecurrent Neural Networks. Firstly, we show that a simple adaptation of\ntruncated backpropagation through time can yield good quality uncertainty\nestimates and superior regularisation at only a small extra computational cost\nduring training, also reducing the amount of parameters by 80\\%. Secondly, we\ndemonstrate how a novel kind of posterior approximation yields further\nimprovements to the performance of Bayesian RNNs. We incorporate local gradient\ninformation into the approximate posterior to sharpen it around the current\nbatch statistics. We show how this technique is not exclusive to recurrent\nneural networks and can be applied more widely to train Bayesian neural\nnetworks. We also empirically demonstrate how Bayesian RNNs are superior to\ntraditional RNNs on a language modelling benchmark and an image captioning\ntask, as well as showing how each of these methods improve our model over a\nvariety of other schemes for training them. We also introduce a new benchmark\nfor studying uncertainty for language models so future methods can be easily\ncompared. \n\n"}
{"id": "1704.03058", "contents": "Title: CERN: Confidence-Energy Recurrent Network for Group Activity Recognition Abstract: This work is about recognizing human activities occurring in videos at\ndistinct semantic levels, including individual actions, interactions, and group\nactivities. The recognition is realized using a two-level hierarchy of Long\nShort-Term Memory (LSTM) networks, forming a feed-forward deep architecture,\nwhich can be trained end-to-end. In comparison with existing architectures of\nLSTMs, we make two key contributions giving the name to our approach as\nConfidence-Energy Recurrent Network -- CERN. First, instead of using the common\nsoftmax layer for prediction, we specify a novel energy layer (EL) for\nestimating the energy of our predictions. Second, rather than finding the\ncommon minimum-energy class assignment, which may be numerically unstable under\nuncertainty, we specify that the EL additionally computes the p-values of the\nsolutions, and in this way estimates the most confident energy minimum. The\nevaluation on the Collective Activity and Volleyball datasets demonstrates: (i)\nadvantages of our two contributions relative to the common softmax and\nenergy-minimization formulations and (ii) a superior performance relative to\nthe state-of-the-art approaches. \n\n"}
{"id": "1704.03414", "contents": "Title: A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection Abstract: How do we learn an object detector that is invariant to occlusions and\ndeformations? Our current solution is to use a data-driven strategy -- collect\nlarge-scale datasets which have object instances under different conditions.\nThe hope is that the final classifier can use these examples to learn\ninvariances. But is it really possible to see all the occlusions in a dataset?\nWe argue that like categories, occlusions and object deformations also follow a\nlong-tail. Some occlusions and deformations are so rare that they hardly\nhappen; yet we want to learn a model invariant to such occurrences. In this\npaper, we propose an alternative solution. We propose to learn an adversarial\nnetwork that generates examples with occlusions and deformations. The goal of\nthe adversary is to generate examples that are difficult for the object\ndetector to classify. In our framework both the original detector and adversary\nare learned in a joint manner. Our experimental results indicate a 2.3% mAP\nboost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge\ncompared to the Fast-RCNN pipeline. We also release the code for this paper. \n\n"}
{"id": "1704.03432", "contents": "Title: Forecasting Human Dynamics from Static Images Abstract: This paper presents the first study on forecasting human dynamics from static\nimages. The problem is to input a single RGB image and generate a sequence of\nupcoming human body poses in 3D. To address the problem, we propose the 3D Pose\nForecasting Network (3D-PFNet). Our 3D-PFNet integrates recent advances on\nsingle-image human pose estimation and sequence prediction, and converts the 2D\npredictions into 3D space. We train our 3D-PFNet using a three-step training\nstrategy to leverage a diverse source of training data, including image and\nvideo based human pose datasets and 3D motion capture (MoCap) data. We\ndemonstrate competitive performance of our 3D-PFNet on 2D pose forecasting and\n3D pose recovery through quantitative and qualitative results. \n\n"}
{"id": "1704.03488", "contents": "Title: Learning Proximal Operators: Using Denoising Networks for Regularizing\n  Inverse Imaging Problems Abstract: While variational methods have been among the most powerful tools for solving\nlinear inverse problems in imaging, deep (convolutional) neural networks have\nrecently taken the lead in many challenging benchmarks. A remaining drawback of\ndeep learning approaches is their requirement for an expensive retraining\nwhenever the specific problem, the noise level, noise type, or desired measure\nof fidelity changes. On the contrary, variational methods have a plug-and-play\nnature as they usually consist of separate data fidelity and regularization\nterms.\n  In this paper we study the possibility of replacing the proximal operator of\nthe regularization used in many convex energy minimization algorithms by a\ndenoising neural network. The latter therefore serves as an implicit natural\nimage prior, while the data term can still be chosen independently. Using a\nfixed denoising neural network in exemplary problems of image deconvolution\nwith different blur kernels and image demosaicking, we obtain state-of-the-art\nreconstruction results. These indicate the high generalizability of our\napproach and a reduction of the need for problem-specific training.\nAdditionally, we discuss novel results on the analysis of possible optimization\nalgorithms to incorporate the network into, as well as the choices of algorithm\nparameters and their relation to the noise level the neural network is trained\non. \n\n"}
{"id": "1704.03557", "contents": "Title: Cutting the Error by Half: Investigation of Very Deep CNN and Advanced\n  Training Strategies for Document Image Classification Abstract: We present an exhaustive investigation of recent Deep Learning architectures,\nalgorithms, and strategies for the task of document image classification to\nfinally reduce the error by more than half. Existing approaches, such as the\nDeepDocClassifier, apply standard Convolutional Network architectures with\ntransfer learning from the object recognition domain. The contribution of the\npaper is threefold: First, it investigates recently introduced very deep neural\nnetwork architectures (GoogLeNet, VGG, ResNet) using transfer learning (from\nreal images). Second, it proposes transfer learning from a huge set of document\nimages, i.e. 400,000 documents. Third, it analyzes the impact of the amount of\ntraining data (document images) and other parameters to the classification\nabilities. We use two datasets, the Tobacco-3482 and the large-scale RVL-CDIP\ndataset. We achieve an accuracy of 91.13% for the Tobacco-3482 dataset while\nearlier approaches reach only 77.6%. Thus, a relative error reduction of more\nthan 60% is achieved. For the large dataset RVL-CDIP, an accuracy of 90.97% is\nachieved, corresponding to a relative error reduction of 11.5%. \n\n"}
{"id": "1704.03895", "contents": "Title: What's in a Question: Using Visual Questions as a Form of Supervision Abstract: Collecting fully annotated image datasets is challenging and expensive. Many\ntypes of weak supervision have been explored: weak manual annotations, web\nsearch results, temporal continuity, ambient sound and others. We focus on one\nparticular unexplored mode: visual questions that are asked about images. The\nkey observation that inspires our work is that the question itself provides\nuseful information about the image (even without the answer being available).\nFor instance, the question \"what is the breed of the dog?\" informs the AI that\nthe animal in the scene is a dog and that there is only one dog present. We\nmake three contributions: (1) providing an extensive qualitative and\nquantitative analysis of the information contained in human visual questions,\n(2) proposing two simple but surprisingly effective modifications to the\nstandard visual question answering models that allow them to make use of weak\nsupervision in the form of unanswered questions associated with images and (3)\ndemonstrating that a simple data augmentation strategy inspired by our insights\nresults in a 7.1% improvement on the standard VQA benchmark. \n\n"}
{"id": "1704.04057", "contents": "Title: DCFNet: Discriminant Correlation Filters Network for Visual Tracking Abstract: Discriminant Correlation Filters (DCF) based methods now become a kind of\ndominant approach to online object tracking. The features used in these\nmethods, however, are either based on hand-crafted features like HoGs, or\nconvolutional features trained independently from other tasks like image\nclassification. In this work, we present an end-to-end lightweight network\narchitecture, namely DCFNet, to learn the convolutional features and perform\nthe correlation tracking process simultaneously. Specifically, we treat DCF as\na special correlation filter layer added in a Siamese network, and carefully\nderive the backpropagation through it by defining the network output as the\nprobability heatmap of object location. Since the derivation is still carried\nout in Fourier frequency domain, the efficiency property of DCF is preserved.\nThis enables our tracker to run at more than 60 FPS during test time, while\nachieving a significant accuracy gain compared with KCF using HoGs. Extensive\nevaluations on OTB-2013, OTB-2015, and VOT2015 benchmarks demonstrate that the\nproposed DCFNet tracker is competitive with several state-of-the-art trackers,\nwhile being more compact and much faster. \n\n"}
{"id": "1704.04516", "contents": "Title: Interpretable 3D Human Action Analysis with Temporal Convolutional\n  Networks Abstract: The discriminative power of modern deep learning models for 3D human action\nrecognition is growing ever so potent. In conjunction with the recent\nresurgence of 3D human action representation with 3D skeletons, the quality and\nthe pace of recent progress have been significant. However, the inner workings\nof state-of-the-art learning based methods in 3D human action recognition still\nremain mostly black-box. In this work, we propose to use a new class of models\nknown as Temporal Convolutional Neural Networks (TCN) for 3D human action\nrecognition. Compared to popular LSTM-based Recurrent Neural Network models,\ngiven interpretable input such as 3D skeletons, TCN provides us a way to\nexplicitly learn readily interpretable spatio-temporal representations for 3D\nhuman action recognition. We provide our strategy in re-designing the TCN with\ninterpretability in mind and how such characteristics of the model is leveraged\nto construct a powerful 3D activity recognition method. Through this work, we\nwish to take a step towards a spatio-temporal model that is easier to\nunderstand, explain and interpret. The resulting model, Res-TCN, achieves\nstate-of-the-art results on the largest 3D human action recognition dataset,\nNTU-RGBD. \n\n"}
{"id": "1704.04805", "contents": "Title: Replicator Equation: Applications Revisited Abstract: The replicator equation is a simple model of evolution that leads to stable\nform of Nash Equilibrium, Evolutionary Stable Strategy (ESS). It has been\nstudied in connection with Evolutionary Game Theory and was originally\ndeveloped for symmetric games. Beyond its first emphasis in biological use,\nevolutionary game theory has been expanded well beyond in social studies for\nbehavioral analysis, in machine learning, computer vision and others. Its\nseveral applications in the fields of machine learning and computer vision has\ndrawn my attention which is the reason to write this extended abstract \n\n"}
{"id": "1704.05051", "contents": "Title: Google's Cloud Vision API Is Not Robust To Noise Abstract: Google has recently introduced the Cloud Vision API for image analysis.\nAccording to the demonstration website, the API \"quickly classifies images into\nthousands of categories, detects individual objects and faces within images,\nand finds and reads printed words contained within images.\" It can be also used\nto \"detect different types of inappropriate content from adult to violent\ncontent.\"\n  In this paper, we evaluate the robustness of Google Cloud Vision API to input\nperturbation. In particular, we show that by adding sufficient noise to the\nimage, the API generates completely different outputs for the noisy image,\nwhile a human observer would perceive its original content. We show that the\nattack is consistently successful, by performing extensive experiments on\ndifferent image types, including natural images, images containing faces and\nimages with texts. For instance, using images from ImageNet dataset, we found\nthat adding an average of 14.25% impulse noise is enough to deceive the API.\nOur findings indicate the vulnerability of the API in adversarial environments.\nFor example, an adversary can bypass an image filtering system by adding noise\nto inappropriate images. We then show that when a noise filter is applied on\ninput images, the API generates mostly the same outputs for restored images as\nfor original images. This observation suggests that cloud vision API can\nreadily benefit from noise filtering, without the need for updating image\nanalysis algorithms. \n\n"}
{"id": "1704.05119", "contents": "Title: Exploring Sparsity in Recurrent Neural Networks Abstract: Recurrent Neural Networks (RNN) are widely used to solve a variety of\nproblems and as the quantity of data and the amount of available compute have\nincreased, so have model sizes. The number of parameters in recent\nstate-of-the-art networks makes them hard to deploy, especially on mobile\nphones and embedded devices. The challenge is due to both the size of the model\nand the time it takes to evaluate it. In order to deploy these RNNs\nefficiently, we propose a technique to reduce the parameters of a network by\npruning weights during the initial training of the network. At the end of\ntraining, the parameters of the network are sparse while accuracy is still\nclose to the original dense neural network. The network size is reduced by 8x\nand the time required to train the model remains constant. Additionally, we can\nprune a larger dense network to achieve better than baseline performance while\nstill reducing the total number of parameters significantly. Pruning RNNs\nreduces the size of the model and can also help achieve significant inference\ntime speed-up using sparse matrix multiply. Benchmarks show that using our\ntechnique model size can be reduced by 90% and speed-up is around 2x to 7x. \n\n"}
{"id": "1704.05310", "contents": "Title: Unsupervised Learning by Predicting Noise Abstract: Convolutional neural networks provide visual features that perform remarkably\nwell in many computer vision applications. However, training these networks\nrequires significant amounts of supervision. This paper introduces a generic\nframework to train deep networks, end-to-end, with no supervision. We propose\nto fix a set of target representations, called Noise As Targets (NAT), and to\nconstrain the deep features to align to them. This domain agnostic approach\navoids the standard unsupervised learning issues of trivial solutions and\ncollapsing of features. Thanks to a stochastic batch reassignment strategy and\na separable square loss function, it scales to millions of images. The proposed\napproach produces representations that perform on par with state-of-the-art\nunsupervised methods on ImageNet and Pascal VOC. \n\n"}
{"id": "1704.06326", "contents": "Title: Good Features to Correlate for Visual Tracking Abstract: During the recent years, correlation filters have shown dominant and\nspectacular results for visual object tracking. The types of the features that\nare employed in these family of trackers significantly affect the performance\nof visual tracking. The ultimate goal is to utilize robust features invariant\nto any kind of appearance change of the object, while predicting the object\nlocation as properly as in the case of no appearance change. As the deep\nlearning based methods have emerged, the study of learning features for\nspecific tasks has accelerated. For instance, discriminative visual tracking\nmethods based on deep architectures have been studied with promising\nperformance. Nevertheless, correlation filter based (CFB) trackers confine\nthemselves to use the pre-trained networks which are trained for object\nclassification problem. To this end, in this manuscript the problem of learning\ndeep fully convolutional features for the CFB visual tracking is formulated. In\norder to learn the proposed model, a novel and efficient backpropagation\nalgorithm is presented based on the loss function of the network. The proposed\nlearning framework enables the network model to be flexible for a custom\ndesign. Moreover, it alleviates the dependency on the network trained for\nclassification. Extensive performance analysis shows the efficacy of the\nproposed custom design in the CFB tracking framework. By fine-tuning the\nconvolutional parts of a state-of-the-art network and integrating this model to\na CFB tracker, which is the top performing one of VOT2016, 18% increase is\nachieved in terms of expected average overlap, and tracking failures are\ndecreased by 25%, while maintaining the superiority over the state-of-the-art\nmethods in OTB-2013 and OTB-2015 tracking datasets. \n\n"}
{"id": "1704.06743", "contents": "Title: Robust, Deep and Inductive Anomaly Detection Abstract: PCA is a classical statistical technique whose simplicity and maturity has\nseen it find widespread use as an anomaly detection technique. However, it is\nlimited in this regard by being sensitive to gross perturbations of the input,\nand by seeking a linear subspace that captures normal behaviour. The first\nissue has been dealt with by robust PCA, a variant of PCA that explicitly\nallows for some data points to be arbitrarily corrupted, however, this does not\nresolve the second issue, and indeed introduces the new issue that one can no\nlonger inductively find anomalies on a test set. This paper addresses both\nissues in a single model, the robust autoencoder. This method learns a\nnonlinear subspace that captures the majority of data points, while allowing\nfor some data to have arbitrary corruption. The model is simple to train and\nleverages recent advances in the optimisation of deep neural networks.\nExperiments on a range of real-world datasets highlight the model's\neffectiveness. \n\n"}
{"id": "1704.06857", "contents": "Title: A Review on Deep Learning Techniques Applied to Semantic Segmentation Abstract: Image semantic segmentation is more and more being of interest for computer\nvision and machine learning researchers. Many applications on the rise need\naccurate and efficient segmentation mechanisms: autonomous driving, indoor\nnavigation, and even virtual or augmented reality systems to name a few. This\ndemand coincides with the rise of deep learning approaches in almost every\nfield or application target related to computer vision, including semantic\nsegmentation or scene understanding. This paper provides a review on deep\nlearning methods for semantic segmentation applied to various application\nareas. Firstly, we describe the terminology of this field as well as mandatory\nbackground concepts. Next, the main datasets and challenges are exposed to help\nresearchers decide which are the ones that best suit their needs and their\ntargets. Then, existing methods are reviewed, highlighting their contributions\nand their significance in the field. Finally, quantitative results are given\nfor the described methods and the datasets in which they were evaluated,\nfollowing up with a discussion of the results. At last, we point out a set of\npromising future works and draw our own conclusions about the state of the art\nof semantic segmentation using deep learning techniques. \n\n"}
{"id": "1704.07077", "contents": "Title: Exploiting Multi-layer Graph Factorization for Multi-attributed Graph\n  Matching Abstract: Multi-attributed graph matching is a problem of finding correspondences\nbetween two sets of data while considering their complex properties described\nin multiple attributes. However, the information of multiple attributes is\nlikely to be oversimplified during a process that makes an integrated\nattribute, and this degrades the matching accuracy. For that reason, a\nmulti-layer graph structure-based algorithm has been proposed recently. It can\neffectively avoid the problem by separating attributes into multiple layers.\nNonetheless, there are several remaining issues such as a scalability problem\ncaused by the huge matrix to describe the multi-layer structure and a\nback-projection problem caused by the continuous relaxation of the quadratic\nassignment problem. In this work, we propose a novel multi-attributed graph\nmatching algorithm based on the multi-layer graph factorization. We reformulate\nthe problem to be solved with several small matrices that are obtained by\nfactorizing the multi-layer structure. Then, we solve the problem using a\nconvex-concave relaxation procedure for the multi-layer structure. The proposed\nalgorithm exhibits better performance than state-of-the-art algorithms based on\nthe single-layer structure. \n\n"}
{"id": "1704.08082", "contents": "Title: AutoDIAL: Automatic DomaIn Alignment Layers Abstract: Classifiers trained on given databases perform poorly when tested on data\nacquired in different settings. This is explained in domain adaptation through\na shift among distributions of the source and target domains. Attempts to align\nthem have traditionally resulted in works reducing the domain shift by\nintroducing appropriate loss terms, measuring the discrepancies between source\nand target distributions, in the objective function. Here we take a different\nroute, proposing to align the learned representations by embedding in any given\nnetwork specific Domain Alignment Layers, designed to match the source and\ntarget feature distributions to a reference one. Opposite to previous works\nwhich define a priori in which layers adaptation should be performed, our\nmethod is able to automatically learn the degree of feature alignment required\nat different levels of the deep network. Thorough experiments on different\npublic benchmarks, in the unsupervised setting, confirm the power of our\napproach. \n\n"}
{"id": "1704.08615", "contents": "Title: Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics Abstract: Dozens of new models on fixation prediction are published every year and\ncompared on open benchmarks such as MIT300 and LSUN. However, progress in the\nfield can be difficult to judge because models are compared using a variety of\ninconsistent metrics. Here we show that no single saliency map can perform well\nunder all metrics. Instead, we propose a principled approach to solve the\nbenchmarking problem by separating the notions of saliency models, maps and\nmetrics. Inspired by Bayesian decision theory, we define a saliency model to be\na probabilistic model of fixation density prediction and a saliency map to be a\nmetric-specific prediction derived from the model density which maximizes the\nexpected performance on that metric given the model density. We derive these\noptimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,\nNSS, CC, SIM, KL-Div) and show that they can be computed analytically or\napproximated with high precision. We show that this leads to consistent\nrankings in all metrics and avoids the penalties of using one saliency map for\nall metrics. Our method allows researchers to have their model compete on many\ndifferent metrics with state-of-the-art in those metrics: \"good\" models will\nperform well in all metrics. \n\n"}
{"id": "1705.00053", "contents": "Title: The Pose Knows: Video Forecasting by Generating Pose Futures Abstract: Current approaches in video forecasting attempt to generate videos directly\nin pixel space using Generative Adversarial Networks (GANs) or Variational\nAutoencoders (VAEs). However, since these approaches try to model all the\nstructure and scene dynamics at once, in unconstrained settings they often\ngenerate uninterpretable results. Our insight is to model the forecasting\nproblem at a higher level of abstraction. Specifically, we exploit human pose\ndetectors as a free source of supervision and break the video forecasting\nproblem into two discrete steps. First we explicitly model the high level\nstructure of active objects in the scene---humans---and use a VAE to model the\npossible future movements of humans in the pose space. We then use the future\nposes generated as conditional information to a GAN to predict the future\nframes of the video in pixel space. By using the structured space of pose as an\nintermediate representation, we sidestep the problems that GANs have in\ngenerating video pixels directly. We show through quantitative and qualitative\nevaluation that our method outperforms state-of-the-art methods for video\nprediction. \n\n"}
{"id": "1705.01583", "contents": "Title: VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera Abstract: We present the first real-time method to capture the full global 3D skeletal\npose of a human in a stable, temporally consistent manner using a single RGB\ncamera. Our method combines a new convolutional neural network (CNN) based pose\nregressor with kinematic skeleton fitting. Our novel fully-convolutional pose\nformulation regresses 2D and 3D joint positions jointly in real time and does\nnot require tightly cropped input frames. A real-time kinematic skeleton\nfitting method uses the CNN output to yield temporally stable 3D global pose\nreconstructions on the basis of a coherent kinematic skeleton. This makes our\napproach the first monocular RGB method usable in real-time applications such\nas 3D character control---thus far, the only monocular methods for such\napplications employed specialized RGB-D cameras. Our method's accuracy is\nquantitatively on par with the best offline 3D monocular RGB pose estimation\nmethods. Our results are qualitatively comparable to, and sometimes better\nthan, results from monocular RGB-D approaches, such as the Kinect. However, we\nshow that our approach is more broadly applicable than RGB-D solutions, i.e. it\nworks for outdoor scenes, community videos, and low quality commodity RGB\ncameras. \n\n"}
{"id": "1705.01782", "contents": "Title: From Zero-shot Learning to Conventional Supervised Classification:\n  Unseen Visual Data Synthesis Abstract: Robust object recognition systems usually rely on powerful feature extraction\nmechanisms from a large number of real images. However, in many realistic\napplications, collecting sufficient images for ever-growing new classes is\nunattainable. In this paper, we propose a new Zero-shot learning (ZSL)\nframework that can synthesise visual features for unseen classes without\nacquiring real images. Using the proposed Unseen Visual Data Synthesis (UVDS)\nalgorithm, semantic attributes are effectively utilised as an intermediate clue\nto synthesise unseen visual features at the training stage. Hereafter, ZSL\nrecognition is converted into the conventional supervised problem, i.e. the\nsynthesised visual features can be straightforwardly fed to typical classifiers\nsuch as SVM. On four benchmark datasets, we demonstrate the benefit of using\nsynthesised unseen data. Extensive experimental results suggest that our\nproposed approach significantly improve the state-of-the-art results. \n\n"}
{"id": "1705.01809", "contents": "Title: Pixel Normalization from Numeric Data as Input to Neural Networks Abstract: Text to image transformation for input to neural networks requires\nintermediate steps. This paper attempts to present a new approach to pixel\nnormalization so as to convert textual data into image, suitable as input for\nneural networks. This method can be further improved by its Graphics Processing\nUnit (GPU) implementation to provide significant speedup in computational time. \n\n"}
{"id": "1705.02082", "contents": "Title: Motion Prediction Under Multimodality with Conditional Stochastic\n  Networks Abstract: Given a visual history, multiple future outcomes for a video scene are\nequally probable, in other words, the distribution of future outcomes has\nmultiple modes. Multimodality is notoriously hard to handle by standard\nregressors or classifiers: the former regress to the mean and the latter\ndiscretize a continuous high dimensional output space. In this work, we present\nstochastic neural network architectures that handle such multimodality through\nstochasticity: future trajectories of objects, body joints or frames are\nrepresented as deep, non-linear transformations of random (as opposed to\ndeterministic) variables. Such random variables are sampled from simple\nGaussian distributions whose means and variances are parametrized by the output\nof convolutional encoders over the visual history. We introduce novel\nconvolutional architectures for predicting future body joint trajectories that\noutperform fully connected alternatives \\cite{DBLP:journals/corr/WalkerDGH16}.\nWe introduce stochastic spatial transformers through optical flow warping for\npredicting future frames, which outperform their deterministic equivalents\n\\cite{DBLP:journals/corr/PatrauceanHC15}. Training stochastic networks involves\nan intractable marginalization over stochastic variables. We compare various\ntraining schemes that handle such marginalization through a) straightforward\nsampling from the prior, b) conditional variational autoencoders\n\\cite{NIPS2015_5775,DBLP:journals/corr/WalkerDGH16}, and, c) a proposed\nK-best-sample loss that penalizes the best prediction under a fixed \"prediction\nbudget\". We show experimental results on object trajectory prediction, human\nbody joint trajectory prediction and video prediction under varying future\nuncertainty, validating quantitatively and qualitatively our architectural\nchoices and training schemes. \n\n"}
{"id": "1705.02139", "contents": "Title: Bridging between Computer and Robot Vision through Data Augmentation: a\n  Case Study on Object Recognition Abstract: Despite the impressive progress brought by deep network in visual object\nrecognition, robot vision is still far from being a solved problem. The most\nsuccessful convolutional architectures are developed starting from ImageNet, a\nlarge scale collection of images of object categories downloaded from the Web.\nThis kind of images is very different from the situated and embodied visual\nexperience of robots deployed in unconstrained settings. To reduce the gap\nbetween these two visual experiences, this paper proposes a simple yet\neffective data augmentation layer that zooms on the object of interest and\nsimulates the object detection outcome of a robot vision system. The layer,\nthat can be used with any convolutional deep architecture, brings to an\nincrease in object recognition performance of up to 7\\%, in experiments\nperformed over three different benchmark databases. Upon acceptance of the\npaper, our robot data augmentation layer will be made publicly available. \n\n"}
{"id": "1705.02145", "contents": "Title: Part-based Deep Hashing for Large-scale Person Re-identification Abstract: Large-scale is a trend in person re-identification (re-id). It is important\nthat real-time search be performed in a large gallery. While previous methods\nmostly focus on discriminative learning, this paper makes the attempt in\nintegrating deep learning and hashing into one framework to evaluate the\nefficiency and accuracy for large-scale person re-id. We integrate spatial\ninformation for discriminative visual representation by partitioning the\npedestrian image into horizontal parts. Specifically, Part-based Deep Hashing\n(PDH) is proposed, in which batches of triplet samples are employed as the\ninput of the deep hashing architecture. Each triplet sample contains two\npedestrian images (or parts) with the same identity and one pedestrian image\n(or part) of the different identity. A triplet loss function is employed with a\nconstraint that the Hamming distance of pedestrian images (or parts) with the\nsame identity is smaller than ones with the different identity. In the\nexperiment, we show that the proposed Part-based Deep Hashing method yields\nvery competitive re-id accuracy on the large-scale Market-1501 and\nMarket-1501+500K datasets. \n\n"}
{"id": "1705.02407", "contents": "Title: Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation Abstract: Human pose estimation using deep neural networks aims to map input images\nwith large variations into multiple body keypoints which must satisfy a set of\ngeometric constraints and inter-dependency imposed by the human body model.\nThis is a very challenging nonlinear manifold learning process in a very high\ndimensional feature space. We believe that the deep neural network, which is\ninherently an algebraic computation system, is not the most effecient way to\ncapture highly sophisticated human knowledge, for example those highly coupled\ngeometric characteristics and interdependence between keypoints in human poses.\nIn this work, we propose to explore how external knowledge can be effectively\nrepresented and injected into the deep neural networks to guide its training\nprocess using learned projections that impose proper prior. Specifically, we\nuse the stacked hourglass design and inception-resnet module to construct a\nfractal network to regress human pose images into heatmaps with no explicit\ngraphical modeling. We encode external knowledge with visual features which are\nable to characterize the constraints of human body models and evaluate the\nfitness of intermediate network output. We then inject these external features\ninto the neural network using a projection matrix learned using an auxiliary\ncost function. The effectiveness of the proposed inception-resnet module and\nthe benefit in guided learning with knowledge projection is evaluated on two\nwidely used benchmarks. Our approach achieves state-of-the-art performance on\nboth datasets. \n\n"}
{"id": "1705.03387", "contents": "Title: Generative Adversarial Trainer: Defense to Adversarial Perturbations\n  with GAN Abstract: We propose a novel technique to make neural network robust to adversarial\nexamples using a generative adversarial network. We alternately train both\nclassifier and generator networks. The generator network generates an\nadversarial perturbation that can easily fool the classifier network by using a\ngradient of each image. Simultaneously, the classifier network is trained to\nclassify correctly both original and adversarial images generated by the\ngenerator. These procedures help the classifier network to become more robust\nto adversarial perturbations. Furthermore, our adversarial training framework\nefficiently reduces overfitting and outperforms other regularization methods\nsuch as Dropout. We applied our method to supervised learning for CIFAR\ndatasets, and experimantal results show that our method significantly lowers\nthe generalization error of the network. To the best of our knowledge, this is\nthe first method which uses GAN to improve supervised learning. \n\n"}
{"id": "1705.03854", "contents": "Title: Predicting the Driver's Focus of Attention: the DR(eye)VE Project Abstract: In this work we aim to predict the driver's focus of attention. The goal is\nto estimate what a person would pay attention to while driving, and which part\nof the scene around the vehicle is more critical for the task. To this end we\npropose a new computer vision model based on a multi-branch deep architecture\nthat integrates three sources of information: raw video, motion and scene\nsemantics. We also introduce DR(eye)VE, the largest dataset of driving scenes\nfor which eye-tracking annotations are available. This dataset features more\nthan 500,000 registered frames, matching ego-centric views (from glasses worn\nby drivers) and car-centric views (from roof-mounted camera), further enriched\nby other sensors measurements. Results highlight that several attention\npatterns are shared across drivers and can be reproduced to some extent. The\nindication of which elements in the scene are likely to capture the driver's\nattention may benefit several applications in the context of human-vehicle\ninteraction and driver attention analysis. \n\n"}
{"id": "1705.04098", "contents": "Title: A Generative Model of People in Clothing Abstract: We present the first image-based generative model of people in clothing for\nthe full body. We sidestep the commonly used complex graphics rendering\npipeline and the need for high-quality 3D scans of dressed people. Instead, we\nlearn generative models from a large image database. The main challenge is to\ncope with the high variance in human pose, shape and appearance. For this\nreason, pure image-based approaches have not been considered so far. We show\nthat this challenge can be overcome by splitting the generating process in two\nparts. First, we learn to generate a semantic segmentation of the body and\nclothing. Second, we learn a conditional model on the resulting segments that\ncreates realistic images. The full model is differentiable and can be\nconditioned on pose, shape or color. The result are samples of people in\ndifferent clothing items and styles. The proposed model can generate entirely\nnew people with realistic clothing. In several experiments we present\nencouraging results that suggest an entirely data-driven approach to people\ngeneration is possible. \n\n"}
{"id": "1705.05301", "contents": "Title: Back to RGB: 3D tracking of hands and hand-object interactions based on\n  short-baseline stereo Abstract: We present a novel solution to the problem of 3D tracking of the articulated\nmotion of human hand(s), possibly in interaction with other objects. The vast\nmajority of contemporary relevant work capitalizes on depth information\nprovided by RGBD cameras. In this work, we show that accurate and efficient 3D\nhand tracking is possible, even for the case of RGB stereo. A straightforward\napproach for solving the problem based on such input would be to first recover\ndepth and then apply a state of the art depth-based 3D hand tracking method.\nUnfortunately, this does not work well in practice because the stereo-based,\ndense 3D reconstruction of hands is far less accurate than the one obtained by\nRGBD cameras. Our approach bypasses 3D reconstruction and follows a completely\ndifferent route: 3D hand tracking is formulated as an optimization problem\nwhose solution is the hand configuration that maximizes the color consistency\nbetween the two views of the hand. We demonstrate the applicability of our\nmethod for real time tracking of a single hand, of a hand manipulating an\nobject and of two interacting hands. The method has been evaluated\nquantitatively on standard datasets and in comparison to relevant, state of the\nart RGBD-based approaches. The obtained results demonstrate that the proposed\nstereo-based method performs equally well to its RGBD-based competitors, and in\nsome cases, it even outperforms them. \n\n"}
{"id": "1705.05524", "contents": "Title: Learning Hard Alignments with Variational Inference Abstract: There has recently been significant interest in hard attention models for\ntasks such as object recognition, visual captioning and speech recognition.\nHard attention can offer benefits over soft attention such as decreased\ncomputational cost, but training hard attention models can be difficult because\nof the discrete latent variables they introduce. Previous work used REINFORCE\nand Q-learning to approach these issues, but those methods can provide\nhigh-variance gradient estimates and be slow to train. In this paper, we tackle\nthe problem of learning hard attention for a sequential task using variational\ninference methods, specifically the recently introduced VIMCO and NVIL.\nFurthermore, we propose a novel baseline that adapts VIMCO to this setting. We\ndemonstrate our method on a phoneme recognition task in clean and noisy\nenvironments and show that our method outperforms REINFORCE, with the\ndifference being greater for a more complicated task. \n\n"}
{"id": "1705.05591", "contents": "Title: Learning Convex Regularizers for Optimal Bayesian Denoising Abstract: We propose a data-driven algorithm for the maximum a posteriori (MAP)\nestimation of stochastic processes from noisy observations. The primary\nstatistical properties of the sought signal is specified by the penalty\nfunction (i.e., negative logarithm of the prior probability density function).\nOur alternating direction method of multipliers (ADMM)-based approach\ntranslates the estimation task into successive applications of the proximal\nmapping of the penalty function. Capitalizing on this direct link, we define\nthe proximal operator as a parametric spline curve and optimize the spline\ncoefficients by minimizing the average reconstruction error for a given\ntraining set. The key aspects of our learning method are that the associated\npenalty function is constrained to be convex and the convergence of the ADMM\niterations is proven. As a result of these theoretical guarantees, adaptation\nof the proposed framework to different levels of measurement noise is extremely\nsimple and does not require any retraining. We apply our method to estimation\nof both sparse and non-sparse models of L\\'{e}vy processes for which the\nminimum mean square error (MMSE) estimators are available. We carry out a\nsingle training session and perform comparisons at various signal-to-noise\nratio (SNR) values. Simulations illustrate that the performance of our\nalgorithm is practically identical to the one of the MMSE estimator\nirrespective of the noise power. \n\n"}
{"id": "1705.05640", "contents": "Title: WebVision Challenge: Visual Learning and Understanding With Web Data Abstract: We present the 2017 WebVision Challenge, a public image recognition challenge\ndesigned for deep learning based on web images without instance-level human\nannotation. Following the spirit of previous vision challenges, such as ILSVRC,\nPlaces2 and PASCAL VOC, which have played critical roles in the development of\ncomputer vision by contributing to the community with large scale annotated\ndata for model designing and standardized benchmarking, we contribute with this\nchallenge a large scale web images dataset, and a public competition with a\nworkshop co-located with CVPR 2017. The WebVision dataset contains more than\n$2.4$ million web images crawled from the Internet by using queries generated\nfrom the $1,000$ semantic concepts of the benchmark ILSVRC 2012 dataset. Meta\ninformation is also included. A validation set and test set containing human\nannotated images are also provided to facilitate algorithmic development. The\n2017 WebVision challenge consists of two tracks, the image classification task\non WebVision test set, and the transfer learning task on PASCAL VOC 2012\ndataset. In this paper, we describe the details of data collection and\nannotation, highlight the characteristics of the dataset, and introduce the\nevaluation metrics. \n\n"}
{"id": "1705.06524", "contents": "Title: A fully dense and globally consistent 3D map reconstruction approach for\n  GI tract to enhance therapeutic relevance of the endoscopic capsule robot Abstract: In the gastrointestinal (GI) tract endoscopy field, ingestible wireless\ncapsule endoscopy is emerging as a novel, minimally invasive diagnostic\ntechnology for inspection of the GI tract and diagnosis of a wide range of\ndiseases and pathologies. Since the development of this technology, medical\ndevice companies and many research groups have made substantial progress in\nconverting passive capsule endoscopes to robotic active capsule endoscopes with\nmost of the functionality of current active flexible endoscopes. However,\nrobotic capsule endoscopy still has some challenges. In particular, the use of\nsuch devices to generate a precise three-dimensional (3D) mapping of the entire\ninner organ remains an unsolved problem. Such global 3D maps of inner organs\nwould help doctors to detect the location and size of diseased areas more\naccurately and intuitively, thus permitting more reliable diagnoses. To our\nknowledge, this paper presents the first complete pipeline for a complete 3D\nvisual map reconstruction of the stomach. The proposed pipeline is modular and\nincludes a preprocessing module, an image registration module, and a final\nshape-from-shading-based 3D reconstruction module; the 3D map is primarily\ngenerated by a combination of image stitching and shape-from-shading\ntechniques, and is updated in a frame-by-frame iterative fashion via capsule\nmotion inside the stomach. A comprehensive quantitative analysis of the\nproposed 3D reconstruction method is performed using an esophagus gastro\nduodenoscopy simulator, three different endoscopic cameras, and a 3D optical\nscanner. \n\n"}
{"id": "1705.06566", "contents": "Title: Learning Texture Manifolds with the Periodic Spatial GAN Abstract: This paper introduces a novel approach to texture synthesis based on\ngenerative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the\nstructure of the input noise distribution by constructing tensors with\ndifferent types of dimensions. We call this technique Periodic Spatial GAN\n(PSGAN). The PSGAN has several novel abilities which surpass the current state\nof the art in texture synthesis. First, we can learn multiple textures from\ndatasets of one or more complex large images. Second, we show that the image\ngeneration with PSGANs has properties of a texture manifold: we can smoothly\ninterpolate between samples in the structured noise space and generate novel\nsamples, which lie perceptually between the textures of the original dataset.\nIn addition, we can also accurately learn periodical textures. We make multiple\nexperiments which show that PSGANs can flexibly handle diverse texture and\nimage data sources. Our method is highly scalable and it can generate output\nimages of arbitrary large size. \n\n"}
{"id": "1705.06870", "contents": "Title: Fiber Orientation Estimation Guided by a Deep Network Abstract: Diffusion magnetic resonance imaging (dMRI) is currently the only tool for\nnoninvasively imaging the brain's white matter tracts. The fiber orientation\n(FO) is a key feature computed from dMRI for fiber tract reconstruction.\nBecause the number of FOs in a voxel is usually small, dictionary-based sparse\nreconstruction has been used to estimate FOs with a relatively small number of\ndiffusion gradients. However, accurate FO estimation in regions with complex FO\nconfigurations in the presence of noise can still be challenging. In this work\nwe explore the use of a deep network for FO estimation in a dictionary-based\nframework and propose an algorithm named Fiber Orientation Reconstruction\nguided by a Deep Network (FORDN). FORDN consists of two steps. First, we use a\nsmaller dictionary encoding coarse basis FOs to represent the diffusion\nsignals. To estimate the mixture fractions of the dictionary atoms (and thus\ncoarse FOs), a deep network is designed specifically for solving the sparse\nreconstruction problem. Here, the smaller dictionary is used to reduce the\ncomputational cost of training. Second, the coarse FOs inform the final FO\nestimation, where a larger dictionary encoding dense basis FOs is used and a\nweighted l1-norm regularized least squares problem is solved to encourage FOs\nthat are consistent with the network output. FORDN was evaluated and compared\nwith state-of-the-art algorithms that estimate FOs using sparse reconstruction\non simulated and real dMRI data, and the results demonstrate the benefit of\nusing a deep network for FO estimation. \n\n"}
{"id": "1705.06950", "contents": "Title: The Kinetics Human Action Video Dataset Abstract: We describe the DeepMind Kinetics human action video dataset. The dataset\ncontains 400 human action classes, with at least 400 video clips for each\naction. Each clip lasts around 10s and is taken from a different YouTube video.\nThe actions are human focussed and cover a broad range of classes including\nhuman-object interactions such as playing instruments, as well as human-human\ninteractions such as shaking hands. We describe the statistics of the dataset,\nhow it was collected, and give some baseline performance figures for neural\nnetwork architectures trained and tested for human action classification on\nthis dataset. We also carry out a preliminary analysis of whether imbalance in\nthe dataset leads to bias in the classifiers. \n\n"}
{"id": "1705.07204", "contents": "Title: Ensemble Adversarial Training: Attacks and Defenses Abstract: Adversarial examples are perturbed inputs designed to fool machine learning\nmodels. Adversarial training injects such examples into training data to\nincrease robustness. To scale this technique to large datasets, perturbations\nare crafted using fast single-step methods that maximize a linear approximation\nof the model's loss. We show that this form of adversarial training converges\nto a degenerate global minimum, wherein small curvature artifacts near the data\npoints obfuscate a linear approximation of the loss. The model thus learns to\ngenerate weak perturbations, rather than defend against strong ones. As a\nresult, we find that adversarial training remains vulnerable to black-box\nattacks, where we transfer perturbations computed on undefended models, as well\nas to a powerful novel single-step attack that escapes the non-smooth vicinity\nof the input data via a small random step. We further introduce Ensemble\nAdversarial Training, a technique that augments training data with\nperturbations transferred from other models. On ImageNet, Ensemble Adversarial\nTraining yields models with strong robustness to black-box attacks. In\nparticular, our most robust model won the first round of the NIPS 2017\ncompetition on Defenses against Adversarial Attacks. However, subsequent work\nfound that more elaborate black-box attacks could significantly enhance\ntransferability and reduce the accuracy of our models. \n\n"}
{"id": "1705.07208", "contents": "Title: PixColor: Pixel Recursive Colorization Abstract: We propose a novel approach to automatically produce multiple colorized\nversions of a grayscale image. Our method results from the observation that the\ntask of automated colorization is relatively easy given a low-resolution\nversion of the color image. We first train a conditional PixelCNN to generate a\nlow resolution color for a given grayscale image. Then, given the generated\nlow-resolution color image and the original grayscale image as inputs, we train\na second CNN to generate a high-resolution colorization of an image. We\ndemonstrate that our approach produces more diverse and plausible colorizations\nthan existing methods, as judged by human raters in a \"Visual Turing Test\". \n\n"}
{"id": "1705.07329", "contents": "Title: Critical Contours: An Invariant Linking Image Flow with Salient Surface\n  Organization Abstract: We exploit a key result from visual psychophysics---that individuals perceive\nshape qualitatively---to develop the use of a geometrical/topological\n\"invariant'' (the Morse--Smale complex) relating image structure with surface\nstructure. Differences across individuals are minimal near certain\nconfigurations such as ridges and boundaries, and it is these configurations\nthat are often represented in line drawings. In particular, we introduce a\nmethod for inferring a qualitative three-dimensional shape from shading\npatterns that link the shape-from-shading inference with shape-from-contour\ninference. For a given shape, certain shading patches approach \"line drawings''\nin a well-defined limit. Under this limit, and invariably with respect to\nrendering choices, these shading patterns provide a qualitative description of\nthe surface. We further show that, under this model, the contours partition the\nsurface into meaningful parts using the Morse--Smale complex. These critical\ncontours are the (perceptually) stable parts of this complex and are invariant\nover a wide class of rendering models. Intuitively, our main result shows that\ncritical contours partition smooth surfaces into bumps and valleys, in effect\nproviding a scaffold on the image from which a full surface can be\ninterpolated. \n\n"}
{"id": "1705.07818", "contents": "Title: TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for\n  Video Action Segmentation Abstract: Action segmentation as a milestone towards building automatic systems to\nunderstand untrimmed videos has received considerable attention in the recent\nyears. It is typically being modeled as a sequence labeling problem but\ncontains intrinsic and sufficient differences than text parsing or speech\nprocessing. In this paper, we introduce a novel hybrid temporal convolutional\nand recurrent network (TricorNet), which has an encoder-decoder architecture:\nthe encoder consists of a hierarchy of temporal convolutional kernels that\ncapture the local motion changes of different actions; the decoder is a\nhierarchy of recurrent neural networks that are able to learn and memorize\nlong-term action dependencies after the encoding stage. Our model is simple but\nextremely effective in terms of video sequence labeling. The experimental\nresults on three public action segmentation datasets have shown that the\nproposed model achieves superior performance over the state of the art. \n\n"}
{"id": "1705.08690", "contents": "Title: Continual Learning with Deep Generative Replay Abstract: Attempts to train a comprehensive artificial intelligence capable of solving\nmultiple tasks have been impeded by a chronic problem called catastrophic\nforgetting. Although simply replaying all previous data alleviates the problem,\nit requires large memory and even worse, often infeasible in real world\napplications where the access to past data is limited. Inspired by the\ngenerative nature of hippocampus as a short-term memory system in primate\nbrain, we propose the Deep Generative Replay, a novel framework with a\ncooperative dual model architecture consisting of a deep generative model\n(\"generator\") and a task solving model (\"solver\"). With only these two models,\ntraining data for previous tasks can easily be sampled and interleaved with\nthose for a new task. We test our methods in several sequential learning\nsettings involving image classification tasks. \n\n"}
{"id": "1705.08865", "contents": "Title: Anti-spoofing Methods for Automatic SpeakerVerification System Abstract: Growing interest in automatic speaker verification (ASV)systems has lead to\nsignificant quality improvement of spoofing attackson them. Many research works\nconfirm that despite the low equal er-ror rate (EER) ASV systems are still\nvulnerable to spoofing attacks. Inthis work we overview different acoustic\nfeature spaces and classifiersto determine reliable and robust countermeasures\nagainst spoofing at-tacks. We compared several spoofing detection systems,\npresented so far,on the development and evaluation datasets of the Automatic\nSpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge\n2015.Experimental results presented in this paper demonstrate that the useof\nmagnitude and phase information combination provides a substantialinput into\nthe efficiency of the spoofing detection systems. Also wavelet-based features\nshow impressive results in terms of equal error rate. Inour overview we compare\nspoofing performance for systems based on dif-ferent classifiers. Comparison\nresults demonstrate that the linear SVMclassifier outperforms the conventional\nGMM approach. However, manyresearchers inspired by the great success of deep\nneural networks (DNN)approaches in the automatic speech recognition, applied\nDNN in thespoofing detection task and obtained quite low EER for known and\nun-known type of spoofing attacks. \n\n"}
{"id": "1705.10245", "contents": "Title: Deep Learning for Patient-Specific Kidney Graft Survival Analysis Abstract: An accurate model of patient-specific kidney graft survival distributions can\nhelp to improve shared-decision making in the treatment and care of patients.\nIn this paper, we propose a deep learning method that directly models the\nsurvival function instead of estimating the hazard function to predict survival\ntimes for graft patients based on the principle of multi-task learning. By\nlearning to jointly predict the time of the event, and its rank in the cox\npartial log likelihood framework, our deep learning approach outperforms, in\nterms of survival time prediction quality and concordance index, other common\nmethods for survival analysis, including the Cox Proportional Hazards model and\na network trained on the cox partial log-likelihood. \n\n"}
{"id": "1706.00051", "contents": "Title: Deep Generative Adversarial Networks for Compressed Sensing Automates\n  MRI Abstract: Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear\ninverse task demanding time and resource intensive computations that can\nsubstantially trade off {\\it accuracy} for {\\it speed} in real-time imaging. In\naddition, state-of-the-art compressed sensing (CS) analytics are not cognizant\nof the image {\\it diagnostic quality}. To cope with these challenges we put\nforth a novel CS framework that permeates benefits from generative adversarial\nnetworks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR\nimages from historical patients. Leveraging a mixture of least-squares (LS)\nGANs and pixel-wise $\\ell_1$ cost, a deep residual network with skip\nconnections is trained as the generator that learns to remove the {\\it\naliasing} artifacts by projecting onto the manifold. LSGAN learns the texture\ndetails, while $\\ell_1$ controls the high-frequency noise. A multilayer\nconvolutional neural network is then jointly trained based on diagnostic\nquality images to discriminate the projection quality. The test phase performs\nfeed-forward propagation over the generator network that demands a very low\ncomputational overhead. Extensive evaluations are performed on a large\ncontrast-enhanced MR dataset of pediatric patients. In particular, images rated\nbased on expert radiologists corroborate that GANCS retrieves high contrast\nimages with detailed texture relative to conventional CS, and pixel-wise\nschemes. In addition, it offers reconstruction under a few milliseconds, two\norders of magnitude faster than state-of-the-art CS-MRI schemes. \n\n"}
{"id": "1706.00631", "contents": "Title: Dual-reference Face Retrieval Abstract: Face retrieval has received much attention over the past few decades, and\nmany efforts have been made in retrieving face images against pose,\nillumination, and expression variations. However, the conventional works fail\nto meet the requirements of a potential and novel task --- retrieving a\nperson's face image at a specific age, especially when the specific 'age' is\nnot given as a numeral, i.e. 'retrieving someone's image at the similar age\nperiod shown by another person's image'. To tackle this problem, we propose a\ndual reference face retrieval framework in this paper, where the system takes\ntwo inputs: an identity reference image which indicates the target identity and\nan age reference image which reflects the target age. In our framework, the raw\nimages are first projected on a joint manifold, which preserves both the age\nand identity locality. Then two similarity metrics of age and identity are\nexploited and optimized by utilizing our proposed quartet-based model. The\nexperiments show promising results, outperforming hierarchical methods. \n\n"}
{"id": "1706.00719", "contents": "Title: Automating Carotid Intima-Media Thickness Video Interpretation with\n  Convolutional Neural Networks Abstract: Cardiovascular disease (CVD) is the leading cause of mortality yet largely\npreventable, but the key to prevention is to identify at-risk individuals\nbefore adverse events. For predicting individual CVD risk, carotid intima-media\nthickness (CIMT), a noninvasive ultrasound method, has proven to be valuable,\noffering several advantages over CT coronary artery calcium score. However,\neach CIMT examination includes several ultrasound videos, and interpreting each\nof these CIMT videos involves three operations: (1) select three end-diastolic\nultrasound frames (EUF) in the video, (2) localize a region of interest (ROI)\nin each selected frame, and (3) trace the lumen-intima interface and the\nmedia-adventitia interface in each ROI to measure CIMT. These operations are\ntedious, laborious, and time consuming, a serious limitation that hinders the\nwidespread utilization of CIMT in clinical practice. To overcome this\nlimitation, this paper presents a new system to automate CIMT video\ninterpretation. Our extensive experiments demonstrate that the suggested system\nsignificantly outperforms the state-of-the-art methods. The superior\nperformance is attributable to our unified framework based on convolutional\nneural networks (CNNs) coupled with our informative image representation and\neffective post-processing of the CNN outputs, which are uniquely designed for\neach of the above three operations. \n\n"}
{"id": "1706.01000", "contents": "Title: Image Compression Based on Compressive Sensing: End-to-End Comparison\n  with JPEG Abstract: We present an end-to-end image compression system based on compressive\nsensing. The presented system integrates the conventional scheme of compressive\nsampling and reconstruction with quantization and entropy coding. The\ncompression performance, in terms of decoded image quality versus data rate, is\nshown to be comparable with JPEG and significantly better at the low rate\nrange. We study the parameters that influence the system performance, including\n(i) the choice of sensing matrix, (ii) the trade-off between quantization and\ncompression ratio, and (iii) the reconstruction algorithms. We propose an\neffective method to jointly control the quantization step and compression ratio\nin order to achieve near optimal quality at any given bit rate. Furthermore,\nour proposed image compression system can be directly used in the compressive\nsensing camera, e.g. the single pixel camera, to construct a hardware\ncompressive sampling system. \n\n"}
{"id": "1706.01021", "contents": "Title: Where and Who? Automatic Semantic-Aware Person Composition Abstract: Image compositing is a method used to generate realistic yet fake imagery by\ninserting contents from one image to another. Previous work in compositing has\nfocused on improving appearance compatibility of a user selected foreground\nsegment and a background image (i.e. color and illumination consistency). In\nthis work, we instead develop a fully automated compositing model that\nadditionally learns to select and transform compatible foreground segments from\na large collection given only an input image background. To simplify the task,\nwe restrict our problem by focusing on human instance composition, because\nhuman segments exhibit strong correlations with their background and because of\nthe availability of large annotated data. We develop a novel branching\nConvolutional Neural Network (CNN) that jointly predicts candidate person\nlocations given a background image. We then use pre-trained deep feature\nrepresentations to retrieve person instances from a large segment database.\nExperimental results show that our model can generate composite images that\nlook visually convincing. We also develop a user interface to demonstrate the\npotential application of our method. \n\n"}
{"id": "1706.01061", "contents": "Title: Face R-CNN Abstract: Faster R-CNN is one of the most representative and successful methods for\nobject detection, and has been becoming increasingly popular in various\nobjection detection applications. In this report, we propose a robust deep face\ndetection approach based on Faster R-CNN. In our approach, we exploit several\nnew techniques including new multi-task loss function design, online hard\nexample mining, and multi-scale training strategy to improve Faster R-CNN in\nmultiple aspects. The proposed approach is well suited for face detection, so\nwe call it Face R-CNN. Extensive experiments are conducted on two most popular\nand challenging face detection benchmarks, FDDB and WIDER FACE, to demonstrate\nthe superiority of the proposed approach over state-of-the-arts. \n\n"}
{"id": "1706.01789", "contents": "Title: Deep Alignment Network: A convolutional neural network for robust face\n  alignment Abstract: In this paper, we propose Deep Alignment Network (DAN), a robust face\nalignment method based on a deep neural network architecture. DAN consists of\nmultiple stages, where each stage improves the locations of the facial\nlandmarks estimated by the previous stage. Our method uses entire face images\nat all stages, contrary to the recently proposed face alignment methods that\nrely on local patches. This is possible thanks to the use of landmark heatmaps\nwhich provide visual information about landmark locations estimated at the\nprevious stages of the algorithm. The use of entire face images rather than\npatches allows DAN to handle face images with large variation in head pose and\ndifficult initializations. An extensive evaluation on two publicly available\ndatasets shows that DAN reduces the state-of-the-art failure rate by up to 70%.\nOur method has also been submitted for evaluation as part of the Menpo\nchallenge. \n\n"}
{"id": "1706.01824", "contents": "Title: Robust Online Multi-Task Learning with Correlative and Personalized\n  Structures Abstract: Multi-Task Learning (MTL) can enhance a classifier's generalization\nperformance by learning multiple related tasks simultaneously. Conventional MTL\nworks under the offline or batch setting, and suffers from expensive training\ncost and poor scalability. To address such inefficiency issues, online learning\ntechniques have been applied to solve MTL problems. However, most existing\nalgorithms of online MTL constrain task relatedness into a presumed structure\nvia a single weight matrix, which is a strict restriction that does not always\nhold in practice. In this paper, we propose a robust online MTL framework that\novercomes this restriction by decomposing the weight matrix into two\ncomponents: the first one captures the low-rank common structure among tasks\nvia a nuclear norm and the second one identifies the personalized patterns of\noutlier tasks via a group lasso. Theoretical analysis shows the proposed\nalgorithm can achieve a sub-linear regret with respect to the best linear model\nin hindsight. Even though the above framework achieves good performance, the\nnuclear norm that simply adds all nonzero singular values together may not be a\ngood low-rank approximation. To improve the results, we use a log-determinant\nfunction as a non-convex rank approximation. The gradient scheme is applied to\noptimize log-determinant function and can obtain a closed-form solution for\nthis refined problem. Experimental results on a number of real-world\napplications verify the efficacy of our method. \n\n"}
{"id": "1706.02071", "contents": "Title: DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data Abstract: A class of recent approaches for generating images, called Generative\nAdversarial Networks (GAN), have been used to generate impressively realistic\nimages of objects, bedrooms, handwritten digits and a variety of other image\nmodalities. However, typical GAN-based approaches require large amounts of\ntraining data to capture the diversity across the image modality. In this\npaper, we propose DeLiGAN -- a novel GAN-based architecture for diverse and\nlimited training data scenarios. In our approach, we reparameterize the latent\ngenerative space as a mixture model and learn the mixture model's parameters\nalong with those of GAN. This seemingly simple modification to the GAN\nframework is surprisingly effective and results in models which enable\ndiversity in generated samples although trained with limited data. In our work,\nwe show that DeLiGAN can generate images of handwritten digits, objects and\nhand-drawn sketches, all using limited amounts of data. To quantitatively\ncharacterize intra-class diversity of generated samples, we also introduce a\nmodified version of \"inception-score\", a measure which has been found to\ncorrelate well with human assessment of generated samples. \n\n"}
{"id": "1706.02337", "contents": "Title: Learning to Extract Semantic Structure from Documents Using Multimodal\n  Fully Convolutional Neural Network Abstract: We present an end-to-end, multimodal, fully convolutional network for\nextracting semantic structures from document images. We consider document\nsemantic structure extraction as a pixel-wise segmentation task, and propose a\nunified model that classifies pixels based not only on their visual appearance,\nas in the traditional page segmentation task, but also on the content of\nunderlying text. Moreover, we propose an efficient synthetic document\ngeneration process that we use to generate pretraining data for our network.\nOnce the network is trained on a large set of synthetic documents, we fine-tune\nthe network on unlabeled real documents using a semi-supervised approach. We\nsystematically study the optimum network architecture and show that both our\nmultimodal approach and the synthetic data pretraining significantly boost the\nperformance. \n\n"}
{"id": "1706.02684", "contents": "Title: Learning Local Receptive Fields and their Weight Sharing Scheme on\n  Graphs Abstract: We propose a simple and generic layer formulation that extends the properties\nof convolutional layers to any domain that can be described by a graph. Namely,\nwe use the support of its adjacency matrix to design learnable weight sharing\nfilters able to exploit the underlying structure of signals in the same fashion\nas for images. The proposed formulation makes it possible to learn the weights\nof the filter as well as a scheme that controls how they are shared across the\ngraph. We perform validation experiments with image datasets and show that\nthese filters offer performances comparable with convolutional ones. \n\n"}
{"id": "1706.03319", "contents": "Title: Style Transfer for Anime Sketches with Enhanced Residual U-net and\n  Auxiliary Classifier GAN Abstract: Recently, with the revolutionary neural style transferring methods,\ncreditable paintings can be synthesized automatically from content images and\nstyle images. However, when it comes to the task of applying a painting's style\nto an anime sketch, these methods will just randomly colorize sketch lines as\noutputs and fail in the main task: specific style tranfer. In this paper, we\nintegrated residual U-net to apply the style to the gray-scale sketch with\nauxiliary classifier generative adversarial network (AC-GAN). The whole process\nis automatic and fast, and the results are creditable in the quality of art\nstyle as well as colorization. \n\n"}
{"id": "1706.03497", "contents": "Title: A filter based approach for inbetweening Abstract: We present a filter based approach for inbetweening. We train a convolutional\nneural network to generate intermediate frames. This network aim to generate\nsmooth animation of line drawings. Our method can process scanned images\ndirectly. Our method does not need to compute correspondence of lines and\ntopological changes explicitly. We experiment our method with real animation\nproduction data. The results show that our method can generate intermediate\nframes partially. \n\n"}
{"id": "1706.03691", "contents": "Title: Certified Defenses for Data Poisoning Attacks Abstract: Machine learning systems trained on user-provided data are susceptible to\ndata poisoning attacks, whereby malicious users inject false training data with\nthe aim of corrupting the learned model. While recent work has proposed a\nnumber of attacks and defenses, little is understood about the worst-case loss\nof a defense in the face of a determined attacker. We address this by\nconstructing approximate upper bounds on the loss across a broad family of\nattacks, for defenders that first perform outlier removal followed by empirical\nrisk minimization. Our approximation relies on two assumptions: (1) that the\ndataset is large enough for statistical concentration between train and test\nerror to hold, and (2) that outliers within the clean (non-poisoned) data do\nnot have a strong effect on the model. Our bound comes paired with a candidate\nattack that often nearly matches the upper bound, giving us a powerful tool for\nquickly assessing defenses on a given dataset. Empirically, we find that even\nunder a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to\nattack, while in contrast the IMDB sentiment dataset can be driven from 12% to\n23% test error by adding only 3% poisoned data. \n\n"}
{"id": "1706.04499", "contents": "Title: SEARNN: Training RNNs with Global-Local Losses Abstract: We propose SEARNN, a novel training algorithm for recurrent neural networks\n(RNNs) inspired by the \"learning to search\" (L2S) approach to structured\nprediction. RNNs have been widely successful in structured prediction\napplications such as machine translation or parsing, and are commonly trained\nusing maximum likelihood estimation (MLE). Unfortunately, this training loss is\nnot always an appropriate surrogate for the test error: by only maximizing the\nground truth probability, it fails to exploit the wealth of information offered\nby structured losses. Further, it introduces discrepancies between training and\npredicting (such as exposure bias) that may hurt test performance. Instead,\nSEARNN leverages test-alike search space exploration to introduce global-local\nlosses that are closer to the test error. We first demonstrate improved\nperformance over MLE on two different tasks: OCR and spelling correction. Then,\nwe propose a subsampling strategy to enable SEARNN to scale to large vocabulary\nsizes. This allows us to validate the benefits of our approach on a machine\ntranslation task. \n\n"}
{"id": "1706.05048", "contents": "Title: Human-like Clustering with Deep Convolutional Neural Networks Abstract: Classification and clustering have been studied separately in machine\nlearning and computer vision. Inspired by the recent success of deep learning\nmodels in solving various vision problems (e.g., object recognition, semantic\nsegmentation) and the fact that humans serve as the gold standard in assessing\nclustering algorithms, here, we advocate for a unified treatment of the two\nproblems and suggest that hierarchical frameworks that progressively build\ncomplex patterns on top of the simpler ones (e.g., convolutional neural\nnetworks) offer a promising solution. We do not dwell much on the learning\nmechanisms in these frameworks as they are still a matter of debate, with\nrespect to biological constraints. Instead, we emphasize on the\ncompositionality of the real world structures and objects. In particular, we\nshow that CNNs, trained end to end using back propagation with noisy labels,\nare able to cluster data points belonging to several overlapping shapes, and do\nso much better than the state of the art algorithms. The main takeaway lesson\nfrom our study is that mechanisms of human vision, particularly the hierarchal\norganization of the visual ventral stream should be taken into account in\nclustering algorithms (e.g., for learning representations in an unsupervised\nmanner or with minimum supervision) to reach human level clustering\nperformance. This, by no means, suggests that other methods do not hold merits.\nFor example, methods relying on pairwise affinities (e.g., spectral clustering)\nhave been very successful in many scenarios but still fail in some cases (e.g.,\noverlapping clusters). \n\n"}
{"id": "1706.05067", "contents": "Title: Face Clustering: Representation and Pairwise Constraints Abstract: Clustering face images according to their identity has two important\napplications: (i) grouping a collection of face images when no external labels\nare associated with images, and (ii) indexing for efficient large scale face\nretrieval. The clustering problem is composed of two key parts: face\nrepresentation and choice of similarity for grouping faces. We first propose a\nrepresentation based on ResNet, which has been shown to perform very well in\nimage classification problems. Given this representation, we design a\nclustering algorithm, Conditional Pairwise Clustering (ConPaC), which directly\nestimates the adjacency matrix only based on the similarity between face\nimages. This allows a dynamic selection of number of clusters and retains\npairwise similarity between faces. ConPaC formulates the clustering problem as\na Conditional Random Field (CRF) model and uses Loopy Belief Propagation to\nfind an approximate solution for maximizing the posterior probability of the\nadjacency matrix. Experimental results on two benchmark face datasets (LFW and\nIJB-B) show that ConPaC outperforms well known clustering algorithms such as\nk-means, spectral clustering and approximate rank-order. Additionally, our\nalgorithm can naturally incorporate pairwise constraints to obtain a\nsemi-supervised version that leads to improved clustering performance. We also\npropose an k-NN variant of ConPaC, which has a linear time complexity given a\nk-NN graph, suitable for large datasets. \n\n"}
{"id": "1706.07446", "contents": "Title: Deep Transfer Learning: A new deep learning glitch classification method\n  for advanced LIGO Abstract: The exquisite sensitivity of the advanced LIGO detectors has enabled the\ndetection of multiple gravitational wave signals. The sophisticated design of\nthese detectors mitigates the effect of most types of noise. However, advanced\nLIGO data streams are contaminated by numerous artifacts known as glitches:\nnon-Gaussian noise transients with complex morphologies. Given their high rate\nof occurrence, glitches can lead to false coincident detections, obscure and\neven mimic gravitational wave signals. Therefore, successfully characterizing\nand removing glitches from advanced LIGO data is of utmost importance. Here, we\npresent the first application of Deep Transfer Learning for glitch\nclassification, showing that knowledge from deep learning algorithms trained\nfor real-world object recognition can be transferred for classifying glitches\nin time-series based on their spectrogram images. Using the Gravity Spy\ndataset, containing hand-labeled, multi-duration spectrograms obtained from\nreal LIGO data, we demonstrate that this method enables optimal use of very\ndeep convolutional neural networks for classification given small training\ndatasets, significantly reduces the time for training the networks, and\nachieves state-of-the-art accuracy above 98.8%, with perfect precision-recall\non 8 out of 22 classes. Furthermore, new types of glitches can be classified\naccurately given few labeled examples with this technique. Once trained via\ntransfer learning, we show that the convolutional neural networks can be\ntruncated and used as excellent feature extractors for unsupervised clustering\nmethods to identify new classes based on their morphology, without any labeled\nexamples. Therefore, this provides a new framework for dynamic glitch\nclassification for gravitational wave detectors, which are expected to\nencounter new types of noise as they undergo gradual improvements to attain\ndesign sensitivity. \n\n"}
{"id": "1706.08126", "contents": "Title: ToolNet: Holistically-Nested Real-Time Segmentation of Robotic Surgical\n  Tools Abstract: Real-time tool segmentation from endoscopic videos is an essential part of\nmany computer-assisted robotic surgical systems and of critical importance in\nrobotic surgical data science. We propose two novel deep learning architectures\nfor automatic segmentation of non-rigid surgical instruments. Both methods take\nadvantage of automated deep-learning-based multi-scale feature extraction while\ntrying to maintain an accurate segmentation quality at all resolutions. The two\nproposed methods encode the multi-scale constraint inside the network\narchitecture. The first proposed architecture enforces it by cascaded\naggregation of predictions and the second proposed network does it by means of\na holistically-nested architecture where the loss at each scale is taken into\naccount for the optimization process. As the proposed methods are for real-time\nsemantic labeling, both present a reduced number of parameters. We propose the\nuse of parametric rectified linear units for semantic labeling in these small\narchitectures to increase the regularization ability of the design and maintain\nthe segmentation accuracy without overfitting the training sets. We compare the\nproposed architectures against state-of-the-art fully convolutional networks.\nWe validate our methods using existing benchmark datasets, including ex vivo\ncases with phantom tissue and different robotic surgical instruments present in\nthe scene. Our results show a statistically significant improved Dice\nSimilarity Coefficient over previous instrument segmentation methods. We\nanalyze our design choices and discuss the key drivers for improving accuracy. \n\n"}
{"id": "1706.09498", "contents": "Title: Real-time Distracted Driver Posture Classification Abstract: In this paper, we present a new dataset for \"distracted driver\" posture\nestimation. In addition, we propose a novel system that achieves 95.98% driving\nposture estimation classification accuracy. The system consists of a\ngenetically-weighted ensemble of Convolutional Neural Networks (CNNs). We show\nthat a weighted ensemble of classifiers using a genetic algorithm yields in\nbetter classification confidence. We also study the effect of different visual\nelements (i.e. hands and face) in distraction detection and classification by\nmeans of face and hand localizations. Finally, we present a thinned version of\nour ensemble that could achieve a 94.29% classification accuracy and operate in\na realtime environment. \n\n"}
{"id": "1706.09858", "contents": "Title: What's Mine is Yours: Pretrained CNNs for Limited Training Sonar ATR Abstract: Finding mines in Sonar imagery is a significant problem with a great deal of\nrelevance for seafaring military and commercial endeavors. Unfortunately, the\nlack of enormous Sonar image data sets has prevented automatic target\nrecognition (ATR) algorithms from some of the same advances seen in other\ncomputer vision fields. Namely, the boom in convolutional neural nets (CNNs)\nwhich have been able to achieve incredible results - even surpassing human\nactors - has not been an easily feasible route for many practitioners of Sonar\nATR. We demonstrate the power of one avenue to incorporating CNNs into Sonar\nATR: transfer learning. We first show how well a straightforward, flexible CNN\nfeature-extraction strategy can be used to obtain impressive if not\nstate-of-the-art results. Secondly, we propose a way to utilize the powerful\ntransfer learning approach towards multiple instance target detection and\nidentification within a provided synthetic aperture Sonar data set. \n\n"}
{"id": "1707.00587", "contents": "Title: Automatic Cardiac Disease Assessment on cine-MRI via Time-Series\n  Segmentation and Domain Specific Features Abstract: Cardiac magnetic resonance imaging improves on diagnosis of cardiovascular\ndiseases by providing images at high spatiotemporal resolution. Manual\nevaluation of these time-series, however, is expensive and prone to biased and\nnon-reproducible outcomes. In this paper, we present a method that addresses\nnamed limitations by integrating segmentation and disease classification into a\nfully automatic processing pipeline. We use an ensemble of UNet inspired\narchitectures for segmentation of cardiac structures such as the left and right\nventricular cavity (LVC, RVC) and the left ventricular myocardium (LVM) on each\ntime instance of the cardiac cycle. For the classification task, information is\nextracted from the segmented time-series in form of comprehensive features\nhandcrafted to reflect diagnostic clinical procedures. Based on these features\nwe train an ensemble of heavily regularized multilayer perceptrons (MLP) and a\nrandom forest classifier to predict the pathologic target class. We evaluated\nour method on the ACDC dataset (4 pathology groups, 1 healthy group) and\nachieve dice scores of 0.945 (LVC), 0.908 (RVC) and 0.905 (LVM) in a\ncross-validation over the training set (100 cases) and 0.950 (LVC), 0.923 (RVC)\nand 0.911 (LVM) on the test set (50 cases). We report a classification accuracy\nof 94% on a training set cross-validation and 92% on the test set. Our results\nunderpin the potential of machine learning methods for accurate, fast and\nreproducible segmentation and computer-assisted diagnosis (CAD). \n\n"}
{"id": "1707.00737", "contents": "Title: High-Quality Face Image SR Using Conditional Generative Adversarial\n  Networks Abstract: We propose a novel single face image super-resolution method, which named\nFace Conditional Generative Adversarial Network(FCGAN), based on boundary\nequilibrium generative adversarial networks. Without taking any facial prior\ninformation, our method can generate a high-resolution face image from a\nlow-resolution one. Compared with existing studies, both our training and\ntesting phases are end-to-end pipeline with little pre/post-processing. To\nenhance the convergence speed and strengthen feature propagation, skip-layer\nconnection is further employed in the generative and discriminative networks.\nExtensive experiments demonstrate that our model achieves competitive\nperformance compared with state-of-the-art models. \n\n"}
{"id": "1707.01220", "contents": "Title: DarkRank: Accelerating Deep Metric Learning via Cross Sample\n  Similarities Transfer Abstract: We have witnessed rapid evolution of deep neural network architecture design\nin the past years. These latest progresses greatly facilitate the developments\nin various areas such as computer vision and natural language processing.\nHowever, along with the extraordinary performance, these state-of-the-art\nmodels also bring in expensive computational cost. Directly deploying these\nmodels into applications with real-time requirement is still infeasible.\nRecently, Hinton etal. have shown that the dark knowledge within a powerful\nteacher model can significantly help the training of a smaller and faster\nstudent network. These knowledge are vastly beneficial to improve the\ngeneralization ability of the student model. Inspired by their work, we\nintroduce a new type of knowledge -- cross sample similarities for model\ncompression and acceleration. This knowledge can be naturally derived from deep\nmetric learning model. To transfer them, we bring the \"learning to rank\"\ntechnique into deep metric learning formulation. We test our proposed DarkRank\nmethod on various metric learning tasks including pedestrian re-identification,\nimage retrieval and image clustering. The results are quite encouraging. Our\nmethod can improve over the baseline method by a large margin. Moreover, it is\nfully compatible with other existing methods. When combined, the performance\ncan be further boosted. \n\n"}
{"id": "1707.01274", "contents": "Title: Learning-based Image Enhancement for Visual Odometry in Challenging HDR\n  Environments Abstract: One of the main open challenges in visual odometry (VO) is the robustness to\ndifficult illumination conditions or high dynamic range (HDR) environments. The\nmain difficulties in these situations come from both the limitations of the\nsensors and the inability to perform a successful tracking of interest points\nbecause of the bold assumptions in VO, such as brightness constancy. We address\nthis problem from a deep learning perspective, for which we first fine-tune a\nDeep Neural Network (DNN) with the purpose of obtaining enhanced\nrepresentations of the sequences for VO. Then, we demonstrate how the insertion\nof Long Short Term Memory (LSTM) allows us to obtain temporally consistent\nsequences, as the estimation depends on previous states. However, the use of\nvery deep networks does not allow the insertion into a real-time VO framework;\ntherefore, we also propose a Convolutional Neural Network (CNN) of reduced size\ncapable of performing faster. Finally, we validate the enhanced representations\nby evaluating the sequences produced by the two architectures in several\nstate-of-art VO algorithms, such as ORB-SLAM and DSO. \n\n"}
{"id": "1707.02392", "contents": "Title: Learning Representations and Generative Models for 3D Point Clouds Abstract: Three-dimensional geometric data offer an excellent domain for studying\nrepresentation learning and generative modeling. In this paper, we look at\ngeometric data represented as point clouds. We introduce a deep AutoEncoder\n(AE) network with state-of-the-art reconstruction quality and generalization\nability. The learned representations outperform existing methods on 3D\nrecognition tasks and enable shape editing via simple algebraic manipulations,\nsuch as semantic part editing, shape analogies and shape interpolation, as well\nas shape completion. We perform a thorough study of different generative models\nincluding GANs operating on the raw point clouds, significantly improved GANs\ntrained in the fixed latent space of our AEs, and Gaussian Mixture Models\n(GMMs). To quantitatively evaluate generative models we introduce measures of\nsample fidelity and diversity based on matchings between sets of point clouds.\nInterestingly, our evaluation of generalization, fidelity and diversity reveals\nthat GMMs trained in the latent space of our AEs yield the best results\noverall. \n\n"}
{"id": "1707.03194", "contents": "Title: Sensitivity Analysis for Mirror-Stratifiable Convex Functions Abstract: This paper provides a set of sensitivity analysis and activity identification\nresults for a class of convex functions with a strong geometric structure, that\nwe coined \"mirror-stratifiable\". These functions are such that there is a\nbijection between a primal and a dual stratification of the space into\npartitioning sets, called strata. This pairing is crucial to track the strata\nthat are identifiable by solutions of parametrized optimization problems or by\niterates of optimization algorithms. This class of functions encompasses all\nregularizers routinely used in signal and image processing, machine learning,\nand statistics. We show that this \"mirror-stratifiable\" structure enjoys a nice\nsensitivity theory, allowing us to study stability of solutions of optimization\nproblems to small perturbations, as well as activity identification of\nfirst-order proximal splitting-type algorithms. Existing results in the\nliterature typically assume that, under a non-degeneracy condition, the active\nset associated to a minimizer is stable to small perturbations and is\nidentified in finite time by optimization schemes. In contrast, our results do\nnot require any non-degeneracy assumption: in consequence, the optimal active\nset is not necessarily stable anymore, but we are able to track precisely the\nset of identifiable strata.We show that these results have crucial implications\nwhen solving challenging ill-posed inverse problems via regularization, a\ntypical scenario where the non-degeneracy condition is not fulfilled. Our\ntheoretical results, illustrated by numerical simulations, allow to\ncharacterize the instability behaviour of the regularized solutions, by\nlocating the set of all low-dimensional strata that can be potentially\nidentified by these solutions. \n\n"}
{"id": "1707.03491", "contents": "Title: Creatism: A deep-learning photographer capable of creating professional\n  work Abstract: Machine-learning excels in many areas with well-defined goals. However, a\nclear goal is usually not available in art forms, such as photography. The\nsuccess of a photograph is measured by its aesthetic value, a very subjective\nconcept. This adds to the challenge for a machine learning approach.\n  We introduce Creatism, a deep-learning system for artistic content creation.\nIn our system, we break down aesthetics into multiple aspects, each can be\nlearned individually from a shared dataset of professional examples. Each\naspect corresponds to an image operation that can be optimized efficiently. A\nnovel editing tool, dramatic mask, is introduced as one operation that improves\ndramatic lighting for a photo. Our training does not require a dataset with\nbefore/after image pairs, or any additional labels to indicate different\naspects in aesthetics.\n  Using our system, we mimic the workflow of a landscape photographer, from\nframing for the best composition to carrying out various post-processing\noperations. The environment for our virtual photographer is simulated by a\ncollection of panorama images from Google Street View. We design a\n\"Turing-test\"-like experiment to objectively measure quality of its creations,\nwhere professional photographers rate a mixture of photographs from different\nsources blindly. Experiments show that a portion of our robot's creation can be\nconfused with professional work. \n\n"}
{"id": "1707.03548", "contents": "Title: Discriminative Block-Diagonal Representation Learning for Image\n  Recognition Abstract: Existing block-diagonal representation researches mainly focuses on casting\nblock-diagonal regularization on training data, while only little attention is\ndedicated to concurrently learning both block-diagonal representations of\ntraining and test data. In this paper, we propose a discriminative\nblock-diagonal low-rank representation (BDLRR) method for recognition. In\nparticular, the elaborate BDLRR is formulated as a joint optimization problem\nof shrinking the unfavorable representation from off-block-diagonal elements\nand strengthening the compact block-diagonal representation under the\nsemi-supervised framework of low-rank representation. To this end, we first\nimpose penalty constraints on the negative representation to eliminate the\ncorrelation between different classes such that the incoherence criterion of\nthe extra-class representation is boosted. Moreover, a constructed subspace\nmodel is developed to enhance the self-expressive power of training samples and\nfurther build the representation bridge between the training and test samples,\nsuch that the coherence of the learned intra-class representation is\nconsistently heightened. Finally, the resulting optimization problem is solved\nelegantly by employing an alternative optimization strategy, and a simple\nrecognition algorithm on the learned representation is utilized for final\nprediction. Extensive experimental results demonstrate that the proposed method\nachieves superb recognition results on four face image datasets, three\ncharacter datasets, and the fifteen scene multi-categories dataset. It not only\nshows superior potential on image recognition but also outperforms\nstate-of-the-art methods. \n\n"}
{"id": "1707.03718", "contents": "Title: LinkNet: Exploiting Encoder Representations for Efficient Semantic\n  Segmentation Abstract: Pixel-wise semantic segmentation for visual scene understanding not only\nneeds to be accurate, but also efficient in order to find any use in real-time\napplication. Existing algorithms even though are accurate but they do not focus\non utilizing the parameters of neural network efficiently. As a result they are\nhuge in terms of parameters and number of operations; hence slow too. In this\npaper, we propose a novel deep neural network architecture which allows it to\nlearn without any significant increase in number of parameters. Our network\nuses only 11.5 million parameters and 21.2 GFLOPs for processing an image of\nresolution 3x640x360. It gives state-of-the-art performance on CamVid and\ncomparable results on Cityscapes dataset. We also compare our networks\nprocessing time on NVIDIA GPU and embedded system device with existing\nstate-of-the-art architectures for different image resolutions. \n\n"}
{"id": "1707.05455", "contents": "Title: Pruning Convolutional Neural Networks for Image Instance Retrieval Abstract: In this work, we focus on the problem of image instance retrieval with deep\ndescriptors extracted from pruned Convolutional Neural Networks (CNN). The\nobjective is to heavily prune convolutional edges while maintaining retrieval\nperformance. To this end, we introduce both data-independent and data-dependent\nheuristics to prune convolutional edges, and evaluate their performance across\nvarious compression rates with different deep descriptors over several\nbenchmark datasets. Further, we present an end-to-end framework to fine-tune\nthe pruned network, with a triplet loss function specially designed for the\nretrieval task. We show that the combination of heuristic pruning and\nfine-tuning offers 5x compression rate without considerable loss in retrieval\nperformance. \n\n"}
{"id": "1707.05474", "contents": "Title: APE-GAN: Adversarial Perturbation Elimination with GAN Abstract: Although neural networks could achieve state-of-the-art performance while\nrecongnizing images, they often suffer a tremendous defeat from adversarial\nexamples--inputs generated by utilizing imperceptible but intentional\nperturbation to clean samples from the datasets. How to defense against\nadversarial examples is an important problem which is well worth researching.\nSo far, very few methods have provided a significant defense to adversarial\nexamples. In this paper, a novel idea is proposed and an effective framework\nbased Generative Adversarial Nets named APE-GAN is implemented to defense\nagainst the adversarial examples. The experimental results on three benchmark\ndatasets including MNIST, CIFAR10 and ImageNet indicate that APE-GAN is\neffective to resist adversarial examples generated from five attacks. \n\n"}
{"id": "1707.05474", "contents": "Title: APE-GAN: Adversarial Perturbation Elimination with GAN Abstract: Although neural networks could achieve state-of-the-art performance while\nrecongnizing images, they often suffer a tremendous defeat from adversarial\nexamples--inputs generated by utilizing imperceptible but intentional\nperturbation to clean samples from the datasets. How to defense against\nadversarial examples is an important problem which is well worth researching.\nSo far, very few methods have provided a significant defense to adversarial\nexamples. In this paper, a novel idea is proposed and an effective framework\nbased Generative Adversarial Nets named APE-GAN is implemented to defense\nagainst the adversarial examples. The experimental results on three benchmark\ndatasets including MNIST, CIFAR10 and ImageNet indicate that APE-GAN is\neffective to resist adversarial examples generated from five attacks. \n\n"}
{"id": "1707.05653", "contents": "Title: Faster Than Real-time Facial Alignment: A 3D Spatial Transformer Network\n  Approach in Unconstrained Poses Abstract: Facial alignment involves finding a set of landmark points on an image with a\nknown semantic meaning. However, this semantic meaning of landmark points is\noften lost in 2D approaches where landmarks are either moved to visible\nboundaries or ignored as the pose of the face changes. In order to extract\nconsistent alignment points across large poses, the 3D structure of the face\nmust be considered in the alignment step. However, extracting a 3D structure\nfrom a single 2D image usually requires alignment in the first place. We\npresent our novel approach to simultaneously extract the 3D shape of the face\nand the semantically consistent 2D alignment through a 3D Spatial Transformer\nNetwork (3DSTN) to model both the camera projection matrix and the warping\nparameters of a 3D model. By utilizing a generic 3D model and a Thin Plate\nSpline (TPS) warping function, we are able to generate subject specific 3D\nshapes without the need for a large 3D shape basis. In addition, our proposed\nnetwork can be trained in an end-to-end framework on entirely synthetic data\nfrom the 300W-LP dataset. Unlike other 3D methods, our approach only requires\none pass through the network resulting in a faster than real-time alignment.\nEvaluations of our model on the Annotated Facial Landmarks in the Wild (AFLW)\nand AFLW2000-3D datasets show our method achieves state-of-the-art performance\nover other 3D approaches to alignment. \n\n"}
{"id": "1707.06436", "contents": "Title: cvpaper.challenge in 2016: Futuristic Computer Vision through 1,600\n  Papers Survey Abstract: The paper gives futuristic challenges disscussed in the cvpaper.challenge. In\n2015 and 2016, we thoroughly study 1,600+ papers in several\nconferences/journals such as CVPR/ICCV/ECCV/NIPS/PAMI/IJCV. \n\n"}
{"id": "1707.07210", "contents": "Title: Inspiring Computer Vision System Solutions Abstract: The \"digital Michelangelo project\" was a seminal computer vision project in\nthe early 2000's that pushed the capabilities of acquisition systems and\ninvolved multiple people from diverse fields, many of whom are now leaders in\nindustry and academia. Reviewing this project with modern eyes provides us with\nthe opportunity to reflect on several issues, relevant now as then to the field\nof computer vision and research in general, that go beyond the technical\naspects of the work.\n  This article was written in the context of a reading group competition at the\nweek-long International Computer Vision Summer School 2017 (ICVSS) on Sicily,\nItaly. To deepen the participants understanding of computer vision and to\nfoster a sense of community, various reading groups were tasked to highlight\nimportant lessons which may be learned from provided literature, going beyond\nthe contents of the paper. This report is the winning entry of this guided\ndiscourse (Fig. 1). The authors closely examined the origins, fruits and most\nimportantly lessons about research in general which may be distilled from the\n\"digital Michelangelo project\". Discussions leading to this report were held\nwithin the group as well as with Hao Li, the group mentor. \n\n"}
{"id": "1707.07397", "contents": "Title: Synthesizing Robust Adversarial Examples Abstract: Standard methods for generating adversarial examples for neural networks do\nnot consistently fool neural network classifiers in the physical world due to a\ncombination of viewpoint shifts, camera noise, and other natural\ntransformations, limiting their relevance to real-world systems. We demonstrate\nthe existence of robust 3D adversarial objects, and we present the first\nalgorithm for synthesizing examples that are adversarial over a chosen\ndistribution of transformations. We synthesize two-dimensional adversarial\nimages that are robust to noise, distortion, and affine transformation. We\napply our algorithm to complex three-dimensional objects, using 3D-printing to\nmanufacture the first physical adversarial objects. Our results demonstrate the\nexistence of 3D adversarial objects in the physical world. \n\n"}
{"id": "1707.07791", "contents": "Title: Deep Feature Learning via Structured Graph Laplacian Embedding for\n  Person Re-Identification Abstract: Learning the distance metric between pairs of examples is of great importance\nfor visual recognition, especially for person re-identification (Re-Id).\nRecently, the contrastive and triplet loss are proposed to enhance the\ndiscriminative power of the deeply learned features, and have achieved\nremarkable success. As can be seen, either the contrastive or triplet loss is\njust one special case of the Euclidean distance relationships among these\ntraining samples. Therefore, we propose a structured graph Laplacian embedding\nalgorithm, which can formulate all these structured distance relationships into\nthe graph Laplacian form. The proposed method can take full advantages of the\nstructured distance relationships among these training samples, with the\nconstructed complete graph. Besides, this formulation makes our method\neasy-to-implement and super-effective. When embedding the proposed algorithm\nwith the softmax loss for the CNN training, our method can obtain much more\nrobust and discriminative deep features with inter-personal dispersion and\nintra-personal compactness, which is essential to person Re-Id. We illustrate\nthe effectiveness of our proposed method on top of three popular networks,\nnamely AlexNet, DGDNet and ResNet50, on recent four widely used Re-Id benchmark\ndatasets. Our proposed method achieves state-of-the-art performances. \n\n"}
{"id": "1707.07815", "contents": "Title: Graph-Theoretic Spatiotemporal Context Modeling for Video Saliency\n  Detection Abstract: As an important and challenging problem in computer vision, video saliency\ndetection is typically cast as a spatiotemporal context modeling problem over\nconsecutive frames. As a result, a key issue in video saliency detection is how\nto effectively capture the intrinsical properties of atomic video structures as\nwell as their associated contextual interactions along the spatial and temporal\ndimensions. Motivated by this observation, we propose a graph-theoretic video\nsaliency detection approach based on adaptive video structure discovery, which\nis carried out within a spatiotemporal atomic graph. Through graph-based\nmanifold propagation, the proposed approach is capable of effectively modeling\nthe semantically contextual interactions among atomic video structures for\nsaliency detection while preserving spatial smoothness and temporal\nconsistency. Experiments demonstrate the effectiveness of the proposed approach\nover several benchmark datasets. \n\n"}
{"id": "1707.08037", "contents": "Title: Automatic Liver Segmentation Using an Adversarial Image-to-Image Network Abstract: Automatic liver segmentation in 3D medical images is essential in many\nclinical applications, such as pathological diagnosis of hepatic diseases,\nsurgical planning, and postoperative assessment. However, it is still a very\nchallenging task due to the complex background, fuzzy boundary, and various\nappearance of liver. In this paper, we propose an automatic and efficient\nalgorithm to segment liver from 3D CT volumes. A deep image-to-image network\n(DI2IN) is first deployed to generate the liver segmentation, employing a\nconvolutional encoder-decoder architecture combined with multi-level feature\nconcatenation and deep supervision. Then an adversarial network is utilized\nduring training process to discriminate the output of DI2IN from ground truth,\nwhich further boosts the performance of DI2IN. The proposed method is trained\non an annotated dataset of 1000 CT volumes with various different scanning\nprotocols (e.g., contrast and non-contrast, various resolution and position)\nand large variations in populations (e.g., ages and pathology). Our approach\noutperforms the state-of-the-art solutions in terms of segmentation accuracy\nand computing efficiency. \n\n"}
{"id": "1707.08289", "contents": "Title: Fast Deep Matting for Portrait Animation on Mobile Phone Abstract: Image matting plays an important role in image and video editing. However,\nthe formulation of image matting is inherently ill-posed. Traditional methods\nusually employ interaction to deal with the image matting problem with trimaps\nand strokes, and cannot run on the mobile phone in real-time. In this paper, we\npropose a real-time automatic deep matting approach for mobile devices. By\nleveraging the densely connected blocks and the dilated convolution, a light\nfull convolutional network is designed to predict a coarse binary mask for\nportrait images. And a feathering block, which is edge-preserving and matting\nadaptive, is further developed to learn the guided filter and transform the\nbinary mask into alpha matte. Finally, an automatic portrait animation system\nbased on fast deep matting is built on mobile devices, which does not need any\ninteraction and can realize real-time matting with 15 fps. The experiments show\nthat the proposed approach achieves comparable results with the\nstate-of-the-art matting solvers. \n\n"}
{"id": "1707.08819", "contents": "Title: A Downsampled Variant of ImageNet as an Alternative to the CIFAR\n  datasets Abstract: The original ImageNet dataset is a popular large-scale benchmark for training\nDeep Neural Networks. Since the cost of performing experiments (e.g, algorithm\ndesign, architecture search, and hyperparameter tuning) on the original dataset\nmight be prohibitive, we propose to consider a downsampled version of ImageNet.\nIn contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,\nour proposed ImageNet32$\\times$32 (and its variants ImageNet64$\\times$64 and\nImageNet16$\\times$16) contains exactly the same number of classes and images as\nImageNet, with the only difference that the images are downsampled to\n32$\\times$32 pixels per image (64$\\times$64 and 16$\\times$16 pixels for the\nvariants, respectively). Experiments on these downsampled variants are\ndramatically faster than on the original ImageNet and the characteristics of\nthe downsampled datasets with respect to optimal hyperparameters appear to\nremain similar. The proposed datasets and scripts to reproduce our results are\navailable at http://image-net.org/download-images and\nhttps://github.com/PatrykChrabaszcz/Imagenet32_Scripts \n\n"}
{"id": "1707.08945", "contents": "Title: Robust Physical-World Attacks on Deep Learning Models Abstract: Recent studies show that the state-of-the-art deep neural networks (DNNs) are\nvulnerable to adversarial examples, resulting from small-magnitude\nperturbations added to the input. Given that that emerging physical systems are\nusing DNNs in safety-critical situations, adversarial examples could mislead\nthese systems and cause dangerous situations.Therefore, understanding\nadversarial examples in the physical world is an important step towards\ndeveloping resilient learning algorithms. We propose a general attack\nalgorithm,Robust Physical Perturbations (RP2), to generate robust visual\nadversarial perturbations under different physical conditions. Using the\nreal-world case of road sign classification, we show that adversarial examples\ngenerated using RP2 achieve high targeted misclassification rates against\nstandard-architecture road sign classifiers in the physical world under various\nenvironmental conditions, including viewpoints. Due to the current lack of a\nstandardized testing method, we propose a two-stage evaluation methodology for\nrobust physical adversarial examples consisting of lab and field tests. Using\nthis methodology, we evaluate the efficacy of physical adversarial\nmanipulations on real objects. Witha perturbation in the form of only black and\nwhite stickers,we attack a real stop sign, causing targeted misclassification\nin 100% of the images obtained in lab settings, and in 84.8%of the captured\nvideo frames obtained on a moving vehicle(field test) for the target\nclassifier. \n\n"}
{"id": "1707.09557", "contents": "Title: Improved Adversarial Systems for 3D Object Generation and Reconstruction Abstract: This paper describes a new approach for training generative adversarial\nnetworks (GAN) to understand the detailed 3D shape of objects. While GANs have\nbeen used in this domain previously, they are notoriously hard to train,\nespecially for the complex joint data distribution over 3D objects of many\ncategories and orientations. Our method extends previous work by employing the\nWasserstein distance normalized with gradient penalization as a training\nobjective. This enables improved generation from the joint object shape\ndistribution. Our system can also reconstruct 3D shape from 2D images and\nperform shape completion from occluded 2.5D range scans. We achieve notable\nquantitative improvements in comparison to existing baselines \n\n"}
{"id": "1707.09842", "contents": "Title: Deep Domain Adaptation by Geodesic Distance Minimization Abstract: In this paper, we propose a new approach called Deep LogCORAL for\nunsupervised visual domain adaptation. Our work builds on the recently proposed\nDeep CORAL method, which proposed to train a convolutional neural network and\nsimultaneously minimize the Euclidean distance of convariance matrices between\nthe source and target domains. We propose to use the Riemannian distance,\napproximated by Log-Euclidean distance, to replace the naive Euclidean distance\nin Deep CORAL. We also consider first-order information, and minimize the\ndistance of mean vectors between two domains. We build an end-to-end model, in\nwhich we minimize both the classification loss, and the domain difference based\non the first and second order information between two domains. Our experiments\non the benchmark Office dataset demonstrate the improvements of our newly\nproposed Deep LogCORAL approach over the Deep CORAL method, as well as further\nimprovement when optimizing both orders of information. \n\n"}
{"id": "1707.09938", "contents": "Title: Deep Convolutional Framelet Denosing for Low-Dose CT via Wavelet\n  Residual Network Abstract: Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT\nare computationally expensive. To address this problem, we recently proposed a\ndeep convolutional neural network (CNN) for low-dose X-ray CT and won the\nsecond place in 2016 AAPM Low-Dose CT Grand Challenge. However, some of the\ntexture were not fully recovered. To address this problem, here we propose a\nnovel framelet-based denoising algorithm using wavelet residual network which\nsynergistically combines the expressive power of deep learning and the\nperformance guarantee from the framelet-based denoising algorithms. The new\nalgorithms were inspired by the recent interpretation of the deep convolutional\nneural network (CNN) as a cascaded convolution framelet signal representation.\nExtensive experimental results confirm that the proposed networks have\nsignificantly improved performance and preserves the detail texture of the\noriginal images. \n\n"}
{"id": "1708.00106", "contents": "Title: Material Editing Using a Physically Based Rendering Network Abstract: The ability to edit materials of objects in images is desirable by many\ncontent creators. However, this is an extremely challenging task as it requires\nto disentangle intrinsic physical properties of an image. We propose an\nend-to-end network architecture that replicates the forward image formation\nprocess to accomplish this task. Specifically, given a single image, the\nnetwork first predicts intrinsic properties, i.e. shape, illumination, and\nmaterial, which are then provided to a rendering layer. This layer performs\nin-network image synthesis, thereby enabling the network to understand the\nphysics behind the image formation process. The proposed rendering layer is\nfully differentiable, supports both diffuse and specular materials, and thus\ncan be applicable in a variety of problem settings. We demonstrate a rich set\nof visually plausible material editing examples and provide an extensive\ncomparative study. \n\n"}
{"id": "1708.01022", "contents": "Title: When Kernel Methods meet Feature Learning: Log-Covariance Network for\n  Action Recognition from Skeletal Data Abstract: Human action recognition from skeletal data is a hot research topic and\nimportant in many open domain applications of computer vision, thanks to\nrecently introduced 3D sensors. In the literature, naive methods simply\ntransfer off-the-shelf techniques from video to the skeletal representation.\nHowever, the current state-of-the-art is contended between to different\nparadigms: kernel-based methods and feature learning with (recurrent) neural\nnetworks. Both approaches show strong performances, yet they exhibit heavy, but\ncomplementary, drawbacks. Motivated by this fact, our work aims at combining\ntogether the best of the two paradigms, by proposing an approach where a\nshallow network is fed with a covariance representation. Our intuition is that,\nas long as the dynamics is effectively modeled, there is no need for the\nclassification network to be deep nor recurrent in order to score favorably. We\nvalidate this hypothesis in a broad experimental analysis over 6 publicly\navailable datasets. \n\n"}
{"id": "1708.01204", "contents": "Title: Improved Speech Reconstruction from Silent Video Abstract: Speechreading is the task of inferring phonetic information from visually\nobserved articulatory facial movements, and is a notoriously difficult task for\nhumans to perform. In this paper we present an end-to-end model based on a\nconvolutional neural network (CNN) for generating an intelligible and\nnatural-sounding acoustic speech signal from silent video frames of a speaking\nperson. We train our model on speakers from the GRID and TCD-TIMIT datasets,\nand evaluate the quality and intelligibility of reconstructed speech using\ncommon objective measurements. We show that speech predictions from the\nproposed model attain scores which indicate significantly improved quality over\nexisting models. In addition, we show promising results towards reconstructing\nspeech from an unconstrained dictionary. \n\n"}
{"id": "1708.02478", "contents": "Title: From Deterministic to Generative: Multi-Modal Stochastic RNNs for Video\n  Captioning Abstract: Video captioning in essential is a complex natural process, which is affected\nby various uncertainties stemming from video content, subjective judgment, etc.\nIn this paper we build on the recent progress in using encoder-decoder\nframework for video captioning and address what we find to be a critical\ndeficiency of the existing methods, that most of the decoders propagate\ndeterministic hidden states. Such complex uncertainty cannot be modeled\nefficiently by the deterministic models. In this paper, we propose a generative\napproach, referred to as multi-modal stochastic RNNs networks (MS-RNN), which\nmodels the uncertainty observed in the data using latent stochastic variables.\nTherefore, MS-RNN can improve the performance of video captioning, and generate\nmultiple sentences to describe a video considering different random factors.\nSpecifically, a multi-modal LSTM (M-LSTM) is first proposed to interact with\nboth visual and textual features to capture a high-level representation. Then,\na backward stochastic LSTM (S-LSTM) is proposed to support uncertainty\npropagation by introducing latent variables. Experimental results on the\nchallenging datasets MSVD and MSR-VTT show that our proposed MS-RNN approach\noutperforms the state-of-the-art video captioning benchmarks. \n\n"}
{"id": "1708.02734", "contents": "Title: Joint Face Alignment and 3D Face Reconstruction with Application to Face\n  Recognition Abstract: Face alignment and 3D face reconstruction are traditionally accomplished as\nseparated tasks. By exploring the strong correlation between 2D landmarks and\n3D shapes, in contrast, we propose a joint face alignment and 3D face\nreconstruction method to simultaneously solve these two problems for 2D face\nimages of arbitrary poses and expressions. This method, based on a summation\nmodel of 3D faces and cascaded regression in 2D and 3D shape spaces,\niteratively and alternately applies two cascaded regressors, one for updating\n2D landmarks and the other for 3D shape. The 3D shape and the landmarks are\ncorrelated via a 3D-to-2D mapping matrix, which is updated in each iteration to\nrefine the location and visibility of 2D landmarks. Unlike existing methods,\nthe proposed method can fully automatically generate both\npose-and-expression-normalized (PEN) and expressive 3D faces and localize both\nvisible and invisible 2D landmarks. Based on the PEN 3D faces, we devise a\nmethod to enhance face recognition accuracy across poses and expressions. Both\nlinear and nonlinear implementations of the proposed method are presented and\nevaluated in this paper. Extensive experiments show that the proposed method\ncan achieve the state-of-the-art accuracy in both face alignment and 3D face\nreconstruction, and benefit face recognition owing to its reconstructed PEN 3D\nface. \n\n"}
{"id": "1708.02735", "contents": "Title: Gaussian Prototypical Networks for Few-Shot Learning on Omniglot Abstract: We propose a novel architecture for $k$-shot classification on the Omniglot\ndataset. Building on prototypical networks, we extend their architecture to\nwhat we call Gaussian prototypical networks. Prototypical networks learn a map\nbetween images and embedding vectors, and use their clustering for\nclassification. In our model, a part of the encoder output is interpreted as a\nconfidence region estimate about the embedding point, and expressed as a\nGaussian covariance matrix. Our network then constructs a direction and class\ndependent distance metric on the embedding space, using uncertainties of\nindividual data points as weights. We show that Gaussian prototypical networks\nare a preferred architecture over vanilla prototypical networks with an\nequivalent number of parameters. We report state-of-the-art performance in\n1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot\n5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.\nWe explore artificially down-sampling a fraction of images in the training set,\nwhich improves our performance even further. We therefore hypothesize that\nGaussian prototypical networks might perform better in less homogeneous,\nnoisier datasets, which are commonplace in real world applications. \n\n"}
{"id": "1708.03273", "contents": "Title: Analysis of Convolutional Neural Networks for Document Image\n  Classification Abstract: Convolutional Neural Networks (CNNs) are state-of-the-art models for document\nimage classification tasks. However, many of these approaches rely on\nparameters and architectures designed for classifying natural images, which\ndiffer from document images. We question whether this is appropriate and\nconduct a large empirical study to find what aspects of CNNs most affect\nperformance on document images. Among other results, we exceed the\nstate-of-the-art on the RVL-CDIP dataset by using shear transform data\naugmentation and an architecture designed for a larger input image.\nAdditionally, we analyze the learned features and find evidence that CNNs\ntrained on RVL-CDIP learn region-specific layout features. \n\n"}
{"id": "1708.03276", "contents": "Title: Document Image Binarization with Fully Convolutional Neural Networks Abstract: Binarization of degraded historical manuscript images is an important\npre-processing step for many document processing tasks. We formulate\nbinarization as a pixel classification learning task and apply a novel Fully\nConvolutional Network (FCN) architecture that operates at multiple image\nscales, including full resolution. The FCN is trained to optimize a continuous\nversion of the Pseudo F-measure metric and an ensemble of FCNs outperform the\ncompetition winners on 4 of 7 DIBCO competitions. This same binarization\ntechnique can also be applied to different domains such as Palm Leaf\nManuscripts with good performance. We analyze the performance of the proposed\nmodel w.r.t. the architectural hyperparameters, size and diversity of training\ndata, and the input features chosen. \n\n"}
{"id": "1708.03985", "contents": "Title: AffectNet: A Database for Facial Expression, Valence, and Arousal\n  Computing in the Wild Abstract: Automated affective computing in the wild setting is a challenging problem in\ncomputer vision. Existing annotated databases of facial expressions in the wild\nare small and mostly cover discrete emotions (aka the categorical model). There\nare very limited annotated facial databases for affective computing in the\ncontinuous dimensional model (e.g., valence and arousal). To meet this need, we\ncollected, annotated, and prepared for public distribution a new database of\nfacial emotions in the wild (called AffectNet). AffectNet contains more than\n1,000,000 facial images from the Internet by querying three major search\nengines using 1250 emotion related keywords in six different languages. About\nhalf of the retrieved images were manually annotated for the presence of seven\ndiscrete facial expressions and the intensity of valence and arousal. AffectNet\nis by far the largest database of facial expression, valence, and arousal in\nthe wild enabling research in automated facial expression recognition in two\ndifferent emotion models. Two baseline deep neural networks are used to\nclassify images in the categorical model and predict the intensity of valence\nand arousal. Various evaluation metrics show that our deep neural network\nbaselines can perform better than conventional machine learning methods and\noff-the-shelf facial expression recognition systems. \n\n"}
{"id": "1708.04503", "contents": "Title: Pathological Pulmonary Lobe Segmentation from CT Images using\n  Progressive Holistically Nested Neural Networks and Random Walker Abstract: Automatic pathological pulmonary lobe segmentation(PPLS) enables regional\nanalyses of lung disease, a clinically important capability. Due to often\nincomplete lobe boundaries, PPLS is difficult even for experts, and most prior\nart requires inference from contextual information. To address this, we propose\na novel PPLS method that couples deep learning with the random walker (RW)\nalgorithm. We first employ the recent progressive holistically-nested network\n(P-HNN) model to identify potential lobar boundaries, then generate final\nsegmentations using a RW that is seeded and weighted by the P-HNN output. We\nare the first to apply deep learning to PPLS. The advantages are independence\nfrom prior airway/vessel segmentations, increased robustness in diseased lungs,\nand methodological simplicity that does not sacrifice accuracy. Our method\nposts a high mean Jaccard score of 0.888$\\pm$0.164 on a held-out set of 154 CT\nscans from lung-disease patients, while also significantly (p < 0.001)\noutperforming a state-of-the-art method. \n\n"}
{"id": "1708.04669", "contents": "Title: Convolutional Neural Networks for Non-iterative Reconstruction of\n  Compressively Sensed Images Abstract: Traditional algorithms for compressive sensing recovery are computationally\nexpensive and are ineffective at low measurement rates. In this work, we\npropose a data driven non-iterative algorithm to overcome the shortcomings of\nearlier iterative algorithms. Our solution, ReconNet, is a deep neural network,\nwhose parameters are learned end-to-end to map block-wise compressive\nmeasurements of the scene to the desired image blocks. Reconstruction of an\nimage becomes a simple forward pass through the network and can be done in\nreal-time. We show empirically that our algorithm yields reconstructions with\nhigher PSNRs compared to iterative algorithms at low measurement rates and in\npresence of measurement noise. We also propose a variant of ReconNet which uses\nadversarial loss in order to further improve reconstruction quality. We discuss\nhow adding a fully connected layer to the existing ReconNet architecture allows\nfor jointly learning the measurement matrix and the reconstruction algorithm in\na single network. Experiments on real data obtained from a block compressive\nimager show that our networks are robust to unseen sensor noise. Finally,\nthrough an experiment in object tracking, we show that even at very low\nmeasurement rates, reconstructions using our algorithm possess rich semantic\ncontent that can be used for high level inference. \n\n"}
{"id": "1708.04728", "contents": "Title: DeepRebirth: Accelerating Deep Neural Network Execution on Mobile\n  Devices Abstract: Deploying deep neural networks on mobile devices is a challenging task.\nCurrent model compression methods such as matrix decomposition effectively\nreduce the deployed model size, but still cannot satisfy real-time processing\nrequirement. This paper first discovers that the major obstacle is the\nexcessive execution time of non-tensor layers such as pooling and normalization\nwithout tensor-like trainable parameters. This motivates us to design a novel\nacceleration framework: DeepRebirth through \"slimming\" existing consecutive and\nparallel non-tensor and tensor layers. The layer slimming is executed at\ndifferent substructures: (a) streamline slimming by merging the consecutive\nnon-tensor and tensor layer vertically; (b) branch slimming by merging\nnon-tensor and tensor branches horizontally. The proposed optimization\noperations significantly accelerate the model execution and also greatly reduce\nthe run-time memory cost since the slimmed model architecture contains less\nhidden layers. To maximally avoid accuracy loss, the parameters in new\ngenerated layers are learned with layer-wise fine-tuning based on both\ntheoretical analysis and empirical verification. As observed in the experiment,\nDeepRebirth achieves more than 3x speed-up and 2.5x run-time memory saving on\nGoogLeNet with only 0.4% drop of top-5 accuracy on ImageNet. Furthermore, by\ncombining with other model compression techniques, DeepRebirth offers an\naverage of 65ms inference time on the CPU of Samsung Galaxy S6 with 86.5% top-5\naccuracy, 14% faster than SqueezeNet which only has a top-5 accuracy of 80.5%. \n\n"}
{"id": "1708.04781", "contents": "Title: Racing Thompson: an Efficient Algorithm for Thompson Sampling with\n  Non-conjugate Priors Abstract: Thompson sampling has impressive empirical performance for many multi-armed\nbandit problems. But current algorithms for Thompson sampling only work for the\ncase of conjugate priors since these algorithms require to infer the posterior,\nwhich is often computationally intractable when the prior is not conjugate. In\nthis paper, we propose a novel algorithm for Thompson sampling which only\nrequires to draw samples from a tractable distribution, so our algorithm is\nefficient even when the prior is non-conjugate. To do this, we reformulate\nThompson sampling as an optimization problem via the Gumbel-Max trick. After\nthat we construct a set of random variables and our goal is to identify the one\nwith highest mean. Finally, we solve it with techniques in best arm\nidentification. \n\n"}
{"id": "1708.04943", "contents": "Title: Stacked Deconvolutional Network for Semantic Segmentation Abstract: Recent progress in semantic segmentation has been driven by improving the\nspatial resolution under Fully Convolutional Networks (FCNs). To address this\nproblem, we propose a Stacked Deconvolutional Network (SDN) for semantic\nsegmentation. In SDN, multiple shallow deconvolutional networks, which are\ncalled as SDN units, are stacked one by one to integrate contextual information\nand guarantee the fine recovery of localization information. Meanwhile,\ninter-unit and intra-unit connections are designed to assist network training\nand enhance feature fusion since the connections improve the flow of\ninformation and gradient propagation throughout the network. Besides,\nhierarchical supervision is applied during the upsampling process of each SDN\nunit, which guarantees the discrimination of feature representations and\nbenefits the network optimization. We carry out comprehensive experiments and\nachieve the new state-of-the-art results on three datasets, including PASCAL\nVOC 2012, CamVid, GATECH. In particular, our best model without CRF\npost-processing achieves an intersection-over-union score of 86.6% in the test\nset. \n\n"}
{"id": "1708.05929", "contents": "Title: Explaining Anomalies in Groups with Characterizing Subspace Rules Abstract: Anomaly detection has numerous applications and has been studied vastly. We\nconsider a complementary problem that has a much sparser literature: anomaly\ndescription. Interpretation of anomalies is crucial for practitioners for\nsense-making, troubleshooting, and planning actions. To this end, we present a\nnew approach called x-PACS (for eXplaining Patterns of Anomalies with\nCharacterizing Subspaces), which \"reverse-engineers\" the known anomalies by\nidentifying (1) the groups (or patterns) that they form, and (2) the\ncharacterizing subspace and feature rules that separate each anomalous pattern\nfrom normal instances. Explaining anomalies in groups not only saves analyst\ntime and gives insight into various types of anomalies, but also draws\nattention to potentially critical, repeating anomalies.\n  In developing x-PACS, we first construct a desiderata for the anomaly\ndescription problem. From a descriptive data mining perspective, our method\nexhibits five desired properties in our desiderata. Namely, it can unearth\nanomalous patterns (i) of multiple different types, (ii) hidden in arbitrary\nsubspaces of a high dimensional space, (iii) interpretable by the analysts,\n(iv) different from normal patterns of the data, and finally (v) succinct,\nproviding the shortest data description. Furthermore, x-PACS is highly\nparallelizable and scales linearly in terms of data size.\n  No existing work on anomaly description satisfies all of these properties\nsimultaneously. While not our primary goal, the anomalous patterns we find\nserve as interpretable \"signatures\" and can be used for detection. We show the\neffectiveness of x-PACS in explanation as well as detection on real-world\ndatasets as compared to state-of-the-art. \n\n"}
{"id": "1708.06767", "contents": "Title: Seeing Through Noise: Visually Driven Speaker Separation and Enhancement Abstract: Isolating the voice of a specific person while filtering out other voices or\nbackground noises is challenging when video is shot in noisy environments. We\npropose audio-visual methods to isolate the voice of a single speaker and\neliminate unrelated sounds. First, face motions captured in the video are used\nto estimate the speaker's voice, by passing the silent video frames through a\nvideo-to-speech neural network-based model. Then the speech predictions are\napplied as a filter on the noisy input audio. This approach avoids using\nmixtures of sounds in the learning process, as the number of such possible\nmixtures is huge, and would inevitably bias the trained model. We evaluate our\nmethod on two audio-visual datasets, GRID and TCD-TIMIT, and show that our\nmethod attains significant SDR and PESQ improvements over the raw\nvideo-to-speech predictions, and a well-known audio-only method. \n\n"}
{"id": "1708.07199", "contents": "Title: 3D Morphable Models as Spatial Transformer Networks Abstract: In this paper, we show how a 3D Morphable Model (i.e. a statistical model of\nthe 3D shape of a class of objects such as faces) can be used to spatially\ntransform input data as a module (a 3DMM-STN) within a convolutional neural\nnetwork. This is an extension of the original spatial transformer network in\nthat we are able to interpret and normalise 3D pose changes and\nself-occlusions. The trained localisation part of the network is independently\nuseful since it learns to fit a 3D morphable model to a single image. We show\nthat the localiser can be trained using only simple geometric loss functions on\na relatively small dataset yet is able to perform robust normalisation on\nhighly uncontrolled images including occlusion, self-occlusion and large pose\nchanges. \n\n"}
{"id": "1708.07755", "contents": "Title: Gait Recognition from Motion Capture Data Abstract: Gait recognition from motion capture data, as a pattern classification\ndiscipline, can be improved by the use of machine learning. This paper\ncontributes to the state-of-the-art with a statistical approach for extracting\nrobust gait features directly from raw data by a modification of Linear\nDiscriminant Analysis with Maximum Margin Criterion. Experiments on the CMU\nMoCap database show that the suggested method outperforms thirteen relevant\nmethods based on geometric features and a method to learn the features by a\ncombination of Principal Component Analysis and Linear Discriminant Analysis.\nThe methods are evaluated in terms of the distribution of biometric templates\nin respective feature spaces expressed in a number of class separability\ncoefficients and classification metrics. Results also indicate a high\nportability of learned features, that means, we can learn what aspects of walk\npeople generally differ in and extract those as general gait features.\nRecognizing people without needing group-specific features is convenient as\nparticular people might not always provide annotated learning data. As a\ncontribution to reproducible research, our evaluation framework and database\nhave been made publicly available. This research makes motion capture\ntechnology directly applicable for human recognition. \n\n"}
{"id": "1708.07860", "contents": "Title: Multi-task Self-Supervised Visual Learning Abstract: We investigate methods for combining multiple self-supervised tasks--i.e.,\nsupervised tasks where data can be collected without manual labeling--in order\nto train a single visual representation. First, we provide an apples-to-apples\ncomparison of four different self-supervised tasks using the very deep\nResNet-101 architecture. We then combine tasks to jointly train a network. We\nalso explore lasso regularization to encourage the network to factorize the\ninformation in its representation, and methods for \"harmonizing\" network inputs\nin order to learn a more unified representation. We evaluate all methods on\nImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our\nresults show that deeper networks work better, and that combining tasks--even\nvia a naive multi-head architecture--always improves performance. Our best\njoint network nearly matches the PASCAL performance of a model pre-trained on\nImageNet classification, and matches the ImageNet network on NYU depth\nprediction. \n\n"}
{"id": "1708.08042", "contents": "Title: Imbalanced Malware Images Classification: a CNN based Approach Abstract: Deep convolutional neural networks (CNNs) can be applied to malware binary\ndetection via image classification. The performance, however, is degraded due\nto the imbalance of malware families (classes). To mitigate this issue, we\npropose a simple yet effective weighted softmax loss which can be employed as\nthe final layer of deep CNNs. The original softmax loss is weighted, and the\nweight value can be determined according to class size. A scaling parameter is\nalso included in computing the weight. Proper selection of this parameter is\nstudied and an empirical option is suggested. The weighted loss aims at\nalleviating the impact of data imbalance in an end-to-end learning fashion. To\nvalidate the efficacy, we deploy the proposed weighted loss in a pre-trained\ndeep CNN model and fine-tune it to achieve promising results on malware images\nclassification. Extensive experiments also demonstrate that the new loss\nfunction can well fit other typical CNNs, yielding an improved classification\nperformance. \n\n"}
{"id": "1708.08689", "contents": "Title: Towards Poisoning of Deep Learning Algorithms with Back-gradient\n  Optimization Abstract: A number of online services nowadays rely upon machine learning to extract\nvaluable information from data collected in the wild. This exposes learning\nalgorithms to the threat of data poisoning, i.e., a coordinate attack in which\na fraction of the training data is controlled by the attacker and manipulated\nto subvert the learning process. To date, these attacks have been devised only\nagainst a limited class of binary learning algorithms, due to the inherent\ncomplexity of the gradient-based procedure used to optimize the poisoning\npoints (a.k.a. adversarial training examples). In this work, we rst extend the\nde nition of poisoning attacks to multiclass problems. We then propose a novel\npoisoning algorithm based on the idea of back-gradient optimization, i.e., to\ncompute the gradient of interest through automatic di erentiation, while also\nreversing the learning procedure to drastically reduce the attack complexity.\nCompared to current poisoning strategies, our approach is able to target a\nwider class of learning algorithms, trained with gradient- based procedures,\nincluding neural networks and deep learning architectures. We empirically\nevaluate its e ectiveness on several application examples, including spam\nltering, malware detection, and handwritten digit recognition. We nally show\nthat, similarly to adversarial test examples, adversarial training examples can\nalso be transferred across di erent learning algorithms. \n\n"}
{"id": "1708.08705", "contents": "Title: Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary\n  Learning Abstract: The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model,\nconsisting of a cascade of convolutional sparse layers, provides a new\ninterpretation of Convolutional Neural Networks (CNNs). Under this framework,\nthe computation of the forward pass in a CNN is equivalent to a pursuit\nalgorithm aiming to estimate the nested sparse representation vectors -- or\nfeature maps -- from a given input signal. Despite having served as a pivotal\nconnection between CNNs and sparse modeling, a deeper understanding of the\nML-CSC is still lacking: there are no pursuit algorithms that can serve this\nmodel exactly, nor are there conditions to guarantee a non-empty model. While\none can easily obtain signals that approximately satisfy the ML-CSC\nconstraints, it remains unclear how to simply sample from the model and, more\nimportantly, how one can train the convolutional filters from real data.\n  In this work, we propose a sound pursuit algorithm for the ML-CSC model by\nadopting a projection approach. We provide new and improved bounds on the\nstability of the solution of such pursuit and we analyze different practical\nalternatives to implement this in practice. We show that the training of the\nfilters is essential to allow for non-trivial signals in the model, and we\nderive an online algorithm to learn the dictionaries from real data,\neffectively resulting in cascaded sparse convolutional layers. Last, but not\nleast, we demonstrate the applicability of the ML-CSC model for several\napplications in an unsupervised setting, providing competitive results. Our\nwork represents a bridge between matrix factorization, sparse dictionary\nlearning and sparse auto-encoders, and we analyze these connections in detail. \n\n"}
{"id": "1708.08917", "contents": "Title: CirCNN: Accelerating and Compressing Deep Neural Networks Using\n  Block-CirculantWeight Matrices Abstract: Large-scale deep neural networks (DNNs) are both compute and memory\nintensive. As the size of DNNs continues to grow, it is critical to improve the\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\nmodel size is an important factor affecting performance, scalability and energy\nefficiency. Weight pruning achieves good compression ratios but suffers from\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\nincreased training complexity; and 3) the lack of rigorous guarantee of\ncompression ratio and inference accuracy. To overcome these limitations, this\npaper proposes CirCNN, a principled approach to represent weights and process\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\ncomputational complexity (both in inference and training) from O(n2) to\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\nmathematical rigor: it can converge to the same effectiveness as DNNs without\ncompression. The CirCNN architecture, a universal DNN inference engine that can\nbe implemented on various hardware/software platforms with configurable network\narchitecture. To demonstrate the performance and energy efficiency, we test\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\narchitecture achieves very high energy efficiency and performance with a small\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\nthe best state-of-the-art results. \n\n"}
{"id": "1708.09066", "contents": "Title: Block-Simultaneous Direction Method of Multipliers: A proximal\n  primal-dual splitting algorithm for nonconvex problems with multiple\n  constraints Abstract: We introduce a generalization of the linearized Alternating Direction Method\nof Multipliers to optimize a real-valued function $f$ of multiple arguments\nwith potentially multiple constraints $g_\\circ$ on each of them. The function\n$f$ may be nonconvex as long as it is convex in every argument, while the\nconstraints $g_\\circ$ need to be convex but not smooth. If $f$ is smooth, the\nproposed Block-Simultaneous Direction Method of Multipliers (bSDMM) can be\ninterpreted as a proximal analog to inexact coordinate descent methods under\nconstraints. Unlike alternative approaches for joint solvers of\nmultiple-constraint problems, we do not require linear operators $L$ of a\nconstraint function $g(L\\ \\cdot)$ to be invertible or linked between each\nother. bSDMM is well-suited for a range of optimization problems, in particular\nfor data analysis, where $f$ is the likelihood function of a model and $L$\ncould be a transformation matrix describing e.g. finite differences or basis\ntransforms. We apply bSDMM to the Non-negative Matrix Factorization task of a\nhyperspectral unmixing problem and demonstrate convergence and effectiveness of\nmultiple constraints on both matrix factors. The algorithms are implemented in\npython and released as an open-source package. \n\n"}
{"id": "1708.09633", "contents": "Title: ALCN: Meta-Learning for Contrast Normalization Applied to Robust 3D Pose\n  Estimation Abstract: To be robust to illumination changes when detecting objects in images, the\ncurrent trend is to train a Deep Network with training images captured under\nmany different lighting conditions. Unfortunately, creating such a training set\nis very cumbersome, or sometimes even impossible, for some applications such as\n3D pose estimation of specific objects, which is the application we focus on in\nthis paper. We therefore propose a novel illumination normalization method that\nlets us learn to detect objects and estimate their 3D pose under challenging\nillumination conditions from very few training samples. Our key insight is that\nnormalization parameters should adapt to the input image. In particular, we\nrealized this via a Convolutional Neural Network trained to predict the\nparameters of a generalization of the Difference-of-Gaussians method. We show\nthat our method significantly outperforms standard normalization methods and\ndemonstrate it on two challenging 3D detection and pose estimation problems. \n\n"}
{"id": "1708.09644", "contents": "Title: Abnormal Event Detection in Videos using Generative Adversarial Nets Abstract: In this paper we address the abnormality detection problem in crowded scenes.\nWe propose to use Generative Adversarial Nets (GANs), which are trained using\nnormal frames and corresponding optical-flow images in order to learn an\ninternal representation of the scene normality. Since our GANs are trained with\nonly normal data, they are not able to generate abnormal events. At testing\ntime the real data are compared with both the appearance and the motion\nrepresentations reconstructed by our GANs and abnormal areas are detected by\ncomputing local differences. Experimental results on challenging abnormality\ndetection datasets show the superiority of the proposed method compared to the\nstate of the art in both frame-level and pixel-level abnormality detection\ntasks. \n\n"}
{"id": "1709.00029", "contents": "Title: EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and\n  Land Cover Classification Abstract: In this paper, we address the challenge of land use and land cover\nclassification using Sentinel-2 satellite images. The Sentinel-2 satellite\nimages are openly and freely accessible provided in the Earth observation\nprogram Copernicus. We present a novel dataset based on Sentinel-2 satellite\nimages covering 13 spectral bands and consisting out of 10 classes with in\ntotal 27,000 labeled and geo-referenced images. We provide benchmarks for this\nnovel dataset with its spectral bands using state-of-the-art deep Convolutional\nNeural Network (CNNs). With the proposed novel dataset, we achieved an overall\nclassification accuracy of 98.57%. The resulting classification system opens a\ngate towards a number of Earth observation applications. We demonstrate how\nthis classification system can be used for detecting land use and land cover\nchanges and how it can assist in improving geographical maps. The\ngeo-referenced dataset EuroSAT is made publicly available at\nhttps://github.com/phelber/eurosat. \n\n"}
{"id": "1709.00141", "contents": "Title: Context Based Visual Content Verification Abstract: In this paper the intermediary visual content verification method based on\nmulti-level co-occurrences is studied. The co-occurrence statistics are in\ngeneral used to determine relational properties between objects based on\ninformation collected from data. As such these measures are heavily subject to\nrelative number of occurrences and give only limited amount of accuracy when\npredicting objects in real world. In order to improve the accuracy of this\nmethod in the verification task, we include the context information such as\nlocation, type of environment etc. In order to train our model we provide new\nannotated dataset the Advanced Attribute VOC (AAVOC) that contains additional\nproperties of the image. We show that the usage of context greatly improve the\naccuracy of verification with up to 16% improvement. \n\n"}
{"id": "1709.00572", "contents": "Title: XFlow: Cross-modal Deep Neural Networks for Audiovisual Classification Abstract: In recent years, there have been numerous developments towards solving\nmultimodal tasks, aiming to learn a stronger representation than through a\nsingle modality. Certain aspects of the data can be particularly useful in this\ncase - for example, correlations in the space or time domain across modalities\n- but should be wisely exploited in order to benefit from their full predictive\npotential. We propose two deep learning architectures with multimodal\ncross-connections that allow for dataflow between several feature extractors\n(XFlow). Our models derive more interpretable features and achieve better\nperformances than models which do not exchange representations, usefully\nexploiting correlations between audio and visual data, which have a different\ndimensionality and are nontrivially exchangeable. Our work improves on existing\nmultimodal deep learning algorithms in two essential ways: (1) it presents a\nnovel method for performing cross-modality (before features are learned from\nindividual modalities) and (2) extends the previously proposed\ncross-connections which only transfer information between streams that process\ncompatible data. Illustrating some of the representations learned by the\nconnections, we analyse their contribution to the increase in discrimination\nability and reveal their compatibility with a lip-reading network intermediate\nrepresentation. We provide the research community with Digits, a new dataset\nconsisting of three data types extracted from videos of people saying the\ndigits 0-9. Results show that both cross-modal architectures outperform their\nbaselines (by up to 11.5%) when evaluated on the AVletters, CUAVE and Digits\ndatasets, achieving state-of-the-art results. \n\n"}
{"id": "1709.00672", "contents": "Title: Unsupervised feature learning with discriminative encoder Abstract: In recent years, deep discriminative models have achieved extraordinary\nperformance on supervised learning tasks, significantly outperforming their\ngenerative counterparts. However, their success relies on the presence of a\nlarge amount of labeled data. How can one use the same discriminative models\nfor learning useful features in the absence of labels? We address this question\nin this paper, by jointly modeling the distribution of data and latent features\nin a manner that explicitly assigns zero probability to unobserved data. Rather\nthan maximizing the marginal probability of observed data, we maximize the\njoint probability of the data and the latent features using a two step EM-like\nprocedure. To prevent the model from overfitting to our initial selection of\nlatent features, we use adversarial regularization. Depending on the task, we\nallow the latent features to be one-hot or real-valued vectors and define a\nsuitable prior on the features. For instance, one-hot features correspond to\nclass labels and are directly used for the unsupervised and semi-supervised\nclassification task, whereas real-valued feature vectors are fed as input to\nsimple classifiers for auxiliary supervised discrimination tasks. The proposed\nmodel, which we dub discriminative encoder (or DisCoder), is flexible in the\ntype of latent features that it can capture. The proposed model achieves\nstate-of-the-art performance on several challenging tasks. \n\n"}
{"id": "1709.01215", "contents": "Title: ALICE: Towards Understanding Adversarial Learning for Joint Distribution\n  Matching Abstract: We investigate the non-identifiability issues associated with bidirectional\nadversarial training for joint distribution matching. Within a framework of\nconditional entropy, we propose both adversarial and non-adversarial approaches\nto learn desirable matched joint distributions for unsupervised and supervised\ntasks. We unify a broad family of adversarial models as joint distribution\nmatching problems. Our approach stabilizes learning of unsupervised\nbidirectional adversarial learning methods. Further, we introduce an extension\nfor semi-supervised learning tasks. Theoretical results are validated in\nsynthetic data and real-world applications. \n\n"}
{"id": "1709.01591", "contents": "Title: Improving Landmark Localization with Semi-Supervised Learning Abstract: We present two techniques to improve landmark localization in images from\npartially annotated datasets. Our primary goal is to leverage the common\nsituation where precise landmark locations are only provided for a small data\nsubset, but where class labels for classification or regression tasks related\nto the landmarks are more abundantly available. First, we propose the framework\nof sequential multitasking and explore it here through an architecture for\nlandmark localization where training with class labels acts as an auxiliary\nsignal to guide the landmark localization on unlabeled data. A key aspect of\nour approach is that errors can be backpropagated through a complete landmark\nlocalization model. Second, we propose and explore an unsupervised learning\ntechnique for landmark localization based on having a model predict equivariant\nlandmarks with respect to transformations applied to the image. We show that\nthese techniques, improve landmark prediction considerably and can learn\neffective detectors even when only a small fraction of the dataset has landmark\nlabels. We present results on two toy datasets and four real datasets, with\nhands and faces, and report new state-of-the-art on two datasets in the wild,\ne.g. with only 5\\% of labeled images we outperform previous state-of-the-art\ntrained on the AFLW dataset. \n\n"}
{"id": "1709.01686", "contents": "Title: BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks Abstract: Deep neural networks are state of the art methods for many learning tasks due\nto their ability to extract increasingly better features at each network layer.\nHowever, the improved performance of additional layers in a deep network comes\nat the cost of added latency and energy usage in feedforward inference. As\nnetworks continue to get deeper and larger, these costs become more prohibitive\nfor real-time and energy-sensitive applications. To address this issue, we\npresent BranchyNet, a novel deep network architecture that is augmented with\nadditional side branch classifiers. The architecture allows prediction results\nfor a large portion of test samples to exit the network early via these\nbranches when samples can already be inferred with high confidence. BranchyNet\nexploits the observation that features learned at an early layer of a network\nmay often be sufficient for the classification of many data points. For more\ndifficult samples, which are expected less frequently, BranchyNet will use\nfurther or all network layers to provide the best likelihood of correct\nprediction. We study the BranchyNet architecture using several well-known\nnetworks (LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that\nit can both improve accuracy and significantly reduce the inference time of the\nnetwork. \n\n"}
{"id": "1709.01727", "contents": "Title: Scene Text Recognition with Sliding Convolutional Character Models Abstract: Scene text recognition has attracted great interests from the computer vision\nand pattern recognition community in recent years. State-of-the-art methods use\nconcolutional neural networks (CNNs), recurrent neural networks with long\nshort-term memory (RNN-LSTM) or the combination of them. In this paper, we\ninvestigate the intrinsic characteristics of text recognition, and inspired by\nhuman cognition mechanisms in reading texts, we propose a scene text\nrecognition method with character models on convolutional feature map. The\nmethod simultaneously detects and recognizes characters by sliding the text\nline image with character models, which are learned end-to-end on text line\nimages labeled with text transcripts. The character classifier outputs on the\nsliding windows are normalized and decoded with Connectionist Temporal\nClassification (CTC) based algorithm. Compared to previous methods, our method\nhas a number of appealing properties: (1) It avoids the difficulty of character\nsegmentation which hinders the performance of segmentation-based recognition\nmethods; (2) The model can be trained simply and efficiently because it avoids\ngradient vanishing/exploding in training RNN-LSTM based models; (3) It bases on\ncharacter models trained free of lexicon, and can recognize unknown words. (4)\nThe recognition process is highly parallel and enables fast recognition. Our\nexperiments on several challenging English and Chinese benchmarks, including\nthe IIIT-5K, SVT, ICDAR03/13 and TRW15 datasets, demonstrate that the proposed\nmethod yields superior or comparable performance to state-of-the-art methods\nwhile the model size is relatively small. \n\n"}
{"id": "1709.02153", "contents": "Title: Real-time convolutional networks for sonar image classification in\n  low-power embedded systems Abstract: Deep Neural Networks have impressive classification performance, but this\ncomes at the expense of significant computational resources at inference time.\nAutonomous Underwater Vehicles use low-power embedded systems for sonar image\nperception, and cannot execute large neural networks in real-time. We propose\nthe use of max-pooling aggressively, and we demonstrate it with a Fire-based\nmodule and a new Tiny module that includes max-pooling in each module. By\nstacking them we build networks that achieve the same accuracy as bigger ones,\nwhile reducing the number of parameters and considerably increasing\ncomputational performance. Our networks can classify a 96x96 sonar image with\n98.8 - 99.7 accuracy on only 41 to 61 milliseconds on a Raspberry Pi 2, which\ncorresponds to speedups of 28.6 - 19.7. \n\n"}
{"id": "1709.02482", "contents": "Title: Scalable Annotation of Fine-Grained Categories Without Experts Abstract: We present a crowdsourcing workflow to collect image annotations for visually\nsimilar synthetic categories without requiring experts. In animals, there is a\ndirect link between taxonomy and visual similarity: e.g. a collie (type of dog)\nlooks more similar to other collies (e.g. smooth collie) than a greyhound\n(another type of dog). However, in synthetic categories such as cars, objects\nwith similar taxonomy can have very different appearance: e.g. a 2011 Ford\nF-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very\ndifferent from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph based\ncrowdsourcing algorithm to automatically group visually indistinguishable\nobjects together. Using our workflow, we label 712,430 images by ~1,000 Amazon\nMechanical Turk workers; resulting in the largest fine-grained visual dataset\nreported to date with 2,657 categories of cars annotated at 1/20th the cost of\nhiring experts. \n\n"}
{"id": "1709.03423", "contents": "Title: Ensemble Methods as a Defense to Adversarial Perturbations Against Deep\n  Neural Networks Abstract: Deep learning has become the state of the art approach in many machine\nlearning problems such as classification. It has recently been shown that deep\nlearning is highly vulnerable to adversarial perturbations. Taking the camera\nsystems of self-driving cars as an example, small adversarial perturbations can\ncause the system to make errors in important tasks, such as classifying traffic\nsigns or detecting pedestrians. Hence, in order to use deep learning without\nsafety concerns a proper defense strategy is required. We propose to use\nensemble methods as a defense strategy against adversarial perturbations. We\nfind that an attack leading one model to misclassify does not imply the same\nfor other networks performing the same task. This makes ensemble methods an\nattractive defense strategy against adversarial attacks. We empirically show\nfor the MNIST and the CIFAR-10 data sets that ensemble methods not only improve\nthe accuracy of neural networks on test data but also increase their robustness\nagainst adversarial perturbations. \n\n"}
{"id": "1709.03697", "contents": "Title: Automatic Ground Truths: Projected Image Annotations for Omnidirectional\n  Vision Abstract: We present a novel data set made up of omnidirectional video of multiple\nobjects whose centroid positions are annotated automatically. Omnidirectional\nvision is an active field of research focused on the use of spherical imagery\nin video analysis and scene understanding, involving tasks such as object\ndetection, tracking and recognition. Our goal is to provide a large and\nconsistently annotated video data set that can be used to train and evaluate\nnew algorithms for these tasks. Here we describe the experimental setup and\nsoftware environment used to capture and map the 3D ground truth positions of\nmultiple objects into the image. Furthermore, we estimate the expected\nsystematic error on the mapped positions. In addition to final data products,\nwe release publicly the software tools and raw data necessary to re-calibrate\nthe camera and/or redo this mapping. The software also provides a simple\nframework for comparing the results of standard image annotation tools or\nvisual tracking systems against our mapped ground truth annotations. \n\n"}
{"id": "1709.04329", "contents": "Title: GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval Abstract: The huge variance of human pose and the misalignment of detected human images\nsignificantly increase the difficulty of person Re-Identification (Re-ID).\nMoreover, efficient Re-ID systems are required to cope with the massive visual\ndata being produced by video surveillance systems. Targeting to solve these\nproblems, this work proposes a Global-Local-Alignment Descriptor (GLAD) and an\nefficient indexing and retrieval framework, respectively. GLAD explicitly\nleverages the local and global cues in human body to generate a discriminative\nand robust representation. It consists of part extraction and descriptor\nlearning modules, where several part regions are first detected and then deep\nneural networks are designed for representation learning on both the local and\nglobal regions. A hierarchical indexing and retrieval framework is designed to\neliminate the huge redundancy in the gallery set, and accelerate the online\nRe-ID procedure. Extensive experimental results show GLAD achieves competitive\naccuracy compared to the state-of-the-art methods. Our retrieval framework\nsignificantly accelerates the online Re-ID procedure without loss of accuracy.\nTherefore, this work has potential to work better on person Re-ID tasks in real\nscenarios. \n\n"}
{"id": "1709.04447", "contents": "Title: A Learning and Masking Approach to Secure Learning Abstract: Deep Neural Networks (DNNs) have been shown to be vulnerable against\nadversarial examples, which are data points cleverly constructed to fool the\nclassifier. Such attacks can be devastating in practice, especially as DNNs are\nbeing applied to ever increasing critical tasks like image recognition in\nautonomous driving. In this paper, we introduce a new perspective on the\nproblem. We do so by first defining robustness of a classifier to adversarial\nexploitation. Next, we show that the problem of adversarial example generation\ncan be posed as learning problem. We also categorize attacks in literature into\nhigh and low perturbation attacks; well-known attacks like fast-gradient sign\nmethod (FGSM) and our attack produce higher perturbation adversarial examples\nwhile the more potent but computationally inefficient Carlini-Wagner (CW)\nattack is low perturbation. Next, we show that the dual approach of the attack\nlearning problem can be used as a defensive technique that is effective against\nhigh perturbation attacks. Finally, we show that a classifier masking method\nachieved by adding noise to the a neural network's logit output protects\nagainst low distortion attacks such as the CW attack. We also show that both\nour learning and masking defense can work simultaneously to protect against\nmultiple attacks. We demonstrate the efficacy of our techniques by\nexperimenting with the MNIST and CIFAR-10 datasets. \n\n"}
{"id": "1709.04751", "contents": "Title: From Plants to Landmarks: Time-invariant Plant Localization that uses\n  Deep Pose Regression in Agricultural Fields Abstract: Agricultural robots are expected to increase yields in a sustainable way and\nautomate precision tasks, such as weeding and plant monitoring. At the same\ntime, they move in a continuously changing, semi-structured field environment,\nin which features can hardly be found and reproduced at a later time.\nChallenges for Lidar and visual detection systems stem from the fact that\nplants can be very small, overlapping and have a steadily changing appearance.\nTherefore, a popular way to localize vehicles with high accuracy is based on\nex- pensive global navigation satellite systems and not on natural landmarks.\nThe contribution of this work is a novel image- based plant localization\ntechnique that uses the time-invariant stem emerging point as a reference. Our\napproach is based on a fully convolutional neural network that learns landmark\nlocalization from RGB and NIR image input in an end-to-end manner. The network\nperforms pose regression to generate a plant location likelihood map. Our\napproach allows us to cope with visual variances of plants both for different\nspecies and different growth stages. We achieve high localization accuracies as\nshown in detailed evaluations of a sugar beet cultivation phase. In experiments\nwith our BoniRob we demonstrate that detections can be robustly reproduced with\ncentimeter accuracy. \n\n"}
{"id": "1709.05107", "contents": "Title: Multi-Label Zero-Shot Human Action Recognition via Joint Latent Ranking\n  Embedding Abstract: Human action recognition refers to automatic recognizing human actions from a\nvideo clip. In reality, there often exist multiple human actions in a video\nstream. Such a video stream is often weakly-annotated with a set of relevant\nhuman action labels at a global level rather than assigning each label to a\nspecific video episode corresponding to a single action, which leads to a\nmulti-label learning problem. Furthermore, there are many meaningful human\nactions in reality but it would be extremely difficult to collect/annotate\nvideo clips regarding all of various human actions, which leads to a zero-shot\nlearning scenario. To the best of our knowledge, there is no work that has\naddressed all the above issues together in human action recognition. In this\npaper, we formulate a real-world human action recognition task as a multi-label\nzero-shot learning problem and propose a framework to tackle this problem in a\nholistic way. Our framework holistically tackles the issue of unknown temporal\nboundaries between different actions for multi-label learning and exploits the\nside information regarding the semantic relationship between different human\nactions for knowledge transfer. Consequently, our framework leads to a joint\nlatent ranking embedding for multi-label zero-shot human action recognition. A\nnovel neural architecture of two component models and an alternate learning\nalgorithm are proposed to carry out the joint latent ranking embedding\nlearning. Thus, multi-label zero-shot recognition is done by measuring\nrelatedness scores of action labels to a test video clip in the joint latent\nvisual and semantic embedding spaces. We evaluate our framework with different\nsettings, including a novel data split scheme designed especially for\nevaluating multi-label zero-shot learning, on two datasets: Breakfast and\nCharades. The experimental results demonstrate the effectiveness of our\nframework. \n\n"}
{"id": "1709.06126", "contents": "Title: How intelligent are convolutional neural networks? Abstract: Motivated by the Gestalt pattern theory, and the Winograd Challenge for\nlanguage understanding, we design synthetic experiments to investigate a deep\nlearning algorithm's ability to infer simple (at least for human) visual\nconcepts, such as symmetry, from examples. A visual concept is represented by\nrandomly generated, positive as well as negative, example images. We then test\nthe ability and speed of algorithms (and humans) to learn the concept from\nthese images. The training and testing are performed progressively in multiple\nrounds, with each subsequent round deliberately designed to be more complex and\nconfusing than the previous round(s), especially if the concept was not grasped\nby the learner. However, if the concept was understood, all the deliberate\ntests would become trivially easy. Our experiments show that humans can often\ninfer a semantic concept quickly after looking at only a very small number of\nexamples (this is often referred to as an \"aha moment\": a moment of sudden\nrealization), and performs perfectly during all testing rounds (except for\ncareless mistakes). On the contrary, deep convolutional neural networks (DCNN)\ncould approximate some concepts statistically, but only after seeing many\n(x10^4) more examples. And it will still make obvious mistakes, especially\nduring deliberate testing rounds or on samples outside the training\ndistributions. This signals a lack of true \"understanding\", or a failure to\nreach the right \"formula\" for the semantics. We did find that some concepts are\neasier for DCNN than others. For example, simple \"counting\" is more learnable\nthan \"symmetry\", while \"uniformity\" or \"conformance\" are much more difficult\nfor DCNN to learn. To conclude, we propose an \"Aha Challenge\" for visual\nperception, calling for focused and quantitative research on Gestalt-style\nmachine intelligence using limited training examples. \n\n"}
{"id": "1709.06158", "contents": "Title: Matterport3D: Learning from RGB-D Data in Indoor Environments Abstract: Access to large, diverse RGB-D datasets is critical for training RGB-D scene\nunderstanding algorithms. However, existing datasets still cover only a limited\nnumber of views or a restricted scale of spaces. In this paper, we introduce\nMatterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views\nfrom 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided\nwith surface reconstructions, camera poses, and 2D and 3D semantic\nsegmentations. The precise global alignment and comprehensive, diverse\npanoramic set of views over entire buildings enable a variety of supervised and\nself-supervised computer vision tasks, including keypoint matching, view\noverlap prediction, normal prediction from color, semantic segmentation, and\nregion classification. \n\n"}
{"id": "1709.06391", "contents": "Title: Human Action Forecasting by Learning Task Grammars Abstract: For effective human-robot interaction, it is important that a robotic\nassistant can forecast the next action a human will consider in a given task.\nUnfortunately, real-world tasks are often very long, complex, and repetitive;\nas a result forecasting is not trivial. In this paper, we propose a novel deep\nrecurrent architecture that takes as input features from a two-stream Residual\naction recognition framework, and learns to estimate the progress of human\nactivities from video sequences -- this surrogate progress estimation task\nimplicitly learns a temporal task grammar with respect to which activities can\nbe localized and forecasted. To learn the task grammar, we propose a stacked\nLSTM based multi-granularity progress estimation framework that uses a novel\ncumulative Euclidean loss as objective. To demonstrate the effectiveness of our\nproposed architecture, we showcase experiments on two challenging robotic\nassistive tasks, namely (i) assembling an Ikea table from its constituents, and\n(ii) changing the tires of a car. Our results demonstrate that learning task\ngrammars offers highly discriminative cues improving the forecasting accuracy\nby more than 9% over the baseline two-stream forecasting model, while also\noutperforming other competitive schemes. \n\n"}
{"id": "1709.07359", "contents": "Title: Class-Splitting Generative Adversarial Networks Abstract: Generative Adversarial Networks (GANs) produce systematically better quality\nsamples when class label information is provided., i.e. in the conditional GAN\nsetup. This is still observed for the recently proposed Wasserstein GAN\nformulation which stabilized adversarial training and allows considering high\ncapacity network architectures such as ResNet. In this work we show how to\nboost conditional GAN by augmenting available class labels. The new classes\ncome from clustering in the representation space learned by the same GAN model.\nThe proposed strategy is also feasible when no class information is available,\ni.e. in the unsupervised setup. Our generated samples reach state-of-the-art\nInception scores for CIFAR-10 and STL-10 datasets in both supervised and\nunsupervised setup. \n\n"}
{"id": "1709.07584", "contents": "Title: Happy Travelers Take Big Pictures: A Psychological Study with Machine\n  Learning and Big Data Abstract: In psychology, theory-driven researches are usually conducted with extensive\nlaboratory experiments, yet rarely tested or disproved with big data. In this\npaper, we make use of 418K travel photos with traveler ratings to test the\ninfluential \"broaden-and-build\" theory, that suggests positive emotions broaden\none's visual attention. The core hypothesis examined in this study is that\npositive emotion is associated with a wider attention, hence highly-rated sites\nwould trigger wide-angle photographs. By analyzing travel photos, we find a\nstrong correlation between a preference for wide-angle photos and the high\nrating of tourist sites on TripAdvisor. We are able to carry out this analysis\nthrough the use of deep learning algorithms to classify the photos into wide\nand narrow angles, and present this study as an exemplar of how big data and\ndeep learning can be used to test laboratory findings in the wild. \n\n"}
{"id": "1709.08524", "contents": "Title: Generative learning for deep networks Abstract: Learning, taking into account full distribution of the data, referred to as\ngenerative, is not feasible with deep neural networks (DNNs) because they model\nonly the conditional distribution of the outputs given the inputs. Current\nsolutions are either based on joint probability models facing difficult\nestimation problems or learn two separate networks, mapping inputs to outputs\n(recognition) and vice-versa (generation). We propose an intermediate approach.\nFirst, we show that forward computation in DNNs with logistic sigmoid\nactivations corresponds to a simplified approximate Bayesian inference in a\ndirected probabilistic multi-layer model. This connection allows to interpret\nDNN as a probabilistic model of the output and all hidden units given the\ninput. Second, we propose that in order for the recognition and generation\nnetworks to be more consistent with the joint model of the data, weights of the\nrecognition and generator network should be related by transposition. We\ndemonstrate in a tentative experiment that such a coupled pair can be learned\ngeneratively, modelling the full distribution of the data, and has enough\ncapacity to perform well in both recognition and generation. \n\n"}
{"id": "1709.08527", "contents": "Title: Multi-view pose estimation with mixtures-of-parts and adaptive viewpoint\n  selection Abstract: We propose a new method for human pose estimation which leverages information\nfrom multiple views to impose a strong prior on articulated pose. The novelty\nof the method concerns the types of coherence modelled. Consistency is\nmaximised over the different views through different terms modelling classical\ngeometric information (coherence of the resulting poses) as well as appearance\ninformation which is modelled as latent variables in the global energy\nfunction. Moreover, adequacy of each view is assessed and their contributions\nare adjusted accordingly. Experiments on the HumanEva and UMPM datasets show\nthat the proposed method significantly decreases the estimation error compared\nto single-view results. \n\n"}
{"id": "1709.09582", "contents": "Title: Connectivity Learning in Multi-Branch Networks Abstract: While much of the work in the design of convolutional networks over the last\nfive years has revolved around the empirical investigation of the importance of\ndepth, filter sizes, and number of feature channels, recent studies have shown\nthat branching, i.e., splitting the computation along parallel but distinct\nthreads and then aggregating their outputs, represents a new promising\ndimension for significant improvements in performance. To combat the complexity\nof design choices in multi-branch architectures, prior work has adopted simple\nstrategies, such as a fixed branching factor, the same input being fed to all\nparallel branches, and an additive combination of the outputs produced by all\nbranches at aggregation points.\n  In this work we remove these predefined choices and propose an algorithm to\nlearn the connections between branches in the network. Instead of being chosen\na priori by the human designer, the multi-branch connectivity is learned\nsimultaneously with the weights of the network by optimizing a single loss\nfunction defined with respect to the end task. We demonstrate our approach on\nthe problem of multi-class image classification using three different datasets\nwhere it yields consistently higher accuracy compared to the state-of-the-art\n\"ResNeXt\" multi-branch network given the same learning capacity. \n\n"}
{"id": "1710.02081", "contents": "Title: Online Photometric Calibration for Auto Exposure Video for Realtime\n  Visual Odometry and SLAM Abstract: Recent direct visual odometry and SLAM algorithms have demonstrated\nimpressive levels of precision. However, they require a photometric camera\ncalibration in order to achieve competitive results. Hence, the respective\nalgorithm cannot be directly applied to an off-the-shelf-camera or to a video\nsequence acquired with an unknown camera. In this work we propose a method for\nonline photometric calibration which enables to process auto exposure videos\nwith visual odometry precisions that are on par with those of photometrically\ncalibrated videos. Our algorithm recovers the exposure times of consecutive\nframes, the camera response function, and the attenuation factors of the sensor\nirradiance due to vignetting. Gain robust KLT feature tracks are used to obtain\nscene point correspondences as input to a nonlinear optimization framework. We\nshow that our approach can reliably calibrate arbitrary video sequences by\nevaluating it on datasets for which full photometric ground truth is available.\nWe further show that our calibration can improve the performance of a\nstate-of-the-art direct visual odometry method that works solely on pixel\nintensities, calibrating for photometric parameters in an online fashion in\nrealtime. \n\n"}
{"id": "1710.02286", "contents": "Title: Deep Convolutional Neural Networks as Generic Feature Extractors Abstract: Recognizing objects in natural images is an intricate problem involving\nmultiple conflicting objectives. Deep convolutional neural networks, trained on\nlarge datasets, achieve convincing results and are currently the\nstate-of-the-art approach for this task. However, the long time needed to train\nsuch deep networks is a major drawback. We tackled this problem by reusing a\npreviously trained network. For this purpose, we first trained a deep\nconvolutional network on the ILSVRC2012 dataset. We then maintained the learned\nconvolution kernels and only retrained the classification part on different\ndatasets. Using this approach, we achieved an accuracy of 67.68 % on CIFAR-100,\ncompared to the previous state-of-the-art result of 65.43 %. Furthermore, our\nfindings indicate that convolutional networks are able to learn generic feature\nextractors that can be used for different tasks. \n\n"}
{"id": "1710.02410", "contents": "Title: End-to-end Driving via Conditional Imitation Learning Abstract: Deep networks trained on demonstrations of human driving have learned to\nfollow roads and avoid obstacles. However, driving policies trained via\nimitation learning cannot be controlled at test time. A vehicle trained\nend-to-end to imitate an expert cannot be guided to take a specific turn at an\nupcoming intersection. This limits the utility of such systems. We propose to\ncondition imitation learning on high-level command input. At test time, the\nlearned driving policy functions as a chauffeur that handles sensorimotor\ncoordination but continues to respond to navigational commands. We evaluate\ndifferent architectures for conditional imitation learning in vision-based\ndriving. We conduct experiments in realistic three-dimensional simulations of\nurban driving and on a 1/5 scale robotic truck that is trained to drive in a\nresidential area. Both systems drive based on visual input yet remain\nresponsive to high-level navigational commands. The supplementary video can be\nviewed at https://youtu.be/cFtnflNe5fM \n\n"}
{"id": "1710.02820", "contents": "Title: Micro-Expression Spotting: A Benchmark Abstract: Micro-expressions are rapid and involuntary facial expressions, which\nindicate the suppressed or concealed emotions. Recently, the research on\nautomatic micro-expression (ME) spotting obtains increasing attention. ME\nspotting is a crucial step prior to further ME analysis tasks. The spotting\nresults can be used as important cues to assist many other human-oriented tasks\nand thus have many potential applications. In this paper, by investigating\nexisting ME spotting methods, we recognize the immediacy of standardizing the\nperformance evaluation of micro-expression spotting methods. To this end, we\nconstruct a micro-expression spotting benchmark (MESB). Firstly, we set up a\nsliding window based multi-scale evaluation framework. Secondly, we introduce a\nseries of protocols. Thirdly, we also provide baseline results of popular\nmethods. The MESB facilitates the research on ME spotting with fairer and more\ncomprehensive evaluation and also enables to leverage the cutting-edge machine\nlearning tools widely. \n\n"}
{"id": "1710.03337", "contents": "Title: Standard detectors aren't (currently) fooled by physical adversarial\n  stop signs Abstract: An adversarial example is an example that has been adjusted to produce the\nwrong label when presented to a system at test time. If adversarial examples\nexisted that could fool a detector, they could be used to (for example) wreak\nhavoc on roads populated with smart vehicles. Recently, we described our\ndifficulties creating physical adversarial stop signs that fool a detector.\nMore recently, Evtimov et al. produced a physical adversarial stop sign that\nfools a proxy model of a detector. In this paper, we show that these physical\nadversarial stop signs do not fool two standard detectors (YOLO and Faster\nRCNN) in standard configuration. Evtimov et al.'s construction relies on a crop\nof the image to the stop sign; this crop is then resized and presented to a\nclassifier. We argue that the cropping and resizing procedure largely\neliminates the effects of rescaling and of view angle. Whether an adversarial\nattack is robust under rescaling and change of view direction remains moot. We\nargue that attacking a classifier is very different from attacking a detector,\nand that the structure of detectors - which must search for their own bounding\nbox, and which cannot estimate that box very accurately - likely makes it\ndifficult to make adversarial patterns. Finally, an adversarial pattern on a\nphysical object that could fool a detector would have to be adversarial in the\nface of a wide family of parametric distortions (scale; view angle; box shift\ninside the detector; illumination; and so on). Such a pattern would be of great\ntheoretical and practical interest. There is currently no evidence that such\npatterns exist. \n\n"}
{"id": "1710.03811", "contents": "Title: DeepSolarEye: Power Loss Prediction and Weakly Supervised Soiling\n  Localization via Fully Convolutional Networks for Solar Panels Abstract: The impact of soiling on solar panels is an important and well-studied\nproblem in renewable energy sector. In this paper, we present the first\nconvolutional neural network (CNN) based approach for solar panel soiling and\ndefect analysis. Our approach takes an RGB image of solar panel and\nenvironmental factors as inputs to predict power loss, soiling localization,\nand soiling type. In computer vision, localization is a complex task which\ntypically requires manually labeled training data such as bounding boxes or\nsegmentation masks. Our proposed approach consists of specialized four stages\nwhich completely avoids localization ground truth and only needs panel images\nwith power loss labels for training. The region of impact area obtained from\nthe predicted localization masks are classified into soiling types using the\nwebly supervised learning. For improving localization capabilities of CNNs, we\nintroduce a novel bi-directional input-aware fusion (BiDIAF) block that\nreinforces the input at different levels of CNN to learn input-specific feature\nmaps. Our empirical study shows that BiDIAF improves the power loss prediction\naccuracy by about 3% and localization accuracy by about 4%. Our end-to-end\nmodel yields further improvement of about 24% on localization when learned in a\nweakly supervised manner. Our approach is generalizable and showed promising\nresults on web crawled solar panel images. Our system has a frame rate of 22\nfps (including all steps) on a NVIDIA TitanX GPU. Additionally, we collected\nfirst of it's kind dataset for solar panel image analysis consisting 45,000+\nimages. \n\n"}
{"id": "1710.04540", "contents": "Title: Hierarchical Convolutional-Deconvolutional Neural Networks for Automatic\n  Liver and Tumor Segmentation Abstract: Automatic segmentation of liver and its tumors is an essential step for\nextracting quantitative imaging biomarkers for accurate tumor detection,\ndiagnosis, prognosis and assessment of tumor response to treatment. MICCAI 2017\nLiver Tumor Segmentation Challenge (LiTS) provides a common platform for\ncomparing different automatic algorithms on contrast-enhanced abdominal CT\nimages in tasks including 1) liver segmentation, 2) liver tumor segmentation,\nand 3) tumor burden estimation. We participate this challenge by developing a\nhierarchical framework based on deep fully convolutional-deconvolutional neural\nnetworks (CDNN). A simple CDNN model is firstly trained to provide a quick but\ncoarse segmentation of the liver on the entire CT volume, then another CDNN is\napplied to the liver region for fine liver segmentation. At last, the segmented\nliver region, which is enhanced by histogram equalization, is employed as an\nadditional input to the third CDNN for tumor segmentation. Jaccard distance is\nused as loss function when training CDNN models to eliminate the need of sample\nre-weighting. Our framework is trained using the 130 challenge training cases\nprovided by LiTS. The evaluation on the 70 challenge testing cases resulted in\na mean Dice Similarity Coefficient (DSC) of 0.963 for liver segmentation, a\nmean DSC of 0.657 for tumor segmentation, and a root mean square error (RMSE)\nof 0.017 for tumor burden estimation, which ranked our method in the first,\nfifth and third place, respectively \n\n"}
{"id": "1710.05172", "contents": "Title: Co-saliency Detection for RGBD Images Based on Multi-constraint Feature\n  Matching and Cross Label Propagation Abstract: Co-saliency detection aims at extracting the common salient regions from an\nimage group containing two or more relevant images. It is a newly emerging\ntopic in computer vision community. Different from the most existing\nco-saliency methods focusing on RGB images, this paper proposes a novel\nco-saliency detection model for RGBD images, which utilizes the depth\ninformation to enhance identification of co-saliency. First, the intra saliency\nmap for each image is generated by the single image saliency model, while the\ninter saliency map is calculated based on the multi-constraint feature\nmatching, which represents the constraint relationship among multiple images.\nThen, the optimization scheme, namely Cross Label Propagation (CLP), is used to\nrefine the intra and inter saliency maps in a cross way. Finally, all the\noriginal and optimized saliency maps are integrated to generate the final\nco-saliency result. The proposed method introduces the depth information and\nmulti-constraint feature matching to improve the performance of co-saliency\ndetection. Moreover, the proposed method can effectively exploit any existing\nsingle image saliency model to work well in co-saliency scenarios. Experiments\non two RGBD co-saliency datasets demonstrate the effectiveness of our proposed\nmodel. \n\n"}
{"id": "1710.05179", "contents": "Title: Regularizing Deep Neural Networks by Noise: Its Interpretation and\n  Optimization Abstract: Overfitting is one of the most critical challenges in deep neural networks,\nand there are various types of regularization methods to improve generalization\nperformance. Injecting noises to hidden units during training, e.g., dropout,\nis known as a successful regularizer, but it is still not clear enough why such\ntraining techniques work well in practice and how we can maximize their benefit\nin the presence of two conflicting objectives---optimizing to true data\ndistribution and preventing overfitting by regularization. This paper addresses\nthe above issues by 1) interpreting that the conventional training methods with\nregularization by noise injection optimize the lower bound of the true\nobjective and 2) proposing a technique to achieve a tighter lower bound using\nmultiple noise samples per training example in a stochastic gradient descent\niteration. We demonstrate the effectiveness of our idea in several computer\nvision applications. \n\n"}
{"id": "1710.05241", "contents": "Title: Robust Decentralized Learning Using ADMM with Unreliable Agents Abstract: Many machine learning problems can be formulated as consensus optimization\nproblems which can be solved efficiently via a cooperative multi-agent system.\nHowever, the agents in the system can be unreliable due to a variety of\nreasons: noise, faults and attacks. Providing erroneous updates leads the\noptimization process in a wrong direction, and degrades the performance of\ndistributed machine learning algorithms. This paper considers the problem of\ndecentralized learning using ADMM in the presence of unreliable agents. First,\nwe rigorously analyze the effect of erroneous updates (in ADMM learning\niterations) on the convergence behavior of multi-agent system. We show that the\nalgorithm linearly converges to a neighborhood of the optimal solution under\ncertain conditions and characterize the neighborhood size analytically. Next,\nwe provide guidelines for network design to achieve a faster convergence. We\nalso provide conditions on the erroneous updates for exact convergence to the\noptimal solution. Finally, to mitigate the influence of unreliable agents, we\npropose \\textsf{ROAD}, a robust variant of ADMM, and show its resilience to\nunreliable agents with an exact convergence to the optimum. \n\n"}
{"id": "1710.05379", "contents": "Title: Towards Automatic Abdominal Multi-Organ Segmentation in Dual Energy CT\n  using Cascaded 3D Fully Convolutional Network Abstract: Automatic multi-organ segmentation of the dual energy computed tomography\n(DECT) data can be beneficial for biomedical research and clinical\napplications. However, it is a challenging task. Recent advances in deep\nlearning showed the feasibility to use 3-D fully convolutional networks (FCN)\nfor voxel-wise dense predictions in single energy computed tomography (SECT).\nIn this paper, we proposed a 3D FCN based method for automatic multi-organ\nsegmentation in DECT. The work was based on a cascaded FCN and a general model\nfor the major organs trained on a large set of SECT data. We preprocessed the\nDECT data by using linear weighting and fine-tuned the model for the DECT data.\nThe method was evaluated using 42 torso DECT data acquired with a clinical\ndual-source CT system. Four abdominal organs (liver, spleen, left and right\nkidneys) were evaluated. Cross-validation was tested. Effect of the weight on\nthe accuracy was researched. In all the tests, we achieved an average Dice\ncoefficient of 93% for the liver, 90% for the spleen, 91% for the right kidney\nand 89% for the left kidney, respectively. The results show our method is\nfeasible and promising. \n\n"}
{"id": "1710.05719", "contents": "Title: Lung Cancer Screening Using Adaptive Memory-Augmented Recurrent Networks Abstract: In this paper, we investigate the effectiveness of deep learning techniques\nfor lung nodule classification in computed tomography scans. Using less than\n10,000 training examples, our deep networks perform two times better than a\nstandard radiology software. Visualization of the networks' neurons reveals\nsemantically meaningful features that are consistent with the clinical\nknowledge and radiologists' perception. Our paper also proposes a novel\nframework for rapidly adapting deep networks to the radiologists' feedback, or\nchange in the data due to the shift in sensor's resolution or patient\npopulation. The classification accuracy of our approach remains above 80% while\npopular deep networks' accuracy is around chance. Finally, we provide in-depth\nanalysis of our framework by asking a radiologist to examine important\nnetworks' features and perform blind re-labeling of networks' mistakes. \n\n"}
{"id": "1710.05956", "contents": "Title: Isointense Infant Brain Segmentation with a Hyper-dense Connected\n  Convolutional Neural Network Abstract: Neonatal brain segmentation in magnetic resonance (MR) is a challenging\nproblem due to poor image quality and low contrast between white and gray\nmatter regions. Most existing approaches for this problem are based on\nmulti-atlas label fusion strategies, which are time-consuming and sensitive to\nregistration errors. As alternative to these methods, we propose a\nhyper-densely connected 3D convolutional neural network that employs MR-T1 and\nT2 images as input, which are processed independently in two separated paths.\nAn important difference with previous densely connected networks is the use of\ndirect connections between layers from the same and different paths. Adopting\nsuch dense connectivity helps the learning process by including deep\nsupervision and improving gradient flow. We evaluated our approach on data from\nthe MICCAI Grand Challenge on 6-month infant Brain MRI Segmentation (iSEG),\nobtaining very competitive results. Among 21 teams, our approach ranked first\nor second in most metrics, translating into a state-of-the-art performance. \n\n"}
{"id": "1710.05958", "contents": "Title: Gradient-free Policy Architecture Search and Adaptation Abstract: We develop a method for policy architecture search and adaptation via\ngradient-free optimization which can learn to perform autonomous driving tasks.\nBy learning from both demonstration and environmental reward we develop a model\nthat can learn with relatively few early catastrophic failures. We first learn\nan architecture of appropriate complexity to perceive aspects of world state\nrelevant to the expert demonstration, and then mitigate the effect of\ndomain-shift during deployment by adapting a policy demonstrated in a source\ndomain to rewards obtained in a target environment. We show that our approach\nallows safer learning than baseline methods, offering a reduced cumulative\ncrash metric over the agent's lifetime as it learns to drive in a realistic\nsimulated environment. \n\n"}
{"id": "1710.06096", "contents": "Title: Spontaneous Symmetry Breaking in Neural Networks Abstract: We propose a framework to understand the unprecedented performance and\nrobustness of deep neural networks using field theory. Correlations between the\nweights within the same layer can be described by symmetries in that layer, and\nnetworks generalize better if such symmetries are broken to reduce the\nredundancies of the weights. Using a two parameter field theory, we find that\nthe network can break such symmetries itself towards the end of training in a\nprocess commonly known in physics as spontaneous symmetry breaking. This\ncorresponds to a network generalizing itself without any user input layers to\nbreak the symmetry, but by communication with adjacent layers. In the layer\ndecoupling limit applicable to residual networks (He et al., 2015), we show\nthat the remnant symmetries that survive the non-linear layers are\nspontaneously broken. The Lagrangian for the non-linear and weight layers\ntogether has striking similarities with the one in quantum field theory of a\nscalar. Using results from quantum field theory we show that our framework is\nable to explain many experimentally observed phenomena,such as training on\nrandom labels with zero error (Zhang et al., 2017), the information bottleneck,\nthe phase transition out of it and gradient variance explosion (Shwartz-Ziv &\nTishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more. \n\n"}
{"id": "1710.07477", "contents": "Title: Anticipating Daily Intention using On-Wrist Motion Triggered Sensing Abstract: Anticipating human intention by observing one's actions has many\napplications. For instance, picking up a cellphone, then a charger (actions)\nimplies that one wants to charge the cellphone (intention). By anticipating the\nintention, an intelligent system can guide the user to the closest power\noutlet. We propose an on-wrist motion triggered sensing system for anticipating\ndaily intentions, where the on-wrist sensors help us to persistently observe\none's actions. The core of the system is a novel Recurrent Neural Network (RNN)\nand Policy Network (PN), where the RNN encodes visual and motion observation to\nanticipate intention, and the PN parsimoniously triggers the process of visual\nobservation to reduce computation requirement. We jointly trained the whole\nnetwork using policy gradient and cross-entropy loss. To evaluate, we collect\nthe first daily \"intention\" dataset consisting of 2379 videos with 34\nintentions and 164 unique action sequences. Our method achieves 92.68%, 90.85%,\n97.56% accuracy on three users while processing only 29% of the visual\nobservation on average. \n\n"}
{"id": "1710.08177", "contents": "Title: Progressive Learning for Systematic Design of Large Neural Networks Abstract: We develop an algorithm for systematic design of a large artificial neural\nnetwork using a progression property. We find that some non-linear functions,\nsuch as the rectifier linear unit and its derivatives, hold the property. The\nsystematic design addresses the choice of network size and regularization of\nparameters. The number of nodes and layers in network increases in progression\nwith the objective of consistently reducing an appropriate cost. Each layer is\noptimized at a time, where appropriate parameters are learned using convex\noptimization. Regularization parameters for convex optimization do not need a\nsignificant manual effort for tuning. We also use random instances for some\nweight matrices, and that helps to reduce the number of parameters we learn.\nThe developed network is expected to show good generalization power due to\nappropriate regularization and use of random weights in the layers. This\nexpectation is verified by extensive experiments for classification and\nregression problems, using standard databases. \n\n"}
{"id": "1710.08299", "contents": "Title: An In-field Automatic Wheat Disease Diagnosis System Abstract: Crop diseases are responsible for the major production reduction and economic\nlosses in agricultural industry world- wide. Monitoring for health status of\ncrops is critical to control the spread of diseases and implement effective\nmanagement. This paper presents an in-field automatic wheat disease diagnosis\nsystem based on a weakly super- vised deep learning framework, i.e. deep\nmultiple instance learning, which achieves an integration of identification for\nwheat diseases and localization for disease areas with only image-level\nannotation for training images in wild conditions. Furthermore, a new in-field\nimage dataset for wheat disease, Wheat Disease Database 2017 (WDD2017), is\ncollected to verify the effectiveness of our system. Under two different\narchitectures, i.e. VGG-FCN-VD16 and VGG-FCN-S, our system achieves the mean\nrecognition accuracies of 97.95% and 95.12% respectively over 5-fold\ncross-validation on WDD2017, exceeding the results of 93.27% and 73.00% by two\nconventional CNN frameworks, i.e. VGG-CNN-VD16 and VGG-CNN-S. Experimental\nresults demonstrate that the proposed system outperforms conventional CNN\narchitectures on recognition accuracy under the same amount of parameters,\nmeanwhile main- taining accurate localization for corresponding disease areas.\nMoreover, the proposed system has been packed into a real-time mobile app to\nprovide support for agricultural disease diagnosis. \n\n"}
{"id": "1710.08864", "contents": "Title: One pixel attack for fooling deep neural networks Abstract: Recent research has revealed that the output of Deep Neural Networks (DNN)\ncan be easily altered by adding relatively small perturbations to the input\nvector. In this paper, we analyze an attack in an extremely limited scenario\nwhere only one pixel can be modified. For that we propose a novel method for\ngenerating one-pixel adversarial perturbations based on differential evolution\n(DE). It requires less adversarial information (a black-box attack) and can\nfool more types of networks due to the inherent features of DE. The results\nshow that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and\n16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least\none target class by modifying just one pixel with 74.03% and 22.91% confidence\non average. We also show the same vulnerability on the original CIFAR-10\ndataset. Thus, the proposed attack explores a different take on adversarial\nmachine learning in an extreme limited scenario, showing that current DNNs are\nalso vulnerable to such low dimension attacks. Besides, we also illustrate an\nimportant application of DE (or broadly speaking, evolutionary computation) in\nthe domain of adversarial machine learning: creating tools that can effectively\ngenerate low-cost adversarial attacks against neural networks for evaluating\nrobustness. \n\n"}
{"id": "1710.09008", "contents": "Title: The Shape of an Image: A Study of Mapper on Images Abstract: We study the topological construction called Mapper in the context of simply\nconnected domains, in particular on images. The Mapper construction can be\nconsidered as a generalization for contour, split, and joint trees on simply\nconnected domains. A contour tree on an image domain assumes the height\nfunction to be a piecewise linear Morse function. This is a rather restrictive\nclass of functions and does not allow us to explore the topology for most real\nworld images. The Mapper construction avoids this limitation by assuming only\ncontinuity on the height function allowing this construction to robustly deal\nwith a significant larger set of images. We provide a customized construction\nfor Mapper on images, give a fast algorithm to compute it, and show how to\nsimplify the Mapper structure in this case. Finally, we provide a simple\nprocedure that guarantees the equivalence of Mapper to contour, join, and split\ntrees on a simply connected domain. \n\n"}
{"id": "1710.09230", "contents": "Title: Supervised Classification: Quite a Brief Overview Abstract: The original problem of supervised classification considers the task of\nautomatically assigning objects to their respective classes on the basis of\nnumerical measurements derived from these objects. Classifiers are the tools\nthat implement the actual functional mapping from these measurements---also\ncalled features or inputs---to the so-called class label---or output. The\nfields of pattern recognition and machine learning study ways of constructing\nsuch classifiers. The main idea behind supervised methods is that of learning\nfrom examples: given a number of example input-output relations, to what extent\ncan the general mapping be learned that takes any new and unseen feature vector\nto its correct class? This chapter provides a basic introduction to the\nunderlying ideas of how to come to a supervised classification problem. In\naddition, it provides an overview of some specific classification techniques,\ndelves into the issues of object representation and classifier evaluation, and\n(very) briefly covers some variations on the basic supervised classification\ntask that may also be of interest to the practitioner. \n\n"}
{"id": "1710.10182", "contents": "Title: High-Quality Facial Photo-Sketch Synthesis Using Multi-Adversarial\n  Networks Abstract: Synthesizing face sketches from real photos and its inverse have many\napplications. However, photo/sketch synthesis remains a challenging problem due\nto the fact that photo and sketch have different characteristics. In this work,\nwe consider this task as an image-to-image translation problem and explore the\nrecently popular generative models (GANs) to generate high-quality realistic\nphotos from sketches and sketches from photos. Recent GAN-based methods have\nshown promising results on image-to-image translation problems and\nphoto-to-sketch synthesis in particular, however, they are known to have\nlimited abilities in generating high-resolution realistic images. To this end,\nwe propose a novel synthesis framework called Photo-Sketch Synthesis using\nMulti-Adversarial Networks, (PS2-MAN) that iteratively generates low resolution\nto high resolution images in an adversarial way. The hidden layers of the\ngenerator are supervised to first generate lower resolution images followed by\nimplicit refinement in the network to generate higher resolution images.\nFurthermore, since photo-sketch synthesis is a coupled/paired translation\nproblem, we leverage the pair information using CycleGAN framework. Both Image\nQuality Assessment (IQA) and Photo-Sketch Matching experiments are conducted to\ndemonstrate the superior performance of our framework in comparison to existing\nstate-of-the-art solutions. Code available at:\nhttps://github.com/lidan1/PhotoSketchMAN. \n\n"}
{"id": "1710.10403", "contents": "Title: Trainable back-propagated functional transfer matrices Abstract: Connections between nodes of fully connected neural networks are usually\nrepresented by weight matrices. In this article, functional transfer matrices\nare introduced as alternatives to the weight matrices: Instead of using real\nweights, a functional transfer matrix uses real functions with trainable\nparameters to represent connections between nodes. Multiple functional transfer\nmatrices are then stacked together with bias vectors and activations to form\ndeep functional transfer neural networks. These neural networks can be trained\nwithin the framework of back-propagation, based on a revision of the delta\nrules and the error transmission rule for functional connections. In\nexperiments, it is demonstrated that the revised rules can be used to train a\nrange of functional connections: 20 different functions are applied to neural\nnetworks with up to 10 hidden layers, and most of them gain high test\naccuracies on the MNIST database. It is also demonstrated that a functional\ntransfer matrix with a memory function can roughly memorise a non-cyclical\nsequence of 400 digits. \n\n"}
{"id": "1710.10898", "contents": "Title: Learning to solve inverse problems using Wasserstein loss Abstract: We propose using the Wasserstein loss for training in inverse problems. In\nparticular, we consider a learned primal-dual reconstruction scheme for\nill-posed inverse problems using the Wasserstein distance as loss function in\nthe learning. This is motivated by miss-alignments in training data, which when\nusing standard mean squared error loss could severely degrade reconstruction\nquality. We prove that training with the Wasserstein loss gives a\nreconstruction operator that correctly compensates for miss-alignments in\ncertain cases, whereas training with the mean squared error gives a smeared\nreconstruction. Moreover, we demonstrate these effects by training a\nreconstruction algorithm using both mean squared error and optimal transport\nloss for a problem in computerized tomography. \n\n"}
{"id": "1710.11473", "contents": "Title: Multi-Resolution Fully Convolutional Neural Networks for Monaural Audio\n  Source Separation Abstract: In deep neural networks with convolutional layers, each layer typically has\nfixed-size/single-resolution receptive field (RF). Convolutional layers with a\nlarge RF capture global information from the input features, while layers with\nsmall RF size capture local details with high resolution from the input\nfeatures. In this work, we introduce novel deep multi-resolution fully\nconvolutional neural networks (MR-FCNN), where each layer has different RF\nsizes to extract multi-resolution features that capture the global and local\ndetails information from its input features. The proposed MR-FCNN is applied to\nseparate a target audio source from a mixture of many audio sources.\nExperimental results show that using MR-FCNN improves the performance compared\nto feedforward deep neural networks (DNNs) and single resolution deep fully\nconvolutional neural networks (FCNNs) on the audio source separation problem. \n\n"}
{"id": "1711.00003", "contents": "Title: Common Representation Learning Using Step-based Correlation Multi-Modal\n  CNN Abstract: Deep learning techniques have been successfully used in learning a common\nrepresentation for multi-view data, wherein the different modalities are\nprojected onto a common subspace. In a broader perspective, the techniques used\nto investigate common representation learning falls under the categories of\ncanonical correlation-based approaches and autoencoder based approaches. In\nthis paper, we investigate the performance of deep autoencoder based methods on\nmulti-view data. We propose a novel step-based correlation multi-modal CNN\n(CorrMCNN) which reconstructs one view of the data given the other while\nincreasing the interaction between the representations at each hidden layer or\nevery intermediate step. Finally, we evaluate the performance of the proposed\nmodel on two benchmark datasets - MNIST and XRMB. Through extensive\nexperiments, we find that the proposed model achieves better performance than\nthe current state-of-the-art techniques on joint common representation learning\nand transfer learning tasks. \n\n"}
{"id": "1711.00848", "contents": "Title: Variational Inference of Disentangled Latent Concepts from Unlabeled\n  Observations Abstract: Disentangled representations, where the higher level data generative factors\nare reflected in disjoint latent dimensions, offer several benefits such as\nease of deriving invariant representations, transferability to other tasks,\ninterpretability, etc. We consider the problem of unsupervised learning of\ndisentangled representations from large pool of unlabeled observations, and\npropose a variational inference based approach to infer disentangled latent\nfactors. We introduce a regularizer on the expectation of the approximate\nposterior over observed data that encourages the disentanglement. We also\npropose a new disentanglement metric which is better aligned with the\nqualitative disentanglement observed in the decoder's output. We empirically\nobserve significant improvement over existing methods in terms of both\ndisentanglement and data likelihood (reconstruction quality). \n\n"}
{"id": "1711.00888", "contents": "Title: Set-to-Set Hashing with Applications in Visual Recognition Abstract: Visual data, such as an image or a sequence of video frames, is often\nnaturally represented as a point set. In this paper, we consider the\nfundamental problem of finding a nearest set from a collection of sets, to a\nquery set. This problem has obvious applications in large-scale visual\nretrieval and recognition, and also in applied fields beyond computer vision.\nOne challenge stands out in solving the problem---set representation and\nmeasure of similarity. Particularly, the query set and the sets in dataset\ncollection can have varying cardinalities. The training collection is large\nenough such that linear scan is impractical. We propose a simple representation\nscheme that encodes both statistical and structural information of the sets.\nThe derived representations are integrated in a kernel framework for flexible\nsimilarity measurement. For the query set process, we adopt a learning-to-hash\npipeline that turns the kernel representations into hash bits based on simple\nlearners, using multiple kernel learning. Experiments on two visual retrieval\ndatasets show unambiguously that our set-to-set hashing framework outperforms\nprior methods that do not take the set-to-set search setting. \n\n"}
{"id": "1711.02257", "contents": "Title: GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep\n  Multitask Networks Abstract: Deep multitask networks, in which one neural network produces multiple\npredictive outputs, can offer better speed and performance than their\nsingle-task counterparts but are challenging to train properly. We present a\ngradient normalization (GradNorm) algorithm that automatically balances\ntraining in deep multitask models by dynamically tuning gradient magnitudes. We\nshow that for various network architectures, for both regression and\nclassification tasks, and on both synthetic and real datasets, GradNorm\nimproves accuracy and reduces overfitting across multiple tasks when compared\nto single-task networks, static baselines, and other adaptive multitask loss\nbalancing techniques. GradNorm also matches or surpasses the performance of\nexhaustive grid search methods, despite only involving a single asymmetry\nhyperparameter $\\alpha$. Thus, what was once a tedious search process that\nincurred exponentially more compute for each task added can now be accomplished\nwithin a few training runs, irrespective of the number of tasks. Ultimately, we\nwill demonstrate that gradient manipulation affords us great control over the\ntraining dynamics of multitask networks and may be one of the keys to unlocking\nthe potential of multitask learning. \n\n"}
{"id": "1711.02329", "contents": "Title: Interpreting Convolutional Neural Networks Through Compression Abstract: Convolutional neural networks (CNNs) achieve state-of-the-art performance in\na wide variety of tasks in computer vision. However, interpreting CNNs still\nremains a challenge. This is mainly due to the large number of parameters in\nthese networks. Here, we investigate the role of compression and particularly\npruning filters in the interpretation of CNNs. We exploit our recently-proposed\ngreedy structural compression scheme that prunes filters in a trained CNN. In\nour compression, the filter importance index is defined as the classification\naccuracy reduction (CAR) of the network after pruning that filter. The filters\nare then iteratively pruned based on the CAR index. We demonstrate the\ninterpretability of CAR-compressed CNNs by showing that our algorithm prunes\nfilters with visually redundant pattern selectivity. Specifically, we show the\nimportance of shape-selective filters for object recognition, as opposed to\ncolor-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of\nthem in the first layer and 14 of them in the second layer are color-selective\nfilters. Finally, we introduce a variant of our CAR importance index that\nquantifies the importance of each image class to each CNN filter. We show that\nthe most and the least important class labels present a meaningful\ninterpretation of each filter that is consistent with the visualized pattern\nselectivity of that filter. \n\n"}
{"id": "1711.03270", "contents": "Title: Predicting Scene Parsing and Motion Dynamics in the Future Abstract: The ability of predicting the future is important for intelligent systems,\ne.g. autonomous vehicles and robots to plan early and make decisions\naccordingly. Future scene parsing and optical flow estimation are two key tasks\nthat help agents better understand their environments as the former provides\ndense semantic information, i.e. what objects will be present and where they\nwill appear, while the latter provides dense motion information, i.e. how the\nobjects will move. In this paper, we propose a novel model to simultaneously\npredict scene parsing and optical flow in unobserved future video frames. To\nour best knowledge, this is the first attempt in jointly predicting scene\nparsing and motion dynamics. In particular, scene parsing enables structured\nmotion prediction by decomposing optical flow into different groups while\noptical flow estimation brings reliable pixel-wise correspondence to scene\nparsing. By exploiting this mutually beneficial relationship, our model shows\nsignificantly better parsing and motion prediction results when compared to\nwell-established baselines and individual prediction models on the large-scale\nCityscapes dataset. In addition, we also demonstrate that our model can be used\nto predict the steering angle of the vehicles, which further verifies the\nability of our model to learn latent representations of scene dynamics. \n\n"}
{"id": "1711.04226", "contents": "Title: AON: Towards Arbitrarily-Oriented Text Recognition Abstract: Recognizing text from natural images is a hot research topic in computer\nvision due to its various applications. Despite the enduring research of\nseveral decades on optical character recognition (OCR), recognizing texts from\nnatural images is still a challenging task. This is because scene texts are\noften in irregular (e.g. curved, arbitrarily-oriented or seriously distorted)\narrangements, which have not yet been well addressed in the literature.\nExisting methods on text recognition mainly work with regular (horizontal and\nfrontal) texts and cannot be trivially generalized to handle irregular texts.\nIn this paper, we develop the arbitrary orientation network (AON) to directly\ncapture the deep features of irregular texts, which are combined into an\nattention-based decoder to generate character sequence. The whole network can\nbe trained end-to-end by using only images and word-level annotations.\nExtensive experiments on various benchmarks, including the CUTE80,\nSVT-Perspective, IIIT5k, SVT and ICDAR datasets, show that the proposed\nAON-based method achieves the-state-of-the-art performance in irregular\ndatasets, and is comparable to major existing methods in regular datasets. \n\n"}
{"id": "1711.04249", "contents": "Title: Feature Enhancement Network: A Refined Scene Text Detector Abstract: In this paper, we propose a refined scene text detector with a \\textit{novel}\nFeature Enhancement Network (FEN) for Region Proposal and Text Detection\nRefinement. Retrospectively, both region proposal with \\textit{only} $3\\times\n3$ sliding-window feature and text detection refinement with \\textit{single\nscale} high level feature are insufficient, especially for smaller scene text.\nTherefore, we design a new FEN network with \\textit{task-specific},\n\\textit{low} and \\textit{high} level semantic features fusion to improve the\nperformance of text detection. Besides, since \\textit{unitary}\nposition-sensitive RoI pooling in general object detection is unreasonable for\nvariable text regions, an \\textit{adaptively weighted} position-sensitive RoI\npooling layer is devised for further enhancing the detecting accuracy. To\ntackle the \\textit{sample-imbalance} problem during the refinement stage, we\nalso propose an effective \\textit{positives mining} strategy for efficiently\ntraining our network. Experiments on ICDAR 2011 and 2013 robust text detection\nbenchmarks demonstrate that our method can achieve state-of-the-art results,\noutperforming all reported methods in terms of F-measure. \n\n"}
{"id": "1711.05847", "contents": "Title: AOGNets: Compositional Grammatical Architectures for Deep Learning Abstract: Neural architectures are the foundation for improving performance of deep\nneural networks (DNNs). This paper presents deep compositional grammatical\narchitectures which harness the best of two worlds: grammar models and DNNs.\nThe proposed architectures integrate compositionality and reconfigurability of\nthe former and the capability of learning rich features of the latter in a\nprincipled way. We utilize AND-OR Grammar (AOG) as network generator in this\npaper and call the resulting networks AOGNets. An AOGNet consists of a number\nof stages each of which is composed of a number of AOG building blocks. An AOG\nbuilding block splits its input feature map into N groups along feature\nchannels and then treat it as a sentence of N words. It then jointly realizes a\nphrase structure grammar and a dependency grammar in bottom-up parsing the\n\"sentence\" for better feature exploration and reuse. It provides a unified\nframework for the best practices developed in state-of-the-art DNNs. In\nexperiments, AOGNet is tested in the CIFAR-10, CIFAR-100 and ImageNet-1K\nclassification benchmark and the MS-COCO object detection and segmentation\nbenchmark. In CIFAR-10, CIFAR-100 and ImageNet-1K, AOGNet obtains better\nperformance than ResNet and most of its variants, ResNeXt and its attention\nbased variants such as SENet, DenseNet and DualPathNet. AOGNet also obtains the\nbest model interpretability score using network dissection. AOGNet further\nshows better potential in adversarial defense. In MS-COCO, AOGNet obtains\nbetter performance than the ResNet and ResNeXt backbones in Mask R-CNN. \n\n"}
{"id": "1711.05862", "contents": "Title: Real-Time Document Image Classification using Deep CNN and Extreme\n  Learning Machines Abstract: This paper presents an approach for real-time training and testing for\ndocument image classification. In production environments, it is crucial to\nperform accurate and (time-)efficient training. Existing deep learning\napproaches for classifying documents do not meet these requirements, as they\nrequire much time for training and fine-tuning the deep architectures.\nMotivated from Computer Vision, we propose a two-stage approach. The first\nstage trains a deep network that works as feature extractor and in the second\nstage, Extreme Learning Machines (ELMs) are used for classification. The\nproposed approach outperforms all previously reported structural and deep\nlearning based methods with a final accuracy of 83.24% on Tobacco-3482 dataset,\nleading to a relative error reduction of 25% when compared to a previous\nConvolutional Neural Network (CNN) based approach (DeepDocClassifier). More\nimportantly, the training time of the ELM is only 1.176 seconds and the overall\nprediction time for 2,482 images is 3.066 seconds. As such, this novel approach\nmakes deep learning-based document classification suitable for large-scale\nreal-time applications. \n\n"}
{"id": "1711.06020", "contents": "Title: Global versus Localized Generative Adversarial Nets Abstract: In this paper, we present a novel localized Generative Adversarial Net (GAN)\nto learn on the manifold of real data. Compared with the classic GAN that {\\em\nglobally} parameterizes a manifold, the Localized GAN (LGAN) uses local\ncoordinate charts to parameterize distinct local geometry of how data points\ncan transform at different locations on the manifold. Specifically, around each\npoint there exists a {\\em local} generator that can produce data following\ndiverse patterns of transformations on the manifold. The locality nature of\nLGAN enables local generators to adapt to and directly access the local\ngeometry without need to invert the generator in a global GAN. Furthermore, it\ncan prevent the manifold from being locally collapsed to a dimensionally\ndeficient tangent subspace by imposing an orthonormality prior between\ntangents. This provides a geometric approach to alleviating mode collapse at\nleast locally on the manifold by imposing independence between data\ntransformations in different tangent directions. We will also demonstrate the\nLGAN can be applied to train a robust classifier that prefers locally\nconsistent classification decisions on the manifold, and the resultant\nregularizer is closely related with the Laplace-Beltrami operator. Our\nexperiments show that the proposed LGANs can not only produce diverse image\ntransformations, but also deliver superior classification performances. \n\n"}
{"id": "1711.06045", "contents": "Title: Frame Interpolation with Multi-Scale Deep Loss Functions and Generative\n  Adversarial Networks Abstract: Frame interpolation attempts to synthesise frames given one or more\nconsecutive video frames. In recent years, deep learning approaches, and\nnotably convolutional neural networks, have succeeded at tackling low- and\nhigh-level computer vision problems including frame interpolation. These\ntechniques often tackle two problems, namely algorithm efficiency and\nreconstruction quality. In this paper, we present a multi-scale generative\nadversarial network for frame interpolation (\\mbox{FIGAN}). To maximise the\nefficiency of our network, we propose a novel multi-scale residual estimation\nmodule where the predicted flow and synthesised frame are constructed in a\ncoarse-to-fine fashion. To improve the quality of synthesised intermediate\nvideo frames, our network is jointly supervised at different levels with a\nperceptual loss function that consists of an adversarial and two content\nlosses. We evaluate the proposed approach using a collection of 60fps videos\nfrom YouTube-8m. Our results improve the state-of-the-art accuracy and provide\nsubjective visual quality comparable to the best performing interpolation\nmethod at x47 faster runtime. \n\n"}
{"id": "1711.06288", "contents": "Title: Language-Based Image Editing with Recurrent Attentive Models Abstract: We investigate the problem of Language-Based Image Editing (LBIE). Given a\nsource image and a natural language description, we want to generate a target\nimage by editing the source image based on the description. We propose a\ngeneric modeling framework for two sub-tasks of LBIE: language-based image\nsegmentation and image colorization. The framework uses recurrent attentive\nmodels to fuse image and language features. Instead of using a fixed step size,\nwe introduce for each region of the image a termination gate to dynamically\ndetermine after each inference step whether to continue extrapolating\nadditional information from the textual description. The effectiveness of the\nframework is validated on three datasets. First, we introduce a synthetic\ndataset, called CoSaL, to evaluate the end-to-end performance of our LBIE\nsystem. Second, we show that the framework leads to state-of-the-art\nperformance on image segmentation on the ReferIt dataset. Third, we present the\nfirst language-based colorization result on the Oxford-102 Flowers dataset. \n\n"}
{"id": "1711.06373", "contents": "Title: Thoracic Disease Identification and Localization with Limited\n  Supervision Abstract: Accurate identification and localization of abnormalities from radiology\nimages play an integral part in clinical diagnosis and treatment planning.\nBuilding a highly accurate prediction model for these tasks usually requires a\nlarge number of images manually annotated with labels and finding sites of\nabnormalities. In reality, however, such annotated data are expensive to\nacquire, especially the ones with location annotations. We need methods that\ncan work well with only a small amount of location annotations. To address this\nchallenge, we present a unified approach that simultaneously performs disease\nidentification and localization through the same underlying model for all\nimages. We demonstrate that our approach can effectively leverage both class\ninformation as well as limited location annotation, and significantly\noutperforms the comparative reference baseline in both classification and\nlocalization tasks. \n\n"}
{"id": "1711.06504", "contents": "Title: Detecting hip fractures with radiologist-level performance using deep\n  neural networks Abstract: We developed an automated deep learning system to detect hip fractures from\nfrontal pelvic x-rays, an important and common radiological task. Our system\nwas trained on a decade of clinical x-rays (~53,000 studies) and can be applied\nto clinical data, automatically excluding inappropriate and technically\nunsatisfactory studies. We demonstrate diagnostic performance equivalent to a\nhuman radiologist and an area under the ROC curve of 0.994. Translated to\nclinical practice, such a system has the potential to increase the efficiency\nof diagnosis, reduce the need for expensive additional testing, expand access\nto expert level medical image interpretation, and improve overall patient\noutcomes. \n\n"}
{"id": "1711.06623", "contents": "Title: Driven to Distraction: Self-Supervised Distractor Learning for Robust\n  Monocular Visual Odometry in Urban Environments Abstract: We present a self-supervised approach to ignoring \"distractors\" in camera\nimages for the purposes of robustly estimating vehicle motion in cluttered\nurban environments. We leverage offline multi-session mapping approaches to\nautomatically generate a per-pixel ephemerality mask and depth map for each\ninput image, which we use to train a deep convolutional network. At run-time we\nuse the predicted ephemerality and depth as an input to a monocular visual\nodometry (VO) pipeline, using either sparse features or dense photometric\nmatching. Our approach yields metric-scale VO using only a single camera and\ncan recover the correct egomotion even when 90% of the image is obscured by\ndynamic, independently moving objects. We evaluate our robust VO methods on\nmore than 400km of driving from the Oxford RobotCar Dataset and demonstrate\nreduced odometry drift and significantly improved egomotion estimation in the\npresence of large moving vehicles in urban traffic. \n\n"}
{"id": "1711.07246", "contents": "Title: Face Attention Network: An Effective Face Detector for the Occluded\n  Faces Abstract: The performance of face detection has been largely improved with the\ndevelopment of convolutional neural network. However, the occlusion issue due\nto mask and sunglasses, is still a challenging problem. The improvement on the\nrecall of these occluded cases usually brings the risk of high false positives.\nIn this paper, we present a novel face detector called Face Attention Network\n(FAN), which can significantly improve the recall of the face detection problem\nin the occluded case without compromising the speed. More specifically, we\npropose a new anchor-level attention, which will highlight the features from\nthe face region. Integrated with our anchor assign strategy and data\naugmentation techniques, we obtain state-of-art results on public face\ndetection benchmarks like WiderFace and MAFA. The code will be released for\nreproduction. \n\n"}
{"id": "1711.07377", "contents": "Title: Pixel-wise object tracking Abstract: In this paper, we propose a novel pixel-wise visual object tracking framework\nthat can track any anonymous object in a noisy background. The framework\nconsists of two submodels, a global attention model and a local segmentation\nmodel. The global model generates a region of interests (ROI) that the object\nmay lie in the new frame based on the past object segmentation maps, while the\nlocal model segments the new image in the ROI. Each model uses a LSTM structure\nto model the temporal dynamics of the motion and appearance, respectively. To\ncircumvent the dependency of the training data between the two models, we use\nan iterative update strategy. Once the models are trained, there is no need to\nrefine them to track specific objects, making our method efficient compared to\nonline learning approaches. We demonstrate our real time pixel-wise object\ntracking framework on a challenging VOT dataset \n\n"}
{"id": "1711.07614", "contents": "Title: Asking the Difficult Questions: Goal-Oriented Visual Question Generation\n  via Intermediate Rewards Abstract: Despite significant progress in a variety of vision-and-language problems,\ndeveloping a method capable of asking intelligent, goal-oriented questions\nabout images is proven to be an inscrutable challenge. Towards this end, we\npropose a Deep Reinforcement Learning framework based on three new intermediate\nrewards, namely goal-achieved, progressive and informativeness that encourage\nthe generation of succinct questions, which in turn uncover valuable\ninformation towards the overall goal. By directly optimizing for questions that\nwork quickly towards fulfilling the overall goal, we avoid the tendency of\nexisting methods to generate long series of insane queries that add little\nvalue. We evaluate our model on the GuessWhat?! dataset and show that the\nresulting questions can help a standard Guesser identify a specific object in\nan image at a much higher success rate. \n\n"}
{"id": "1711.07714", "contents": "Title: Residual Parameter Transfer for Deep Domain Adaptation Abstract: The goal of Deep Domain Adaptation is to make it possible to use Deep Nets\ntrained in one domain where there is enough annotated training data in another\nwhere there is little or none. Most current approaches have focused on learning\nfeature representations that are invariant to the changes that occur when going\nfrom one domain to the other, which means using the same network parameters in\nboth domains. While some recent algorithms explicitly model the changes by\nadapting the network parameters, they either severely restrict the possible\ndomain changes, or significantly increase the number of model parameters.\n  By contrast, we introduce a network architecture that includes auxiliary\nresidual networks, which we train to predict the parameters in the domain with\nlittle annotated data from those in the other one. This architecture enables us\nto flexibly preserve the similarities between domains where they exist and\nmodel the differences when necessary. We demonstrate that our approach yields\nhigher accuracy than state-of-the-art methods without undue complexity. \n\n"}
{"id": "1711.08006", "contents": "Title: Relating Input Concepts to Convolutional Neural Network Decisions Abstract: Many current methods to interpret convolutional neural networks (CNNs) use\nvisualization techniques and words to highlight concepts of the input seemingly\nrelevant to a CNN's decision. The methods hypothesize that the recognition of\nthese concepts are instrumental in the decision a CNN reaches, but the nature\nof this relationship has not been well explored. To address this gap, this\npaper examines the quality of a concept's recognition by a CNN and the degree\nto which the recognitions are associated with CNN decisions. The study\nconsiders a CNN trained for scene recognition over the ADE20k dataset. It uses\na novel approach to find and score the strength of minimally distributed\nrepresentations of input concepts (defined by objects in scene images) across\nlate stage feature maps. Subsequent analysis finds evidence that concept\nrecognition impacts decision making. Strong recognition of concepts\nfrequently-occurring in few scenes are indicative of correct decisions, but\nrecognizing concepts common to many scenes may mislead the network. \n\n"}
{"id": "1711.08996", "contents": "Title: Dense 3D Regression for Hand Pose Estimation Abstract: We present a simple and effective method for 3D hand pose estimation from a\nsingle depth frame. As opposed to previous state-of-the-art methods based on\nholistic 3D regression, our method works on dense pixel-wise estimation. This\nis achieved by careful design choices in pose parameterization, which leverages\nboth 2D and 3D properties of depth map. Specifically, we decompose the pose\nparameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat\nmaps and unit 3D directional vector fields. The 2D/3D joint heat maps and 3D\njoint offsets are estimated via multi-task network cascades, which is trained\nend-to-end. The pixel-wise estimations can be directly translated into a vote\ncasting scheme. A variant of mean shift is then used to aggregate local votes\nwhile enforcing consensus between the the estimated 3D pose and the pixel-wise\n2D and 3D estimations by design. Our method is efficient and highly accurate.\nOn MSRA and NYU hand dataset, our method outperforms all previous\nstate-of-the-art approaches by a large margin. On the ICVL hand dataset, our\nmethod achieves similar accuracy compared to the currently proposed nearly\nsaturated result and outperforms various other proposed methods. Code is\navailable $\\href{\"https://github.com/melonwan/denseReg\"}{\\text{online}}$. \n\n"}
{"id": "1711.08998", "contents": "Title: Visual Feature Attribution using Wasserstein GANs Abstract: Attributing the pixels of an input image to a certain category is an\nimportant and well-studied problem in computer vision, with applications\nranging from weakly supervised localisation to understanding hidden effects in\nthe data. In recent years, approaches based on interpreting a previously\ntrained neural network classifier have become the de facto state-of-the-art and\nare commonly used on medical as well as natural image datasets. In this paper,\nwe discuss a limitation of these approaches which may lead to only a subset of\nthe category specific features being detected. To address this problem we\ndevelop a novel feature attribution technique based on Wasserstein Generative\nAdversarial Networks (WGAN), which does not suffer from this limitation. We\nshow that our proposed method performs substantially better than the\nstate-of-the-art for visual attribution on a synthetic dataset and on real 3D\nneuroimaging data from patients with mild cognitive impairment (MCI) and\nAlzheimer's disease (AD). For AD patients the method produces compellingly\nrealistic disease effect maps which are very close to the observed effects. \n\n"}
{"id": "1711.09219", "contents": "Title: Stacked Kernel Network Abstract: Kernel methods are powerful tools to capture nonlinear patterns behind data.\nThey implicitly learn high (even infinite) dimensional nonlinear features in\nthe Reproducing Kernel Hilbert Space (RKHS) while making the computation\ntractable by leveraging the kernel trick. Classic kernel methods learn a single\nlayer of nonlinear features, whose representational power may be limited.\nMotivated by recent success of deep neural networks (DNNs) that learn\nmulti-layer hierarchical representations, we propose a Stacked Kernel Network\n(SKN) that learns a hierarchy of RKHS-based nonlinear features. SKN interleaves\nseveral layers of nonlinear transformations (from a linear space to a RKHS) and\nlinear transformations (from a RKHS to a linear space). Similar to DNNs, a SKN\nis composed of multiple layers of hidden units, but each parameterized by a\nRKHS function rather than a finite-dimensional vector. We propose three ways to\nrepresent the RKHS functions in SKN: (1)nonparametric representation,\n(2)parametric representation and (3)random Fourier feature representation.\nFurthermore, we expand SKN into CNN architecture called Stacked Kernel\nConvolutional Network (SKCN). SKCN learning a hierarchy of RKHS-based nonlinear\nfeatures by convolutional operation with each filter also parameterized by a\nRKHS function rather than a finite-dimensional matrix in CNN, which is suitable\nfor image inputs. Experiments on various datasets demonstrate the effectiveness\nof SKN and SKCN, which outperform the competitive methods. \n\n"}
{"id": "1711.09280", "contents": "Title: Gradually Updated Neural Networks for Large-Scale Image Recognition Abstract: Depth is one of the keys that make neural networks succeed in the task of\nlarge-scale image recognition. The state-of-the-art network architectures\nusually increase the depths by cascading convolutional layers or building\nblocks. In this paper, we present an alternative method to increase the depth.\nOur method is by introducing computation orderings to the channels within\nconvolutional layers or blocks, based on which we gradually compute the outputs\nin a channel-wise manner. The added orderings not only increase the depths and\nthe learning capacities of the networks without any additional computation\ncosts, but also eliminate the overlap singularities so that the networks are\nable to converge faster and perform better. Experiments show that the networks\nbased on our method achieve the state-of-the-art performances on CIFAR and\nImageNet datasets. \n\n"}
{"id": "1711.09869", "contents": "Title: Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs Abstract: We propose a novel deep learning-based framework to tackle the challenge of\nsemantic segmentation of large-scale point clouds of millions of points. We\nargue that the organization of 3D point clouds can be efficiently captured by a\nstructure called superpoint graph (SPG), derived from a partition of the\nscanned scene into geometrically homogeneous elements. SPGs offer a compact yet\nrich representation of contextual relationships between object parts, which is\nthen exploited by a graph convolutional network. Our framework sets a new state\nof the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for\nboth Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the\nS3DIS dataset). \n\n"}
{"id": "1711.10143", "contents": "Title: Revisiting hand-crafted feature for action recognition: a set of\n  improved dense trajectories Abstract: We propose a feature for action recognition called Trajectory-Set (TS), on\ntop of the improved Dense Trajectory (iDT). The TS feature encodes only\ntrajectories around densely sampled interest points, without any appearance\nfeatures. Experimental results on the UCF50, UCF101, and HMDB51 action datasets\ndemonstrate that TS is comparable to state-of-the-arts, and outperforms many\nother methods; for HMDB the accuracy of 85.4%, compared to the best accuracy of\n80.2% obtained by a deep method. Our code is available on-line at\nhttps://github.com/Gauffret/TrajectorySet . \n\n"}
{"id": "1711.10275", "contents": "Title: 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks Abstract: Convolutional networks are the de-facto standard for analyzing\nspatio-temporal data such as images, videos, and 3D shapes. Whilst some of this\ndata is naturally dense (e.g., photos), many other data sources are inherently\nsparse. Examples include 3D point clouds that were obtained using a LiDAR\nscanner or RGB-D camera. Standard \"dense\" implementations of convolutional\nnetworks are very inefficient when applied on such sparse data. We introduce\nnew sparse convolutional operations that are designed to process\nspatially-sparse data more efficiently, and use them to develop\nspatially-sparse convolutional networks. We demonstrate the strong performance\nof the resulting models, called submanifold sparse convolutional networks\n(SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In\nparticular, our models outperform all prior state-of-the-art on the test set of\na recent semantic segmentation competition. \n\n"}
{"id": "1711.10305", "contents": "Title: Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks Abstract: Convolutional Neural Networks (CNN) have been regarded as a powerful class of\nmodels for image recognition problems. Nevertheless, it is not trivial when\nutilizing a CNN for learning spatio-temporal video representation. A few\nstudies have shown that performing 3D convolutions is a rewarding approach to\ncapture both spatial and temporal dimensions in videos. However, the\ndevelopment of a very deep 3D CNN from scratch results in expensive\ncomputational cost and memory demand. A valid question is why not recycle\noff-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiple\nvariants of bottleneck building blocks in a residual learning framework by\nsimulating $3\\times3\\times3$ convolutions with $1\\times3\\times3$ convolutional\nfilters on spatial domain (equivalent to 2D CNN) plus $3\\times1\\times1$\nconvolutions to construct temporal connections on adjacent feature maps in\ntime. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net\n(P3D ResNet), that exploits all the variants of blocks but composes each in\ndifferent placement of ResNet, following the philosophy that enhancing\nstructural diversity with going deep could improve the power of neural\nnetworks. Our P3D ResNet achieves clear improvements on Sports-1M video\nclassification dataset against 3D CNN and frame-based 2D CNN by 5.3% and 1.8%,\nrespectively. We further examine the generalization performance of video\nrepresentation produced by our pre-trained P3D ResNet on five different\nbenchmarks and three different tasks, demonstrating superior performances over\nseveral state-of-the-art techniques. \n\n"}
{"id": "1711.10352", "contents": "Title: Learning Face Age Progression: A Pyramid Architecture of GANs Abstract: The two underlying requirements of face age progression, i.e. aging accuracy\nand identity permanence, are not well studied in the literature. In this paper,\nwe present a novel generative adversarial network based approach. It separately\nmodels the constraints for the intrinsic subject-specific characteristics and\nthe age-specific facial changes with respect to the elapsed time, ensuring that\nthe generated faces present desired aging effects while simultaneously keeping\npersonalized properties stable. Further, to generate more lifelike facial\ndetails, high-level age-specific features conveyed by the synthesized face are\nestimated by a pyramidal adversarial discriminator at multiple scales, which\nsimulates the aging effects in a finer manner. The proposed method is\napplicable to diverse face samples in the presence of variations in pose,\nexpression, makeup, etc., and remarkably vivid aging effects are achieved. Both\nvisual fidelity and quantitative evaluations show that the approach advances\nthe state-of-the-art. \n\n"}
{"id": "1711.10669", "contents": "Title: Image2Mesh: A Learning Framework for Single Image 3D Reconstruction Abstract: One challenge that remains open in 3D deep learning is how to efficiently\nrepresent 3D data to feed deep networks. Recent works have relied on volumetric\nor point cloud representations, but such approaches suffer from a number of\nissues such as computational complexity, unordered data, and lack of finer\ngeometry. This paper demonstrates that a mesh representation (i.e. vertices and\nfaces to form polygonal surfaces) is able to capture fine-grained geometry for\n3D reconstruction tasks. A mesh however is also unstructured data similar to\npoint clouds. We address this problem by proposing a learning framework to\ninfer the parameters of a compact mesh representation rather than learning from\nthe mesh itself. This compact representation encodes a mesh using free-form\ndeformation and a sparse linear combination of models allowing us to\nreconstruct 3D meshes from single images. In contrast to prior work, we do not\nrely on silhouettes and landmarks to perform 3D reconstruction. We evaluate our\nmethod on synthetic and real-world datasets with very promising results. Our\nframework efficiently reconstructs 3D objects in a low-dimensional way while\npreserving its important geometrical aspects. \n\n"}
{"id": "1711.10703", "contents": "Title: FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors Abstract: Face Super-Resolution (SR) is a domain-specific super-resolution problem. The\nspecific facial prior knowledge could be leveraged for better super-resolving\nface images. We present a novel deep end-to-end trainable Face Super-Resolution\nNetwork (FSRNet), which makes full use of the geometry prior, i.e., facial\nlandmark heatmaps and parsing maps, to super-resolve very low-resolution (LR)\nface images without well-aligned requirement. Specifically, we first construct\na coarse SR network to recover a coarse high-resolution (HR) image. Then, the\ncoarse HR image is sent to two branches: a fine SR encoder and a prior\ninformation estimation network, which extracts the image features, and\nestimates landmark heatmaps/parsing maps respectively. Both image features and\nprior information are sent to a fine SR decoder to recover the HR image. To\nfurther generate realistic faces, we propose the Face Super-Resolution\nGenerative Adversarial Network (FSRGAN) to incorporate the adversarial loss\ninto FSRNet. Moreover, we introduce two related tasks, face alignment and\nparsing, as the new evaluation metrics for face SR, which address the\ninconsistency of classic metrics w.r.t. visual perception. Extensive benchmark\nexperiments show that FSRNet and FSRGAN significantly outperforms state of the\narts for very LR face SR, both quantitatively and qualitatively. Code will be\nmade available upon publication. \n\n"}
{"id": "1711.10729", "contents": "Title: Deep Eyes: Binocular Depth-from-Focus on Focal Stack Pairs Abstract: Human visual system relies on both binocular stereo cues and monocular\nfocusness cues to gain effective 3D perception. In computer vision, the two\nproblems are traditionally solved in separate tracks. In this paper, we present\na unified learning-based technique that simultaneously uses both types of cues\nfor depth inference. Specifically, we use a pair of focal stacks as input to\nemulate human perception. We first construct a comprehensive focal stack\ntraining dataset synthesized by depth-guided light field rendering. We then\nconstruct three individual networks: a Focus-Net to extract depth from a single\nfocal stack, a EDoF-Net to obtain the extended depth of field (EDoF) image from\nthe focal stack, and a Stereo-Net to conduct stereo matching. We show how to\nintegrate them into a unified BDfF-Net to obtain high-quality depth maps.\nComprehensive experiments show that our approach outperforms the\nstate-of-the-art in both accuracy and speed and effectively emulates human\nvision systems. \n\n"}
{"id": "1711.10918", "contents": "Title: Joint Blind Motion Deblurring and Depth Estimation of Light Field Abstract: Removing camera motion blur from a single light field is a challenging task\nsince it is highly ill-posed inverse problem. The problem becomes even worse\nwhen blur kernel varies spatially due to scene depth variation and high-order\ncamera motion. In this paper, we propose a novel algorithm to estimate all blur\nmodel variables jointly, including latent sub-aperture image, camera motion,\nand scene depth from the blurred 4D light field. Exploiting multi-view nature\nof a light field relieves the inverse property of the optimization by utilizing\nstrong depth cues and multi-view blur observation. The proposed joint\nestimation achieves high quality light field deblurring and depth estimation\nsimultaneously under arbitrary 6-DOF camera motion and unconstrained scene\ndepth. Intensive experiment on real and synthetic blurred light field confirms\nthat the proposed algorithm outperforms the state-of-the-art light field\ndeblurring and depth estimation methods. \n\n"}
{"id": "1711.11443", "contents": "Title: ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and\n  Uncovering Biases Abstract: ConvNets and Imagenet have driven the recent success of deep learning for\nimage classification. However, the marked slowdown in performance improvement\ncombined with the lack of robustness of neural networks to adversarial examples\nand their tendency to exhibit undesirable biases question the reliability of\nthese methods. This work investigates these questions from the perspective of\nthe end-user by using human subject studies and explanations. The contribution\nof this study is threefold. We first experimentally demonstrate that the\naccuracy and robustness of ConvNets measured on Imagenet are vastly\nunderestimated. Next, we show that explanations can mitigate the impact of\nmisclassified adversarial examples from the perspective of the end-user. We\nfinally introduce a novel tool for uncovering the undesirable biases learned by\na model. These contributions also show that explanations are a valuable tool\nboth for improving our understanding of ConvNets' predictions and for designing\nmore reliable models. \n\n"}
{"id": "1712.00123", "contents": "Title: Label Efficient Learning of Transferable Representations across Domains\n  and Tasks Abstract: We propose a framework that learns a representation transferable across\ndifferent domains and tasks in a label efficient manner. Our approach battles\ndomain shift with a domain adversarial loss, and generalizes the embedding to\nnovel task using a metric learning-based approach. Our model is simultaneously\noptimized on labeled source data and unlabeled or sparsely labeled data in the\ntarget domain. Our method shows compelling results on novel classes within a\nnew domain even when only a few labeled examples per class are available,\noutperforming the prevalent fine-tuning approach. In addition, we demonstrate\nthe effectiveness of our framework on the transfer learning task from image\nobject recognition to video action recognition. \n\n"}
{"id": "1712.00725", "contents": "Title: Sentiment Classification using Images and Label Embeddings Abstract: In this project we analysed how much semantic information images carry, and\nhow much value image data can add to sentiment analysis of the text associated\nwith the images. To better understand the contribution from images, we compared\nmodels which only made use of image data, models which only made use of text\ndata, and models which combined both data types. We also analysed if this\napproach could help sentiment classifiers generalize to unknown sentiments. \n\n"}
{"id": "1712.00955", "contents": "Title: Composite Quantization Abstract: This paper studies the compact coding approach to approximate nearest\nneighbor search. We introduce a composite quantization framework. It uses the\ncomposition of several ($M$) elements, each of which is selected from a\ndifferent dictionary, to accurately approximate a $D$-dimensional vector, thus\nyielding accurate search, and represents the data vector by a short code\ncomposed of the indices of the selected elements in the corresponding\ndictionaries. Our key contribution lies in introducing a near-orthogonality\nconstraint, which makes the search efficiency is guaranteed as the cost of the\ndistance computation is reduced to $O(M)$ from $O(D)$ through a distance table\nlookup scheme. The resulting approach is called near-orthogonal composite\nquantization. We theoretically justify the equivalence between near-orthogonal\ncomposite quantization and minimizing an upper bound of a function formed by\njointly considering the quantization error and the search cost according to a\ngeneralized triangle inequality. We empirically show the efficacy of the\nproposed approach over several benchmark datasets. In addition, we demonstrate\nthe superior performances in other three applications: combination with\ninverted multi-index, quantizing the query for mobile search, and inner-product\nsimilarity search. \n\n"}
{"id": "1712.00971", "contents": "Title: Face Translation between Images and Videos using Identity-aware CycleGAN Abstract: This paper presents a new problem of unpaired face translation between images\nand videos, which can be applied to facial video prediction and enhancement. In\nthis problem there exist two major technical challenges: 1) designing a robust\ntranslation model between static images and dynamic videos, and 2) preserving\nfacial identity during image-video translation. To address such two problems,\nwe generalize the state-of-the-art image-to-image translation network\n(Cycle-Consistent Adversarial Networks) to the image-to-video/video-to-image\ntranslation context by exploiting a image-video translation model and an\nidentity preservation model. In particular, we apply the state-of-the-art\nWasserstein GAN technique to the setting of image-video translation for better\nconvergence, and we meanwhile introduce a face verificator to ensure the\nidentity. Experiments on standard image/video face datasets demonstrate the\neffectiveness of the proposed model in both terms of qualitative and\nquantitative evaluations. \n\n"}
{"id": "1712.01455", "contents": "Title: Multimodal Storytelling via Generative Adversarial Imitation Learning Abstract: Deriving event storylines is an effective summarization method to succinctly\norganize extensive information, which can significantly alleviate the pain of\ninformation overload. The critical challenge is the lack of widely recognized\ndefinition of storyline metric. Prior studies have developed various approaches\nbased on different assumptions about users' interests. These works can extract\ninteresting patterns, but their assumptions do not guarantee that the derived\npatterns will match users' preference. On the other hand, their exclusiveness\nof single modality source misses cross-modality information. This paper\nproposes a method, multimodal imitation learning via generative adversarial\nnetworks(MIL-GAN), to directly model users' interests as reflected by various\ndata. In particular, the proposed model addresses the critical challenge by\nimitating users' demonstrated storylines. Our proposed model is designed to\nlearn the reward patterns given user-provided storylines and then applies the\nlearned policy to unseen data. The proposed approach is demonstrated to be\ncapable of acquiring the user's implicit intent and outperforming competing\nmethods by a substantial margin with a user study. \n\n"}
{"id": "1712.01727", "contents": "Title: OL\\'E: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for\n  Deep Learning Abstract: Deep neural networks trained using a softmax layer at the top and the\ncross-entropy loss are ubiquitous tools for image classification. Yet, this\ndoes not naturally enforce intra-class similarity nor inter-class margin of the\nlearned deep representations. To simultaneously achieve these two goals,\ndifferent solutions have been proposed in the literature, such as the pairwise\nor triplet losses. However, such solutions carry the extra task of selecting\npairs or triplets, and the extra computational burden of computing and learning\nfor many combinations of them. In this paper, we propose a plug-and-play loss\nterm for deep networks that explicitly reduces intra-class variance and\nenforces inter-class margin simultaneously, in a simple and elegant geometric\nmanner. For each class, the deep features are collapsed into a learned linear\nsubspace, or union of them, and inter-class subspaces are pushed to be as\northogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\\'E) does\nnot require carefully crafting pairs or triplets of samples for training, and\nworks standalone as a classification loss, being the first reported deep metric\nlearning framework of its kind. Because of the improved margin between features\nof different classes, the resulting deep networks generalize better, are more\ndiscriminative, and more robust. We demonstrate improved classification\nperformance in general object recognition, plugging the proposed loss term into\nexisting off-the-shelf architectures. In particular, we show the advantage of\nthe proposed loss in the small data/model scenario, and we significantly\nadvance the state-of-the-art on the Stanford STL-10 benchmark. \n\n"}
{"id": "1712.02198", "contents": "Title: Lung Nodule Classification by the Combination of Fusion Classifier and\n  Cascaded Convolutional Neural Networks Abstract: Lung nodule classification is a class imbalanced problem, as nodules are\nfound with much lower frequency than non-nodules. In the class imbalanced\nproblem, conventional classifiers tend to be overwhelmed by the majority class\nand ignore the minority class. We showed that cascaded convolutional neural\nnetworks can classify the nodule candidates precisely for a class imbalanced\nnodule candidate data set in our previous study. In this paper, we propose\nFusion classifier in conjunction with the cascaded convolutional neural network\nmodels. To fuse the models, nodule probabilities are calculated by using the\nconvolutional neural network models at first. Then, Fusion classifier is\ntrained and tested by the nodule probabilities. The proposed method achieved\nthe sensitivity of 94.4% and 95.9% at 4 and 8 false positives per scan in Free\nReceiver Operating Characteristics (FROC) curve analysis, respectively. \n\n"}
{"id": "1712.02463", "contents": "Title: CURE-TSR: Challenging Unreal and Real Environments for Traffic Sign\n  Recognition Abstract: In this paper, we investigate the robustness of traffic sign recognition\nalgorithms under challenging conditions. Existing datasets are limited in terms\nof their size and challenging condition coverage, which motivated us to\ngenerate the Challenging Unreal and Real Environments for Traffic Sign\nRecognition (CURE-TSR) dataset. It includes more than two million traffic sign\nimages that are based on real-world and simulator data. We benchmark the\nperformance of existing solutions in real-world scenarios and analyze the\nperformance variation with respect to challenging conditions. We show that\nchallenging conditions can decrease the performance of baseline methods\nsignificantly, especially if these challenging conditions result in loss or\nmisplacement of spatial information. We also investigate the effect of data\naugmentation and show that utilization of simulator data along with real-world\ndata enhance the average recognition performance in real-world scenarios. The\ndataset is publicly available at https://ghassanalregib.com/cure-tsr/. \n\n"}
{"id": "1712.02779", "contents": "Title: Exploring the Landscape of Spatial Robustness Abstract: The study of adversarial robustness has so far largely focused on\nperturbations bound in p-norms. However, state-of-the-art models turn out to be\nalso vulnerable to other, more natural classes of perturbations such as\ntranslations and rotations. In this work, we thoroughly investigate the\nvulnerability of neural network--based classifiers to rotations and\ntranslations. While data augmentation offers relatively small robustness, we\nuse ideas from robust optimization and test-time input aggregation to\nsignificantly improve robustness. Finally we find that, in contrast to the\np-norm case, first-order methods cannot reliably find worst-case perturbations.\nThis highlights spatial robustness as a fundamentally different setting\nrequiring additional study. Code available at\nhttps://github.com/MadryLab/adversarial_spatial and\nhttps://github.com/MadryLab/spatial-pytorch. \n\n"}
{"id": "1712.02864", "contents": "Title: Learned Perceptual Image Enhancement Abstract: Learning a typical image enhancement pipeline involves minimization of a loss\nfunction between enhanced and reference images. While L1 and L2 losses are\nperhaps the most widely used functions for this purpose, they do not\nnecessarily lead to perceptually compelling results. In this paper, we show\nthat adding a learned no-reference image quality metric to the loss can\nsignificantly improve enhancement operators. This metric is implemented using a\nCNN (convolutional neural network) trained on a large-scale dataset labelled\nwith aesthetic preferences of human raters. This loss allows us to conveniently\nperform back-propagation in our learning framework to simultaneously optimize\nfor similarity to a given ground truth reference and perceptual quality. This\nperceptual loss is only used to train parameters of image processing operators,\nand does not impose any extra complexity at inference time. Our experiments\ndemonstrate that this loss can be effective for tuning a variety of operators\nsuch as local tone mapping and dehazing. \n\n"}
{"id": "1712.03337", "contents": "Title: Bayesian Joint Matrix Decomposition for Data Integration with\n  Heterogeneous Noise Abstract: Matrix decomposition is a popular and fundamental approach in machine\nlearning and data mining. It has been successfully applied into various fields.\nMost matrix decomposition methods focus on decomposing a data matrix from one\nsingle source. However, it is common that data are from different sources with\nheterogeneous noise. A few of matrix decomposition methods have been extended\nfor such multi-view data integration and pattern discovery. While only few\nmethods were designed to consider the heterogeneity of noise in such multi-view\ndata for data integration explicitly. To this end, we propose a joint matrix\ndecomposition framework (BJMD), which models the heterogeneity of noise by\nGaussian distribution in a Bayesian framework. We develop two algorithms to\nsolve this model: one is a variational Bayesian inference algorithm, which\nmakes full use of the posterior distribution; and another is a maximum a\nposterior algorithm, which is more scalable and can be easily paralleled.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nBJMD considering the heterogeneity of noise is superior or competitive to the\nstate-of-the-art methods. \n\n"}
{"id": "1712.03342", "contents": "Title: Geometry-Aware Learning of Maps for Camera Localization Abstract: Maps are a key component in image-based camera localization and visual SLAM\nsystems: they are used to establish geometric constraints between images,\ncorrect drift in relative pose estimation, and relocalize cameras after lost\ntracking. The exact definitions of maps, however, are often\napplication-specific and hand-crafted for different scenarios (e.g. 3D\nlandmarks, lines, planes, bags of visual words). We propose to represent maps\nas a deep neural net called MapNet, which enables learning a data-driven map\nrepresentation. Unlike prior work on learning maps, MapNet exploits cheap and\nubiquitous sensory inputs like visual odometry and GPS in addition to images\nand fuses them together for camera localization. Geometric constraints\nexpressed by these inputs, which have traditionally been used in bundle\nadjustment or pose-graph optimization, are formulated as loss terms in MapNet\ntraining and also used during inference. In addition to directly improving\nlocalization accuracy, this allows us to update the MapNet (i.e., maps) in a\nself-supervised manner using additional unlabeled video sequences from the\nscene. We also propose a novel parameterization for camera rotation which is\nbetter suited for deep-learning based camera pose regression. Experimental\nresults on both the indoor 7-Scenes dataset and the outdoor Oxford RobotCar\ndataset show significant performance improvement over prior work. The MapNet\nproject webpage is https://goo.gl/mRB3Au. \n\n"}
{"id": "1712.03747", "contents": "Title: Deep convolutional neural networks for brain image analysis on magnetic\n  resonance imaging: a review Abstract: In recent years, deep convolutional neural networks (CNNs) have shown\nrecord-shattering performance in a variety of computer vision problems, such as\nvisual object recognition, detection and segmentation. These methods have also\nbeen utilised in medical image analysis domain for lesion segmentation,\nanatomical segmentation and classification. We present an extensive literature\nreview of CNN techniques applied in brain magnetic resonance imaging (MRI)\nanalysis, focusing on the architectures, pre-processing, data-preparation and\npost-processing strategies available in these works. The aim of this study is\nthree-fold. Our primary goal is to report how different CNN architectures have\nevolved, discuss state-of-the-art strategies, condense their results obtained\nusing public datasets and examine their pros and cons. Second, this paper is\nintended to be a detailed reference of the research activity in deep CNN for\nbrain MRI analysis. Finally, we present a perspective on the future of CNNs in\nwhich we hint some of the research directions in subsequent years. \n\n"}
{"id": "1712.04008", "contents": "Title: Investigating the Impact of Data Volume and Domain Similarity on\n  Transfer Learning Applications Abstract: Transfer learning allows practitioners to recognize and apply knowledge\nlearned in previous tasks (source task) to new tasks or new domains (target\ntask), which share some commonality. The two important factors impacting the\nperformance of transfer learning models are: (a) the size of the target\ndataset, and (b) the similarity in distribution between source and target\ndomains. Thus far, there has been little investigation into just how important\nthese factors are. In this paper, we investigate the impact of target dataset\nsize and source/target domain similarity on model performance through a series\nof experiments. We find that more data is always beneficial, and model\nperformance improves linearly with the log of data size, until we are out of\ndata. As source/target domains differ, more data is required and fine tuning\nwill render better performance than feature extraction. When source/target\ndomains are similar and data size is small, fine tuning and feature extraction\nrenders equivalent performance. Our hope is that by beginning this quantitative\ninvestigation on the effect of data volume and domain similarity in transfer\nlearning we might inspire others to explore the significance of data in\ndeveloping more accurate statistical models. \n\n"}
{"id": "1712.05870", "contents": "Title: Multi-dimensional imaging data recovery via minimizing the partial sum\n  of tubal nuclear norm Abstract: In this paper, we investigate tensor recovery problems within the tensor\nsingular value decomposition (t-SVD) framework. We propose the partial sum of\nthe tubal nuclear norm (PSTNN) of a tensor. The PSTNN is a surrogate of the\ntensor tubal multi-rank. We build two PSTNN-based minimization models for two\ntypical tensor recovery problems, i.e., the tensor completion and the tensor\nprincipal component analysis. We give two algorithms based on the alternating\ndirection method of multipliers (ADMM) to solve proposed PSTNN-based tensor\nrecovery models. Experimental results on the synthetic data and real-world data\nreveal the superior of the proposed PSTNN. \n\n"}
{"id": "1712.05877", "contents": "Title: Quantization and Training of Neural Networks for Efficient\n  Integer-Arithmetic-Only Inference Abstract: The rising popularity of intelligent mobile devices and the daunting\ncomputational cost of deep learning-based models call for efficient and\naccurate on-device inference schemes. We propose a quantization scheme that\nallows inference to be carried out using integer-only arithmetic, which can be\nimplemented more efficiently than floating point inference on commonly\navailable integer-only hardware. We also co-design a training procedure to\npreserve end-to-end model accuracy post quantization. As a result, the proposed\nquantization scheme improves the tradeoff between accuracy and on-device\nlatency. The improvements are significant even on MobileNets, a model family\nknown for run-time efficiency, and are demonstrated in ImageNet classification\nand COCO detection on popular CPUs. \n\n"}
{"id": "1712.07107", "contents": "Title: Adversarial Examples: Attacks and Defenses for Deep Learning Abstract: With rapid progress and significant successes in a wide spectrum of\napplications, deep learning is being applied in many safety-critical\nenvironments. However, deep neural networks have been recently found vulnerable\nto well-designed input samples, called adversarial examples. Adversarial\nexamples are imperceptible to human but can easily fool deep neural networks in\nthe testing/deploying stage. The vulnerability to adversarial examples becomes\none of the major risks for applying deep neural networks in safety-critical\nenvironments. Therefore, attacks and defenses on adversarial examples draw\ngreat attention. In this paper, we review recent findings on adversarial\nexamples for deep neural networks, summarize the methods for generating\nadversarial examples, and propose a taxonomy of these methods. Under the\ntaxonomy, applications for adversarial examples are investigated. We further\nelaborate on countermeasures for adversarial examples and explore the\nchallenges and the potential solutions. \n\n"}
{"id": "1712.07195", "contents": "Title: Deep Regression Forests for Age Estimation Abstract: Age estimation from facial images is typically cast as a nonlinear regression\nproblem. The main challenge of this problem is the facial feature space w.r.t.\nages is heterogeneous, due to the large variation in facial appearance across\ndifferent persons of the same age and the non-stationary property of aging\npatterns. In this paper, we propose Deep Regression Forests (DRFs), an\nend-to-end model, for age estimation. DRFs connect the split nodes to a fully\nconnected layer of a convolutional neural network (CNN) and deal with\nheterogeneous data by jointly learning input-dependant data partitions at the\nsplit nodes and data abstractions at the leaf nodes. This joint learning\nfollows an alternating strategy: First, by fixing the leaf nodes, the split\nnodes as well as the CNN parameters are optimized by Back-propagation; Then, by\nfixing the split nodes, the leaf nodes are optimized by iterating a step-size\nfree and fast-converging update rule derived from Variational Bounding. We\nverify the proposed DRFs on three standard age estimation benchmarks and\nachieve state-of-the-art results on all of them. \n\n"}
{"id": "1712.07384", "contents": "Title: DeepFuse: A Deep Unsupervised Approach for Exposure Fusion with Extreme\n  Exposure Image Pairs Abstract: We present a novel deep learning architecture for fusing static\nmulti-exposure images. Current multi-exposure fusion (MEF) approaches use\nhand-crafted features to fuse input sequence. However, the weak hand-crafted\nrepresentations are not robust to varying input conditions. Moreover, they\nperform poorly for extreme exposure image pairs. Thus, it is highly desirable\nto have a method that is robust to varying input conditions and capable of\nhandling extreme exposure without artifacts. Deep representations have known to\nbe robust to input conditions and have shown phenomenal performance in a\nsupervised setting. However, the stumbling block in using deep learning for MEF\nwas the lack of sufficient training data and an oracle to provide the\nground-truth for supervision. To address the above issues, we have gathered a\nlarge dataset of multi-exposure image stacks for training and to circumvent the\nneed for ground truth images, we propose an unsupervised deep learning\nframework for MEF utilizing a no-reference quality metric as loss function. The\nproposed approach uses a novel CNN architecture trained to learn the fusion\noperation without reference ground truth image. The model fuses a set of common\nlow level features extracted from each image to generate artifact-free\nperceptually pleasing results. We perform extensive quantitative and\nqualitative evaluation and show that the proposed technique outperforms\nexisting state-of-the-art approaches for a variety of natural images. \n\n"}
{"id": "1712.08062", "contents": "Title: Note on Attacking Object Detectors with Adversarial Stickers Abstract: Deep learning has proven to be a powerful tool for computer vision and has\nseen widespread adoption for numerous tasks. However, deep learning algorithms\nare known to be vulnerable to adversarial examples. These adversarial inputs\nare created such that, when provided to a deep learning algorithm, they are\nvery likely to be mislabeled. This can be problematic when deep learning is\nused to assist in safety critical decisions. Recent research has shown that\nclassifiers can be attacked by physical adversarial examples under various\nphysical conditions. Given the fact that state-of-the-art objection detection\nalgorithms are harder to be fooled by the same set of adversarial examples,\nhere we show that these detectors can also be attacked by physical adversarial\nexamples. In this note, we briefly show both static and dynamic test results.\nWe design an algorithm that produces physical adversarial inputs, which can\nfool the YOLO object detector and can also attack Faster-RCNN with relatively\nhigh success rate based on transferability. Furthermore, our algorithm can\ncompress the size of the adversarial inputs to stickers that, when attached to\nthe targeted object, result in the detector either mislabeling or not detecting\nthe object a high percentage of the time. This note provides a small set of\nresults. Our upcoming paper will contain a thorough evaluation on other object\ndetectors, and will present the algorithm. \n\n"}
{"id": "1712.09491", "contents": "Title: Exploring the Space of Black-box Attacks on Deep Neural Networks Abstract: Existing black-box attacks on deep neural networks (DNNs) so far have largely\nfocused on transferability, where an adversarial instance generated for a\nlocally trained model can \"transfer\" to attack other learning models. In this\npaper, we propose novel Gradient Estimation black-box attacks for adversaries\nwith query access to the target model's class probabilities, which do not rely\non transferability. We also propose strategies to decouple the number of\nqueries required to generate each adversarial sample from the dimensionality of\nthe input. An iterative variant of our attack achieves close to 100%\nadversarial success rates for both targeted and untargeted attacks on DNNs. We\ncarry out extensive experiments for a thorough comparative evaluation of\nblack-box attacks and show that the proposed Gradient Estimation attacks\noutperform all transferability based black-box attacks we tested on both MNIST\nand CIFAR-10 datasets, achieving adversarial success rates similar to well\nknown, state-of-the-art white-box attacks. We also apply the Gradient\nEstimation attacks successfully against a real-world Content Moderation\nclassifier hosted by Clarifai. Furthermore, we evaluate black-box attacks\nagainst state-of-the-art defenses. We show that the Gradient Estimation attacks\nare very effective even against these defenses. \n\n"}
{"id": "1712.10215", "contents": "Title: ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for\n  3D Scans Abstract: We introduce ScanComplete, a novel data-driven approach for taking an\nincomplete 3D scan of a scene as input and predicting a complete 3D model along\nwith per-voxel semantic labels. The key contribution of our method is its\nability to handle large scenes with varying spatial extent, managing the cubic\ngrowth in data size as scene size increases. To this end, we devise a\nfully-convolutional generative 3D CNN model whose filter kernels are invariant\nto the overall scene size. The model can be trained on scene subvolumes but\ndeployed on arbitrarily large scenes at test time. In addition, we propose a\ncoarse-to-fine inference strategy in order to produce high-resolution output\nwhile also leveraging large input context sizes. In an extensive series of\nexperiments, we carefully evaluate different model design choices, considering\nboth deterministic and probabilistic models for completion and semantic\ninference. Our results show that we outperform other methods not only in the\nsize of the environments handled and processing efficiency, but also with\nregard to completion quality and semantic segmentation performance by a\nsignificant margin. \n\n"}
{"id": "1801.00101", "contents": "Title: Parameter-free online learning via model selection Abstract: We introduce an efficient algorithmic framework for model selection in online\nlearning, also known as parameter-free online learning. Departing from previous\nwork, which has focused on highly structured function classes such as nested\nballs in Hilbert space, we propose a generic meta-algorithm framework that\nachieves online model selection oracle inequalities under minimal structural\nassumptions. We give the first computationally efficient parameter-free\nalgorithms that work in arbitrary Banach spaces under mild smoothness\nassumptions; previous results applied only to Hilbert spaces. We further derive\nnew oracle inequalities for matrix classes, non-nested convex sets, and\n$\\mathbb{R}^{d}$ with generic regularizers. Finally, we generalize these\nresults by providing oracle inequalities for arbitrary non-linear classes in\nthe online supervised learning model. These results are all derived through a\nunified meta-algorithm scheme using a novel \"multi-scale\" algorithm for\nprediction with expert advice based on random playout, which may be of\nindependent interest. \n\n"}
{"id": "1801.00508", "contents": "Title: Depth-Adaptive Computational Policies for Efficient Visual Tracking Abstract: Current convolutional neural networks algorithms for video object tracking\nspend the same amount of computation for each object and video frame. However,\nit is harder to track an object in some frames than others, due to the varying\namount of clutter, scene complexity, amount of motion, and object's\ndistinctiveness against its background. We propose a depth-adaptive\nconvolutional Siamese network that performs video tracking adaptively at\nmultiple neural network depths. Parametric gating functions are trained to\ncontrol the depth of the convolutional feature extractor by minimizing a joint\nloss of computational cost and tracking error. Our network achieves accuracy\ncomparable to the state-of-the-art on the VOT2016 benchmark. Furthermore, our\nadaptive depth computation achieves higher accuracy for a given computational\ncost than traditional fixed-structure neural networks. The presented framework\nextends to other tasks that use convolutional neural networks and enables\ntrading speed for accuracy at runtime. \n\n"}
{"id": "1801.00553", "contents": "Title: Threat of Adversarial Attacks on Deep Learning in Computer Vision: A\n  Survey Abstract: Deep learning is at the heart of the current rise of machine learning and\nartificial intelligence. In the field of Computer Vision, it has become the\nworkhorse for applications ranging from self-driving cars to surveillance and\nsecurity. Whereas deep neural networks have demonstrated phenomenal success\n(often beyond human capabilities) in solving complex problems, recent studies\nshow that they are vulnerable to adversarial attacks in the form of subtle\nperturbations to inputs that lead a model to predict incorrect outputs. For\nimages, such perturbations are often too small to be perceptible, yet they\ncompletely fool the deep learning models. Adversarial attacks pose a serious\nthreat to the success of deep learning in practice. This fact has lead to a\nlarge influx of contributions in this direction. This article presents the\nfirst comprehensive survey on adversarial attacks on deep learning in Computer\nVision. We review the works that design adversarial attacks, analyze the\nexistence of such attacks and propose defenses against them. To emphasize that\nadversarial attacks are possible in practical conditions, we separately review\nthe contributions that evaluate adversarial attacks in the real-world\nscenarios. Finally, we draw on the literature to provide a broader outlook of\nthe research direction. \n\n"}
{"id": "1801.00553", "contents": "Title: Threat of Adversarial Attacks on Deep Learning in Computer Vision: A\n  Survey Abstract: Deep learning is at the heart of the current rise of machine learning and\nartificial intelligence. In the field of Computer Vision, it has become the\nworkhorse for applications ranging from self-driving cars to surveillance and\nsecurity. Whereas deep neural networks have demonstrated phenomenal success\n(often beyond human capabilities) in solving complex problems, recent studies\nshow that they are vulnerable to adversarial attacks in the form of subtle\nperturbations to inputs that lead a model to predict incorrect outputs. For\nimages, such perturbations are often too small to be perceptible, yet they\ncompletely fool the deep learning models. Adversarial attacks pose a serious\nthreat to the success of deep learning in practice. This fact has lead to a\nlarge influx of contributions in this direction. This article presents the\nfirst comprehensive survey on adversarial attacks on deep learning in Computer\nVision. We review the works that design adversarial attacks, analyze the\nexistence of such attacks and propose defenses against them. To emphasize that\nadversarial attacks are possible in practical conditions, we separately review\nthe contributions that evaluate adversarial attacks in the real-world\nscenarios. Finally, we draw on the literature to provide a broader outlook of\nthe research direction. \n\n"}
{"id": "1801.03002", "contents": "Title: DeepStyle: Multimodal Search Engine for Fashion and Interior Design Abstract: In this paper, we propose a multimodal search engine that combines visual and\ntextual cues to retrieve items from a multimedia database aesthetically similar\nto the query. The goal of our engine is to enable intuitive retrieval of\nfashion merchandise such as clothes or furniture. Existing search engines treat\ntextual input only as an additional source of information about the query image\nand do not correspond to the real-life scenario where the user looks for 'the\nsame shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those\nshortcomings by using a joint neural network architecture to model contextual\ndependencies between features of different modalities. We prove the robustness\nof this approach on two different challenging datasets of fashion items and\nfurniture where our DeepStyle engine outperforms baseline methods by 18-21% on\nthe tested datasets. Our search engine is commercially deployed and available\nthrough a Web-based application. \n\n"}
{"id": "1801.03049", "contents": "Title: Meta-Tracker: Fast and Robust Online Adaptation for Visual Object\n  Trackers Abstract: This paper improves state-of-the-art visual object trackers that use online\nadaptation. Our core contribution is an offline meta-learning-based method to\nadjust the initial deep networks used in online adaptation-based tracking. The\nmeta learning is driven by the goal of deep networks that can quickly be\nadapted to robustly model a particular target in future frames. Ideally the\nresulting models focus on features that are useful for future frames, and avoid\noverfitting to background clutter, small parts of the target, or noise. By\nenforcing a small number of update iterations during meta-learning, the\nresulting networks train significantly faster. We demonstrate this approach on\ntop of the high performance tracking approaches: tracking-by-detection based\nMDNet and the correlation based CREST. Experimental results on standard\nbenchmarks, OTB2015 and VOT2016, show that our meta-learned versions of both\ntrackers improve speed, accuracy, and robustness. \n\n"}
{"id": "1801.03399", "contents": "Title: Deep Supervision with Intermediate Concepts Abstract: Recent data-driven approaches to scene interpretation predominantly pose\ninference as an end-to-end black-box mapping, commonly performed by a\nConvolutional Neural Network (CNN). However, decades of work on perceptual\norganization in both human and machine vision suggests that there are often\nintermediate representations that are intrinsic to an inference task, and which\nprovide essential structure to improve generalization. In this work, we explore\nan approach for injecting prior domain structure into neural network training\nby supervising hidden layers of a CNN with intermediate concepts that normally\nare not observed in practice. We formulate a probabilistic framework which\nformalizes these notions and predicts improved generalization via this deep\nsupervision method. One advantage of this approach is that we are able to train\nonly from synthetic CAD renderings of cluttered scenes, where concept values\ncan be extracted, but apply the results to real images. Our implementation\nachieves the state-of-the-art performance of 2D/3D keypoint localization and\nimage classification on real image benchmarks, including KITTI, PASCAL VOC,\nPASCAL3D+, IKEA, and CIFAR100. We provide additional evidence that our approach\noutperforms alternative forms of supervision, such as multi-task networks. \n\n"}
{"id": "1801.03983", "contents": "Title: Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low\n  Resolution Action Recognition Abstract: A major emerging challenge is how to protect people's privacy as cameras and\ncomputer vision are increasingly integrated into our daily lives, including in\nsmart devices inside homes. A potential solution is to capture and record just\nthe minimum amount of information needed to perform a task of interest. In this\npaper, we propose a fully-coupled two-stream spatiotemporal architecture for\nreliable human action recognition on extremely low resolution (e.g., 12x16\npixel) videos. We provide an efficient method to extract spatial and temporal\nfeatures and to aggregate them into a robust feature representation for an\nentire action video sequence. We also consider how to incorporate high\nresolution videos during training in order to build better low resolution\naction recognition models. We evaluate on two publicly-available datasets,\nshowing significant improvements over the state-of-the-art. \n\n"}
{"id": "1801.04062", "contents": "Title: MINE: Mutual Information Neural Estimation Abstract: We argue that the estimation of mutual information between high dimensional\ncontinuous random variables can be achieved by gradient descent over neural\nnetworks. We present a Mutual Information Neural Estimator (MINE) that is\nlinearly scalable in dimensionality as well as in sample size, trainable\nthrough back-prop, and strongly consistent. We present a handful of\napplications on which MINE can be used to minimize or maximize mutual\ninformation. We apply MINE to improve adversarially trained generative models.\nWe also use MINE to implement Information Bottleneck, applying it to supervised\nclassification; our results demonstrate substantial improvement in flexibility\nand performance in these settings. \n\n"}
{"id": "1801.04065", "contents": "Title: Deep Stereo Matching with Explicit Cost Aggregation Sub-Architecture Abstract: Deep neural networks have shown excellent performance for stereo matching.\nMany efforts focus on the feature extraction and similarity measurement of the\nmatching cost computation step while less attention is paid on cost aggregation\nwhich is crucial for stereo matching. In this paper, we present a\nlearning-based cost aggregation method for stereo matching by a novel\nsub-architecture in the end-to-end trainable pipeline. We reformulate the cost\naggregation as a learning process of the generation and selection of cost\naggregation proposals which indicate the possible cost aggregation results. The\ncost aggregation sub-architecture is realized by a two-stream network: one for\nthe generation of cost aggregation proposals, the other for the selection of\nthe proposals. The criterion for the selection is determined by the low-level\nstructure information obtained from a light convolutional network. The\ntwo-stream network offers a global view guidance for the cost aggregation to\nrectify the mismatching value stemming from the limited view of the matching\ncost computation. The comprehensive experiments on challenge datasets such as\nKITTI and Scene Flow show that our method outperforms the state-of-the-art\nmethods. \n\n"}
{"id": "1801.04381", "contents": "Title: MobileNetV2: Inverted Residuals and Linear Bottlenecks Abstract: In this paper we describe a new mobile architecture, MobileNetV2, that\nimproves the state of the art performance of mobile models on multiple tasks\nand benchmarks as well as across a spectrum of different model sizes. We also\ndescribe efficient ways of applying these mobile models to object detection in\na novel framework we call SSDLite. Additionally, we demonstrate how to build\nmobile semantic segmentation models through a reduced form of DeepLabv3 which\nwe call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where\nthe input and output of the residual block are thin bottleneck layers opposite\nto traditional residual models which use expanded representations in the input\nan MobileNetV2 uses lightweight depthwise convolutions to filter features in\nthe intermediate expansion layer. Additionally, we find that it is important to\nremove non-linearities in the narrow layers in order to maintain\nrepresentational power. We demonstrate that this improves performance and\nprovide an intuition that led to this design. Finally, our approach allows\ndecoupling of the input/output domains from the expressiveness of the\ntransformation, which provides a convenient framework for further analysis. We\nmeasure our performance on Imagenet classification, COCO object detection, VOC\nimage segmentation. We evaluate the trade-offs between accuracy, and number of\noperations measured by multiply-adds (MAdd), as well as the number of\nparameters \n\n"}
{"id": "1801.05038", "contents": "Title: An octree cells occupancy geometric dimensionality descriptor for\n  massive on-server point cloud visualisation and classification Abstract: Lidar datasets are becoming more and more common. They are appreciated for\ntheir precise 3D nature, and have a wide range of applications, such as surface\nreconstruction, object detection, visualisation, etc. For all this\napplications, having additional semantic information per point has potential of\nincreasing the quality and the efficiency of the application. In the last\ndecade the use of Machine Learning and more specifically classification methods\nhave proved to be successful to create this semantic information. In this\nparadigm, the goal is to classify points into a set of given classes (for\ninstance tree, building, ground, other). Some of these methods use descriptors\n(also called feature) of a point to learn and predict its class. Designing the\ndescriptors is then the heart of these methods. Descriptors can be based on\npoints geometry and attributes, use contextual information, etc. Furthermore,\ndescriptors can be used by humans for easier visual understanding and sometimes\nfiltering. In this work we propose a new simple geometric descriptor that gives\ninformation about the implicit local dimensionality of the point cloud at\nvarious scale. For instance a tree seen from afar is more volumetric in nature\n(3D), yet locally each leaves is rather planar (2D). To do so we build an\noctree centred on the point to consider, and compare the variation of the\noccupancy of the cells across the levels of the octree. We compare this\ndescriptor with the state of the art dimensionality descriptor and show its\ninterest. We further test the descriptor for classification within the Point\nCloud Server, and demonstrate efficiency and correctness results. \n\n"}
{"id": "1801.05365", "contents": "Title: Learning Deep Features for One-Class Classification Abstract: We propose a deep learning-based solution for the problem of feature learning\nin one-class classification. The proposed method operates on top of a\nConvolutional Neural Network (CNN) of choice and produces descriptive features\nwhile maintaining a low intra-class variance in the feature space for the given\nclass. For this purpose two loss functions, compactness loss and\ndescriptiveness loss are proposed along with a parallel CNN architecture. A\ntemplate matching-based framework is introduced to facilitate the testing\nprocess. Extensive experiments on publicly available anomaly detection, novelty\ndetection and mobile active authentication datasets show that the proposed Deep\nOne-Class (DOC) classification method achieves significant improvements over\nthe state-of-the-art. \n\n"}
{"id": "1801.05574", "contents": "Title: Brenier approach for optimal transportation between a quasi-discrete\n  measure and a discrete measure Abstract: Correctly estimating the discrepancy between two data distributions has\nalways been an important task in Machine Learning. Recently, Cuturi proposed\nthe Sinkhorn distance which makes use of an approximate Optimal Transport cost\nbetween two distributions as a distance to describe distribution discrepancy.\nAlthough it has been successfully adopted in various machine learning\napplications (e.g. in Natural Language Processing and Computer Vision) since\nthen, the Sinkhorn distance also suffers from two unnegligible limitations. The\nfirst one is that the Sinkhorn distance only gives an approximation of the real\nWasserstein distance, the second one is the `divide by zero' problem which\noften occurs during matrix scaling when setting the entropy regularization\ncoefficient to a small value. In this paper, we introduce a new Brenier\napproach for calculating a more accurate Wasserstein distance between two\ndiscrete distributions, this approach successfully avoids the two limitations\nshown above for Sinkhorn distance and gives an alternative way for estimating\ndistribution discrepancy. \n\n"}
{"id": "1801.05912", "contents": "Title: On the influence of Dice loss function in multi-class organ segmentation\n  of abdominal CT using 3D fully convolutional networks Abstract: Deep learning-based methods achieved impressive results for the segmentation\nof medical images. With the development of 3D fully convolutional networks\n(FCNs), it has become feasible to produce improved results for multi-organ\nsegmentation of 3D computed tomography (CT) images. The results of multi-organ\nsegmentation using deep learning-based methods not only depend on the choice of\nnetworks architecture, but also strongly rely on the choice of loss function.\nIn this paper, we present a discussion on the influence of Dice-based loss\nfunctions for multi-class organ segmentation using a dataset of abdominal CT\nvolumes. We investigated three different types of weighting the Dice loss\nfunctions based on class label frequencies (uniform, simple and square) and\nevaluate their influence on segmentation accuracies. Furthermore, we compared\nthe influence of different initial learning rates. We achieved average Dice\nscores of 81.3%, 59.5% and 31.7% for uniform, simple and square types of\nweighting when the learning rate is 0.001, and 78.2%, 81.0% and 58.5% for each\nweighting when the learning rate is 0.01. Our experiments indicated a strong\nrelationship between class balancing weights and initial learning rate in\ntraining. \n\n"}
{"id": "1801.06519", "contents": "Title: Piggyback: Adapting a Single Network to Multiple Tasks by Learning to\n  Mask Weights Abstract: This work presents a method for adapting a single, fixed deep neural network\nto multiple tasks without affecting performance on already learned tasks. By\nbuilding upon ideas from network quantization and pruning, we learn binary\nmasks that piggyback on an existing network, or are applied to unmodified\nweights of that network to provide good performance on a new task. These masks\nare learned in an end-to-end differentiable fashion, and incur a low overhead\nof 1 bit per network parameter, per task. Even though the underlying network is\nfixed, the ability to mask individual weights allows for the learning of a\nlarge number of filters. We show performance comparable to dedicated fine-tuned\nnetworks for a variety of classification tasks, including those with large\ndomain shifts from the initial task (ImageNet), and a variety of network\narchitectures. Unlike prior work, we do not suffer from catastrophic forgetting\nor competition between tasks, and our performance is agnostic to task ordering.\nCode available at https://github.com/arunmallya/piggyback. \n\n"}
{"id": "1801.08614", "contents": "Title: Accurate Weakly Supervised Deep Lesion Segmentation on CT Scans:\n  Self-Paced 3D Mask Generation from RECIST Abstract: Volumetric lesion segmentation via medical imaging is a powerful means to\nprecisely assess multiple time-point lesion/tumor changes. Because manual 3D\nsegmentation is prohibitively time consuming and requires radiological\nexperience, current practices rely on an imprecise surrogate called response\nevaluation criteria in solid tumors (RECIST). Despite their coarseness, RECIST\nmarks are commonly found in current hospital picture and archiving systems\n(PACS), meaning they can provide a potentially powerful, yet extraordinarily\nchallenging, source of weak supervision for full 3D segmentation. Toward this\nend, we introduce a convolutional neural network based weakly supervised\nself-paced segmentation (WSSS) method to 1) generate the initial lesion\nsegmentation on the axial RECIST-slice; 2) learn the data distribution on\nRECIST-slices; 3) adapt to segment the whole volume slice by slice to finally\nobtain a volumetric segmentation. In addition, we explore how super-resolution\nimages (2~5 times beyond the physical CT imaging), generated from a proposed\nstacked generative adversarial network, can aid the WSSS performance. We employ\nthe DeepLesion dataset, a comprehensive CT-image lesion dataset of 32,735\nPACS-bookmarked findings, which include lesions, tumors, and lymph nodes of\nvarying sizes, categories, body regions and surrounding contexts. These are\ndrawn from 10,594 studies of 4,459 patients. We also validate on a lymph-node\ndataset, where 3D ground truth masks are available for all images. For the\nDeepLesion dataset, we report mean Dice coefficients of 93% on RECIST-slices\nand 76% in 3D lesion volumes. We further validate using a subjective user\nstudy, where an experienced radiologist accepted our WSSS-generated lesion\nsegmentation results with a high probability of 92.4%. \n\n"}
{"id": "1801.08616", "contents": "Title: DeepPap: Deep Convolutional Networks for Cervical Cell Classification Abstract: Automation-assisted cervical screening via Pap smear or liquid-based cytology\n(LBC) is a highly effective cell imaging based cancer detection tool, where\ncells are partitioned into \"abnormal\" and \"normal\" categories. However, the\nsuccess of most traditional classification methods relies on the presence of\naccurate cell segmentations. Despite sixty years of research in this field,\naccurate segmentation remains a challenge in the presence of cell clusters and\npathologies. Moreover, previous classification methods are only built upon the\nextraction of hand-crafted features, such as morphology and texture. This paper\naddresses these limitations by proposing a method to directly classify cervical\ncells - without prior segmentation - based on deep features, using\nconvolutional neural networks (ConvNets). First, the ConvNet is pre-trained on\na natural image dataset. It is subsequently fine-tuned on a cervical cell\ndataset consisting of adaptively re-sampled image patches coarsely centered on\nthe nuclei. In the testing phase, aggregation is used to average the prediction\nscores of a similar set of image patches. The proposed method is evaluated on\nboth Pap smear and LBC datasets. Results show that our method outperforms\nprevious algorithms in classification accuracy (98.3%), area under the curve\n(AUC) (0.99) values, and especially specificity (98.3%), when applied to the\nHerlev benchmark Pap smear dataset and evaluated using five-fold\ncross-validation. Similar superior performances are also achieved on the HEMLBC\n(H&E stained manual LBC) dataset. Our method is promising for the development\nof automation-assisted reading systems in primary cervical screening. \n\n"}
{"id": "1801.09097", "contents": "Title: Towards an Understanding of Neural Networks in Natural-Image Spaces Abstract: Two major uncertainties, dataset bias and adversarial examples, prevail in\nstate-of-the-art AI algorithms with deep neural networks. In this paper, we\npresent an intuitive explanation for these issues as well as an interpretation\nof the performance of deep networks in a natural-image space. The explanation\nconsists of two parts: the philosophy of neural networks and a hypothetical\nmodel of natural-image spaces. Following the explanation, we 1) demonstrate\nthat the values of training samples differ, 2) provide incremental boost to the\naccuracy of a CIFAR-10 classifier by introducing an additional \"random-noise\"\ncategory during training, 3) alleviate over-fitting thereby enhancing the\nrobustness against adversarial examples by detecting and excluding illusive\ntraining samples that are consistently misclassified. Our overall contribution\nis therefore twofold. First, while most existing algorithms treat data equally\nand have a strong appetite for more data, we demonstrate in contrast that an\nindividual datum can sometimes have disproportionate and counterproductive\ninfluence and that it is not always better to train neural networks with more\ndata. Next, we consider more thoughtful strategies by taking into account the\ngeometric and topological properties of natural-image spaces to which deep\nnetworks are applied. \n\n"}
{"id": "1801.09321", "contents": "Title: Document Image Classification with Intra-Domain Transfer Learning and\n  Stacked Generalization of Deep Convolutional Neural Networks Abstract: In this work, a region-based Deep Convolutional Neural Network framework is\nproposed for document structure learning. The contribution of this work\ninvolves efficient training of region based classifiers and effective\nensembling for document image classification. A primary level of `inter-domain'\ntransfer learning is used by exporting weights from a pre-trained VGG16\narchitecture on the ImageNet dataset to train a document classifier on whole\ndocument images. Exploiting the nature of region based influence modelling, a\nsecondary level of `intra-domain' transfer learning is used for rapid training\nof deep learning models for image segments. Finally, stacked generalization\nbased ensembling is utilized for combining the predictions of the base deep\nneural network models. The proposed method achieves state-of-the-art accuracy\nof 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks\nset by existing algorithms. \n\n"}
{"id": "1801.10068", "contents": "Title: Deep Adversarial Attention Alignment for Unsupervised Domain Adaptation:\n  the Benefit of Target Expectation Maximization Abstract: In this paper, we make two contributions to unsupervised domain adaptation\n(UDA) using the convolutional neural network (CNN). First, our approach\ntransfers knowledge in all the convolutional layers through attention\nalignment. Most previous methods align high-level representations, e.g.,\nactivations of the fully connected (FC) layers. In these methods, however, the\nconvolutional layers which underpin critical low-level domain knowledge cannot\nbe updated directly towards reducing domain discrepancy. Specifically, we\nassume that the discriminative regions in an image are relatively invariant to\nimage style changes. Based on this assumption, we propose an attention\nalignment scheme on all the target convolutional layers to uncover the\nknowledge shared by the source domain. Second, we estimate the posterior label\ndistribution of the unlabeled data for target network training. Previous\nmethods, which iteratively update the pseudo labels by the target network and\nrefine the target network by the updated pseudo labels, are vulnerable to label\nestimation errors. Instead, our approach uses category distribution to\ncalculate the cross-entropy loss for training, thereby ameliorating the error\naccumulation of the estimated labels. The two contributions allow our approach\nto outperform the state-of-the-art methods by +2.6% on the Office-31 dataset. \n\n"}
{"id": "1802.00470", "contents": "Title: Learning random-walk label propagation for weakly-supervised semantic\n  segmentation Abstract: Large-scale training for semantic segmentation is challenging due to the\nexpense of obtaining training data for this task relative to other vision\ntasks. We propose a novel training approach to address this difficulty. Given\ncheaply-obtained sparse image labelings, we propagate the sparse labels to\nproduce guessed dense labelings. A standard CNN-based segmentation network is\ntrained to mimic these labelings. The label-propagation process is defined via\nrandom-walk hitting probabilities, which leads to a differentiable\nparameterization with uncertainty estimates that are incorporated into our\nloss. We show that by learning the label-propagator jointly with the\nsegmentation predictor, we are able to effectively learn semantic edges given\nno direct edge supervision. Experiments also show that training a segmentation\nnetwork in this way outperforms the naive approach. \n\n"}
{"id": "1802.00614", "contents": "Title: Visual Interpretability for Deep Learning: a Survey Abstract: This paper reviews recent studies in understanding neural-network\nrepresentations and learning neural networks with interpretable/disentangled\nmiddle-layer representations. Although deep neural networks have exhibited\nsuperior performance in various tasks, the interpretability is always the\nAchilles' heel of deep neural networks. At present, deep neural networks obtain\nhigh discrimination power at the cost of low interpretability of their\nblack-box representations. We believe that high model interpretability may help\npeople to break several bottlenecks of deep learning, e.g., learning from very\nfew annotations, learning via human-computer communications at the semantic\nlevel, and semantically debugging network representations. We focus on\nconvolutional neural networks (CNNs), and we revisit the visualization of CNN\nrepresentations, methods of diagnosing representations of pre-trained CNNs,\napproaches for disentangling pre-trained CNN representations, learning of CNNs\nwith disentangled representations, and middle-to-end learning based on model\ninterpretability. Finally, we discuss prospective trends in explainable\nartificial intelligence. \n\n"}
{"id": "1802.01894", "contents": "Title: The steerable graph Laplacian and its application to filtering image\n  data-sets Abstract: In recent years, improvements in various image acquisition techniques gave\nrise to the need for adaptive processing methods, aimed particularly for large\ndatasets corrupted by noise and deformations. In this work, we consider\ndatasets of images sampled from a low-dimensional manifold (i.e. an\nimage-valued manifold), where the images can assume arbitrary planar rotations.\nTo derive an adaptive and rotation-invariant framework for processing such\ndatasets, we introduce a graph Laplacian (GL)-like operator over the dataset,\ntermed ${\\textit{steerable graph Laplacian}}$. Essentially, the steerable GL\nextends the standard GL by accounting for all (infinitely-many) planar\nrotations of all images. As it turns out, similarly to the standard GL, a\nproperly normalized steerable GL converges to the Laplace-Beltrami operator on\nthe low-dimensional manifold. However, the steerable GL admits an improved\nconvergence rate compared to the GL, where the improved convergence behaves as\nif the intrinsic dimension of the underlying manifold is lower by one.\nMoreover, it is shown that the steerable GL admits eigenfunctions of the form\nof Fourier modes (along the orbits of the images' rotations) multiplied by\neigenvectors of certain matrices, which can be computed efficiently by the FFT.\nFor image datasets corrupted by noise, we employ a subset of these\neigenfunctions to \"filter\" the dataset via a Fourier-like filtering scheme,\nessentially using all images and their rotations simultaneously. We demonstrate\nour filtering framework by de-noising simulated single-particle cryo-EM image\ndatasets. \n\n"}
{"id": "1802.02216", "contents": "Title: The Heart of an Image: Quantum Superposition and Entanglement in Visual\n  Perception Abstract: We analyse the way in which the principle that 'the whole is greater than the\nsum of its parts' manifests itself with phenomena of visual perception. For\nthis investigation we use insights and techniques coming from quantum\ncognition, and more specifically we are inspired by the correspondence of this\nprinciple with the phenomenon of the conjunction effect in human cognition. We\nidentify entities of meaning within artefacts of visual perception and rely on\nhow such entities are modelled for corpuses of texts such as the webpages of\nthe World-Wide Web for our study of how they appear in phenomena of visual\nperception. We identify concretely the conjunction effect in visual artefacts\nand analyse its structure in the example of a photograph. We also analyse\nquantum entanglement between different aspects of meaning in artefacts of\nvisual perception. We confirm its presence by showing that well elected\nexperiments on images retrieved accordingly by Google Images give rise to\nprobabilities and expectation values violating the Clauser Horne Shimony Holt\nversion of Bell's inequalities. We point out how this approach can lead to a\nmathematical description of the meaning content of a visual artefact such as a\nphotograph. \n\n"}
{"id": "1802.02604", "contents": "Title: An Unsupervised Learning Model for Deformable Medical Image Registration Abstract: We present a fast learning-based algorithm for deformable, pairwise 3D\nmedical image registration. Current registration methods optimize an objective\nfunction independently for each pair of images, which can be time-consuming for\nlarge data. We define registration as a parametric function, and optimize its\nparameters given a set of images from a collection of interest. Given a new\npair of scans, we can quickly compute a registration field by directly\nevaluating the function using the learned parameters. We model this function\nusing a convolutional neural network (CNN), and use a spatial transform layer\nto reconstruct one image from another while imposing smoothness constraints on\nthe registration field. The proposed method does not require supervised\ninformation such as ground truth registration fields or anatomical landmarks.\nWe demonstrate registration accuracy comparable to state-of-the-art 3D image\nregistration, while operating orders of magnitude faster in practice. Our\nmethod promises to significantly speed up medical image analysis and processing\npipelines, while facilitating novel directions in learning-based registration\nand its applications. Our code is available at\nhttps://github.com/balakg/voxelmorph . \n\n"}
{"id": "1802.02611", "contents": "Title: Encoder-Decoder with Atrous Separable Convolution for Semantic Image\n  Segmentation Abstract: Spatial pyramid pooling module or encode-decoder structure are used in deep\nneural networks for semantic segmentation task. The former networks are able to\nencode multi-scale contextual information by probing the incoming features with\nfilters or pooling operations at multiple rates and multiple effective\nfields-of-view, while the latter networks can capture sharper object boundaries\nby gradually recovering the spatial information. In this work, we propose to\ncombine the advantages from both methods. Specifically, our proposed model,\nDeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module\nto refine the segmentation results especially along object boundaries. We\nfurther explore the Xception model and apply the depthwise separable\nconvolution to both Atrous Spatial Pyramid Pooling and decoder modules,\nresulting in a faster and stronger encoder-decoder network. We demonstrate the\neffectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets,\nachieving the test set performance of 89.0\\% and 82.1\\% without any\npost-processing. Our paper is accompanied with a publicly available reference\nimplementation of the proposed models in Tensorflow at\n\\url{https://github.com/tensorflow/models/tree/master/research/deeplab}. \n\n"}
{"id": "1802.03043", "contents": "Title: PoTrojan: powerful neural-level trojan designs in deep learning models Abstract: With the popularity of deep learning (DL), artificial intelligence (AI) has\nbeen applied in many areas of human life. Neural network or artificial neural\nnetwork (NN), the main technique behind DL, has been extensively studied to\nfacilitate computer vision and natural language recognition. However, the more\nwe rely on information technology, the more vulnerable we are. That is,\nmalicious NNs could bring huge threat in the so-called coming AI era. In this\npaper, for the first time in the literature, we propose a novel approach to\ndesign and insert powerful neural-level trojans or PoTrojan in pre-trained NN\nmodels. Most of the time, PoTrojans remain inactive, not affecting the normal\nfunctions of their host NN models. PoTrojans could only be triggered in very\nrare conditions. Once activated, however, the PoTrojans could cause the host NN\nmodels to malfunction, either falsely predicting or classifying, which is a\nsignificant threat to human society of the AI era. We would explain the\nprinciples of PoTrojans and the easiness of designing and inserting them in\npre-trained deep learning models. PoTrojans doesn't modify the existing\narchitecture or parameters of the pre-trained models, without re-training.\nHence, the proposed method is very efficient. \n\n"}
{"id": "1802.03133", "contents": "Title: Batch Kalman Normalization: Towards Training Deep Neural Networks with\n  Micro-Batches Abstract: As an indispensable component, Batch Normalization (BN) has successfully\nimproved the training of deep neural networks (DNNs) with mini-batches, by\nnormalizing the distribution of the internal representation for each hidden\nlayer. However, the effectiveness of BN would diminish with scenario of\nmicro-batch (e.g., less than 10 samples in a mini-batch), since the estimated\nstatistics in a mini-batch are not reliable with insufficient samples. In this\npaper, we present a novel normalization method, called Batch Kalman\nNormalization (BKN), for improving and accelerating the training of DNNs,\nparticularly under the context of micro-batches. Specifically, unlike the\nexisting solutions treating each hidden layer as an isolated system, BKN treats\nall the layers in a network as a whole system, and estimates the statistics of\na certain layer by considering the distributions of all its preceding layers,\nmimicking the merits of Kalman Filtering. BKN has two appealing properties.\nFirst, it enables more stable training and faster convergence compared to\nprevious works. Second, training DNNs using BKN performs substantially better\nthan those using BN and its variants, especially when very small mini-batches\nare presented. On the image classification benchmark of ImageNet, using BKN\npowered networks we improve upon the best-published model-zoo results: reaching\n74.0% top-1 val accuracy for InceptionV2. More importantly, using BKN achieves\nthe comparable accuracy with extremely smaller batch size, such as 64 times\nsmaller on CIFAR-10/100 and 8 times smaller on ImageNet. \n\n"}
{"id": "1802.03446", "contents": "Title: Pros and Cons of GAN Evaluation Measures Abstract: Generative models, in particular generative adversarial networks (GANs), have\nreceived significant attention recently. A number of GAN variants have been\nproposed and have been utilized in many applications. Despite large strides in\nterms of theoretical progress, evaluating and comparing GANs remains a daunting\ntask. While several measures have been introduced, as of yet, there is no\nconsensus as to which measure best captures strengths and limitations of models\nand should be used for fair model comparison. As in other areas of computer\nvision and machine learning, it is critical to settle on one or few good\nmeasures to steer the progress in this field. In this paper, I review and\ncritically discuss more than 24 quantitative and 5 qualitative measures for\nevaluating generative models with a particular emphasis on GAN-derived models.\nI also provide a set of 7 desiderata followed by an evaluation of whether a\ngiven measure or a family of measures is compatible with them. \n\n"}
{"id": "1802.03584", "contents": "Title: Joint Learning for Pulmonary Nodule Segmentation, Attributes and\n  Malignancy Prediction Abstract: Refer to the literature of lung nodule classification, many studies adopt\nConvolutional Neural Networks (CNN) to directly predict the malignancy of lung\nnodules with original thoracic Computed Tomography (CT) and nodule location.\nHowever, these studies cannot tell how the CNN works in terms of predicting the\nmalignancy of the given nodule, e.g., it's hard to conclude that whether the\nregion within the nodule or the contextual information matters according to the\noutput of the CNN. In this paper, we propose an interpretable and multi-task\nlearning CNN -- Joint learning for \\textbf{P}ulmonary \\textbf{N}odule\n\\textbf{S}egmentation \\textbf{A}ttributes and \\textbf{M}alignancy\n\\textbf{P}rediction (PN-SAMP). It is able to not only accurately predict the\nmalignancy of lung nodules, but also provide semantic high-level attributes as\nwell as the areas of detected nodules. Moreover, the combination of nodule\nsegmentation, attributes and malignancy prediction is helpful to improve the\nperformance of each single task. In addition, inspired by the fact that\nradiologists often change window widths and window centers to help to make\ndecision on uncertain nodules, PN-SAMP mixes multiple WW/WC together to gain\ninformation for the raw CT input images. To verify the effectiveness of the\nproposed method, the evaluation is implemented on the public LIDC-IDRI dataset,\nwhich is one of the largest dataset for lung nodule malignancy prediction.\nExperiments indicate that the proposed PN-SAMP achieves significant improvement\nwith respect to lung nodule classification, and promising performance on lung\nnodule segmentation and attribute learning, compared with the-state-of-the-art\nmethods. \n\n"}
{"id": "1802.04557", "contents": "Title: Automatic localization and decoding of honeybee markers using deep\n  convolutional neural networks Abstract: The honeybee is a fascinating model animal to investigate how collective\nbehavior emerges from (inter-)actions of thousands of individuals. Bees may\nacquire unique memories throughout their lives. These experiences affect social\ninteractions even over large time frames. Tracking and identifying all bees in\nthe colony over their lifetimes therefore may likely shed light on the\ninterplay of individual differences and colony behavior. This paper proposes a\nsoftware pipeline based on two deep convolutional neural networks for the\nlocalization and decoding of custom binary markers that honeybees carry from\ntheir first to the last day in their life. We show that this approach\noutperforms similar systems proposed in recent literature. By opening this\nsoftware for the public, we hope that the resulting datasets will help\nadvancing the understanding of honeybee collective intelligence. \n\n"}
{"id": "1802.04962", "contents": "Title: Disjoint Multi-task Learning between Heterogeneous Human-centric Tasks Abstract: Human behavior understanding is arguably one of the most important mid-level\ncomponents in artificial intelligence. In order to efficiently make use of\ndata, multi-task learning has been studied in diverse computer vision tasks\nincluding human behavior understanding. However, multi-task learning relies on\ntask specific datasets and constructing such datasets can be cumbersome. It\nrequires huge amounts of data, labeling efforts, statistical consideration etc.\nIn this paper, we leverage existing single-task datasets for human action\nclassification and captioning data for efficient human behavior learning. Since\nthe data in each dataset has respective heterogeneous annotations, traditional\nmulti-task learning is not effective in this scenario. To this end, we propose\na novel alternating directional optimization method to efficiently learn from\nthe heterogeneous data. We demonstrate the effectiveness of our model and show\nperformance improvements on both classification and sentence retrieval tasks in\ncomparison to the models trained on each of the single-task datasets. \n\n"}
{"id": "1802.05451", "contents": "Title: Mapping Images to Scene Graphs with Permutation-Invariant Structured\n  Prediction Abstract: Machine understanding of complex images is a key goal of artificial\nintelligence. One challenge underlying this task is that visual scenes contain\nmultiple inter-related objects, and that global context plays an important role\nin interpreting the scene. A natural modeling framework for capturing such\neffects is structured prediction, which optimizes over complex labels, while\nmodeling within-label interactions. However, it is unclear what principles\nshould guide the design of a structured prediction model that utilizes the\npower of deep learning components. Here we propose a design principle for such\narchitectures that follows from a natural requirement of permutation\ninvariance. We prove a necessary and sufficient characterization for\narchitectures that follow this invariance, and discuss its implication on model\ndesign. Finally, we show that the resulting model achieves new state of the art\nresults on the Visual Genome scene graph labeling benchmark, outperforming all\nrecent approaches. \n\n"}
{"id": "1802.06367", "contents": "Title: Efficient Sparse-Winograd Convolutional Neural Networks Abstract: Convolutional Neural Networks (CNNs) are computationally intensive, which\nlimits their application on mobile devices. Their energy is dominated by the\nnumber of multiplies needed to perform the convolutions. Winograd's minimal\nfiltering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can\nreduce the operation count, but these two methods cannot be directly combined\n$-$ applying the Winograd transform fills in the sparsity in both the weights\nand the activations. We propose two modifications to Winograd-based CNNs to\nenable these methods to exploit sparsity. First, we move the ReLU operation\ninto the Winograd domain to increase the sparsity of the transformed\nactivations. Second, we prune the weights in the Winograd domain to exploit\nstatic weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet\ndatasets, our method reduces the number of multiplications by $10.4\\times$,\n$6.8\\times$ and $10.8\\times$ respectively with loss of accuracy less than\n$0.1\\%$, outperforming previous baselines by $2.0\\times$-$3.0\\times$. We also\nshow that moving ReLU to the Winograd domain allows more aggressive pruning. \n\n"}
{"id": "1802.06552", "contents": "Title: Are Generative Classifiers More Robust to Adversarial Attacks? Abstract: There is a rising interest in studying the robustness of deep neural network\nclassifiers against adversaries, with both advanced attack and defence\ntechniques being actively developed. However, most recent work focuses on\ndiscriminative classifiers, which only model the conditional distribution of\nthe labels given the inputs. In this paper, we propose and investigate the deep\nBayes classifier, which improves classical naive Bayes with conditional deep\ngenerative models. We further develop detection methods for adversarial\nexamples, which reject inputs with low likelihood under the generative model.\nExperimental results suggest that deep Bayes classifiers are more robust than\ndeep discriminative classifiers, and that the proposed detection methods are\neffective against many recently proposed attacks. \n\n"}
{"id": "1802.07129", "contents": "Title: Deep BCD-Net Using Identical Encoding-Decoding CNN Structures for\n  Iterative Image Recovery Abstract: In \"extreme\" computational imaging that collects extremely undersampled or\nnoisy measurements, obtaining an accurate image within a reasonable computing\ntime is challenging. Incorporating image mapping convolutional neural networks\n(CNN) into iterative image recovery has great potential to resolve this issue.\nThis paper 1) incorporates image mapping CNN using identical convolutional\nkernels in both encoders and decoders into a block coordinate descent (BCD)\nsignal recovery method and 2) applies alternating direction method of\nmultipliers to train the aforementioned image mapping CNN. We refer to the\nproposed recurrent network as BCD-Net using identical encoding-decoding CNN\nstructures. Numerical experiments show that, for a) denoising low\nsignal-to-noise-ratio images and b) extremely undersampled magnetic resonance\nimaging, the proposed BCD-Net achieves significantly more accurate image\nrecovery, compared to BCD-Net using distinct encoding-decoding structures\nand/or the conventional image recovery model using both wavelets and total\nvariation. \n\n"}
{"id": "1802.07447", "contents": "Title: Load Balanced GANs for Multi-view Face Image Synthesis Abstract: Multi-view face synthesis from a single image is an ill-posed problem and\noften suffers from serious appearance distortion. Producing photo-realistic and\nidentity preserving multi-view results is still a not well defined synthesis\nproblem. This paper proposes Load Balanced Generative Adversarial Networks\n(LB-GAN) to precisely rotate the yaw angle of an input face image to any\nspecified angle. LB-GAN decomposes the challenging synthesis problem into two\nwell constrained subtasks that correspond to a face normalizer and a face\neditor respectively. The normalizer first frontalizes an input image, and then\nthe editor rotates the frontalized image to a desired pose guided by a remote\ncode. In order to generate photo-realistic local details, the normalizer and\nthe editor are trained in a two-stage manner and regulated by a conditional\nself-cycle loss and an attention based L2 loss. Exhaustive experiments on\ncontrolled and uncontrolled environments demonstrate that the proposed method\nnot only improves the visual realism of multi-view synthetic images, but also\npreserves identity information well. \n\n"}
{"id": "1802.07589", "contents": "Title: Collaboratively Weighting Deep and Classic Representation via L2\n  Regularization for Image Classification Abstract: Deep convolutional neural networks provide a powerful feature learning\ncapability for image classification. The deep image features can be utilized to\ndeal with many image understanding tasks like image classification and object\nrecognition. However, the robustness obtained in one dataset can be hardly\nreproduced in the other domain, which leads to inefficient models far from\nstate-of-the-art. We propose a deep collaborative weight-based classification\n(DeepCWC) method to resolve this problem, by providing a novel option to fully\ntake advantage of deep features in classic machine learning. It firstly\nperforms the L2-norm based collaborative representation on the original images,\nas well as the deep features extracted by deep CNN models. Then, two distance\nvectors, obtained based on the pair of linear representations, are fused\ntogether via a novel collaborative weight. This collaborative weight enables\ndeep and classic representations to weigh each other. We observed the\ncomplementarity between two representations in a series of experiments on 10\nfacial and object datasets. The proposed DeepCWC produces very promising\nclassification results, and outperforms many other benchmark methods,\nespecially the ones claimed for Fashion-MNIST. The code is going to be\npublished in our public repository. \n\n"}
{"id": "1802.07623", "contents": "Title: Explanations based on the Missing: Towards Contrastive Explanations with\n  Pertinent Negatives Abstract: In this paper we propose a novel method that provides contrastive\nexplanations justifying the classification of an input by a black box\nclassifier such as a deep neural network. Given an input we find what should be\n%necessarily and minimally and sufficiently present (viz. important object\npixels in an image) to justify its classification and analogously what should\nbe minimally and necessarily \\emph{absent} (viz. certain background pixels). We\nargue that such explanations are natural for humans and are used commonly in\ndomains such as health care and criminology. What is minimally but critically\n\\emph{absent} is an important part of an explanation, which to the best of our\nknowledge, has not been explicitly identified by current explanation methods\nthat explain predictions of neural networks. We validate our approach on three\nreal datasets obtained from diverse domains; namely, a handwritten digits\ndataset MNIST, a large procurement fraud dataset and a brain activity strength\ndataset. In all three cases, we witness the power of our approach in generating\nprecise explanations that are also easy for human experts to understand and\nevaluate. \n\n"}
{"id": "1802.07796", "contents": "Title: Continuous Relaxation of MAP Inference: A Nonconvex Perspective Abstract: In this paper, we study a nonconvex continuous relaxation of MAP inference in\ndiscrete Markov random fields (MRFs). We show that for arbitrary MRFs, this\nrelaxation is tight, and a discrete stationary point of it can be easily\nreached by a simple block coordinate descent algorithm. In addition, we study\nthe resolution of this relaxation using popular gradient methods, and further\npropose a more effective solution using a multilinear decomposition framework\nbased on the alternating direction method of multipliers (ADMM). Experiments on\nmany real-world problems demonstrate that the proposed ADMM significantly\noutperforms other nonconvex relaxation based methods, and compares favorably\nwith state of the art MRF optimization algorithms in different settings. \n\n"}
{"id": "1802.08903", "contents": "Title: Product Kernel Interpolation for Scalable Gaussian Processes Abstract: Recent work shows that inference for Gaussian processes can be performed\nefficiently using iterative methods that rely only on matrix-vector\nmultiplications (MVMs). Structured Kernel Interpolation (SKI) exploits these\ntechniques by deriving approximate kernels with very fast MVMs. Unfortunately,\nsuch strategies suffer badly from the curse of dimensionality. We develop a new\ntechnique for MVM based learning that exploits product kernel structure. We\ndemonstrate that this technique is broadly applicable, resulting in linear\nrather than exponential runtime with dimension for SKI, as well as\nstate-of-the-art asymptotic complexity for multi-task GPs. \n\n"}
{"id": "1802.09064", "contents": "Title: Model Agnostic Time Series Analysis via Matrix Estimation Abstract: We propose an algorithm to impute and forecast a time series by transforming\nthe observed time series into a matrix, utilizing matrix estimation to recover\nmissing values and de-noise observed entries, and performing linear regression\nto make predictions. At the core of our analysis is a representation result,\nwhich states that for a large model class, the transformed time series matrix\nis (approximately) low-rank. In effect, this generalizes the widely used\nSingular Spectrum Analysis (SSA) in time series literature, and allows us to\nestablish a rigorous link between time series analysis and matrix estimation.\nThe key to establishing this link is constructing a Page matrix with\nnon-overlapping entries rather than a Hankel matrix as is commonly done in the\nliterature (e.g., SSA). This particular matrix structure allows us to provide\nfinite sample analysis for imputation and prediction, and prove the asymptotic\nconsistency of our method. Another salient feature of our algorithm is that it\nis model agnostic with respect to both the underlying time dynamics and the\nnoise distribution in the observations. The noise agnostic property of our\napproach allows us to recover the latent states when only given access to noisy\nand partial observations a la a Hidden Markov Model; e.g., recovering the\ntime-varying parameter of a Poisson process without knowing that the underlying\nprocess is Poisson. Furthermore, since our forecasting algorithm requires\nregression with noisy features, our approach suggests a matrix estimation based\nmethod - coupled with a novel, non-standard matrix estimation error metric - to\nsolve the error-in-variable regression problem, which could be of interest in\nits own right. Through synthetic and real-world datasets, we demonstrate that\nour algorithm outperforms standard software packages (including R libraries) in\nthe presence of missing data as well as high levels of noise. \n\n"}
{"id": "1802.09129", "contents": "Title: Multi-Evidence Filtering and Fusion for Multi-Label Classification,\n  Object Detection and Semantic Segmentation Based on Weakly Supervised\n  Learning Abstract: Supervised object detection and semantic segmentation require object or even\npixel level annotations. When there exist image level labels only, it is\nchallenging for weakly supervised algorithms to achieve accurate predictions.\nThe accuracy achieved by top weakly supervised algorithms is still\nsignificantly lower than their fully supervised counterparts. In this paper, we\npropose a novel weakly supervised curriculum learning pipeline for multi-label\nobject recognition, detection and semantic segmentation. In this pipeline, we\nfirst obtain intermediate object localization and pixel labeling results for\nthe training images, and then use such results to train task-specific deep\nnetworks in a fully supervised manner. The entire process consists of four\nstages, including object localization in the training images, filtering and\nfusing object instances, pixel labeling for the training images, and\ntask-specific network training. To obtain clean object instances in the\ntraining images, we propose a novel algorithm for filtering, fusing and\nclassifying object instances collected from multiple solution mechanisms. In\nthis algorithm, we incorporate both metric learning and density-based\nclustering to filter detected object instances. Experiments show that our\nweakly supervised pipeline achieves state-of-the-art results in multi-label\nimage classification as well as weakly supervised object detection and very\ncompetitive results in weakly supervised semantic segmentation on MS-COCO,\nPASCAL VOC 2007 and PASCAL VOC 2012. \n\n"}
{"id": "1802.10548", "contents": "Title: Using Deep Learning for Segmentation and Counting within Microscopy Data Abstract: Cell counting is a ubiquitous, yet tedious task that would greatly benefit\nfrom automation. From basic biological questions to clinical trials, cell\ncounts provide key quantitative feedback that drive research. Unfortunately,\ncell counting is most commonly a manual task and can be time-intensive. The\ntask is made even more difficult due to overlapping cells, existence of\nmultiple focal planes, and poor imaging quality, among other factors. Here, we\ndescribe a convolutional neural network approach, using a recently described\nfeature pyramid network combined with a VGG-style neural network, for\nsegmenting and subsequent counting of cells in a given microscopy image. \n\n"}
{"id": "1803.00227", "contents": "Title: WRPN & Apprentice: Methods for Training and Inference using\n  Low-Precision Numerics Abstract: Today's high performance deep learning architectures involve large models\nwith numerous parameters. Low precision numerics has emerged as a popular\ntechnique to reduce both the compute and memory requirements of these large\nmodels. However, lowering precision often leads to accuracy degradation. We\ndescribe three schemes whereby one can both train and do efficient inference\nusing low precision numerics without hurting accuracy. Finally, we describe an\nefficient hardware accelerator that can take advantage of the proposed low\nprecision numerics. \n\n"}
{"id": "1803.01129", "contents": "Title: OIL: Observational Imitation Learning Abstract: Recent work has explored the problem of autonomous navigation by imitating a\nteacher and learning an end-to-end policy, which directly predicts controls\nfrom raw images. However, these approaches tend to be sensitive to mistakes by\nthe teacher and do not scale well to other environments or vehicles. To this\nend, we propose Observational Imitation Learning (OIL), a novel imitation\nlearning variant that supports online training and automatic selection of\noptimal behavior by observing multiple imperfect teachers. We apply our\nproposed methodology to the challenging problems of autonomous driving and UAV\nracing. For both tasks, we utilize the Sim4CV simulator that enables the\ngeneration of large amounts of synthetic training data and also allows for\nonline learning and evaluation. We train a perception network to predict\nwaypoints from raw image data and use OIL to train another network to predict\ncontrols from these waypoints. Extensive experiments demonstrate that our\ntrained network outperforms its teachers, conventional imitation learning (IL)\nand reinforcement learning (RL) baselines and even humans in simulation. The\nproject website is available at https://sites.google.com/kaust.edu.sa/oil/ and\na video at https://youtu.be/_rhq8a0qgeg \n\n"}
{"id": "1803.01159", "contents": "Title: Enhancement of land-use change modeling using convolutional neural\n  networks and convolutional denoising autoencoders Abstract: The neighborhood effect is a key driving factor for the land-use change (LUC)\nprocess. This study applies convolutional neural networks (CNN) to capture\nneighborhood characteristics from satellite images and to enhance the\nperformance of LUC modeling. We develop a hybrid CNN model (conv-net) to\npredict the LU transition probability by combining satellite images and\ngeographical features. A spatial weight layer is designed to incorporate the\ndistance-decay characteristics of neighborhood effect into conv-net. As an\nalternative model, we also develop a hybrid convolutional denoising autoencoder\nand multi-layer perceptron model (CDAE-net), which specifically learns latent\nrepresentations from satellite images and denoises the image data. Finally, a\nDINAMICA-based cellular automata (CA) model simulates the LU pattern. The\nresults show that the convolutional-based models improve the modeling\nperformances compared with a model that accepts only the geographical features.\nOverall, conv-net outperforms CDAE-net in terms of LUC predictive performance.\nNonetheless, CDAE-net performs better when the data are noisy. \n\n"}
{"id": "1803.01485", "contents": "Title: Totally Looks Like - How Humans Compare, Compared to Machines Abstract: Perceptual judgment of image similarity by humans relies on rich internal\nrepresentations ranging from low-level features to high-level concepts, scene\nproperties and even cultural associations. However, existing methods and\ndatasets attempting to explain perceived similarity use stimuli which arguably\ndo not cover the full breadth of factors that affect human similarity\njudgments, even those geared toward this goal. We introduce a new dataset\ndubbed Totally-Looks-Like (TLL) after a popular entertainment website, which\ncontains images paired by humans as being visually similar. The dataset\ncontains 6016 image-pairs from the wild, shedding light upon a rich and diverse\nset of criteria employed by human beings. We conduct experiments to try to\nreproduce the pairings via features extracted from state-of-the-art deep\nconvolutional neural networks, as well as additional human experiments to\nverify the consistency of the collected data. Though we create conditions to\nartificially make the matching task increasingly easier, we show that\nmachine-extracted representations perform very poorly in terms of reproducing\nthe matching selected by humans. We discuss and analyze these results,\nsuggesting future directions for improvement of learned image representations. \n\n"}
{"id": "1803.01599", "contents": "Title: AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation Abstract: Supervised deep learning methods have shown promising results for the task of\nmonocular depth estimation; but acquiring ground truth is costly, and prone to\nnoise as well as inaccuracies. While synthetic datasets have been used to\ncircumvent above problems, the resultant models do not generalize well to\nnatural scenes due to the inherent domain shift. Recent adversarial approaches\nfor domain adaption have performed well in mitigating the differences between\nthe source and target domains. But these methods are mostly limited to a\nclassification setup and do not scale well for fully-convolutional\narchitectures. In this work, we propose AdaDepth - an unsupervised domain\nadaptation strategy for the pixel-wise regression task of monocular depth\nestimation. The proposed approach is devoid of above limitations through a)\nadversarial learning and b) explicit imposition of content consistency on the\nadapted target representation. Our unsupervised approach performs competitively\nwith other established approaches on depth estimation tasks and achieves\nstate-of-the-art results in a semi-supervised setting. \n\n"}
{"id": "1803.02544", "contents": "Title: Visual Explanations From Deep 3D Convolutional Neural Networks for\n  Alzheimer's Disease Classification Abstract: We develop three efficient approaches for generating visual explanations from\n3D convolutional neural networks (3D-CNNs) for Alzheimer's disease\nclassification. One approach conducts sensitivity analysis on hierarchical 3D\nimage segmentation, and the other two visualize network activations on a\nspatial map. Visual checks and a quantitative localization benchmark indicate\nthat all approaches identify important brain parts for Alzheimer's disease\ndiagnosis. Comparative analysis show that the sensitivity analysis based\napproach has difficulty handling loosely distributed cerebral cortex, and\napproaches based on visualization of activations are constrained by the\nresolution of the convolutional layer. The complementarity of these methods\nimproves the understanding of 3D-CNNs in Alzheimer's disease classification\nfrom different perspectives. \n\n"}
{"id": "1803.02579", "contents": "Title: Concurrent Spatial and Channel Squeeze & Excitation in Fully\n  Convolutional Networks Abstract: Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in\nimage segmentation for a plethora of applications. Architectural innovations\nwithin F-CNNs have mainly focused on improving spatial encoding or network\nconnectivity to aid gradient flow. In this paper, we explore an alternate\ndirection of recalibrating the feature maps adaptively, to boost meaningful\nfeatures, while suppressing weak ones. We draw inspiration from the recently\nproposed squeeze & excitation (SE) module for channel recalibration of feature\nmaps for image classification. Towards this end, we introduce three variants of\nSE modules for image segmentation, (i) squeezing spatially and exciting\nchannel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE)\nand (iii) concurrent spatial and channel squeeze & excitation (scSE). We\neffectively incorporate these SE modules within three different\nstate-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent\nimprovement of performance across all architectures, while minimally effecting\nmodel complexity. Evaluations are performed on two challenging applications:\nwhole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset)\nand organ segmentation on whole body contrast enhanced CT scans (Visceral\nDataset). \n\n"}
{"id": "1803.03317", "contents": "Title: Analysis of Hand Segmentation in the Wild Abstract: A large number of works in egocentric vision have concentrated on action and\nobject recognition. Detection and segmentation of hands in first-person videos,\nhowever, has less been explored. For many applications in this domain, it is\nnecessary to accurately segment not only hands of the camera wearer but also\nthe hands of others with whom he is interacting. Here, we take an in-depth look\nat the hand segmentation problem. In the quest for robust hand segmentation\nmethods, we evaluated the performance of the state of the art semantic\nsegmentation methods, off the shelf and fine-tuned, on existing datasets. We\nfine-tune RefineNet, a leading semantic segmentation method, for hand\nsegmentation and find that it does much better than the best contenders.\nExisting hand segmentation datasets are collected in the laboratory settings.\nTo overcome this limitation, we contribute by collecting two new datasets: a)\nEgoYouTubeHands including egocentric videos containing hands in the wild, and\nb) HandOverFace to analyze the performance of our models in presence of similar\nappearance occlusions. We further explore whether conditional random fields can\nhelp refine generated hand segmentations. To demonstrate the benefit of\naccurate hand maps, we train a CNN for hand-based activity recognition and\nachieve higher accuracy when a CNN was trained using hand maps produced by the\nfine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset for\nfine-grained action recognition and show that an accuracy of 58.6% can be\nachieved by just looking at a single hand pose which is much better than the\nchance level (12.5%). \n\n"}
{"id": "1803.03764", "contents": "Title: Variance Networks: When Expectation Does Not Meet Your Expectations Abstract: Ordinary stochastic neural networks mostly rely on the expected values of\ntheir weights to make predictions, whereas the induced noise is mostly used to\ncapture the uncertainty, prevent overfitting and slightly boost the performance\nthrough test-time averaging. In this paper, we introduce variance layers, a\ndifferent kind of stochastic layers. Each weight of a variance layer follows a\nzero-mean distribution and is only parameterized by its variance. We show that\nsuch layers can learn surprisingly well, can serve as an efficient exploration\ntool in reinforcement learning tasks and provide a decent defense against\nadversarial attacks. We also show that a number of conventional Bayesian neural\nnetworks naturally converge to such zero-mean posteriors. We observe that in\nthese cases such zero-mean parameterization leads to a much better training\nobjective than conventional parameterizations where the mean is being learned. \n\n"}
{"id": "1803.03857", "contents": "Title: Learning from Noisy Web Data with Category-level Supervision Abstract: As tons of photos are being uploaded to public websites (e.g., Flickr, Bing,\nand Google) every day, learning from web data has become an increasingly\npopular research direction because of freely available web resources, which is\nalso referred to as webly supervised learning. Nevertheless, the performance\ngap between webly supervised learning and traditional supervised learning is\nstill very large, owning to the label noise of web data. To be exact, the\nlabels of images crawled from public websites are very noisy and often\ninaccurate. Some existing works tend to facilitate learning from web data with\nthe aid of extra information, such as augmenting or purifying web data by\nvirtue of instance-level supervision, which is usually in demand of heavy\nmanual annotation. Instead, we propose to tackle the label noise by leveraging\nmore accessible category-level supervision. In particular, we build our method\nupon variational autoencoder (VAE), in which the classification network is\nattached on the hidden layer of VAE in a way that the classification network\nand VAE can jointly leverage the category-level hybrid semantic information.\nThe effectiveness of our proposed method is clearly demonstrated by extensive\nexperiments on three benchmark datasets. \n\n"}
{"id": "1803.04376", "contents": "Title: Discriminability objective for training descriptive captions Abstract: One property that remains lacking in image captions generated by contemporary\nmethods is discriminability: being able to tell two images apart given the\ncaption for one of them. We propose a way to improve this aspect of caption\ngeneration. By incorporating into the captioning training objective a loss\ncomponent directly related to ability (by a machine) to disambiguate\nimage/caption matches, we obtain systems that produce much more discriminative\ncaption, according to human evaluation. Remarkably, our approach leads to\nimprovement in other aspects of generated captions, reflected by a battery of\nstandard scores such as BLEU, SPICE etc. Our approach is modular and can be\napplied to a variety of model/loss combinations commonly proposed for image\ncaptioning. \n\n"}
{"id": "1803.04837", "contents": "Title: Learning the Joint Representation of Heterogeneous Temporal Events for\n  Clinical Endpoint Prediction Abstract: The availability of a large amount of electronic health records (EHR)\nprovides huge opportunities to improve health care service by mining these\ndata. One important application is clinical endpoint prediction, which aims to\npredict whether a disease, a symptom or an abnormal lab test will happen in the\nfuture according to patients' history records. This paper develops deep\nlearning techniques for clinical endpoint prediction, which are effective in\nmany practical applications. However, the problem is very challenging since\npatients' history records contain multiple heterogeneous temporal events such\nas lab tests, diagnosis, and drug administrations. The visiting patterns of\ndifferent types of events vary significantly, and there exist complex nonlinear\nrelationships between different events. In this paper, we propose a novel model\nfor learning the joint representation of heterogeneous temporal events. The\nmodel adds a new gate to control the visiting rates of different events which\neffectively models the irregular patterns of different events and their\nnonlinear correlations. Experiment results with real-world clinical data on the\ntasks of predicting death and abnormal lab tests prove the effectiveness of our\nproposed approach over competitive baselines. \n\n"}
{"id": "1803.05848", "contents": "Title: Towards Clinical Diagnosis: Automated Stroke Lesion Segmentation on\n  Multimodal MR Image Using Convolutional Neural Network Abstract: The patient with ischemic stroke can benefit most from the earliest possible\ndefinitive diagnosis. While the high quality medical resources are quite scarce\nacross the globe, an automated diagnostic tool is expected in analyzing the\nmagnetic resonance (MR) images to provide reference in clinical diagnosis. In\nthis paper, we propose a deep learning method to automatically segment ischemic\nstroke lesions from multi-modal MR images. By using atrous convolution and\nglobal convolution network, our proposed residual-structured fully\nconvolutional network (Res-FCN) is able to capture features from large\nreceptive fields. The network architecture is validated on a large dataset of\n212 clinically acquired multi-modal MR images, which is shown to achieve a mean\ndice coefficient of 0.645 with a mean number of false negative lesions of\n1.515. The false negatives can reach a value that close to a common medical\nimage doctor, making it exceptive for a real clinical application. \n\n"}
{"id": "1803.06252", "contents": "Title: Joint Recognition of Handwritten Text and Named Entities with a Neural\n  End-to-end Model Abstract: When extracting information from handwritten documents, text transcription\nand named entity recognition are usually faced as separate subsequent tasks.\nThis has the disadvantage that errors in the first module affect heavily the\nperformance of the second module. In this work we propose to do both tasks\njointly, using a single neural network with a common architecture used for\nplain text recognition. Experimentally, the work has been tested on a\ncollection of historical marriage records. Results of experiments are presented\nto show the effect on the performance for different configurations: different\nways of encoding the information, doing or not transfer learning and processing\nat text line or multi-line region level. The results are comparable to state of\nthe art reported in the ICDAR 2017 Information Extraction competition, even\nthough the proposed technique does not use any dictionaries, language modeling\nor post processing. \n\n"}
{"id": "1803.06541", "contents": "Title: Adaptive strategy for superpixel-based region-growing image segmentation Abstract: This work presents a region-growing image segmentation approach based on\nsuperpixel decomposition. From an initial contour-constrained over-segmentation\nof the input image, the image segmentation is achieved by iteratively merging\nsimilar superpixels into regions. This approach raises two key issues: (1) how\nto compute the similarity between superpixels in order to perform accurate\nmerging and (2) in which order those superpixels must be merged together. In\nthis perspective, we firstly introduce a robust adaptive multi-scale superpixel\nsimilarity in which region comparisons are made both at content and common\nborder level. Secondly, we propose a global merging strategy to efficiently\nguide the region merging process. Such strategy uses an adpative merging\ncriterion to ensure that best region aggregations are given highest priorities.\nThis allows to reach a final segmentation into consistent regions with strong\nboundary adherence. We perform experiments on the BSDS500 image dataset to\nhighlight to which extent our method compares favorably against other\nwell-known image segmentation algorithms. The obtained results demonstrate the\npromising potential of the proposed approach. \n\n"}
{"id": "1803.06641", "contents": "Title: Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains Abstract: Despite the recent success of stereo matching with convolutional neural\nnetworks (CNNs), it remains arduous to generalize a pre-trained deep stereo\nmodel to a novel domain. A major difficulty is to collect accurate ground-truth\ndisparities for stereo pairs in the target domain. In this work, we propose a\nself-adaptation approach for CNN training, utilizing both synthetic training\ndata (with ground-truth disparities) and stereo pairs in the new domain\n(without ground-truths). Our method is driven by two empirical observations. By\nfeeding real stereo pairs of different domains to stereo models pre-trained\nwith synthetic data, we see that: i) a pre-trained model does not generalize\nwell to the new domain, producing artifacts at boundaries and ill-posed\nregions; however, ii) feeding an up-sampled stereo pair leads to a disparity\nmap with extra details. To avoid i) while exploiting ii), we formulate an\niterative optimization problem with graph Laplacian regularization. At each\niteration, the CNN adapts itself better to the new domain: we let the CNN learn\nits own higher-resolution output; at the meanwhile, a graph Laplacian\nregularization is imposed to discriminatively keep the desired edges while\nsmoothing out the artifacts. We demonstrate the effectiveness of our method in\ntwo domains: daily scenes collected by smartphone cameras, and street views\ncaptured in a driving car. \n\n"}
{"id": "1803.07624", "contents": "Title: Dynamic Filtering with Large Sampling Field for ConvNets Abstract: We propose a dynamic filtering strategy with large sampling field for\nConvNets (LS-DFN), where the position-specific kernels learn from not only the\nidentical position but also multiple sampled neighbor regions. During sampling,\nresidual learning is introduced to ease training and an attention mechanism is\napplied to fuse features from different samples. Such multiple samples enlarge\nthe kernels' receptive fields significantly without requiring more parameters.\nWhile LS-DFN inherits the advantages of DFN, namely avoiding feature map\nblurring by position-wise kernels while keeping translation invariance, it also\nefficiently alleviates the overfitting issue caused by much more parameters\nthan normal CNNs. Our model is efficient and can be trained end-to-end via\nstandard back-propagation. We demonstrate the merits of our LS-DFN on both\nsparse and dense prediction tasks involving object detection, semantic\nsegmentation, and flow estimation. Our results show LS-DFN enjoys stronger\nrecognition abilities in object detection and semantic segmentation tasks on\nVOC benchmark and sharper responses in flow estimation on FlyingChairs dataset\ncompared to strong baselines. \n\n"}
{"id": "1803.08225", "contents": "Title: PersonLab: Person Pose Estimation and Instance Segmentation with a\n  Bottom-Up, Part-Based, Geometric Embedding Model Abstract: We present a box-free bottom-up approach for the tasks of pose estimation and\ninstance segmentation of people in multi-person images using an efficient\nsingle-shot model. The proposed PersonLab model tackles both semantic-level\nreasoning and object-part associations using part-based modeling. Our model\nemploys a convolutional network which learns to detect individual keypoints and\npredict their relative displacements, allowing us to group keypoints into\nperson pose instances. Further, we propose a part-induced geometric embedding\ndescriptor which allows us to associate semantic person pixels with their\ncorresponding person instance, delivering instance-level person segmentations.\nOur system is based on a fully-convolutional architecture and allows for\nefficient inference, with runtime essentially independent of the number of\npeople present in the scene. Trained on COCO data alone, our system achieves\nCOCO test-dev keypoint average precision of 0.665 using single-scale inference\nand 0.687 using multi-scale inference, significantly outperforming all previous\nbottom-up pose estimation systems. We are also the first bottom-up method to\nreport competitive results for the person class in the COCO instance\nsegmentation task, achieving a person category average precision of 0.417. \n\n"}
{"id": "1803.08435", "contents": "Title: Guided Image Inpainting: Replacing an Image Region by Pulling Content\n  from Another Image Abstract: Deep generative models have shown success in automatically synthesizing\nmissing image regions using surrounding context. However, users cannot directly\ndecide what content to synthesize with such approaches. We propose an\nend-to-end network for image inpainting that uses a different image to guide\nthe synthesis of new content to fill the hole. A key challenge addressed by our\napproach is synthesizing new content in regions where the guidance image and\nthe context of the original image are inconsistent. We conduct four studies\nthat demonstrate our results yield more realistic image inpainting results over\nseven baselines. \n\n"}
{"id": "1803.08457", "contents": "Title: Clustering-driven Deep Embedding with Pairwise Constraints Abstract: Recently, there has been increasing interest to leverage the competence of\nneural networks to analyze data. In particular, new clustering methods that\nemploy deep embeddings have been presented. In this paper, we depart from\ncentroid-based models and suggest a new framework, called Clustering-driven\ndeep embedding with PAirwise Constraints (CPAC), for non-parametric clustering\nusing a neural network. We present a clustering-driven embedding based on a\nSiamese network that encourages pairs of data points to output similar\nrepresentations in the latent space. Our pair-based model allows augmenting the\ninformation with labeled pairs to constitute a semi-supervised framework. Our\napproach is based on analyzing the losses associated with each pair to refine\nthe set of constraints. We show that clustering performance increases when\nusing this scheme, even with a limited amount of user queries. We demonstrate\nhow our architecture is adapted for various types of data and present the first\ndeep framework to cluster 3D shapes. \n\n"}
{"id": "1803.08494", "contents": "Title: Group Normalization Abstract: Batch Normalization (BN) is a milestone technique in the development of deep\nlearning, enabling various networks to train. However, normalizing along the\nbatch dimension introduces problems --- BN's error increases rapidly when the\nbatch size becomes smaller, caused by inaccurate batch statistics estimation.\nThis limits BN's usage for training larger models and transferring features to\ncomputer vision tasks including detection, segmentation, and video, which\nrequire small batches constrained by memory consumption. In this paper, we\npresent Group Normalization (GN) as a simple alternative to BN. GN divides the\nchannels into groups and computes within each group the mean and variance for\nnormalization. GN's computation is independent of batch sizes, and its accuracy\nis stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN\nhas 10.6% lower error than its BN counterpart when using a batch size of 2;\nwhen using typical batch sizes, GN is comparably good with BN and outperforms\nother normalization variants. Moreover, GN can be naturally transferred from\npre-training to fine-tuning. GN can outperform its BN-based counterparts for\nobject detection and segmentation in COCO, and for video classification in\nKinetics, showing that GN can effectively replace the powerful BN in a variety\nof tasks. GN can be easily implemented by a few lines of code in modern\nlibraries. \n\n"}
{"id": "1803.09492", "contents": "Title: Latency and Throughput Characterization of Convolutional Neural Networks\n  for Mobile Computer Vision Abstract: We study performance characteristics of convolutional neural networks (CNN)\nfor mobile computer vision systems. CNNs have proven to be a powerful and\nefficient approach to implement such systems. However, the system performance\ndepends largely on the utilization of hardware accelerators, which are able to\nspeed up the execution of the underlying mathematical operations tremendously\nthrough massive parallelism. Our contribution is performance characterization\nof multiple CNN-based models for object recognition and detection with several\ndifferent hardware platforms and software frameworks, using both local\n(on-device) and remote (network-side server) computation. The measurements are\nconducted using real workloads and real processing platforms. On the platform\nside, we concentrate especially on TensorFlow and TensorRT. Our measurements\ninclude embedded processors found on mobile devices and high-performance\nprocessors that can be used on the network side of mobile systems. We show that\nthere exists significant latency--throughput trade-offs but the behavior is\nvery complex. We demonstrate and discuss several factors that affect the\nperformance and yield this complex behavior. \n\n"}
{"id": "1803.10590", "contents": "Title: Feed-forward Uncertainty Propagation in Belief and Neural Networks Abstract: We propose a feed-forward inference method applicable to belief and neural\nnetworks. In a belief network, the method estimates an approximate factorized\nposterior of all hidden units given the input. In neural networks the method\npropagates uncertainty of the input through all the layers. In neural networks\nwith injected noise, the method analytically takes into account uncertainties\nresulting from this noise. Such feed-forward analytic propagation is\ndifferentiable in parameters and can be trained end-to-end. Compared to\nstandard NN, which can be viewed as propagating only the means, we propagate\nthe mean and variance. The method can be useful in all scenarios that require\nknowledge of the neuron statistics, e.g. when dealing with uncertain inputs,\nconsidering sigmoid activations as probabilities of Bernoulli units, training\nthe models regularized by injected noise (dropout) or estimating activation\nstatistics over the dataset (as needed for normalization methods). In the\nexperiments we show the possible utility of the method in all these tasks as\nwell as its current limitations. \n\n"}
{"id": "1803.10630", "contents": "Title: Person re-identification with fusion of hand-crafted and deep pose-based\n  body region features Abstract: Person re-identification (re-ID) aims to accurately re- trieve a person from\na large-scale database of images cap- tured across multiple cameras. Existing\nworks learn deep representations using a large training subset of unique per-\nsons. However, identifying unseen persons is critical for a good re-ID\nalgorithm. Moreover, the misalignment be- tween person crops to detection\nerrors or pose variations leads to poor feature matching. In this work, we\npresent a fusion of handcrafted features and deep feature representa- tion\nlearned using multiple body parts to complement the global body features that\nachieves high performance on un- seen test images. Pose information is used to\ndetect body regions that are passed through Convolutional Neural Net- works\n(CNN) to guide feature learning. Finally, a metric learning step enables robust\ndistance matching on a dis- criminative subspace. Experimental results on 4\npopular re-ID benchmark datasets namely VIPer, DukeMTMC-reID, Market-1501 and\nCUHK03 show that the proposed method achieves state-of-the-art performance in\nimage-based per- son re-identification. \n\n"}
{"id": "1803.11095", "contents": "Title: Mining on Manifolds: Metric Learning without Labels Abstract: In this work we present a novel unsupervised framework for hard training\nexample mining. The only input to the method is a collection of images relevant\nto the target application and a meaningful initial representation, provided\ne.g. by pre-trained CNN. Positive examples are distant points on a single\nmanifold, while negative examples are nearby points on different manifolds.\nBoth types of examples are revealed by disagreements between Euclidean and\nmanifold similarities. The discovered examples can be used in training with any\ndiscriminative loss. The method is applied to unsupervised fine-tuning of\npre-trained networks for fine-grained classification and particular object\nretrieval. Our models are on par or are outperforming prior models that are\nfully or partially supervised. \n\n"}
{"id": "1803.11157", "contents": "Title: Security Consideration For Deep Learning-Based Image Forensics Abstract: Recently, image forensics community has paied attention to the research on\nthe design of effective algorithms based on deep learning technology and facts\nproved that combining the domain knowledge of image forensics and deep learning\nwould achieve more robust and better performance than the traditional schemes.\nInstead of improving it, in this paper, the safety of deep learning based\nmethods in the field of image forensics is taken into account. To the best of\nour knowledge, this is a first work focusing on this topic. Specifically, we\nexperimentally find that the method using deep learning would fail when adding\nthe slight noise into the images (adversarial images). Furthermore, two kinds\nof strategys are proposed to enforce security of deep learning-based method.\nFirstly, an extra penalty term to the loss function is added, which is referred\nto the 2-norm of the gradient of the loss with respect to the input images, and\nthen an novel training method are adopt to train the model by fusing the normal\nand adversarial images. Experimental results show that the proposed algorithm\ncan achieve good performance even in the case of adversarial images and provide\na safety consideration for deep learning-based image forensics \n\n"}
{"id": "1803.11182", "contents": "Title: Towards Open-Set Identity Preserving Face Synthesis Abstract: We propose a framework based on Generative Adversarial Networks to\ndisentangle the identity and attributes of faces, such that we can conveniently\nrecombine different identities and attributes for identity preserving face\nsynthesis in open domains. Previous identity preserving face synthesis\nprocesses are largely confined to synthesizing faces with known identities that\nare already in the training dataset. To synthesize a face with identity outside\nthe training dataset, our framework requires one input image of that subject to\nproduce an identity vector, and any other input face image to extract an\nattribute vector capturing, e.g., pose, emotion, illumination, and even the\nbackground. We then recombine the identity vector and the attribute vector to\nsynthesize a new face of the subject with the extracted attribute. Our proposed\nframework does not need to annotate the attributes of faces in any way. It is\ntrained with an asymmetric loss function to better preserve the identity and\nstabilize the training process. It can also effectively leverage large amounts\nof unlabeled training face images to further improve the fidelity of the\nsynthesized faces for subjects that are not presented in the labeled training\nface dataset. Our experiments demonstrate the efficacy of the proposed\nframework. We also present its usage in a much broader set of applications\nincluding face frontalization, face attribute morphing, and face adversarial\nexample detection. \n\n"}
{"id": "1803.11232", "contents": "Title: Euphrates: Algorithm-SoC Co-Design for Low-Power Mobile Continuous\n  Vision Abstract: Continuous computer vision (CV) tasks increasingly rely on convolutional\nneural networks (CNN). However, CNNs have massive compute demands that far\nexceed the performance and energy constraints of mobile devices. In this paper,\nwe propose and develop an algorithm-architecture co-designed system, Euphrates,\nthat simultaneously improves the energy-efficiency and performance of\ncontinuous vision tasks.\n  Our key observation is that changes in pixel data between consecutive frames\nrepresents visual motion. We first propose an algorithm that leverages this\nmotion information to relax the number of expensive CNN inferences required by\ncontinuous vision applications. We co-design a mobile System-on-a-Chip (SoC)\narchitecture to maximize the efficiency of the new algorithm. The key to our\narchitectural augmentation is to co-optimize different SoC IP blocks in the\nvision pipeline collectively. Specifically, we propose to expose the motion\ndata that is naturally generated by the Image Signal Processor (ISP) early in\nthe vision pipeline to the CNN engine. Measurement and synthesis results show\nthat Euphrates achieves up to 66% SoC-level energy savings (4 times for the\nvision computations), with only 1% accuracy loss. \n\n"}
{"id": "1803.11366", "contents": "Title: Disentangling Features in 3D Face Shapes for Joint Face Reconstruction\n  and Recognition Abstract: This paper proposes an encoder-decoder network to disentangle shape features\nduring 3D face reconstruction from single 2D images, such that the tasks of\nreconstructing accurate 3D face shapes and learning discriminative shape\nfeatures for face recognition can be accomplished simultaneously. Unlike\nexisting 3D face reconstruction methods, our proposed method directly regresses\ndense 3D face shapes from single 2D images, and tackles identity and residual\n(i.e., non-identity) components in 3D face shapes explicitly and separately\nbased on a composite 3D face shape model with latent representations. We devise\na training process for the proposed network with a joint loss measuring both\nface identification error and 3D face shape reconstruction error. To construct\ntraining data we develop a method for fitting 3D morphable model (3DMM) to\nmultiple 2D images of a subject. Comprehensive experiments have been done on\nMICC, BU3DFE, LFW and YTF databases. The results show that our method expands\nthe capacity of 3DMM for capturing discriminative shape features and facial\ndetail, and thus outperforms existing methods both in 3D face reconstruction\naccuracy and in face recognition accuracy. \n\n"}
{"id": "1804.00248", "contents": "Title: SampleAhead: Online Classifier-Sampler Communication for Learning from\n  Synthesized Data Abstract: State-of-the-art techniques of artificial intelligence, in particular deep\nlearning, are mostly data-driven. However, collecting and manually labeling a\nlarge scale dataset is both difficult and expensive. A promising alternative is\nto introduce synthesized training data, so that the dataset size can be\nsignificantly enlarged with little human labor. But, this raises an important\nproblem in active vision: given an {\\bf infinite} data space, how to\neffectively sample a {\\bf finite} subset to train a visual classifier? This\npaper presents an approach for learning from synthesized data effectively. The\nmotivation is straightforward -- increasing the probability of seeing difficult\ntraining data. We introduce a module named {\\bf SampleAhead} to formulate the\nlearning process into an online communication between a {\\em classifier} and a\n{\\em sampler}, and update them iteratively. In each round, we adjust the\nsampling distribution according to the classification results, and train the\nclassifier using the data sampled from the updated distribution. Experiments\nare performed by introducing synthesized images rendered from ShapeNet models\nto assist PASCAL3D+ classification. Our approach enjoys higher classification\naccuracy, especially in the scenario of a limited number of training samples.\nThis demonstrates its efficiency in exploring the infinite data space. \n\n"}
{"id": "1804.00292", "contents": "Title: EarthMapper: A Tool Box for the Semantic Segmentation of Remote Sensing\n  Imagery Abstract: Deep learning continues to push state-of-the-art performance for the semantic\nsegmentation of color (i.e., RGB) imagery; however, the lack of annotated data\nfor many remote sensing sensors (i.e. hyperspectral imagery (HSI)) prevents\nresearchers from taking advantage of this recent success. Since generating\nsensor specific datasets is time intensive and cost prohibitive, remote sensing\nresearchers have embraced deep unsupervised feature extraction. Although these\nmethods have pushed state-of-the-art performance on current HSI benchmarks,\nmany of these tools are not readily accessible to many researchers. In this\nletter, we introduce a software pipeline, which we call EarthMapper, for the\nsemantic segmentation of non-RGB remote sensing imagery. It includes\nself-taught spatial-spectral feature extraction, various standard and deep\nlearning classifiers, and undirected graphical models for post-processing. We\nevaluated EarthMapper on the Indian Pines and Pavia University datasets and\nhave released this code for public use. \n\n"}
{"id": "1804.00410", "contents": "Title: SyncGAN: Synchronize the Latent Space of Cross-modal Generative\n  Adversarial Networks Abstract: Generative adversarial network (GAN) has achieved impressive success on\ncross-domain generation, but it faces difficulty in cross-modal generation due\nto the lack of a common distribution between heterogeneous data. Most existing\nmethods of conditional based cross-modal GANs adopt the strategy of\none-directional transfer and have achieved preliminary success on text-to-image\ntransfer. Instead of learning the transfer between different modalities, we aim\nto learn a synchronous latent space representing the cross-modal common\nconcept. A novel network component named synchronizer is proposed in this work\nto judge whether the paired data is synchronous/corresponding or not, which can\nconstrain the latent space of generators in the GANs. Our GAN model, named as\nSyncGAN, can successfully generate synchronous data (e.g., a pair of image and\nsound) from identical random noise. For transforming data from one modality to\nanother, we recover the latent code by inverting the mappings of a generator\nand use it to generate data of different modality. In addition, the proposed\nmodel can achieve semi-supervised learning, which makes our model more flexible\nfor practical applications. \n\n"}
{"id": "1804.00432", "contents": "Title: Deep Residual Learning for Accelerated MRI using Magnitude and Phase\n  Networks Abstract: Accelerated magnetic resonance (MR) scan acquisition with compressed sensing\n(CS) and parallel imaging is a powerful method to reduce MR imaging scan time.\nHowever, many reconstruction algorithms have high computational costs. To\naddress this, we investigate deep residual learning networks to remove aliasing\nartifacts from artifact corrupted images. The proposed deep residual learning\nnetworks are composed of magnitude and phase networks that are separately\ntrained. If both phase and magnitude information are available, the proposed\nalgorithm can work as an iterative k-space interpolation algorithm using\nframelet representation. When only magnitude data is available, the proposed\napproach works as an image domain post-processing algorithm. Even with strong\ncoherent aliasing artifacts, the proposed network successfully learned and\nremoved the aliasing artifacts, whereas current parallel and CS reconstruction\nmethods were unable to remove these artifacts. Comparisons using single and\nmultiple coil show that the proposed residual network provides good\nreconstruction results with orders of magnitude faster computational time than\nexisting compressed sensing methods. The proposed deep learning framework may\nhave a great potential for accelerated MR reconstruction by generating accurate\nresults immediately. \n\n"}
{"id": "1804.00521", "contents": "Title: CompNet: Complementary Segmentation Network for Brain MRI Extraction Abstract: Brain extraction is a fundamental step for most brain imaging studies. In\nthis paper, we investigate the problem of skull stripping and propose\ncomplementary segmentation networks (CompNets) to accurately extract the brain\nfrom T1-weighted MRI scans, for both normal and pathological brain images. The\nproposed networks are designed in the framework of encoder-decoder networks and\nhave two pathways to learn features from both the brain tissue and its\ncomplementary part located outside of the brain. The complementary pathway\nextracts the features in the non-brain region and leads to a robust solution to\nbrain extraction from MRIs with pathologies, which do not exist in our training\ndataset. We demonstrate the effectiveness of our networks by evaluating them on\nthe OASIS dataset, resulting in the state of the art performance under the\ntwo-fold cross-validation setting. Moreover, the robustness of our networks is\nverified by testing on images with introduced pathologies and by showing its\ninvariance to unseen brain pathologies. In addition, our complementary network\ndesign is general and can be extended to address other image segmentation\nproblems with better generalization. \n\n"}
{"id": "1804.00525", "contents": "Title: DeepScores -- A Dataset for Segmentation, Detection and Classification\n  of Tiny Objects Abstract: We present the DeepScores dataset with the goal of advancing the\nstate-of-the-art in small objects recognition, and by placing the question of\nobject recognition in the context of scene understanding. DeepScores contains\nhigh quality images of musical scores, partitioned into 300,000 sheets of\nwritten music that contain symbols of different shapes and sizes. With close to\na hundred millions of small objects, this makes our dataset not only unique,\nbut also the largest public dataset. DeepScores comes with ground truth for\nobject classification, detection and semantic segmentation. DeepScores thus\nposes a relevant challenge for computer vision in general, beyond the scope of\noptical music recognition (OMR) research. We present a detailed statistical\nanalysis of the dataset, comparing it with other computer vision datasets like\nCaltech101/256, PASCAL VOC, SUN, SVHN, ImageNet, MS-COCO, smaller computer\nvision datasets, as well as with other OMR datasets. Finally, we provide\nbaseline performances for object classification and give pointers to future\nresearch based on this dataset. \n\n"}
{"id": "1804.00532", "contents": "Title: Predictions of short-term driving intention using recurrent neural\n  network on sequential data Abstract: Predictions of driver's intentions and their behaviors using the road is of\ngreat importance for planning and decision making processes of autonomous\ndriving vehicles. In particular, relatively short-term driving intentions are\nthe fundamental units that constitute more sophisticated driving goals,\nbehaviors, such as overtaking the slow vehicle in front, exit or merge onto a\nhigh way, etc. While it is not uncommon that most of the time human driver can\nrationalize, in advance, various on-road behaviors, intentions, as well as the\nassociated risks, aggressiveness, reciprocity characteristics, etc., such\nreasoning skills can be challenging and difficult for an autonomous driving\nsystem to learn. In this article, we demonstrate a disciplined methodology that\ncan be used to build and train a predictive drive system, therefore to learn\nthe on-road characteristics aforementioned. \n\n"}
{"id": "1804.00792", "contents": "Title: Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks Abstract: Data poisoning is an attack on machine learning models wherein the attacker\nadds examples to the training set to manipulate the behavior of the model at\ntest time. This paper explores poisoning attacks on neural nets. The proposed\nattacks use \"clean-labels\"; they don't require the attacker to have any control\nover the labeling of training data. They are also targeted; they control the\nbehavior of the classifier on a $\\textit{specific}$ test instance without\ndegrading overall classifier performance. For example, an attacker could add a\nseemingly innocuous image (that is properly labeled) to a training set for a\nface recognition engine, and control the identity of a chosen person at test\ntime. Because the attacker does not need to control the labeling function,\npoisons could be entered into the training set simply by leaving them on the\nweb and waiting for them to be scraped by a data collection bot.\n  We present an optimization-based method for crafting poisons, and show that\njust one single poison image can control classifier behavior when transfer\nlearning is used. For full end-to-end training, we present a \"watermarking\"\nstrategy that makes poisoning reliable using multiple ($\\approx$50) poisoned\ntraining instances. We demonstrate our method by generating poisoned frog\nimages from the CIFAR dataset and using them to manipulate image classifiers. \n\n"}
{"id": "1804.01071", "contents": "Title: Average performance analysis of the stochastic gradient method for\n  online PCA Abstract: This paper studies the complexity of the stochastic gradient algorithm for\nPCA when the data are observed in a streaming setting. We also propose an\nonline approach for selecting the learning rate. Simulation experiments confirm\nthe practical relevance of the plain stochastic gradient approach and that\ndrastic improvements can be achieved by learning the learning rate. \n\n"}
{"id": "1804.01422", "contents": "Title: Unsupervised Semantic-based Aggregation of Deep Convolutional Features Abstract: In this paper, we propose a simple but effective semantic-based aggregation\n(SBA) method. The proposed SBA utilizes the discriminative filters of deep\nconvolutional layers as semantic detectors. Moreover, we propose the effective\nunsupervised strategy to select some semantic detectors to generate the\n\"probabilistic proposals\", which highlight certain discriminative pattern of\nobjects and suppress the noise of background. The final global SBA\nrepresentation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \"probabilistic proposals\"\ncorresponding to various semantic content. Our unsupervised SBA is easy to\ngeneralize and achieves excellent performance on various tasks. We conduct\ncomprehensive experiments and show that our unsupervised SBA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods on image\nretrieval, place recognition and cloud classification. \n\n"}
{"id": "1804.02181", "contents": "Title: Generative adversarial network-based approach to signal reconstruction\n  from magnitude spectrograms Abstract: In this paper, we address the problem of reconstructing a time-domain signal\n(or a phase spectrogram) solely from a magnitude spectrogram. Since magnitude\nspectrograms do not contain phase information, we must restore or infer phase\ninformation to reconstruct a time-domain signal. One widely used approach for\ndealing with the signal reconstruction problem was proposed by Griffin and Lim.\nThis method usually requires many iterations for the signal reconstruction\nprocess and depending on the inputs, it does not always produce high-quality\naudio signals. To overcome these shortcomings, we apply a learning-based\napproach to the signal reconstruction problem by modeling the signal\nreconstruction process using a deep neural network and training it using the\nidea of a generative adversarial network. Experimental evaluations revealed\nthat our method was able to reconstruct signals faster with higher quality than\nthe Griffin-Lim method. \n\n"}
{"id": "1804.02541", "contents": "Title: Statistical transformer networks: learning shape and appearance models\n  via self supervision Abstract: We generalise Spatial Transformer Networks (STN) by replacing the parametric\ntransformation of a fixed, regular sampling grid with a deformable, statistical\nshape model which is itself learnt. We call this a Statistical Transformer\nNetwork (StaTN). By training a network containing a StaTN end-to-end for a\nparticular task, the network learns the optimal nonrigid alignment of the input\ndata for the task. Moreover, the statistical shape model is learnt with no\ndirect supervision (such as landmarks) and can be reused for other tasks.\nBesides training for a specific task, we also show that a StaTN can learn a\nshape model using generic loss functions. This includes a loss inspired by the\nminimum description length principle in which an appearance model is also\nlearnt from scratch. In this configuration, our model learns an active\nappearance model and a means to fit the model from scratch with no supervision\nat all, even identity labels. \n\n"}
{"id": "1804.02958", "contents": "Title: Generative Adversarial Networks for Extreme Learned Image Compression Abstract: We present a learned image compression system based on GANs, operating at\nextremely low bitrates. Our proposed framework combines an encoder,\ndecoder/generator and a multi-scale discriminator, which we train jointly for a\ngenerative learned compression objective. The model synthesizes details it\ncannot afford to store, obtaining visually pleasing results at bitrates where\nprevious methods fail and show strong artifacts. Furthermore, if a semantic\nlabel map of the original image is available, our method can fully synthesize\nunimportant regions in the decoded image such as streets and trees from the\nlabel map, proportionally reducing the storage cost. A user study confirms that\nfor low bitrates, our approach is preferred to state-of-the-art methods, even\nwhen they use more than double the bits. \n\n"}
{"id": "1804.03312", "contents": "Title: Crafting a Toolchain for Image Restoration by Deep Reinforcement\n  Learning Abstract: We investigate a novel approach for image restoration by reinforcement\nlearning. Unlike existing studies that mostly train a single large network for\na specialized task, we prepare a toolbox consisting of small-scale\nconvolutional networks of different complexities and specialized in different\ntasks. Our method, RL-Restore, then learns a policy to select appropriate tools\nfrom the toolbox to progressively restore the quality of a corrupted image. We\nformulate a step-wise reward function proportional to how well the image is\nrestored at each step to learn the action policy. We also devise a joint\nlearning scheme to train the agent and tools for better performance in handling\nuncertainty. In comparison to conventional human-designed networks, RL-Restore\nis capable of restoring images corrupted with complex and unknown distortions\nin a more parameter-efficient manner using the dynamically formed toolchain. \n\n"}
{"id": "1804.03390", "contents": "Title: Learning Pose Specific Representations by Predicting Different Views Abstract: The labeled data required to learn pose estimation for articulated objects is\ndifficult to provide in the desired quantity, realism, density, and accuracy.\nTo address this issue, we develop a method to learn representations, which are\nvery specific for articulated poses, without the need for labeled training\ndata. We exploit the observation that the object pose of a known object is\npredictive for the appearance in any known view. That is, given only the pose\nand shape parameters of a hand, the hand's appearance from any viewpoint can be\napproximated. To exploit this observation, we train a model that -- given input\nfrom one view -- estimates a latent representation, which is trained to be\npredictive for the appearance of the object when captured from another\nviewpoint. Thus, the only necessary supervision is the second view. The\ntraining process of this model reveals an implicit pose representation in the\nlatent space. Importantly, at test time the pose representation can be inferred\nusing only a single view. In qualitative and quantitative experiments we show\nthat the learned representations capture detailed pose information. Moreover,\nwhen training the proposed method jointly with labeled and unlabeled data, it\nconsistently surpasses the performance of its fully supervised counterpart,\nwhile reducing the amount of needed labeled samples by at least one order of\nmagnitude. \n\n"}
{"id": "1804.03447", "contents": "Title: RSGAN: Face Swapping and Editing using Face and Hair Representation in\n  Latent Spaces Abstract: In this paper, we present an integrated system for automatically generating\nand editing face images through face swapping, attribute-based editing, and\nrandom face parts synthesis. The proposed system is based on a deep neural\nnetwork that variationally learns the face and hair regions with large-scale\nface image datasets. Different from conventional variational methods, the\nproposed network represents the latent spaces individually for faces and hairs.\nWe refer to the proposed network as region-separative generative adversarial\nnetwork (RSGAN). The proposed network independently handles face and hair\nappearances in the latent spaces, and then, face swapping is achieved by\nreplacing the latent-space representations of the faces, and reconstruct the\nentire face image with them. This approach in the latent space robustly\nperforms face swapping even for images which the previous methods result in\nfailure due to inappropriate fitting or the 3D morphable models. In addition,\nthe proposed system can further edit face-swapped images with the same network\nby manipulating visual attributes or by composing them with randomly generated\nface or hair parts. \n\n"}
{"id": "1804.03583", "contents": "Title: Classification of Point Cloud Scenes with Multiscale Voxel Deep Network Abstract: In this article we describe a new convolutional neural network (CNN) to\nclassify 3D point clouds of urban or indoor scenes. Solutions are given to the\nproblems encountered working on scene point clouds, and a network is described\nthat allows for point classification using only the position of points in a\nmulti-scale neighborhood.\n  On the reduced-8 Semantic3D benchmark [Hackel et al., 2017], this network,\nranked second, beats the state of the art of point classification methods\n(those not using a regularization step). \n\n"}
{"id": "1804.03641", "contents": "Title: Audio-Visual Scene Analysis with Self-Supervised Multisensory Features Abstract: The thud of a bouncing ball, the onset of speech as lips open -- when visual\nand audio events occur together, it suggests that there might be a common,\nunderlying event that produced both signals. In this paper, we argue that the\nvisual and audio components of a video signal should be modeled jointly using a\nfused multisensory representation. We propose to learn such a representation in\na self-supervised way, by training a neural network to predict whether video\nframes and audio are temporally aligned. We use this learned representation for\nthree applications: (a) sound source localization, i.e. visualizing the source\nof sound in a video; (b) audio-visual action recognition; and (c) on/off-screen\naudio source separation, e.g. removing the off-screen translator's voice from a\nforeign official's speech. Code, models, and video results are available on our\nwebpage: http://andrewowens.com/multisensory \n\n"}
{"id": "1804.03675", "contents": "Title: Semi-supervised Adversarial Learning to Generate Photorealistic Face\n  Images of New Identities from 3D Morphable Model Abstract: We propose a novel end-to-end semi-supervised adversarial framework to\ngenerate photorealistic face images of new identities with wide ranges of\nexpressions, poses, and illuminations conditioned by a 3D morphable model.\nPrevious adversarial style-transfer methods either supervise their networks\nwith large volume of paired data or use unpaired data with a highly\nunder-constrained two-way generative framework in an unsupervised fashion. We\nintroduce pairwise adversarial supervision to constrain two-way domain\nadaptation by a small number of paired real and synthetic images for training\nalong with the large volume of unpaired data. Extensive qualitative and\nquantitative experiments are performed to validate our idea. Generated face\nimages of new identities contain pose, lighting and expression diversity and\nqualitative results show that they are highly constraint by the synthetic input\nimage while adding photorealism and retaining identity information. We combine\nface images generated by the proposed method with the real data set to train\nface recognition algorithms. We evaluated the model on two challenging data\nsets: LFW and IJB-A. We observe that the generated images from our framework\nconsistently improves over the performance of deep face recognition network\ntrained with Oxford VGG Face dataset and achieves comparable results to the\nstate-of-the-art. \n\n"}
{"id": "1804.04687", "contents": "Title: Cross-Domain Visual Recognition via Domain Adaptive Dictionary Learning Abstract: In real-world visual recognition problems, the assumption that the training\ndata (source domain) and test data (target domain) are sampled from the same\ndistribution is often violated. This is known as the domain adaptation problem.\nIn this work, we propose a novel domain-adaptive dictionary learning framework\nfor cross-domain visual recognition. Our method generates a set of intermediate\ndomains. These intermediate domains form a smooth path and bridge the gap\nbetween the source and target domains. Specifically, we not only learn a common\ndictionary to encode the domain-shared features, but also learn a set of\ndomain-specific dictionaries to model the domain shift. The separation of the\ncommon and domain-specific dictionaries enables us to learn more compact and\nreconstructive dictionaries for domain adaptation. These dictionaries are\nlearned by alternating between domain-adaptive sparse coding and dictionary\nupdating steps. Meanwhile, our approach gradually recovers the feature\nrepresentations of both source and target data along the domain path. By\naligning all the recovered domain data, we derive the final domain-adaptive\nfeatures for cross-domain visual recognition. Extensive experiments on three\npublic datasets demonstrates that our approach outperforms most\nstate-of-the-art methods. \n\n"}
{"id": "1804.04779", "contents": "Title: A Hybrid Model for Identity Obfuscation by Face Replacement Abstract: As more and more personal photos are shared and tagged in social media,\navoiding privacy risks such as unintended recognition becomes increasingly\nchallenging. We propose a new hybrid approach to obfuscate identities in photos\nby head replacement. Our approach combines state of the art parametric face\nsynthesis with latest advances in Generative Adversarial Networks (GAN) for\ndata-driven image synthesis. On the one hand, the parametric part of our method\ngives us control over the facial parameters and allows for explicit\nmanipulation of the identity. On the other hand, the data-driven aspects allow\nfor adding fine details and overall realism as well as seamless blending into\nthe scene context. In our experiments, we show highly realistic output of our\nsystem that improves over the previous state of the art in obfuscation rate\nwhile preserving a higher similarity to the original image content. \n\n"}
{"id": "1804.05018", "contents": "Title: Comparatives, Quantifiers, Proportions: A Multi-Task Model for the\n  Learning of Quantities from Vision Abstract: The present work investigates whether different quantification mechanisms\n(set comparison, vague quantification, and proportional estimation) can be\njointly learned from visual scenes by a multi-task computational model. The\nmotivation is that, in humans, these processes underlie the same cognitive,\nnon-symbolic ability, which allows an automatic estimation and comparison of\nset magnitudes. We show that when information about lower-complexity tasks is\navailable, the higher-level proportional task becomes more accurate than when\nperformed in isolation. Moreover, the multi-task model is able to generalize to\nunseen combinations of target/non-target objects. Consistently with behavioral\nevidence showing the interference of absolute number in the proportional task,\nthe multi-task model no longer works when asked to provide the number of target\nobjects in the scene. \n\n"}
{"id": "1804.05195", "contents": "Title: Motion-based Object Segmentation based on Dense RGB-D Scene Flow Abstract: Given two consecutive RGB-D images, we propose a model that estimates a dense\n3D motion field, also known as scene flow. We take advantage of the fact that\nin robot manipulation scenarios, scenes often consist of a set of rigidly\nmoving objects. Our model jointly estimates (i) the segmentation of the scene\ninto an unknown but finite number of objects, (ii) the motion trajectories of\nthese objects and (iii) the object scene flow. We employ an hourglass, deep\nneural network architecture. In the encoding stage, the RGB and depth images\nundergo spatial compression and correlation. In the decoding stage, the model\noutputs three images containing a per-pixel estimate of the corresponding\nobject center as well as object translation and rotation. This forms the basis\nfor inferring the object segmentation and final object scene flow. To evaluate\nour model, we generated a new and challenging, large-scale, synthetic dataset\nthat is specifically targeted at robotic manipulation: It contains a large\nnumber of scenes with a very diverse set of simultaneously moving 3D objects\nand is recorded with a simulated, static RGB-D camera. In quantitative\nexperiments, we show that we outperform state-of-the-art scene flow and\nmotion-segmentation methods on this data set. In qualitative experiments, we\nshow how our learned model transfers to challenging real-world scenes, visually\ngenerating better results than existing methods. \n\n"}
{"id": "1804.06215", "contents": "Title: DetNet: A Backbone network for Object Detection Abstract: Recent CNN based object detectors, no matter one-stage methods like YOLO,\nSSD, and RetinaNe or two-stage detectors like Faster R-CNN, R-FCN and FPN are\nusually trying to directly finetune from ImageNet pre-trained models designed\nfor image classification. There has been little work discussing on the backbone\nfeature extractor specifically designed for the object detection. More\nimportantly, there are several differences between the tasks of image\nclassification and object detection. 1. Recent object detectors like FPN and\nRetinaNet usually involve extra stages against the task of image classification\nto handle the objects with various scales. 2. Object detection not only needs\nto recognize the category of the object instances but also spatially locate the\nposition. Large downsampling factor brings large valid receptive field, which\nis good for image classification but compromises the object location ability.\nDue to the gap between the image classification and object detection, we\npropose DetNet in this paper, which is a novel backbone network specifically\ndesigned for object detection. Moreover, DetNet includes the extra stages\nagainst traditional backbone network for image classification, while maintains\nhigh spatial resolution in deeper layers. Without any bells and whistles,\nstate-of-the-art results have been obtained for both object detection and\ninstance segmentation on the MSCOCO benchmark based on our DetNet~(4.8G FLOPs)\nbackbone. The code will be released for the reproduction. \n\n"}
{"id": "1804.06352", "contents": "Title: High Dimensional Time Series Generators Abstract: Multidimensional time series are sequences of real valued vectors. They occur\nin different areas, for example handwritten characters, GPS tracking, and\ngestures of modern virtual reality motion controllers. Within these areas, a\ncommon task is to search for similar time series. Dynamic Time Warping (DTW) is\na common distance function to compare two time series. The Edit Distance with\nReal Penalty (ERP) and the Dog Keeper Distance (DK) are two more distance\nfunctions on time series. Their behaviour has been analyzed on 1-dimensional\ntime series. However, it is not easy to evaluate their behaviour in relation to\ngrowing dimensionality. For this reason we propose two new data synthesizers\ngenerating multidimensional time series. The first synthesizer extends the well\nknown cylinder-bell-funnel (CBF) dataset to multidimensional time series. Here,\neach time series has an arbitrary type (cylinder, bell, or funnel) in each\ndimension, thus for $d$-dimensional time series there are $3^{d}$ different\nclasses. The second synthesizer (RAM) creates time series with ideas adapted\nfrom Brownian motions which is a common model of movement in physics. Finally,\nwe evaluate the applicability of a 1-nearest neighbor classifier using DTW on\ndatasets generated by our synthesizers. \n\n"}
{"id": "1804.06423", "contents": "Title: Deep Object Co-Segmentation Abstract: This work presents a deep object co-segmentation (DOCS) approach for\nsegmenting common objects of the same class within a pair of images. This means\nthat the method learns to ignore common, or uncommon, background stuff and\nfocuses on objects. If multiple object classes are presented in the image pair,\nthey are jointly extracted as foreground. To address this task, we propose a\nCNN-based Siamese encoder-decoder architecture. The encoder extracts high-level\nsemantic features of the foreground objects, a mutual correlation layer detects\nthe common objects, and finally, the decoder generates the output foreground\nmasks for each image. To train our model, we compile a large object\nco-segmentation dataset consisting of image pairs from the PASCAL VOC dataset\nwith common objects masks. We evaluate our approach on commonly used datasets\nfor co-segmentation tasks and observe that our approach consistently\noutperforms competing methods, for both seen and unseen object classes. \n\n"}
{"id": "1804.06655", "contents": "Title: Deep Face Recognition: A Survey Abstract: Deep learning applies multiple processing layers to learn representations of\ndata with multiple levels of feature extraction. This emerging technique has\nreshaped the research landscape of face recognition (FR) since 2014, launched\nby the breakthroughs of DeepFace and DeepID. Since then, deep learning\ntechnique, characterized by the hierarchical architecture to stitch together\npixels into invariant face representation, has dramatically improved the\nstate-of-the-art performance and fostered successful real-world applications.\nIn this survey, we provide a comprehensive review of the recent developments on\ndeep FR, covering broad topics on algorithm designs, databases, protocols, and\napplication scenes. First, we summarize different network architectures and\nloss functions proposed in the rapid evolution of the deep FR methods. Second,\nthe related face processing methods are categorized into two classes:\n\"one-to-many augmentation\" and \"many-to-one normalization\". Then, we summarize\nand compare the commonly used databases for both model training and evaluation.\nThird, we review miscellaneous scenes in deep FR, such as cross-factor,\nheterogenous, multiple-media and industrial scenes. Finally, the technical\nchallenges and several promising directions are highlighted. \n\n"}
{"id": "1804.07353", "contents": "Title: Unsupervised Representation Adversarial Learning Network: from\n  Reconstruction to Generation Abstract: A good representation for arbitrarily complicated data should have the\ncapability of semantic generation, clustering and reconstruction. Previous\nresearch has already achieved impressive performance on either one. This paper\naims at learning a disentangled representation effective for all of them in an\nunsupervised way. To achieve all the three tasks together, we learn the forward\nand inverse mapping between data and representation on the basis of a symmetric\nadversarial process. In theory, we minimize the upper bound of the two\nconditional entropy loss between the latent variables and the observations\ntogether to achieve the cycle consistency. The newly proposed RepGAN is tested\non MNIST, fashionMNIST, CelebA, and SVHN datasets to perform unsupervised\nclassification, generation and reconstruction tasks. The result demonstrates\nthat RepGAN is able to learn a useful and competitive representation. To the\nauthor's knowledge, our work is the first one to achieve both a high\nunsupervised classification accuracy and low reconstruction error on MNIST.\nCodes are available at https://github.com/yzhouas/RepGAN-tensorflow. \n\n"}
{"id": "1804.08328", "contents": "Title: Taskonomy: Disentangling Task Transfer Learning Abstract: Do visual tasks have a relationship, or are they unrelated? For instance,\ncould having surface normals simplify estimating the depth of an image?\nIntuition answers these questions positively, implying existence of a structure\namong visual tasks. Knowing this structure has notable values; it is the\nconcept underlying transfer learning and provides a principled way for\nidentifying redundancies across tasks, e.g., to seamlessly reuse supervision\namong related tasks or solve many tasks in one system without piling up the\ncomplexity.\n  We proposes a fully computational approach for modeling the structure of\nspace of visual tasks. This is done via finding (first and higher-order)\ntransfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D,\nand semantic tasks in a latent space. The product is a computational taxonomic\nmap for task transfer learning. We study the consequences of this structure,\ne.g. nontrivial emerged relationships, and exploit them to reduce the demand\nfor labeled data. For example, we show that the total number of labeled\ndatapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3\n(compared to training independently) while keeping the performance nearly the\nsame. We provide a set of tools for computing and probing this taxonomical\nstructure including a solver that users can employ to devise efficient\nsupervision policies for their use cases. \n\n"}
{"id": "1804.08348", "contents": "Title: Deep Facial Expression Recognition: A Survey Abstract: With the transition of facial expression recognition (FER) from\nlaboratory-controlled to challenging in-the-wild conditions and the recent\nsuccess of deep learning techniques in various fields, deep neural networks\nhave increasingly been leveraged to learn discriminative representations for\nautomatic FER. Recent deep FER systems generally focus on two important issues:\noverfitting caused by a lack of sufficient training data and\nexpression-unrelated variations, such as illumination, head pose and identity\nbias. In this paper, we provide a comprehensive survey on deep FER, including\ndatasets and algorithms that provide insights into these intrinsic problems.\nFirst, we describe the standard pipeline of a deep FER system with the related\nbackground knowledge and suggestions of applicable implementations for each\nstage. We then introduce the available datasets that are widely used in the\nliterature and provide accepted data selection and evaluation principles for\nthese datasets. For the state of the art in deep FER, we review existing novel\ndeep neural networks and related training strategies that are designed for FER\nbased on both static images and dynamic image sequences, and discuss their\nadvantages and limitations. Competitive performances on widely used benchmarks\nare also summarized in this section. We then extend our survey to additional\nrelated issues and application scenarios. Finally, we review the remaining\nchallenges and corresponding opportunities in this field as well as future\ndirections for the design of robust deep FER systems. \n\n"}
{"id": "1804.08369", "contents": "Title: Gaussian Material Synthesis Abstract: We present a learning-based system for rapid mass-scale material synthesis\nthat is useful for novice and expert users alike. The user preferences are\nlearned via Gaussian Process Regression and can be easily sampled for new\nrecommendations. Typically, each recommendation takes 40-60 seconds to render\nwith global illumination, which makes this process impracticable for real-world\nworkflows. Our neural network eliminates this bottleneck by providing\nhigh-quality image predictions in real time, after which it is possible to pick\nthe desired materials from a gallery and assign them to a scene in an intuitive\nmanner. Workflow timings against Disney's \"principled\" shader reveal that our\nsystem scales well with the number of sought materials, thus empowering even\nnovice users to generate hundreds of high-quality material models without any\nexpertise in material modeling. Similarly, expert users experience a\nsignificant decrease in the total modeling time when populating a scene with\nmaterials. Furthermore, our proposed solution also offers controllable\nrecommendations and a novel latent space variant generation step to enable the\nreal-time fine-tuning of materials without requiring any domain expertise. \n\n"}
{"id": "1804.09111", "contents": "Title: Structure Aware SLAM using Quadrics and Planes Abstract: Simultaneous Localization And Mapping (SLAM) is a fundamental problem in\nmobile robotics. While point-based SLAM methods provide accurate camera\nlocalization, the generated maps lack semantic information. On the other hand,\nstate of the art object detection methods provide rich information about\nentities present in the scene from a single image. This work marries the two\nand proposes a method for representing generic objects as quadrics which allows\nobject detections to be seamlessly integrated in a SLAM framework. For scene\ncoverage, additional dominant planar structures are modeled as infinite planes.\nExperiments show that the proposed points-planes-quadrics representation can\neasily incorporate Manhattan and object affordance constraints, greatly\nimproving camera localization and leading to semantically meaningful maps. The\nperformance of our SLAM system is demonstrated in https://youtu.be/dR-rB9keF8M . \n\n"}
{"id": "1804.09699", "contents": "Title: Towards Fast Computation of Certified Robustness for ReLU Networks Abstract: Verifying the robustness property of a general Rectified Linear Unit (ReLU)\nnetwork is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer\nCAV17]. Although finding the exact minimum adversarial distortion is hard,\ngiving a certified lower bound of the minimum distortion is possible. Current\navailable methods of computing such a bound are either time-consuming or\ndelivering low quality bounds that are too loose to be useful. In this paper,\nwe exploit the special structure of ReLU networks and provide two\ncomputationally efficient algorithms Fast-Lin and Fast-Lip that are able to\ncertify non-trivial lower bounds of minimum distortions, by bounding the ReLU\nunits with appropriate linear functions Fast-Lin, or by bounding the local\nLipschitz constant Fast-Lip. Experiments show that (1) our proposed methods\ndeliver bounds close to (the gap is 2-3X) exact minimum distortion found by\nReluplex in small MNIST networks while our algorithms are more than 10,000\ntimes faster; (2) our methods deliver similar quality of bounds (the gap is\nwithin 35% and usually around 10%; sometimes our bounds are even better) for\nlarger networks compared to the methods based on solving linear programming\nproblems but our algorithms are 33-14,000 times faster; (3) our method is\ncapable of solving large MNIST and CIFAR networks up to 7 layers with more than\n10,000 neurons within tens of seconds on a single CPU core.\n  In addition, we show that, in fact, there is no polynomial time algorithm\nthat can approximately find the minimum $\\ell_1$ adversarial distortion of a\nReLU network with a $0.99\\ln n$ approximation ratio unless\n$\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network. \n\n"}
{"id": "1804.09858", "contents": "Title: Generative Model for Heterogeneous Inference Abstract: Generative models (GMs) such as Generative Adversary Network (GAN) and\nVariational Auto-Encoder (VAE) have thrived these years and achieved high\nquality results in generating new samples. Especially in Computer Vision, GMs\nhave been used in image inpainting, denoising and completion, which can be\ntreated as the inference from observed pixels to corrupted pixels. However,\nimages are hierarchically structured which are quite different from many\nreal-world inference scenarios with non-hierarchical features. These inference\nscenarios contain heterogeneous stochastic variables and irregular mutual\ndependences. Traditionally they are modeled by Bayesian Network (BN). However,\nthe learning and inference of BN model are NP-hard thus the number of\nstochastic variables in BN is highly constrained. In this paper, we adapt\ntypical GMs to enable heterogeneous learning and inference in polynomial\ntime.We also propose an extended autoregressive (EAR) model and an EAR with\nadversary loss (EARA) model and give theoretical results on their\neffectiveness. Experiments on several BN datasets show that our proposed EAR\nmodel achieves the best performance in most cases compared to other GMs. Except\nfor black box analysis, we've also done a serial of experiments on Markov\nborder inference of GMs for white box analysis and give theoretical results. \n\n"}
{"id": "1804.10073", "contents": "Title: Visual Data Synthesis via GAN for Zero-Shot Video Classification Abstract: Zero-Shot Learning (ZSL) in video classification is a promising research\ndirection, which aims to tackle the challenge from explosive growth of video\ncategories. Most existing methods exploit seen-to-unseen correlation via\nlearning a projection between visual and semantic spaces. However, such\nprojection-based paradigms cannot fully utilize the discriminative information\nimplied in data distribution, and commonly suffer from the information\ndegradation issue caused by \"heterogeneity gap\". In this paper, we propose a\nvisual data synthesis framework via GAN to address these problems.\nSpecifically, both semantic knowledge and visual distribution are leveraged to\nsynthesize video feature of unseen categories, and ZSL can be turned into\ntypical supervised problem with the synthetic features. First, we propose\nmulti-level semantic inference to boost video feature synthesis, which captures\nthe discriminative information implied in joint visual-semantic distribution\nvia feature-level and label-level semantic inference. Second, we propose\nMatching-aware Mutual Information Correlation to overcome information\ndegradation issue, which captures seen-to-unseen correlation in matched and\nmismatched visual-semantic pairs by mutual information, providing the zero-shot\nsynthesis procedure with robust guidance signals. Experimental results on four\nvideo datasets demonstrate that our approach can improve the zero-shot video\nclassification performance significantly. \n\n"}
{"id": "1804.10743", "contents": "Title: Precise Box Score: Extract More Information from Datasets to Improve the\n  Performance of Face Detection Abstract: For the training of face detection network based on R-CNN framework, anchors\nare assigned to be positive samples if intersection-over-unions (IoUs) with\nground-truth are higher than the first threshold(such as 0.7); and to be\nnegative samples if their IoUs are lower than the second threshold(such as\n0.3). And the face detection model is trained by the above labels. However,\nanchors with IoU between first threshold and second threshold are not used. We\npropose a novel training strategy, Precise Box Score(PBS), to train object\ndetection models. The proposed training strategy uses the anchors with IoUs\nbetween the first and second threshold, which can consistently improve the\nperformance of face detection. Our proposed training strategy extracts more\ninformation from datasets, making better utilization of existing datasets.\nWhat's more, we also introduce a simple but effective model compression\nmethod(SEMCM), which can boost the performance of face detectors further.\nExperimental results show that the performance of face detection network can\nconsistently be improved based on our proposed scheme. \n\n"}
{"id": "1805.00309", "contents": "Title: An Universal Image Attractiveness Ranking Framework Abstract: We propose a new framework to rank image attractiveness using a novel\npairwise deep network trained with a large set of side-by-side multi-labeled\nimage pairs from a web image index. The judges only provide relative ranking\nbetween two images without the need to directly assign an absolute score, or\nrate any predefined image attribute, thus making the rating more intuitive and\naccurate. We investigate a deep attractiveness rank net (DARN), a combination\nof deep convolutional neural network and rank net, to directly learn an\nattractiveness score mean and variance for each image and the underlying\ncriteria the judges use to label each pair. The extension of this model\n(DARN-V2) is able to adapt to individual judge's personal preference. We also\nshow the attractiveness of search results are significantly improved by using\nthis attractiveness information in a real commercial search engine. We evaluate\nour model against other state-of-the-art models on our side-by-side web test\ndata and another public aesthetic data set. With much less judgments (1M vs\n50M), our model outperforms on side-by-side labeled data, and is comparable on\ndata labeled by absolute score. \n\n"}
{"id": "1805.00316", "contents": "Title: Versatile Auxiliary Classifier with Generative Adversarial Network\n  (VAC+GAN) Abstract: One of the most interesting challenges in Artificial Intelligence is to train\nconditional generators which are able to provide labeled adversarial samples\ndrawn from a specific distribution. In this work, a new framework is presented\nto train a deep conditional generator by placing a classifier in parallel with\nthe discriminator and back propagate the classification error through the\ngenerator network. The method is versatile and is applicable to any variations\nof Generative Adversarial Network (GAN) implementation, and also gives superior\nresults compared to similar methods. \n\n"}
{"id": "1805.00460", "contents": "Title: Customized Image Narrative Generation via Interactive Visual Question\n  Generation and Answering Abstract: Image description task has been invariably examined in a static manner with\nqualitative presumptions held to be universally applicable, regardless of the\nscope or target of the description. In practice, however, different viewers may\npay attention to different aspects of the image, and yield different\ndescriptions or interpretations under various contexts. Such diversity in\nperspectives is difficult to derive with conventional image description\ntechniques. In this paper, we propose a customized image narrative generation\ntask, in which the users are interactively engaged in the generation process by\nproviding answers to the questions. We further attempt to learn the user's\ninterest via repeating such interactive stages, and to automatically reflect\nthe interest in descriptions for new images. Experimental results demonstrate\nthat our model can generate a variety of descriptions from single image that\ncover a wider range of topics than conventional models, while being\ncustomizable to the target user of interaction. \n\n"}
{"id": "1805.00500", "contents": "Title: Adapting Mask-RCNN for Automatic Nucleus Segmentation Abstract: Automatic segmentation of microscopy images is an important task in medical\nimage processing and analysis. Nucleus detection is an important example of\nthis task. Mask-RCNN is a recently proposed state-of-the-art algorithm for\nobject detection, object localization, and object instance segmentation of\nnatural images. In this paper we demonstrate that Mask-RCNN can be used to\nperform highly effective and efficient automatic segmentations of a wide range\nof microscopy images of cell nuclei, for a variety of cells acquired under a\nvariety of conditions. \n\n"}
{"id": "1805.00545", "contents": "Title: Weakly Supervised Attention Learning for Textual Phrases Grounding Abstract: Grounding textual phrases in visual content is a meaningful yet challenging\nproblem with various potential applications such as image-text inference or\ntext-driven multimedia interaction. Most of the current existing methods adopt\nthe supervised learning mechanism which requires ground-truth at pixel level\nduring training. However, fine-grained level ground-truth annotation is quite\ntime-consuming and severely narrows the scope for more general applications. In\nthis extended abstract, we explore methods to localize flexibly image regions\nfrom the top-down signal (in a form of one-hot label or natural languages) with\na weakly supervised attention learning mechanism. In our model, two types of\nmodules are utilized: a backbone module for visual feature capturing, and an\nattentive module generating maps based on regularized bilinear pooling. We\nconstruct the model in an end-to-end fashion which is trained by encouraging\nthe spatial attentive map to shift and focus on the region that consists of the\nbest matched visual features with the top-down signal. We demonstrate the\npreliminary yet promising results on a testbed that is synthesized with\nmulti-label MNIST data. \n\n"}
{"id": "1805.00784", "contents": "Title: Markov Chain Neural Networks Abstract: In this work we present a modified neural network model which is capable to\nsimulate Markov Chains. We show how to express and train such a network, how to\nensure given statistical properties reflected in the training data and we\ndemonstrate several applications where the network produces non-deterministic\noutcomes. One example is a random walker model, e.g. useful for simulation of\nBrownian motions or a natural Tic-Tac-Toe network which ensures\nnon-deterministic game behavior. \n\n"}
{"id": "1805.00862", "contents": "Title: Spectral clustering algorithms for the detection of clusters in\n  block-cyclic and block-acyclic graphs Abstract: We propose two spectral algorithms for partitioning nodes in directed graphs\nrespectively with a cyclic and an acyclic pattern of connection between groups\nof nodes. Our methods are based on the computation of extremal eigenvalues of\nthe transition matrix associated to the directed graph. The two algorithms\noutperform state-of-the art methods for directed graph clustering on synthetic\ndatasets, including methods based on blockmodels, bibliometric symmetrization\nand random walks. Our algorithms have the same space complexity as classical\nspectral clustering algorithms for undirected graphs and their time complexity\nis also linear in the number of edges in the graph. One of our methods is\napplied to a trophic network based on predator-prey relationships. It\nsuccessfully extracts common categories of preys and predators encountered in\nfood chains. The same method is also applied to highlight the hierarchical\nstructure of a worldwide network of Autonomous Systems depicting business\nagreements between Internet Service Providers. \n\n"}
{"id": "1805.01818", "contents": "Title: Object and Text-guided Semantics for CNN-based Activity Recognition Abstract: Many previous methods have demonstrated the importance of considering\nsemantically relevant objects for carrying out video-based human activity\nrecognition, yet none of the methods have harvested the power of large text\ncorpora to relate the objects and the activities to be transferred into\nlearning a unified deep convolutional neural network. We present a novel\nactivity recognition CNN which co-learns the object recognition task in an\nend-to-end multitask learning scheme to improve upon the baseline activity\nrecognition performance. We further improve upon the multitask learning\napproach by exploiting a text-guided semantic space to select the most relevant\nobjects with respect to the target activities. To the best of our knowledge, we\nare the first to investigate this approach. \n\n"}
{"id": "1805.01972", "contents": "Title: Fast-converging Conditional Generative Adversarial Networks for Image\n  Synthesis Abstract: Building on top of the success of generative adversarial networks (GANs),\nconditional GANs attempt to better direct the data generation process by\nconditioning with certain additional information. Inspired by the most recent\nAC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In\naddition to the real/fake classifier used in vanilla GANs, our discriminator\nhas an advanced auxiliary classifier which distinguishes each real class from\nan extra `fake' class. The `fake' class avoids mixing generated data with real\ndata, which can potentially confuse the classification of real data as AC-GAN\ndoes, and makes the advanced auxiliary classifier behave as another real/fake\nclassifier. As a result, FC-GAN can accelerate the process of differentiation\nof all classes, thus boost the convergence speed. Experimental results on image\nsynthesis demonstrate our model is competitive in the quality of images\ngenerated while achieving a faster convergence rate. \n\n"}
{"id": "1805.02369", "contents": "Title: GAN Based Medical Image Registration Abstract: Conventional approaches to image registration consist of time consuming\niterative methods. Most current deep learning (DL) based registration methods\nextract deep features to use in an iterative setting. We propose an end-to-end\nDL method for registering multimodal images. Our approach uses generative\nadversarial networks (GANs) that eliminates the need for time consuming\niterative methods, and directly generates the registered image with the\ndeformation field. Appropriate constraints in the GAN cost function produce\naccurately registered images in less than a second. Experiments demonstrate\ntheir accuracy for multimodal retinal and cardiac MR image registration. \n\n"}
{"id": "1805.02456", "contents": "Title: Unpaired Multi-Domain Image Generation via Regularized Conditional GANs Abstract: In this paper, we study the problem of multi-domain image generation, the\ngoal of which is to generate pairs of corresponding images from different\ndomains. With the recent development in generative models, image generation has\nachieved great progress and has been applied to various computer vision tasks.\nHowever, multi-domain image generation may not achieve the desired performance\ndue to the difficulty of learning the correspondence of different domain\nimages, especially when the information of paired samples is not given. To\ntackle this problem, we propose Regularized Conditional GAN (RegCGAN) which is\ncapable of learning to generate corresponding images in the absence of paired\ntraining data. RegCGAN is based on the conditional GAN, and we introduce two\nregularizers to guide the model to learn the corresponding semantics of\ndifferent domains. We evaluate the proposed model on several tasks for which\npaired training data is not given, including the generation of edges and\nphotos, the generation of faces with different attributes, etc. The\nexperimental results show that our model can successfully generate\ncorresponding images for all these tasks, while outperforms the baseline\nmethods. We also introduce an approach of applying RegCGAN to unsupervised\ndomain adaptation. \n\n"}
{"id": "1805.02481", "contents": "Title: MEGAN: Mixture of Experts of Generative Adversarial Networks for\n  Multimodal Image Generation Abstract: Recently, generative adversarial networks (GANs) have shown promising\nperformance in generating realistic images. However, they often struggle in\nlearning complex underlying modalities in a given dataset, resulting in\npoor-quality generated images. To mitigate this problem, we present a novel\napproach called mixture of experts GAN (MEGAN), an ensemble approach of\nmultiple generator networks. Each generator network in MEGAN specializes in\ngenerating images with a particular subset of modalities, e.g., an image class.\nInstead of incorporating a separate step of handcrafted clustering of multiple\nmodalities, our proposed model is trained through an end-to-end learning of\nmultiple generators via gating networks, which is responsible for choosing the\nappropriate generator network for a given condition. We adopt the categorical\nreparameterization trick for a categorical decision to be made in selecting a\ngenerator while maintaining the flow of the gradients. We demonstrate that\nindividual generators learn different and salient subparts of the data and\nachieve a multiscale structural similarity (MS-SSIM) score of 0.2470 for CelebA\nand a competitive unsupervised inception score of 8.33 in CIFAR-10. \n\n"}
{"id": "1805.02834", "contents": "Title: Weakly-Supervised Video Object Grounding from Text by Loss Weighting and\n  Object Interaction Abstract: We study weakly-supervised video object grounding: given a video segment and\na corresponding descriptive sentence, the goal is to localize objects that are\nmentioned from the sentence in the video. During training, no object bounding\nboxes are available, but the set of possible objects to be grounded is known\nbeforehand. Existing approaches in the image domain use Multiple Instance\nLearning (MIL) to ground objects by enforcing matches between visual and\nsemantic features. A naive extension of this approach to the video domain is to\ntreat the entire segment as a bag of spatial object proposals. However, an\nobject existing sparsely across multiple frames might not be detected\ncompletely since successfully spotting it from one single frame would trigger a\nsatisfactory match. To this end, we propagate the weak supervisory signal from\nthe segment level to frames that likely contain the target object. For frames\nthat are unlikely to contain the target objects, we use an alternative penalty\nloss. We also leverage the interactions among objects as a textual guide for\nthe grounding. We evaluate our model on the newly-collected benchmark\nYouCook2-BoundingBox and show improvements over competitive baselines. \n\n"}
{"id": "1805.03096", "contents": "Title: Fast Feature Extraction with CNNs with Pooling Layers Abstract: In recent years, many publications showed that convolutional neural network\nbased features can have a superior performance to engineered features. However,\nnot much effort was taken so far to extract local features efficiently for a\nwhole image. In this paper, we present an approach to compute patch-based local\nfeature descriptors efficiently in presence of pooling and striding layers for\nwhole images at once. Our approach is generic and can be applied to nearly all\nexisting network architectures. This includes networks for all local feature\nextraction tasks like camera calibration, Patchmatching, optical flow\nestimation and stereo matching. In addition, our approach can be applied to\nother patch-based approaches like sliding window object detection and\nrecognition. We complete our paper with a speed benchmark of popular CNN based\nfeature extraction approaches applied on a whole image, with and without our\nspeedup, and example code (for Torch) that shows how an arbitrary CNN\narchitecture can be easily converted by our approach. \n\n"}
{"id": "1805.03779", "contents": "Title: k-Space Deep Learning for Accelerated MRI Abstract: The annihilating filter-based low-rank Hankel matrix approach (ALOHA) is one\nof the state-of-the-art compressed sensing approaches that directly\ninterpolates the missing k-space data using low-rank Hankel matrix completion.\nThe success of ALOHA is due to the concise signal representation in the k-space\ndomain thanks to the duality between structured low-rankness in the k-space\ndomain and the image domain sparsity. Inspired by the recent mathematical\ndiscovery that links convolutional neural networks to Hankel matrix\ndecomposition using data-driven framelet basis, here we propose a fully\ndata-driven deep learning algorithm for k-space interpolation. Our network can\nbe also easily applied to non-Cartesian k-space trajectories by simply adding\nan additional regridding layer. Extensive numerical experiments show that the\nproposed deep learning method consistently outperforms the existing\nimage-domain deep learning approaches. \n\n"}
{"id": "1805.04252", "contents": "Title: Adaptive Selection of Deep Learning Models on Embedded Systems Abstract: The recent ground-breaking advances in deep learning networks ( DNNs ) make\nthem attractive for embedded systems. However, it can take a long time for DNNs\nto make an inference on resource-limited embedded devices. Offloading the\ncomputation into the cloud is often infeasible due to privacy concerns, high\nlatency, or the lack of connectivity. As such, there is a critical need to find\na way to effectively execute the DNN models locally on the devices. This paper\npresents an adaptive scheme to determine which DNN model to use for a given\ninput, by considering the desired accuracy and inference time. Our approach\nemploys machine learning to develop a predictive model to quickly select a\npre-trained DNN to use for a given input and the optimization constraint. We\nachieve this by first training off-line a predictive model, and then use the\nlearnt model to select a DNN model to use for new, unseen inputs. We apply our\napproach to the image classification task and evaluate it on a Jetson TX2\nembedded deep learning platform using the ImageNet ILSVRC 2012 validation\ndataset. We consider a range of influential DNN models. Experimental results\nshow that our approach achieves a 7.52% improvement in inference accuracy, and\na 1.8x reduction in inference time over the most-capable single DNN model. \n\n"}
{"id": "1805.04276", "contents": "Title: Leveraging Grammar and Reinforcement Learning for Neural Program\n  Synthesis Abstract: Program synthesis is the task of automatically generating a program\nconsistent with a specification. Recent years have seen proposal of a number of\nneural approaches for program synthesis, many of which adopt a sequence\ngeneration paradigm similar to neural machine translation, in which\nsequence-to-sequence models are trained to maximize the likelihood of known\nreference programs. While achieving impressive results, this strategy has two\nkey limitations. First, it ignores Program Aliasing: the fact that many\ndifferent programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many\nsemantically correct programs, which can adversely affect the synthesizer\nperformance. Second, this strategy overlooks the fact that programs have a\nstrict syntax that can be efficiently checked. To address the first limitation,\nwe perform reinforcement learning on top of a supervised model with an\nobjective that explicitly maximizes the likelihood of generating semantically\ncorrect programs. For addressing the second limitation, we introduce a training\nprocedure that directly maximizes the probability of generating syntactically\ncorrect programs that fulfill the specification. We show that our contributions\nlead to improved accuracy of the models, especially in cases where the training\ndata is limited. \n\n"}
{"id": "1805.04714", "contents": "Title: Exploring object-centric and scene-centric CNN features and their\n  complementarity for human rights violations recognition in images Abstract: Identifying potential abuses of human rights through imagery is a novel and\nchallenging task in the field of computer vision, that will enable to expose\nhuman rights violations over large-scale data that may otherwise be impossible.\nWhile standard databases for object and scene categorisation contain hundreds\nof different classes, the largest available dataset of human rights violations\ncontains only 4 classes. Here, we introduce the `Human Rights Archive Database'\n(HRA), a verified-by-experts repository of 3050 human rights violations\nphotographs, labelled with human rights semantic categories, comprising a list\nof the types of human rights abuses encountered at present. With the HRA\ndataset and a two-phase transfer learning scheme, we fine-tuned the\nstate-of-the-art deep convolutional neural networks (CNNs) to provide human\nrights violations classification CNNs (HRA-CNNs). We also present extensive\nexperiments refined to evaluate how well object-centric and scene-centric CNN\nfeatures can be combined for the task of recognising human rights abuses. With\nthis, we show that HRA database poses a challenge at a higher level for the\nwell studied representation learning methods, and provide a benchmark in the\ntask of human rights violations recognition in visual context. We expect this\ndataset can help to open up new horizons on creating systems able of\nrecognising rich information about human rights violations. Our dataset, codes\nand trained models are available online at\nhttps://github.com/GKalliatakis/Human-Rights-Archive-CNNs. \n\n"}
{"id": "1805.05421", "contents": "Title: Energy Efficient Hadamard Neural Networks Abstract: Deep learning has made significant improvements at many image processing\ntasks in recent years, such as image classification, object recognition and\nobject detection. Convolutional neural networks (CNN), which is a popular deep\nlearning architecture designed to process data in multiple array form, show\ngreat success to almost all detection \\& recognition problems and computer\nvision tasks. However, the number of parameters in a CNN is too high such that\nthe computers require more energy and larger memory size. In order to solve\nthis problem, we propose a novel energy efficient model Binary Weight and\nHadamard-transformed Image Network (BWHIN), which is a combination of Binary\nWeight Network (BWN) and Hadamard-transformed Image Network (HIN). It is\nobserved that energy efficiency is achieved with a slight sacrifice at\nclassification accuracy. Among all energy efficient networks, our novel\nensemble model outperforms other energy efficient models. \n\n"}
{"id": "1805.06749", "contents": "Title: Action Completion: A Temporal Model for Moment Detection Abstract: We introduce completion moment detection for actions - the problem of\nlocating the moment of completion, when the action's goal is confidently\nconsidered achieved. The paper proposes a joint classification-regression\nrecurrent model that predicts completion from a given frame, and then\nintegrates frame-level contributions to detect sequence-level completion\nmoment. We introduce a recurrent voting node that predicts the frame's relative\nposition of the completion moment by either classification or regression. The\nmethod is also capable of detecting incompletion. For example, the method is\ncapable of detecting a missed ball-catch, as well as the moment at which the\nball is safely caught. We test the method on 16 actions from three public\ndatasets, covering sports as well as daily actions. Results show that when\ncombining contributions from frames prior to the completion moment as well as\nframes post completion, the completion moment is detected within one second in\n89% of all tested sequences. \n\n"}
{"id": "1805.07159", "contents": "Title: Low-Cost Recurrent Neural Network Expected Performance Evaluation Abstract: Recurrent neural networks are a powerful tool, but they are very sensitive to\ntheir hyper-parameter configuration. Moreover, training properly a recurrent\nneural network is a tough task, therefore selecting an appropriate\nconfiguration is critical. Varied strategies have been proposed to tackle this\nissue. However, most of them are still impractical because of the\ntime/resources needed. In this study, we propose a low computational cost model\nto evaluate the expected performance of a given architecture based on the\ndistribution of the error of random samples of the weights. We empirically\nvalidate our proposal using three use cases. The results suggest that this is a\npromising alternative to reduce the cost of exploration for hyper-parameter\noptimization. \n\n"}
{"id": "1805.07339", "contents": "Title: Scanner: Efficient Video Analysis at Scale Abstract: A growing number of visual computing applications depend on the analysis of\nlarge video collections. The challenge is that scaling applications to operate\non these datasets requires efficient systems for pixel data access and parallel\nprocessing across large numbers of machines. Few programmers have the\ncapability to operate efficiently at these scales, limiting the field's ability\nto explore new applications that leverage big video data. In response, we have\ncreated Scanner, a system for productive and efficient video analysis at scale.\nScanner organizes video collections as tables in a data store optimized for\nsampling frames from compressed video, and executes pixel processing\ncomputations, expressed as dataflow graphs, on these frames. Scanner schedules\nvideo analysis applications expressed using these abstractions onto\nheterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and\nmedia processing ASICs, for high-throughput pixel processing. We demonstrate\nthe productivity of Scanner by authoring a variety of video processing\napplications including the synthesis of stereo VR video streams from\nmulti-camera rigs, markerless 3D human pose reconstruction from video, and\ndata-mining big video datasets such as hundreds of feature-length films or over\n70,000 hours of TV news. These applications achieve near-expert performance on\na single machine and scale efficiently to hundreds of machines, enabling\nformerly long-running big video data analysis tasks to be carried out in\nminutes to hours. \n\n"}
{"id": "1805.07457", "contents": "Title: Adversarial Structure Matching for Structured Prediction Tasks Abstract: Pixel-wise losses, e.g., cross-entropy or L2, have been widely used in\nstructured prediction tasks as a spatial extension of generic image\nclassification or regression. However, its i.i.d. assumption neglects the\nstructural regularity present in natural images. Various attempts have been\nmade to incorporate structural reasoning mostly through structure priors in a\ncooperative way where co-occurring patterns are encouraged.\n  We, on the other hand, approach this problem from an opposing angle and\npropose a new framework, Adversarial Structure Matching (ASM), for training\nsuch structured prediction networks via an adversarial process, in which we\ntrain a structure analyzer that provides the supervisory signals, the ASM loss.\nThe structure analyzer is trained to maximize the ASM loss, or to emphasize\nrecurring multi-scale hard negative structural mistakes among co-occurring\npatterns. On the contrary, the structured prediction network is trained to\nreduce those mistakes and is thus enabled to distinguish fine-grained\nstructures. As a result, training structured prediction networks using ASM\nreduces contextual confusion among objects and improves boundary localization.\nWe demonstrate that our ASM outperforms pixel-wise IID loss or structural prior\nGAN loss on three different structured prediction tasks: semantic segmentation,\nmonocular depth estimation, and surface normal prediction. \n\n"}
{"id": "1805.07621", "contents": "Title: CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule\n  Subspaces Abstract: In this paper, we formalize the idea behind capsule nets of using a capsule\nvector rather than a neuron activation to predict the label of samples. To this\nend, we propose to learn a group of capsule subspaces onto which an input\nfeature vector is projected. Then the lengths of resultant capsules are used to\nscore the probability of belonging to different classes. We train such a\nCapsule Projection Network (CapProNet) by learning an orthogonal projection\nmatrix for each capsule subspace, and show that each capsule subspace is\nupdated until it contains input feature vectors corresponding to the associated\nclass. We will also show that the capsule projection can be viewed as\nnormalizing the multiple columns of the weight matrix simultaneously to form an\northogonal basis, which makes it more effective in incorporating novel\ncomponents of input features to update capsule representations. In other words,\nthe capsule projection can be viewed as a multi-dimensional weight\nnormalization in capsule subspaces, where the conventional weight normalization\nis simply a special case of the capsule projection onto 1D lines. Only a small\nnegligible computing overhead is incurred to train the network in\nlow-dimensional capsule subspaces or through an alternative hyper-power\niteration to estimate the normalization matrix. Experiment results on image\ndatasets show the presented model can greatly improve the performance of the\nstate-of-the-art ResNet backbones by $10-20\\%$ and that of the Densenet by\n$5-7\\%$ respectively at the same level of computing and memory expenses. The\nCapProNet establishes the competitive state-of-the-art performance for the\nfamily of capsule nets by significantly reducing test errors on the benchmark\ndatasets. \n\n"}
{"id": "1805.07848", "contents": "Title: A Universal Music Translation Network Abstract: We present a method for translating music across musical instruments, genres,\nand styles. This method is based on a multi-domain wavenet autoencoder, with a\nshared encoder and a disentangled latent space that is trained end-to-end on\nwaveforms. Employing a diverse training dataset and large net capacity, the\ndomain-independent encoder allows us to translate even from musical domains\nthat were not seen during training. The method is unsupervised and does not\nrely on supervision in the form of matched samples between domains or musical\ntranscriptions. We evaluate our method on NSynth, as well as on a dataset\ncollected from professional musicians, and achieve convincing translations,\neven when translating from whistling, potentially enabling the creation of\ninstrumental music by untrained humans. \n\n"}
{"id": "1805.07872", "contents": "Title: Spherical Convolutional Neural Network for 3D Point Clouds Abstract: We propose a neural network for 3D point cloud processing that exploits\n`spherical' convolution kernels and octree partitioning of space. The proposed\nmetric-based spherical kernels systematically quantize point neighborhoods to\nidentify local geometric structures in data, while maintaining the properties\nof translation-invariance and asymmetry. The network architecture itself is\nguided by octree data structuring that takes full advantage of the sparse\nnature of irregular point clouds. We specify spherical kernels with the help of\nneurons in each layer that in turn are associated with spatial locations. We\nexploit this association to avert dynamic kernel generation during network\ntraining, that enables efficient learning with high resolution point clouds. We\ndemonstrate the utility of the spherical convolutional neural network for 3D\nobject classification on standard benchmark datasets. \n\n"}
{"id": "1805.07932", "contents": "Title: Bilinear Attention Networks Abstract: Attention networks in multimodal learning provide an efficient way to utilize\ngiven visual information selectively. However, the computational cost to learn\nattention distributions for every pair of multimodal input channels is\nprohibitively expensive. To solve this problem, co-attention builds two\nseparate attention distributions for each modality neglecting the interaction\nbetween multimodal inputs. In this paper, we propose bilinear attention\nnetworks (BAN) that find bilinear attention distributions to utilize given\nvision-language information seamlessly. BAN considers bilinear interactions\namong two groups of input channels, while low-rank bilinear pooling extracts\nthe joint representations for each pair of channels. Furthermore, we propose a\nvariant of multimodal residual networks to exploit eight-attention maps of the\nBAN efficiently. We quantitatively and qualitatively evaluate our model on\nvisual question answering (VQA 2.0) and Flickr30k Entities datasets, showing\nthat BAN significantly outperforms previous methods and achieves new\nstate-of-the-arts on both datasets. \n\n"}
{"id": "1805.09110", "contents": "Title: The Topology ToolKit Abstract: This system paper presents the Topology ToolKit (TTK), a software platform\ndesigned for topological data analysis in scientific visualization. TTK\nprovides a unified, generic, efficient, and robust implementation of key\nalgorithms for the topological analysis of scalar data, including: critical\npoints, integral lines, persistence diagrams, persistence curves, merge trees,\ncontour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots,\nJacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due\nto a tight integration with ParaView. It is also easily accessible to\ndevelopers through a variety of bindings (Python, VTK/C++) for fast prototyping\nor through direct, dependence-free, C++, to ease integration into pre-existing\ncomplex systems. While developing TTK, we faced several algorithmic and\nsoftware engineering challenges, which we document in this paper. In\nparticular, we present an algorithm for the construction of a discrete gradient\nthat complies to the critical points extracted in the piecewise-linear setting.\nThis algorithm guarantees a combinatorial consistency across the topological\nabstractions supported by TTK, and importantly, a unified implementation of\ntopological data simplification for multi-scale exploration and analysis. We\nalso present a cached triangulation data structure, that supports time\nefficient and generic traversals, which self-adjusts its memory usage on demand\nfor input simplicial meshes and which implicitly emulates a triangulation for\nregular grids with no memory overhead. Finally, we describe an original\nsoftware architecture, which guarantees memory efficient and direct accesses to\nTTK features, while still allowing for researchers powerful and easy bindings\nand extensions. TTK is open source (BSD license) and its code, online\ndocumentation and video tutorials are available on TTK's website. \n\n"}
{"id": "1805.09501", "contents": "Title: AutoAugment: Learning Augmentation Policies from Data Abstract: Data augmentation is an effective technique for improving the accuracy of\nmodern image classifiers. However, current data augmentation implementations\nare manually designed. In this paper, we describe a simple procedure called\nAutoAugment to automatically search for improved data augmentation policies. In\nour implementation, we have designed a search space where a policy consists of\nmany sub-policies, one of which is randomly chosen for each image in each\nmini-batch. A sub-policy consists of two operations, each operation being an\nimage processing function such as translation, rotation, or shearing, and the\nprobabilities and magnitudes with which the functions are applied. We use a\nsearch algorithm to find the best policy such that the neural network yields\nthe highest validation accuracy on a target dataset. Our method achieves\nstate-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without\nadditional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is\n0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error\nrate of 1.5%, which is 0.6% better than the previous state-of-the-art.\nAugmentation policies we find are transferable between datasets. The policy\nlearned on ImageNet transfers well to achieve significant improvements on other\ndatasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft,\nand Stanford Cars. \n\n"}
{"id": "1805.10123", "contents": "Title: TADAM: Task dependent adaptive metric for improved few-shot learning Abstract: Few-shot learning has become essential for producing models that generalize\nfrom few examples. In this work, we identify that metric scaling and metric\ntask conditioning are important to improve the performance of few-shot\nalgorithms. Our analysis reveals that simple metric scaling completely changes\nthe nature of few-shot algorithm parameter updates. Metric scaling provides\nimprovements up to 14% in accuracy for certain metrics on the mini-Imagenet\n5-way 5-shot classification task. We further propose a simple and effective way\nof conditioning a learner on the task sample set, resulting in learning a\ntask-dependent metric space. Moreover, we propose and empirically test a\npractical end-to-end optimization procedure based on auxiliary task co-training\nto learn a task-dependent metric space. The resulting few-shot learning model\nbased on the task-dependent scaled metric achieves state of the art on\nmini-Imagenet. We confirm these results on another few-shot dataset that we\nintroduce in this paper based on CIFAR100. Our code is publicly available at\nhttps://github.com/ElementAI/TADAM. \n\n"}
{"id": "1805.10133", "contents": "Title: Laplacian Networks: Bounding Indicator Function Smoothness for Neural\n  Network Robustness Abstract: For the past few years, Deep Neural Network (DNN) robustness has become a\nquestion of paramount importance. As a matter of fact, in sensitive settings\nmisclassification can lead to dramatic consequences. Such misclassifications\nare likely to occur when facing adversarial attacks, hardware failures or\nlimitations, and imperfect signal acquisition. To address this question,\nauthors have proposed different approaches aiming at increasing the robustness\nof DNNs, such as adding regularizers or training using noisy examples. In this\npaper we propose a new regularizer built upon the Laplacian of similarity\ngraphs obtained from the representation of training data at each layer of the\nDNN architecture. This regularizer penalizes large changes (across consecutive\nlayers in the architecture) in the distance between examples of different\nclasses, and as such enforces smooth variations of the class boundaries. Since\nit is agnostic to the type of deformations that are expected when predicting\nwith the DNN, the proposed regularizer can be combined with existing ad-hoc\nmethods. We provide theoretical justification for this regularizer and\ndemonstrate its effectiveness to improve robustness of DNNs on classical\nsupervised learning vision datasets. \n\n"}
{"id": "1805.10604", "contents": "Title: Deployment of Customized Deep Learning based Video Analytics On\n  Surveillance Cameras Abstract: This paper demonstrates the effectiveness of our customized deep learning\nbased video analytics system in various applications focused on security,\nsafety, customer analytics and process compliance. We describe our video\nanalytics system comprising of Search, Summarize, Statistics and real-time\nalerting, and outline its building blocks. These building blocks include object\ndetection, tracking, face detection and recognition, human and face\nsub-attribute analytics. In each case, we demonstrate how custom models trained\nusing data from the deployment scenarios provide considerably superior\naccuracies than off-the-shelf models. Towards this end, we describe our data\nprocessing and model training pipeline, which can train and fine-tune models\nfrom videos with a quick turnaround time. Finally, since most of these models\nare deployed on-site, it is important to have resource constrained models which\ndo not require GPUs. We demonstrate how we custom train resource constrained\nmodels and deploy them on embedded devices without significant loss in\naccuracy. To our knowledge, this is the first work which provides a\ncomprehensive evaluation of different deep learning models on various\nreal-world customer deployment scenarios of surveillance video analytics. By\nsharing our implementation details and the experiences learned from deploying\ncustomized deep learning models for various customers, we hope that customized\ndeep learning based video analytics is widely incorporated in commercial\nproducts around the world. \n\n"}
{"id": "1805.10720", "contents": "Title: Multi-region segmentation of bladder cancer structures in MRI with\n  progressive dilated convolutional networks Abstract: Precise segmentation of bladder walls and tumor regions is an essential step\ntowards non-invasive identification of tumor stage and grade, which is critical\nfor treatment decision and prognosis of patients with bladder cancer (BC).\nHowever, the automatic delineation of bladder walls and tumor in magnetic\nresonance images (MRI) is a challenging task, due to important bladder shape\nvariations, strong intensity inhomogeneity in urine and very high variability\nacross population, particularly on tumors appearance. To tackle these issues,\nwe propose to use a deep fully convolutional neural network. The proposed\nnetwork includes dilated convolutions to increase the receptive field without\nincurring extra cost nor degrading its performance. Furthermore, we introduce\nprogressive dilations in each convolutional block, thereby enabling extensive\nreceptive fields without the need for large dilation rates. The proposed\nnetwork is evaluated on 3.0T T2-weighted MRI scans from 60 pathologically\nconfirmed patients with BC. Experiments shows the proposed model to achieve\nhigh accuracy, with a mean Dice similarity coefficient of 0.98, 0.84 and 0.69\nfor inner wall, outer wall and tumor region, respectively. These results\nrepresent a very good agreement with reference contours and an increase in\nperformance compared to existing methods. In addition, inference times are less\nthan a second for a whole 3D volume, which is between 2-3 orders of magnitude\nfaster than related state-of-the-art methods for this application. We showed\nthat a CNN can yield precise segmentation of bladder walls and tumors in\nbladder cancer patients on MRI. The whole segmentation process is\nfully-automatic and yields results in very good agreement with the reference\nstandard, demonstrating the viability of deep learning models for the automatic\nmulti-region segmentation of bladder cancer MRI images. \n\n"}
{"id": "1805.11592", "contents": "Title: Playing hard exploration games by watching YouTube Abstract: Deep reinforcement learning methods traditionally struggle with tasks where\nenvironment rewards are particularly sparse. One successful method of guiding\nexploration in these domains is to imitate trajectories provided by a human\ndemonstrator. However, these demonstrations are typically collected under\nartificial conditions, i.e. with access to the agent's exact environment setup\nand the demonstrator's action and reward trajectories. Here we propose a\ntwo-stage method that overcomes these limitations by relying on noisy,\nunaligned footage without access to such data. First, we learn to map unaligned\nvideos from multiple sources to a common representation using self-supervised\nobjectives constructed over both time and modality (i.e. vision and sound).\nSecond, we embed a single YouTube video in this representation to construct a\nreward function that encourages an agent to imitate human gameplay. This method\nof one-shot imitation allows our agent to convincingly exceed human-level\nperformance on the infamously hard exploration games Montezuma's Revenge,\nPitfall! and Private Eye for the first time, even if the agent is not presented\nwith any environment rewards. \n\n"}
{"id": "1805.11686", "contents": "Title: Variational Inverse Control with Events: A General Framework for\n  Data-Driven Reward Definition Abstract: The design of a reward function often poses a major practical challenge to\nreal-world applications of reinforcement learning. Approaches such as inverse\nreinforcement learning attempt to overcome this challenge, but require expert\ndemonstrations, which can be difficult or expensive to obtain in practice. We\npropose variational inverse control with events (VICE), which generalizes\ninverse reinforcement learning methods to cases where full demonstrations are\nnot needed, such as when only samples of desired goal states are available. Our\nmethod is grounded in an alternative perspective on control and reinforcement\nlearning, where an agent's goal is to maximize the probability that one or more\nevents will happen at some point in the future, rather than maximizing\ncumulative rewards. We demonstrate the effectiveness of our methods on\ncontinuous control tasks, with a focus on high-dimensional observations like\nimages where rewards are hard or even impossible to specify. \n\n"}
{"id": "1805.11778", "contents": "Title: Object Detection using Domain Randomization and Generative Adversarial\n  Refinement of Synthetic Images Abstract: In this work, we present an application of domain randomization and\ngenerative adversarial networks (GAN) to train a near real-time object detector\nfor industrial electric parts, entirely in a simulated environment. Large scale\navailability of labelled real world data is typically rare and difficult to\nobtain in many industrial settings. As such here, only a few hundred of\nunlabelled real images are used to train a Cyclic-GAN network, in combination\nwith various degree of domain randomization procedures. We demonstrate that\nthis enables robust translation of synthetic images to the real world domain.\nWe show that a combination of the original synthetic (simulation) and GAN\ntranslated images, when used for training a Mask-RCNN object detection network\nachieves greater than 0.95 mean average precision in detecting and classifying\na collection of industrial electric parts. We evaluate the performance across\ndifferent combinations of training data. \n\n"}
{"id": "1805.12177", "contents": "Title: Why do deep convolutional networks generalize so poorly to small image\n  transformations? Abstract: Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to\nsmall image transformations: either because of the convolutional architecture\nor because they were trained using data augmentation. Recently, several authors\nhave shown that this is not the case: small translations or rescalings of the\ninput image can drastically change the network's prediction. In this paper, we\nquantify this phenomena and ask why neither the convolutional architecture nor\ndata augmentation are sufficient to achieve the desired invariance.\nSpecifically, we show that the convolutional architecture does not give\ninvariance since architectures ignore the classical sampling theorem, and data\naugmentation does not give invariance because the CNNs learn to be invariant to\ntransformations only for images that are very similar to typical images from\nthe training set. We discuss two possible solutions to this problem: (1)\nantialiasing the intermediate representations and (2) increasing data\naugmentation and show that they provide only a partial solution at best. Taken\ntogether, our results indicate that the problem of insuring invariance to small\nimage transformations in neural networks while preserving high accuracy remains\nunsolved. \n\n"}
{"id": "1806.00047", "contents": "Title: Following High-level Navigation Instructions on a Simulated Quadcopter\n  with Imitation Learning Abstract: We introduce a method for following high-level navigation instructions by\nmapping directly from images, instructions and pose estimates to continuous\nlow-level velocity commands for real-time control. The Grounded Semantic\nMapping Network (GSMN) is a fully-differentiable neural network architecture\nthat builds an explicit semantic map in the world reference frame by\nincorporating a pinhole camera projection model within the network. The\ninformation stored in the map is learned from experience, while the\nlocal-to-world transformation is computed explicitly. We train the model using\nDAggerFM, a modified variant of DAgger that trades tabular convergence\nguarantees for improved training speed and memory use. We test GSMN in virtual\nenvironments on a realistic quadcopter simulator and show that incorporating an\nexplicit mapping and grounding modules allows GSMN to outperform strong neural\nbaselines and almost reach an expert policy performance. Finally, we analyze\nthe learned map representations and show that using an explicit map leads to an\ninterpretable instruction-following model. \n\n"}
{"id": "1806.00088", "contents": "Title: PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks Abstract: Deep learning systems have become ubiquitous in many aspects of our lives.\nUnfortunately, it has been shown that such systems are vulnerable to\nadversarial attacks, making them prone to potential unlawful uses. Designing\ndeep neural networks that are robust to adversarial attacks is a fundamental\nstep in making such systems safer and deployable in a broader variety of\napplications (e.g. autonomous driving), but more importantly is a necessary\nstep to design novel and more advanced architectures built on new computational\nparadigms rather than marginally building on the existing ones. In this paper\nwe introduce PeerNets, a novel family of convolutional networks alternating\nclassical Euclidean convolutions with graph convolutions to harness information\nfrom a graph of peer samples. This results in a form of non-local forward\npropagation in the model, where latent features are conditioned on the global\nstructure induced by the graph, that is up to 3 times more robust to a variety\nof white- and black-box adversarial attacks compared to conventional\narchitectures with almost no drop in accuracy. \n\n"}
{"id": "1806.00250", "contents": "Title: TAPAS: Train-less Accuracy Predictor for Architecture Search Abstract: In recent years an increasing number of researchers and practitioners have\nbeen suggesting algorithms for large-scale neural network architecture search:\ngenetic algorithms, reinforcement learning, learning curve extrapolation, and\naccuracy predictors. None of them, however, demonstrated high-performance\nwithout training new experiments in the presence of unseen datasets. We propose\na new deep neural network accuracy predictor, that estimates in fractions of a\nsecond classification performance for unseen input datasets, without training.\nIn contrast to previously proposed approaches, our prediction is not only\ncalibrated on the topological network information, but also on the\ncharacterization of the dataset-difficulty which allows us to re-tune the\nprediction without any training. Our predictor achieves a performance which\nexceeds 100 networks per second on a single GPU, thus creating the opportunity\nto perform large-scale architecture search within a few minutes. We present\nresults of two searches performed in 400 seconds on a single GPU. Our best\ndiscovered networks reach 93.67% accuracy for CIFAR-10 and 81.01% for\nCIFAR-100, verified by training. These networks are performance competitive\nwith other automatically discovered state-of-the-art networks however we only\nneeded a small fraction of the time to solution and computational resources. \n\n"}
{"id": "1806.00428", "contents": "Title: A Classification approach towards Unsupervised Learning of Visual\n  Representations Abstract: In this paper, we present a technique for unsupervised learning of visual\nrepresentations. Specifically, we train a model for foreground and background\nclassification task, in the process of which it learns visual representations.\nForeground and background patches for training come af- ter mining for such\npatches from hundreds and thousands of unlabelled videos available on the web\nwhich we ex- tract using a proposed patch extraction algorithm. With- out using\nany supervision, with just using 150, 000 unla- belled videos and the PASCAL\nVOC 2007 dataset, we train a object recognition model that achieves 45.3 mAP\nwhich is close to the best performing unsupervised feature learn- ing technique\nwhereas better than many other proposed al- gorithms. The code for patch\nextraction is implemented in Matlab and available open source at the following\nlink . \n\n"}
{"id": "1806.01817", "contents": "Title: Perturbative Neural Networks Abstract: Convolutional neural networks are witnessing wide adoption in computer vision\nsystems with numerous applications across a range of visual recognition tasks.\nMuch of this progress is fueled through advances in convolutional neural\nnetwork architectures and learning algorithms even as the basic premise of a\nconvolutional layer has remained unchanged. In this paper, we seek to revisit\nthe convolutional layer that has been the workhorse of state-of-the-art visual\nrecognition models. We introduce a very simple, yet effective, module called a\nperturbation layer as an alternative to a convolutional layer. The perturbation\nlayer does away with convolution in the traditional sense and instead computes\nits response as a weighted linear combination of non-linearly activated\nadditive noise perturbed inputs. We demonstrate both analytically and\nempirically that this perturbation layer can be an effective replacement for a\nstandard convolutional layer. Empirically, deep neural networks with\nperturbation layers, called Perturbative Neural Networks (PNNs), in lieu of\nconvolutional layers perform comparably with standard CNNs on a range of visual\ndatasets (MNIST, CIFAR-10, PASCAL VOC, and ImageNet) with fewer parameters. \n\n"}
{"id": "1806.02284", "contents": "Title: Corpus Conversion Service: A Machine Learning Platform to Ingest\n  Documents at Scale Abstract: Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make the\ncontained knowledge discoverable. Unfortunately, both the format of these\ndocuments (e.g. the PDF format or bitmap images) as well as the presentation of\nthe data (e.g. complex tables) make the extraction of qualitative and\nquantitive data extremely challenging. In this paper, we present a modular,\ncloud-based platform to ingest documents at scale. This platform, called the\nCorpus Conversion Service (CCS), implements a pipeline which allows users to\nparse and annotate documents (i.e. collect ground-truth), train\nmachine-learning classification algorithms and ultimately convert any type of\nPDF or bitmap-documents to a structured content representation format. We will\nshow that each of the modules is scalable due to an asynchronous microservice\narchitecture and can therefore handle massive amounts of documents.\nFurthermore, we will show that our capability to gather ground-truth is\naccelerated by machine-learning algorithms by at least one order of magnitude.\nThis allows us to both gather large amounts of ground-truth in very little time\nand obtain very good precision/recall metrics in the range of 99\\% with regard\nto content conversion to structured output. The CCS platform is currently\ndeployed on IBM internal infrastructure and serving more than 250 active users\nfor knowledge-engineering project engagements. \n\n"}
{"id": "1806.02311", "contents": "Title: Unsupervised Attention-guided Image to Image Translation Abstract: Current unsupervised image-to-image translation techniques struggle to focus\ntheir attention on individual objects without altering the background or the\nway multiple objects interact within a scene. Motivated by the important role\nof attention in human perception, we tackle this limitation by introducing\nunsupervised attention mechanisms that are jointly adversarialy trained with\nthe generators and discriminators. We demonstrate qualitatively and\nquantitatively that our approach is able to attend to relevant regions in the\nimage without requiring supervision, and that by doing so it achieves more\nrealistic mappings compared to recent approaches. \n\n"}
{"id": "1806.03361", "contents": "Title: A Content-Based Late Fusion Approach Applied to Pedestrian Detection Abstract: The variety of pedestrians detectors proposed in recent years has encouraged\nsome works to fuse pedestrian detectors to achieve a more accurate detection.\nThe intuition behind is to combine the detectors based on its spatial\nconsensus. We propose a novel method called Content-Based Spatial Consensus\n(CSBC), which, in addition to relying on spatial consensus, considers the\ncontent of the detection windows to learn a weighted-fusion of pedestrian\ndetectors. The result is a reduction in false alarms and an enhancement in the\ndetection. In this work, we also demonstrate that there is small influence of\nthe feature used to learn the contents of the windows of each detector, which\nenables our method to be efficient even employing simple features. The CSBC\novercomes state-of-the-art fusion methods in the ETH dataset and in the Caltech\ndataset. Particularly, our method is more efficient since fewer detectors are\nnecessary to achieve expressive results. \n\n"}
{"id": "1806.03486", "contents": "Title: Learning to Grasp from a Single Demonstration Abstract: Learning-based approaches for robotic grasping using visual sensors typically\nrequire collecting a large size dataset, either manually labeled or by many\ntrial and errors of a robotic manipulator in the real or simulated world. We\npropose a simpler learning-from-demonstration approach that is able to detect\nthe object to grasp from merely a single demonstration using a convolutional\nneural network we call GraspNet. In order to increase robustness and decrease\nthe training time even further, we leverage data from previous demonstrations\nto quickly fine-tune a GrapNet for each new demonstration. We present some\npreliminary results on a grasping experiment with the Franka Panda cobot for\nwhich we can train a GraspNet with only hundreds of train iterations. \n\n"}
{"id": "1806.03987", "contents": "Title: Writing Style Invariant Deep Learning Model for Historical Manuscripts\n  Alignment Abstract: Historical manuscript alignment is a widely known problem in document\nanalysis. Finding the differences between manuscript editions is mostly done\nmanually. In this paper, we present a writer independent deep learning model\nwhich is trained on several writing styles, and able to achieve high detection\naccuracy when tested on writing styles not present in training data. We test\nour model using cross validation, each time we train the model on five\nmanuscripts, and test it on the other two manuscripts, never seen in the\ntraining data. We've applied cross validation on seven manuscripts, netting 21\ndifferent tests, achieving average accuracy of $\\%92.17$. We also present a new\nalignment algorithm based on dynamic sized sliding window, which is able to\nsuccessfully handle complex cases. \n\n"}
{"id": "1806.04265", "contents": "Title: Accurate and Robust Neural Networks for Security Related Applications\n  Exampled by Face Morphing Attacks Abstract: Artificial neural networks tend to learn only what they need for a task. A\nmanipulation of the training data can counter this phenomenon. In this paper,\nwe study the effect of different alterations of the training data, which limit\nthe amount and position of information that is available for the decision\nmaking. We analyze the accuracy and robustness against semantic and black box\nattacks on the networks that were trained on different training data\nmodifications for the particular example of morphing attacks. A morphing attack\nis an attack on a biometric facial recognition system where the system is\nfooled to match two different individuals with the same synthetic face image.\nSuch a synthetic image can be created by aligning and blending images of the\ntwo individuals that should be matched with this image. \n\n"}
{"id": "1806.04284", "contents": "Title: iParaphrasing: Extracting Visually Grounded Paraphrases via an Image Abstract: A paraphrase is a restatement of the meaning of a text in other words.\nParaphrases have been studied to enhance the performance of many natural\nlanguage processing tasks. In this paper, we propose a novel task iParaphrasing\nto extract visually grounded paraphrases (VGPs), which are different phrasal\nexpressions describing the same visual concept in an image. These extracted\nVGPs have the potential to improve language and image multimodal tasks such as\nvisual question answering and image captioning. How to model the similarity\nbetween VGPs is the key of iParaphrasing. We apply various existing methods as\nwell as propose a novel neural network-based method with image attention, and\nreport the results of the first attempt toward iParaphrasing. \n\n"}
{"id": "1806.04498", "contents": "Title: The Unusual Effectiveness of Averaging in GAN Training Abstract: We examine two different techniques for parameter averaging in GAN training.\nMoving Average (MA) computes the time-average of parameters, whereas\nExponential Moving Average (EMA) computes an exponentially discounted sum.\nWhilst MA is known to lead to convergence in bilinear settings, we provide the\n-- to our knowledge -- first theoretical arguments in support of EMA. We show\nthat EMA converges to limit cycles around the equilibrium with vanishing\namplitude as the discount parameter approaches one for simple bilinear games\nand also enhances the stability of general GAN training. We establish\nexperimentally that both techniques are strikingly effective in the\nnon-convex-concave GAN setting as well. Both improve inception and FID scores\non different architectures and for different GAN objectives. We provide\ncomprehensive experimental results across a range of datasets -- mixture of\nGaussians, CIFAR-10, STL-10, CelebA and ImageNet -- to demonstrate its\neffectiveness. We achieve state-of-the-art results on CIFAR-10 and produce\nclean CelebA face images.\\footnote{~The code is available at\n\\url{https://github.com/yasinyazici/EMA_GAN}} \n\n"}
{"id": "1806.04646", "contents": "Title: Adversarial Attacks on Variational Autoencoders Abstract: Adversarial attacks are malicious inputs that derail machine-learning models.\nWe propose a scheme to attack autoencoders, as well as a quantitative\nevaluation framework that correlates well with the qualitative assessment of\nthe attacks. We assess --- with statistically validated experiments --- the\nresistance to attacks of three variational autoencoders (simple, convolutional,\nand DRAW) in three datasets (MNIST, SVHN, CelebA), showing that both DRAW's\nrecurrence and attention mechanism lead to better resistance. As autoencoders\nare proposed for compressing data --- a scenario in which their safety is\nparamount --- we expect more attention will be given to adversarial attacks on\nthem. \n\n"}
{"id": "1806.04845", "contents": "Title: Interpretable Partitioned Embedding for Customized Fashion Outfit\n  Composition Abstract: Intelligent fashion outfit composition becomes more and more popular in these\nyears. Some deep learning based approaches reveal competitive composition\nrecently. However, the unexplainable characteristic makes such deep learning\nbased approach cannot meet the the designer, businesses and consumers' urge to\ncomprehend the importance of different attributes in an outfit composition. To\nrealize interpretable and customized fashion outfit compositions, we propose a\npartitioned embedding network to learn interpretable representations from\nclothing items. The overall network architecture consists of three components:\nan auto-encoder module, a supervised attributes module and a multi-independent\nmodule. The auto-encoder module serves to encode all useful information into\nthe embedding. In the supervised attributes module, multiple attributes labels\nare adopted to ensure that different parts of the overall embedding correspond\nto different attributes. In the multi-independent module, adversarial operation\nare adopted to fulfill the mutually independent constraint. With the\ninterpretable and partitioned embedding, we then construct an outfit\ncomposition graph and an attribute matching map. Given specified attributes\ndescription, our model can recommend a ranked list of outfit composition with\ninterpretable matching scores. Extensive experiments demonstrate that 1) the\npartitioned embedding have unmingled parts which corresponding to different\nattributes and 2) outfits recommended by our model are more desirable in\ncomparison with the existing methods. \n\n"}
{"id": "1806.04942", "contents": "Title: Convolutional Sparse Coding for High Dynamic Range Imaging Abstract: Current HDR acquisition techniques are based on either (i) fusing\nmultibracketed, low dynamic range (LDR) images, (ii) modifying existing\nhardware and capturing different exposures simultaneously with multiple\nsensors, or (iii) reconstructing a single image with spatially-varying pixel\nexposures. In this paper, we propose a novel algorithm to recover high-quality\nHDRI images from a single, coded exposure. The proposed reconstruction method\nbuilds on recently-introduced ideas of convolutional sparse coding (CSC); this\npaper demonstrates how to make CSC practical for HDR imaging. We demonstrate\nthat the proposed algorithm achieves higher-quality reconstructions than\nalternative methods, we evaluate optical coding schemes, analyze algorithmic\nparameters, and build a prototype coded HDR camera that demonstrates the\nutility of convolutional sparse HDRI coding with a custom hardware platform. \n\n"}
{"id": "1806.05154", "contents": "Title: Automated Performance Assessment in Transoesophageal Echocardiography\n  with Convolutional Neural Networks Abstract: Transoesophageal echocardiography (TEE) is a valuable diagnostic and\nmonitoring imaging modality. Proper image acquisition is essential for\ndiagnosis, yet current assessment techniques are solely based on manual expert\nreview. This paper presents a supervised deep learn ing framework for\nautomatically evaluating and grading the quality of TEE images. To obtain the\nnecessary dataset, 38 participants of varied experience performed TEE exams\nwith a high-fidelity virtual reality (VR) platform. Two Convolutional Neural\nNetwork (CNN) architectures, AlexNet and VGG, structured to perform regression,\nwere finetuned and validated on manually graded images from three evaluators.\nTwo different scoring strategies, a criteria-based percentage and an overall\ngeneral impression, were used. The developed CNN models estimate the average\nscore with a root mean square accuracy ranging between 84%-93%, indicating the\nability to replicate expert valuation. Proposed strategies for automated TEE\nassessment can have a significant impact on the training process of new TEE\noperators, providing direct feedback and facilitating the development of the\nnecessary dexterous skills. \n\n"}
{"id": "1806.05361", "contents": "Title: View-volume Network for Semantic Scene Completion from a Single Depth\n  Image Abstract: We introduce a View-Volume convolutional neural network (VVNet) for inferring\nthe occupancy and semantic labels of a volumetric 3D scene from a single depth\nimage. The VVNet concatenates a 2D view CNN and a 3D volume CNN with a\ndifferentiable projection layer. Given a single RGBD image, our method extracts\nthe detailed geometric features from the input depth image with a 2D view CNN\nand then projects the features into a 3D volume according to the input depth\nmap via a projection layer. After that, we learn the 3D context information of\nthe scene with a 3D volume CNN for computing the result volumetric occupancy\nand semantic labels. With combined 2D and 3D representations, the VVNet\nefficiently reduces the computational cost, enables feature extraction from\nmulti-channel high resolution inputs, and thus significantly improves the\nresult accuracy. We validate our method and demonstrate its efficiency and\neffectiveness on both synthetic SUNCG and real NYU dataset. \n\n"}
{"id": "1806.05569", "contents": "Title: Cardiac Motion Scoring with Segment- and Subject-level Non-Local\n  Modeling Abstract: Motion scoring of cardiac myocardium is of paramount importance for early\ndetection and diagnosis of various cardiac disease. It aims at identifying\nregional wall motions into one of the four types including normal, hypokinetic,\nakinetic, and dyskinetic, and is extremely challenging due to the complex\nmyocardium deformation and subtle inter-class difference of motion patterns.\nAll existing work on automated motion analysis are focused on binary\nabnormality detection to avoid the much more demanding motion scoring, which is\nurgently required in real clinical practice yet has never been investigated\nbefore. In this work, we propose Cardiac-MOS, the first powerful method for\ncardiac motion scoring from cardiac MR sequences based on deep convolution\nneural network. Due to the locality of convolution, the relationship between\ndistant non-local responses of the feature map cannot be explored, which is\nclosely related to motion difference between segments. In Cardiac-MOS, such\nnon-local relationship is modeled with non-local neural network within each\nsegment and across all segments of one subject, i.e., segment- and\nsubject-level non-local modeling, and lead to obvious performance improvement.\nBesides, Cardiac-MOS can effectively extract motion information from MR\nsequences of various lengths by interpolating the convolution kernel along the\ntemporal dimension, therefore can be applied to MR sequences of multiple\nsources. Experiments on 1440 myocardium segments of 90 subjects from short axis\nMR sequences of multiple lengths prove that Cardiac-MOS achieves reliable\nperformance, with correlation of 0.926 for motion score index estimation and\naccuracy of 77.4\\% for motion scoring. Cardiac-MOS also outperforms all\nexisting work for binary abnormality detection. As the first automatic motion\nscoring solution, Cardiac-MOS demonstrates great potential in future clinical\napplication. \n\n"}
{"id": "1806.05946", "contents": "Title: Efficient Nearest Neighbors Search for Large-Scale Landmark Recognition Abstract: The problem of landmark recognition has achieved excellent results in\nsmall-scale datasets. When dealing with large-scale retrieval, issues that were\nirrelevant with small amount of data, quickly become fundamental for an\nefficient retrieval phase. In particular, computational time needs to be kept\nas low as possible, whilst the retrieval accuracy has to be preserved as much\nas possible. In this paper we propose a novel multi-index hashing method called\nBag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows\nto drastically reduce the query time and outperforms the accuracy results\ncompared to the state-of-the-art methods for large-scale landmark recognition.\nIt has been demonstrated that this family of algorithms can be applied on\ndifferent embedding techniques like VLAD and R-MAC obtaining excellent results\nin very short times on different public datasets: Holidays+Flickr1M, Oxford105k\nand Paris106k. \n\n"}
{"id": "1806.05978", "contents": "Title: Uncertainty Estimations by Softplus normalization in Bayesian\n  Convolutional Neural Networks with Variational Inference Abstract: We introduce a novel uncertainty estimation for classification tasks for\nBayesian convolutional neural networks with variational inference. By\nnormalizing the output of a Softplus function in the final layer, we estimate\naleatoric and epistemic uncertainty in a coherent manner. The intractable\nposterior probability distributions over weights are inferred by Bayes by\nBackprop. Firstly, we demonstrate how this reliable variational inference\nmethod can serve as a fundamental construct for various network architectures.\nOn multiple datasets in supervised learning settings (MNIST, CIFAR-10,\nCIFAR-100), this variational inference method achieves performances equivalent\nto frequentist inference in identical architectures, while the two desiderata,\na measure for uncertainty and regularization are incorporated naturally.\nSecondly, we examine how our proposed measure for aleatoric and epistemic\nuncertainties is derived and validate it on the aforementioned datasets. \n\n"}
{"id": "1806.06392", "contents": "Title: Task-Relevant Object Discovery and Categorization for Playing\n  First-person Shooter Games Abstract: We consider the problem of learning to play first-person shooter (FPS) video\ngames using raw screen images as observations and keyboard inputs as actions.\nThe high-dimensionality of the observations in this type of applications leads\nto prohibitive needs of training data for model-free methods, such as the deep\nQ-network (DQN), and its recurrent variant DRQN. Thus, recent works focused on\nlearning low-dimensional representations that may reduce the need for data.\nThis paper presents a new and efficient method for learning such\nrepresentations. Salient segments of consecutive frames are detected from their\noptical flow, and clustered based on their feature descriptors. The clusters\ntypically correspond to different discovered categories of objects. Segments\ndetected in new frames are then classified based on their nearest clusters.\nBecause only a few categories are relevant to a given task, the importance of a\ncategory is defined as the correlation between its occurrence and the agent's\nperformance. The result is encoded as a vector indicating objects that are in\nthe frame and their locations, and used as a side input to DRQN. Experiments on\nthe game Doom provide a good evidence for the benefit of this approach. \n\n"}
{"id": "1806.06423", "contents": "Title: A Novel Hybrid Machine Learning Model for Auto-Classification of Retinal\n  Diseases Abstract: Automatic clinical diagnosis of retinal diseases has emerged as a promising\napproach to facilitate discovery in areas with limited access to specialists.\nWe propose a novel visual-assisted diagnosis hybrid model based on the support\nvector machine (SVM) and deep neural networks (DNNs). The model incorporates\ncomplementary strengths of DNNs and SVM. Furthermore, we present a new clinical\nretina label collection for ophthalmology incorporating 32 retina diseases\nclasses. Using EyeNet, our model achieves 89.73% diagnosis accuracy and the\nmodel performance is comparable to the professional ophthalmologists. \n\n"}
{"id": "1806.06970", "contents": "Title: Deconvolving convolution neural network for cell detection Abstract: Automatic cell detection in histology images is a challenging task due to\nvarying size, shape and features of cells and stain variations across a large\ncohort. Conventional deep learning methods regress the probability of each\npixel belonging to the centre of a cell followed by detection of local maxima.\nWe present deconvolution as an alternate approach to local maxima detection.\nThe ground truth points are convolved with a mapping filter to generate\nartifical labels. A convolutional neural network (CNN) is modified to convolve\nit's output with the same mapping filter and is trained for the mapped labels.\nOutput of the trained CNN is then deconvolved to generate points as cell\ndetection. We compare our method with state-of-the-art deep learning approaches\nwhere the results show that the proposed approach detects cells with\ncomparatively high precision and F1-score. \n\n"}
{"id": "1806.07237", "contents": "Title: Magnetic Resonance Spectroscopy Quantification using Deep Learning Abstract: Magnetic resonance spectroscopy (MRS) is an important technique in biomedical\nresearch and it has the unique capability to give a non-invasive access to the\nbiochemical content (metabolites) of scanned organs. In the literature, the\nquantification (the extraction of the potential biomarkers from the MRS\nsignals) involves the resolution of an inverse problem based on a parametric\nmodel of the metabolite signal. However, poor signal-to-noise ratio (SNR),\npresence of the macromolecule signal or high correlation between metabolite\nspectral patterns can cause high uncertainties for most of the metabolites,\nwhich is one of the main reasons that prevents use of MRS in clinical routine.\nIn this paper, quantification of metabolites in MR Spectroscopic imaging using\ndeep learning is proposed. A regression framework based on the Convolutional\nNeural Networks (CNN) is introduced for an accurate estimation of spectral\nparameters. The proposed model learns the spectral features from a large-scale\nsimulated data set with different variations of human brain spectra and SNRs.\nExperimental results demonstrate the accuracy of the proposed method, compared\nto state of the art standard quantification method (QUEST), on concentration of\n20 metabolites and the macromolecule. \n\n"}
{"id": "1806.07382", "contents": "Title: In situ TensorView: In situ Visualization of Convolutional Neural\n  Networks Abstract: Convolutional Neural Networks(CNNs) are complex systems. They are trained so\nthey can adapt their internal connections to recognize images, texts and more.\nIt is both interesting and helpful to visualize the dynamics within such deep\nartificial neural networks so that people can understand how these artificial\nnetworks are learning and making predictions. In the field of scientific\nsimulations, visualization tools like Paraview have long been utilized to\nprovide insights and understandings. We present in situ TensorView to visualize\nthe training and functioning of CNNs as if they are systems of scientific\nsimulations. In situ TensorView is a loosely coupled in situ visualization open\nframework that provides multiple viewers to help users to visualize and\nunderstand their networks. It leverages the capability of co-processing from\nParaview to provide real-time visualization during training and predicting\nphases. This avoid heavy I/O overhead for visualizing large dynamic systems.\nOnly a small number of lines of codes are injected in TensorFlow framework. The\nvisualization can provide guidance to adjust the architecture of networks, or\ncompress the pre-trained networks. We showcase visualizing the training of\nLeNet-5 and VGG16 using in situ TensorView. \n\n"}
{"id": "1806.07441", "contents": "Title: Wall Stress Estimation of Cerebral Aneurysm based on Zernike\n  Convolutional Neural Networks Abstract: Convolutional neural networks (ConvNets) have demonstrated an exceptional\ncapacity to discern visual patterns from digital images and signals.\nUnfortunately, such powerful ConvNets do not generalize well to\narbitrary-shaped manifolds, where data representation does not fit into a\ntensor-like grid. Hence, many fields of science and engineering, where data\npoints possess some manifold structure, cannot enjoy the full benefits of the\nrecent advances in ConvNets. The aneurysm wall stress estimation problem\nintroduced in this paper is one of many such problems. The problem is\nwell-known to be of a paramount clinical importance, but yet, traditional\nConvNets cannot be applied due to the manifold structure of the data, neither\ndoes the state-of-the-art geometric ConvNets perform well. Motivated by this,\nwe propose a new geometric ConvNet method named ZerNet, which builds upon our\nnovel mathematical generalization of convolution and pooling operations on\nmanifolds. Our study shows that the ZerNet outperforms the other\nstate-of-the-art geometric ConvNets in terms of accuracy. \n\n"}
{"id": "1806.07589", "contents": "Title: A CADe System for Gliomas in Brain MRI using Convolutional Neural\n  Networks Abstract: Inspired by the success of Convolutional Neural Networks (CNN), we develop a\nnovel Computer Aided Detection (CADe) system using CNN for Glioblastoma\nMultiforme (GBM) detection and segmentation from multi channel MRI data. A\ntwo-stage approach first identifies the presence of GBM. This is followed by a\nGBM localization in each \"abnormal\" MR slice. As part of the CADe system, two\nCNN architectures viz. Classification CNN (C-CNN) and Detection CNN (D-CNN) are\nemployed. The CADe system considers MRI data consisting of four sequences\n($T_1$, $T_{1c},$ $T_2$, and $T_{2FLAIR}$) as input, and automatically\ngenerates the bounding boxes encompassing the tumor regions in each slice which\nis deemed abnormal. Experimental results demonstrate that the proposed CADe\nsystem, when used as a preliminary step before segmentation, can allow improved\ndelineation of tumor region while reducing false positives arising in normal\nareas of the brain. The GrowCut method, employed for tumor segmentation,\ntypically requires a foreground and background seed region for initialization.\nHere the algorithm is initialized with seeds automatically generated from the\noutput of the proposed CADe system, thereby resulting in improved performance\nas compared to that using random seeds. \n\n"}
{"id": "1806.07836", "contents": "Title: How Bad is Good enough: Noisy annotations for instrument pose estimation Abstract: Though analysis of Medical Images by Deep Learning achieves unprecedented\nresults across various applications, the effect of \\emph{noisy training\nannotations} is rarely studied in a systematic manner. In Medical Image\nAnalysis, most reports addressing this question concentrate on studying\nsegmentation performance of deep learning classifiers. The absence of\ncontinuous ground truth annotations in these studies limits the value of\nconclusions for applications, where regression is the primary method of choice.\nIn the application of surgical instrument pose estimation, where precision has\na direct clinical impact on patient outcome, studying the effect of \\emph{noisy\nannotations} on deep learning pose estimation techniques is of supreme\nimportance. Real x-ray images are inadequate for this evaluation due to the\nunavailability of ground truth annotations. We circumvent this problem by\ngenerating synthetic radiographs, where the ground truth pose is known and\ntherefore the pose estimation error made by the medical-expert can be estimated\nfrom experiments. Furthermore, this study shows the property of deep neural\nnetworks to learn dominant signals from noisy annotations with sufficient data\nin a regression setting. \n\n"}
{"id": "1806.07840", "contents": "Title: Edge Intelligence: On-Demand Deep Learning Model Co-Inference with\n  Device-Edge Synergy Abstract: As the backbone technology of machine learning, deep neural networks (DNNs)\nhave have quickly ascended to the spotlight. Running DNNs on\nresource-constrained mobile devices is, however, by no means trivial, since it\nincurs high performance and energy overhead. While offloading DNNs to the cloud\nfor execution suffers unpredictable performance, due to the uncontrolled long\nwide-area network latency. To address these challenges, in this paper, we\npropose Edgent, a collaborative and on-demand DNN co-inference framework with\ndevice-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that\nadaptively partitions DNN computation between device and edge, in order to\nleverage hybrid computation resources in proximity for real-time DNN inference.\n(2) DNN right-sizing that accelerates DNN inference through early-exit at a\nproper intermediate DNN layer to further reduce the computation latency. The\nprototype implementation and extensive evaluations based on Raspberry Pi\ndemonstrate Edgent's effectiveness in enabling on-demand low-latency edge\nintelligence. \n\n"}
{"id": "1806.08198", "contents": "Title: DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural\n  Architectures Abstract: Recent breakthroughs in Neural Architectural Search (NAS) have achieved\nstate-of-the-art performances in applications such as image classification and\nlanguage modeling. However, these techniques typically ignore device-related\nobjectives such as inference time, memory usage, and power consumption.\nOptimizing neural architecture for device-related objectives is immensely\ncrucial for deploying deep networks on portable devices with limited computing\nresources. We propose DPP-Net: Device-aware Progressive Search for\nPareto-optimal Neural Architectures, optimizing for both device-related (e.g.,\ninference time and memory usage) and device-agnostic (e.g., accuracy and model\nsize) objectives. DPP-Net employs a compact search space inspired by current\nstate-of-the-art mobile CNNs, and further improves search efficiency by\nadopting progressive search (Liu et al. 2017). Experimental results on CIFAR-10\nare poised to demonstrate the effectiveness of Pareto-optimal networks found by\nDPP-Net, for three different devices: (1) a workstation with Titan X GPU, (2)\nNVIDIA Jetson TX1 embedded system, and (3) mobile phone with ARM Cortex-A53.\nCompared to CondenseNet and NASNet (Mobile), DPP-Net achieves better\nperformances: higher accuracy and shorter inference time on various devices.\nAdditional experimental results show that models found by DPP-Net also achieve\nconsiderably-good performance on ImageNet as well. \n\n"}
{"id": "1806.08859", "contents": "Title: A deep learning framework for segmentation of retinal layers from OCT\n  images Abstract: Segmentation of retinal layers from Optical Coherence Tomography (OCT)\nvolumes is a fundamental problem for any computer aided diagnostic algorithm\ndevelopment. This requires preprocessing steps such as denoising, region of\ninterest extraction, flattening and edge detection all of which involve\nseparate parameter tuning. In this paper, we explore deep learning techniques\nto automate all these steps and handle the presence/absence of pathologies. A\nmodel is proposed consisting of a combination of Convolutional Neural Network\n(CNN) and Long Short Term Memory (LSTM). The CNN is used to extract layers of\ninterest image and extract the edges, while the LSTM is used to trace the layer\nboundary. This model is trained on a mixture of normal and AMD cases using\nminimal data. Validation results on three public datasets show that the\npixel-wise mean absolute error obtained with our system is 1.30 plus or minus\n0.48 which is lower than the inter-marker error of 1.79 plus or minus 0.76. Our\nmodel's performance is also on par with the existing methods. \n\n"}
{"id": "1806.08867", "contents": "Title: xGEMs: Generating Examplars to Explain Black-Box Models Abstract: This work proposes xGEMs or manifold guided exemplars, a framework to\nunderstand black-box classifier behavior by exploring the landscape of the\nunderlying data manifold as data points cross decision boundaries. To do so, we\ntrain an unsupervised implicit generative model -- treated as a proxy to the\ndata manifold. We summarize black-box model behavior quantitatively by\nperturbing data samples along the manifold. We demonstrate xGEMs' ability to\ndetect and quantify bias in model learning and also for understanding the\nchanges in model behavior as training progresses. \n\n"}
{"id": "1806.08990", "contents": "Title: Stroke-based Character Reconstruction Abstract: Background elimination for noisy character images or character images from\nreal scene is still a challenging problem, due to the bewildering backgrounds,\nuneven illumination, low resolution and different distortions. We propose a\nstroke-based character reconstruction(SCR) method that use a weighted quadratic\nBezier curve(WQBC) to represent strokes of a character. Only training on our\nsynthetic data, our stroke extractor can achieve excellent reconstruction\neffect in real scenes. Meanwhile. It can also help achieve great ability in\ndefending adversarial attacks of character recognizers. \n\n"}
{"id": "1806.09648", "contents": "Title: 3D Context Enhanced Region-based Convolutional Neural Network for\n  End-to-End Lesion Detection Abstract: Detecting lesions from computed tomography (CT) scans is an important but\ndifficult problem because non-lesions and true lesions can appear similar. 3D\ncontext is known to be helpful in this differentiation task. However, existing\nend-to-end detection frameworks of convolutional neural networks (CNNs) are\nmostly designed for 2D images. In this paper, we propose 3D context enhanced\nregion-based CNN (3DCE) to incorporate 3D context information efficiently by\naggregating feature maps of 2D images. 3DCE is easy to train and end-to-end in\ntraining and inference. A universal lesion detector is developed to detect all\nkinds of lesions in one algorithm using the DeepLesion dataset. Experimental\nresults on this challenging task prove the effectiveness of 3DCE. We have\nreleased the code of 3DCE in\nhttps://github.com/rsummers11/CADLab/tree/master/lesion_detector_3DCE. \n\n"}
{"id": "1806.10443", "contents": "Title: Deep Steganalysis: End-to-End Learning with Supervisory Information\n  beyond Class Labels Abstract: Recently, deep learning has shown its power in steganalysis. However, the\nproposed deep models have been often learned from pre-calculated noise\nresiduals with fixed high-pass filters rather than from raw images. In this\npaper, we propose a new end-to-end learning framework that can learn\nsteganalytic features directly from pixels. In the meantime, the high-pass\nfilters are also automatically learned. Besides class labels, we make use of\nadditional pixel level supervision of cover-stego image pair to jointly and\niteratively train the proposed network which consists of a residual calculation\nnetwork and a steganalysis network. The experimental results prove the\neffectiveness of the proposed architecture. \n\n"}
{"id": "1806.10574", "contents": "Title: This Looks Like That: Deep Learning for Interpretable Image Recognition Abstract: When we are faced with challenging image classification tasks, we often\nexplain our reasoning by dissecting the image, and pointing out prototypical\naspects of one class or another. The mounting evidence for each of the classes\nhelps us make our final decision. In this work, we introduce a deep network\narchitecture -- prototypical part network (ProtoPNet), that reasons in a\nsimilar way: the network dissects the image by finding prototypical parts, and\ncombines evidence from the prototypes to make a final classification. The model\nthus reasons in a way that is qualitatively similar to the way ornithologists,\nphysicians, and others would explain to people on how to solve challenging\nimage classification tasks. The network uses only image-level labels for\ntraining without any annotations for parts of images. We demonstrate our method\non the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show\nthat ProtoPNet can achieve comparable accuracy with its analogous\nnon-interpretable counterpart, and when several ProtoPNets are combined into a\nlarger network, it can achieve an accuracy that is on par with some of the\nbest-performing deep models. Moreover, ProtoPNet provides a level of\ninterpretability that is absent in other interpretable deep models. \n\n"}
{"id": "1806.11248", "contents": "Title: XGBoost: Scalable GPU Accelerated Learning Abstract: We describe the multi-GPU gradient boosting algorithm implemented in the\nXGBoost library (https://github.com/dmlc/xgboost). Our algorithm allows fast,\nscalable training on multi-GPU systems with all of the features of the XGBoost\nlibrary. We employ data compression techniques to minimise the usage of scarce\nGPU memory while still allowing highly efficient implementation. Using our\nalgorithm we show that it is possible to process 115 million training instances\nin under three minutes on a publicly available cloud computing instance. The\nalgorithm is implemented using end-to-end GPU parallelism, with prediction,\ngradient calculation, feature quantisation, decision tree construction and\nevaluation phases all computed on device. \n\n"}
{"id": "1806.11269", "contents": "Title: Action Recognition for Depth Video using Multi-view Dynamic Images Abstract: Dynamic imaging is a recently proposed action description paradigm for\nsimultaneously capturing motion and temporal evolution information,\nparticularly in the context of deep convolutional neural networks (CNNs).\nCompared with optical flow for motion characterization, dynamic imaging\nexhibits superior efficiency and compactness. Inspired by the success of\ndynamic imaging in RGB video, this study extends it to the depth domain. To\nbetter exploit three-dimensional (3D) characteristics, multi-view dynamic\nimages are proposed. In particular, the raw depth video is densely projected\nwith respect to different virtual imaging viewpoints by rotating the virtual\ncamera within the 3D space. Subsequently, dynamic images are extracted from the\nobtained multi-view depth videos and multi-view dynamic images are thus\nconstructed from these images. Accordingly, more view-tolerant visual cues can\nbe involved. A novel CNN model is then proposed to perform feature learning on\nmulti-view dynamic images. Particularly, the dynamic images from different\nviews share the same convolutional layers but correspond to different fully\nconnected layers. This is aimed at enhancing the tuning effectiveness on\nshallow convolutional layers by alleviating the gradient vanishing problem.\nMoreover, as the spatial occurrence variation of the actions may impair the\nCNN, an action proposal approach is also put forth. In experiments, the\nproposed approach can achieve state-of-the-art performance on three challenging\ndatasets. \n\n"}
{"id": "1807.00053", "contents": "Title: Task-Driven Convolutional Recurrent Models of the Visual System Abstract: Feed-forward convolutional neural networks (CNNs) are currently\nstate-of-the-art for object classification tasks such as ImageNet. Further,\nthey are quantitatively accurate models of temporally-averaged responses of\nneurons in the primate brain's visual system. However, biological visual\nsystems have two ubiquitous architectural features not shared with typical\nCNNs: local recurrence within cortical areas, and long-range feedback from\ndownstream areas to upstream areas. Here we explored the role of recurrence in\nimproving classification performance. We found that standard forms of\nrecurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the\nImageNet task. In contrast, novel cells that incorporated two structural\nfeatures, bypassing and gating, were able to boost task accuracy substantially.\nWe extended these design principles in an automated search over thousands of\nmodel architectures, which identified novel local recurrent cells and\nlong-range feedback connections useful for object recognition. Moreover, these\ntask-optimized ConvRNNs matched the dynamics of neural activity in the primate\nvisual system better than feedforward networks, suggesting a role for the\nbrain's recurrent connections in performing difficult visual behaviors. \n\n"}
{"id": "1807.01216", "contents": "Title: Local Gradients Smoothing: Defense against localized adversarial attacks Abstract: Deep neural networks (DNNs) have shown vulnerability to adversarial attacks,\ni.e., carefully perturbed inputs designed to mislead the network at inference\ntime. Recently introduced localized attacks, Localized and Visible Adversarial\nNoise (LaVAN) and Adversarial patch, pose a new challenge to deep learning\nsecurity by adding adversarial noise only within a specific region without\naffecting the salient objects in an image. Driven by the observation that such\nattacks introduce concentrated high-frequency changes at a particular image\nlocation, we have developed an effective method to estimate noise location in\ngradient domain and transform those high activation regions caused by\nadversarial noise in image domain while having minimal effect on the salient\nobject that is important for correct classification. Our proposed Local\nGradients Smoothing (LGS) scheme achieves this by regularizing gradients in the\nestimated noisy region before feeding the image to DNN for inference. We have\nshown the effectiveness of our method in comparison to other defense methods\nincluding Digital Watermarking, JPEG compression, Total Variance Minimization\n(TVM) and Feature squeezing on ImageNet dataset. In addition, we systematically\nstudy the robustness of the proposed defense mechanism against Back Pass\nDifferentiable Approximation (BPDA), a state of the art attack recently\ndeveloped to break defenses that transform an input sample to minimize the\nadversarial effect. Compared to other defense mechanisms, LGS is by far the\nmost resistant to BPDA in localized adversarial attack setting. \n\n"}
{"id": "1807.01670", "contents": "Title: Encoding Spatial Relations from Natural Language Abstract: Natural language processing has made significant inroads into learning the\nsemantics of words through distributional approaches, however representations\nlearnt via these methods fail to capture certain kinds of information implicit\nin the real world. In particular, spatial relations are encoded in a way that\nis inconsistent with human spatial reasoning and lacking invariance to\nviewpoint changes. We present a system capable of capturing the semantics of\nspatial relations such as behind, left of, etc from natural language. Our key\ncontributions are a novel multi-modal objective based on generating images of\nscenes from their textual descriptions, and a new dataset on which to train it.\nWe demonstrate that internal representations are robust to meaning preserving\ntransformations of descriptions (paraphrase invariance), while viewpoint\ninvariance is an emergent property of the system. \n\n"}
{"id": "1807.01990", "contents": "Title: Transfer Learning From Synthetic To Real Images Using Variational\n  Autoencoders For Precise Position Detection Abstract: Capturing and labeling camera images in the real world is an expensive task,\nwhereas synthesizing labeled images in a simulation environment is easy for\ncollecting large-scale image data. However, learning from only synthetic images\nmay not achieve the desired performance in the real world due to a gap between\nsynthetic and real images. We propose a method that transfers learned detection\nof an object position from a simulation environment to the real world. This\nmethod uses only a significantly limited dataset of real images while\nleveraging a large dataset of synthetic images using variational autoencoders.\nAdditionally, the proposed method consistently performed well in different\nlighting conditions, in the presence of other distractor objects, and on\ndifferent backgrounds. Experimental results showed that it achieved accuracy of\n1.5mm to 3.5mm on average. Furthermore, we showed how the method can be used in\na real-world scenario like a \"pick-and-place\" robotic task. \n\n"}
{"id": "1807.02739", "contents": "Title: Detecting Synapse Location and Connectivity by Signed Proximity\n  Estimation and Pruning with Deep Nets Abstract: Synaptic connectivity detection is a critical task for neural reconstruction\nfrom Electron Microscopy (EM) data. Most of the existing algorithms for synapse\ndetection do not identify the cleft location and direction of connectivity\nsimultaneously. The few methods that computes direction along with contact\nlocation have only been demonstrated to work on either dyadic (most common in\nvertebrate brain) or polyadic (found in fruit fly brain) synapses, but not on\nboth types. In this paper, we present an algorithm to automatically predict the\nlocation as well as the direction of both dyadic and polyadic synapses. The\nproposed algorithm first generates candidate synaptic connections from\nvoxelwise predictions of signed proximity generated by a 3D U-net. A second 3D\nCNN then prunes the set of candidates to produce the final detection of cleft\nand connectivity orientation. Experimental results demonstrate that the\nproposed method outperforms the existing methods for determining synapses in\nboth rodent and fruit fly brain. \n\n"}
{"id": "1807.03122", "contents": "Title: Fully Convolutional Networks for Automated Segmentation of Abdominal\n  Adipose Tissue Depots in Multicenter Water-Fat MRI Abstract: Purpose: An approach for the automated segmentation of visceral adipose\ntissue (VAT) and subcutaneous adipose tissue (SAT) in multicenter water-fat MRI\nscans of the abdomen was investigated, using two different neural network\narchitectures.\n  Methods: The two fully convolutional network architectures U-Net and V-Net\nwere trained, evaluated and compared on the water-fat MRI data. Data of the\nstudy Tellus with 90 scans from a single center was used for a 10-fold\ncross-validation in which the most successful configuration for both networks\nwas determined. These configurations were then tested on 20 scans of the\nmulticenter study beta-cell function in JUvenile Diabetes and Obesity\n(BetaJudo), which involved a different study population and scanning device.\n  Results: The U-Net outperformed the used implementation of the V-Net in both\ncross-validation and testing. In cross-validation, the U-Net reached average\ndice scores of 0.988 (VAT) and 0.992 (SAT). The average of the absolute\nquantification errors amount to 0.67% (VAT) and 0.39% (SAT). On the\nmulti-center test data, the U-Net performs only slightly worse, with average\ndice scores of 0.970 (VAT) and 0.987 (SAT) and quantification errors of 2.80%\n(VAT) and 1.65% (SAT).\n  Conclusion: The segmentations generated by the U-Net allow for reliable\nquantification and could therefore be viable for high-quality automated\nmeasurements of VAT and SAT in large-scale studies with minimal need for human\nintervention. The high performance on the multicenter test data furthermore\nshows the robustness of this approach for data of different patient\ndemographics and imaging centers, as long as a consistent imaging protocol is\nused. \n\n"}
{"id": "1807.03165", "contents": "Title: Sparse Deep Neural Network Exact Solutions Abstract: Deep neural networks (DNNs) have emerged as key enablers of machine learning.\nApplying larger DNNs to more diverse applications is an important challenge.\nThe computations performed during DNN training and inference are dominated by\noperations on the weight matrices describing the DNN. As DNNs incorporate more\nlayers and more neurons per layers, these weight matrices may be required to be\nsparse because of memory limitations. Sparse DNNs are one possible approach,\nbut the underlying theory is in the early stages of development and presents a\nnumber of challenges, including determining the accuracy of inference and\nselecting nonzero weights for training. Associative array algebra has been\ndeveloped by the big data community to combine and extend database, matrix, and\ngraph/network concepts for use in large, sparse data problems. Applying this\nmathematics to DNNs simplifies the formulation of DNN mathematics and reveals\nthat DNNs are linear over oscillating semirings. This work uses associative\narray DNNs to construct exact solutions and corresponding perturbation models\nto the rectified linear unit (ReLU) DNN equations that can be used to construct\ntest vectors for sparse DNN implementations over various precisions. These\nsolutions can be used for DNN verification, theoretical explorations of DNN\nproperties, and a starting point for the challenge of sparse training. \n\n"}
{"id": "1807.03470", "contents": "Title: Learning a Single Tucker Decomposition Network for Lossy Image\n  Compression with Multiple Bits-Per-Pixel Rates Abstract: Lossy image compression (LIC), which aims to utilize inexact approximations\nto represent an image more compactly, is a classical problem in image\nprocessing. Recently, deep convolutional neural networks (CNNs) have achieved\ninteresting results in LIC by learning an encoder-quantizer-decoder network\nfrom a large amount of data. However, existing CNN-based LIC methods usually\ncan only train a network for a specific bits-per-pixel (bpp). Such a \"one\nnetwork per bpp\" problem limits the generality and flexibility of CNNs to\npractical LIC applications. In this paper, we propose to learn a single CNN\nwhich can perform LIC at multiple bpp rates. A simple yet effective Tucker\nDecomposition Network (TDNet) is developed, where there is a novel tucker\ndecomposition layer (TDL) to decompose a latent image representation into a set\nof projection matrices and a core tensor. By changing the rank of the core\ntensor and its quantization, we can easily adjust the bpp rate of latent image\nrepresentation within a single CNN. Furthermore, an iterative non-uniform\nquantization scheme is presented to optimize the quantizer, and a\ncoarse-to-fine training strategy is introduced to reconstruct the decompressed\nimages. Extensive experiments demonstrate the state-of-the-art compression\nperformance of TDNet in terms of both PSNR and MS-SSIM indices. \n\n"}
{"id": "1807.04001", "contents": "Title: Learning Neural Models for End-to-End Clustering Abstract: We propose a novel end-to-end neural network architecture that, once trained,\ndirectly outputs a probabilistic clustering of a batch of input examples in one\npass. It estimates a distribution over the number of clusters $k$, and for each\n$1 \\leq k \\leq k_\\mathrm{max}$, a distribution over the individual cluster\nassignment for each data point. The network is trained in advance in a\nsupervised fashion on separate data to learn grouping by any perceptual\nsimilarity criterion based on pairwise labels (same/different group). It can\nthen be applied to different data containing different groups. We demonstrate\npromising performance on high-dimensional data like images (COIL-100) and\nspeech (TIMIT). We call this ``learning to cluster'' and show its conceptual\ndifference to deep metric learning, semi-supervise clustering and other related\napproaches while having the advantage of performing learnable clustering fully\nend-to-end. \n\n"}
{"id": "1807.04740", "contents": "Title: Negative Momentum for Improved Game Dynamics Abstract: Games generalize the single-objective optimization paradigm by introducing\ndifferent objective functions for different players. Differentiable games often\nproceed by simultaneous or alternating gradient updates. In machine learning,\ngames are gaining new importance through formulations like generative\nadversarial networks (GANs) and actor-critic systems. However, compared to\nsingle-objective optimization, game dynamics are more complex and less\nunderstood. In this paper, we analyze gradient-based methods with momentum on\nsimple games. We prove that alternating updates are more stable than\nsimultaneous updates. Next, we show both theoretically and empirically that\nalternating gradient updates with a negative momentum term achieves convergence\nin a difficult toy adversarial problem, but also on the notoriously difficult\nto train saturating GANs. \n\n"}
{"id": "1807.05162", "contents": "Title: Large-Scale Visual Speech Recognition Abstract: This work presents a scalable solution to open-vocabulary visual speech\nrecognition. To achieve this, we constructed the largest existing visual speech\nrecognition dataset, consisting of pairs of text and video clips of faces\nspeaking (3,886 hours of video). In tandem, we designed and trained an\nintegrated lipreading system, consisting of a video processing pipeline that\nmaps raw video to stable videos of lips and sequences of phonemes, a scalable\ndeep neural network that maps the lip videos to sequences of phoneme\ndistributions, and a production-level speech decoder that outputs sequences of\nwords. The proposed system achieves a word error rate (WER) of 40.9% as\nmeasured on a held-out set. In comparison, professional lipreaders achieve\neither 86.4% or 92.9% WER on the same dataset when having access to additional\ntypes of contextual information. Our approach significantly improves on other\nlipreading approaches, including variants of LipNet and of Watch, Attend, and\nSpell (WAS), which are only capable of 89.8% and 76.8% WER respectively. \n\n"}
{"id": "1807.06270", "contents": "Title: Bench-Marking Information Extraction in Semi-Structured Historical\n  Handwritten Records Abstract: In this report, we present our findings from benchmarking experiments for\ninformation extraction on historical handwritten marriage records Esposalles\nfrom IEHHR - ICDAR 2017 robust reading competition. The information extraction\nis modeled as semantic labeling of the sequence across 2 set of labels. This\ncan be achieved by sequentially or jointly applying handwritten text\nrecognition (HTR) and named entity recognition (NER). We deploy a pipeline\napproach where first we use state-of-the-art HTR and use its output as input\nfor NER. We show that given low resource setup and simple structure of the\nrecords, high performance of HTR ensures overall high performance. We explore\nthe various configurations of conditional random fields and neural networks to\nbenchmark NER on given certain noisy input. The best model on 10-fold\ncross-validation as well as blind test data uses n-gram features with\nbidirectional long short-term memory. \n\n"}
{"id": "1807.06677", "contents": "Title: Query-Conditioned Three-Player Adversarial Network for Video\n  Summarization Abstract: Video summarization plays an important role in video understanding by\nselecting key frames/shots. Traditionally, it aims to find the most\nrepresentative and diverse contents in a video as short summaries. Recently, a\nmore generalized task, query-conditioned video summarization, has been\nintroduced, which takes user queries into consideration to learn more\nuser-oriented summaries. In this paper, we propose a query-conditioned\nthree-player generative adversarial network to tackle this challenge. The\ngenerator learns the joint representation of the user query and the video\ncontent, and the discriminator takes three pairs of query-conditioned summaries\nas the input to discriminate the real summary from a generated and a random\none. A three-player loss is introduced for joint training of the generator and\nthe discriminator, which forces the generator to learn better summary results,\nand avoids the generation of random trivial summaries. Experiments on a\nrecently proposed query-conditioned video summarization benchmark dataset show\nthe efficiency and efficacy of our proposed method. \n\n"}
{"id": "1807.06757", "contents": "Title: On Evaluation of Embodied Navigation Agents Abstract: Skillful mobile operation in three-dimensional environments is a primary\ntopic of study in Artificial Intelligence. The past two years have seen a surge\nof creative work on navigation. This creative output has produced a plethora of\nsometimes incompatible task definitions and evaluation protocols. To coordinate\nongoing and future research in this area, we have convened a working group to\nstudy empirical methodology in navigation research. The present document\nsummarizes the consensus recommendations of this working group. We discuss\ndifferent problem statements and the role of generalization, present evaluation\nmeasures, and provide standard scenarios that can be used for benchmarking. \n\n"}
{"id": "1807.07295", "contents": "Title: Operator-in-the-Loop Deep Sequential Multi-camera Feature Fusion for\n  Person Re-identification Abstract: Given a target image as query, person re-identification systems retrieve a\nranked list of candidate matches on a per-camera basis. In deployed systems, a\nhuman operator scans these lists and labels sighted targets by touch or\nmouse-based selection. However, classical re-id approaches generate per-camera\nlists independently. Therefore, target identifications by operator in a subset\nof cameras cannot be utilized to improve ranking of the target in remaining set\nof network cameras. To address this shortcoming, we propose a novel sequential\nmulti-camera re-id approach. The proposed approach can accommodate human\noperator inputs and provides early gains via a monotonic improvement in target\nranking. At the heart of our approach is a fusion function which operates on\ndeep feature representations of query and candidate matches. We formulate an\noptimization procedure custom-designed to incrementally improve query\nrepresentation. Since existing evaluation methods cannot be directly adopted to\nour setting, we also propose two novel evaluation protocols. The results on two\nlarge-scale re-id datasets (Market-1501, DukeMTMC-reID) demonstrate that our\nmulti-camera method significantly outperforms baselines and other popular\nfeature fusion schemes. Additionally, we conduct a comparative subject-based\nstudy of human operator performance. The superior operator performance enabled\nby our approach makes a compelling case for its integration into deployable\nvideo-surveillance systems. \n\n"}
{"id": "1807.07433", "contents": "Title: Real-Time Stereo Vision for Road Surface 3-D Reconstruction Abstract: Stereo vision techniques have been widely used in civil engineering to\nacquire 3-D road data. The two important factors of stereo vision are accuracy\nand speed. However, it is very challenging to achieve both of them\nsimultaneously and therefore the main aim of developing a stereo vision system\nis to improve the trade-off between these two factors. In this paper, we\npresent a real-time stereo vision system used for road surface 3-D\nreconstruction. The proposed system is developed from our previously published\n3-D reconstruction algorithm where the perspective view of the target image is\nfirst transformed into the reference view, which not only increases the\ndisparity accuracy but also improves the processing speed. Then, the\ncorrelation cost between each pair of blocks is computed and stored in two 3-D\ncost volumes. To adaptively aggregate the matching costs from neighbourhood\nsystems, bilateral filtering is performed on the cost volumes. This greatly\nreduces the ambiguities during stereo matching and further improves the\nprecision of the estimated disparities. Finally, the subpixel resolution is\nachieved by conducting a parabola interpolation and the subpixel disparity map\nis used to reconstruct the 3-D road surface. The proposed algorithm is\nimplemented on an NVIDIA GTX 1080 GPU for the real-time purpose. The\nexperimental results illustrate that the reconstruction accuracy is around 3\nmm. \n\n"}
{"id": "1807.07674", "contents": "Title: Bounding Box Embedding for Single Shot Person Instance Segmentation Abstract: We present a bottom-up approach for the task of object instance segmentation\nusing a single-shot model. The proposed model employs a fully convolutional\nnetwork which is trained to predict class-wise segmentation masks as well as\nthe bounding boxes of the object instances to which each pixel belongs. This\nallows us to group object pixels into individual instances. Our network\narchitecture is based on the DeepLabv3+ model, and requires only minimal extra\ncomputation to achieve pixel-wise instance assignments. We apply our method to\nthe task of person instance segmentation, a common task relevant to many\napplications. We train our model with COCO data and report competitive results\nfor the person class in the COCO instance segmentation task. \n\n"}
{"id": "1807.07769", "contents": "Title: Physical Adversarial Examples for Object Detectors Abstract: Deep neural networks (DNNs) are vulnerable to adversarial\nexamples-maliciously crafted inputs that cause DNNs to make incorrect\npredictions. Recent work has shown that these attacks generalize to the\nphysical domain, to create perturbations on physical objects that fool image\nclassifiers under a variety of real-world conditions. Such attacks pose a risk\nto deep learning models used in safety-critical cyber-physical systems. In this\nwork, we extend physical attacks to more challenging object detection models, a\nbroader class of deep learning algorithms widely used to detect and label\nmultiple objects within a scene. Improving upon a previous physical attack on\nimage classifiers, we create perturbed physical objects that are either ignored\nor mislabeled by object detection models. We implement a Disappearance Attack,\nin which we cause a Stop sign to \"disappear\" according to the detector-either\nby covering thesign with an adversarial Stop sign poster, or by adding\nadversarial stickers onto the sign. In a video recorded in a controlled lab\nenvironment, the state-of-the-art YOLOv2 detector failed to recognize these\nadversarial Stop signs in over 85% of the video frames. In an outdoor\nexperiment, YOLO was fooled by the poster and sticker attacks in 72.5% and\n63.5% of the video frames respectively. We also use Faster R-CNN, a different\nobject detection model, to demonstrate the transferability of our adversarial\nperturbations. The created poster perturbation is able to fool Faster R-CNN in\n85.9% of the video frames in a controlled lab environment, and 40.2% of the\nvideo frames in an outdoor environment. Finally, we present preliminary results\nwith a new Creation Attack, where in innocuous physical stickers fool a model\ninto detecting nonexistent objects. \n\n"}
{"id": "1807.07946", "contents": "Title: Future Semantic Segmentation with Convolutional LSTM Abstract: We consider the problem of predicting semantic segmentation of future frames\nin a video. Given several observed frames in a video, our goal is to predict\nthe semantic segmentation map of future frames that are not yet observed. A\nreliable solution to this problem is useful in many applications that require\nreal-time decision making, such as autonomous driving. We propose a novel model\nthat uses convolutional LSTM (ConvLSTM) to encode the spatiotemporal\ninformation of observed frames for future prediction. We also extend our model\nto use bidirectional ConvLSTM to capture temporal information in both\ndirections. Our proposed approach outperforms other state-of-the-art methods on\nthe benchmark dataset. \n\n"}
{"id": "1807.08368", "contents": "Title: Modeling Brain Networks with Artificial Neural Networks Abstract: In this study, we propose a neural network approach to capture the functional\nconnectivities among anatomic brain regions. The suggested approach estimates a\nset of brain networks, each of which represents the connectivity patterns of a\ncognitive process. We employ two different architectures of neural networks to\nextract directed and undirected brain networks from functional Magnetic\nResonance Imaging (fMRI) data. Then, we use the edge weights of the estimated\nbrain networks to train a classifier, namely, Support Vector Machines(SVM) to\nlabel the underlying cognitive process. We compare our brain network models\nwith popular models, which generate similar functional brain networks. We\nobserve that both undirected and directed brain networks surpass the\nperformances of the network models used in the fMRI literature. We also observe\nthat directed brain networks offer more discriminative features compared to the\nundirected ones for recognizing the cognitive processes. The representation\npower of the suggested brain networks are tested in a task-fMRI dataset of\nHuman Connectome Project and a Complex Problem Solving dataset. \n\n"}
{"id": "1807.08381", "contents": "Title: Pedestrian Trajectory Prediction with Structured Memory Hierarchies Abstract: This paper presents a novel framework for human trajectory prediction based\non multimodal data (video and radar). Motivated by recent neuroscience\ndiscoveries, we propose incorporating a structured memory component in the\nhuman trajectory prediction pipeline to capture historical information to\nimprove performance. We introduce structured LSTM cells for modelling the\nmemory content hierarchically, preserving the spatiotemporal structure of the\ninformation and enabling us to capture both short-term and long-term context.\nWe demonstrate how this architecture can be extended to integrate salient\ninformation from multiple modalities to automatically store and retrieve\nimportant information for decision making without any supervision. We evaluate\nthe effectiveness of the proposed models on a novel multimodal dataset that we\nintroduce, consisting of 40,000 pedestrian trajectories, acquired jointly from\na radar system and a CCTV camera system installed in a public place. The\nperformance is also evaluated on the publicly available New York Grand Central\npedestrian database. In both settings, the proposed models demonstrate their\ncapability to better anticipate future pedestrian motion compared to existing\nstate of the art. \n\n"}
{"id": "1807.08931", "contents": "Title: CReaM: Condensed Real-time Models for Depth Prediction using\n  Convolutional Neural Networks Abstract: Since the resurgence of CNNs the robotic vision community has developed a\nrange of algorithms that perform classification, semantic segmentation and\nstructure prediction (depths, normals, surface curvature) using neural\nnetworks. While some of these models achieve state-of-the art results and super\nhuman level performance, deploying these models in a time critical robotic\nenvironment remains an ongoing challenge. Real-time frameworks are of paramount\nimportance to build a robotic society where humans and robots integrate\nseamlessly. To this end, we present a novel real-time structure prediction\nframework that predicts depth at 30fps on an NVIDIA-TX2. At the time of\nwriting, this is the first piece of work to showcase such a capability on a\nmobile platform. We also demonstrate with extensive experiments that neural\nnetworks with very large model capacities can be leveraged in order to train\naccurate condensed model architectures in a \"from teacher to student\" style\nknowledge transfer. \n\n"}
{"id": "1807.09289", "contents": "Title: Noise Contrastive Priors for Functional Uncertainty Abstract: Obtaining reliable uncertainty estimates of neural network predictions is a\nlong standing challenge. Bayesian neural networks have been proposed as a\nsolution, but it remains open how to specify their prior. In particular, the\ncommon practice of an independent normal prior in weight space imposes\nrelatively weak constraints on the function posterior, allowing it to\ngeneralize in unforeseen ways on inputs outside of the training distribution.\nWe propose noise contrastive priors (NCPs) to obtain reliable uncertainty\nestimates. The key idea is to train the model to output high uncertainty for\ndata points outside of the training distribution. NCPs do so using an input\nprior, which adds noise to the inputs of the current mini batch, and an output\nprior, which is a wide distribution given these inputs. NCPs are compatible\nwith any model that can output uncertainty estimates, are easy to scale, and\nyield reliable uncertainty estimates throughout training. Empirically, we show\nthat NCPs prevent overfitting outside of the training distribution and result\nin uncertainty estimates that are useful for active learning. We demonstrate\nthe scalability of our method on the flight delays data set, where we\nsignificantly improve upon previously published results. \n\n"}
{"id": "1807.09499", "contents": "Title: How good is my GAN? Abstract: Generative adversarial networks (GANs) are one of the most popular methods\nfor generating images today. While impressive results have been validated by\nvisual inspection, a number of quantitative criteria have emerged only\nrecently. We argue here that the existing ones are insufficient and need to be\nin adequation with the task at hand. In this paper we introduce two measures\nbased on image classification---GAN-train and GAN-test, which approximate the\nrecall (diversity) and precision (quality of the image) of GANs respectively.\nWe evaluate a number of recent GAN approaches based on these two measures and\ndemonstrate a clear difference in performance. Furthermore, we observe that the\nincreasing difficulty of the dataset, from CIFAR10 over CIFAR100 to ImageNet,\nshows an inverse correlation with the quality of the GANs, as clearly evident\nfrom our measures. \n\n"}
{"id": "1807.09937", "contents": "Title: HiDDeN: Hiding Data With Deep Networks Abstract: Recent work has shown that deep neural networks are highly sensitive to tiny\nperturbations of input images, giving rise to adversarial examples. Though this\nproperty is usually considered a weakness of learned models, we explore whether\nit can be beneficial. We find that neural networks can learn to use invisible\nperturbations to encode a rich amount of useful information. In fact, one can\nexploit this capability for the task of data hiding. We jointly train encoder\nand decoder networks, where given an input message and cover image, the encoder\nproduces a visually indistinguishable encoded image, from which the decoder can\nrecover the original message. We show that these encodings are competitive with\nexisting data hiding algorithms, and further that they can be made robust to\nnoise: our models learn to reconstruct hidden information in an encoded image\ndespite the presence of Gaussian blurring, pixel-wise dropout, cropping, and\nJPEG compression. Even though JPEG is non-differentiable, we show that a robust\nmodel can be trained using differentiable approximations. Finally, we\ndemonstrate that adversarial training improves the visual quality of encoded\nimages. \n\n"}
{"id": "1807.10037", "contents": "Title: Motion Feature Network: Fixed Motion Filter for Action Recognition Abstract: Spatio-temporal representations in frame sequences play an important role in\nthe task of action recognition. Previously, a method of using optical flow as a\ntemporal information in combination with a set of RGB images that contain\nspatial information has shown great performance enhancement in the action\nrecognition tasks. However, it has an expensive computational cost and requires\ntwo-stream (RGB and optical flow) framework. In this paper, we propose MFNet\n(Motion Feature Network) containing motion blocks which make it possible to\nencode spatio-temporal information between adjacent frames in a unified network\nthat can be trained end-to-end. The motion block can be attached to any\nexisting CNN-based action recognition frameworks with only a small additional\ncost. We evaluated our network on two of the action recognition datasets\n(Jester and Something-Something) and achieved competitive performances for both\ndatasets by training the networks from scratch. \n\n"}
{"id": "1807.10129", "contents": "Title: A Benchmark of Selected Algorithmic Differentiation Tools on Some\n  Problems in Computer Vision and Machine Learning Abstract: Algorithmic differentiation (AD) allows exact computation of derivatives\ngiven only an implementation of an objective function. Although many AD tools\nare available, a proper and efficient implementation of AD methods is not\nstraightforward. The existing tools are often too different to allow for a\ngeneral test suite. In this paper, we compare fifteen ways of computing\nderivatives including eleven automatic differentiation tools implementing\nvarious methods and written in various languages (C++, F#, MATLAB, Julia and\nPython), two symbolic differentiation tools, finite differences, and\nhand-derived computation.\n  We look at three objective functions from computer vision and machine\nlearning. These objectives are for the most part simple, in the sense that no\niterative loops are involved, and conditional statements are encapsulated in\nfunctions such as {\\tt abs} or {\\tt logsumexp}. However, it is important for\nthe success of algorithmic differentiation that such `simple' objective\nfunctions are handled efficiently, as so many problems in computer vision and\nmachine learning are of this form.\n  Of course, our results depend on programmer skill, and familiarity with the\ntools. However, we contend that this paper presents an important datapoint: a\nskilled programmer devoting roughly a week to each tool produced the timings we\npresent. We have made our implementations available as open source to allow the\ncommunity to replicate and update these benchmarks. \n\n"}
{"id": "1807.10335", "contents": "Title: A general metric for identifying adversarial images Abstract: It is well known that a determined adversary can fool a neural network by\nmaking imperceptible adversarial perturbations to an image. Recent studies have\nshown that these perturbations can be detected even without information about\nthe neural network if the strategy taken by the adversary is known beforehand.\nUnfortunately, these studies suffer from the generalization limitation -- the\ndetection method has to be recalibrated every time the adversary changes his\nstrategy. In this study, we attempt to overcome the generalization limitation\nby deriving a metric which reliably identifies adversarial images even when the\napproach taken by the adversary is unknown. Our metric leverages key\ndifferences between the spectra of clean and adversarial images when an image\nis treated as a matrix. Our metric is able to detect adversarial images across\ndifferent datasets and attack strategies without any additional re-calibration.\nIn addition, our approach provides geometric insights into several unanswered\nquestions about adversarial perturbations. \n\n"}
{"id": "1807.10413", "contents": "Title: Adapting control policies from simulation to reality using a pairwise\n  loss Abstract: This paper proposes an approach to domain transfer based on a pairwise loss\nfunction that helps transfer control policies learned in simulation onto a real\nrobot. We explore the idea in the context of a 'category level' manipulation\ntask where a control policy is learned that enables a robot to perform a mating\ntask involving novel objects. We explore the case where depth images are used\nas the main form of sensor input. Our experimental results demonstrate that\nproposed method consistently outperforms baseline methods that train only in\nsimulation or that combine real and simulated data in a naive way. \n\n"}
{"id": "1807.10437", "contents": "Title: Connecting Gaze, Scene, and Attention: Generalized Attention Estimation\n  via Joint Modeling of Gaze and Scene Saliency Abstract: This paper addresses the challenging problem of estimating the general visual\nattention of people in images. Our proposed method is designed to work across\nmultiple naturalistic social scenarios and provides a full picture of the\nsubject's attention and gaze. In contrast, earlier works on gaze and attention\nestimation have focused on constrained problems in more specific contexts. In\nparticular, our model explicitly represents the gaze direction and handles\nout-of-frame gaze targets. We leverage three different datasets using a\nmulti-task learning approach. We evaluate our method on widely used benchmarks\nfor single-tasks such as gaze angle estimation and attention-within-an-image,\nas well as on the new challenging task of generalized visual attention\nprediction. In addition, we have created extended annotations for the MMDB and\nGazeFollow datasets which are used in our experiments, which we will publicly\nrelease. \n\n"}
{"id": "1807.10574", "contents": "Title: Deep Learning Hyperspectral Image Classification Using Multiple\n  Class-based Denoising Autoencoders, Mixed Pixel Training Augmentation, and\n  Morphological Operations Abstract: Herein, we present a system for hyperspectral image segmentation that\nutilizes multiple class--based denoising autoencoders which are efficiently\ntrained. Moreover, we present a novel hyperspectral data augmentation method\nfor labelled HSI data using linear mixtures of pixels from each class, which\nhelps the system with edge pixels which are almost always mixed pixels.\nFinally, we utilize a deep neural network and morphological hole-filling to\nprovide robust image classification. Results run on the Salinas dataset verify\nthe high performance of the proposed algorithm. \n\n"}
{"id": "1807.10889", "contents": "Title: Pairwise Body-Part Attention for Recognizing Human-Object Interactions Abstract: In human-object interactions (HOI) recognition, conventional methods consider\nthe human body as a whole and pay a uniform attention to the entire body\nregion. They ignore the fact that normally, human interacts with an object by\nusing some parts of the body. In this paper, we argue that different body parts\nshould be paid with different attention in HOI recognition, and the\ncorrelations between different body parts should be further considered. This is\nbecause our body parts always work collaboratively. We propose a new pairwise\nbody-part attention model which can learn to focus on crucial parts, and their\ncorrelations for HOI recognition. A novel attention based feature selection\nmethod and a feature representation scheme that can capture pairwise\ncorrelations between body parts are introduced in the model. Our proposed\napproach achieved 4% improvement over the state-of-the-art results in HOI\nrecognition on the HICO dataset. We will make our model and source codes\npublicly available. \n\n"}
{"id": "1807.11206", "contents": "Title: Hard-Aware Point-to-Set Deep Metric for Person Re-identification Abstract: Person re-identification (re-ID) is a highly challenging task due to large\nvariations of pose, viewpoint, illumination, and occlusion. Deep metric\nlearning provides a satisfactory solution to person re-ID by training a deep\nnetwork under supervision of metric loss, e.g., triplet loss. However, the\nperformance of deep metric learning is greatly limited by traditional sampling\nmethods. To solve this problem, we propose a Hard-Aware Point-to-Set (HAP2S)\nloss with a soft hard-mining scheme. Based on the point-to-set triplet loss\nframework, the HAP2S loss adaptively assigns greater weights to harder samples.\nSeveral advantageous properties are observed when compared with other\nstate-of-the-art loss functions: 1) Accuracy: HAP2S loss consistently achieves\nhigher re-ID accuracies than other alternatives on three large-scale benchmark\ndatasets; 2) Robustness: HAP2S loss is more robust to outliers than other\nlosses; 3) Flexibility: HAP2S loss does not rely on a specific weight function,\ni.e., different instantiations of HAP2S loss are equally effective. 4)\nGenerality: In addition to person re-ID, we apply the proposed method to\ngeneric deep metric learning benchmarks including CUB-200-2011 and Cars196, and\nalso achieve state-of-the-art results. \n\n"}
{"id": "1807.11573", "contents": "Title: State-of-the-art and gaps for deep learning on limited training data in\n  remote sensing Abstract: Deep learning usually requires big data, with respect to both volume and\nvariety. However, most remote sensing applications only have limited training\ndata, of which a small subset is labeled. Herein, we review three\nstate-of-the-art approaches in deep learning to combat this challenge. The\nfirst topic is transfer learning, in which some aspects of one domain, e.g.,\nfeatures, are transferred to another domain. The next is unsupervised learning,\ne.g., autoencoders, which operate on unlabeled data. The last is generative\nadversarial networks, which can generate realistic looking data that can fool\nthe likes of both a deep learning network and human. The aim of this article is\nto raise awareness of this dilemma, to direct the reader to existing work and\nto highlight current gaps that need solving. \n\n"}
{"id": "1807.11626", "contents": "Title: MnasNet: Platform-Aware Neural Architecture Search for Mobile Abstract: Designing convolutional neural networks (CNN) for mobile devices is\nchallenging because mobile models need to be small and fast, yet still\naccurate. Although significant efforts have been dedicated to design and\nimprove mobile CNNs on all dimensions, it is very difficult to manually balance\nthese trade-offs when there are so many architectural possibilities to\nconsider. In this paper, we propose an automated mobile neural architecture\nsearch (MNAS) approach, which explicitly incorporate model latency into the\nmain objective so that the search can identify a model that achieves a good\ntrade-off between accuracy and latency. Unlike previous work, where latency is\nconsidered via another, often inaccurate proxy (e.g., FLOPS), our approach\ndirectly measures real-world inference latency by executing the model on mobile\nphones. To further strike the right balance between flexibility and search\nspace size, we propose a novel factorized hierarchical search space that\nencourages layer diversity throughout the network. Experimental results show\nthat our approach consistently outperforms state-of-the-art mobile CNN models\nacross multiple vision tasks. On the ImageNet classification task, our MnasNet\nachieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x\nfaster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than\nNASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP\nquality than MobileNets for COCO object detection. Code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/mnasnet \n\n"}
{"id": "1808.00022", "contents": "Title: Analyzing Human-Human Interactions: A Survey Abstract: Many videos depict people, and it is their interactions that inform us of\ntheir activities, relation to one another and the cultural and social setting.\nWith advances in human action recognition, researchers have begun to address\nthe automated recognition of these human-human interactions from video. The\nmain challenges stem from dealing with the considerable variation in recording\nsetting, the appearance of the people depicted and the coordinated performance\nof their interaction. This survey provides a summary of these challenges and\ndatasets to address these, followed by an in-depth discussion of relevant\nvision-based recognition and detection methods. We focus on recent, promising\nwork based on deep learning and convolutional neural networks (CNNs). Finally,\nwe outline directions to overcome the limitations of the current\nstate-of-the-art to analyze and, eventually, understand social human actions. \n\n"}
{"id": "1808.00046", "contents": "Title: Lip-Reading Driven Deep Learning Approach for Speech Enhancement Abstract: This paper proposes a novel lip-reading driven deep learning framework for\nspeech enhancement. The proposed approach leverages the complementary strengths\nof both deep learning and analytical acoustic modelling (filtering based\napproach) as compared to recently published, comparatively simpler benchmark\napproaches that rely only on deep learning. The proposed audio-visual (AV)\nspeech enhancement framework operates at two levels. In the first level, a\nnovel deep learning-based lip-reading regression model is employed. In the\nsecond level, lip-reading approximated clean-audio features are exploited,\nusing an enhanced, visually-derived Wiener filter (EVWF), for the clean audio\npower spectrum estimation. Specifically, a stacked long-short-term memory\n(LSTM) based lip-reading regression model is designed for clean audio features\nestimation using only temporal visual features considering different number of\nprior visual frames. For clean speech spectrum estimation, a new\nfilterbank-domain EVWF is formulated, which exploits estimated speech features.\nThe proposed EVWF is compared with conventional Spectral Subtraction and\nLog-Minimum Mean-Square Error methods using both ideal AV mapping and LSTM\ndriven AV mapping. The potential of the proposed speech enhancement framework\nis evaluated under different dynamic real-world commercially-motivated\nscenarios (e.g. cafe, public transport, pedestrian area) at different SNR\nlevels (ranging from low to high SNRs) using benchmark Grid and ChiME3 corpora.\nFor objective testing, perceptual evaluation of speech quality is used to\nevaluate the quality of restored speech. For subjective testing, the standard\nmean-opinion-score method is used with inferential statistics. Comparative\nsimulation results demonstrate significant lip-reading and speech enhancement\nimprovement in terms of both speech quality and speech intelligibility. \n\n"}
{"id": "1808.00193", "contents": "Title: Reinforced Evolutionary Neural Architecture Search Abstract: Neural Architecture Search (NAS) is an important yet challenging task in\nnetwork design due to its high computational consumption. To address this\nissue, we propose the Reinforced Evolutionary Neural Architecture Search (RE-\nNAS), which is an evolutionary method with the reinforced mutation for NAS. Our\nmethod integrates reinforced mutation into an evolution algorithm for neural\narchitecture exploration, in which a mutation controller is introduced to learn\nthe effects of slight modifications and make mutation actions. The reinforced\nmutation controller guides the model population to evolve efficiently.\nFurthermore, as child models can inherit parameters from their parents during\nevolution, our method requires very limited computational resources. In\nexperiments, we conduct the proposed search method on CIFAR-10 and obtain a\npowerful network architecture, RENASNet. This architecture achieves a\ncompetitive result on CIFAR-10. The explored network architecture is\ntransferable to ImageNet and achieves a new state-of-the-art accuracy, i.e.,\n75.7% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test\nits performance on semantic segmentation with DeepLabv3 on the PASCAL VOC.\nRENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83%\nmIOU without being pre-trained on COCO. \n\n"}
{"id": "1808.00783", "contents": "Title: The Quest for the Golden Activation Function Abstract: Deep Neural Networks have been shown to be beneficial for a variety of tasks,\nin particular allowing for end-to-end learning and reducing the requirement for\nmanual design decisions. However, still many parameters have to be chosen in\nadvance, also raising the need to optimize them. One important, but often\nignored system parameter is the selection of a proper activation function.\nThus, in this paper we target to demonstrate the importance of activation\nfunctions in general and show that for different tasks different activation\nfunctions might be meaningful. To avoid the manual design or selection of\nactivation functions, we build on the idea of genetic algorithms to learn the\nbest activation function for a given task. In addition, we introduce two new\nactivation functions, ELiSH and HardELiSH, which can easily be incorporated in\nour framework. In this way, we demonstrate for three different image\nclassification benchmarks that different activation functions are learned, also\nshowing improved results compared to typically used baselines. \n\n"}
{"id": "1808.01101", "contents": "Title: Exploiting Local Indexing and Deep Feature Confidence Scores for Fast\n  Image-to-Video Search Abstract: The cost-effective visual representation and fast query-by-example search are\ntwo challenging goals that should be maintained for web-scale visual retrieval\ntasks on moderate hardware. This paper introduces a fast and robust method that\nensures both of these goals by obtaining state-of-the-art performance for an\nimage-to-video search scenario. Hence, we present critical enhancements to\nwell-known indexing and visual representation techniques by promoting faster,\nbetter and moderate retrieval performance. We also boost the superiority of our\nmethod for some visual challenges by exploiting individual decisions of local\nand global descriptors at query time. For instance, local content descriptors\nrepresent copied/duplicated scenes with large geometric deformations such as\nscale, orientation and affine transformation. In contrast, the use of global\ncontent descriptors is more practical for near-duplicate and semantic searches.\nExperiments are conducted on a large-scale Stanford I2V dataset. The\nexperimental results show that our method is useful in terms of complexity and\nquery processing time for large-scale visual retrieval scenarios, even if local\nand global representations are used together. The proposed method is superior\nand achieves state-of-the-art performance based on the mean average precision\n(MAP) score of this dataset. Lastly, we report additional MAP scores after\nupdating the ground annotations unveiled by retrieval results of the proposed\nmethod, and it shows that the actual performance. \n\n"}
{"id": "1808.03096", "contents": "Title: On feature selection and evaluation of transportation mode prediction\n  strategies Abstract: Transportation modes prediction is a fundamental task for decision making in\nsmart cities and traffic management systems. Traffic policies designed based on\ntrajectory mining can save money and time for authorities and the public. It\nmay reduce the fuel consumption and commute time and moreover, may provide more\npleasant moments for residents and tourists. Since the number of features that\nmay be used to predict a user transportation mode can be substantial, finding a\nsubset of features that maximizes a performance measure is worth investigating.\nIn this work, we explore wrapper and information retrieval methods to find the\nbest subset of trajectory features. After finding the best classifier and the\nbest feature subset, our results were compared with two related papers that\napplied deep learning methods and the results showed that our framework\nachieved better performance. Furthermore, two types of cross-validation\napproaches were investigated, and the performance results show that the random\ncross-validation method provides optimistic results. \n\n"}
{"id": "1808.03823", "contents": "Title: Learning Discriminative 3D Shape Representations by View Discerning\n  Networks Abstract: In view-based 3D shape recognition, extracting discriminative visual\nrepresentation of 3D shapes from projected images is considered the core\nproblem. Projections with low discriminative ability can adversely influence\nthe final 3D shape representation. Especially under the real situations with\nbackground clutter and object occlusion, the adverse effect is even more\nsevere. To resolve this problem, we propose a novel deep neural network, View\nDiscerning Network, which learns to judge the quality of views and adjust their\ncontributions to the representation of shapes. In this network, a Score\nGeneration Unit is devised to evaluate the quality of each projected image with\nscore vectors. These score vectors are used to weight the image features and\nthe weighted features perform much better than original features in 3D shape\nrecognition task. In particular, we introduce two structures of Score\nGeneration Unit, Channel-wise Score Unit and Part-wise Score Unit, to assess\nthe quality of feature maps from different perspectives. Our network aggregates\nfeatures and scores in an end-to-end framework, so that final shape descriptors\nare directly obtained from its output. Our experiments on ModelNet and ShapeNet\nCore55 show that View Discerning Network outperforms the state-of-the-arts in\nterms of the retrieval task, with excellent robustness against background\nclutter and object occlusion. \n\n"}
{"id": "1808.04334", "contents": "Title: Angular-Based Word Meta-Embedding Learning Abstract: Ensembling word embeddings to improve distributed word representations has\nshown good success for natural language processing tasks in recent years. These\napproaches either carry out straightforward mathematical operations over a set\nof vectors or use unsupervised learning to find a lower-dimensional\nrepresentation. This work compares meta-embeddings trained for different\nlosses, namely loss functions that account for angular distance between the\nreconstructed embedding and the target and those that account normalized\ndistances based on the vector length. We argue that meta-embeddings are better\nto treat the ensemble set equally in unsupervised learning as the respective\nquality of each embedding is unknown for upstream tasks prior to\nmeta-embedding. We show that normalization methods that account for this such\nas cosine and KL-divergence objectives outperform meta-embedding trained on\nstandard $\\ell_1$ and $\\ell_2$ loss on \\textit{defacto} word similarity and\nrelatedness datasets and find it outperforms existing meta-learning strategies. \n\n"}
{"id": "1808.04503", "contents": "Title: Shared Multi-Task Imitation Learning for Indoor Self-Navigation Abstract: Deep imitation learning enables robots to learn from expert demonstrations to\nperform tasks such as lane following or obstacle avoidance. However, in the\ntraditional imitation learning framework, one model only learns one task, and\nthus it lacks of the capability to support a robot to perform various different\nnavigation tasks with one model in indoor environments. This paper proposes a\nnew framework, Shared Multi-headed Imitation Learning(SMIL), that allows a\nrobot to perform multiple tasks with one model without switching among\ndifferent models. We model each task as a sub-policy and design a multi-headed\npolicy to learn the shared information among related tasks by summing up\nactivations from all sub-policies. Compared to single or non-shared\nmulti-headed policies, this framework is able to leverage correlated\ninformation among tasks to increase performance.We have implemented this\nframework using a robot based on NVIDIA TX2 and performed extensive experiments\nin indoor environments with different baseline solutions. The results\ndemonstrate that SMIL has doubled the performance over nonshared multi-headed\npolicy. \n\n"}
{"id": "1808.04572", "contents": "Title: Small Sample Learning in Big Data Era Abstract: As a promising area in artificial intelligence, a new learning paradigm,\ncalled Small Sample Learning (SSL), has been attracting prominent research\nattention in the recent years. In this paper, we aim to present a survey to\ncomprehensively introduce the current techniques proposed on this topic.\nSpecifically, current SSL techniques can be mainly divided into two categories.\nThe first category of SSL approaches can be called \"concept learning\", which\nemphasizes learning new concepts from only few related observations. The\npurpose is mainly to simulate human learning behaviors like recognition,\ngeneration, imagination, synthesis and analysis. The second category is called\n\"experience learning\", which usually co-exists with the large sample learning\nmanner of conventional machine learning. This category mainly focuses on\nlearning with insufficient samples, and can also be called small data learning\nin some literatures. More extensive surveys on both categories of SSL\ntechniques are introduced and some neuroscience evidences are provided to\nclarify the rationality of the entire SSL regime, and the relationship with\nhuman learning process. Some discussions on the main challenges and possible\nfuture research directions along this line are also presented. \n\n"}
{"id": "1808.05355", "contents": "Title: Conceptual Domain Adaptation Using Deep Learning Abstract: Deep learning has recently been shown to be instrumental in the problem of\ndomain adaptation, where the goal is to learn a model on a target domain using\na similar --but not identical-- source domain. The rationale for coupling both\ntechniques is the possibility of extracting common concepts across domains.\nConsidering (strictly) local representations, traditional deep learning assumes\ncommon concepts must be captured in the same hidden units. We contend that\njointly training a model with source and target data using a single deep\nnetwork is prone to failure when there is inherently lower-level\nrepresentational discrepancy between the two domains; such discrepancy leads to\na misalignment of corresponding concepts in separate hidden units. We introduce\na search framework to correctly align high-level representations when training\ndeep networks; such framework leads to the notion of conceptual --as opposed to\nrepresentational-- domain adaptation. \n\n"}
{"id": "1808.05730", "contents": "Title: Efficient Single-Shot Multibox Detector for Construction Site Monitoring Abstract: Asset monitoring in construction sites is an intricate, manually intensive\ntask, that can highly benefit from automated solutions engineered using deep\nneural networks. We use Single-Shot Multibox Detector --- SSD, for its fine\nbalance between speed and accuracy, to leverage ubiquitously available images\nand videos from the surveillance cameras on the construction sites and automate\nthe monitoring tasks, hence enabling project managers to better track the\nperformance and optimize the utilization of each resource. We propose to\nimprove the performance of SSD by clustering the predicted boxes instead of a\ngreedy approach like non-maximum suppression. We do so using Affinity\nPropagation Clustering --- APC to cluster the predicted boxes based on the\nsimilarity index computed using the spatial features as well as location of\npredicted boxes. In our attempts, we have been able to improve the mean average\nprecision of SSD by 3.77% on custom dataset consist of images from construction\nsites and by 1.67% on PASCAL VOC Challenge. \n\n"}
{"id": "1808.06220", "contents": "Title: Jointly Deep Multi-View Learning for Clustering Analysis Abstract: In this paper, we propose a novel Joint framework for Deep Multi-view\nClustering (DMJC), where multiple deep embedded features, multi-view fusion\nmechanism and clustering assignments can be learned simultaneously. Our key\nidea is that the joint learning strategy can sufficiently exploit\nclustering-friendly multi-view features and useful multi-view complementary\ninformation to improve the clustering performance. How to realize the\nmulti-view fusion in such a joint framework is the primary challenge. To do so,\nwe design two ingenious variants of deep multi-view joint clustering models\nunder the proposed framework, where multi-view fusion is implemented by two\ndifferent schemes. The first model, called DMJC-S, performs multi-view fusion\nin an implicit way via a novel multi-view soft assignment distribution. The\nsecond model, termed DMJC-T, defines a novel multi-view auxiliary target\ndistribution to conduct the multi-view fusion explicitly. Both DMJC-S and\nDMJC-T are optimized under a KL divergence like clustering objective.\nExperiments on six challenging image datasets demonstrate the superiority of\nboth DMJC-S and DMJC-T over single/multi-view baselines and the\nstate-of-the-art multiview clustering methods, which proves the effectiveness\nof the proposed DMJC framework. To our best knowledge, this is the first work\nto model the multi-view clustering in a deep joint framework, which will\nprovide a meaningful thinking in unsupervised multi-view learning. \n\n"}
{"id": "1808.06560", "contents": "Title: Multi-View Graph Embedding Using Randomized Shortest Paths Abstract: Real-world data sets often provide multiple types of information about the\nsame set of entities. This data is well represented by multi-view graphs, which\nconsist of several distinct sets of edges over the same nodes. These can be\nused to analyze how entities interact from different viewpoints. Combining\nmultiple views improves the quality of inferences drawn from the underlying\ndata, which has increased interest in developing efficient multi-view graph\nembedding methods. We propose an algorithm, C-RSP, that generates a common (C)\nembedding of a multi-view graph using Randomized Shortest Paths (RSP). This\nalgorithm generates a dissimilarity measure between nodes by minimizing the\nexpected cost of a random walk between any two nodes across all views of a\nmulti-view graph, in doing so encoding both the local and global structure of\nthe graph. We test C-RSP on both real and synthetic data and show that it\noutperforms benchmark algorithms at embedding and clustering tasks while\nremaining computationally efficient. \n\n"}
{"id": "1808.06809", "contents": "Title: Are You Tampering With My Data? Abstract: We propose a novel approach towards adversarial attacks on neural networks\n(NN), focusing on tampering the data used for training instead of generating\nattacks on trained models. Our network-agnostic method creates a backdoor\nduring training which can be exploited at test time to force a neural network\nto exhibit abnormal behaviour. We demonstrate on two widely used datasets\n(CIFAR-10 and SVHN) that a universal modification of just one pixel per image\nfor all the images of a class in the training set is enough to corrupt the\ntraining procedure of several state-of-the-art deep neural networks causing the\nnetworks to misclassify any images to which the modification is applied. Our\naim is to bring to the attention of the machine learning community, the\npossibility that even learning-based methods that are personally trained on\npublic datasets can be subject to attacks by a skillful adversary. \n\n"}
{"id": "1808.07413", "contents": "Title: Manipulating Attributes of Natural Scenes via Hallucination Abstract: In this study, we explore building a two-stage framework for enabling users\nto directly manipulate high-level attributes of a natural scene. The key to our\napproach is a deep generative network which can hallucinate images of a scene\nas if they were taken at a different season (e.g. during winter), weather\ncondition (e.g. in a cloudy day) or time of the day (e.g. at sunset). Once the\nscene is hallucinated with the given attributes, the corresponding look is then\ntransferred to the input image while preserving the semantic details intact,\ngiving a photo-realistic manipulation result. As the proposed framework\nhallucinates what the scene will look like, it does not require any reference\nstyle image as commonly utilized in most of the appearance or style transfer\napproaches. Moreover, it allows to simultaneously manipulate a given scene\naccording to a diverse set of transient attributes within a single model,\neliminating the need of training multiple networks per each translation task.\nOur comprehensive set of qualitative and quantitative results demonstrate the\neffectiveness of our approach against the competing methods. \n\n"}
{"id": "1808.07784", "contents": "Title: Time-Agnostic Prediction: Predicting Predictable Video Frames Abstract: Prediction is arguably one of the most basic functions of an intelligent\nsystem. In general, the problem of predicting events in the future or between\ntwo waypoints is exceedingly difficult. However, most phenomena naturally pass\nthrough relatively predictable bottlenecks---while we cannot predict the\nprecise trajectory of a robot arm between being at rest and holding an object\nup, we can be certain that it must have picked the object up. To exploit this,\nwe decouple visual prediction from a rigid notion of time. While conventional\napproaches predict frames at regularly spaced temporal intervals, our\ntime-agnostic predictors (TAP) are not tied to specific times so that they may\ninstead discover predictable \"bottleneck\" frames no matter when they occur. We\nevaluate our approach for future and intermediate frame prediction across three\nrobotic manipulation tasks. Our predictions are not only of higher visual\nquality, but also correspond to coherent semantic subgoals in temporally\nextended tasks. \n\n"}
{"id": "1808.07982", "contents": "Title: Proximal Policy Optimization and its Dynamic Version for Sequence\n  Generation Abstract: In sequence generation task, many works use policy gradient for model\noptimization to tackle the intractable backpropagation issue when maximizing\nthe non-differentiable evaluation metrics or fooling the discriminator in\nadversarial learning. In this paper, we replace policy gradient with proximal\npolicy optimization (PPO), which is a proved more efficient reinforcement\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\nperformance. \n\n"}
{"id": "1808.08692", "contents": "Title: Generalized Capsule Networks with Trainable Routing Procedure Abstract: CapsNet (Capsule Network) was first proposed by~\\citet{capsule} and later\nanother version of CapsNet was proposed by~\\citet{emrouting}. CapsNet has been\nproved effective in modeling spatial features with much fewer parameters.\nHowever, the routing procedures in both papers are not well incorporated into\nthe whole training process. The optimal number of routing procedure is misery\nwhich has to be found manually. To overcome this disadvantages of current\nrouting procedures in CapsNet, we embed the routing procedure into the\noptimization procedure with all other parameters in neural networks, namely,\nmake coupling coefficients in the routing procedure become completely\ntrainable. We call it Generalized CapsNet (G-CapsNet). We implement both\n\"full-connected\" version of G-CapsNet and \"convolutional\" version of G-CapsNet.\nG-CapsNet achieves a similar performance in the dataset MNIST as in the\noriginal papers. We also test two capsule packing method (cross feature maps or\nwith feature maps) from previous convolutional layers and see no evident\ndifference. Besides, we also explored possibility of stacking multiple capsule\nlayers. The code is shared on\n\\hyperlink{https://github.com/chenzhenhua986/CAFFE-CapsNet}{CAFFE-CapsNet}. \n\n"}
{"id": "1808.08931", "contents": "Title: Smoothed Dilated Convolutions for Improved Dense Prediction Abstract: Dilated convolutions, also known as atrous convolutions, have been widely\nexplored in deep convolutional neural networks (DCNNs) for various dense\nprediction tasks. However, dilated convolutions suffer from the gridding\nartifacts, which hampers the performance. In this work, we propose two simple\nyet effective degridding methods by studying a decomposition of dilated\nconvolutions. Unlike existing models, which explore solutions by focusing on a\nblock of cascaded dilated convolutional layers, our methods address the\ngridding artifacts by smoothing the dilated convolution itself. In addition, we\npoint out that the two degridding approaches are intrinsically related and\ndefine separable and shared (SS) operations, which generalize the proposed\nmethods. We further explore SS operations in view of operations on graphs and\npropose the SS output layer, which is able to smooth the entire DCNNs by only\nreplacing the output layer. We evaluate our degridding methods and the SS\noutput layer thoroughly, and visualize the smoothing effect through effective\nreceptive field analysis. Results show that our methods degridding yield\nconsistent improvements on the performance of dense prediction tasks, while\nadding negligible amounts of extra training parameters. And the SS output layer\nimproves the performance significantly and is very efficient in terms of number\nof training parameters. \n\n"}
{"id": "1808.09740", "contents": "Title: Cross-Domain Collaborative Learning via Cluster Canonical Correlation\n  Analysis and Random Walker for Hyperspectral Image Classification Abstract: This paper introduces a novel heterogenous domain adaptation (HDA) method for\nhyperspectral image classification with a limited amount of labeled samples in\nboth domains. The method is achieved in the way of cross-domain collaborative\nlearning (CDCL), which is addressed via cluster canonical correlation analysis\n(C-CCA) and random walker (RW) algorithms. To be specific, the proposed CDCL\nmethod is an iterative process of three main stages, i.e. twice of RW-based\npseudolabeling and cross domain learning via C-CCA. Firstly, given the\ninitially labeled target samples as training set ($\\mathbf{TS}$), the RW-based\npseudolabeling is employed to update $\\mathbf{TS}$ and extract target clusters\n($\\mathbf{TCs}$) by fusing the segmentation results obtained by RW and extended\nRW (ERW) classifiers. Secondly, cross domain learning via C-CCA is applied\nusing labeled source samples and $\\mathbf{TCs}$. The unlabeled target samples\nare then classified with the estimated probability maps using the model trained\nin the projected correlation subspace. Thirdly, both $\\mathbf{TS}$ and\nestimated probability maps are used for updating $\\mathbf{TS}$ again via\nRW-based pseudolabeling. When the iterative process finishes, the result\nobtained by the ERW classifier using the final $\\mathbf{TS}$ and estimated\nprobability maps is regarded as the final classification map. Experimental\nresults on four real HSIs demonstrate that the proposed method can achieve\nbetter performance compared with the state-of-the-art HDA and ERW methods. \n\n"}
{"id": "1808.09829", "contents": "Title: MACNet: Multi-scale Atrous Convolution Networks for Food Places\n  Classification in Egocentric Photo-streams Abstract: First-person (wearable) camera continually captures unscripted interactions\nof the camera user with objects, people, and scenes reflecting his personal and\nrelational tendencies. One of the preferences of people is their interaction\nwith food events. The regulation of food intake and its duration has a great\nimportance to protect against diseases. Consequently, this work aims to develop\na smart model that is able to determine the recurrences of a person on food\nplaces during a day. This model is based on a deep end-to-end model for\nautomatic food places recognition by analyzing egocentric photo-streams. In\nthis paper, we apply multi-scale Atrous convolution networks to extract the key\nfeatures related to food places of the input images. The proposed model is\nevaluated on an in-house private dataset called \"EgoFoodPlaces\". Experimental\nresults shows promising results of food places classification recognition in\negocentric photo-streams. \n\n"}
{"id": "1808.09940", "contents": "Title: Adversarial Deep Reinforcement Learning in Portfolio Management Abstract: In this paper, we implement three state-of-art continuous reinforcement\nlearning algorithms, Deep Deterministic Policy Gradient (DDPG), Proximal Policy\nOptimization (PPO) and Policy Gradient (PG)in portfolio management. All of them\nare widely-used in game playing and robot control. What's more, PPO has\nappealing theoretical propeties which is hopefully potential in portfolio\nmanagement. We present the performances of them under different settings,\nincluding different learning rates, objective functions, feature combinations,\nin order to provide insights for parameters tuning, features selection and data\npreparation. We also conduct intensive experiments in China Stock market and\nshow that PG is more desirable in financial market than DDPG and PPO, although\nboth of them are more advanced. What's more, we propose a so called Adversarial\nTraining method and show that it can greatly improve the training efficiency\nand significantly promote average daily return and sharpe ratio in back test.\nBased on this new modification, our experiments results show that our agent\nbased on Policy Gradient can outperform UCRP. \n\n"}
{"id": "1808.10146", "contents": "Title: Dense Scene Flow from Stereo Disparity and Optical Flow Abstract: Scene flow describes 3D motion in a 3D scene. It can either be modeled as a\nsingle task, or it can be reconstructed from the auxiliary tasks of stereo\ndepth and optical flow estimation. While the second method can achieve\nreal-time performance by using real-time auxiliary methods, it will typically\nproduce non-dense results. In this representation of a basic combination\napproach for scene flow estimation, we will tackle the problem of non-density\nby interpolation. \n\n"}
{"id": "1808.10356", "contents": "Title: Gaussian Mixture Generative Adversarial Networks for Diverse Datasets,\n  and the Unsupervised Clustering of Images Abstract: Generative Adversarial Networks (GANs) have been shown to produce\nrealistically looking synthetic images with remarkable success, yet their\nperformance seems less impressive when the training set is highly diverse. In\norder to provide a better fit to the target data distribution when the dataset\nincludes many different classes, we propose a variant of the basic GAN model,\ncalled Gaussian Mixture GAN (GM-GAN), where the probability distribution over\nthe latent space is a mixture of Gaussians. We also propose a supervised\nvariant which is capable of conditional sample synthesis. In order to evaluate\nthe model's performance, we propose a new scoring method which separately takes\ninto account two (typically conflicting) measures - diversity vs. quality of\nthe generated data. Through a series of empirical experiments, using both\nsynthetic and real-world datasets, we quantitatively show that GM-GANs\noutperform baselines, both when evaluated using the commonly used Inception\nScore, and when evaluated using our own alternative scoring method. In\naddition, we qualitatively demonstrate how the \\textit{unsupervised} variant of\nGM-GAN tends to map latent vectors sampled from different Gaussians in the\nlatent space to samples of different classes in the data space. We show how\nthis phenomenon can be exploited for the task of unsupervised clustering, and\nprovide quantitative evaluation showing the superiority of our method for the\nunsupervised clustering of image datasets. Finally, we demonstrate a feature\nwhich further sets our model apart from other GAN models: the option to control\nthe quality-diversity trade-off by altering, post-training, the probability\ndistribution of the latent space. This allows one to sample higher quality and\nlower diversity samples, or vice versa, according to one's needs. \n\n"}
{"id": "1808.10393", "contents": "Title: Learning End-to-end Autonomous Driving using Guided Auxiliary\n  Supervision Abstract: Learning to drive faithfully in highly stochastic urban settings remains an\nopen problem. To that end, we propose a Multi-task Learning from Demonstration\n(MT-LfD) framework which uses supervised auxiliary task prediction to guide the\nmain task of predicting the driving commands. Our framework involves an\nend-to-end trainable network for imitating the expert demonstrator's driving\ncommands. The network intermediately predicts visual affordances and action\nprimitives through direct supervision which provide the aforementioned\nauxiliary supervised guidance. We demonstrate that such joint learning and\nsupervised guidance facilitates hierarchical task decomposition, assisting the\nagent to learn faster, achieve better driving performance and increases\ntransparency of the otherwise black-box end-to-end network. We run our\nexperiments to validate the MT-LfD framework in CARLA, an open-source urban\ndriving simulator. We introduce multiple non-player agents in CARLA and induce\ntemporal noise in them for realistic stochasticity. \n\n"}
{"id": "1809.00060", "contents": "Title: Aesthetic Features for Personalized Photo Recommendation Abstract: Many photography websites such as Flickr, 500px, Unsplash, and Adobe Behance\nare used by amateur and professional photography enthusiasts. Unlike\ncontent-based image search, such users of photography websites are not just\nlooking for photos with certain content, but more generally for photos with a\ncertain photographic \"aesthetic\". In this context, we explore personalized\nphoto recommendation and propose two aesthetic feature extraction methods based\non (i) color space and (ii) deep style transfer embeddings. Using a dataset\nfrom 500px, we evaluate how these features can be best leveraged by\ncollaborative filtering methods and show that (ii) provides a significant boost\nin photo recommendation performance. \n\n"}
{"id": "1809.00496", "contents": "Title: LRS3-TED: a large-scale dataset for visual speech recognition Abstract: This paper introduces a new multi-modal dataset for visual and audio-visual\nspeech recognition. It includes face tracks from over 400 hours of TED and TEDx\nvideos, along with the corresponding subtitles and word alignment boundaries.\nThe new dataset is substantially larger in scale compared to other public\ndatasets that are available for general research. \n\n"}
{"id": "1809.01123", "contents": "Title: VideoMatch: Matching based Video Object Segmentation Abstract: Video object segmentation is challenging yet important in a wide variety of\napplications for video analysis. Recent works formulate video object\nsegmentation as a prediction task using deep nets to achieve appealing\nstate-of-the-art performance. Due to the formulation as a prediction task, most\nof these methods require fine-tuning during test time, such that the deep nets\nmemorize the appearance of the objects of interest in the given video. However,\nfine-tuning is time-consuming and computationally expensive, hence the\nalgorithms are far from real time. To address this issue, we develop a novel\nmatching based algorithm for video object segmentation. In contrast to\nmemorization based classification techniques, the proposed approach learns to\nmatch extracted features to a provided template without memorizing the\nappearance of the objects. We validate the effectiveness and the robustness of\nthe proposed method on the challenging DAVIS-16, DAVIS-17, Youtube-Objects and\nJumpCut datasets. Extensive results show that our method achieves comparable\nperformance without fine-tuning and is much more favorable in terms of\ncomputational time. \n\n"}
{"id": "1809.01124", "contents": "Title: Straight to the Facts: Learning Knowledge Base Retrieval for Factual\n  Visual Question Answering Abstract: Question answering is an important task for autonomous agents and virtual\nassistants alike and was shown to support the disabled in efficiently\nnavigating an overwhelming environment. Many existing methods focus on\nobservation-based questions, ignoring our ability to seamlessly combine\nobserved content with general knowledge. To understand interactions with a\nknowledge base, a dataset has been introduced recently and keyword matching\ntechniques were shown to yield compelling results despite being vulnerable to\nmisconceptions due to synonyms and homographs. To address this issue, we\ndevelop a learning-based approach which goes straight to the facts via a\nlearned embedding space. We demonstrate state-of-the-art results on the\nchallenging recently introduced fact-based visual question answering dataset,\noutperforming competing methods by more than 5%. \n\n"}
{"id": "1809.01185", "contents": "Title: DeepPINK: reproducible feature selection in deep neural networks Abstract: Deep learning has become increasingly popular in both supervised and\nunsupervised machine learning thanks to its outstanding empirical performance.\nHowever, because of their intrinsic complexity, most deep learning methods are\nlargely treated as black box tools with little interpretability. Even though\nrecent attempts have been made to facilitate the interpretability of deep\nneural networks (DNNs), existing methods are susceptible to noise and lack of\nrobustness.\n  Therefore, scientists are justifiably cautious about the reproducibility of\nthe discoveries, which is often related to the interpretability of the\nunderlying statistical models. In this paper, we describe a method to increase\nthe interpretability and reproducibility of DNNs by incorporating the idea of\nfeature selection with controlled error rate. By designing a new DNN\narchitecture and integrating it with the recently proposed knockoffs framework,\nwe perform feature selection with a controlled error rate, while maintaining\nhigh power. This new method, DeepPINK (Deep feature selection using\nPaired-Input Nonlinear Knockoffs), is applied to both simulated and real data\nsets to demonstrate its empirical utility. \n\n"}
{"id": "1809.01354", "contents": "Title: Semantic Human Matting Abstract: Human matting, high quality extraction of humans from natural images, is\ncrucial for a wide variety of applications. Since the matting problem is\nseverely under-constrained, most previous methods require user interactions to\ntake user designated trimaps or scribbles as constraints. This user-in-the-loop\nnature makes them difficult to be applied to large scale data or time-sensitive\nscenarios. In this paper, instead of using explicit user input constraints, we\nemploy implicit semantic constraints learned from data and propose an automatic\nhuman matting algorithm (SHM). SHM is the first algorithm that learns to\njointly fit both semantic information and high quality details with deep\nnetworks. In practice, simultaneously learning both coarse semantics and fine\ndetails is challenging. We propose a novel fusion strategy which naturally\ngives a probabilistic estimation of the alpha matte. We also construct a very\nlarge dataset with high quality annotations consisting of 35,513 unique\nforegrounds to facilitate the learning and evaluation of human matting.\nExtensive experiments on this dataset and plenty of real images show that SHM\nachieves comparable results with state-of-the-art interactive matting methods. \n\n"}
{"id": "1809.01372", "contents": "Title: Temporally Coherent Video Harmonization Using Adversarial Networks Abstract: Compositing is one of the most important editing operations for images and\nvideos. The process of improving the realism of composite results is often\ncalled harmonization. Previous approaches for harmonization mainly focus on\nimages. In this work, we take one step further to attack the problem of video\nharmonization. Specifically, we train a convolutional neural network in an\nadversarial way, exploiting a pixel-wise disharmony discriminator to achieve\nmore realistic harmonized results and introducing a temporal loss to increase\ntemporal consistency between consecutive harmonized frames. Thanks to the\npixel-wise disharmony discriminator, we are also able to relieve the need of\ninput foreground masks. Since existing video datasets which have ground-truth\nforeground masks and optical flows are not sufficiently large, we propose a\nsimple yet efficient method to build up a synthetic dataset supporting\nsupervised training of the proposed adversarial network. Experiments show that\ntraining on our synthetic dataset generalizes well to the real-world composite\ndataset. Also, our method successfully incorporates temporal consistency during\ntraining and achieves more harmonious results than previous methods. \n\n"}
{"id": "1809.01438", "contents": "Title: How is Contrast Encoded in Deep Neural Networks? Abstract: Contrast is a crucial factor in visual information processing. It is desired\nfor a visual system - irrespective of being biological or artificial - to\n\"perceive\" the world robustly under large potential changes in illumination. In\nthis work, we studied the responses of deep neural networks (DNN) to identical\nimages at different levels of contrast. We analysed the activation of kernels\nin the convolutional layers of eight prominent networks with distinct\narchitectures (e.g. VGG and Inception). The results of our experiments indicate\nthat those networks with a higher tolerance to alteration of contrast have more\nthan one convolutional layer prior to the first max-pooling operator. It\nappears that the last convolutional layer before the first max-pooling acts as\na mitigator of contrast variation in input images. In our investigation,\ninterestingly, we observed many similarities between the mechanisms of these\nDNNs and biological visual systems. These comparisons allow us to understand\nmore profoundly the underlying mechanisms of a visual system that is grounded\non the basis of \"data-analysis\". \n\n"}
{"id": "1809.03137", "contents": "Title: Tracking by Animation: Unsupervised Learning of Multi-Object Attentive\n  Trackers Abstract: Online Multi-Object Tracking (MOT) from videos is a challenging computer\nvision task which has been extensively studied for decades. Most of the\nexisting MOT algorithms are based on the Tracking-by-Detection (TBD) paradigm\ncombined with popular machine learning approaches which largely reduce the\nhuman effort to tune algorithm parameters. However, the commonly used\nsupervised learning approaches require the labeled data (e.g., bounding boxes),\nwhich is expensive for videos. Also, the TBD framework is usually suboptimal\nsince it is not end-to-end, i.e., it considers the task as detection and\ntracking, but not jointly. To achieve both label-free and end-to-end learning\nof MOT, we propose a Tracking-by-Animation framework, where a differentiable\nneural model first tracks objects from input frames and then animates these\nobjects into reconstructed frames. Learning is then driven by the\nreconstruction error through backpropagation. We further propose a\nReprioritized Attentive Tracking to improve the robustness of data association.\nExperiments conducted on both synthetic and real video datasets show the\npotential of the proposed model. Our project page is publicly available at:\nhttps://github.com/zhen-he/tracking-by-animation \n\n"}
{"id": "1809.03415", "contents": "Title: Monocular Object and Plane SLAM in Structured Environments Abstract: In this paper, we present a monocular Simultaneous Localization and Mapping\n(SLAM) algorithm using high-level object and plane landmarks. The built map is\ndenser, more compact and semantic meaningful compared to feature point based\nSLAM. We first propose a high order graphical model to jointly infer the 3D\nobject and layout planes from single images considering occlusions and semantic\nconstraints. The extracted objects and planes are further optimized with camera\nposes in a unified SLAM framework. Objects and planes can provide more semantic\nconstraints such as Manhattan plane and object supporting relationships\ncompared to points. Experiments on various public and collected datasets\nincluding ICL NUIM and TUM Mono show that our algorithm can improve camera\nlocalization accuracy compared to state-of-the-art SLAM especially when there\nis no loop closure, and also generate dense maps robustly in many structured\nenvironments. \n\n"}
{"id": "1809.03668", "contents": "Title: Comparing Computing Platforms for Deep Learning on a Humanoid Robot Abstract: The goal of this study is to test two different computing platforms with\nrespect to their suitability for running deep networks as part of a humanoid\nrobot software system. One of the platforms is the CPU-centered Intel NUC7i7BNH\nand the other is a NVIDIA Jetson TX2 system that puts more emphasis on GPU\nprocessing. The experiments addressed a number of benchmarking tasks including\npedestrian detection using deep neural networks. Some of the results were\nunexpected but demonstrate that platforms exhibit both advantages and\ndisadvantages when taking computational performance and electrical power\nrequirements of such a system into account. \n\n"}
{"id": "1809.03851", "contents": "Title: Visualizing Convolutional Neural Networks to Improve Decision Support\n  for Skin Lesion Classification Abstract: Because of their state-of-the-art performance in computer vision, CNNs are\nbecoming increasingly popular in a variety of fields, including medicine.\nHowever, as neural networks are black box function approximators, it is\ndifficult, if not impossible, for a medical expert to reason about their\noutput. This could potentially result in the expert distrusting the network\nwhen he or she does not agree with its output. In such a case, explaining why\nthe CNN makes a certain decision becomes valuable information. In this paper,\nwe try to open the black box of the CNN by inspecting and visualizing the\nlearned feature maps, in the field of dermatology. We show that, to some\nextent, CNNs focus on features similar to those used by dermatologists to make\na diagnosis. However, more research is required for fully explaining their\noutput. \n\n"}
{"id": "1809.04184", "contents": "Title: Searching for Efficient Multi-Scale Architectures for Dense Image\n  Prediction Abstract: The design of neural network architectures is an important component for\nachieving state-of-the-art performance with machine learning systems across a\nbroad array of tasks. Much work has endeavored to design and build\narchitectures automatically through clever construction of a search space\npaired with simple learning algorithms. Recent progress has demonstrated that\nsuch meta-learning methods may exceed scalable human-invented architectures on\nimage classification tasks. An open question is the degree to which such\nmethods may generalize to new domains. In this work we explore the construction\nof meta-learning techniques for dense image prediction focused on the tasks of\nscene parsing, person-part segmentation, and semantic image segmentation.\nConstructing viable search spaces in this domain is challenging because of the\nmulti-scale representation of visual information and the necessity to operate\non high resolution imagery. Based on a survey of techniques in dense image\nprediction, we construct a recursive search space and demonstrate that even\nwith efficient random search, we can identify architectures that outperform\nhuman-invented architectures and achieve state-of-the-art performance on three\ndense prediction tasks including 82.7\\% on Cityscapes (street scene parsing),\n71.3\\% on PASCAL-Person-Part (person-part segmentation), and 87.9\\% on PASCAL\nVOC 2012 (semantic image segmentation). Additionally, the resulting\narchitecture is more computationally efficient, requiring half the parameters\nand half the computational cost as previous state of the art systems. \n\n"}
{"id": "1809.04430", "contents": "Title: Deep learning to achieve clinically applicable segmentation of head and\n  neck anatomy for radiotherapy Abstract: Over half a million individuals are diagnosed with head and neck cancer each\nyear worldwide. Radiotherapy is an important curative treatment for this\ndisease, but it requires manual time consuming delineation of radio-sensitive\norgans at risk (OARs). This planning process can delay treatment, while also\nintroducing inter-operator variability with resulting downstream radiation dose\ndifferences. While auto-segmentation algorithms offer a potentially time-saving\nsolution, the challenges in defining, quantifying and achieving expert\nperformance remain. Adopting a deep learning approach, we demonstrate a 3D\nU-Net architecture that achieves expert-level performance in delineating 21\ndistinct head and neck OARs commonly segmented in clinical practice. The model\nwas trained on a dataset of 663 deidentified computed tomography (CT) scans\nacquired in routine clinical practice and with both segmentations taken from\nclinical practice and segmentations created by experienced radiographers as\npart of this research, all in accordance with consensus OAR definitions. We\ndemonstrate the model's clinical applicability by assessing its performance on\na test set of 21 CT scans from clinical practice, each with the 21 OARs\nsegmented by two independent experts. We also introduce surface Dice similarity\ncoefficient (surface DSC), a new metric for the comparison of organ\ndelineation, to quantify deviation between OAR surface contours rather than\nvolumes, better reflecting the clinical task of correcting errors in the\nautomated organ segmentations. The model's generalisability is then\ndemonstrated on two distinct open source datasets, reflecting different centres\nand countries to model training. With appropriate validation studies and\nregulatory approvals, this system could improve the efficiency, consistency,\nand safety of radiotherapy pathways. \n\n"}
{"id": "1809.04987", "contents": "Title: Synthetic Occlusion Augmentation with Volumetric Heatmaps for the 2018\n  ECCV PoseTrack Challenge on 3D Human Pose Estimation Abstract: In this paper we present our winning entry at the 2018 ECCV PoseTrack\nChallenge on 3D human pose estimation. Using a fully-convolutional backbone\narchitecture, we obtain volumetric heatmaps per body joint, which we convert to\ncoordinates using soft-argmax. Absolute person center depth is estimated by a\n1D heatmap prediction head. The coordinates are back-projected to 3D camera\nspace, where we minimize the L1 loss. Key to our good results is the training\ndata augmentation with randomly placed occluders from the Pascal VOC dataset.\nIn addition to reaching first place in the Challenge, our method also surpasses\nthe state-of-the-art on the full Human3.6M benchmark among methods that use no\nadditional pose datasets in training. Code for applying synthetic occlusions is\navailabe at https://github.com/isarandi/synthetic-occlusion. \n\n"}
{"id": "1809.05165", "contents": "Title: Defensive Dropout for Hardening Deep Neural Networks under Adversarial\n  Attacks Abstract: Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That\nis, adversarial examples, obtained by adding delicately crafted distortions\nonto original legal inputs, can mislead a DNN to classify them as any target\nlabels. This work provides a solution to hardening DNNs under adversarial\nattacks through defensive dropout. Besides using dropout during training for\nthe best test accuracy, we propose to use dropout also at test time to achieve\nstrong defense effects. We consider the problem of building robust DNNs as an\nattacker-defender two-player game, where the attacker and the defender know\neach others' strategies and try to optimize their own strategies towards an\nequilibrium. Based on the observations of the effect of test dropout rate on\ntest accuracy and attack success rate, we propose a defensive dropout algorithm\nto determine an optimal test dropout rate given the neural network model and\nthe attacker's strategy for generating adversarial examples.We also investigate\nthe mechanism behind the outstanding defense effects achieved by the proposed\ndefensive dropout. Comparing with stochastic activation pruning (SAP), another\ndefense method through introducing randomness into the DNN model, we find that\nour defensive dropout achieves much larger variances of the gradients, which is\nthe key for the improved defense effects (much lower attack success rate). For\nexample, our defensive dropout can reduce the attack success rate from 100% to\n13.89% under the currently strongest attack i.e., C&W attack on MNIST dataset. \n\n"}
{"id": "1809.05267", "contents": "Title: Detection-by-Localization: Maintenance-Free Change Object Detector Abstract: Recent researches demonstrate that self-localization performance is a very\nuseful measure of likelihood-of-change (LoC) for change detection. In this\npaper, this \"detection-by-localization\" scheme is studied in a novel\ngeneralized task of object-level change detection. In our framework, a given\nquery image is segmented into object-level subimages (termed \"scene parts\"),\nwhich are then converted to subimage-level pixel-wise LoC maps via the\ndetection-by-localization scheme. Our approach models a self-localization\nsystem as a ranking function, outputting a ranked list of reference images,\nwithout requiring relevance score. Thanks to this new setting, we can\ngeneralize our approach to a broad class of self-localization systems. Our\nranking based self-localization model allows to fuse self-localization results\nfrom different modalities via an unsupervised rank fusion derived from a field\nof multi-modal information retrieval (MMR). \n\n"}
{"id": "1809.05680", "contents": "Title: A New Multi-vehicle Trajectory Generator to Simulate Vehicle-to-Vehicle\n  Encounters Abstract: Generating multi-vehicle trajectories from existing limited data can provide\nrich resources for autonomous vehicle development and testing. This paper\nintroduces a multi-vehicle trajectory generator (MTG) that can encode\nmulti-vehicle interaction scenarios (called driving encounters) into an\ninterpretable representation from which new driving encounter scenarios are\ngenerated by sampling. The MTG consists of a bi-directional encoder and a\nmulti-branch decoder. A new disentanglement metric is then developed for model\nanalyses and comparisons in terms of model robustness and the independence of\nthe latent codes. Comparison of our proposed MTG with $\\beta$-VAE and InfoGAN\ndemonstrates that the MTG has stronger capability to purposely generate\nrational vehicle-to-vehicle encounters through operating the disentangled\nlatent codes. Thus the MTG could provide more data for engineers and\nresearchers to develop testing and evaluation scenarios for autonomous\nvehicles. \n\n"}
{"id": "1809.05864", "contents": "Title: In Defense of the Classification Loss for Person Re-Identification Abstract: The recent research for person re-identification has been focused on two\ntrends. One is learning the part-based local features to form more informative\nfeature descriptors. The other is designing effective metric learning loss\nfunctions such as the triplet loss family. We argue that learning global\nfeatures with classification loss could achieve the same goal, even with some\nsimple and cost-effective architecture design. In this paper, we first explain\nwhy the person re-id framework with standard classification loss usually has\ninferior performance compared to metric learning. Based on that, we further\npropose a person re-id framework featured by channel grouping and multi-branch\nstrategy, which divides global features into multiple channel groups and learns\nthe discriminative channel group features by multi-branch classification\nlayers. The extensive experiments show that our framework outperforms prior\nstate-of-the-arts in terms of both accuracy and inference speed. \n\n"}
{"id": "1809.06045", "contents": "Title: Building Prior Knowledge: A Markov Based Pedestrian Prediction Model\n  Using Urban Environmental Data Abstract: Autonomous Vehicles navigating in urban areas have a need to understand and\npredict future pedestrian behavior for safer navigation. This high level of\nsituational awareness requires observing pedestrian behavior and extrapolating\ntheir positions to know future positions. While some work has been done in this\nfield using Hidden Markov Models (HMMs), one of the few observed drawbacks of\nthe method is the need for informed priors for learning behavior. In this work,\nan extension to the Growing Hidden Markov Model (GHMM) method is proposed to\nsolve some of these drawbacks. This is achieved by building on existing work\nusing potential cost maps and the principle of Natural Vision. As a\nconsequence, the proposed model is able to predict pedestrian positions more\nprecisely over a longer horizon compared to the state of the art. The method is\ntested over \"legal\" and \"illegal\" behavior of pedestrians, having trained the\nmodel with sparse observations and partial trajectories. The method, with no\ntraining data, is compared against a trained state of the art model. It is\nobserved that the proposed method is robust even in new, previously unseen\nareas. \n\n"}
{"id": "1809.06213", "contents": "Title: Context-Dependent Diffusion Network for Visual Relationship Detection Abstract: Visual relationship detection can bridge the gap between computer vision and\nnatural language for scene understanding of images. Different from pure object\nrecognition tasks, the relation triplets of subject-predicate-object lie on an\nextreme diversity space, such as \\textit{person-behind-person} and\n\\textit{car-behind-building}, while suffering from the problem of combinatorial\nexplosion. In this paper, we propose a context-dependent diffusion network\n(CDDN) framework to deal with visual relationship detection. To capture the\ninteractions of different object instances, two types of graphs, word semantic\ngraph and visual scene graph, are constructed to encode global context\ninterdependency. The semantic graph is built through language priors to model\nsemantic correlations across objects, whilst the visual scene graph defines the\nconnections of scene objects so as to utilize the surrounding scene\ninformation. For the graph-structured data, we design a diffusion network to\nadaptively aggregate information from contexts, which can effectively learn\nlatent representations of visual relationships and well cater to visual\nrelationship detection in view of its isomorphic invariance to graphs.\nExperiments on two widely-used datasets demonstrate that our proposed method is\nmore effective and achieves the state-of-the-art performance. \n\n"}
{"id": "1809.06783", "contents": "Title: Generalized Content-Preserving Warps for Image Stitching Abstract: Local misalignment caused by global homography is a common issue in image\nstitching task. Content-Preserving Warping (CPW) is a typical method to deal\nwith this issue, in which geometric and photometric constraints are imposed to\nguide the warping process. One of its essential condition however, is colour\nconsistency, and an elusive goal in real world applications. In this paper, we\npropose a Generalized Content-Preserving Warping (GCPW) method to alleviate\nthis problem. GCPW extends the original CPW by applying a colour model that\nexpresses the colour transformation between images locally, thus meeting the\nphotometric constraint requirements for effective image stitching. We combine\nthe photometric and geometric constraints and jointly estimate the colour\ntransformation and the warped mesh vertexes, simultaneously. We align images\nlocally with an optimal grid mesh generated by our GCPW method. Experiments on\nboth synthetic and real images demonstrate that our new method is robust to\ncolour variations, outperforming other state-of-the-art CPW-based image\nstitching methods. \n\n"}
{"id": "1809.07016", "contents": "Title: Generating 3D Adversarial Point Clouds Abstract: Deep neural networks are known to be vulnerable to adversarial examples which\nare carefully crafted instances to cause the models to make wrong predictions.\nWhile adversarial examples for 2D images and CNNs have been extensively\nstudied, less attention has been paid to 3D data such as point clouds. Given\nmany safety-critical 3D applications such as autonomous driving, it is\nimportant to study how adversarial point clouds could affect current deep 3D\nmodels. In this work, we propose several novel algorithms to craft adversarial\npoint clouds against PointNet, a widely used deep neural network for point\ncloud processing. Our algorithms work in two ways: adversarial point\nperturbation and adversarial point generation. For point perturbation, we shift\nexisting points negligibly. For point generation, we generate either a set of\nindependent and scattered points or a small number (1-3) of point clusters with\nmeaningful shapes such as balls and airplanes which could be hidden in the\nhuman psyche. In addition, we formulate six perturbation measurement metrics\ntailored to the attacks in point clouds and conduct extensive experiments to\nevaluate the proposed algorithms on the ModelNet40 3D shape classification\ndataset. Overall, our attack algorithms achieve a success rate higher than 99%\nfor all targeted attacks \n\n"}
{"id": "1809.07196", "contents": "Title: Characterising Across-Stack Optimisations for Deep Convolutional Neural\n  Networks Abstract: Convolutional Neural Networks (CNNs) are extremely computationally demanding,\npresenting a large barrier to their deployment on resource-constrained devices.\nSince such systems are where some of their most useful applications lie (e.g.\nobstacle detection for mobile robots, vision-based medical assistive\ntechnology), significant bodies of work from both machine learning and systems\ncommunities have attempted to provide optimisations that will make CNNs\navailable to edge devices. In this paper we unify the two viewpoints in a Deep\nLearning Inference Stack and take an across-stack approach by implementing and\nevaluating the most common neural network compression techniques (weight\npruning, channel pruning, and quantisation) and optimising their parallel\nexecution with a range of programming approaches (OpenMP, OpenCL) and hardware\narchitectures (CPU, GPU). We provide comprehensive Pareto curves to instruct\ntrade-offs under constraints of accuracy, execution time, and memory space. \n\n"}
{"id": "1809.08377", "contents": "Title: Galaxy morphology prediction using capsule networks Abstract: Understanding morphological types of galaxies is a key parameter for studying\ntheir formation and evolution. Neural networks that have been used previously\nfor galaxy morphology classification have some disadvantages, such as not being\ninvariant under rotation. In this work, we studied the performance of Capsule\nNetwork, a recently introduced neural network architecture that is rotationally\ninvariant and spatially aware, on the task of galaxy morphology classification.\nWe designed two evaluation scenarios based on the answers from the question\ntree in the Galaxy Zoo project. In the first scenario, we used Capsule Network\nfor regression and predicted probabilities for all of the questions. In the\nsecond scenario, we chose the answer to the first morphology question that had\nthe highest user agreement as the class of the object and trained a Capsule\nNetwork classifier, where we also reconstructed galaxy images. We achieved\npromising results in both of these scenarios. Automated approaches such as the\none introduced here will greatly decrease the workload of astronomers and will\nplay a critical role in the upcoming large sky surveys. \n\n"}
{"id": "1809.08493", "contents": "Title: SelfKin: Self Adjusted Deep Model For Kinship Verification Abstract: One of the unsolved challenges in the field of biometrics and face\nrecognition is Kinship Verification. This problem aims to understand if two\npeople are family-related and how (sisters, brothers, etc.) Solving this\nproblem can give rise to varied tasks and applications. In the area of homeland\nsecurity (HLS) it is crucial to auto-detect if the person questioned is related\nto a wanted suspect, In the field of biometrics, kinship-verification can help\nto discriminate between families by photos and in the field of predicting or\nfashion it can help to predict an older or younger model of people faces.\nLately, and with the advanced deep learning technology, this problem has gained\nfocus from the research community in matters of data and research. In this\narticle, we propose using a Deep Learning approach for solving the\nKinship-Verification problem. Further, we offer a novel self-learning deep\nmodel, which learns the essential features from different faces. We show that\nour model wins the Recognize Families In the Wild(RFIW2018,FG2018) challenge\nand obtains state-of-the-art results. Moreover, we show that our proposed model\ncan reduce the size of the network by half without loss in performance. \n\n"}
{"id": "1809.08799", "contents": "Title: Chargrid: Towards Understanding 2D Documents Abstract: We introduce a novel type of text representation that preserves the 2D layout\nof a document. This is achieved by encoding each document page as a\ntwo-dimensional grid of characters. Based on this representation, we present a\ngeneric document understanding pipeline for structured documents. This pipeline\nmakes use of a fully convolutional encoder-decoder network that predicts a\nsegmentation mask and bounding boxes. We demonstrate its capabilities on an\ninformation extraction task from invoices and show that it significantly\noutperforms approaches based on sequential text or document images. \n\n"}
{"id": "1809.08854", "contents": "Title: A Framework towards Domain Specific Video Summarization Abstract: In the light of exponentially increasing video content, video summarization\nhas attracted a lot of attention recently due to its ability to optimize time\nand storage. Characteristics of a good summary of a video depend on the\nparticular domain under question. We propose a novel framework for domain\nspecific video summarization. Given a video of a particular domain, our system\ncan produce a summary based on what is important for that domain in addition to\npossessing other desired characteristics like representativeness, coverage,\ndiversity etc. as suitable to that domain. Past related work has focused either\non using supervised approaches for ranking the snippets to produce summary or\non using unsupervised approaches of generating the summary as a subset of\nsnippets with the above characteristics. We look at the joint problem of\nlearning domain specific importance of segments as well as the desired summary\ncharacteristic for that domain. Our studies show that the more efficient way of\nincorporating domain specific relevances into a summary is by obtaining ratings\nof shots as opposed to binary inclusion/exclusion information. We also argue\nthat ratings can be seen as unified representation of all possible ground truth\nsummaries of a video, taking us one step closer in dealing with challenges\nassociated with multiple ground truth summaries of a video. We also propose a\nnovel evaluation measure which is more naturally suited in assessing the\nquality of video summary for the task at hand than F1 like measures. It\nleverages the ratings information and is richer in appropriately modeling\ndesirable and undesirable characteristics of a summary. Lastly, we release a\ngold standard dataset for furthering research in domain specific video\nsummarization, which to our knowledge is the first dataset with long videos\nacross several domains with rating annotations. \n\n"}
{"id": "1809.09318", "contents": "Title: Floyd-Warshall Reinforcement Learning: Learning from Past Experiences to\n  Reach New Goals Abstract: Consider mutli-goal tasks that involve static environments and dynamic goals.\nExamples of such tasks, such as goal-directed navigation and pick-and-place in\nrobotics, abound. Two types of Reinforcement Learning (RL) algorithms are used\nfor such tasks: model-free or model-based. Each of these approaches has\nlimitations. Model-free RL struggles to transfer learned information when the\ngoal location changes, but achieves high asymptotic accuracy in single goal\ntasks. Model-based RL can transfer learned information to new goal locations by\nretaining the explicitly learned state-dynamics, but is limited by the fact\nthat small errors in modelling these dynamics accumulate over long-term\nplanning. In this work, we improve upon the limitations of model-free RL in\nmulti-goal domains. We do this by adapting the Floyd-Warshall algorithm for RL\nand call the adaptation Floyd-Warshall RL (FWRL). The proposed algorithm learns\na goal-conditioned action-value function by constraining the value of the\noptimal path between any two states to be greater than or equal to the value of\npaths via intermediary states. Experimentally, we show that FWRL is more\nsample-efficient and learns higher reward strategies in multi-goal tasks as\ncompared to Q-learning, model-based RL and other relevant baselines in a\ntabular domain. \n\n"}
{"id": "1809.10241", "contents": "Title: Classifying Mammographic Breast Density by Residual Learning Abstract: Mammographic breast density, a parameter used to describe the proportion of\nbreast tissue fibrosis, is widely adopted as an evaluation characteristic of\nthe likelihood of breast cancer incidence. In this study, we present a\nradiomics approach based on residual learning for the classification of\nmammographic breast densities. Our method possesses several encouraging\nproperties such as being almost fully automatic, possessing big model capacity\nand flexibility. It can obtain outstanding classification results without the\nnecessity of result compensation using mammographs taken from different views.\nThe proposed method was instantiated with the INbreast dataset and\nclassification accuracies of 92.6% and 96.8% were obtained for the four BI-RADS\n(Breast Imaging and Reporting Data System) category task and the two BI-RADS\ncategory task,respectively. The superior performances achieved compared to the\nexisting state-of-the-art methods along with its encouraging properties\nindicate that our method has a great potential to be applied as a\ncomputer-aided diagnosis tool. \n\n"}
{"id": "1809.10243", "contents": "Title: Segmentation of Skin Lesions and their Attributes Using Multi-Scale\n  Convolutional Neural Networks and Domain Specific Augmentations Abstract: Computer-aided diagnosis systems for classification of different type of skin\nlesions have been an active field of research in recent decades. It has been\nshown that introducing lesions and their attributes masks into lesion\nclassification pipeline can greatly improve the performance. In this paper, we\npropose a framework by incorporating transfer learning for segmenting lesions\nand their attributes based on the convolutional neural networks. The proposed\nframework is based on the encoder-decoder architecture which utilizes a variety\nof pre-trained networks in the encoding path and generates the prediction map\nby combining multi-scale information in decoding path using a pyramid pooling\nmanner. To address the lack of training data and increase the proposed model\ngeneralization, an extensive set of novel domain-specific augmentation routines\nhave been applied to simulate the real variations in dermoscopy images.\nFinally, by performing broad experiments on three different data sets obtained\nfrom International Skin Imaging Collaboration archive (ISIC2016, ISIC2017, and\nISIC2018 challenges data sets), we show that the proposed method outperforms\nother state-of-the-art approaches for ISIC2016 and ISIC2017 segmentation task\nand achieved the first rank on the leader-board of ISIC2018 attribute detection\ntask. \n\n"}
{"id": "1809.10333", "contents": "Title: Using Autoencoders To Learn Interesting Features For Detecting\n  Surveillance Aircraft Abstract: This paper explores using a Long short-term memory (LSTM) based sequence\nautoencoder to learn interesting features for detecting surveillance aircraft\nusing ADS-B flight data. An aircraft periodically broadcasts ADS-B (Automatic\nDependent Surveillance - Broadcast) data to ground receivers. The ability of\nLSTM networks to model varying length time series data and remember\ndependencies that span across events makes it an ideal candidate for\nimplementing a sequence autoencoder for ADS-B data because of its possible\nvariable length time series, irregular sampling and dependencies that span\nacross events. \n\n"}
{"id": "1809.10572", "contents": "Title: Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural\n  Networks Abstract: Quantization of weights and activations in Deep Neural Networks (DNNs) is a\npowerful technique for network compression, and has enjoyed significant\nattention and success. However, much of the inference-time benefit of\nquantization is accessible only through the use of customized hardware\naccelerators or by providing an FPGA implementation of quantized arithmetic.\n  Building on prior work, we show how to construct arbitrary bit-precise signed\nand unsigned integer operations using a software technique which logically\n\\emph{embeds} a vector architecture with custom bit-width lanes in universally\navailable fixed-width scalar arithmetic.\n  We evaluate our approach on a high-end Intel Haswell processor, and an\nembedded ARM processor. Our approach yields very fast implementations of\nbit-precise custom DNN operations, which often match or exceed the performance\nof operations quantized to the sizes supported in native arithmetic. At the\nstrongest level of quantization, our approach yields a maximum speedup of\n$\\thicksim6\\times$ on the Intel platform, and $\\thicksim10\\times$ on the ARM\nplatform versus quantization to native 8-bit integers. \n\n"}
{"id": "1809.10877", "contents": "Title: Learning for Single-Shot Confidence Calibration in Deep Neural Networks\n  through Stochastic Inferences Abstract: We propose a generic framework to calibrate accuracy and confidence of a\nprediction in deep neural networks through stochastic inferences. We interpret\nstochastic regularization using a Bayesian model, and analyze the relation\nbetween predictive uncertainty of networks and variance of the prediction\nscores obtained by stochastic inferences for a single example. Our empirical\nstudy shows that the accuracy and the score of a prediction are highly\ncorrelated with the variance of multiple stochastic inferences given by\nstochastic depth or dropout. Motivated by this observation, we design a novel\nvariance-weighted confidence-integrated loss function that is composed of two\ncross-entropy loss terms with respect to ground-truth and uniform distribution,\nwhich are balanced by variance of stochastic prediction scores. The proposed\nloss function enables us to learn deep neural networks that predict confidence\ncalibrated scores using a single inference. Our algorithm presents outstanding\nconfidence calibration performance and improves classification accuracy when\ncombined with two popular stochastic regularization techniques---stochastic\ndepth and dropout---in multiple models and datasets; it alleviates\noverconfidence issue in deep neural networks significantly by training networks\nto achieve prediction accuracy proportional to confidence of prediction. \n\n"}
{"id": "1809.11096", "contents": "Title: Large Scale GAN Training for High Fidelity Natural Image Synthesis Abstract: Despite recent progress in generative image modeling, successfully generating\nhigh-resolution, diverse samples from complex datasets such as ImageNet remains\nan elusive goal. To this end, we train Generative Adversarial Networks at the\nlargest scale yet attempted, and study the instabilities specific to such\nscale. We find that applying orthogonal regularization to the generator renders\nit amenable to a simple \"truncation trick,\" allowing fine control over the\ntrade-off between sample fidelity and variety by reducing the variance of the\nGenerator's input. Our modifications lead to models which set the new state of\nthe art in class-conditional image synthesis. When trained on ImageNet at\n128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of\n166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous\nbest IS of 52.52 and FID of 18.6. \n\n"}
{"id": "1810.00240", "contents": "Title: Reinforcement Learning in R Abstract: Reinforcement learning refers to a group of methods from artificial\nintelligence where an agent performs learning through trial and error. It\ndiffers from supervised learning, since reinforcement learning requires no\nexplicit labels; instead, the agent interacts continuously with its\nenvironment. That is, the agent starts in a specific state and then performs an\naction, based on which it transitions to a new state and, depending on the\noutcome, receives a reward. Different strategies (e.g. Q-learning) have been\nproposed to maximize the overall reward, resulting in a so-called policy, which\ndefines the best possible action in each state. Mathematically, this process\ncan be formalized by a Markov decision process and it has been implemented by\npackages in R; however, there is currently no package available for\nreinforcement learning. As a remedy, this paper demonstrates how to perform\nreinforcement learning in R and, for this purpose, introduces the\nReinforcementLearning package. The package provides a remarkably flexible\nframework and is easily applied to a wide range of different problems. We\ndemonstrate its use by drawing upon common examples from the literature (e.g.\nfinding optimal game strategies). \n\n"}
{"id": "1810.00378", "contents": "Title: Pseudo-Random Number Generation using Generative Adversarial Networks Abstract: Pseudo-random number generators (PRNG) are a fundamental element of many\nsecurity algorithms. We introduce a novel approach to their implementation, by\nproposing the use of generative adversarial networks (GAN) to train a neural\nnetwork to behave as a PRNG. Furthermore, we showcase a number of interesting\nmodifications to the standard GAN architecture. The most significant is\npartially concealing the output of the GAN's generator, and training the\nadversary to discover a mapping from the overt part to the concealed part. The\ngenerator therefore learns to produce values the adversary cannot predict,\nrather than to approximate an explicit reference distribution. We demonstrate\nthat a GAN can effectively train even a small feed-forward fully connected\nneural network to produce pseudo-random number sequences with good statistical\nproperties. At best, subjected to the NIST test suite, the trained generator\npassed around 99% of test instances and 98% of overall tests, outperforming a\nnumber of standard non-cryptographic PRNGs. \n\n"}
{"id": "1810.00490", "contents": "Title: Learning Deep Representations from Clinical Data for Chronic Kidney\n  Disease Abstract: We study the behavior of a Time-Aware Long Short-Term Memory Autoencoder, a\nstate-of-the-art method, in the context of learning latent representations from\nirregularly sampled patient data. We identify a key issue in the way such\nrecurrent neural network models are being currently used and show that the\nsolution of the issue leads to significant improvements in the learnt\nrepresentations on both synthetic and real datasets. A detailed analysis of the\nimproved methodology for representing patients suffering from Chronic Kidney\nDisease (CKD) using clinical data is provided. Experimental results show that\nthe proposed T-LSTM model is able to capture the long-term trends in the data,\nwhile effectively handling the noise in the signal. Finally, we show that by\nusing the latent representations of the CKD patients obtained from the T-LSTM\nautoencoder, one can identify unusual patient profiles from the target\npopulation. \n\n"}
{"id": "1810.00609", "contents": "Title: One-Click Annotation with Guided Hierarchical Object Detection Abstract: The increase in data collection has made data annotation an interesting and\nvaluable task in the contemporary world. This paper presents a new methodology\nfor quickly annotating data using click-supervision and hierarchical object\ndetection. The proposed work is semi-automatic in nature where the task of\nannotations is split between the human and a neural network. We show that our\nimproved method of annotation reduces the time, cost and mental stress on a\nhuman annotator. The research also highlights how our method performs better\nthan the current approach in different circumstances such as variation in\nnumber of objects, object size and different datasets. Our approach also\nproposes a new method of using object detectors making it suitable for data\nannotation task. The experiment conducted on PASCAL VOC dataset revealed that\nannotation created from our approach achieves a mAP of 0.995 and a recall of\n0.903. The Our Approach has shown an overall improvement by 8.5%, 18.6% in mean\naverage precision and recall score for KITTI and 69.6%, 36% for CITYSCAPES\ndataset. The proposed framework is 3-4 times faster as compared to the standard\nannotation method. \n\n"}
{"id": "1810.00760", "contents": "Title: Riemannian Adaptive Optimization Methods Abstract: Several first order stochastic optimization methods commonly used in the\nEuclidean domain such as stochastic gradient descent (SGD), accelerated\ngradient descent or variance reduced methods have already been adapted to\ncertain Riemannian settings. However, some of the most popular of these\noptimization tools - namely Adam , Adagrad and the more recent Amsgrad - remain\nto be generalized to Riemannian manifolds. We discuss the difficulty of\ngeneralizing such adaptive schemes to the most agnostic Riemannian setting, and\nthen provide algorithms and convergence proofs for geodesically convex\nobjectives in the particular case of a product of Riemannian manifolds, in\nwhich adaptivity is implemented across manifolds in the cartesian product. Our\ngeneralization is tight in the sense that choosing the Euclidean space as\nRiemannian manifold yields the same algorithms and regret bounds as those that\nwere already known for the standard algorithms. Experimentally, we show faster\nconvergence and to a lower train loss value for Riemannian adaptive methods\nover their corresponding baselines on the realistic task of embedding the\nWordNet taxonomy in the Poincare ball. \n\n"}
{"id": "1810.00953", "contents": "Title: Improved robustness to adversarial examples using Lipschitz\n  regularization of the loss Abstract: We augment adversarial training (AT) with worst case adversarial training\n(WCAT) which improves adversarial robustness by 11% over the current\nstate-of-the-art result in the $\\ell_2$ norm on CIFAR-10. We obtain verifiable\naverage case and worst case robustness guarantees, based on the expected and\nmaximum values of the norm of the gradient of the loss. We interpret\nadversarial training as Total Variation Regularization, which is a fundamental\ntool in mathematical image processing, and WCAT as Lipschitz regularization. \n\n"}
{"id": "1810.01876", "contents": "Title: Spurious samples in deep generative models: bug or feature? Abstract: Traditional wisdom in generative modeling literature is that spurious samples\nthat a model can generate are errors and they should be avoided. Recent\nresearch, however, has shown interest in studying or even exploiting such\nsamples instead of eliminating them. In this paper, we ask the question whether\nsuch samples can be eliminated all together without sacrificing coverage of the\ngenerating distribution. For the class of models we consider, we experimentally\ndemonstrate that this is not possible without losing the ability to model some\nof the test samples. While our results need to be confirmed on a broader set of\nmodel families, these initial findings provide partial evidence that spurious\nsamples share structural properties with the learned dataset, which, in turn,\nsuggests they are not simply errors but a feature of deep generative nets. \n\n"}
{"id": "1810.01898", "contents": "Title: A Multi-Face Challenging Dataset for Robust Face Recognition Abstract: Face recognition in images is an active area of interest among the computer\nvision researchers. However, recognizing human face in an unconstrained\nenvironment, is a relatively less-explored area of research. Multiple face\nrecognition in unconstrained environment is a challenging task, due to the\nvariation of view-point, scale, pose, illumination and expression of the face\nimages. Partial occlusion of faces makes the recognition task even more\nchallenging. The contribution of this paper is two-folds: introducing a\nchallenging multiface dataset (i.e., IIITS MFace Dataset) for face recognition\nin unconstrained environment and evaluating the performance of state-of-the-art\nhand-designed and deep learning based face descriptors on the dataset. The\nproposed IIITS MFace dataset contains faces with challenges like pose\nvariation, occlusion, mask, spectacle, expressions, change of illumination,\netc. We experiment with several state-of-the-art face descriptors, including\nrecent deep learning based face descriptors like VGGFace, and compare with the\nexisting benchmark face datasets. Results of the experiments clearly show that\nthe difficulty level of the proposed dataset is much higher compared to the\nbenchmark datasets. \n\n"}
{"id": "1810.02020", "contents": "Title: Transfer Incremental Learning using Data Augmentation Abstract: Deep learning-based methods have reached state of the art performances,\nrelying on large quantity of available data and computational power. Such\nmethods still remain highly inappropriate when facing a major open machine\nlearning problem, which consists of learning incrementally new classes and\nexamples over time. Combining the outstanding performances of Deep Neural\nNetworks (DNNs) with the flexibility of incremental learning techniques is a\npromising venue of research. In this contribution, we introduce Transfer\nIncremental Learning using Data Augmentation (TILDA). TILDA is based on\npre-trained DNNs as feature extractor, robust selection of feature vectors in\nsubspaces using a nearest-class-mean based technique, majority votes and data\naugmentation at both the training and the prediction stages. Experiments on\nchallenging vision datasets demonstrate the ability of the proposed method for\nlow complexity incremental learning, while achieving significantly better\naccuracy than existing incremental counterparts. \n\n"}
{"id": "1810.02845", "contents": "Title: Deep Generative Video Compression Abstract: The usage of deep generative models for image compression has led to\nimpressive performance gains over classical codecs while neural video\ncompression is still in its infancy. Here, we propose an end-to-end, deep\ngenerative modeling approach to compress temporal sequences with a focus on\nvideo. Our approach builds upon variational autoencoder (VAE) models for\nsequential data and combines them with recent work on neural image compression.\nThe approach jointly learns to transform the original sequence into a\nlower-dimensional representation as well as to discretize and entropy code this\nrepresentation according to predictions of the sequential VAE. Rate-distortion\nevaluations on small videos from public data sets with varying complexity and\ndiversity show that our model yields competitive results when trained on\ngeneric video content. Extreme compression performance is achieved when\ntraining the model on specialized content. \n\n"}
{"id": "1810.03068", "contents": "Title: Geometric Scattering for Graph Data Analysis Abstract: We explore the generalization of scattering transforms from traditional\n(e.g., image or audio) signals to graph data, analogous to the generalization\nof ConvNets in geometric deep learning, and the utility of extracted graph\nfeatures in graph data analysis. In particular, we focus on the capacity of\nthese features to retain informative variability and relations in the data\n(e.g., between individual graphs, or in aggregate), while relating our\nconstruction to previous theoretical results that establish the stability of\nsimilar transforms to families of graph deformations. We demonstrate the\napplication the our geometric scattering features in graph classification of\nsocial network data, and in data exploration of biochemistry data. \n\n"}
{"id": "1810.03292", "contents": "Title: Sanity Checks for Saliency Maps Abstract: Saliency methods have emerged as a popular tool to highlight features in an\ninput deemed relevant for the prediction of a learned model. Several saliency\nmethods have been proposed, often guided by visual appeal on image data. In\nthis work, we propose an actionable methodology to evaluate what kinds of\nexplanations a given method can and cannot provide. We find that reliance,\nsolely, on visual assessment can be misleading. Through extensive experiments\nwe show that some existing saliency methods are independent both of the model\nand of the data generating process. Consequently, methods that fail the\nproposed tests are inadequate for tasks that are sensitive to either data or\nmodel, such as, finding outliers in the data, explaining the relationship\nbetween inputs and outputs that the model learned, and debugging the model. We\ninterpret our findings through an analogy with edge detection in images, a\ntechnique that requires neither training data nor model. Theory in the case of\na linear model and a single-layer convolutional neural network supports our\nexperimental findings. \n\n"}
{"id": "1810.03649", "contents": "Title: Overcoming Language Priors in Visual Question Answering with Adversarial\n  Regularization Abstract: Modern Visual Question Answering (VQA) models have been shown to rely heavily\non superficial correlations between question and answer words learned during\ntraining such as overwhelmingly reporting the type of room as kitchen or the\nsport being played as tennis, irrespective of the image. Most alarmingly, this\nshortcoming is often not well reflected during evaluation because the same\nstrong priors exist in test distributions; however, a VQA system that fails to\nground questions in image content would likely perform poorly in real-world\nsettings. In this work, we present a novel regularization scheme for VQA that\nreduces this effect. We introduce a question-only model that takes as input the\nquestion encoding from the VQA model and must leverage language biases in order\nto succeed. We then pose training as an adversarial game between the VQA model\nand this question-only adversary -- discouraging the VQA model from capturing\nlanguage biases in its question encoding. Further,we leverage this\nquestion-only model to estimate the increase in model confidence after\nconsidering the image, which we maximize explicitly to encourage visual\ngrounding. Our approach is a model agnostic training procedure and simple to\nimplement. We show empirically that it can improve performance significantly on\na bias-sensitive split of the VQA dataset for multiple base models -- achieving\nstate-of-the-art on this task. Further, on standard VQA tasks, our approach\nshows significantly less drop in accuracy compared to existing bias-reducing\nVQA models. \n\n"}
{"id": "1810.03654", "contents": "Title: Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo\n  Videos Abstract: Learning depth and optical flow via deep neural networks by watching videos\nhas made significant progress recently. In this paper, we jointly solve the two\ntasks by exploiting the underlying geometric rules within stereo videos.\nSpecifically, given two consecutive stereo image pairs from a video, we first\nestimate depth, camera ego-motion and optical flow from three neural networks.\nThen the whole scene is decomposed into moving foreground and static background\nby compar- ing the estimated optical flow and rigid flow derived from the depth\nand ego-motion. We propose a novel consistency loss to let the optical flow\nlearn from the more accurate rigid flow in static regions. We also design a\nrigid alignment module which helps refine ego-motion estimation by using the\nestimated depth and optical flow. Experiments on the KITTI dataset show that\nour results significantly outperform other state-of- the-art algorithms. Source\ncodes can be found at https: //github.com/baidu-research/UnDepthflow \n\n"}
{"id": "1810.03779", "contents": "Title: Reinforcement Learning for Improving Agent Design Abstract: In many reinforcement learning tasks, the goal is to learn a policy to\nmanipulate an agent, whose design is fixed, to maximize some notion of\ncumulative reward. The design of the agent's physical structure is rarely\noptimized for the task at hand. In this work, we explore the possibility of\nlearning a version of the agent's design that is better suited for its task,\njointly with the policy. We propose an alteration to the popular OpenAI Gym\nframework, where we parameterize parts of an environment, and allow an agent to\njointly learn to modify these environment parameters along with its policy. We\ndemonstrate that an agent can learn a better structure of its body that is not\nonly better suited for the task, but also facilitates policy learning. Joint\nlearning of policy and structure may even uncover design principles that are\nuseful for assisted-design applications. Videos of results at\nhttps://designrl.github.io/ \n\n"}
{"id": "1810.03867", "contents": "Title: Functionally Modular and Interpretable Temporal Filtering for Robust\n  Segmentation Abstract: The performance of autonomous systems heavily relies on their ability to\ngenerate a robust representation of the environment. Deep neural networks have\ngreatly improved vision-based perception systems but still fail in challenging\nsituations, e.g. sensor outages or heavy weather. These failures are often\nintroduced by data-inherent perturbations, which significantly reduce the\ninformation provided to the perception system. We propose a functionally\nmodularized temporal filter, which stabilizes an abstract feature\nrepresentation of a single-frame segmentation model using information of\nprevious time steps. Our filter module splits the filter task into multiple\nless complex and more interpretable subtasks. The basic structure of the filter\nis inspired by a Bayes estimator consisting of a prediction and an update step.\nTo make the prediction more transparent, we implement it using a geometric\nprojection and estimate its parameters. This additionally enables the\ndecomposition of the filter task into static representation filtering and\nlow-dimensional motion filtering. Our model can cope with missing frames and is\ntrainable in an end-to-end fashion. Using photorealistic, synthetic video data,\nwe show the ability of the proposed architecture to overcome data-inherent\nperturbations. The experiments especially highlight advantages introduced by an\ninterpretable and explicit filter module. \n\n"}
{"id": "1810.03969", "contents": "Title: A Generative Adversarial Model for Right Ventricle Segmentation Abstract: The clinical management of several cardiovascular conditions, such as\npulmonary hypertension, require the assessment of the right ventricular (RV)\nfunction. This work addresses the fully automatic and robust access to one of\nthe key RV biomarkers, its ejection fraction, from the gold standard imaging\nmodality, MRI. The problem becomes the accurate segmentation of the RV blood\npool from cine MRI sequences. This work proposes a solution based on Fully\nConvolutional Neural Networks (FCNN), where our first contribution is the\noptimal combination of three concepts (the convolution Gated Recurrent Units\n(GRU), the Generative Adversarial Networks (GAN), and the L1 loss function)\nthat achieves an improvement of 0.05 and 3.49 mm in Dice Index and Hausdorff\nDistance respectively with respect to the baseline FCNN. This improvement is\nthen doubled by our second contribution, the ROI-GAN, that sets two GANs to\ncooperate working at two fields of view of the image, its full resolution and\nthe region of interest (ROI). Our rationale here is to better guide the FCNN\nlearning by combining global (full resolution) and local Region Of Interest\n(ROI) features. The study is conducted in a large in-house dataset of $\\sim$\n23.000 segmented MRI slices, and its generality is verified in a publicly\navailable dataset. \n\n"}
{"id": "1810.04029", "contents": "Title: Selective Distillation of Weakly Annotated GTD for Vision-based Slab\n  Identification System Abstract: This paper proposes an algorithm for recognizing slab identification numbers\nin factory scenes. In the development of a deep-learning based system, manual\nlabeling to make ground truth data (GTD) is an important but expensive task.\nFurthermore, the quality of GTD is closely related to the performance of a\nsupervised learning algorithm. To reduce manual work in the labeling process,\nwe generated weakly annotated GTD by marking only character centroids. Whereas\nbounding-boxes for characters require at least a drag-and-drop operation or two\nclicks to annotate a character location, the weakly annotated GTD requires a\nsingle click to record a character location. The main contribution of this\npaper is on selective distillation to improve the quality of the weakly\nannotated GTD. Because manual GTD are usually generated by many people, it may\ncontain personal bias or human error. To address this problem, the information\nin manual GTD is integrated and refined by selective distillation. In the\nprocess of selective distillation, a fully convolutional network is trained\nusing the weakly annotated GTD, and its prediction maps are selectively used to\nrevise locations and boundaries of semantic regions of characters in the\ninitial GTD. The modified GTD are used in the main training stage, and a\npost-processing is conducted to retrieve text information. Experiments were\nthoroughly conducted on actual industry data collected at a steelmaking factory\nto demonstrate the effectiveness of the proposed method. \n\n"}
{"id": "1810.04231", "contents": "Title: Penetrating the Fog: the Path to Efficient CNN Models Abstract: With the increasing demand to deploy convolutional neural networks (CNNs) on\nmobile platforms, the sparse kernel approach was proposed, which could save\nmore parameters than the standard convolution while maintaining accuracy.\nHowever, despite the great potential, no prior research has pointed out how to\ncraft an sparse kernel design with such potential (i.e., effective design), and\nall prior works just adopt simple combinations of existing sparse kernels such\nas group convolution. Meanwhile due to the large design space it is also\nimpossible to try all combinations of existing sparse kernels. In this paper,\nwe are the first in the field to consider how to craft an effective sparse\nkernel design by eliminating the large design space. Specifically, we present a\nsparse kernel scheme to illustrate how to reduce the space from three aspects.\nFirst, in terms of composition we remove designs composed of repeated layers.\nSecond, to remove designs with large accuracy degradation, we find an unified\nproperty named information field behind various sparse kernel designs, which\ncould directly indicate the final accuracy. Last, we remove designs in two\ncases where a better parameter efficiency could be achieved. Additionally, we\nprovide detailed efficiency analysis on the final four designs in our scheme.\nExperimental results validate the idea of our scheme by showing that our scheme\nis able to find designs which are more efficient in using parameters and\ncomputation with similar or higher accuracy. \n\n"}
{"id": "1810.04991", "contents": "Title: SingleGAN: Image-to-Image Translation by a Single-Generator Network\n  using Multiple Generative Adversarial Learning Abstract: Image translation is a burgeoning field in computer vision where the goal is\nto learn the mapping between an input image and an output image. However, most\nrecent methods require multiple generators for modeling different domain\nmappings, which are inefficient and ineffective on some multi-domain image\ntranslation tasks. In this paper, we propose a novel method, SingleGAN, to\nperform multi-domain image-to-image translations with a single generator. We\nintroduce the domain code to explicitly control the different generative tasks\nand integrate multiple optimization goals to ensure the translation.\nExperimental results on several unpaired datasets show superior performance of\nour model in translation between two domains. Besides, we explore variants of\nSingleGAN for different tasks, including one-to-many domain translation,\nmany-to-many domain translation and one-to-one domain translation with\nmultimodality. The extended experiments show the universality and extensibility\nof our model. \n\n"}
{"id": "1810.05247", "contents": "Title: Real-time Faulted Line Localization and PMU Placement in Power Systems\n  through Convolutional Neural Networks Abstract: Diverse fault types, fast re-closures, and complicated transient states after\na fault event make real-time fault location in power grids challenging.\nExisting localization techniques in this area rely on simplistic assumptions,\nsuch as static loads, or require much higher sampling rates or total\nmeasurement availability. This paper proposes a faulted line localization\nmethod based on a Convolutional Neural Network (CNN) classifier using bus\nvoltages. Unlike prior data-driven methods, the proposed classifier is based on\nfeatures with physical interpretations that improve the robustness of the\nlocation performance. The accuracy of our CNN based localization tool is\ndemonstrably superior to other machine learning classifiers in the literature.\nTo further improve the location performance, a joint phasor measurement units\n(PMU) placement strategy is proposed and validated against other methods. A\nsignificant aspect of our methodology is that under very low observability (7%\nof buses), the algorithm is still able to localize the faulted line to a small\nneighborhood with high probability. The performance of our scheme is validated\nthrough simulations of faults of various types in the IEEE 39-bus and 68-bus\npower systems under varying uncertain conditions, system observability, and\nmeasurement quality. \n\n"}
{"id": "1810.05724", "contents": "Title: Unpaired High-Resolution and Scalable Style Transfer Using Generative\n  Adversarial Networks Abstract: Neural networks have proven their capabilities by outperforming many other\napproaches on regression or classification tasks on various kinds of data.\nOther astonishing results have been achieved using neural nets as data\ngenerators, especially in settings of generative adversarial networks (GANs).\nOne special application is the field of image domain translations. Here, the\ngoal is to take an image with a certain style (e.g. a photography) and\ntransform it into another one (e.g. a painting). If such a task is performed\nfor unpaired training examples, the corresponding GAN setting is complex, the\nneural networks are large, and this leads to a high peak memory consumption\nduring, both, training and evaluation phase. This sets a limit to the highest\nprocessable image size. We address this issue by the idea of not processing the\nwhole image at once, but to train and evaluate the domain translation on the\nlevel of overlapping image subsamples. This new approach not only enables us to\ntranslate high-resolution images that otherwise cannot be processed by the\nneural network at once, but also allows us to work with comparably small neural\nnetworks and with limited hardware resources. Additionally, the number of\nimages required for the training process is significantly reduced. We present\nhigh-quality results on images with a total resolution of up to over 50\nmegapixels and emonstrate that our method helps to preserve local image details\nwhile it also keeps global consistency. \n\n"}
{"id": "1810.05786", "contents": "Title: Learning to Globally Edit Images with Textual Description Abstract: We show how we can globally edit images using textual instructions: given a\nsource image and a textual instruction for the edit, generate a new image\ntransformed under this instruction. To tackle this novel problem, we develop\nthree different trainable models based on RNN and Generative Adversarial\nNetwork (GAN). The models (bucket, filter bank, and end-to-end) differ in how\nmuch expert knowledge is encoded, with the most general version being purely\nend-to-end. To train these systems, we use Amazon Mechanical Turk to collect\ntextual descriptions for around 2000 image pairs sampled from several datasets.\nExperimental results evaluated on our dataset validate our approaches. In\naddition, given that the filter bank model is a good compromise between\ngenerality and performance, we investigate it further by replacing RNN with\nGraph RNN, and show that Graph RNN improves performance. To the best of our\nknowledge, this is the first computational photography work on global image\nediting that is purely based on free-form textual instructions. \n\n"}
{"id": "1810.05989", "contents": "Title: Lung Structures Enhancement in Chest Radiographs via CT based FCNN\n  Training Abstract: The abundance of overlapping anatomical structures appearing in chest\nradiographs can reduce the performance of lung pathology detection by automated\nalgorithms (CAD) as well as the human reader. In this paper, we present a deep\nlearning based image processing technique for enhancing the contrast of soft\nlung structures in chest radiographs using Fully Convolutional Neural Networks\n(FCNN). Two 2D FCNN architectures were trained to accomplish the task: The\nfirst performs 2D lung segmentation which is used for normalization of the lung\narea. The second FCNN is trained to extract lung structures. To create the\ntraining images, we employed Simulated X-Ray or Digitally Reconstructed\nRadiographs (DRR) derived from 516 scans belonging to the LIDC-IDRI dataset. By\nfirst segmenting the lungs in the CT domain, we are able to create a dataset of\n2D lung masks to be used for training the segmentation FCNN. For training the\nextraction FCNN, we create DRR images of only voxels belonging to the 3D lung\nsegmentation which we call \"Lung X-ray\" and use them as target images. Once the\nlung structures are extracted, the original image can be enhanced by fusing the\noriginal input x-ray and the synthesized \"Lung X-ray\". We show that our\nenhancement technique is applicable to real x-ray data, and display our results\non the recently released NIH Chest X-Ray-14 dataset. We see promising results\nwhen training a DenseNet-121 based architecture to work directly on the lung\nenhanced X-ray images. \n\n"}
{"id": "1810.06695", "contents": "Title: Exploring the Use of Attention within an Neural Machine Translation\n  Decoder States to Translate Idioms Abstract: Idioms pose problems to almost all Machine Translation systems. This type of\nlanguage is very frequent in day-to-day language use and cannot be simply\nignored. The recent interest in memory augmented models in the field of\nLanguage Modelling has aided the systems to achieve good results by bridging\nlong-distance dependencies. In this paper we explore the use of such techniques\ninto a Neural Machine Translation system to help in translation of idiomatic\nlanguage. \n\n"}
{"id": "1810.06892", "contents": "Title: A Generative Model of Textures Using Hierarchical Probabilistic\n  Principal Component Analysis Abstract: Modeling of textures in natural images is an important task to make a\nmicroscopic model of natural images. Portilla and Simoncelli proposed a\ngenerative texture model, which is based on the mechanism of visual systems in\nbrains, with a set of texture features and a feature matching. On the other\nhand, the texture features, used in Portillas' model, have redundancy between\nits components came from typical natural textures. In this paper, we propose a\ncontracted texture model which provides a dimension reduction for the\nPortillas' feature. This model is based on a hierarchical principal components\nanalysis using known group structure of the feature. In the experiment, we\nreveal effective dimensions to describe texture is fewer than the original\ndescription. Moreover, we also demonstrate how well the textures can be\nsynthesized from the contracted texture representations. \n\n"}
{"id": "1810.07003", "contents": "Title: Dense Multi-path U-Net for Ischemic Stroke Lesion Segmentation in\n  Multiple Image Modalities Abstract: Delineating infarcted tissue in ischemic stroke lesions is crucial to\ndetermine the extend of damage and optimal treatment for this life-threatening\ncondition. However, this problem remains challenging due to high variability of\nischemic strokes' location and shape. Recently, fully-convolutional neural\nnetworks (CNN), in particular those based on U-Net, have led to improved\nperformances for this task. In this work, we propose a novel architecture that\nimproves standard U-Net based methods in three important ways. First, instead\nof combining the available image modalities at the input, each of them is\nprocessed in a different path to better exploit their unique information.\nMoreover, the network is densely-connected (i.e., each layer is connected to\nall following layers), both within each path and across different paths,\nsimilar to HyperDenseNet. This gives our model the freedom to learn the scale\nat which modalities should be processed and combined. Finally, inspired by the\nInception architecture, we improve standard U-Net modules by extending\ninception modules with two convolutional blocks with dilated convolutions of\ndifferent scale. This helps handling the variability in lesion sizes. We split\nthe 93 stroke datasets into training and validation sets containing 83 and 9\nexamples respectively. Our network was trained on a NVidia TITAN XP GPU with 16\nGBs RAM, using ADAM as optimizer and a learning rate of 1$\\times$10$^{-5}$\nduring 200 epochs. Training took around 5 hours and segmentation of a whole\nvolume took between 0.2 and 2 seconds, as average. The performance on the test\nset obtained by our method is compared to several baselines, to demonstrate the\neffectiveness of our architecture, and to a state-of-art architecture that\nemploys factorized dilated convolutions, i.e., ERFNet. \n\n"}
{"id": "1810.07901", "contents": "Title: Decoupling Semantic Context and Color Correlation with multi-class cross\n  branch regularization Abstract: This paper presents a novel design methodology for architecting a\nlight-weight and faster DNN architecture for vision applications. The\neffectiveness of the architecture is demonstrated on Color-Constancy use case\nan inherent block in camera and imaging pipelines. Specifically, we present a\nmulti-branch architecture that disassembles the contextual features and color\nproperties from an image, and later combines them to predict a global property\n(e.g. Global Illumination). We also propose an implicit regularization\ntechnique by designing cross-branch regularization block that enables the\nnetwork to retain high generalization accuracy. With a conservative use of best\ncomputational operators, the proposed architecture achieves state-of-the-art\naccuracy with 30X lesser model parameters and 70X faster inference time for\ncolor constancy. It is also shown that the proposed architecture is generic and\nachieves similar efficiency in other vision applications such as Low-Light\nphotography. \n\n"}
{"id": "1810.07961", "contents": "Title: LeukoNet: DCT-based CNN architecture for the classification of normal\n  versus Leukemic blasts in B-ALL Cancer Abstract: Acute lymphoblastic leukemia (ALL) constitutes approximately 25% of the\npediatric cancers. In general, the task of identifying immature leukemic blasts\nfrom normal cells under the microscope is challenging because morphologically\nthe images of the two cells appear similar. In this paper, we propose a deep\nlearning framework for classifying immature leukemic blasts and normal cells.\nThe proposed model combines the Discrete Cosine Transform (DCT) domain features\nextracted via CNN with the Optical Density (OD) space features to build a\nrobust classifier. Elaborate experiments have been conducted to validate the\nproposed LeukoNet classifier. \n\n"}
{"id": "1810.08229", "contents": "Title: MRI Reconstruction via Cascaded Channel-wise Attention Network Abstract: We consider an MRI reconstruction problem with input of k-space data at a\nvery low undersampled rate. This can practically benefit patient due to reduced\ntime of MRI scan, but it is also challenging since quality of reconstruction\nmay be compromised. Currently, deep learning based methods dominate MRI\nreconstruction over traditional approaches such as Compressed Sensing, but they\nrarely show satisfactory performance in the case of low undersampled k-space\ndata. One explanation is that these methods treat channel-wise features\nequally, which results in degraded representation ability of the neural\nnetwork. To solve this problem, we propose a new model called MRI Cascaded\nChannel-wise Attention Network (MICCAN), highlighted by three components: (i) a\nvariant of U-net with Channel-wise Attention (UCA) module, (ii) a long skip\nconnection and (iii) a combined loss. Our model is able to attend to salient\ninformation by filtering irrelevant features and also concentrate on\nhigh-frequency information by enforcing low-frequency information bypassed to\nthe final output. We conduct both quantitative evaluation and qualitative\nanalysis of our method on a cardiac dataset. The experiment shows that our\nmethod achieves very promising results in terms of three common metrics on the\nMRI reconstruction with low undersampled k-space data. \n\n"}
{"id": "1810.08534", "contents": "Title: MsCGAN: Multi-scale Conditional Generative Adversarial Networks for\n  Person Image Generation Abstract: To synthesize high-quality person images with arbitrary poses is challenging.\nIn this paper, we propose a novel Multi-scale Conditional Generative\nAdversarial Networks (MsCGAN), aiming to convert the input conditional person\nimage to a synthetic image of any given target pose, whose appearance and the\ntexture are consistent with the input image. MsCGAN is a multi-scale\nadversarial network consisting of two generators and two discriminators. One\ngenerator transforms the conditional person image into a coarse image of the\ntarget pose globally, and the other is to enhance the detailed quality of the\nsynthetic person image through a local reinforcement network. The outputs of\nthe two generators are then merged into a synthetic, discriminant and\nhigh-resolution image. On the other hand, the synthetic image is downsampled to\nmultiple resolutions as the input to multi-scale discriminator networks. The\nproposed multi-scale generators and discriminators handling different levels of\nvisual features can benefit to synthesizing high-resolution person images with\nrealistic appearance and texture. Experiments are conducted on the Market-1501\nand DeepFashion datasets to evaluate the proposed model, and both qualitative\nand quantitative results demonstrate the superior performance of the proposed\nMsCGAN. \n\n"}
{"id": "1810.09044", "contents": "Title: VIENA2: A Driving Anticipation Dataset Abstract: Action anticipation is critical in scenarios where one needs to react before\nthe action is finalized. This is, for instance, the case in automated driving,\nwhere a car needs to, e.g., avoid hitting pedestrians and respect traffic\nlights. While solutions have been proposed to tackle subsets of the driving\nanticipation tasks, by making use of diverse, task-specific sensors, there is\nno single dataset or framework that addresses them all in a consistent manner.\nIn this paper, we therefore introduce a new, large-scale dataset, called\nVIENA2, covering 5 generic driving scenarios, with a total of 25 distinct\naction classes. It contains more than 15K full HD, 5s long videos acquired in\nvarious driving conditions, weathers, daytimes and environments, complemented\nwith a common and realistic set of sensor measurements. This amounts to more\nthan 2.25M frames, each annotated with an action label, corresponding to 600\nsamples per action class. We discuss our data acquisition strategy and the\nstatistics of our dataset, and benchmark state-of-the-art action anticipation\ntechniques, including a new multi-modal LSTM architecture with an effective\nloss function for action anticipation in driving scenarios. \n\n"}
{"id": "1810.09676", "contents": "Title: Action-Agnostic Human Pose Forecasting Abstract: Predicting and forecasting human dynamics is a very interesting but\nchallenging task with several prospective applications in robotics,\nhealth-care, etc. Recently, several methods have been developed for human pose\nforecasting; however, they often introduce a number of limitations in their\nsettings. For instance, previous work either focused only on short-term or\nlong-term predictions, while sacrificing one or the other. Furthermore, they\nincluded the activity labels as part of the training process, and require them\nat testing time. These limitations confine the usage of pose forecasting models\nfor real-world applications, as often there are no activity-related annotations\nfor testing scenarios. In this paper, we propose a new action-agnostic method\nfor short- and long-term human pose forecasting. To this end, we propose a new\nrecurrent neural network for modeling the hierarchical and multi-scale\ncharacteristics of the human dynamics, denoted by triangular-prism RNN\n(TP-RNN). Our model captures the latent hierarchical structure embedded in\ntemporal human pose sequences by encoding the temporal dependencies with\ndifferent time-scales. For evaluation, we run an extensive set of experiments\non Human 3.6M and Penn Action datasets and show that our method outperforms\nbaseline and state-of-the-art methods quantitatively and qualitatively. Codes\nare available at https://github.com/eddyhkchiu/pose_forecast_wacv/ \n\n"}
{"id": "1810.09726", "contents": "Title: CEREALS - Cost-Effective REgion-based Active Learning for Semantic\n  Segmentation Abstract: State of the art methods for semantic image segmentation are trained in a\nsupervised fashion using a large corpus of fully labeled training images.\nHowever, gathering such a corpus is expensive, due to human annotation effort,\nin contrast to gathering unlabeled data. We propose an active learning-based\nstrategy, called CEREALS, in which a human only has to hand-label a few,\nautomatically selected, regions within an unlabeled image corpus. This\nminimizes human annotation effort while maximizing the performance of a\nsemantic image segmentation method. The automatic selection procedure is\nachieved by: a) using a suitable information measure combined with an estimate\nabout human annotation effort, which is inferred from a learned cost model, and\nb) exploiting the spatial coherency of an image. The performance of CEREALS is\ndemonstrated on Cityscapes, where we are able to reduce the annotation effort\nto 17%, while keeping 95% of the mean Intersection over Union (mIoU) of a model\nthat was trained with the fully annotated training set of Cityscapes. \n\n"}
{"id": "1810.09951", "contents": "Title: GhostVLAD for set-based face recognition Abstract: The objective of this paper is to learn a compact representation of image\nsets for template-based face recognition. We make the following contributions:\nfirst, we propose a network architecture which aggregates and embeds the face\ndescriptors produced by deep convolutional neural networks into a compact\nfixed-length representation. This compact representation requires minimal\nmemory storage and enables efficient similarity computation. Second, we propose\na novel GhostVLAD layer that includes {\\em ghost clusters}, that do not\ncontribute to the aggregation. We show that a quality weighting on the input\nfaces emerges automatically such that informative images contribute more than\nthose with low quality, and that the ghost clusters enhance the network's\nability to deal with poor quality images. Third, we explore how input feature\ndimension, number of clusters and different training techniques affect the\nrecognition performance. Given this analysis, we train a network that far\nexceeds the state-of-the-art on the IJB-B face recognition dataset. This is\ncurrently one of the most challenging public benchmarks, and we surpass the\nstate-of-the-art on both the identification and verification protocols. \n\n"}
{"id": "1810.10039", "contents": "Title: DeepLSR: a deep learning approach for laser speckle reduction Abstract: Speckle artifacts degrade image quality in virtually all modalities that\nutilize coherent energy, including optical coherence tomography, reflectance\nconfocal microscopy, ultrasound, and widefield imaging with laser illumination.\nWe present an adversarial deep learning framework for laser speckle reduction,\ncalled DeepLSR (https://durr.jhu.edu/DeepLSR), that transforms images from a\nsource domain of coherent illumination to a target domain of speckle-free,\nincoherent illumination. We apply this method to widefield images of objects\nand tissues illuminated with a multi-wavelength laser, using light emitting\ndiode-illuminated images as ground truth. In images of gastrointestinal\ntissues, DeepLSR reduces laser speckle noise by 6.4 dB, compared to a 2.9 dB\nreduction from optimized non-local means processing, a 3.0 dB reduction from\nBM3D, and a 3.7 dB reduction from an optical speckle reducer utilizing an\noscillating diffuser. Further, DeepLSR can be combined with optical speckle\nreduction to reduce speckle noise by 9.4 dB. This dramatic reduction in speckle\nnoise may enable the use of coherent light sources in applications that require\nsmall illumination sources and high-quality imaging, including medical\nendoscopy. \n\n"}
{"id": "1810.10093", "contents": "Title: Structured Domain Randomization: Bridging the Reality Gap by\n  Context-Aware Synthetic Data Abstract: We present structured domain randomization (SDR), a variant of domain\nrandomization (DR) that takes into account the structure and context of the\nscene. In contrast to DR, which places objects and distractors randomly\naccording to a uniform probability distribution, SDR places objects and\ndistractors randomly according to probability distributions that arise from the\nspecific problem at hand. In this manner, SDR-generated imagery enables the\nneural network to take the context around an object into consideration during\ndetection. We demonstrate the power of SDR for the problem of 2D bounding box\ncar detection, achieving competitive results on real data after training only\non synthetic data. On the KITTI easy, moderate, and hard tasks, we show that\nSDR outperforms other approaches to generating synthetic data (VKITTI, Sim\n200k, or DR), as well as real data collected in a different domain (BDD100K).\nMoreover, synthetic SDR data combined with real KITTI data outperforms real\nKITTI data alone. \n\n"}
{"id": "1810.10155", "contents": "Title: Background Subtraction using Compressed Low-resolution Images Abstract: Image processing and recognition are an important part of the modern society,\nwith applications in fields such as advanced artificial intelligence, smart\nassistants, and security surveillance. The essential first step involved in\nalmost all the visual tasks is background subtraction with a static camera.\nEnsuring that this critical step is performed in the most efficient manner\nwould therefore improve all aspects related to objects recognition and\ntracking, behavior comprehension, etc.. Although background subtraction method\nhas been applied for many years, its application suffers from real-time\nrequirement. In this letter, we present a novel approach in implementing the\nbackground subtraction. The proposed method uses compressed, low-resolution\ngrayscale image for the background subtraction. These low-resolution grayscale\nimages were found to preserve the salient information very well. To verify the\nfeasibility of our methodology, two prevalent methods, ViBe and GMM, are used\nin the experiment. The results of the proposed methodology confirm the\neffectiveness of our approach. \n\n"}
{"id": "1810.10343", "contents": "Title: From Machine to Machine: An OCT-trained Deep Learning Algorithm for\n  Objective Quantification of Glaucomatous Damage in Fundus Photographs Abstract: Previous approaches using deep learning algorithms to classify glaucomatous\ndamage on fundus photographs have been limited by the requirement for human\nlabeling of a reference training set. We propose a new approach using\nspectral-domain optical coherence tomography (SDOCT) data to train a deep\nlearning algorithm to quantify glaucomatous structural damage on optic disc\nphotographs. The dataset included 32,820 pairs of optic disc photos and SDOCT\nretinal nerve fiber layer (RNFL) scans from 2,312 eyes of 1,198 subjects. A\ndeep learning convolutional neural network was trained to assess optic disc\nphotographs and predict SDOCT average RNFL thickness. The performance of the\nalgorithm was evaluated in an independent test sample. The mean prediction of\naverage RNFL thickness from all 6,292 optic disc photos in the test set was\n83.3$\\pm$14.5 $\\mu$m, whereas the mean average RNFL thickness from all\ncorresponding SDOCT scans was 82.5$\\pm$16.8 $\\mu$m (P = 0.164). There was a\nvery strong correlation between predicted and observed RNFL thickness values (r\n= 0.832; P<0.001), with mean absolute error of the predictions of 7.39 $\\mu$m.\nThe areas under the receiver operating characteristic curves for discriminating\nglaucoma from healthy eyes with the deep learning predictions and actual SDOCT\nmeasurements were 0.944 (95$\\%$ CI: 0.912- 0.966) and 0.940 (95$\\%$ CI: 0.902 -\n0.966), respectively (P = 0.724). In conclusion, we introduced a novel deep\nlearning approach to assess optic disc photographs and provide quantitative\ninformation about the amount of neural damage. This approach could potentially\nbe used to diagnose and stage glaucomatous damage from optic disc photographs. \n\n"}
{"id": "1810.10358", "contents": "Title: Implicit Modeling with Uncertainty Estimation for Intravoxel Incoherent\n  Motion Imaging Abstract: Intravoxel incoherent motion (IVIM) imaging allows contrast-agent free in\nvivo perfusion quantification with magnetic resonance imaging (MRI). However,\nits use is limited by typically low accuracy due to low signal-to-noise ratio\n(SNR) at large gradient encoding magnitudes as well as dephasing artefacts\ncaused by subject motion, which is particularly challenging in fetal MRI. To\nmitigate this problem, we propose an implicit IVIM signal acquisition model\nwith which we learn full posterior distribution of perfusion parameters using\nartificial neural networks. This posterior then encapsulates the uncertainty of\nthe inferred parameter estimates, which we validate herein via numerical\nexperiments with rejection-based Bayesian sampling. Compared to\nstate-of-the-art IVIM estimation method of segmented least-squares fitting, our\nproposed approach improves parameter estimation accuracy by 65% on synthetic\nanisotropic perfusion data. On paired rescans of in vivo fetal MRI, our method\nincreases repeatability of parameter estimation in placenta by 46%. \n\n"}
{"id": "1810.10627", "contents": "Title: Streaming Graph Neural Networks Abstract: Graphs are essential representations of many real-world data such as social\nnetworks. Recent years have witnessed the increasing efforts made to extend the\nneural network models to graph-structured data. These methods, which are\nusually known as the graph neural networks, have been applied to advance many\ngraphs related tasks such as reasoning dynamics of the physical system, graph\nclassification, and node classification. Most of the existing graph neural\nnetwork models have been designed for static graphs, while many real-world\ngraphs are inherently dynamic. For example, social networks are naturally\nevolving as new users joining and new relations being created. Current graph\nneural network models cannot utilize the dynamic information in dynamic graphs.\nHowever, the dynamic information has been proven to enhance the performance of\nmany graph analytic tasks such as community detection and link prediction.\nHence, it is necessary to design dedicated graph neural networks for dynamic\ngraphs. In this paper, we propose DGNN, a new {\\bf D}ynamic {\\bf G}raph {\\bf\nN}eural {\\bf N}etwork model, which can model the dynamic information as the\ngraph evolving. In particular, the proposed framework can keep updating node\ninformation by capturing the sequential information of edges (interactions),\nthe time intervals between edges and information propagation coherently.\nExperimental results on various dynamic graphs demonstrate the effectiveness of\nthe proposed framework. \n\n"}
{"id": "1810.10658", "contents": "Title: Sports Camera Calibration via Synthetic Data Abstract: Calibrating sports cameras is important for autonomous broadcasting and\nsports analysis. Here we propose a highly automatic method for calibrating\nsports cameras from a single image using synthetic data. First, we develop a\nnovel camera pose engine. The camera pose engine has only three significant\nfree parameters so that it can effectively generate a lot of camera poses and\ncorresponding edge (i.e, field marking) images. Then, we learn compact deep\nfeatures via a siamese network from paired edge image and camera pose and build\na feature-pose database. After that, we use a novel two-GAN (generative\nadversarial network) model to detect field markings in real images. Finally, we\nquery an initial camera pose from the feature-pose database and refine camera\nposes using truncated distance images. We evaluate our method on both synthetic\nand real data. Our method not only demonstrates the robustness on the synthetic\ndata but also achieves the state-of-the-art accuracy on a standard soccer\ndataset and very high performance on a volleyball dataset. \n\n"}
{"id": "1810.11181", "contents": "Title: Neural Modular Control for Embodied Question Answering Abstract: We present a modular approach for learning policies for navigation over long\nplanning horizons from language input. Our hierarchical policy operates at\nmultiple timescales, where the higher-level master policy proposes subgoals to\nbe executed by specialized sub-policies. Our choice of subgoals is\ncompositional and semantic, i.e. they can be sequentially combined in arbitrary\norderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find\nkitchen', 'find refrigerator', etc.).\n  We use imitation learning to warm-start policies at each level of the\nhierarchy, dramatically increasing sample efficiency, followed by reinforcement\nlearning. Independent reinforcement learning at each level of hierarchy enables\nsub-policies to adapt to consequences of their actions and recover from errors.\nSubsequent joint hierarchical training enables the master policy to adapt to\nthe sub-policies.\n  On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al.,\n2018), requiring navigating diverse realistic indoor environments, our approach\noutperforms prior work by a significant margin, both in terms of navigation and\nquestion answering. \n\n"}
{"id": "1810.12091", "contents": "Title: Embedding Geographic Locations for Modelling the Natural Environment\n  using Flickr Tags and Structured Data Abstract: Meta-data from photo-sharing websites such as Flickr can be used to obtain\nrich bag-of-words descriptions of geographic locations, which have proven\nvaluable, among others, for modelling and predicting ecological features. One\nimportant insight from previous work is that the descriptions obtained from\nFlickr tend to be complementary to the structured information that is available\nfrom traditional scientific resources. To better integrate these two diverse\nsources of information, in this paper we consider a method for learning vector\nspace embeddings of geographic locations. We show experimentally that this\nmethod improves on existing approaches, especially in cases where structured\ninformation is available. \n\n"}
{"id": "1810.12348", "contents": "Title: Gather-Excite: Exploiting Feature Context in Convolutional Neural\n  Networks Abstract: While the use of bottom-up local operators in convolutional neural networks\n(CNNs) matches well some of the statistics of natural images, it may also\nprevent such models from capturing contextual long-range feature interactions.\nIn this work, we propose a simple, lightweight approach for better context\nexploitation in CNNs. We do so by introducing a pair of operators: gather,\nwhich efficiently aggregates feature responses from a large spatial extent, and\nexcite, which redistributes the pooled information to local features. The\noperators are cheap, both in terms of number of added parameters and\ncomputational complexity, and can be integrated directly in existing\narchitectures to improve their performance. Experiments on several datasets\nshow that gather-excite can bring benefits comparable to increasing the depth\nof a CNN at a fraction of the cost. For example, we find ResNet-50 with\ngather-excite operators is able to outperform its 101-layer counterpart on\nImageNet with no additional learnable parameters. We also propose a parametric\ngather-excite operator pair which yields further performance gains, relate it\nto the recently-introduced Squeeze-and-Excitation Networks, and analyse the\neffects of these changes to the CNN feature activation statistics. \n\n"}
{"id": "1810.12997", "contents": "Title: An Online-Learning Approach to Inverse Optimization Abstract: In this paper, we demonstrate how to learn the objective function of a\ndecision-maker while only observing the problem input data and the\ndecision-maker's corresponding decisions over multiple rounds. We present exact\nalgorithms for this online version of inverse optimization which converge at a\nrate of $ \\mathcal{O}(1/\\sqrt{T}) $ in the number of observations~$T$ and\ncompare their further properties. Especially, they all allow taking decisions\nwhich are essentially as good as those of the observed decision-maker already\nafter relatively few iterations, but are suited best for different settings\neach. Our approach is based on online learning and works for linear objectives\nover arbitrary feasible sets for which we have a linear optimization oracle. As\nsuch, it generalizes previous approaches based on KKT-system decomposition and\ndualization. We also introduce several generalizations, such as the approximate\nlearning of non-linear objective functions, dynamically changing as well as\nparameterized objectives and the case of suboptimal observed decisions. When\napplied to the stochastic offline case, our algorithms are able to give\nguarantees on the quality of the learned objectives in expectation. Finally, we\nshow the effectiveness and possible applications of our methods in indicative\ncomputational experiments. \n\n"}
{"id": "1811.00344", "contents": "Title: Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual\n  Super-resolution Network Abstract: Convolutional neural network (CNN) based methods have recently achieved great\nsuccess for image super-resolution (SR). However, most deep CNN based SR models\nattempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while\nresulting in poor quantified perceptual quality (e.g. human opinion score,\nno-reference quality measures such as NIQE). Few works have attempted to\nimprove the perceptual quality at the cost of performance reduction in\ndistortion measures. A very recent study has revealed that distortion and\nperceptual quality are at odds with each other and there is always a trade-off\nbetween the two. Often the restoration algorithms that are superior in terms of\nperceptual quality, are inferior in terms of distortion measures. Our work\nattempts to analyze the trade-off between distortion and perceptual quality for\nthe problem of single image SR. To this end, we use the well-known SR\narchitecture-enhanced deep super-resolution (EDSR) network and show that it can\nbe adapted to achieve better perceptual quality for a specific range of the\ndistortion measure. While the original network of EDSR was trained to minimize\nthe error defined based on per-pixel accuracy alone, we train our network using\na generative adversarial network framework with EDSR as the generator module.\nOur proposed network, called enhanced perceptual super-resolution network\n(EPSR), is trained with a combination of mean squared error loss, perceptual\nloss, and adversarial loss. Our experiments reveal that EPSR achieves the\nstate-of-the-art trade-off between distortion and perceptual quality while the\nexisting methods perform well in either of these measures alone. \n\n"}
{"id": "1811.00473", "contents": "Title: Unsupervised representation learning using convolutional and stacked\n  auto-encoders: a domain and cross-domain feature space analysis Abstract: A feature learning task involves training models that are capable of\ninferring good representations (transformations of the original space) from\ninput data alone. When working with limited or unlabelled data, and also when\nmultiple visual domains are considered, methods that rely on large annotated\ndatasets, such as Convolutional Neural Networks (CNNs), cannot be employed. In\nthis paper we investigate different auto-encoder (AE) architectures, which\nrequire no labels, and explore training strategies to learn representations\nfrom images. The models are evaluated considering both the reconstruction error\nof the images and the feature spaces in terms of their discriminative power. We\nstudy the role of dense and convolutional layers on the results, as well as the\ndepth and capacity of the networks, since those are shown to affect both the\ndimensionality reduction and the capability of generalising for different\nvisual domains. Classification results with AE features were as discriminative\nas pre-trained CNN features. Our findings can be used as guidelines for the\ndesign of unsupervised representation learning methods within and across\ndomains. \n\n"}
{"id": "1811.01558", "contents": "Title: Stochastic Modified Equations and Dynamics of Stochastic Gradient\n  Algorithms I: Mathematical Foundations Abstract: We develop the mathematical foundations of the stochastic modified equations\n(SME) framework for analyzing the dynamics of stochastic gradient algorithms,\nwhere the latter is approximated by a class of stochastic differential\nequations with small noise parameters. We prove that this approximation can be\nunderstood mathematically as an weak approximation, which leads to a number of\nprecise and useful results on the approximations of stochastic gradient descent\n(SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in\nthe general setting of stochastic objectives. We also demonstrate through\nexplicit calculations that this continuous-time approach can uncover important\nanalytical insights into the stochastic gradient algorithms under consideration\nthat may not be easy to obtain in a purely discrete-time setting. \n\n"}
{"id": "1811.01713", "contents": "Title: Word Mover's Embedding: From Word2Vec to Document Embedding Abstract: While the celebrated Word2Vec technique yields semantically rich\nrepresentations for individual words, there has been relatively less success in\nextending to generate unsupervised sentences or documents embeddings. Recent\nwork has demonstrated that a distance measure between documents called\n\\emph{Word Mover's Distance} (WMD) that aligns semantically similar words,\nyields unprecedented KNN classification accuracy. However, WMD is expensive to\ncompute, and it is hard to extend its use beyond a KNN classifier. In this\npaper, we propose the \\emph{Word Mover's Embedding } (WME), a novel approach to\nbuilding an unsupervised document (sentence) embedding from pre-trained word\nembeddings. In our experiments on 9 benchmark text classification datasets and\n22 textual similarity tasks, the proposed technique consistently matches or\noutperforms state-of-the-art techniques, with significantly higher accuracy on\nproblems of short length. \n\n"}
{"id": "1811.02248", "contents": "Title: SparseFool: a few pixels make a big difference Abstract: Deep Neural Networks have achieved extraordinary results on image\nclassification tasks, but have been shown to be vulnerable to attacks with\ncarefully crafted perturbations of the input data. Although most attacks\nusually change values of many image's pixels, it has been shown that deep\nnetworks are also vulnerable to sparse alterations of the input. However, no\ncomputationally efficient method has been proposed to compute sparse\nperturbations. In this paper, we exploit the low mean curvature of the decision\nboundary, and propose SparseFool, a geometry inspired sparse attack that\ncontrols the sparsity of the perturbations. Extensive evaluations show that our\napproach computes sparse perturbations very fast, and scales efficiently to\nhigh dimensional data. We further analyze the transferability and the visual\neffects of the perturbations, and show the existence of shared semantic\ninformation across the images and the networks. Finally, we show that\nadversarial training can only slightly improve the robustness against sparse\nadditive perturbations computed with SparseFool. \n\n"}
{"id": "1811.02384", "contents": "Title: Robust Bhattacharyya bound linear discriminant analysis through adaptive\n  algorithm Abstract: In this paper, we propose a novel linear discriminant analysis criterion via\nthe Bhattacharyya error bound estimation based on a novel L1-norm (L1BLDA) and\nL2-norm (L2BLDA). Both L1BLDA and L2BLDA maximize the between-class scatters\nwhich are measured by the weighted pairwise distances of class means and\nmeanwhile minimize the within-class scatters under the L1-norm and L2-norm,\nrespectively. The proposed models can avoid the small sample size (SSS) problem\nand have no rank limit that may encounter in LDA. It is worth mentioning that,\nthe employment of L1-norm gives a robust performance of L1BLDA, and L1BLDA is\nsolved through an effective non-greedy alternating direction method of\nmultipliers (ADMM), where all the projection vectors can be obtained once for\nall. In addition, the weighting constants of L1BLDA and L2BLDA between the\nbetween-class and within-class terms are determined by the involved data set,\nwhich makes our L1BLDA and L2BLDA adaptive. The experimental results on both\nbenchmark data sets as well as the handwritten digit databases demonstrate the\neffectiveness of the proposed methods. \n\n"}
{"id": "1811.02471", "contents": "Title: Convolutional LSTMs for Cloud-Robust Segmentation of Remote Sensing\n  Imagery Abstract: Clouds frequently cover the Earth's surface and pose an omnipresent challenge\nto optical Earth observation methods. The vast majority of remote sensing\napproaches either selectively choose single cloud-free observations or employ a\npre-classification strategy to identify and mask cloudy pixels. We follow a\ndifferent strategy and treat cloud coverage as noise that is inherent to the\nobserved satellite data. In prior work, we directly employed a straightforward\n\\emph{convolutional long short-term memory} network for vegetation\nclassification without explicit cloud filtering and achieved state-of-the-art\nclassification accuracies. In this work, we investigate this cloud-robustness\nfurther by visualizing internal cell activations and performing an ablation\nexperiment on datasets of different cloud coverage. In the visualizations of\nnetwork states, we identified some cells in which modulation and input gates\nclosed on cloudy pixels. This indicates that the network has internalized a\ncloud-filtering mechanism without being specifically trained on cloud labels.\nOverall, our results question the necessity of sophisticated pre-processing\npipelines for multi-temporal deep learning approaches. \n\n"}
{"id": "1811.02658", "contents": "Title: When Not to Classify: Detection of Reverse Engineering Attacks on DNN\n  Image Classifiers Abstract: This paper addresses detection of a reverse engineering (RE) attack targeting\na deep neural network (DNN) image classifier; by querying, RE's aim is to\ndiscover the classifier's decision rule. RE can enable test-time evasion\nattacks, which require knowledge of the classifier. Recently, we proposed a\nquite effective approach (ADA) to detect test-time evasion attacks. In this\npaper, we extend ADA to detect RE attacks (ADA-RE). We demonstrate our method\nis successful in detecting \"stealthy\" RE attacks before they learn enough to\nlaunch effective test-time evasion attacks. \n\n"}
{"id": "1811.02667", "contents": "Title: Band Selection from Hyperspectral Images Using Attention-based\n  Convolutional Neural Networks Abstract: This paper introduces new attention-based convolutional neural networks for\nselecting bands from hyperspectral images. The proposed approach re-uses\nconvolutional activations at different depths, identifying the most informative\nregions of the spectrum with the help of gating mechanisms. Our attention\ntechniques are modular and easy to implement, and they can be seamlessly\ntrained end-to-end using gradient descent. Our rigorous experiments showed that\ndeep models equipped with the attention mechanism deliver high-quality\nclassification, and repeatedly identify significant bands in the training data,\npermitting the creation of refined and extremely compact sets that retain the\nmost meaningful features. \n\n"}
{"id": "1811.02689", "contents": "Title: Training Domain Specific Models for Energy-Efficient Object Detection Abstract: We propose an end-to-end framework for training domain specific models (DSMs)\nto obtain both high accuracy and computational efficiency for object detection\ntasks. DSMs are trained with distillation \\cite{hinton2015distilling} and focus\non achieving high accuracy at a limited domain (e.g. fixed view of an\nintersection). We argue that DSMs can capture essential features well even with\na small model size, enabling higher accuracy and efficiency than traditional\ntechniques. In addition, we improve the training efficiency by reducing the\ndataset size by culling easy to classify images from the training set. For the\nlimited domain, we observed that compact DSMs significantly surpass the\naccuracy of COCO trained models of the same size. By training on a compact\ndataset, we show that with an accuracy drop of only 3.6\\%, the training time\ncan be reduced by 93\\%. The codes are uploaded in\nhttps://github.com/kentaroy47/training-domain-specific-models. \n\n"}
{"id": "1811.02796", "contents": "Title: Amalgamating Knowledge towards Comprehensive Classification Abstract: With the rapid development of deep learning, there have been an\nunprecedentedly large number of trained deep network models available online.\nReusing such trained models can significantly reduce the cost of training the\nnew models from scratch, if not infeasible at all as the annotations used for\nthe training original networks are often unavailable to public. We propose in\nthis paper to study a new model-reusing task, which we term as \\emph{knowledge\namalgamation}. Given multiple trained teacher networks, each of which\nspecializes in a different classification problem, the goal of knowledge\namalgamation is to learn a lightweight student model capable of handling the\ncomprehensive classification. We assume no other annotations except the outputs\nfrom the teacher models are available, and thus focus on extracting and\namalgamating knowledge from the multiple teachers. To this end, we propose a\npilot two-step strategy to tackle the knowledge amalgamation task, by learning\nfirst the compact feature representations from teachers and then the network\nparameters in a layer-wise manner so as to build the student model. We apply\nthis approach to four public datasets and obtain very encouraging results: even\nwithout any human annotation, the obtained student model is competent to handle\nthe comprehensive classification task and in most cases outperforms the\nteachers in individual sub-tasks. \n\n"}
{"id": "1811.03014", "contents": "Title: Beyond the Leaderboard: Insight and Deployment Challenges to Address\n  Research Problems Abstract: In the medical image analysis field, organizing challenges with associated\nworkshops at international conferences began in 2007 and has grown to include\nover 150 challenges. Several of these challenges have had a major impact in the\nfield. However, whereas well-designed challenges have the potential to unite\nand focus the field on creating solutions to important problems, poorly\ndesigned and documented challenges can equally impede a field and lead to\npursuing incremental improvements in metric scores with no theoretic or\nclinical significance. This is supported by a critical assessment of challenges\nat the international MICCAI conference. In this assessment the main observation\nwas that small changes to the underlying challenge data can drastically change\nthe ranking order on the leaderboard. Related to this is the practice of\nleaderboard climbing, which is characterized by participants focusing on\nincrementally improving metric results rather than advancing science or solving\nthe driving problem of a challenge. In this abstract we look beyond the\nleaderboard of a challenge and instead look at the conclusions that can be\ndrawn from a challenge with respect to the research problem that it is\naddressing. Research study design is well described in other research areas and\ncan be translated to challenge design when viewing challenges as research\nstudies on algorithm performance that address a research problem. Based on the\ntwo main types of scientific research study design, we propose two main\nchallenge types, which we think would benefit other research areas as well: 1)\nan insight challenge that is based on a qualitative study design and 2) a\ndeployment challenge that is based on a quantitative study design. In addition\nwe briefly touch upon related considerations with respect to statistical\nsignificance versus practical significance, generalizability and data\nsaturation. \n\n"}
{"id": "1811.03063", "contents": "Title: Generative Adversarial Speaker Embedding Networks for Domain Robust\n  End-to-End Speaker Verification Abstract: This article presents a novel approach for learning domain-invariant speaker\nembeddings using Generative Adversarial Networks. The main idea is to confuse a\ndomain discriminator so that is can't tell if embeddings are from the source or\ntarget domains. We train several GAN variants using our proposed framework and\napply them to the speaker verification task. On the challenging NIST-SRE 2016\ndataset, we are able to match the performance of a strong baseline x-vector\nsystem. In contrast to the the baseline systems which are dependent on\ndimensionality reduction (LDA) and an external classifier (PLDA), our proposed\nspeaker embeddings can be scored using simple cosine distance. This is achieved\nby optimizing our models end-to-end, using an angular margin loss function.\nFurthermore, we are able to significantly boost verification performance by\naveraging our different GAN models at the score level, achieving a relative\nimprovement of 7.2% over the baseline. \n\n"}
{"id": "1811.03173", "contents": "Title: Automatic Thresholding of SIFT Descriptors Abstract: We introduce a method to perform automatic thresholding of SIFT descriptors\nthat improves matching performance by at least 15.9% on the Oxford image\nmatching benchmark. The method uses a contrario methodology to determine a\nunique bin magnitude threshold. This is done by building a generative uniform\nbackground model for descriptors and determining when bin magnitudes have\nreached a sufficient level. The presented method, called meaningful clamping,\ncontrasts from the current SIFT implementation by efficiently computing a\nclamping threshold that is unique for every descriptor. \n\n"}
{"id": "1811.03208", "contents": "Title: Deep Semantic Instance Segmentation of Tree-like Structures Using\n  Synthetic Data Abstract: Tree-like structures, such as blood vessels, often express complexity at very\nfine scales, requiring high-resolution grids to adequately describe their\nshape. Such sparse morphology can alternately be represented by locations of\ncentreline points, but learning from this type of data with deep learning is\nchallenging due to it being unordered, and permutation invariant. In this work,\nwe propose a deep neural network that directly consumes unordered points along\nthe centreline of a branching structure, to identify the topology of the\nrepresented structure in a single-shot. Key to our approach is the use of a\nnovel multi-task loss function, enabling instance segmentation of arbitrarily\ncomplex branching structures. We train the network solely using synthetically\ngenerated data, utilizing domain randomization to facilitate the transfer to\nreal 2D and 3D data. Results show that our network can reliably extract\nmeaningful information about branch locations, bifurcations and endpoints, and\nsets a new benchmark for semantic instance segmentation in branching\nstructures. \n\n"}
{"id": "1811.03214", "contents": "Title: Facial Landmark Detection for Manga Images Abstract: The topic of facial landmark detection has been widely covered for pictures\nof human faces, but it is still a challenge for drawings. Indeed, the\nproportions and symmetry of standard human faces are not always used for comics\nor mangas. The personal style of the author, the limitation of colors, etc.\nmakes the landmark detection on faces in drawings a difficult task. Detecting\nthe landmarks on manga images will be useful to provide new services for easily\nediting the character faces, estimating the character emotions, or generating\nautomatically some animations such as lip or eye movements.\n  This paper contains two main contributions: 1) a new landmark annotation\nmodel for manga faces, and 2) a deep learning approach to detect these\nlandmarks. We use the \"Deep Alignment Network\", a multi stage architecture\nwhere the first stage makes an initial estimation which gets refined in further\nstages. The first results show that the proposed method succeed to accurately\nfind the landmarks in more than 80% of the cases. \n\n"}
{"id": "1811.04064", "contents": "Title: Block Belief Propagation for Parameter Learning in Markov Random Fields Abstract: Traditional learning methods for training Markov random fields require doing\ninference over all variables to compute the likelihood gradient. The iteration\ncomplexity for those methods therefore scales with the size of the graphical\nmodels. In this paper, we propose \\emph{block belief propagation learning}\n(BBPL), which uses block-coordinate updates of approximate marginals to compute\napproximate gradients, removing the need to compute inference on the entire\ngraphical model. Thus, the iteration complexity of BBPL does not scale with the\nsize of the graphs. We prove that the method converges to the same solution as\nthat obtained by using full inference per iteration, despite these\napproximations, and we empirically demonstrate its scalability improvements\nover standard training methods. \n\n"}
{"id": "1811.04172", "contents": "Title: Use of Neural Signals to Evaluate the Quality of Generative Adversarial\n  Network Performance in Facial Image Generation Abstract: There is a growing interest in using generative adversarial networks (GANs)\nto produce image content that is indistinguishable from real images as judged\nby a typical person. A number of GAN variants for this purpose have been\nproposed, however, evaluating GANs performance is inherently difficult because\ncurrent methods for measuring the quality of their output are not always\nconsistent with what a human perceives. We propose a novel approach that\ncombines a brain-computer interface (BCI) with GANs to generate a measure we\ncall Neuroscore, which closely mirrors the behavioral ground truth measured\nfrom participants tasked with discerning real from synthetic images. This\ntechnique we call a neuro-AI interface, as it provides an interface between a\nhuman's neural systems and an AI process. In this paper, we first compare the\nthree most widely used metrics in the literature for evaluating GANs in terms\nof visual quality and compare their outputs with human judgments. Secondly we\npropose and demonstrate a novel approach using neural signals and rapid serial\nvisual presentation (RSVP) that directly measures a human perceptual response\nto facial production quality, independent of a behavioral response measurement.\nThe correlation between our proposed Neuroscore and human perceptual judgments\nhas Pearson correlation statistics: $\\mathrm{r}(48) = -0.767, \\mathrm{p} =\n2.089e-10$. We also present the bootstrap result for the correlation i.e.,\n$\\mathrm{p}\\leq 0.0001$. Results show that our Neuroscore is more consistent\nwith human judgment compared to the conventional metrics we evaluated. We\nconclude that neural signals have potential applications for high quality,\nrapid evaluation of GANs in the context of visual image synthesis. \n\n"}
{"id": "1811.04678", "contents": "Title: Towards Adversarial Denoising of Radar Micro-Doppler Signatures Abstract: Generative Adversarial Networks (GANs) are considered the state-of-the-art in\nthe field of image generation. They learn the joint distribution of the\ntraining data and attempt to generate new data samples in high dimensional\nspace following the same distribution as the input. Recent improvements in GANs\nopened the field to many other computer vision applications based on improving\nand changing the characteristics of the input image to follow some given\ntraining requirements. In this paper, we propose a novel technique for the\ndenoising and reconstruction of the micro-Doppler ($\\boldsymbol{\\mu}$-D)\nspectra of walking humans based on GANs. Two sets of experiments were collected\non 22 subjects walking on a treadmill at an intermediate velocity using a\n\\unit[25]{GHz} CW radar. In one set, a clean $\\boldsymbol{\\mu}$-D spectrum is\ncollected for each subject by placing the radar at a close distance to the\nsubject. In the other set, variations are introduced in the experiment setup to\nintroduce different noise and clutter effects on the spectrum by changing the\ndistance and placing reflective objects between the radar and the target.\nSynthetic paired noisy and noise-free spectra were used for training, while\nvalidation was carried out on the real noisy measured data. Finally,\nqualitative and quantitative comparison with other classical radar denoising\napproaches in the literature demonstrated the proposed GANs framework is better\nand more robust to different noise levels. \n\n"}
{"id": "1811.06308", "contents": "Title: A Neurodynamic model of Saliency prediction in V1 Abstract: Lateral connections in the primary visual cortex (V1) have long been\nhypothesized to be responsible of several visual processing mechanisms such as\nbrightness induction, chromatic induction, visual discomfort and bottom-up\nvisual attention (also named saliency). Many computational models have been\ndeveloped to independently predict these and other visual processes, but no\ncomputational model has been able to reproduce all of them simultaneously. In\nthis work we show that a biologically plausible computational model of lateral\ninteractions of V1 is able to simultaneously predict saliency and all the\naforementioned visual processes. Our model's (NSWAM) architecture is based on\nPennachio's neurodynamic model of lateral connections of V1. It is defined as a\nnetwork of firing rate neurons, sensitive to visual features such as\nbrightness, color, orientation and scale. We tested NSWAM saliency predictions\nusing images from several eye tracking datasets. We show that accuracy of\npredictions, using shuffled metrics, obtained by our architecture is similar to\nother state-of-the-art computational methods, particularly with synthetic\nimages (CAT2000-Pattern & SID4VAM) which mainly contain low level features.\nMoreover, we outperform other biologically-inspired saliency models that are\nspecifically designed to exclusively reproduce saliency. Hence, we show that\nour biologically plausible model of lateral connections can simultaneously\nexplain different visual proceses present in V1 (without applying any type of\ntraining or optimization and keeping the same parametrization for all the\nvisual processes). This can be useful for the definition of a unified\narchitecture of the primary visual cortex. \n\n"}
{"id": "1811.06783", "contents": "Title: DropFilter: A Novel Regularization Method for Learning Convolutional\n  Neural Networks Abstract: The past few years have witnessed the fast development of different\nregularization methods for deep learning models such as fully-connected deep\nneural networks (DNNs) and Convolutional Neural Networks (CNNs). Most of\nprevious methods mainly consider to drop features from input data and hidden\nlayers, such as Dropout, Cutout and DropBlocks. DropConnect select to drop\nconnections between fully-connected layers. By randomly discard some features\nor connections, the above mentioned methods control the overfitting problem and\nimprove the performance of neural networks. In this paper, we proposed two\nnovel regularization methods, namely DropFilter and DropFilter-PLUS, for the\nlearning of CNNs. Different from the previous methods, DropFilter and\nDropFilter-PLUS selects to modify the convolution filters. For DropFilter-PLUS,\nwe find a suitable way to accelerate the learning process based on theoretical\nanalysis. Experimental results on MNIST show that using DropFilter and\nDropFilter-PLUS may improve performance on image classification tasks. \n\n"}
{"id": "1811.06930", "contents": "Title: Pre-training Graph Neural Networks with Kernels Abstract: Many machine learning techniques have been proposed in the last few years to\nprocess data represented in graph-structured form. Graphs can be used to model\nseveral scenarios, from molecules and materials to RNA secondary structures.\nSeveral kernel functions have been defined on graphs that coupled with\nkernelized learning algorithms, have shown state-of-the-art performances on\nmany tasks. Recently, several definitions of Neural Networks for Graph (GNNs)\nhave been proposed, but their accuracy is not yet satisfying. In this paper, we\npropose a task-independent pre-training methodology that allows a GNN to learn\nthe representation induced by state-of-the-art graph kernels. Then, the\nsupervised learning phase will fine-tune this representation for the task at\nhand. The proposed technique is agnostic on the adopted GNN architecture and\nkernel function, and shows consistent improvements in the predictive\nperformance of GNNs in our preliminary experimental results. \n\n"}
{"id": "1811.06981", "contents": "Title: Learned Video Compression Abstract: We present a new algorithm for video coding, learned end-to-end for the\nlow-latency mode. In this setting, our approach outperforms all existing video\ncodecs across nearly the entire bitrate range. To our knowledge, this is the\nfirst ML-based method to do so.\n  We evaluate our approach on standard video compression test sets of varying\nresolutions, and benchmark against all mainstream commercial codecs, in the\nlow-latency mode. On standard-definition videos, relative to our algorithm,\nHEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On\nhigh-definition 1080p videos, H.265 and VP9 typically produce codes up to 20%\nlarger, and H.264 up to 35% larger. Furthermore, our approach does not suffer\nfrom blocking artifacts and pixelation, and thus produces videos that are more\nvisually pleasing.\n  We propose two main contributions. The first is a novel architecture for\nvideo compression, which (1) generalizes motion estimation to perform any\nlearned compensation beyond simple translations, (2) rather than strictly\nrelying on previously transmitted reference frames, maintains a state of\narbitrary information learned by the model, and (3) enables jointly compressing\nall transmitted signals (such as optical flow and residual).\n  Secondly, we present a framework for ML-based spatial rate control: namely, a\nmechanism for assigning variable bitrates across space for each frame. This is\na critical component for video coding, which to our knowledge had not been\ndeveloped within a machine learning setting. \n\n"}
{"id": "1811.07126", "contents": "Title: SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated\n  Objects Abstract: Object detection has been a building block in computer vision. Though\nconsiderable progress has been made, there still exist challenges for objects\nwith small size, arbitrary direction, and dense distribution. Apart from\nnatural images, such issues are especially pronounced for aerial images of\ngreat importance. This paper presents a novel multi-category rotation detector\nfor small, cluttered and rotated objects, namely SCRDet. Specifically, a\nsampling fusion network is devised which fuses multi-layer feature with\neffective anchor sampling, to improve the sensitivity to small objects.\nMeanwhile, the supervised pixel attention network and the channel attention\nnetwork are jointly explored for small and cluttered object detection by\nsuppressing the noise and highlighting the objects feature. For more accurate\nrotation estimation, the IoU constant factor is added to the smooth L1 loss to\naddress the boundary problem for the rotating bounding box. Extensive\nexperiments on two remote sensing public datasets DOTA, NWPU VHR-10 as well as\nnatural image datasets COCO, VOC2007 and scene text data ICDAR2015 show the\nstate-of-the-art performance of our detector. The code and models will be\navailable at https://github.com/DetectionTeamUCAS. \n\n"}
{"id": "1811.07460", "contents": "Title: Segregated Temporal Assembly Recurrent Networks for Weakly Supervised\n  Multiple Action Detection Abstract: This paper proposes a segregated temporal assembly recurrent (STAR) network\nfor weakly-supervised multiple action detection. The model learns from\nuntrimmed videos with only supervision of video-level labels and makes\nprediction of intervals of multiple actions. Specifically, we first assemble\nvideo clips according to class labels by an attention mechanism that learns\nclass-variable attention weights and thus helps the noise relieving from\nbackground or other actions. Secondly, we build temporal relationship between\nactions by feeding the assembled features into an enhanced recurrent neural\nnetwork. Finally, we transform the output of recurrent neural network into the\ncorresponding action distribution. In order to generate more precise temporal\nproposals, we design a score term called segregated temporal gradient-weighted\nclass activation mapping (ST-GradCAM) fused with attention weights. Experiments\non THUMOS'14 and ActivityNet1.3 datasets show that our approach outperforms the\nstate-of-the-art weakly-supervised method, and performs at par with the\nfully-supervised counterparts. \n\n"}
{"id": "1811.07465", "contents": "Title: Bayesian Cycle-Consistent Generative Adversarial Networks via\n  Marginalizing Latent Sampling Abstract: Recent techniques built on Generative Adversarial Networks (GANs), such as\nCycle-Consistent GANs, are able to learn mappings among different domains built\nfrom unpaired datasets, through min-max optimization games between generators\nand discriminators. However, it remains challenging to stabilize the training\nprocess and thus cyclic models fall into mode collapse accompanied by the\nsuccess of discriminator. To address this problem, we propose an novel Bayesian\ncyclic model and an integrated cyclic framework for inter-domain mappings. The\nproposed method motivated by Bayesian GAN explores the full posteriors of\ncyclic model via sampling latent variables and optimizes the model with maximum\na posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In\naddition, original CycleGAN cannot generate diversified results. But it is\nfeasible for Bayesian framework to diversify generated images by replacing\nrestricted latent variables in inference process. We evaluate the proposed\nBayesian CycleGAN on multiple benchmark datasets, including Cityscapes, Maps,\nand Monet2photo. The proposed method improve the per-pixel accuracy by 15% for\nthe Cityscapes semantic segmentation task within origin framework and improve\n20% within the proposed integrated framework, showing better resilience to\nimbalance confrontation. The diversified results of Monet2Photo style transfer\nalso demonstrate its superiority over original cyclic model. We provide codes\nfor all of our experiments in https://github.com/ranery/Bayesian-CycleGAN. \n\n"}
{"id": "1811.07498", "contents": "Title: Robust Visual Tracking using Multi-Frame Multi-Feature Joint Modeling Abstract: It remains a huge challenge to design effective and efficient trackers under\ncomplex scenarios, including occlusions, illumination changes and pose\nvariations. To cope with this problem, a promising solution is to integrate the\ntemporal consistency across consecutive frames and multiple feature cues in a\nunified model. Motivated by this idea, we propose a novel correlation\nfilter-based tracker in this work, in which the temporal relatedness is\nreconciled under a multi-task learning framework and the multiple feature cues\nare modeled using a multi-view learning approach. We demonstrate the resulting\nregression model can be efficiently learned by exploiting the structure of\nblockwise diagonal matrix. A fast blockwise diagonal matrix inversion algorithm\nis developed thereafter for efficient online tracking. Meanwhile, we\nincorporate an adaptive scale estimation mechanism to strengthen the stability\nof scale variation tracking. We implement our tracker using two types of\nfeatures and test it on two benchmark datasets. Experimental results\ndemonstrate the superiority of our proposed approach when compared with other\nstate-of-the-art trackers. project homepage\nhttp://bmal.hust.edu.cn/project/KMF2JMTtracking.html \n\n"}
{"id": "1811.08264", "contents": "Title: Transferable Interactiveness Knowledge for Human-Object Interaction\n  Detection Abstract: Human-Object Interaction (HOI) Detection is an important problem to\nunderstand how humans interact with objects. In this paper, we explore\nInteractiveness Knowledge which indicates whether human and object interact\nwith each other or not. We found that interactiveness knowledge can be learned\nacross HOI datasets, regardless of HOI category settings. Our core idea is to\nexploit an Interactiveness Network to learn the general interactiveness\nknowledge from multiple HOI datasets and perform Non-Interaction Suppression\nbefore HOI classification in inference. On account of the generalization of\ninteractiveness, interactiveness network is a transferable knowledge learner\nand can be cooperated with any HOI detection models to achieve desirable\nresults. We extensively evaluate the proposed method on HICO-DET and V-COCO\ndatasets. Our framework outperforms state-of-the-art HOI detection results by a\ngreat margin, verifying its efficacy and flexibility. Code is available at\nhttps://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network. \n\n"}
{"id": "1811.09020", "contents": "Title: Task-generalizable Adversarial Attack based on Perceptual Metric Abstract: Deep neural networks (DNNs) can be easily fooled by adding human\nimperceptible perturbations to the images. These perturbed images are known as\n`adversarial examples' and pose a serious threat to security and safety\ncritical systems. A litmus test for the strength of adversarial examples is\ntheir transferability across different DNN models in a black box setting (i.e.\nwhen the target model's architecture and parameters are not known to attacker).\nCurrent attack algorithms that seek to enhance adversarial transferability work\non the decision level i.e. generate perturbations that alter the network\ndecisions. This leads to two key limitations: (a) An attack is dependent on the\ntask-specific loss function (e.g. softmax cross-entropy for object recognition)\nand therefore does not generalize beyond its original task. (b) The adversarial\nexamples are specific to the network architecture and demonstrate poor\ntransferability to other network architectures. We propose a novel approach to\ncreate adversarial examples that can broadly fool different networks on\nmultiple tasks. Our approach is based on the following intuition: \"Perpetual\nmetrics based on neural network features are highly generalizable and show\nexcellent performance in measuring and stabilizing input distortions. Therefore\nan ideal attack that creates maximum distortions in the network feature space\nshould realize highly transferable examples\". We report extensive experiments\nto show how adversarial examples generalize across multiple networks for\nclassification, object detection and segmentation tasks. \n\n"}
{"id": "1811.09083", "contents": "Title: Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement\n  Learning Abstract: In hierarchical reinforcement learning a major challenge is determining\nappropriate low-level policies. We propose an unsupervised learning scheme,\nbased on asymmetric self-play from Sukhbaatar et al. (2018), that automatically\nlearns a good representation of sub-goals in the environment and a low-level\npolicy that can execute them. A high-level policy can then direct the lower one\nby generating a sequence of continuous sub-goal vectors. We evaluate our model\nusing Mazebase and Mujoco environments, including the challenging AntGather\ntask. Visualizations of the sub-goal embeddings reveal a logical decomposition\nof tasks within the environment. Quantitatively, our approach obtains\ncompelling performance gains over non-hierarchical approaches. \n\n"}
{"id": "1811.09350", "contents": "Title: Predicting Diabetes Disease Evolution Using Financial Records and\n  Recurrent Neural Networks Abstract: Managing patients with chronic diseases is a major and growing healthcare\nchallenge in several countries. A chronic condition, such as diabetes, is an\nillness that lasts a long time and does not go away, and often leads to the\npatient's health gradually getting worse. While recent works involve raw\nelectronic health record (EHR) from hospitals, this work uses only financial\nrecords from health plan providers (medical claims) to predict diabetes disease\nevolution with a self-attentive recurrent neural network. The use of financial\ndata is due to the possibility of being an interface to international\nstandards, as the records standard encodes medical procedures. The main goal\nwas to assess high risk diabetics, so we predict records related to diabetes\nacute complications such as amputations and debridements, revascularization and\nhemodialysis. Our work succeeds to anticipate complications between 60 to 240\ndays with an area under ROC curve ranging from 0.81 to 0.94. In this paper we\ndescribe the first half of a work-in-progress developed within a health plan\nprovider with ROC curve ranging from 0.81 to 0.83. This assessment will give\nhealthcare providers the chance to intervene earlier and head off\nhospitalizations. We are aiming to deliver personalized predictions and\npersonalized recommendations to individual patients, with the goal of improving\noutcomes and reducing costs \n\n"}
{"id": "1811.09539", "contents": "Title: State of the Art in Fair ML: From Moral Philosophy and Legislation to\n  Fair Classifiers Abstract: Machine learning is becoming an ever present part in our lives as many\ndecisions, e.g. to lend a credit, are no longer made by humans but by machine\nlearning algorithms. However those decisions are often unfair and\ndiscriminating individuals belonging to protected groups based on race or\ngender. With the recent General Data Protection Regulation (GDPR) coming into\neffect, new awareness has been raised for such issues and with computer\nscientists having such a large impact on peoples lives it is necessary that\nactions are taken to discover and prevent discrimination. This work aims to\ngive an introduction into discrimination, legislative foundations to counter it\nand strategies to detect and prevent machine learning algorithms from showing\nsuch behavior. \n\n"}
{"id": "1811.09675", "contents": "Title: CNN based dense underwater 3D scene reconstruction by transfer learning\n  using bubble database Abstract: Dense 3D shape acquisition of swimming human or live fish is an important\nresearch topic for sports, biological science and so on. For this purpose,\nactive stereo sensor is usually used in the air, however it cannot be applied\nto the underwater environment because of refraction, strong light attenuation\nand severe interference of bubbles. Passive stereo is a simple solution for\ncapturing dynamic scenes at underwater environment, however the shape with\ntextureless surfaces or irregular reflections cannot be recovered. Recently,\nthe stereo camera pair with a pattern projector for adding artificial textures\non the objects is proposed. However, to use the system for underwater\nenvironment, several problems should be compensated, i.e., disturbance by\nfluctuation and bubbles. Simple solution is to use convolutional neural network\nfor stereo to cancel the effects of bubbles and/or water fluctuation. Since it\nis not easy to train CNN with small size of database with large variation, we\ndevelop a special bubble generation device to efficiently create real bubble\ndatabase of multiple size and density. In addition, we propose a transfer\nlearning technique for multi-scale CNN to effectively remove bubbles and\nprojected-patterns on the object. Further, we develop a real system and\nactually captured live swimming human, which has not been done before.\nExperiments are conducted to show the effectiveness of our method compared with\nthe state of the art techniques. \n\n"}
{"id": "1811.09862", "contents": "Title: On Periodic Functions as Regularizers for Quantization of Neural\n  Networks Abstract: Deep learning models have been successfully used in computer vision and many\nother fields. We propose an unorthodox algorithm for performing quantization of\nthe model parameters. In contrast with popular quantization schemes based on\nthresholds, we use a novel technique based on periodic functions, such as\ncontinuous trigonometric sine or cosine as well as non-continuous hat\nfunctions. We apply these functions component-wise and add the sum over the\nmodel parameters as a regularizer to the model loss during training. The\nfrequency and amplitude hyper-parameters of these functions can be adjusted\nduring training. The regularization pushes the weights into discrete points\nthat can be encoded as integers. We show that using this technique the\nresulting quantized models exhibit the same accuracy as the original ones on\nCIFAR-10 and ImageNet datasets. \n\n"}
{"id": "1811.09897", "contents": "Title: Conditional Recurrent Flow: Conditional Generation of Longitudinal\n  Samples with Applications to Neuroimaging Abstract: Generative models using neural network have opened a door to large-scale\nstudies for various application domains, especially for studies that suffer\nfrom lack of real samples to obtain statistically robust inference. Typically,\nthese generative models would train on existing data to learn the underlying\ndistribution of the measurements (e.g., images) in latent spaces conditioned on\ncovariates (e.g., image labels), and generate independent samples that are\nidentically distributed in the latent space. Such models may work for\ncross-sectional studies, however, they are not suitable to generate data for\nlongitudinal studies that focus on \"progressive\" behavior in a sequence of\ndata. In practice, this is a quite common case in various neuroimaging studies\nwhose goal is to characterize a trajectory of pathologies of a specific disease\neven from early stages. This may be too ambitious especially when the sample\nsize is small (e.g., up to a few hundreds). Motivated from the setup above, we\nseek to develop a conditional generative model for longitudinal data generation\nby designing an invertable neural network. Inspired by recurrent nature of\nlongitudinal data, we propose a novel neural network that incorporates\nrecurrent subnetwork and context gating to include smooth transition in a\nsequence of generated data. Our model is validated on a video sequence dataset\nand a longitudinal AD dataset with various experimental settings for\nqualitative and quantitative evaluations of the generated samples. The results\nwith the AD dataset captures AD specific group differences with sufficiently\ngenerated longitudinal samples that are consistent with existing literature,\nwhich implies a great potential to be applicable to other disease studies. \n\n"}
{"id": "1811.10399", "contents": "Title: A Convolutional Neural Network based Live Object Recognition System as\n  Blind Aid Abstract: This paper introduces a live object recognition system that serves as a blind\naid. Visually impaired people heavily rely on their other senses such as touch\nand auditory signals for understanding the environment around them. The act of\nknowing what object is in front of the blind person without touching it (by\nhand or some other tool) is very difficult. In some cases, the physical contact\nbetween the person and object can be dangerous, and even lethal.\n  This project employs a Convolutional Neural Network for recognition of\npre-trained objects on the ImageNet dataset. A camera, aligned with the\nsystem's predetermined orientation serves as input to the computer system,\nwhich has the object recognition Neural Network deployed to carry out real-time\nobject detection. Output from the network can then be parsed to present to the\nvisually impaired person either in the form of audio or Braille text. \n\n"}
{"id": "1811.10720", "contents": "Title: IGNOR: Image-guided Neural Object Rendering Abstract: We propose a learned image-guided rendering technique that combines the\nbenefits of image-based rendering and GAN-based image synthesis. The goal of\nour method is to generate photo-realistic re-renderings of reconstructed\nobjects for virtual and augmented reality applications (e.g., virtual\nshowrooms, virtual tours \\& sightseeing, the digital inspection of historical\nartifacts). A core component of our work is the handling of view-dependent\neffects. Specifically, we directly train an object-specific deep neural network\nto synthesize the view-dependent appearance of an object. As input data we are\nusing an RGB video of the object. This video is used to reconstruct a proxy\ngeometry of the object via multi-view stereo. Based on this 3D proxy, the\nappearance of a captured view can be warped into a new target view as in\nclassical image-based rendering. This warping assumes diffuse surfaces, in case\nof view-dependent effects, such as specular highlights, it leads to artifacts.\nTo this end, we propose EffectsNet, a deep neural network that predicts\nview-dependent effects. Based on these estimations, we are able to convert\nobserved images to diffuse images. These diffuse images can be projected into\nother views. In the target view, our pipeline reinserts the new view-dependent\neffects. To composite multiple reprojected images to a final output, we learn a\ncomposition network that outputs photo-realistic results. Using this\nimage-guided approach, the network does not have to allocate capacity on\n``remembering'' object appearance, instead it learns how to combine the\nappearance of captured images. We demonstrate the effectiveness of our approach\nboth qualitatively and quantitatively on synthetic as well as on real data. \n\n"}
{"id": "1811.10943", "contents": "Title: Deep Geometric Prior for Surface Reconstruction Abstract: The reconstruction of a discrete surface from a point cloud is a fundamental\ngeometry processing problem that has been studied for decades, with many\nmethods developed. We propose the use of a deep neural network as a geometric\nprior for surface reconstruction. Specifically, we overfit a neural network\nrepresenting a local chart parameterization to part of an input point cloud\nusing the Wasserstein distance as a measure of approximation. By jointly\nfitting many such networks to overlapping parts of the point cloud, while\nenforcing a consistency condition, we compute a manifold atlas. By sampling\nthis atlas, we can produce a dense reconstruction of the surface approximating\nthe input cloud. The entire procedure does not require any training data or\nexplicit regularization, yet, we show that it is able to perform remarkably\nwell: not introducing typical overfitting artifacts, and approximating sharp\nfeatures closely at the same time. We experimentally show that this geometric\nprior produces good results for both man-made objects containing sharp features\nand smoother organic objects, as well as noisy inputs. We compare our method\nwith a number of well-known reconstruction methods on a standard surface\nreconstruction benchmark. \n\n"}
{"id": "1811.11163", "contents": "Title: Class-Distinct and Class-Mutual Image Generation with GANs Abstract: Class-conditional extensions of generative adversarial networks (GANs), such\nas auxiliary classifier GAN (AC-GAN) and conditional GAN (cGAN), have garnered\nattention owing to their ability to decompose representations into class labels\nand other factors and to boost the training stability. However, a limitation is\nthat they assume that each class is separable and ignore the relationship\nbetween classes even though class overlapping frequently occurs in a real-world\nscenario when data are collected on the basis of diverse or ambiguous criteria.\nTo overcome this limitation, we address a novel problem called class-distinct\nand class-mutual image generation, in which the goal is to construct a\ngenerator that can capture between-class relationships and generate an image\nselectively conditioned on the class specificity. To solve this problem without\nadditional supervision, we propose classifier's posterior GAN (CP-GAN), in\nwhich we redesign the generator input and the objective function of AC-GAN for\nclass-overlapping data. Precisely, we incorporate the classifier's posterior\ninto the generator input and optimize the generator so that the classifier's\nposterior of generated data corresponds with that of real data. We demonstrate\nthe effectiveness of CP-GAN using both controlled and real-world\nclass-overlapping data with a model configuration analysis and comparative\nstudy. Our code is available at https://github.com/takuhirok/CP-GAN/. \n\n"}
{"id": "1811.11187", "contents": "Title: Scan2CAD: Learning CAD Model Alignment in RGB-D Scans Abstract: We present Scan2CAD, a novel data-driven method that learns to align clean 3D\nCAD models from a shape database to the noisy and incomplete geometry of a\ncommodity RGB-D scan. For a 3D reconstruction of an indoor scene, our method\ntakes as input a set of CAD models, and predicts a 9DoF pose that aligns each\nmodel to the underlying scan geometry. To tackle this problem, we create a new\nscan-to-CAD alignment dataset based on 1506 ScanNet scans with 97607 annotated\nkeypoint pairs between 14225 CAD models from ShapeNet and their counterpart\nobjects in the scans. Our method selects a set of representative keypoints in a\n3D scan for which we find correspondences to the CAD geometry. To this end, we\ndesign a novel 3D CNN architecture that learns a joint embedding between real\nand synthetic objects, and from this predicts a correspondence heatmap. Based\non these correspondence heatmaps, we formulate a variational energy\nminimization that aligns a given set of CAD models to the reconstruction. We\nevaluate our approach on our newly introduced Scan2CAD benchmark where we\noutperform both handcrafted feature descriptor as well as state-of-the-art CNN\nbased methods by 21.39%. \n\n"}
{"id": "1811.11239", "contents": "Title: A Compositional Textual Model for Recognition of Imperfect Word Images Abstract: Printed text recognition is an important problem for industrial OCR systems.\nPrinted text is constructed in a standard procedural fashion in most settings.\nWe develop a mathematical model for this process that can be applied to the\nbackward inference problem of text recognition from an image. Through ablation\nexperiments we show that this model is realistic and that a multi-task\nobjective setting can help to stabilize estimation of its free parameters,\nenabling use of conventional deep learning methods. Furthermore, by directly\nmodeling the geometric perturbations of text synthesis we show that our model\ncan help recover missing characters from incomplete text regions, the bane of\nmulticomponent OCR systems, enabling recognition even when the detection\nreturns incomplete information. \n\n"}
{"id": "1811.11304", "contents": "Title: Universal Adversarial Training Abstract: Standard adversarial attacks change the predicted class label of a selected\nimage by adding specially tailored small perturbations to its pixels. In\ncontrast, a universal perturbation is an update that can be added to any image\nin a broad class of images, while still changing the predicted class label. We\nstudy the efficient generation of universal adversarial perturbations, and also\nefficient methods for hardening networks to these attacks. We propose a simple\noptimization-based universal attack that reduces the top-1 accuracy of various\nnetwork architectures on ImageNet to less than 20%, while learning the\nuniversal perturbation 13X faster than the standard method.\n  To defend against these perturbations, we propose universal adversarial\ntraining, which models the problem of robust classifier generation as a\ntwo-player min-max game, and produces robust models with only 2X the cost of\nnatural training. We also propose a simultaneous stochastic gradient method\nthat is almost free of extra computation, which allows us to do universal\nadversarial training on ImageNet. \n\n"}
{"id": "1811.11455", "contents": "Title: CrowdCam: Dynamic Region Segmentation Abstract: We consider the problem of segmenting dynamic regions in CrowdCam images,\nwhere a dynamic region is the projection of a moving 3D object on the image\nplane. Quite often, these regions are the most interesting parts of an image.\nCrowdCam images is a set of images of the same dynamic event, captured by a\ngroup of non-collaborating users. Almost every event of interest today is\ncaptured this way. This new type of images raises the need to develop new\nalgorithms tailored specifically for it. We propose a comprehensive solution to\nthe problem. Our solution combines cues that are based on geometry, appearance\nand proximity. First, geometric reasoning is used to produce rough score maps\nthat determine, for every pixel, how likely it is to be the projection of a\nstatic or dynamic scene point. These maps are noisy because CrowdCam images are\nusually few and far apart both in space and in time. Then, we use similarity in\nappearance space and proximity in the image plane to encourage neighboring\npixels to be labeled similarly as either static or dynamic. We collected a new,\nand challenging, data set to evaluate our algorithm. Results show that the\nsuccess score of our algorithm is nearly double that of the current state of\nthe art approach. \n\n"}
{"id": "1811.12297", "contents": "Title: Incremental Scene Synthesis Abstract: We present a method to incrementally generate complete 2D or 3D scenes with\nthe following properties: (a) it is globally consistent at each step according\nto a learned scene prior, (b) real observations of a scene can be incorporated\nwhile observing global consistency, (c) unobserved regions can be hallucinated\nlocally in consistence with previous observations, hallucinations and global\npriors, and (d) hallucinations are statistical in nature, i.e., different\nscenes can be generated from the same observations. To achieve this, we model\nthe virtual scene, where an active agent at each step can either perceive an\nobserved part of the scene or generate a local hallucination. The latter can be\ninterpreted as the agent's expectation at this step through the scene and can\nbe applied to autonomous navigation. In the limit of observing real data at\neach point, our method converges to solving the SLAM problem. It can otherwise\nsample entirely imagined scenes from prior distributions. Besides autonomous\nagents, applications include problems where large data is required for building\nrobust real-world applications, but few samples are available. We demonstrate\nefficacy on various 2D as well as 3D data. \n\n"}
{"id": "1811.12596", "contents": "Title: Parsing R-CNN for Instance-Level Human Analysis Abstract: Instance-level human analysis is common in real-life scenarios and has\nmultiple manifestations, such as human part segmentation, dense pose\nestimation, human-object interactions, etc. Models need to distinguish\ndifferent human instances in the image panel and learn rich features to\nrepresent the details of each instance. In this paper, we present an end-to-end\npipeline for solving the instance-level human analysis, named Parsing R-CNN. It\nprocesses a set of human instances simultaneously through comprehensive\nconsidering the characteristics of region-based approach and the appearance of\na human, thus allowing representing the details of instances. Parsing R-CNN is\nvery flexible and efficient, which is applicable to many issues in human\ninstance analysis. Our approach outperforms all state-of-the-art methods on\nCIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and\nDensePose-COCO datasets. Based on the proposed Parsing R-CNN, we reach the 1st\nplace in the COCO 2018 Challenge DensePose Estimation task. Code and models are\npublic available. \n\n"}
{"id": "1811.12758", "contents": "Title: Non-Local Video Denoising by CNN Abstract: Non-local patch based methods were until recently state-of-the-art for image\ndenoising but are now outperformed by CNNs. Yet they are still the\nstate-of-the-art for video denoising, as video redundancy is a key factor to\nattain high denoising performance. The problem is that CNN architectures are\nhardly compatible with the search for self-similarities. In this work we\npropose a new and efficient way to feed video self-similarities to a CNN. The\nnon-locality is incorporated into the network via a first non-trainable layer\nwhich finds for each patch in the input image its most similar patches in a\nsearch region. The central values of these patches are then gathered in a\nfeature vector which is assigned to each image pixel. This information is\npresented to a CNN which is trained to predict the clean image. We apply the\nproposed architecture to image and video denoising. For the latter patches are\nsearched for in a 3D spatio-temporal volume. The proposed architecture achieves\nstate-of-the-art results. To the best of our knowledge, this is the first\nsuccessful application of a CNN to video denoising. \n\n"}
{"id": "1812.00020", "contents": "Title: TextureNet: Consistent Local Parametrizations for Learning from\n  High-Resolution Signals on Meshes Abstract: We introduce, TextureNet, a neural network architecture designed to extract\nfeatures from high-resolution signals associated with 3D surface meshes (e.g.,\ncolor texture maps). The key idea is to utilize a 4-rotational symmetric\n(4-RoSy) field to define a domain for convolution on a surface. Though 4-RoSy\nfields have several properties favorable for convolution on surfaces (low\ndistortion, few singularities, consistent parameterization, etc.), orientations\nare ambiguous up to 4-fold rotation at any sample point. So, we introduce a new\nconvolutional operator invariant to the 4-RoSy ambiguity and use it in a\nnetwork to extract features from high-resolution signals on geodesic\nneighborhoods of a surface. In comparison to alternatives, such as PointNet\nbased methods which lack a notion of orientation, the coherent structure given\nby these neighborhoods results in significantly stronger features. As an\nexample application, we demonstrate the benefits of our architecture for 3D\nsemantic segmentation of textured 3D meshes. The results show that our method\noutperforms all existing methods on the basis of mean IoU by a significant\nmargin in both geometry-only (6.4%) and RGB+Geometry (6.9-8.2%) settings. \n\n"}
{"id": "1812.00202", "contents": "Title: Traversing the Continuous Spectrum of Image Retrieval with Deep Dynamic\n  Models Abstract: We introduce the first work to tackle the image retrieval problem as a\ncontinuous operation. While the proposed approaches in the literature can be\nroughly categorized into two main groups: category- and instance-based\nretrieval, in this work we show that the retrieval task is much richer and more\ncomplex. Image similarity goes beyond this discrete vantage point and spans a\ncontinuous spectrum among the classical operating points of category and\ninstance similarity. However, current retrieval models are static and incapable\nof exploring this rich structure of the retrieval space since they are trained\nand evaluated with a single operating point as a target objective. Hence, we\nintroduce a novel retrieval model that for a given query is capable of\nproducing a dynamic embedding that can target an arbitrary point along the\ncontinuous retrieval spectrum. Our model disentangles the visual signal of a\nquery image into its basic components of categorical and attribute information.\nFurthermore, using a continuous control parameter our model learns to\nreconstruct a dynamic embedding of the query by mixing these components with\ndifferent proportions to target a specific point along the retrieval simplex.\nWe demonstrate our idea in a comprehensive evaluation of the proposed model and\nhighlight the advantages of our approach against a set of well-established\ndiscrete retrieval models. \n\n"}
{"id": "1812.00303", "contents": "Title: Multi-modal Capsule Routing for Actor and Action Video Segmentation\n  Conditioned on Natural Language Queries Abstract: In this paper, we propose an end-to-end capsule network for pixel level\nlocalization of actors and actions present in a video. The localization is\nperformed based on a natural language query through which an actor and action\nare specified. We propose to encode both the video as well as textual input in\nthe form of capsules, which provide more effective representation in comparison\nwith standard convolution based features. We introduce a novel capsule based\nattention mechanism for fusion of video and text capsules for text selected\nvideo segmentation. The attention mechanism is performed via joint EM routing\nover video and text capsules for text selected actor and action localization.\nThe existing works on actor-action localization are mainly focused on\nlocalization in a single frame instead of the full video. Different from\nexisting works, we propose to perform the localization on all frames of the\nvideo. To validate the potential of the proposed network for actor and action\nlocalization on all the frames of a video, we extend an existing actor-action\ndataset (A2D) with annotations for all the frames. The experimental evaluation\ndemonstrates the effectiveness of the proposed capsule network for text\nselective actor and action localization in videos, and it also improves upon\nthe performance of the existing state-of-the art works on single frame-based\nlocalization. \n\n"}
{"id": "1812.00547", "contents": "Title: Semi-supervised Rare Disease Detection Using Generative Adversarial\n  Network Abstract: Rare diseases affect a relatively small number of people, which limits\ninvestment in research for treatments and cures. Developing an efficient method\nfor rare disease detection is a crucial first step towards subsequent clinical\nresearch. In this paper, we present a semi-supervised learning framework for\nrare disease detection using generative adversarial networks. Our method takes\nadvantage of the large amount of unlabeled data for disease detection and\nachieves the best results in terms of precision-recall score compared to\nbaseline techniques. \n\n"}
{"id": "1812.00555", "contents": "Title: SUSAN: Segment Unannotated image Structure using Adversarial Network Abstract: Segmentation of magnetic resonance (MR) images is a fundamental step in many\nmedical imaging-based applications. The recent implementation of deep\nconvolutional neural networks (CNNs) in image processing has been shown to have\nsignificant impacts on medical image segmentation. Network training of\nsegmentation CNNs typically requires images and paired annotation data\nrepresenting pixel-wise tissue labels referred to as masks. However, the\nsupervised training of highly efficient CNNs with deeper structure and more\nnetwork parameters requires a large number of training images and paired tissue\nmasks. Thus, there is great need to develop a generalized CNN-based\nsegmentation method which would be applicable for a wide variety of MR image\ndatasets with different tissue contrasts. The purpose of this study was to\ndevelop and evaluate a generalized CNN-based method for fully-automated\nsegmentation of different MR image datasets using a single set of annotated\ntraining data. A technique called cycle-consistent generative adversarial\nnetwork (CycleGAN) is applied as the core of the proposed method to perform\nimage-to-image translation between MR image datasets with different tissue\ncontrasts. A joint segmentation network is incorporated into the adversarial\nnetwork to obtain additional segmentation functionality. The proposed method\nwas evaluated for segmenting bone and cartilage on two clinical knee MR image\ndatasets acquired at our institution using only a single set of annotated data\nfrom a publicly available knee MR image dataset. The new technique may further\nimprove the applicability and efficiency of CNN-based segmentation of medical\nimages while eliminating the need for large amounts of annotated training data. \n\n"}
{"id": "1812.00879", "contents": "Title: Image-based model parameter optimization using Model-Assisted Generative\n  Adversarial Networks Abstract: We propose and demonstrate the use of a model-assisted generative adversarial\nnetwork (GAN) to produce fake images that accurately match true images through\nthe variation of the parameters of the model that describes the features of the\nimages. The generator learns the model parameter values that produce fake\nimages that best match the true images. Two case studies show excellent\nagreement between the generated best match parameters and the true parameters.\nThe best match model parameter values can be used to retune the default\nsimulation to minimize any bias when applying image recognition techniques to\nfake and true images. In the case of a real-world experiment, the true images\nare experimental data with unknown true model parameter values, and the fake\nimages are produced by a simulation that takes the model parameters as input.\nThe model-assisted GAN uses a convolutional neural network to emulate the\nsimulation for all parameter values that, when trained, can be used as a\nconditional generator for fast fake-image production. \n\n"}
{"id": "1812.01681", "contents": "Title: Deep Bayesian Self-Training Abstract: Supervised Deep Learning has been highly successful in recent years,\nachieving state-of-the-art results in most tasks. However, with the ongoing\nuptake of such methods in industrial applications, the requirement for large\namounts of annotated data is often a challenge. In most real world problems,\nmanual annotation is practically intractable due to time/labour constraints,\nthus the development of automated and adaptive data annotation systems is\nhighly sought after. In this paper, we propose both a (i) Deep Bayesian\nSelf-Training methodology for automatic data annotation, by leveraging\npredictive uncertainty estimates using variational inference and modern Neural\nNetwork architectures, as well as (ii) a practical adaptation procedure for\nhandling high label variability between different dataset distributions through\nclustering of Neural Network latent variable representations. An experimental\nstudy on both public and private datasets is presented illustrating the\nsuperior performance of the proposed approach over standard Self-Training\nbaselines, highlighting the importance of predictive uncertainty estimates in\nsafety-critical domains. \n\n"}
{"id": "1812.01687", "contents": "Title: PointCloud Saliency Maps Abstract: 3D point-cloud recognition with PointNet and its variants has received\nremarkable progress. A missing ingredient, however, is the ability to\nautomatically evaluate point-wise importance w.r.t.\\! classification\nperformance, which is usually reflected by a saliency map. A saliency map is an\nimportant tool as it allows one to perform further processes on point-cloud\ndata. In this paper, we propose a novel way of characterizing critical points\nand segments to build point-cloud saliency maps. Our method assigns each point\na score reflecting its contribution to the model-recognition loss. The saliency\nmap explicitly explains which points are the key for model recognition.\nFurthermore, aggregations of highly-scored points indicate important\nsegments/subsets in a point-cloud. Our motivation for constructing a saliency\nmap is by point dropping, which is a non-differentiable operator. To overcome\nthis issue, we approximate point-dropping with a differentiable procedure of\nshifting points towards the cloud centroid. Consequently, each saliency score\ncan be efficiently measured by the corresponding gradient of the loss w.r.t the\npoint under the spherical coordinates. Extensive evaluations on several\nstate-of-the-art point-cloud recognition models, including PointNet, PointNet++\nand DGCNN, demonstrate the veracity and generality of our proposed saliency\nmap. Code for experiments is released on\n\\url{https://github.com/tianzheng4/PointCloud-Saliency-Maps}. \n\n"}
{"id": "1812.01699", "contents": "Title: Assigning a Grade: Accurate Measurement of Road Quality Using Satellite\n  Imagery Abstract: Roads are critically important infrastructure to societal and economic\ndevelopment, with huge investments made by governments every year. However,\nmethods for monitoring those investments tend to be time-consuming, laborious,\nand expensive, placing them out of reach for many developing regions. In this\nwork, we develop a model for monitoring the quality of road infrastructure\nusing satellite imagery. For this task, we harness two trends: the increasing\navailability of high-resolution, often-updated satellite imagery, and the\nenormous improvement in speed and accuracy of convolutional neural\nnetwork-based methods for performing computer vision tasks. We employ a unique\ndataset of road quality information on 7000km of roads in Kenya combined with\n50cm resolution satellite imagery. We create models for a binary classification\ntask as well as a comprehensive 5-category classification task, with accuracy\nscores of 88 and 73 percent respectively. We also provide evidence of the\nrobustness of our methods with challenging held-out scenarios, though we note\nsome improvement is still required for confident analysis of a never before\nseen road. We believe these results are well-positioned to have substantial\nimpact on a broad set of transport applications. \n\n"}
{"id": "1812.01718", "contents": "Title: Deep Learning for Classical Japanese Literature Abstract: Much of machine learning research focuses on producing models which perform\nwell on benchmark tasks, in turn improving our understanding of the challenges\nassociated with those tasks. From the perspective of ML researchers, the\ncontent of the task itself is largely irrelevant, and thus there have\nincreasingly been calls for benchmark tasks to more heavily focus on problems\nwhich are of social or cultural relevance. In this work, we introduce\nKuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as\nwell as two larger, more challenging datasets, Kuzushiji-49 and\nKuzushiji-Kanji. Through these datasets, we wish to engage the machine learning\ncommunity into the world of classical Japanese literature. Dataset available at\nhttps://github.com/rois-codh/kmnist \n\n"}
{"id": "1812.01756", "contents": "Title: Multi$^{\\mathbf{3}}$Net: Segmenting Flooded Buildings via Fusion of\n  Multiresolution, Multisensor, and Multitemporal Satellite Imagery Abstract: We propose a novel approach for rapid segmentation of flooded buildings by\nfusing multiresolution, multisensor, and multitemporal satellite imagery in a\nconvolutional neural network. Our model significantly expedites the generation\nof satellite imagery-based flood maps, crucial for first responders and local\nauthorities in the early stages of flood events. By incorporating multitemporal\nsatellite imagery, our model allows for rapid and accurate post-disaster damage\nassessment and can be used by governments to better coordinate medium- and\nlong-term financial assistance programs for affected areas. The network\nconsists of multiple streams of encoder-decoder architectures that extract\nspatiotemporal information from medium-resolution images and spatial\ninformation from high-resolution images before fusing the resulting\nrepresentations into a single medium-resolution segmentation map of flooded\nbuildings. We compare our model to state-of-the-art methods for building\nfootprint segmentation as well as to alternative fusion approaches for the\nsegmentation of flooded buildings and find that our model performs best on both\ntasks. We also demonstrate that our model produces highly accurate segmentation\nmaps of flooded buildings using only publicly available medium-resolution data\ninstead of significantly more detailed but sparsely available very\nhigh-resolution data. We release the first open-source dataset of fully\npreprocessed and labeled multiresolution, multispectral, and multitemporal\nsatellite images of disaster sites along with our source code. \n\n"}
{"id": "1812.02303", "contents": "Title: Neural Abstractive Text Summarization with Sequence-to-Sequence Models Abstract: In the past few years, neural abstractive text summarization with\nsequence-to-sequence (seq2seq) models have gained a lot of popularity. Many\ninteresting techniques have been proposed to improve seq2seq models, making\nthem capable of handling different challenges, such as saliency, fluency and\nhuman readability, and generate high-quality summaries. Generally speaking,\nmost of these techniques differ in one of these three categories: network\nstructure, parameter inference, and decoding/generation. There are also other\nconcerns, such as efficiency and parallelism for training a model. In this\npaper, we provide a comprehensive literature survey on different seq2seq models\nfor abstractive text summarization from the viewpoint of network structures,\ntraining strategies, and summary generation algorithms. Several models were\nfirst proposed for language modeling and generation tasks, such as machine\ntranslation, and later applied to abstractive text summarization. Hence, we\nalso provide a brief review of these models. As part of this survey, we also\ndevelop an open source library, namely, Neural Abstractive Text Summarizer\n(NATS) toolkit, for the abstractive text summarization. An extensive set of\nexperiments have been conducted on the widely used CNN/Daily Mail dataset to\nexamine the effectiveness of several different neural network components.\nFinally, we benchmark two models implemented in NATS on the two recently\nreleased datasets, namely, Newsroom and Bytecup. \n\n"}
{"id": "1812.02405", "contents": "Title: Web Applicable Computer-aided Diagnosis of Glaucoma Using Deep Learning Abstract: Glaucoma is a major eye disease, leading to vision loss in the absence of\nproper medical treatment. Current diagnosis of glaucoma is performed by\nophthalmologists who are often analyzing several types of medical images\ngenerated by different types of medical equipment. Capturing and analyzing\nthese medical images is labor-intensive and expensive. In this paper, we\npresent a novel computational approach towards glaucoma diagnosis and\nlocalization, only making use of eye fundus images that are analyzed by\nstate-of-the-art deep learning techniques. Specifically, our approach leverages\nConvolutional Neural Networks (CNNs) and Gradient-weighted Class Activation\nMapping (Grad-CAM) for glaucoma diagnosis and localization, respectively.\nQuantitative and qualitative results, as obtained for a small-sized dataset\nwith no segmentation ground truth, demonstrate that the proposed approach is\npromising, for instance achieving an accuracy of 0.91$\\pm0.02$ and an ROC-AUC\nscore of 0.94 for the diagnosis task. Furthermore, we present a publicly\navailable prototype web application that integrates our predictive model, with\nthe goal of making effective glaucoma diagnosis available to a wide audience. \n\n"}
{"id": "1812.02427", "contents": "Title: Segmentation of Head and Neck Organs at Risk Using CNN with Batch Dice\n  Loss Abstract: This paper deals with segmentation of organs at risk (OAR) in head and neck\narea in CT images which is a crucial step for reliable intensity modulated\nradiotherapy treatment. We introduce a convolution neural network with\nencoder-decoder architecture and a new loss function, the batch soft Dice loss\nfunction, used to train the network. The resulting model produces segmentations\nof every OAR in the public MICCAI 2015 Head And Neck Auto-Segmentation\nChallenge dataset. Despite the heavy class imbalance in the data, we improve\naccuracy of current state-of-the-art methods by 0.33 mm in terms of average\nsurface distance and by 0.11 in terms of Dice overlap coefficient on average. \n\n"}
{"id": "1812.02771", "contents": "Title: Neural Word Search in Historical Manuscript Collections Abstract: We address the problem of segmenting and retrieving word images in\ncollections of historical manuscripts given a text query. This is commonly\nreferred to as \"word spotting\". To this end, we first propose an end-to-end\ntrainable model based on deep neural networks that we dub Ctrl-F-Net. The model\nsimultaneously generates region proposals and embeds them into a word embedding\nspace, wherein a search is performed. We further introduce a simplified version\ncalled Ctrl-F-Mini. It is faster with similar performance, though it is limited\nto more easily segmented manuscripts. We evaluate both models on common\nbenchmark datasets and surpass the previous state of the art. Finally, in\ncollaboration with historians, we employ the Ctrl-F-Net to search within a\nlarge manuscript collection of over 100 thousand pages, written across two\ncenturies. With only 11 training pages, we enable large scale data collection\nin manuscript-based historical research. This results in a speed up of data\ncollection and the number of manuscripts processed by orders of magnitude.\nGiven the time consuming manual work required to study old manuscripts in the\nhumanities, quick and robust tools for word spotting has the potential to\nrevolutionise domains like history, religion and language. \n\n"}
{"id": "1812.02967", "contents": "Title: Scale-aware multi-level guidance for interactive instance segmentation Abstract: In interactive instance segmentation, users give feedback to iteratively\nrefine segmentation masks. The user-provided clicks are transformed into\nguidance maps which provide the network with necessary cues on the whereabouts\nof the object of interest. Guidance maps used in current systems are purely\ndistance-based and are either too localized or non-informative. We propose a\nnovel transformation of user clicks to generate scale-aware guidance maps that\nleverage the hierarchical structural information present in an image. Using our\nguidance maps, even the most basic FCNs are able to outperform existing\napproaches that require state-of-the-art segmentation networks pre-trained on\nlarge scale segmentation datasets. We demonstrate the effectiveness of our\nproposed transformation strategy through comprehensive experimentation in which\nwe significantly raise state-of-the-art on four standard interactive\nsegmentation benchmarks. \n\n"}
{"id": "1812.03170", "contents": "Title: Variational Saccading: Efficient Inference for Large Resolution Images Abstract: Image classification with deep neural networks is typically restricted to\nimages of small dimensionality such as 224 x 244 in Resnet models [24]. This\nlimitation excludes the 4000 x 3000 dimensional images that are taken by modern\nsmartphone cameras and smart devices. In this work, we aim to mitigate the\nprohibitive inferential and memory costs of operating in such large dimensional\nspaces. To sample from the high-resolution original input distribution, we\npropose using a smaller proxy distribution to learn the co-ordinates that\ncorrespond to regions of interest in the high-dimensional space. We introduce a\nnew principled variational lower bound that captures the relationship of the\nproxy distribution's posterior and the original image's co-ordinate space in a\nway that maximizes the conditional classification likelihood. We empirically\ndemonstrate on one synthetic benchmark and one real world large resolution DSLR\ncamera image dataset that our method produces comparable results with ~10x\nfaster inference and lower memory consumption than a model that utilizes the\nentire original input distribution. Finally, we experiment with a more complex\nsetting using mini-maps from Starcraft II [56] to infer the number of\ncharacters in a complex 3d-rendered scene. Even in such complicated scenes our\nmodel provides strong localization: a feature missing from traditional\nclassification models. \n\n"}
{"id": "1812.03190", "contents": "Title: Deep-RBF Networks Revisited: Robust Classification with Rejection Abstract: One of the main drawbacks of deep neural networks, like many other\nclassifiers, is their vulnerability to adversarial attacks. An important reason\nfor their vulnerability is assigning high confidence to regions with few or\neven no feature points. By feature points, we mean a nonlinear transformation\nof the input space extracting a meaningful representation of the input data. On\nthe other hand, deep-RBF networks assign high confidence only to the regions\ncontaining enough feature points, but they have been discounted due to the\nwidely-held belief that they have the vanishing gradient problem. In this\npaper, we revisit the deep-RBF networks by first giving a general formulation\nfor them, and then proposing a family of cost functions thereof inspired by\nmetric learning. In the proposed deep-RBF learning algorithm, the vanishing\ngradient problem does not occur. We make these networks robust to adversarial\nattack by adding the reject option to their output layer. Through several\nexperiments on the MNIST dataset, we demonstrate that our proposed method not\nonly achieves significant classification accuracy but is also very resistant to\nvarious adversarial attacks. \n\n"}
{"id": "1812.03264", "contents": "Title: Neural Abstract Style Transfer for Chinese Traditional Painting Abstract: Chinese traditional painting is one of the most historical artworks in the\nworld. It is very popular in Eastern and Southeast Asia due to being\naesthetically appealing. Compared with western artistic painting, it is usually\nmore visually abstract and textureless. Recently, neural network based style\ntransfer methods have shown promising and appealing results which are mainly\nfocused on western painting. It remains a challenging problem to preserve\nabstraction in neural style transfer. In this paper, we present a Neural\nAbstract Style Transfer method for Chinese traditional painting. It learns to\npreserve abstraction and other style jointly end-to-end via a novel\nMXDoG-guided filter (Modified version of the eXtended Difference-of-Gaussians)\nand three fully differentiable loss terms. To the best of our knowledge, there\nis little work study on neural style transfer of Chinese traditional painting.\nTo promote research on this direction, we collect a new dataset with diverse\nphoto-realistic images and Chinese traditional paintings. In experiments, the\nproposed method shows more appealing stylized results in transferring the style\nof Chinese traditional painting than state-of-the-art neural style transfer\nmethods. \n\n"}
{"id": "1812.03889", "contents": "Title: Regularization by architecture: A deep prior approach for inverse\n  problems Abstract: The present paper studies so-called deep image prior (DIP) techniques in the\ncontext of ill-posed inverse problems. DIP networks have been recently\nintroduced for applications in image processing; also first experimental\nresults for applying DIP to inverse problems have been reported. This paper\naims at discussing different interpretations of DIP and to obtain analytic\nresults for specific network designs and linear operators. The main\ncontribution is to introduce the idea of viewing these approaches as the\noptimization of Tikhonov functionals rather than optimizing networks. Besides\ntheoretical results, we present numerical verifications. \n\n"}
{"id": "1812.03928", "contents": "Title: Learning Representations of Sets through Optimized Permutations Abstract: Representations of sets are challenging to learn because operations on sets\nshould be permutation-invariant. To this end, we propose a\nPermutation-Optimisation module that learns how to permute a set end-to-end.\nThe permuted set can be further processed to learn a permutation-invariant\nrepresentation of that set, avoiding a bottleneck in traditional set models. We\ndemonstrate our model's ability to learn permutations and set representations\nwith either explicit or implicit supervision on four datasets, on which we\nachieve state-of-the-art results: number sorting, image mosaics, classification\nfrom image mosaics, and visual question answering. \n\n"}
{"id": "1812.03953", "contents": "Title: An Intelligent Safety System for Human-Centered Semi-Autonomous Vehicles Abstract: Nowadays, automobile manufacturers make efforts to develop ways to make cars\nfully safe. Monitoring driver's actions by computer vision techniques to detect\ndriving mistakes in real-time and then planning for autonomous driving to avoid\nvehicle collisions is one of the most important issues that has been\ninvestigated in the machine vision and Intelligent Transportation Systems\n(ITS). The main goal of this study is to prevent accidents caused by fatigue,\ndrowsiness, and driver distraction. To avoid these incidents, this paper\nproposes an integrated safety system that continuously monitors the driver's\nattention and vehicle surroundings, and finally decides whether the actual\nsteering control status is safe or not. For this purpose, we equipped an\nordinary car called FARAZ with a vision system consisting of four mounted\ncameras along with a universal car tool for communicating with surrounding\nfactory-installed sensors and other car systems, and sending commands to\nactuators. The proposed system leverages a scene understanding pipeline using\ndeep convolutional encoder-decoder networks and a driver state detection\npipeline. We have been identifying and assessing domestic capabilities for the\ndevelopment of technologies specifically of the ordinary vehicles in order to\nmanufacture smart cars and eke providing an intelligent system to increase\nsafety and to assist the driver in various conditions/situations. \n\n"}
{"id": "1812.04204", "contents": "Title: 2.5D Visual Sound Abstract: Binaural audio provides a listener with 3D sound sensation, allowing a rich\nperceptual experience of the scene. However, binaural recordings are scarcely\navailable and require nontrivial expertise and equipment to obtain. We propose\nto convert common monaural audio into binaural audio by leveraging video. The\nkey idea is that visual frames reveal significant spatial cues that, while\nexplicitly lacking in the accompanying single-channel audio, are strongly\nlinked to it. Our multi-modal approach recovers this link from unlabeled video.\nWe devise a deep convolutional neural network that learns to decode the\nmonaural (single-channel) soundtrack into its binaural counterpart by injecting\nvisual information about object and scene configurations. We call the resulting\noutput 2.5D visual sound---the visual stream helps \"lift\" the flat single\nchannel audio into spatialized sound. In addition to sound generation, we show\nthe self-supervised representation learned by our network benefits audio-visual\nsource separation. Our video results:\nhttp://vision.cs.utexas.edu/projects/2.5D_visual_sound/ \n\n"}
{"id": "1812.04751", "contents": "Title: Considering Race a Problem of Transfer Learning Abstract: As biometric applications are fielded to serve large population groups,\nissues of performance differences between individual sub-groups are becoming\nincreasingly important. In this paper we examine cases where we believe race is\none such factor. We look in particular at two forms of problem; facial\nclassification and image synthesis. We take the novel approach of considering\nrace as a boundary for transfer learning in both the task (facial\nclassification) and the domain (synthesis over distinct datasets). We\ndemonstrate a series of techniques to improve transfer learning of facial\nclassification; outperforming similar models trained in the target's own\ndomain. We conduct a study to evaluate the performance drop of Generative\nAdversarial Networks trained to conduct image synthesis, in this process, we\nproduce a new annotation for the Celeb-A dataset by race. These networks are\ntrained solely on one race and tested on another - demonstrating the subsets of\nthe CelebA to be distinct domains for this task. \n\n"}
{"id": "1812.05040", "contents": "Title: Learning Semantic Segmentation from Synthetic Data: A Geometrically\n  Guided Input-Output Adaptation Approach Abstract: Recently, increasing attention has been drawn to training semantic\nsegmentation models using synthetic data and computer-generated annotation.\nHowever, domain gap remains a major barrier and prevents models learned from\nsynthetic data from generalizing well to real-world applications. In this work,\nwe take the advantage of additional geometric information from synthetic data,\na powerful yet largely neglected cue, to bridge the domain gap. Such geometric\ninformation can be generated easily from synthetic data, and is proven to be\nclosely coupled with semantic information. With the geometric information, we\npropose a model to reduce domain shift on two levels: on the input level, we\naugment the traditional image translation network with the additional geometric\ninformation to translate synthetic images into realistic styles; on the output\nlevel, we build a task network which simultaneously performs depth estimation\nand semantic segmentation on the synthetic data. Meanwhile, we encourage the\nnetwork to preserve correlation between depth and semantics by adversarial\ntraining on the output space. We then validate our method on two pairs of\nsynthetic to real dataset: Virtual KITTI to KITTI, and SYNTHIA to Cityscapes,\nwhere we achieve a significant performance gain compared to the non-adapt\nbaseline and methods using only semantic label. This demonstrates the\nusefulness of geometric information from synthetic data for cross-domain\nsemantic segmentation. \n\n"}
{"id": "1812.05082", "contents": "Title: Features Extraction Based on an Origami Representation of 3D Landmarks Abstract: Feature extraction analysis has been widely investigated during the last\ndecades in computer vision community due to the large range of possible\napplications. Significant work has been done in order to improve the\nperformance of the emotion detection methods. Classification algorithms have\nbeen refined, novel preprocessing techniques have been applied and novel\nrepresentations from images and videos have been introduced. In this paper, we\npropose a preprocessing method and a novel facial landmarks' representation\naiming to improve the facial emotion detection accuracy. We apply our novel\nmethodology on the extended Cohn-Kanade (CK+) dataset and other datasets for\naffect classification based on Action Units (AU). The performance evaluation\ndemonstrates an improvement on facial emotion classification (accuracy and F1\nscore) that indicates the superiority of the proposed methodology. \n\n"}
{"id": "1812.05477", "contents": "Title: Gaussian Process Deep Belief Networks: A Smooth Generative Model of\n  Shape with Uncertainty Propagation Abstract: The shape of an object is an important characteristic for many vision\nproblems such as segmentation, detection and tracking. Being independent of\nappearance, it is possible to generalize to a large range of objects from only\nsmall amounts of data. However, shapes represented as silhouette images are\nchallenging to model due to complicated likelihood functions leading to\nintractable posteriors. In this paper we present a generative model of shapes\nwhich provides a low dimensional latent encoding which importantly resides on a\nsmooth manifold with respect to the silhouette images. The proposed model\npropagates uncertainty in a principled manner allowing it to learn from small\namounts of data and providing predictions with associated uncertainty. We\nprovide experiments that show how our proposed model provides favorable\nquantitative results compared with the state-of-the-art while simultaneously\nproviding a representation that resides on a low-dimensional interpretable\nmanifold. \n\n"}
{"id": "1812.05642", "contents": "Title: SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception Abstract: Unsupervised learning for geometric perception (depth, optical flow, etc.) is\nof great interest to autonomous systems. Recent works on unsupervised learning\nhave made considerable progress on perceiving geometry; however, they usually\nignore the coherence of objects and perform poorly under scenarios with dark\nand noisy environments. In contrast, supervised learning algorithms, which are\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\na novel framework that provides robust geometry perception without requiring\ngeometrically informative labels. Specifically, SIGNet integrates semantic\ninformation to make depth and flow predictions consistent with objects and\nrobust to low lighting conditions. SIGNet is shown to improve upon the\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic object class\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\nwill be made available at https://github.com/mengyuest/SIGNet \n\n"}
{"id": "1812.06576", "contents": "Title: Learning Incremental Triplet Margin for Person Re-identification Abstract: Person re-identification (ReID) aims to match people across multiple\nnon-overlapping video cameras deployed at different locations. To address this\nchallenging problem, many metric learning approaches have been proposed, among\nwhich triplet loss is one of the state-of-the-arts. In this work, we explore\nthe margin between positive and negative pairs of triplets and prove that large\nmargin is beneficial. In particular, we propose a novel multi-stage training\nstrategy which learns incremental triplet margin and improves triplet loss\neffectively. Multiple levels of feature maps are exploited to make the learned\nfeatures more discriminative. Besides, we introduce global hard identity\nsearching method to sample hard identities when generating a training batch.\nExtensive experiments on Market-1501, CUHK03, and DukeMTMCreID show that our\napproach yields a performance boost and outperforms most existing\nstate-of-the-art methods. \n\n"}
{"id": "1812.06663", "contents": "Title: Attending Category Disentangled Global Context for Image Classification Abstract: In this paper, we propose a general framework for image classification using\nthe attention mechanism and global context, which could incorporate with\nvarious network architectures to improve their performance. To investigate the\ncapability of the global context, we compare four mathematical models and\nobserve the global context encoded in the category disentangled conditional\ngenerative model could give more guidance as \"know what is task irrelevant will\nalso know what is relevant\". Based on this observation, we define a novel\nCategory Disentangled Global Context (CDGC) and devise a deep network to obtain\nit. By attending CDGC, the baseline networks could identify the objects of\ninterest more accurately, thus improving the performance. We apply the\nframework to many different network architectures and compare with the\nstate-of-the-art on four publicly available datasets. Extensive results\nvalidate the effectiveness and superiority of our approach. Code will be made\npublic upon paper acceptance. \n\n"}
{"id": "1812.06869", "contents": "Title: BriarPatches: Pixel-Space Interventions for Inducing Demographic Parity Abstract: We introduce the BriarPatch, a pixel-space intervention that obscures\nsensitive attributes from representations encoded in pre-trained classifiers.\nThe patches encourage internal model representations not to encode sensitive\ninformation, which has the effect of pushing downstream predictors towards\nexhibiting demographic parity with respect to the sensitive information. The\nnet result is that these BriarPatches provide an intervention mechanism\navailable at user level, and complements prior research on fair representations\nthat were previously only applicable by model developers and ML experts. \n\n"}
{"id": "1812.06932", "contents": "Title: Fast Learning-based Registration of Sparse 3D Clinical Images Abstract: We introduce SparseVM, a method that registers clinical-quality 3D MR scans\nboth faster and more accurately than previously possible. Deformable alignment,\nor registration, of clinical scans is a fundamental task for many clinical\nneuroscience studies. However, most registration algorithms are designed for\nhigh-resolution research-quality scans. In contrast to research-quality scans,\nclinical scans are often sparse, missing up to 86% of the slices available in\nresearch-quality scans. Existing methods for registering these sparse images\nare either inaccurate or extremely slow. We present a learning-based\nregistration method, SparseVM, that is more accurate and orders of magnitude\nfaster than the most accurate clinical registration methods. To our knowledge,\nit is the first method to use deep learning specifically tailored to\nregistering clinical images. We demonstrate our method on a clinically-acquired\nMRI dataset of stroke patients and on a simulated sparse MRI dataset. Our code\nis available as part of the VoxelMorph package at http://voxelmorph.mit.edu/. \n\n"}
{"id": "1812.07003", "contents": "Title: 3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans Abstract: We introduce 3D-SIS, a novel neural network architecture for 3D semantic\ninstance segmentation in commodity RGB-D scans. The core idea of our method is\nto jointly learn from both geometric and color signal, thus enabling accurate\ninstance predictions. Rather than operate solely on 2D frames, we observe that\nmost computer vision applications have multi-view RGB-D input available, which\nwe leverage to construct an approach for 3D instance segmentation that\neffectively fuses together these multi-modal inputs. Our network leverages\nhigh-resolution RGB input by associating 2D images with the volumetric grid\nbased on the pose alignment of the 3D reconstruction. For each image, we first\nextract 2D features for each pixel with a series of 2D convolutions; we then\nbackproject the resulting feature vector to the associated voxel in the 3D\ngrid. This combination of 2D and 3D feature learning allows significantly\nhigher accuracy object detection and instance segmentation than\nstate-of-the-art alternatives. We show results on both synthetic and real-world\npublic benchmarks, achieving an improvement in mAP of over 13 on real-world\ndata. \n\n"}
{"id": "1812.07023", "contents": "Title: From FiLM to Video: Multi-turn Question Answering with Multi-modal\n  Context Abstract: Understanding audio-visual content and the ability to have an informative\nconversation about it have both been challenging areas for intelligent systems.\nThe Audio Visual Scene-aware Dialog (AVSD) challenge, organized as a track of\nthe Dialog System Technology Challenge 7 (DSTC7), proposes a combined task,\nwhere a system has to answer questions pertaining to a video given a dialogue\nwith previous question-answer pairs and the video itself. We propose for this\ntask a hierarchical encoder-decoder model which computes a multi-modal\nembedding of the dialogue context. It first embeds the dialogue history using\ntwo LSTMs. We extract video and audio frames at regular intervals and compute\nsemantic features using pre-trained I3D and VGGish models, respectively. Before\nsummarizing both modalities into fixed-length vectors using LSTMs, we use FiLM\nblocks to condition them on the embeddings of the current question, which\nallows us to reduce the dimensionality considerably. Finally, we use an LSTM\ndecoder that we train with scheduled sampling and evaluate using beam search.\nCompared to the modality-fusing baseline model released by the AVSD challenge\norganizers, our model achieves a relative improvements of more than 16%,\nscoring 0.36 BLEU-4 and more than 33%, scoring 0.997 CIDEr. \n\n"}
{"id": "1812.07119", "contents": "Title: Composing Text and Image for Image Retrieval - An Empirical Odyssey Abstract: In this paper, we study the task of image retrieval, where the input query is\nspecified in the form of an image plus some text that describes desired\nmodifications to the input image. For example, we may present an image of the\nEiffel tower, and ask the system to find images which are visually similar but\nare modified in small ways, such as being taken at nighttime instead of during\nthe day. To tackle this task, we learn a similarity metric between a target\nimage and a source image plus source text, an embedding and composing function\nsuch that target image feature is close to the source image plus text\ncomposition feature. We propose a new way to combine image and text using such\nfunction that is designed for the retrieval task. We show this outperforms\nexisting approaches on 3 different datasets, namely Fashion-200k, MIT-States\nand a new synthetic dataset we create based on CLEVR. We also show that our\napproach can be used to classify input queries, in addition to image retrieval. \n\n"}
{"id": "1812.07150", "contents": "Title: Interactive Naming for Explaining Deep Neural Networks: A Formative\n  Study Abstract: We consider the problem of explaining the decisions of deep neural networks\nfor image recognition in terms of human-recognizable visual concepts. In\nparticular, given a test set of images, we aim to explain each classification\nin terms of a small number of image regions, or activation maps, which have\nbeen associated with semantic concepts by a human annotator. This allows for\ngenerating summary views of the typical reasons for classifications, which can\nhelp build trust in a classifier and/or identify example types for which the\nclassifier may not be trusted. For this purpose, we developed a user interface\nfor \"interactive naming,\" which allows a human annotator to manually cluster\nsignificant activation maps in a test set into meaningful groups called \"visual\nconcepts\". The main contribution of this paper is a systematic study of the\nvisual concepts produced by five human annotators using the interactive naming\ninterface. In particular, we consider the adequacy of the concepts for\nexplaining the classification of test-set images, correspondence of the\nconcepts to activations of individual neurons, and the inter-annotator\nagreement of visual concepts. We find that a large fraction of the activation\nmaps have recognizable visual concepts, and that there is significant agreement\nbetween the different annotators about their denotations. Our work is an\nexploratory study of the interplay between machine learning and human\nrecognition mediated by visualizations of the results of learning. \n\n"}
{"id": "1812.07169", "contents": "Title: Explaining Neural Networks Semantically and Quantitatively Abstract: This paper presents a method to explain the knowledge encoded in a\nconvolutional neural network (CNN) quantitatively and semantically. The\nanalysis of the specific rationale of each prediction made by the CNN presents\na key issue of understanding neural networks, but it is also of significant\npractical values in certain applications. In this study, we propose to distill\nknowledge from the CNN into an explainable additive model, so that we can use\nthe explainable model to provide a quantitative explanation for the CNN\nprediction. We analyze the typical bias-interpreting problem of the explainable\nmodel and develop prior losses to guide the learning of the explainable\nadditive model. Experimental results have demonstrated the effectiveness of our\nmethod. \n\n"}
{"id": "1812.07252", "contents": "Title: Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via\n  Randomized-to-Canonical Adaptation Networks Abstract: Real world data, especially in the domain of robotics, is notoriously costly\nto collect. One way to circumvent this can be to leverage the power of\nsimulation to produce large amounts of labelled data. However, training models\non simulated images does not readily transfer to real-world ones. Using domain\nadaptation methods to cross this \"reality gap\" requires a large amount of\nunlabelled real-world data, whilst domain randomization alone can waste\nmodeling power. In this paper, we present Randomized-to-Canonical Adaptation\nNetworks (RCANs), a novel approach to crossing the visual reality gap that uses\nno real-world data. Our method learns to translate randomized rendered images\ninto their equivalent non-randomized, canonical versions. This in turn allows\nfor real images to also be translated into canonical sim images. We demonstrate\nthe effectiveness of this sim-to-real approach by training a vision-based\nclosed-loop grasping reinforcement learning agent in simulation, and then\ntransferring it to the real world to attain 70% zero-shot grasp success on\nunseen objects, a result that almost doubles the success of learning the same\ntask directly on domain randomization alone. Additionally, by joint finetuning\nin the real-world with only 5,000 real-world grasps, our method achieves 91%,\nattaining comparable performance to a state-of-the-art system trained with\n580,000 real-world grasps, resulting in a reduction of real-world data by more\nthan 99%. \n\n"}
{"id": "1812.07809", "contents": "Title: Found in Translation: Learning Robust Joint Representations by Cyclic\n  Translations Between Modalities Abstract: Multimodal sentiment analysis is a core research area that studies speaker\nsentiment expressed from the language, visual, and acoustic modalities. The\ncentral challenge in multimodal learning involves inferring joint\nrepresentations that can process and relate information from these modalities.\nHowever, existing work learns joint representations by requiring all modalities\nas input and as a result, the learned representations may be sensitive to noisy\nor missing modalities at test time. With the recent success of sequence to\nsequence (Seq2Seq) models in machine translation, there is an opportunity to\nexplore new ways of learning joint representations that may not require all\ninput modalities at test time. In this paper, we propose a method to learn\nrobust joint representations by translating between modalities. Our method is\nbased on the key insight that translation from a source to a target modality\nprovides a method of learning joint representations using only the source\nmodality as input. We augment modality translations with a cycle consistency\nloss to ensure that our joint representations retain maximal information from\nall modalities. Once our translation model is trained with paired multimodal\ndata, we only need data from the source modality at test time for final\nsentiment prediction. This ensures that our model remains robust from\nperturbations or missing information in the other modalities. We train our\nmodel with a coupled translation-prediction objective and it achieves new\nstate-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI,\nICT-MMMO, and YouTube. Additional experiments show that our model learns\nincreasingly discriminative joint representations with more input modalities\nwhile maintaining robustness to missing or perturbed modalities. \n\n"}
{"id": "1812.08849", "contents": "Title: Three Dimensional Reconstruction of Botanical Trees with Simulatable\n  Geometry Abstract: We tackle the challenging problem of creating full and accurate three\ndimensional reconstructions of botanical trees with the topological and\ngeometric accuracy required for subsequent physical simulation, e.g. in\nresponse to wind forces. Although certain aspects of our approach would benefit\nfrom various improvements, our results exceed the state of the art especially\nin geometric and topological complexity and accuracy. Starting with two\ndimensional RGB image data acquired from cameras attached to drones, we create\npoint clouds, textured triangle meshes, and a simulatable and skinned\ncylindrical articulated rigid body model. We discuss the pros and cons of each\nstep of our pipeline, and in order to stimulate future research we make the raw\nand processed data from every step of the pipeline as well as the final\ngeometric reconstructions publicly available. \n\n"}
{"id": "1812.08861", "contents": "Title: Animating Arbitrary Objects via Deep Motion Transfer Abstract: This paper introduces a novel deep learning framework for image animation.\nGiven an input image with a target object and a driving video sequence\ndepicting a moving object, our framework generates a video in which the target\nobject is animated according to the driving sequence. This is achieved through\na deep architecture that decouples appearance and motion information. Our\nframework consists of three main modules: (i) a Keypoint Detector unsupervisely\ntrained to extract object keypoints, (ii) a Dense Motion prediction network for\ngenerating dense heatmaps from sparse keypoints, in order to better encode\nmotion information and (iii) a Motion Transfer Network, which uses the motion\nheatmaps and appearance information extracted from the input image to\nsynthesize the output frames. We demonstrate the effectiveness of our method on\nseveral benchmark datasets, spanning a wide variety of object appearances, and\nshow that our approach outperforms state-of-the-art image animation and video\ngeneration methods. Our source code is publicly available. \n\n"}
{"id": "1812.09834", "contents": "Title: Holistic Decomposition Convolution for Effective Semantic Segmentation\n  of 3D MR Images Abstract: Convolutional Neural Networks (CNNs) have achieved state-of-the-art\nperformance in many different 2D medical image analysis tasks. In clinical\npractice, however, a large part of the medical imaging data available is in 3D.\nThis has motivated the development of 3D CNNs for volumetric image segmentation\nin order to benefit from more spatial context. Due to GPU memory restrictions\ncaused by moving to fully 3D, state-of-the-art methods depend on\nsubvolume/patch processing and the size of the input patch is usually small,\nlimiting the incorporation of larger context information for a better\nperformance. In this paper, we propose a novel Holistic Decomposition\nConvolution (HDC), for an effective and efficient semantic segmentation of\nvolumetric images. HDC consists of a periodic down-shuffling operation followed\nby a conventional 3D convolution. HDC has the advantage of significantly\nreducing the size of the data for sub-sequential processing while using all the\ninformation available in the input irrespective of the down-shuffling factors.\nResults obtained from comprehensive experiments conducted on hip T1 MR images\nand intervertebral disc T2 MR images demonstrate the efficacy of the present\napproach. \n\n"}
{"id": "1812.10071", "contents": "Title: Coupled Recurrent Network (CRN) Abstract: Many semantic video analysis tasks can benefit from multiple, heterogenous\nsignals. For example, in addition to the original RGB input sequences,\nsequences of optical flow are usually used to boost the performance of human\naction recognition in videos. To learn from these heterogenous input sources,\nexisting methods reply on two-stream architectural designs that contain\nindependent, parallel streams of Recurrent Neural Networks (RNNs). However,\ntwo-stream RNNs do not fully exploit the reciprocal information contained in\nthe multiple signals, let alone exploit it in a recurrent manner. To this end,\nwe propose in this paper a novel recurrent architecture, termed Coupled\nRecurrent Network (CRN), to deal with multiple input sources. In CRN, the\nparallel streams of RNNs are coupled together. Key design of CRN is a Recurrent\nInterpretation Block (RIB) that supports learning of reciprocal feature\nrepresentations from multiple signals in a recurrent manner. Different from\nRNNs which stack the training loss at each time step or the last time step, we\npropose an effective and efficient training strategy for CRN. Experiments show\nthe efficacy of the proposed CRN. In particular, we achieve the new state of\nthe art on the benchmark datasets of human action recognition and multi-person\npose estimation. \n\n"}
{"id": "1812.10191", "contents": "Title: FPD-M-net: Fingerprint Image Denoising and Inpainting Using M-Net Based\n  Convolutional Neural Networks Abstract: Fingerprint is a common biometric used for authentication and verification of\nan individual. These images are degraded when fingers are wet, dirty, dry or\nwounded and due to the failure of the sensors, etc. The extraction of the\nfingerprint from a degraded image requires denoising and inpainting. We propose\nto address these problems with an end-to-end trainable Convolutional Neural\nNetwork based architecture called FPD-M-net, by posing the fingerprint\ndenoising and inpainting problem as a segmentation (foreground) task. Our\narchitecture is based on the M-net with a change: structure similarity loss\nfunction, used for better extraction of the fingerprint from the noisy\nbackground. Our method outperforms the baseline method and achieves an overall\n3rd rank in the Chalearn LAP Inpainting Competition Track 3 - Fingerprint\nDenoising and Inpainting, ECCV 2018 \n\n"}
{"id": "1812.10320", "contents": "Title: Structure-Aware 3D Hourglass Network for Hand Pose Estimation from\n  Single Depth Image Abstract: In this paper, we propose a novel structure-aware 3D hourglass network for\nhand pose estimation from a single depth image, which achieves state-of-the-art\nresults on MSRA and NYU datasets. Compared to existing works that perform\nimage-to-coordination regression, our network takes 3D voxel as input and\ndirectly regresses 3D heatmap for each joint. To be specific, we use hourglass\nnetwork as our backbone network and modify it into 3D form. We explicitly model\ntree-like finger bone into the network as well as in the loss function in an\nend-to-end manner, in order to take the skeleton constraints into\nconsideration. Final estimation can then be easily obtained from voxel density\nmap with simple post-processing. Experimental results show that the proposed\nstructure-aware 3D hourglass network is able to achieve a mean joint error of\n7.4 mm in MSRA and 8.9 mm in NYU datasets, respectively. \n\n"}
{"id": "1812.10482", "contents": "Title: Finger-GAN: Generating Realistic Fingerprint Images Using Connectivity\n  Imposed GAN Abstract: Generating realistic biometric images has been an interesting and, at the\nsame time, challenging problem. Classical statistical models fail to generate\nrealistic-looking fingerprint images, as they are not powerful enough to\ncapture the complicated texture representation in fingerprint images. In this\nwork, we present a machine learning framework based on generative adversarial\nnetworks (GAN), which is able to generate fingerprint images sampled from a\nprior distribution (learned from a set of training images). We also add a\nsuitable regularization term to the loss function, to impose the connectivity\nof generated fingerprint images. This is highly desirable for fingerprints, as\nthe lines in each finger are usually connected. We apply this framework to two\npopular fingerprint databases, and generate images which look very realistic,\nand similar to the samples in those databases. Through experimental results, we\nshow that the generated fingerprint images have a good diversity, and are able\nto capture different parts of the prior distribution. We also evaluate the\nFrechet Inception distance (FID) of our proposed model, and show that our model\nis able to achieve good quantitative performance in terms of this score. \n\n"}
{"id": "1812.10779", "contents": "Title: Semantic Driven Multi-Camera Pedestrian Detection Abstract: In the current worldwide situation, pedestrian detection has reemerged as a\npivotal tool for intelligent video-based systems aiming to solve tasks such as\npedestrian tracking, social distancing monitoring or pedestrian mass counting.\nPedestrian detection methods, even the top performing ones, are highly\nsensitive to occlusions among pedestrians, which dramatically degrades their\nperformance in crowded scenarios. The generalization of multi-camera set-ups\npermits to better confront occlusions by combining information from different\nviewpoints. In this paper, we present a multi-camera approach to globally\ncombine pedestrian detections leveraging automatically extracted scene context.\nContrarily to the majority of the methods of the state-of-the-art, the proposed\napproach is scene-agnostic, not requiring a tailored adaptation to the target\nscenario\\textemdash e.g., via fine-tunning. This noteworthy attribute does not\nrequire \\textit{ad hoc} training with labelled data, expediting the deployment\nof the proposed method in real-world situations. Context information, obtained\nvia semantic segmentation, is used 1) to automatically generate a common Area\nof Interest for the scene and all the cameras, avoiding the usual need of\nmanually defining it; and 2) to obtain detections for each camera by solving a\nglobal optimization problem that maximizes coherence of detections both in each\n2D image and in the 3D scene. This process yields tightly-fitted bounding boxes\nthat circumvent occlusions or miss-detections. Experimental results on five\npublicly available datasets show that the proposed approach outperforms\nstate-of-the-art multi-camera pedestrian detectors, even some specifically\ntrained on the target scenario, signifying the versatility and robustness of\nthe proposed method without requiring ad-hoc annotations nor human-guided\nconfiguration. \n\n"}
{"id": "1812.11317", "contents": "Title: Support Vector Guided Softmax Loss for Face Recognition Abstract: Face recognition has witnessed significant progresses due to the advances of\ndeep convolutional neural networks (CNNs), the central challenge of which, is\nfeature discrimination. To address it, one group tries to exploit mining-based\nstrategies (\\textit{e.g.}, hard example mining and focal loss) to focus on the\ninformative examples. The other group devotes to designing margin-based loss\nfunctions (\\textit{e.g.}, angular, additive and additive angular margins) to\nincrease the feature margin from the perspective of ground truth class. Both of\nthem have been well-verified to learn discriminative features. However, they\nsuffer from either the ambiguity of hard examples or the lack of discriminative\npower of other classes. In this paper, we design a novel loss function, namely\nsupport vector guided softmax loss (SV-Softmax), which adaptively emphasizes\nthe mis-classified points (support vectors) to guide the discriminative\nfeatures learning. So the developed SV-Softmax loss is able to eliminate the\nambiguity of hard examples as well as absorb the discriminative power of other\nclasses, and thus results in more discrimiantive features. To the best of our\nknowledge, this is the first attempt to inherit the advantages of mining-based\nand margin-based losses into one framework. Experimental results on several\nbenchmarks have demonstrated the effectiveness of our approach over\nstate-of-the-arts. \n\n"}
{"id": "1901.00398", "contents": "Title: Judge the Judges: A Large-Scale Evaluation Study of Neural Language\n  Models for Online Review Generation Abstract: We conduct a large-scale, systematic study to evaluate the existing\nevaluation methods for natural language generation in the context of generating\nonline product reviews. We compare human-based evaluators with a variety of\nautomated evaluation procedures, including discriminative evaluators that\nmeasure how well machine-generated text can be distinguished from human-written\ntext, as well as word overlap metrics that assess how similar the generated\ntext compares to human-written references. We determine to what extent these\ndifferent evaluators agree on the ranking of a dozen of state-of-the-art\ngenerators for online product reviews. We find that human evaluators do not\ncorrelate well with discriminative evaluators, leaving a bigger question of\nwhether adversarial accuracy is the correct objective for natural language\ngeneration. In general, distinguishing machine-generated text is challenging\neven for human evaluators, and human decisions correlate better with lexical\noverlaps. We find lexical diversity an intriguing metric that is indicative of\nthe assessments of different evaluators. A post-experiment survey of\nparticipants provides insights into how to evaluate and improve the quality of\nnatural language generation systems. \n\n"}
{"id": "1901.00889", "contents": "Title: Polarimetric Thermal to Visible Face Verification via Attribute\n  Preserved Synthesis Abstract: Thermal to visible face verification is a challenging problem due to the\nlarge domain discrepancy between the modalities. Existing approaches either\nattempt to synthesize visible faces from thermal faces or extract robust\nfeatures from these modalities for cross-modal matching. In this paper, we take\na different approach in which we make use of the attributes extracted from the\nvisible image to synthesize the attribute-preserved visible image from the\ninput thermal image for cross-modal matching. A pre-trained VGG-Face network is\nused to extract the attributes from the visible image. Then, a novel Attribute\nPreserved Generative Adversarial Network (AP-GAN) is proposed to synthesize the\nvisible image from the thermal image guided by the extracted attributes.\nFinally, a deep network is used to extract features from the synthesized image\nand the input visible image for verification. Extensive experiments on the ARL\nPolarimetric face dataset show that the proposed method achieves significant\nimprovements over the state-of-the-art methods. \n\n"}
{"id": "1901.01928", "contents": "Title: DSConv: Efficient Convolution Operator Abstract: Quantization is a popular way of increasing the speed and lowering the memory\nusage of Convolution Neural Networks (CNNs). When labelled training data is\navailable, network weights and activations have successfully been quantized\ndown to 1-bit. The same cannot be said about the scenario when labelled\ntraining data is not available, e.g. when quantizing a pre-trained model, where\ncurrent approaches show, at best, no loss of accuracy at 8-bit quantizations.\nWe introduce DSConv, a flexible quantized convolution operator that replaces\nsingle-precision operations with their far less expensive integer counterparts,\nwhile maintaining the probability distributions over both the kernel weights\nand the outputs. We test our model as a plug-and-play replacement for standard\nconvolution on most popular neural network architectures, ResNet, DenseNet,\nGoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with\nless than 1% loss of accuracy, without retraining, using only 4-bit\nquantization. We also show how a distillation-based adaptation stage with\nunlabelled data can improve results even further. \n\n"}
{"id": "1901.02000", "contents": "Title: Forecasting People Trajectories and Head Poses by Jointly Reasoning on\n  Tracklets and Vislets Abstract: In this work, we explore the correlation between people trajectories and\ntheir head orientations. We argue that people trajectory and head pose\nforecasting can be modelled as a joint problem. Recent approaches on trajectory\nforecasting leverage short-term trajectories (aka tracklets) of pedestrians to\npredict their future paths. In addition, sociological cues, such as expected\ndestination or pedestrian interaction, are often combined with tracklets. In\nthis paper, we propose MiXing-LSTM (MX-LSTM) to capture the interplay between\npositions and head orientations (vislets) thanks to a joint unconstrained\noptimization of full covariance matrices during the LSTM backpropagation. We\nadditionally exploit the head orientations as a proxy for the visual attention,\nwhen modeling social interactions. MX-LSTM predicts future pedestrians location\nand head pose, increasing the standard capabilities of the current approaches\non long-term trajectory forecasting. Compared to the state-of-the-art, our\napproach shows better performances on an extensive set of public benchmarks.\nMX-LSTM is particularly effective when people move slowly, i.e. the most\nchallenging scenario for all other models. The proposed approach also allows\nfor accurate predictions on a longer time horizon. \n\n"}
{"id": "1901.02579", "contents": "Title: Manipulation-skill Assessment from Videos with Spatial Attention Network Abstract: Recent advances in computer vision have made it possible to automatically\nassess from videos the manipulation skills of humans in performing a task,\nwhich breeds many important applications in domains such as health\nrehabilitation and manufacturing. Previous methods of video-based skill\nassessment did not consider the attention mechanism humans use in assessing\nvideos, limiting their performance as only a small part of video regions is\ninformative for skill assessment. Our motivation here is to estimate attention\nin videos that helps to focus on critically important video regions for better\nskill assessment. In particular, we propose a novel RNN-based spatial attention\nmodel that considers accumulated attention state from previous frames as well\nas high-level knowledge about the progress of an undergoing task. We evaluate\nour approach on a newly collected dataset of infant grasping task and four\nexisting datasets of hand manipulation tasks. Experiment results demonstrate\nthat state-of-the-art performance can be achieved by considering attention in\nautomatic skill assessment. \n\n"}
{"id": "1901.03035", "contents": "Title: Self-Monitoring Navigation Agent via Auxiliary Progress Estimation Abstract: The Vision-and-Language Navigation (VLN) task entails an agent following\nnavigational instruction in photo-realistic unknown environments. This\nchallenging task demands that the agent be aware of which instruction was\ncompleted, which instruction is needed next, which way to go, and its\nnavigation progress towards the goal. In this paper, we introduce a\nself-monitoring agent with two complementary components: (1) visual-textual\nco-grounding module to locate the instruction completed in the past, the\ninstruction required for the next action, and the next moving direction from\nsurrounding images and (2) progress monitor to ensure the grounded instruction\ncorrectly reflects the navigation progress. We test our self-monitoring agent\non a standard benchmark and analyze our proposed approach through a series of\nablation studies that elucidate the contributions of the primary components.\nUsing our proposed method, we set the new state of the art by a significant\nmargin (8% absolute increase in success rate on the unseen test set). Code is\navailable at https://github.com/chihyaoma/selfmonitoring-agent . \n\n"}
{"id": "1901.03665", "contents": "Title: A Biologically Inspired Visual Working Memory for Deep Networks Abstract: The ability to look multiple times through a series of pose-adjusted glimpses\nis fundamental to human vision. This critical faculty allows us to understand\nhighly complex visual scenes. Short term memory plays an integral role in\naggregating the information obtained from these glimpses and informing our\ninterpretation of the scene. Computational models have attempted to address\nglimpsing and visual attention but have failed to incorporate the notion of\nmemory. We introduce a novel, biologically inspired visual working memory\narchitecture that we term the Hebb-Rosenblatt memory. We subsequently introduce\na fully differentiable Short Term Attentive Working Memory model (STAWM) which\nuses transformational attention to learn a memory over each image it sees. The\nstate of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space\nof a layer. By projecting different queries through this layer we can obtain\ngoal-oriented latent representations for tasks including classification and\nvisual reconstruction. Our model obtains highly competitive classification\nperformance on MNIST and CIFAR-10. As demonstrated through the CelebA dataset,\nto perform reconstruction the model learns to make a sequence of updates to a\ncanvas which constitute a parts-based representation. Classification with the\nself supervised representation obtained from MNIST is shown to be in line with\nthe state of the art models (none of which use a visual attention mechanism).\nFinally, we show that STAWM can be trained under the dual constraints of\nclassification and reconstruction to provide an interpretable visual sketchpad\nwhich helps open the 'black-box' of deep learning. \n\n"}
{"id": "1901.04252", "contents": "Title: DeepFlash: Turning a Flash Selfie into a Studio Portrait Abstract: We present a method for turning a flash selfie taken with a smartphone into a\nphotograph as if it was taken in a studio setting with uniform lighting. Our\nmethod uses a convolutional neural network trained on a set of pairs of\nphotographs acquired in an ad-hoc acquisition campaign. Each pair consists of\none photograph of a subject's face taken with the camera flash enabled and\nanother one of the same subject in the same pose illuminated using a\nphotographic studio-lighting setup. We show how our method can amend defects\nintroduced by a close-up camera flash, such as specular highlights, shadows,\nskin shine, and flattened images. \n\n"}
{"id": "1901.04355", "contents": "Title: Iterative Deep Learning Based Unbiased Stereology With Human-in-the-Loop Abstract: Lack of enough labeled data is a major problem in building machine learning\nbased models when the manual annotation (labeling) is error-prone, expensive,\ntedious, and time-consuming. In this paper, we introduce an iterative deep\nlearning based method to improve segmentation and counting of cells based on\nunbiased stereology applied to regions of interest of extended depth of field\n(EDF) images. This method uses an existing machine learning algorithm called\nthe adaptive segmentation algorithm (ASA) to generate masks (verified by a\nuser) for EDF images to train deep learning models. Then an iterative deep\nlearning approach is used to feed newly predicted and accepted deep learning\nmasks/images (verified by a user) to the training set of the deep learning\nmodel. The error rate in unbiased stereology count of cells on an unseen test\nset reduced from about 3 % to less than 1 % after 5 iterations of the iterative\ndeep learning based unbiased stereology process. \n\n"}
{"id": "1901.05105", "contents": "Title: Uncertainty-Aware Driver Trajectory Prediction at Urban Intersections Abstract: Predicting the motion of a driver's vehicle is crucial for advanced driving\nsystems, enabling detection of potential risks towards shared control between\nthe driver and automation systems. In this paper, we propose a variational\nneural network approach that predicts future driver trajectory distributions\nfor the vehicle based on multiple sensors. Our predictor generates both a\nconditional variational distribution of future trajectories, as well as a\nconfidence estimate for different time horizons. Our approach allows us to\nhandle inherently uncertain situations, and reason about information gain from\neach input, as well as combine our model with additional predictors, creating a\nmixture of experts. We show how to augment the variational predictor with a\nphysics-based predictor, and based on their confidence estimations, improve\noverall system performance. The resulting combined model is aware of the\nuncertainty associated with its predictions, which can help the vehicle\nautonomy to make decisions with more confidence. The model is validated on\nreal-world urban driving data collected in multiple locations. This validation\ndemonstrates that our approach improves the prediction error of a physics-based\nmodel by 25% while successfully identifying the uncertain cases with 82%\naccuracy. \n\n"}
{"id": "1901.05355", "contents": "Title: Lightweight Markerless Monocular Face Capture with 3D Spatial Priors Abstract: We present a simple lightweight markerless facial performance capture\nframework using just a monocular video input that combines Active Appearance\nModels for feature tracking and prior constraints on 3D shapes into an\nintegrated objective function. 2D monocular inputs inherently lack information\nalong the depth axis and can lead to physically implausible solutions. In order\nto address this loss of information, we enforce a constraint on our objective\nfunction within a probabilistic framework that uses preexisting animations\nobtained from accurate 3D tracking systems, thus achieving more plausible\nresults. Our system fits a Blendshape model to tracked 2D features while also\nhandling noise in estimation of features and camera parameters. We learn\nseparate constraints for the upper and lower regions of the face thus\nmaintaining flexibility. We show that using this approach, we can obtain\nsignificant improvement in tracking especially along the depth dimension. Our\nmethod uses easily obtainable prior animation data. We show that our method can\ngenerate convincing animations using only a monocular video input. We\nquantitatively evaluate our results comparing it with an approach using a\nmonocular input without our spatial constraints and show that our results are\ncloser to the ground-truth geometry. Finally, we also evaluate the effect that\nthe choice of the Blendshape set has on the results of the solver by solving\nfor a different set of Blendshapes and quantitatively comparing it with our\nprevious results and to the ground truth. We show that while the choice of\nBlendshapes does make a difference, the use of our spatial constraints\ngenerates results that are closer to the ground truth. \n\n"}
{"id": "1901.05427", "contents": "Title: Domain Adaptation for Structured Output via Discriminative Patch\n  Representations Abstract: Predicting structured outputs such as semantic segmentation relies on\nexpensive per-pixel annotations to learn supervised models like convolutional\nneural networks. However, models trained on one data domain may not generalize\nwell to other domains without annotations for model finetuning. To avoid the\nlabor-intensive process of annotation, we develop a domain adaptation method to\nadapt the source data to the unlabeled target domain. We propose to learn\ndiscriminative feature representations of patches in the source domain by\ndiscovering multiple modes of patch-wise output distribution through the\nconstruction of a clustered space. With such representations as guidance, we\nuse an adversarial learning scheme to push the feature representations of\ntarget patches in the clustered space closer to the distributions of source\npatches. In addition, we show that our framework is complementary to existing\ndomain adaptation techniques and achieves consistent improvements on semantic\nsegmentation. Extensive ablations and results are demonstrated on numerous\nbenchmark datasets with various settings, such as synthetic-to-real and\ncross-city scenarios. \n\n"}
{"id": "1901.05761", "contents": "Title: Attentive Neural Processes Abstract: Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by\nlearning to map a context set of observed input-output pairs to a distribution\nover regression functions. Each function models the distribution of the output\ngiven an input, conditioned on the context. NPs have the benefit of fitting\nobserved data efficiently with linear complexity in the number of context\ninput-output pairs, and can learn a wide family of conditional distributions;\nthey learn predictive distributions conditioned on context sets of arbitrary\nsize. Nonetheless, we show that NPs suffer a fundamental drawback of\nunderfitting, giving inaccurate predictions at the inputs of the observed data\nthey condition on. We address this issue by incorporating attention into NPs,\nallowing each input location to attend to the relevant context points for the\nprediction. We show that this greatly improves the accuracy of predictions,\nresults in noticeably faster training, and expands the range of functions that\ncan be modelled. \n\n"}
{"id": "1901.06722", "contents": "Title: Fitting 3D Shapes from Partial and Noisy Point Clouds with Evolutionary\n  Computing Abstract: Point clouds obtained from photogrammetry are noisy and incomplete models of\nreality. We propose an evolutionary optimization methodology that is able to\napproximate the underlying object geometry on such point clouds. This approach\nassumes a priori knowledge on the 3D structure modeled and enables the\nidentification of a collection of primitive shapes approximating the scene.\nBuilt-in mechanisms that enforce high shape diversity and adaptive population\nsize make this method suitable to modeling both simple and complex scenes. We\nfocus here on the case of cylinder approximations and we describe, test, and\ncompare a set of mutation operators designed for optimal exploration of their\nsearch space. We assess the robustness and limitations of this algorithm\nthrough a series of synthetic examples, and we finally demonstrate its general\napplicability on two real-life cases in vegetation and industrial settings. \n\n"}
{"id": "1901.06823", "contents": "Title: Salient Object Detection with Lossless Feature Reflection and Weighted\n  Structural Loss Abstract: Salient object detection (SOD), which aims to identify and locate the most\nsalient pixels or regions in images, has been attracting more and more interest\ndue to its various real-world applications. However, this vision task is quite\nchallenging, especially under complex image scenes. Inspired by the intrinsic\nreflection of natural images, in this paper we propose a novel feature learning\nframework for large-scale salient object detection. Specifically, we design a\nsymmetrical fully convolutional network (SFCN) to effectively learn\ncomplementary saliency features under the guidance of lossless feature\nreflection. The location information, together with contextual and semantic\ninformation, of salient objects are jointly utilized to supervise the proposed\nnetwork for more accurate saliency predictions. In addition, to overcome the\nblurry boundary problem, we propose a new weighted structural loss function to\nensure clear object boundaries and spatially consistent saliency. The coarse\nprediction results are effectively refined by these structural information for\nperformance improvements. Extensive experiments on seven saliency detection\ndatasets demonstrate that our approach achieves consistently superior\nperformance and outperforms the very recent state-of-the-art methods with a\nlarge margin. \n\n"}
{"id": "1901.07012", "contents": "Title: Understanding the Impact of Label Granularity on CNN-based Image\n  Classification Abstract: In recent years, supervised learning using Convolutional Neural Networks\n(CNNs) has achieved great success in image classification tasks, and large\nscale labeled datasets have contributed significantly to this achievement.\nHowever, the definition of a label is often application dependent. For example,\nan image of a cat can be labeled as \"cat\" or perhaps more specifically \"Persian\ncat.\" We refer to this as label granularity. In this paper, we conduct\nextensive experiments using various datasets to demonstrate and analyze how and\nwhy training based on fine-grain labeling, such as \"Persian cat\" can improve\nCNN accuracy on classifying coarse-grain classes, in this case \"cat.\" The\nexperimental results show that training CNNs with fine-grain labels improves\nboth network's optimization and generalization capabilities, as intuitively it\nencourages the network to learn more features, and hence increases\nclassification accuracy on coarse-grain classes under all datasets considered.\nMoreover, fine-grain labels enhance data efficiency in CNN training. For\nexample, a CNN trained with fine-grain labels and only 40% of the total\ntraining data can achieve higher accuracy than a CNN trained with the full\ntraining dataset and coarse-grain labels. These results point to two possible\napplications of this work: (i) with sufficient human resources, one can improve\nCNN performance by re-labeling the dataset with fine-grain labels, and (ii)\nwith limited human resources, to improve CNN performance, rather than\ncollecting more training data, one may instead use fine-grain labels for the\ndataset. We further propose a metric called Average Confusion Ratio to\ncharacterize the effectiveness of fine-grain labeling, and show its use through\nextensive experimentation. Code is available at\nhttps://github.com/cmu-enyac/Label-Granularity. \n\n"}
{"id": "1901.07295", "contents": "Title: Adversarial Pseudo Healthy Synthesis Needs Pathology Factorization Abstract: Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy'\nimage from a pathological one, could be helpful in tasks such as anomaly\ndetection, understanding changes induced by pathology and disease or even as\ndata augmentation. We treat this task as a factor decomposition problem: we aim\nto separate what appears to be healthy and where disease is (as a map). The two\nfactors are then recombined (by a network) to reconstruct the input disease\nimage. We train our models in an adversarial way using either paired or\nunpaired settings, where we pair disease images and maps (as segmentation\nmasks) when available. We quantitatively evaluate the quality of pseudo healthy\nimages. We show in a series of experiments, performed in ISLES and BraTS\ndatasets, that our method is better than conditional GAN and CycleGAN,\nhighlighting challenges in using adversarial methods in the image translation\ntask of pseudo healthy image generation. \n\n"}
{"id": "1901.07417", "contents": "Title: On Connected Sublevel Sets in Deep Learning Abstract: This paper shows that every sublevel set of the loss function of a class of\ndeep over-parameterized neural nets with piecewise linear activation functions\nis connected and unbounded. This implies that the loss has no bad local valleys\nand all of its global minima are connected within a unique and potentially very\nlarge global valley. \n\n"}
{"id": "1901.07647", "contents": "Title: Understanding Geometry of Encoder-Decoder CNNs Abstract: Encoder-decoder networks using convolutional neural network (CNN)\narchitecture have been extensively used in deep learning literatures thanks to\nits excellent performance for various inverse problems. However, it is still\ndifficult to obtain coherent geometric view why such an architecture gives the\ndesired performance. Inspired by recent theoretical understanding on\ngeneralizability, expressivity and optimization landscape of neural networks,\nas well as the theory of convolutional framelets, here we provide a unified\ntheoretical framework that leads to a better understanding of geometry of\nencoder-decoder CNNs. Our unified mathematical framework shows that\nencoder-decoder CNN architecture is closely related to nonlinear basis\nrepresentation using combinatorial convolution frames, whose expressibility\nincreases exponentially with the network depth. We also demonstrate the\nimportance of skipped connection in terms of expressibility, and optimization\nlandscape. \n\n"}
{"id": "1901.07827", "contents": "Title: Towards Compact ConvNets via Structure-Sparsity Regularized Filter\n  Pruning Abstract: The success of convolutional neural networks (CNNs) in computer vision\napplications has been accompanied by a significant increase of computation and\nmemory costs, which prohibits its usage on resource-limited environments such\nas mobile or embedded devices. To this end, the research of CNN compression has\nrecently become emerging. In this paper, we propose a novel filter pruning\nscheme, termed structured sparsity regularization (SSR), to simultaneously\nspeedup the computation and reduce the memory overhead of CNNs, which can be\nwell supported by various off-the-shelf deep learning libraries. Concretely,\nthe proposed scheme incorporates two different regularizers of structured\nsparsity into the original objective function of filter pruning, which fully\ncoordinates the global outputs and local pruning operations to adaptively prune\nfilters. We further propose an Alternative Updating with Lagrange Multipliers\n(AULM) scheme to efficiently solve its optimization. AULM follows the principle\nof ADMM and alternates between promoting the structured sparsity of CNNs and\noptimizing the recognition loss, which leads to a very efficient solver (2.5x\nto the most recent work that directly solves the group sparsity-based\nregularization). Moreover, by imposing the structured sparsity, the online\ninference is extremely memory-light, since the number of filters and the output\nfeature maps are simultaneously reduced. The proposed scheme has been deployed\nto a variety of state-of-the-art CNN structures including LeNet, AlexNet, VGG,\nResNet and GoogLeNet over different datasets. Quantitative results demonstrate\nthat the proposed scheme achieves superior performance over the\nstate-of-the-art methods. We further demonstrate the proposed compression\nscheme for the task of transfer learning, including domain adaptation and\nobject detection, which also show exciting performance gains over the\nstate-of-the-arts. \n\n"}
{"id": "1901.08292", "contents": "Title: Anomaly Detection in Road Traffic Using Visual Surveillance: A Survey Abstract: Computer vision has evolved in the last decade as a key technology for\nnumerous applications replacing human supervision. In this paper, we present a\nsurvey on relevant visual surveillance related researches for anomaly detection\nin public places, focusing primarily on roads. Firstly, we revisit the surveys\ndone in the last 10 years in this field. Since the underlying building block of\na typical anomaly detection is learning, we emphasize more on learning methods\napplied on video scenes. We then summarize the important contributions made\nduring last six years on anomaly detection primarily focusing on features,\nunderlying techniques, applied scenarios and types of anomalies using single\nstatic camera. Finally, we discuss the challenges in the computer vision\nrelated anomaly detection techniques and some of the important future\npossibilities. \n\n"}
{"id": "1901.08486", "contents": "Title: Never Forget: Balancing Exploration and Exploitation via Learning\n  Optical Flow Abstract: Exploration bonus derived from the novelty of the states in an environment\nhas become a popular approach to motivate exploration for deep reinforcement\nlearning agents in the past few years. Recent methods such as curiosity-driven\nexploration usually estimate the novelty of new observations by the prediction\nerrors of their system dynamics models. Due to the capacity limitation of the\nmodels and difficulty of performing next-frame prediction, however, these\nmethods typically fail to balance between exploration and exploitation in\nhigh-dimensional observation tasks, resulting in the agents forgetting the\nvisited paths and exploring those states repeatedly. Such inefficient\nexploration behavior causes significant performance drops, especially in large\nenvironments with sparse reward signals. In this paper, we propose to introduce\nthe concept of optical flow estimation from the field of computer vision to\ndeal with the above issue. We propose to employ optical flow estimation errors\nto examine the novelty of new observations, such that agents are able to\nmemorize and understand the visited states in a more comprehensive fashion. We\ncompare our method against the previous approaches in a number of experimental\nexperiments. Our results indicate that the proposed method appears to deliver\nsuperior and long-lasting performance than the previous methods. We further\nprovide a set of comprehensive ablative analysis of the proposed method, and\ninvestigate the impact of optical flow estimation on the learning curves of the\nDRL agents. \n\n"}
{"id": "1901.08755", "contents": "Title: SecureBoost: A Lossless Federated Learning Framework Abstract: The protection of user privacy is an important concern in machine learning,\nas evidenced by the rolling out of the General Data Protection Regulation\n(GDPR) in the European Union (EU) in May 2018. The GDPR is designed to give\nusers more control over their personal data, which motivates us to explore\nmachine learning frameworks for data sharing that do not violate user privacy.\nTo meet this goal, in this paper, we propose a novel lossless\nprivacy-preserving tree-boosting system known as SecureBoost in the setting of\nfederated learning. SecureBoost first conducts entity alignment under a\nprivacy-preserving protocol and then constructs boosting trees across multiple\nparties with a carefully designed encryption strategy. This federated learning\nsystem allows the learning process to be jointly conducted over multiple\nparties with common user samples but different feature sets, which corresponds\nto a vertically partitioned data set. An advantage of SecureBoost is that it\nprovides the same level of accuracy as the non-privacy-preserving approach\nwhile at the same time, reveals no information of each private data provider.\nWe show that the SecureBoost framework is as accurate as other non-federated\ngradient tree-boosting algorithms that require centralized data and thus it is\nhighly scalable and practical for industrial applications such as credit risk\nanalysis. To this end, we discuss information leakage during the protocol\nexecution and propose ways to provably reduce it. \n\n"}
{"id": "1901.09614", "contents": "Title: A Simple Method to Reduce Off-chip Memory Accesses on Convolutional\n  Neural Networks Abstract: For convolutional neural networks, a simple algorithm to reduce off-chip\nmemory accesses is proposed by maximally utilizing on-chip memory in a neural\nprocess unit. Especially, the algorithm provides an effective way to process a\nmodule which consists of multiple branches and a merge layer. For Inception-V3\non Samsung's NPU in Exynos, our evaluation shows that the proposed algorithm\nmakes off-chip memory accesses reduced by 1/50, and accordingly achieves 97.59\n% reduction in the amount of feature-map data to be transferred from/to\noff-chip memory. \n\n"}
{"id": "1901.09822", "contents": "Title: Virtual Conditional Generative Adversarial Networks Abstract: When trained on multimodal image datasets, normal Generative Adversarial\nNetworks (GANs) are usually outperformed by class-conditional GANs and ensemble\nGANs, but conditional GANs is restricted to labeled datasets and ensemble GANs\nlack efficiency. We propose a novel GAN variant called virtual conditional GAN\n(vcGAN) which is not only an ensemble GAN with multiple generative paths while\nadding almost zero network parameters, but also a conditional GAN that can be\ntrained on unlabeled datasets without explicit clustering steps or objectives\nother than the adversary loss. Inside the vcGAN's generator, a learnable\n``analog-to-digital converter (ADC)\" module maps a slice of the inputted\nmultivariate Gaussian noise to discrete/digital noise (virtual label),\naccording to which a selector selects the corresponding generative path to\nproduce the sample. All the generative paths share the same decoder network\nwhile in each path the decoder network is fed with a concatenation of a\ndifferent pre-computed amplified one-hot vector and the inputted Gaussian\nnoise. We conducted a lot of experiments on several balanced/imbalanced image\ndatasets to demonstrate that vcGAN converges faster and achieves improved\nFrech\\'et Inception Distance (FID). In addition, we show the training byproduct\nthat the ADC in vcGAN learned the categorical probability of each mode and that\neach generative path generates samples of specific mode, which enables\nclass-conditional sampling. Codes are available at\n\\url{https://github.com/annonnymmouss/vcgan} \n\n"}
{"id": "1901.10271", "contents": "Title: Combined tract segmentation and orientation mapping for bundle-specific\n  tractography Abstract: While the major white matter tracts are of great interest to numerous studies\nin neuroscience and medicine, their manual dissection in larger cohorts from\ndiffusion MRI tractograms is time-consuming, requires expert knowledge and is\nhard to reproduce. In previous work we presented tract orientation mapping\n(TOM) as a novel concept for bundle-specific tractography. It is based on a\nlearned mapping from the original fiber orientation distribution function (FOD)\npeaks to tract specific peaks, called tract orientation maps. Each tract\norientation map represents the voxel-wise principal orientation of one tract.\nHere, we present an extension of this approach that combines TOM with accurate\nsegmentations of the tract outline and its start and end region. We also\nintroduce a custom probabilistic tracking algorithm that samples from a\nGaussian distribution with fixed standard deviation centered on each peak thus\nenabling more complete trackings on the tract orientation maps than\ndeterministic tracking. These extensions enable the automatic creation of\nbundle-specific tractograms with previously unseen accuracy. We show for 72\ndifferent bundles on high quality, low quality and phantom data that our\napproach runs faster and produces more accurate bundle-specific tractograms\nthan 7 state of the art benchmark methods while avoiding cumbersome processing\nsteps like whole brain tractography, non-linear registration, clustering or\nmanual dissection. Moreover, we show on 17 datasets that our approach\ngeneralizes well to datasets acquired with different scanners and settings as\nwell as with pathologies. The code of our method is openly available at\nhttps://github.com/MIC-DKFZ/TractSeg. \n\n"}
{"id": "1901.10415", "contents": "Title: MgNet: A Unified Framework of Multigrid and Convolutional Neural Network Abstract: We develop a unified model, known as MgNet, that simultaneously recovers some\nconvolutional neural networks (CNN) for image classification and multigrid (MG)\nmethods for solving discretized partial differential equations (PDEs). This\nmodel is based on close connections that we have observed and uncovered between\nthe CNN and MG methodologies. For example, pooling operation and feature\nextraction in CNN correspond directly to restriction operation and iterative\nsmoothers in MG, respectively. As the solution space is often the dual of the\ndata space in PDEs, the analogous concept of feature space and data space\n(which are dual to each other) is introduced in CNN. With such connections and\nnew concept in the unified model, the function of various convolution\noperations and pooling used in CNN can be better understood. As a result,\nmodified CNN models (with fewer weights and hyper parameters) are developed\nthat exhibit competitive and sometimes better performance in comparison with\nexisting CNN models when applied to both CIFAR-10 and CIFAR-100 data sets. \n\n"}
{"id": "1901.11078", "contents": "Title: Real-world Mapping of Gaze Fixations Using Instance Segmentation for\n  Road Construction Safety Applications Abstract: Research studies have shown that a large proportion of hazards remain\nunrecognized, which expose construction workers to unanticipated safety risks.\nRecent studies have also found that a strong correlation exists between viewing\npatterns of workers, captured using eye-tracking devices, and their hazard\nrecognition performance. Therefore, it is important to analyze the viewing\npatterns of workers to gain a better understanding of their hazard recognition\nperformance. This paper proposes a method that can automatically map the gaze\nfixations collected using a wearable eye-tracker to the predefined areas of\ninterests. The proposed method detects these areas or objects (i.e., hazards)\nof interests through a computer vision-based segmentation technique and\ntransfer learning. The mapped fixation data is then used to analyze the viewing\nbehaviors of workers and compute their attention distribution. The proposed\nmethod is implemented on an under construction road as a case study to evaluate\nthe performance of the proposed method. \n\n"}
{"id": "1901.11390", "contents": "Title: MONet: Unsupervised Scene Decomposition and Representation Abstract: The ability to decompose scenes in terms of abstract building blocks is\ncrucial for general intelligence. Where those basic building blocks share\nmeaningful properties, interactions and other regularities across scenes, such\ndecompositions can simplify reasoning and facilitate imagination of novel\nscenarios. In particular, representing perceptual observations in terms of\nentities should improve data efficiency and transfer performance on a wide\nrange of tasks. Thus we need models capable of discovering useful\ndecompositions of scenes by identifying units with such regularities and\nrepresenting them in a common format. To address this problem, we have\ndeveloped the Multi-Object Network (MONet). In this model, a VAE is trained\nend-to-end together with a recurrent attention network -- in a purely\nunsupervised manner -- to provide attention masks around, and reconstructions\nof, regions of images. We show that this model is capable of learning to\ndecompose and represent challenging 3D scenes into semantically meaningful\ncomponents, such as objects and background elements. \n\n"}
{"id": "cs/0506089", "contents": "Title: Field geology with a wearable computer: 1st results of the Cyborg\n  Astrobiologist System Abstract: We present results from the first geological field tests of the `Cyborg\nAstrobiologist', which is a wearable computer and video camcorder system that\nwe are using to test and train a computer-vision system towards having some of\nthe autonomous decision-making capabilities of a field-geologist. The Cyborg\nAstrobiologist platform has thus far been used for testing and development of\nthese algorithms and systems: robotic acquisition of quasi-mosaics of images,\nreal-time image segmentation, and real-time determination of interesting points\nin the image mosaics. This work is more of a test of the whole system, rather\nthan of any one part of the system. However, beyond the concept of the system\nitself, the uncommon map (despite its simplicity) is the main innovative part\nof the system. The uncommon map helps to determine interest-points in a\ncontext-free manner. Overall, the hardware and software systems function\nreliably, and the computer-vision algorithms are adequate for the first field\ntests. In addition to the proof-of-concept aspect of these field tests, the\nmain result of these field tests is the enumeration of those issues that we can\nimprove in the future, including: dealing with structural shadow and\nmicrotexture, and also, controlling the camera's zoom lens in an intelligent\nmanner. Nonetheless, despite these and other technical inadequacies, this\nCyborg Astrobiologist system, consisting of a camera-equipped wearable-computer\nand its computer-vision algorithms, has demonstrated its ability of finding\ngenuinely interesting points in real-time in the geological scenery, and then\ngathering more information about these interest points in an automated manner.\nWe use these capabilities for autonomous guidance towards geological\npoints-of-interest. \n\n"}
