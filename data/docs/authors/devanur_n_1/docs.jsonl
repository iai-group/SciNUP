{"id": "0704.1678", "contents": "Title: Settling the Complexity of Computing Two-Player Nash Equilibria Abstract: We settle a long-standing open question in algorithmic game theory. We prove\nthat Bimatrix, the problem of finding a Nash equilibrium in a two-player game,\nis complete for the complexity class PPAD Polynomial Parity Argument, Directed\nversion) introduced by Papadimitriou in 1991.\n  This is the first of a series of results concerning the complexity of Nash\nequilibria. In particular, we prove the following theorems:\n  Bimatrix does not have a fully polynomial-time approximation scheme unless\nevery problem in PPAD is solvable in polynomial time. The smoothed complexity\nof the classic Lemke-Howson algorithm and, in fact, of any algorithm for\nBimatrix is not polynomial unless every problem in PPAD is solvable in\nrandomized polynomial time. Our results demonstrate that, even in the simplest\nform of non-cooperative games, equilibrium computation and approximation are\npolynomial-time equivalent to fixed point computation. Our results also have\ntwo broad complexity implications in mathematical economics and operations\nresearch: Arrow-Debreu market equilibria are PPAD-hard to compute. The P-Matrix\nLinear Complementary Problem is computationally harder than convex programming\nunless every problem in PPAD is solvable in polynomial time. \n\n"}
{"id": "0704.2014", "contents": "Title: Extensive Games with Possibly Unaware Players Abstract: Standard game theory assumes that the structure of the game is common\nknowledge among players. We relax this assumption by considering extensive\ngames where agents may be unaware of the complete structure of the game. In\nparticular, they may not be aware of moves that they and other agents can make.\nWe show how such games can be represented; the key idea is to describe the game\nfrom the point of view of every agent at every node of the game tree. We\nprovide a generalization of Nash equilibrium and show that every game with\nawareness has a generalized Nash equilibrium. Finally, we extend these results\nto games with awareness of unawareness, where a player i may be aware that a\nplayer j can make moves that i is not aware of, and to subjective games, where\npayers may have no common knowledge regarding the actual game and their beliefs\nare incompatible with a common prior. \n\n"}
{"id": "0704.2017", "contents": "Title: Large System Analysis of Game-Theoretic Power Control in UWB Wireless\n  Networks with Rake Receivers Abstract: This paper studies the performance of partial-Rake (PRake) receivers in\nimpulse-radio ultrawideband wireless networks when an energy-efficient power\ncontrol scheme is adopted. Due to the large bandwidth of the system, the\nmultipath channel is assumed to be frequency-selective. By using noncooperative\ngame-theoretic models and large system analysis, explicit expressions are\nderived in terms of network parameters to measure the effects of self- and\nmultiple-access interference at a receiving access point. Performance of the\nPRake is compared in terms of achieved utilities and loss to that of the\nall-Rake receiver. \n\n"}
{"id": "0706.1001", "contents": "Title: Epistemic Analysis of Strategic Games with Arbitrary Strategy Sets Abstract: We provide here an epistemic analysis of arbitrary strategic games based on\nthe possibility correspondences. Such an analysis calls for the use of\ntransfinite iterations of the corresponding operators. Our approach is based on\nTarski's Fixpoint Theorem and applies both to the notions of rationalizability\nand the iterated elimination of strictly dominated strategies. \n\n"}
{"id": "0709.0435", "contents": "Title: A Generic Approach to Coalition Formation Abstract: We propose an abstract approach to coalition formation that focuses on simple\nmerge and split rules transforming partitions of a group of players. We\nidentify conditions under which every iteration of these rules yields a unique\npartition. The main conceptual tool is a specific notion of a stable partition.\nThe results are parametrized by a preference relation between partitions of a\ngroup of players and naturally apply to coalitional TU-games, hedonic games and\nexchange economy games. \n\n"}
{"id": "0711.1569", "contents": "Title: Capacity as a Fundamental Metric for Mechanism Design in the Information\n  Economy Abstract: The auction theory literature has so far focused mostly on the design of\nmechanisms that takes the revenue or the efficiency as a yardstick. However,\nscenarios where the {\\it capacity}, which we define as \\textit{``the number of\nbidders the auctioneer wants to have a positive probability of getting the\nitem''}, is a fundamental concern are ubiquitous in the information economy.\nFor instance, in sponsored search auctions (SSA's) or in online ad-exchanges,\nthe true value of an ad-slot for an advertiser is inherently derived from the\nconversion-rate, which in turn depends on whether the advertiser actually\nobtained the ad-slot or not; thus, unless the capacity of the underlying\nauction is large, key parameters, such as true valuations and\nadvertiser-specific conversion rates, will remain unknown or uncertain leading\nto inherent inefficiencies in the system. In general, the same holds true for\nall information goods/digital goods. We initiate a study of mechanisms, which\ntake capacity as a yardstick, in addition to revenue/efficiency. We show that\nin the case of a single indivisible item one simple way to incorporate capacity\nconstraints is via designing mechanisms to sell probability distributions, and\nthat under certain conditions, such optimal probability distributions could be\nidentified using a Linear programming approach. We define a quantity called\n{\\it price of capacity} to capture the tradeoff between capacity and\nrevenue/efficiency. We also study the case of sponsored search auctions.\nFinally, we discuss how general such an approach via probability spikes can be\nmade, and potential directions for future investigations. \n\n"}
{"id": "0801.1573", "contents": "Title: Taking a shower in Youth Hostels: risks and delights of heterogeneity Abstract: Tuning one's shower in some hotels may turn into a challenging coordination\ngame with imperfect information. The temperature sensitivity increases with the\nnumber of agents, making the problem possibly unlearnable. Because there is in\npractice a finite number of possible tap positions, identical agents are\nunlikely to reach even approximately their favorite water temperature. We show\nthat a population of agents with homogeneous strategies is evolutionary\nunstable, which gives insights into the emergence of heterogeneity, the latter\nbeing tempting but risky. \n\n"}
{"id": "0802.2159", "contents": "Title: A Distributed Merge and Split Algorithm for Fair Cooperation in Wireless\n  Networks Abstract: This paper introduces a novel concept from coalitional game theory which\nallows the dynamic formation of coalitions among wireless nodes. A simple and\ndistributed merge and split algorithm for coalition formation is constructed.\nThis algorithm is applied to study the gains resulting from the cooperation\namong single antenna transmitters for virtual MIMO formation. The aim is to\nfind an ultimate transmitters coalition structure that allows cooperating users\nto maximize their utilities while accounting for the cost of coalition\nformation. Through this novel game theoretical framework, the wireless network\ntransmitters are able to self-organize and form a structured network composed\nof disjoint stable coalitions. Simulation results show that the proposed\nalgorithm can improve the average individual user utility by 26.4% as well as\ncope with the mobility of the distributed users. \n\n"}
{"id": "0806.2139", "contents": "Title: Beyond Nash Equilibrium: Solution Concepts for the 21st Century Abstract: Nash equilibrium is the most commonly-used notion of equilibrium in game\ntheory. However, it suffers from numerous problems. Some are well known in the\ngame theory community; for example, the Nash equilibrium of repeated prisoner's\ndilemma is neither normatively nor descriptively reasonable. However, new\nproblems arise when considering Nash equilibrium from a computer science\nperspective: for example, Nash equilibrium is not robust (it does not tolerate\n``faulty'' or ``unexpected'' behavior), it does not deal with coalitions, it\ndoes not take computation cost into account, and it does not deal with cases\nwhere players are not aware of all aspects of the game. Solution concepts that\ntry to address these shortcomings of Nash equilibrium are discussed. \n\n"}
{"id": "0808.3746", "contents": "Title: A game-theoretic version of Oakes' example for randomized forecasting Abstract: Using the game-theoretic framework for probability, Vovk and Shafer. have\nshown that it is always possible, using randomization, to make sequential\nprobability forecasts that pass any countable set of well-behaved statistical\ntests. This result generalizes work by other authors, who consider only tests\nof calbration.\n  We complement this result with a lower bound. We show that Vovk and Shafer's\nresult is valid only when the forecasts are computed with unrestrictedly\nincreasing degree of accuracy.\n  When some level of discreteness is fixed, we present a game-theoretic\ngeneralization of Oakes' example for randomized forecasting that is a test\nfailing any given method of deferministic forecasting; originally, this example\nwas presented for deterministic calibration. \n\n"}
{"id": "0810.2861", "contents": "Title: A comparison of the notions of optimality in soft constraints and\n  graphical games Abstract: The notion of optimality naturally arises in many areas of applied\nmathematics and computer science concerned with decision making. Here we\nconsider this notion in the context of two formalisms used for different\npurposes and in different research areas: graphical games and soft constraints.\nWe relate the notion of optimality used in the area of soft constraint\nsatisfaction problems (SCSPs) to that used in graphical games, showing that for\na large class of SCSPs that includes weighted constraints every optimal\nsolution corresponds to a Nash equilibrium that is also a Pareto efficient\njoint strategy. \n\n"}
{"id": "0810.3150", "contents": "Title: Semidefinite Programming for Min-Max Problems and Games Abstract: We introduce two min-max problems: the first problem is to minimize the\nsupremum of finitely many rational functions over a compact basic\nsemi-algebraic set whereas the second problem is a 2-player zero-sum polynomial\ngame in randomized strategies and with compact basic semi-algebraic pure\nstrategy sets. It is proved that their optimal solution can be approximated by\nsolving a hierarchy of semidefinite relaxations, in the spirit of the moment\napproach developed in Lasserre. This provides a unified approach and a class of\nalgorithms to approximate all Nash equilibria and min-max strategies of many\nstatic and dynamic games. Each semidefinite relaxation can be solved in time\nwhich is polynomial in its input size and practice from global optimization\nsuggests that very often few relaxations are needed for a good approximation\n(and sometimes even finite convergence). \n\n"}
{"id": "0812.0743", "contents": "Title: A Novel Clustering Algorithm Based on Quantum Games Abstract: Enormous successes have been made by quantum algorithms during the last\ndecade. In this paper, we combine the quantum game with the problem of data\nclustering, and then develop a quantum-game-based clustering algorithm, in\nwhich data points in a dataset are considered as players who can make decisions\nand implement quantum strategies in quantum games. After each round of a\nquantum game, each player's expected payoff is calculated. Later, he uses a\nlink-removing-and-rewiring (LRR) function to change his neighbors and adjust\nthe strength of links connecting to them in order to maximize his payoff.\nFurther, algorithms are discussed and analyzed in two cases of strategies, two\npayoff matrixes and two LRR functions. Consequently, the simulation results\nhave demonstrated that data points in datasets are clustered reasonably and\nefficiently, and the clustering algorithms have fast rates of convergence.\nMoreover, the comparison with other algorithms also provides an indication of\nthe effectiveness of the proposed approach. \n\n"}
{"id": "0812.0956", "contents": "Title: EcoTRADE - a multi player network game of a tradable permit market for\n  biodiversity credits Abstract: EcoTRADE is a multi player network game of a virtual biodiversity credit\nmarket. Each player controls the land use of a certain amount of parcels on a\nvirtual landscape. The biodiversity credits of a particular parcel depend on\nneighboring parcels, which may be owned by other players. The game can be used\nto study the strategies of players in experiments or classroom games and also\nas a communication tool for stakeholders participating in credit markets that\ninclude spatially interdependent credits. \n\n"}
{"id": "0904.2540", "contents": "Title: What does Newcomb's paradox teach us? Abstract: In Newcomb's paradox you choose to receive either the contents of a\nparticular closed box, or the contents of both that closed box and another one.\nBefore you choose though, an antagonist uses a prediction algorithm to deduce\nyour choice, and fills the two boxes based on that deduction. Newcomb's paradox\nis that game theory's expected utility and dominance principles appear to\nprovide conflicting recommendations for what you should choose. A recent\nextension of game theory provides a powerful tool for resolving paradoxes\nconcerning human choice, which formulates such paradoxes in terms of Bayes\nnets. Here we apply this to ol to Newcomb's scenario. We show that the\nconflicting recommendations in Newcomb's scenario use different Bayes nets to\nrelate your choice and the algorithm's prediction. These two Bayes nets are\nincompatible. This resolves the paradox: the reason there appears to be two\nconflicting recommendations is that the specification of the underlying Bayes\nnet is open to two, conflicting interpretations. We then show that the accuracy\nof the prediction algorithm in Newcomb's paradox, the focus of much previous\nwork, is irrelevant. We similarly show that the utility functions of you and\nthe antagonist are irrelevant. We end by showing that Newcomb's paradox is\ntime-reversal invariant; both the paradox and its resolution are unchanged if\nthe algorithm makes its `prediction' \\emph{after} you make your choice rather\nthan before. \n\n"}
{"id": "0904.2541", "contents": "Title: Disproof of the Neighborhood Conjecture with Implications to SAT Abstract: We study a Maker/Breaker game described by Beck. As a result we disprove a\nconjecture of Beck on positional games, establish a connection between this\ngame and SAT and construct an unsatisfiable k-CNF formula with few occurrences\nper variable, thereby improving a previous result by Hoory and Szeider and\nshowing that the bound obtained from the Lovasz Local Lemma is tight up to a\nconstant factor. The Maker/Breaker game we study is as follows. Maker and\nBreaker take turns in choosing vertices from a given n-uniform hypergraph F,\nwith Maker going first. Maker's goal is to completely occupy a hyperedge and\nBreaker tries to avoid this. Beck conjectures that if the maximum neighborhood\nsize of F is at most 2^(n-1) then Breaker has a winning strategy. We disprove\nthis conjecture by establishing an n-uniform hypergraph with maximum\nneighborhood size 3*2^(n - 3) where Maker has a winning strategy. Moreover, we\nshow how to construct an n-uniform hypergraph with maximum degree (2^(n-1))/n\nwhere maker has a winning strategy. Finally, we establish a connection between\nSAT and the Maker/Breaker game we study. We can use this connection to derive\nnew results in SAT. Kratochvil, Savicky and Tuza showed that for every k >= 3\nthere is an integer f(k) such that every (k,f(k))-formula is satisfiable, but\n(k,f(k) + 1)-SAT is already NP-complete (it is not known whether f(k) is\ncomputable). Kratochvil, Savicky and Tuza also gave the best known lower bound\nf(k) = Omega(2^k/k), which is a consequence of the Lovasz Local Lemma. We prove\nthat, in fact, f(k) = Theta(2^k/k), improving upon the best known upper bound\nO((log k) * 2^k/k) by Hoory and Szeider. \n\n"}
{"id": "0906.3857", "contents": "Title: Games for width parameters and monotonicity Abstract: We introduce a search game for two players played on a \"scenario\" consisting\nof a ground set together with a collection of feasible partitions. This general\nsetting allows us to obtain new characterisations of many width parameters such\nas rank-width and carving-width of graphs, matroid tree-width and\nGF(4)-rank-width. We show that the monotone game variant corresponds to a tree\ndecomposition of the ground set along feasible partitions. Our framework also\ncaptures many other decompositions into \"simple\" subsets of the ground set,\nsuch as decompositions into planar subgraphs.\n  Within our general framework, we take a step towards characterising monotone\nsearch games. We exhibit a large class of \"monotone\" scenarios, i.e. of\nscenarios where the game and its monotone variant coincide. As a consequence,\ndetermining the winner is in NP for these games. This result implies\nmonotonicity for all our search games, that are equivalent to branch-width of a\nsubmodular function.\n  Finally, we include a proof showing that the matroid tree-width of a graphic\nmatroid is not larger than the tree-width of the corresponding graph. This\nproof is considerably shorter than the original proof and it is purely graph\ntheoretic. \n\n"}
{"id": "0906.4316", "contents": "Title: Constructive Decision Theory Abstract: In most contemporary approaches to decision making, a decision problem is\ndescribed by a sets of states and set of outcomes, and a rich set of acts,\nwhich are functions from states to outcomes over which the decision maker (DM)\nhas preferences. Most interesting decision problems, however, do not come with\na state space and an outcome space. Indeed, in complex problems it is often far\nfrom clear what the state and outcome spaces would be. We present an\nalternative foundation for decision making, in which the primitive objects of\nchoice are syntactic programs. A representation theorem is proved in the spirit\nof standard representation theorems, showing that if the DM's preference\nrelation on objects of choice satisfies appropriate axioms, then there exist a\nset S of states, a set O of outcomes, a way of interpreting the objects of\nchoice as functions from S to O, a probability on S, and a utility function on\nO, such that the DM prefers choice a to choice b if and only if the expected\nutility of a is higher than that of b. Thus, the state space and outcome space\nare subjective, just like the probability and utility; they are not part of the\ndescription of the problem. In principle, a modeler can test for SEU behavior\nwithout having access to states or outcomes. We illustrate the power of our\napproach by showing that it can capture decision makers who are subject to\nframing effects. \n\n"}
{"id": "0906.4321", "contents": "Title: Reasoning About Knowledge of Unawareness Revisited Abstract: In earlier work, we proposed a logic that extends the Logic of General\nAwareness of Fagin and Halpern [1988] by allowing quantification over primitive\npropositions. This makes it possible to express the fact that an agent knows\nthat there are some facts of which he is unaware. In that logic, it is not\npossible to model an agent who is uncertain about whether he is aware of all\nformulas. To overcome this problem, we keep the syntax of the earlier paper,\nbut allow models where, with each world, a possibly different language is\nassociated. We provide a sound and complete axiomatization for this logic and\nshow that, under natural assumptions, the quantifier-free fragment of the logic\nis characterized by exactly the same axioms as the logic of Heifetz, Meier, and\nSchipper [2008]. \n\n"}
{"id": "0906.4827", "contents": "Title: Physical Layer Security: Coalitional Games for Distributed Cooperation Abstract: Cooperation between wireless network nodes is a promising technique for\nimproving the physical layer security of wireless transmission, in terms of\nsecrecy capacity, in the presence of multiple eavesdroppers. While existing\nphysical layer security literature answered the question \"what are the\nlink-level secrecy capacity gains from cooperation?\", this paper attempts to\nanswer the question of \"how to achieve those gains in a practical decentralized\nwireless network and in the presence of a secrecy capacity cost for information\nexchange?\". For this purpose, we model the physical layer security cooperation\nproblem as a coalitional game with non-transferable utility and propose a\ndistributed algorithm for coalition formation. Through the proposed algorithm,\nthe wireless users can autonomously cooperate and self-organize into disjoint\nindependent coalitions, while maximizing their secrecy capacity taking into\naccount the security costs during information exchange. We analyze the\nresulting coalitional structures, discuss their properties, and study how the\nusers can self-adapt the network topology to environmental changes such as\nmobility. Simulation results show that the proposed algorithm allows the users\nto cooperate and self-organize while improving the average secrecy capacity per\nuser up to 25.32% relative to the non-cooperative case. \n\n"}
{"id": "0911.1767", "contents": "Title: A Natural Dynamics for Bargaining on Exchange Networks Abstract: Bargaining networks model the behavior of a set of players that need to reach\npairwise agreements for making profits. Nash bargaining solutions are special\noutcomes of such games that are both stable and balanced. Kleinberg and Tardos\nproved a sharp algorithmic characterization of such outcomes, but left open the\nproblem of how the actual bargaining process converges to them. A partial\nanswer was provided by Azar et al. who proposed a distributed algorithm for\nconstructing Nash bargaining solutions, but without polynomial bounds on its\nconvergence rate. In this paper, we introduce a simple and natural model for\nthis process, and study its convergence rate to Nash bargaining solutions. At\neach time step, each player proposes a deal to each of her neighbors. The\nproposal consists of a share of the potential profit in case of agreement. The\nshare is chosen to be balanced in Nash's sense as far as this is feasible (with\nrespect to the current best alternatives for both players). We prove that,\nwhenever the Nash bargaining solution is unique (and satisfies a positive gap\ncondition) this dynamics converges to it in polynomial time. Our analysis is\nbased on an approximate decoupling phenomenon between the dynamics on different\nsubstructures of the network. This approach may be of general interest for the\nanalysis of local algorithms on networks. \n\n"}
{"id": "0912.0448", "contents": "Title: The strange algebra of combinatorial games Abstract: We present an algebraic framework for the analysis of combinatorial games.\nThis framework embraces the classical theory of partizan games as well as a\nnumber of misere games, comply-constrain games, and card games that have been\nstudied more recently. It focuses on the construction of the quotient monoid of\na game, an idea that has been successively applied to several classes of games. \n\n"}
{"id": "1002.3931", "contents": "Title: Competitive Spectrum Management with Incomplete Information Abstract: This paper studies an interference interaction (game) between selfish and\nindependent wireless communication systems in the same frequency band. Each\nsystem (player) has incomplete information about the other player's channel\nconditions. A trivial Nash equilibrium point in this game is where players\nmutually full spread (FS) their transmit spectrum and interfere with each\nother. This point may lead to poor spectrum utilization from a global network\npoint of view and even for each user individually.\n  In this paper, we provide a closed form expression for a non pure-FS\nepsilon-Nash equilibrium point; i.e., an equilibrium point where players choose\nFDM for some channel realizations and FS for the others. We show that operating\nin this non pure-FS epsilon-Nash equilibrium point increases each user's\nthroughput and therefore improves the spectrum utilization, and demonstrate\nthat this performance gain can be substantial. Finally, important insights are\nprovided into the behaviour of selfish and rational wireless users as a\nfunction of the channel parameters such as fading probabilities, the\ninterference-to-signal ratio. \n\n"}
{"id": "1002.4577", "contents": "Title: Bounded Rationality, Strategy Simplification, and Equilibrium Abstract: It is frequently suggested that predictions made by game theory could be\nimproved by considering computational restrictions when modeling agents. Under\nthe supposition that players in a game may desire to balance maximization of\npayoff with minimization of strategy complexity, Rubinstein and co-authors\nstudied forms of Nash equilibrium where strategies are maximally simplified in\nthat no strategy can be further simplified without sacrificing payoff. Inspired\nby this line of work, we introduce a notion of equilibrium whereby strategies\nare also maximally simplified, but with respect to a simplification procedure\nthat is more careful in that a player will not simplify if the simplification\nincents other players to deviate. We study such equilibria in two-player\nmachine games in which players choose finite automata that succinctly represent\nstrategies for repeated games; in this context, we present techniques for\nestablishing that an outcome is at equilibrium and present results on the\nstructure of equilibria. \n\n"}
{"id": "1004.2079", "contents": "Title: Bargaining dynamics in exchange networks Abstract: We consider a one-sided assignment market or exchange network with\ntransferable utility and propose a model for the dynamics of bargaining in such\na market. Our dynamical model is local, involving iterative updates of 'offers'\nbased on estimated best alternative matches, in the spirit of pairwise Nash\nbargaining. We establish that when a balanced outcome (a generalization of the\npairwise Nash bargaining solution to networks) exists, our dynamics converges\nrapidly to such an outcome. We extend our results to the cases of (i) general\nagent 'capacity constraints', i.e., an agent may be allowed to participate in\nmultiple matches, and (ii) 'unequal bargaining powers' (where we also find a\nsurprising change in rate of convergence). \n\n"}
{"id": "1004.4317", "contents": "Title: The cooperative game theory foundations of network bargaining games Abstract: We study bargaining games between suppliers and manufacturers in a network\ncontext. Agents wish to enter into contracts in order to generate surplus which\nthen must be divided among the participants. Potential contracts and their\nsurplus are represented by weighted edges in our bipartite network. Each agent\nin the market is additionally limited by a capacity representing the number of\ncontracts which he or she may undertake. When all agents are limited to just\none contract each, prior research applied natural generalizations of the Nash\nbargaining solution to the networked setting, defined the new solution concepts\nof stable and balanced, and characterized the resulting bargaining outcomes. We\nsimplify and generalize these results to a setting in which participants in\nonly one side of the market are limited to one contract each. The heart of our\nresults uses a linear-programming formulation to establish a novel connection\nbetween well-studied cooperative game theory concepts (such as core and\nprekernel) and the solution concepts of stable and balanced defined for the\nbargaining games. This immediately implies one can take advantage of the\nresults and algorithms in cooperative game theory to reproduce results such as\nthose of Azar et al. [1] and Kleinberg and Tardos [29] and also generalize them\nto our setting. The cooperative-game-theoretic connection also inspires us to\nrefine our solution space using standard solution concepts from that literature\nsuch as nucleolus and lexicographic kernel. The nucleolus is particularly\nattractive as it is unique, always exists, and is supported by experimental\ndata in the network bargaining literature. Guided by algorithms from\ncooperative game theory, we show how to compute the nucleolus by pruning and\niteratively solving a natural linear-programming formulation. \n\n"}
{"id": "1005.5698", "contents": "Title: Normalized Range Voting Broadly Resists Control Abstract: We study the behavior of Range Voting and Normalized Range Voting with\nrespect to electoral control. Electoral control encompasses attempts from an\nelection chair to alter the structure of an election in order to change the\noutcome. We show that a voting system resists a case of control by proving that\nperforming that case of control is computationally infeasible. Range Voting is\na natural extension of approval voting, and Normalized Range Voting is a simple\nvariant which alters each vote to maximize the potential impact of each voter.\nWe show that Normalized Range Voting has among the largest number of control\nresistances among natural voting systems. \n\n"}
{"id": "1007.0571", "contents": "Title: Quickest Detection with Social Learning: Interaction of local and global\n  decision makers Abstract: We consider how local and global decision policies interact in stopping time\nproblems such as quickest time change detection. Individual agents make myopic\nlocal decisions via social learning, that is, each agent records a private\nobservation of a noisy underlying state process, selfishly optimizes its local\nutility and then broadcasts its local decision. Given these local decisions,\nhow can a global decision maker achieve quickest time change detection when the\nunderlying state changes according to a phase-type distribution? The paper\npresents four results. First, using Blackwell dominance of measures, it is\nshown that the optimal cost incurred in social learning based quickest\ndetection is always larger than that of classical quickest detection. Second,\nit is shown that in general the optimal decision policy for social learning\nbased quickest detection is characterized by multiple thresholds within the\nspace of Bayesian distributions. Third, using lattice programming and\nstochastic dominance, sufficient conditions are given for the optimal decision\npolicy to consist of a single linear hyperplane, or, more generally, a\nthreshold curve. Estimation of the optimal linear approximation to this\nthreshold curve is formulated as a simulation-based stochastic optimization\nproblem. Finally, the paper shows that in multi-agent sensor management with\nquickest detection, where each agent views the world according to its prior,\nthe optimal policy has a similar structure to social learning. \n\n"}
{"id": "1007.2694", "contents": "Title: Cache Me If You Can: Capacitated Selfish Replication Games in Networks Abstract: In Peer-to-Peer (P2P) network systems, content (object) delivery between\nnodes is often required. One way to study such a distributed system is by\ndefining games, which involve selfish nodes that make strategic choices on\nreplicating content in their local limited memory (cache) or accessing content\nfrom other nodes for a cost. These Selfish Replication games have been\nintroduced in [8] for nodes that do not have any capacity limits, leaving the\ncapacitated problem, i.e. Capacitated Selfish Replication (CSR) games, open.\n  In this work, we first form the model of the CSR games, for which we perform\na Nash equilibria analysis. In particular, we focus on hierarchical networks,\ngiven their extensive use to model communication costs of content delivery in\nP2P systems. We present an exact polynomial-time algorithm for any hierarchical\nnetwork, under two constraints on the utility functions: 1) \"Nearer is better\",\ni.e. the closest the content is to the node the less its access cost is, and 2)\n\"Independence of irrelevant alternatives\", i.e. aggregation of individual node\npreferences. This generalization represents a vast class of utilities and more\ninterestingly allows each of the nodes to have simultaneously completely\ndifferent functional forms of utility functions. In this general framework, we\npresent CSR games results on arbitrary networks and outline the boundary\nbetween intractability and effective computability in terms of the network\nstructure, object preferences, and the total number of objects. Moreover, we\nprove that the problem of equilibria existence becomes NP-hard for general CSR\ngames. \n\n"}
{"id": "1007.3801", "contents": "Title: On the Approximability of Budget Feasible Mechanisms Abstract: Budget feasible mechanisms, recently initiated by Singer (FOCS 2010), extend\nalgorithmic mechanism design problems to a realistic setting with a budget\nconstraint. We consider the problem of designing truthful budget feasible\nmechanisms for general submodular functions: we give a randomized mechanism\nwith approximation ratio $7.91$ (improving the previous best-known result 112),\nand a deterministic mechanism with approximation ratio $8.34$. Further we study\nthe knapsack problem, which is special submodular function, give a $2+\\sqrt{2}$\napproximation deterministic mechanism (improving the previous best-known result\n6), and a 3 approximation randomized mechanism. We provide a similar result for\nan extended knapsack problem with heterogeneous items, where items are divided\ninto groups and one can pick at most one item from each group.\n  Finally we show a lower bound of approximation ratio of $1+\\sqrt{2}$ for\ndeterministic mechanisms and 2 for randomized mechanisms for knapsack, as well\nas the general submodular functions. Our lower bounds are unconditional, which\ndo not rely on any computational or complexity assumptions. \n\n"}
{"id": "1008.1501", "contents": "Title: Dodgson's Rule Approximations and Absurdity Abstract: With the Dodgson rule, cloning the electorate can change the winner, which\nYoung (1977) considers an \"absurdity\". Removing this absurdity results in a new\nrule (Fishburn, 1977) for which we can compute the winner in polynomial time\n(Rothe et al., 2003), unlike the traditional Dodgson rule. We call this rule DC\nand introduce two new related rules (DR and D&). Dodgson did not explicitly\npropose the \"Dodgson rule\" (Tideman, 1987); we argue that DC and DR are better\nrealizations of the principle behind the Dodgson rule than the traditional\nDodgson rule. These rules, especially D&, are also effective approximations to\nthe traditional Dodgson's rule. We show that, unlike the rules we have\nconsidered previously, the DC, DR and D& scores differ from the Dodgson score\nby no more than a fixed amount given a fixed number of alternatives, and thus\nthese new rules converge to Dodgson under any reasonable assumption on voter\nbehaviour, including the Impartial Anonymous Culture assumption. \n\n"}
{"id": "1008.3287", "contents": "Title: A note on revelation principle from an energy perspective Abstract: The revelation principle has been known in the economics society for decades.\nIn this paper, I will investigate it from an energy perspective, i.e.,\nconsidering the energy consumed by agents and the designer in participating a\nmechanism. The main result is that when the strategies of agents are actions\nrather than messages, an additional energy condition should be added to make\nthe revelation principle hold in the real world. \n\n"}
{"id": "1008.3829", "contents": "Title: Approximate Judgement Aggregation Abstract: In this paper we analyze judgement aggregation problems in which a group of\nagents independently votes on a set of complex propositions that has some\ninterdependency constraint between them(e.g., transitivity when describing\npreferences). We consider the issue of judgement aggregation from the\nperspective of approximation. That is, we generalize the previous results by\nstudying approximate judgement aggregation. We relax the main two constraints\nassumed in the current literature, Consistency and Independence and consider\nmechanisms that only approximately satisfy these constraints, that is, satisfy\nthem up to a small portion of the inputs. The main question we raise is whether\nthe relaxation of these notions significantly alters the class of satisfying\naggregation mechanisms. The recent works for preference aggregation of Kalai,\nMossel, and Keller fit into this framework. The main result of this paper is\nthat, as in the case of preference aggregation, in the case of a subclass of a\nnatural class of aggregation problems termed `truth-functional agendas', the\nset of satisfying aggregation mechanisms does not extend non-trivially when\nrelaxing the constraints. Our proof techniques involve Boolean Fourier\ntransform and analysis of voter influences for voting protocols. The question\nwe raise for Approximate Aggregation can be stated in terms of Property\nTesting. For instance, as a corollary from our result we get a generalization\nof the classic result for property testing of linearity of Boolean functions.\n  An updated version (RePEc:huj:dispap:dp574R) is available at\nhttp://www.ratio.huji.ac.il/dp_files/dp574R.pdf \n\n"}
{"id": "1008.4220", "contents": "Title: Structured sparsity-inducing norms through submodular functions Abstract: Sparse methods for supervised learning aim at finding good linear predictors\nfrom as few variables as possible, i.e., with small cardinality of their\nsupports. This combinatorial selection problem is often turned into a convex\noptimization problem by replacing the cardinality function by its convex\nenvelope (tightest convex lower bound), in this case the L1-norm. In this\npaper, we investigate more general set-functions than the cardinality, that may\nincorporate prior knowledge or structural constraints which are common in many\napplications: namely, we show that for nondecreasing submodular set-functions,\nthe corresponding convex envelope can be obtained from its \\lova extension, a\ncommon tool in submodular analysis. This defines a family of polyhedral norms,\nfor which we provide generic algorithmic tools (subgradients and proximal\noperators) and theoretical results (conditions for support recovery or\nhigh-dimensional inference). By selecting specific submodular functions, we can\ngive a new interpretation to known norms, such as those based on\nrank-statistics or grouped norms with potentially overlapping groups; we also\ndefine new norms, in particular ones that can be used as non-factorial priors\nfor supervised learning. \n\n"}
{"id": "1008.4776", "contents": "Title: Targeting by Transnational Terrorist Groups Abstract: Many successful terrorist groups operate across international borders where\ndifferent countries host different stages of terrorist operations. Often the\nrecruits for the group come from one country or countries, while the targets of\nthe operations are in another. Stopping such attacks is difficult because\nintervention in any region or route might merely shift the terrorists\nelsewhere. Here we propose a model of transnational terrorism based on the\ntheory of activity networks. The model represents attacks on different\ncountries as paths in a network. The group is assumed to prefer paths of lowest\ncost (or risk) and maximal yield from attacks. The parameters of the model are\ncomputed for the Islamist-Salafi terrorist movement based on open source data\nand then used for estimation of risks of future attacks. The central finding is\nthat the USA has an enduring appeal as a target, due to lack of other nations\nof matching geopolitical weight or openness. It is also shown that countries in\nAfrica and Asia that have been overlooked as terrorist bases may become highly\nsignificant threats in the future. The model quantifies the dilemmas facing\ncountries in the effort to cut such networks, and points to a limitation of\ndeterrence against transnational terrorists. \n\n"}
{"id": "1009.2823", "contents": "Title: Theory and applications of lattice point methods for binomial ideals Abstract: This survey of methods surrounding lattice point methods for binomial ideals\nbegins with a leisurely treatment of the geometric combinatorics of binomial\nprimary decomposition. It then proceeds to three independent applications whose\nmotivations come from outside of commutative algebra: hypergeometric systems,\ncombinatorial game theory, and chemical dynamics. The exposition is aimed at\nstudents and researchers in algebra; it includes many examples, open problems,\nand elementary introductions to the motivations and background from outside of\nalgebra. \n\n"}
{"id": "1010.5081", "contents": "Title: Dynamics of Profit-Sharing Games Abstract: An important task in the analysis of multiagent systems is to understand how\ngroups of selfish players can form coalitions, i.e., work together in teams. In\nthis paper, we study the dynamics of coalition formation under bounded\nrationality. We consider settings where each team's profit is given by a convex\nfunction, and propose three profit-sharing schemes, each of which is based on\nthe concept of marginal utility. The agents are assumed to be myopic, i.e.,\nthey keep changing teams as long as they can increase their payoff by doing so.\nWe study the properties (such as closeness to Nash equilibrium or total profit)\nof the states that result after a polynomial number of such moves, and prove\nbounds on the price of anarchy and the price of stability of the corresponding\ngames. \n\n"}
{"id": "1011.5384", "contents": "Title: Spectrum Sharing as Spatial Congestion Games Abstract: In this paper, we present and analyze the properties of a new class of games\n- the spatial congestion game (SCG), which is a generalization of the classical\ncongestion game (CG). In a classical congestion game, multiple users share the\nsame set of resources and a user's payoff for using any resource is a function\nof the total number of users sharing it. As a potential game, this game enjoys\nsome very appealing properties, including the existence of a pure strategy Nash\nequilibrium (NE) and that every improvement path is finite and leads to such a\nNE (also called the finite improvement property or FIP). While it's tempting to\nuse this model to study spectrum sharing, it does not capture the spatial reuse\nfeature of wireless communication, where resources (interpreted as channels)\nmay be reused without increasing congestion provided that users are located far\naway from each other. This motivates us to study an extended form of the\ncongestion game where a user's payoff for using a resource is a function of the\nnumber of its interfering users sharing it. This naturally results in a spatial\ncongestion game (SCG), where users are placed over a network (or a conflict\ngraph). We study fundamental properties of a spatial congestion game; in\nparticular, we seek to answer under what conditions this game possesses the\nfinite improvement property or a Nash equilibrium. We also discuss the\nimplications of these results when applied to wireless spectrum sharing. \n\n"}
{"id": "1012.5141", "contents": "Title: Quantum Strategic Game Theory Abstract: We propose a simple yet rich model to extend the notions of Nash equilibria\nand correlated equilibria of strategic games to the quantum setting, in which\nwe then study the relations between classical and quantum equilibria. Unlike\nthe previous work that focus on qualitative questions on specific games of\nsmall sizes, we address the following fundamental and quantitative question for\ngeneral games:\n  How much \"advantage\" can playing quantum strategies provide, if any?\n  Two measures of the advantage are studied, summarized as follows.\n  1. A natural measure is the increase of payoff. We consider natural mappings\nbetween classical and quantum states, and study how well those mappings\npreserve the equilibrium properties. Among other results, we exhibit correlated\nequilibrium $p$ whose quantum superposition counterpart $\\sum_s\n\\sqrt{p(s)}\\ket{s}$ is far from being a quantum correlated equilibrium;\nactually a player can increase her payoff from almost 0 to almost 1 in a\n[0,1]-normalized game. We achieve this by a tensor product construction on\ncarefully designed base cases.\n  2. For studying the hardness of generating correlated equilibria, we propose\nto examine \\emph{correlation complexity}, a new complexity measure for\ncorrelation generation. We show that there are $n$-bit correlated equilibria\nwhich can be generated by only one EPR pair followed by local operation\n(without communication), but need at least $\\log(n)$ classical shared random\nbits plus communication. The randomized lower bound can be improved to $n$, the\nbest possible, assuming (even a much weaker version of) a recent conjecture in\nlinear algebra. We believe that the correlation complexity, as a\ncomplexity-theoretical counterpart of the celebrated Bell's inequality, has\nindependent interest in both physics and computational complexity theory and\ndeserves more explorations. \n\n"}
{"id": "1101.0340", "contents": "Title: A Round-Robin Tournament of the Iterated Prisoner's Dilemma with\n  Complete Memory-Size-Three Strategies Abstract: In this paper the results of a simulation of a prisoner's dilemma robin-round\ntournament are presented. In the tournament each participating strategy plays\nan iterated prisoner's dilemma against each other strategy (round-robin) and as\na variant also against itself. The participants of a tournament are all\nstrategies that are deterministic and have the same size of memory with regard\nto their own and their opponent's past actions: up to three most recent actions\nof their opponent and up to two most recent actions of their own. A focus is\nset on the investigation of the influence of the number of iterations, details\nof the payoff matrix, and the influence of memory size. The main result is that\nfor the tournament as carried out here, different strategies emerge as winners\nfor different payoff matrices, even for different payoff matrices being similar\njudged on if they fulfill relations T + S = P + R or 2R > T + S. As a\nconsequence of this result it is suggested that whenever the iterated\nprisoner's dilemma is used to model a real system that does not explicitly fix\nthe payoff matrix, one should check if conclusions remain valid, when a\ndifferent payoff matrix is used. \n\n"}
{"id": "1102.0203", "contents": "Title: A Primer on Strategic Games Abstract: This is a short introduction to the subject of strategic games. We focus on\nthe concepts of best response, Nash equilibrium, strict and weak dominance, and\nmixed strategies, and study the relation between these concepts in the context\nof the iterated elimination of strategies. Also, we discuss some variants of\nthe original definition of a strategic game. Finally, we introduce the basics\nof mechanism design and use pre-Bayesian games to explain it. \n\n"}
{"id": "1102.3615", "contents": "Title: Measuring Permissiveness in Parity Games: Mean-Payoff Parity Games\n  Revisited Abstract: We study nondeterministic strategies in parity games with the aim of\ncomputing a most permissive winning strategy. Following earlier work, we\nmeasure permissiveness in terms of the average number/weight of transitions\nblocked by the strategy. Using a translation into mean-payoff parity games, we\nprove that the problem of computing (the permissiveness of) a most permissive\nwinning strategy is in NP intersected coNP. Along the way, we provide a new\nstudy of mean-payoff parity games. In particular, we prove that the opponent\nplayer has a memoryless optimal strategy and give a new algorithm for solving\nthese games. \n\n"}
{"id": "1104.0458", "contents": "Title: On the Payoff Mechanisms in Peer-Assisted Services with Multiple Content\n  Providers: Rationality and Fairness Abstract: This paper studies an incentive structure for cooperation and its stability\nin peer-assisted services when there exist multiple content providers, using a\ncoalition game theoretic approach. We first consider a generalized coalition\nstructure consisting of multiple providers with many assisting peers, where\npeers assist providers to reduce the operational cost in content distribution.\nTo distribute the profit from cost reduction to players (i.e., providers and\npeers), we then establish a generalized formula for individual payoffs when a\n\"Shapley-like\" payoff mechanism is adopted. We show that the grand coalition is\nunstable, even when the operational cost functions are concave, which is in\nsharp contrast to the recently studied case of a single provider where the\ngrand coalition is stable. We also show that irrespective of stability of the\ngrand coalition, there always exist coalition structures which are not\nconvergent to the grand coalition under a dynamic among coalition structures.\nOur results give us an incontestable fact that a provider does not tend to\ncooperate with other providers in peer-assisted services, and be separated from\nthem. Three facets of the noncooperative (selfish) providers are illustrated;\n(i) underpaid peers, (ii) service monopoly, and (iii) oscillatory coalition\nstructure. Lastly, we propose a stable payoff mechanism which improves fairness\nof profit-sharing by regulating the selfishness of the players as well as\ngrants the content providers a limited right of realistic bargaining. Our study\nopens many new questions such as realistic and efficient incentive structures\nand the tradeoffs between fairness and individual providers' competition in\npeer-assisted services. \n\n"}
{"id": "1104.1227", "contents": "Title: Intervention in Power Control Games With Selfish Users Abstract: We study the power control problem in wireless ad hoc networks with selfish\nusers. Without incentive schemes, selfish users tend to transmit at their\nmaximum power levels, causing significant interference to each other. In this\npaper, we study a class of incentive schemes based on intervention to induce\nselfish users to transmit at desired power levels. An intervention scheme can\nbe implemented by introducing an intervention device that can monitor the power\nlevels of users and then transmit power to cause interference to users. We\nmainly consider first-order intervention rules based on individual transmit\npowers. We derive conditions on design parameters and the intervention\ncapability to achieve a desired outcome as a (unique) Nash equilibrium and\npropose a dynamic adjustment process that the designer can use to guide users\nand the intervention device to the desired outcome. The effect of using\nintervention rules based on aggregate receive power is also analyzed. Our\nresults show that with perfect monitoring intervention schemes can be designed\nto achieve any positive power profile while using interference from the\nintervention device only as a threat. We also analyze the case of imperfect\nmonitoring and show that a performance loss can occur. Lastly, simulation\nresults are presented to illustrate the performance improvement from using\nintervention rules and compare the performances of different intervention\nrules. \n\n"}
{"id": "1104.3055", "contents": "Title: Deciding the Value 1 Problem of Probabilistic Leaktight Automata Abstract: The value 1 problem is a decision problem for probabilistic automata over\nfinite words: given a probabilistic automaton A, are there words accepted by A\nwith probability arbitrarily close to 1? This problem was proved undecidable\nrecently. We sharpen this result, showing that the undecidability result holds\neven if the probabilistic automata have only one probabilistic transition. Our\nmain contribution is to introduce a new class of probabilistic automata, called\nleaktight automata, for which the value 1 problem is shown decidable (and\nPSPACE-complete). We construct an algorithm based on the computation of a\nmonoid abstracting the behaviours of the automaton, and rely on algebraic\ntechniques developed by Simon for the correctness proof. The class of leaktight\nautomata is decidable in PSPACE, subsumes all subclasses of probabilistic\nautomata whose value 1 problem is known to be decidable (in particular\ndeterministic automata), and is closed under two natural composition operators. \n\n"}
{"id": "1104.5070", "contents": "Title: Online Learning: Stochastic and Constrained Adversaries Abstract: Learning theory has largely focused on two main learning scenarios. The first\nis the classical statistical setting where instances are drawn i.i.d. from a\nfixed distribution and the second scenario is the online learning, completely\nadversarial scenario where adversary at every time step picks the worst\ninstance to provide the learner with. It can be argued that in the real world\nneither of these assumptions are reasonable. It is therefore important to study\nproblems with a range of assumptions on data. Unfortunately, theoretical\nresults in this area are scarce, possibly due to absence of general tools for\nanalysis. Focusing on the regret formulation, we define the minimax value of a\ngame where the adversary is restricted in his moves. The framework captures\nstochastic and non-stochastic assumptions on data. Building on the sequential\nsymmetrization approach, we define a notion of distribution-dependent\nRademacher complexity for the spectrum of problems ranging from i.i.d. to\nworst-case. The bounds let us immediately deduce variation-type bounds. We then\nconsider the i.i.d. adversary and show equivalence of online and batch\nlearnability. In the supervised setting, we consider various hybrid assumptions\non the way that x and y variables are chosen. Finally, we consider smoothed\nlearning problems and show that half-spaces are online learnable in the\nsmoothed model. In fact, exponentially small noise added to adversary's\ndecisions turns this problem with infinite Littlestone's dimension into a\nlearnable problem. \n\n"}
{"id": "1105.0558", "contents": "Title: If more than Analytical Modeling is Needed to Predict Real Agents'\n  Strategic Interaction Abstract: This paper presents the research on the interdisciplinary research\ninfrastructure for understanding human reasoning in game-theoretic terms.\nStrategic reasoning is considered to impact human decision making in social,\neconomical and competitive interactions. The provided introduction explains and\nconnects concepts from AI, game theory and psychology. First result is a\nconcept of interdisciplinary game description language as a part of the focused\ninterdisciplinary research infrastructure. The need of this domain-specific\nlanguage is motivated and is aimed to accelerate the current developments. As\nsecond result, the paper provides a summary of ongoing research and its\nsignificance. \n\n"}
{"id": "1105.2470", "contents": "Title: The game of go as a complex network Abstract: We study the game of go from a complex network perspective. We construct a\ndirected network using a suitable definition of tactical moves including local\npatterns, and study this network for different datasets of professional\ntournaments and amateur games. The move distribution follows Zipf's law and the\nnetwork is scale free, with statistical peculiarities different from other real\ndirected networks, such as e. g. the World Wide Web. These specificities\nreflect in the outcome of ranking algorithms applied to it. The fine study of\nthe eigenvalues and eigenvectors of matrices used by the ranking algorithms\nsingles out certain strategic situations. Our results should pave the way to a\nbetter modelization of board games and other types of human strategic scheming. \n\n"}
{"id": "1107.2994", "contents": "Title: Budget Feasible Mechanism Design via Random Sampling Abstract: Budget feasible mechanism considers algorithmic mechanism design questions\nwhere there is a budget constraint on the total payment of the mechanism. An\nimportant question in the field is that under which valuation domains there\nexist budget feasible mechanisms that admit `small' approximations (compared to\na socially optimal solution). Singer \\cite{PS10} showed that additive and\nsubmodular functions admit a constant approximation mechanism. Recently,\nDobzinski, Papadimitriou, and Singer \\cite{DPS11} gave an $O(\\log^2n)$\napproximation mechanism for subadditive functions and remarked that: \"A\nfundamental question is whether, regardless of computational constraints, a\nconstant-factor budget feasible mechanism exists for subadditive function.\"\n  In this paper, we give the first attempt to this question. We give a\npolynomial time $O(\\frac{\\log n}{\\log\\log n})$ sub-logarithmic approximation\nratio mechanism for subadditive functions, improving the best known ratio\n$O(\\log^2 n)$. Further, we connect budget feasible mechanism design to the\nconcept of approximate core in cooperative game theory, and show that there is\na mechanism for subadditive functions whose approximation is, via a\ncharacterization of the integrality gap of a linear program, linear to the\nlargest value to which an approximate core exists. Our result implies in\nparticular that the class of XOS functions, which is a superclass of submodular\nfunctions, admits a constant approximation mechanism. We believe that our work\ncould be a solid step towards solving the above fundamental problem eventually,\nand possibly, with an affirmative answer. \n\n"}
{"id": "1107.4935", "contents": "Title: Public Announcement Logic in Geometric Frameworks Abstract: In this paper we introduce public announcement logic in different geometric\nframeworks. First, we consider topological models, and then extend our\ndiscussion to a more expressive model, namely, subset space models.\nFurthermore, we prove the completeness of public announcement logic in those\nframeworks. Moreover, we apply our results to different issues: announcement\nstabilization, backward induction and persistence. \n\n"}
{"id": "1107.5354", "contents": "Title: Replicator Dynamics of Co-Evolving Networks Abstract: We propose a simple model of network co-evolution in a game-dynamical system\nof interacting agents that play repeated games with their neighbors, and adapt\ntheir behaviors and network links based on the outcome of those games. The\nadaptation is achieved through a simple reinforcement learning scheme. We show\nthat the collective evolution of such a system can be described by\nappropriately defined replicator dynamics equations. In particular, we suggest\nan appropriate factorization of the agents' strategies that results in a\ncoupled system of equations characterizing the evolution of both strategies and\nnetwork structure, and illustrate the framework on two simple examples. \n\n"}
{"id": "1108.5025", "contents": "Title: Robust Stackelberg game in communication systems Abstract: This paper studies multi-user communication systems with two groups of users:\nleaders which possess system information, and followers which have no system\ninformation using the formulation of Stackelberg games. In such games, the\nleaders play and choose their actions based on their information about the\nsystem and the followers choose their actions myopically according to their\nobservations of the aggregate impact of other users. However, obtaining the\nexact value of these parameters is not practical in communication systems. To\nstudy the effect of uncertainty and preserve the players' utilities in these\nconditions, we introduce a robust equilibrium for Stackelberg games. In this\nframework, the leaders' information and the followers' observations are\nuncertain parameters, and the leaders and the followers choose their actions by\nsolving the worst-case robust optimizations. We show that the followers'\nuncertain parameters always increase the leaders' utilities and decrease the\nfollowers' utilities. Conversely, the leaders' uncertain information reduces\nthe leaders' utilities and increases the followers' utilities. We illustrate\nour theoretical results with the numerical results obtained based on the power\ncontrol games in the interference channels. \n\n"}
{"id": "1109.4250", "contents": "Title: Complex dynamics in learning complicated games Abstract: Game theory is the standard tool used to model strategic interactions in\nevolutionary biology and social science. Traditional game theory studies the\nequilibria of simple games. But is traditional game theory applicable if the\ngame is complicated, and if not, what is? We investigate this question here,\ndefining a complicated game as one with many possible moves, and therefore many\npossible payoffs conditional on those moves. We investigate two-person games in\nwhich the players learn based on experience. By generating games at random we\nshow that under some circumstances the strategies of the two players converge\nto fixed points, but under others they follow limit cycles or chaotic\nattractors. The dimension of the chaotic attractors can be very high, implying\nthat the dynamics of the strategies are effectively random. In the chaotic\nregime the payoffs fluctuate intermittently, showing bursts of rapid change\npunctuated by periods of quiescence, similar to what is observed in fluid\nturbulence and financial markets. Our results suggest that such intermittency\nis a highly generic phenomenon, and that there is a large parameter regime for\nwhich complicated strategic interactions generate inherently unpredictable\nbehavior that is best described in the language of dynamical systems theory \n\n"}
{"id": "1110.1785", "contents": "Title: Voting with Limited Information and Many Alternatives Abstract: The traditional axiomatic approach to voting is motivated by the problem of\nreconciling differences in subjective preferences. In contrast, a dominant line\nof work in the theory of voting over the past 15 years has considered a\ndifferent kind of scenario, also fundamental to voting, in which there is a\ngenuinely \"best\" outcome that voters would agree on if they only had enough\ninformation. This type of scenario has its roots in the classical Condorcet\nJury Theorem; it includes cases such as jurors in a criminal trial who all want\nto reach the correct verdict but disagree in their inferences from the\navailable evidence, or a corporate board of directors who all want to improve\nthe company's revenue, but who have different information that favors different\noptions.\n  This style of voting leads to a natural set of questions: each voter has a\n{\\em private signal} that provides probabilistic information about which option\nis best, and a central question is whether a simple plurality voting system,\nwhich tabulates votes for different options, can cause the group decision to\narrive at the correct option. We show that plurality voting is powerful enough\nto achieve this: there is a way for voters to map their signals into votes for\noptions in such a way that --- with sufficiently many voters --- the correct\noption receives the greatest number of votes with high probability. We show\nfurther, however, that any process for achieving this is inherently expensive\nin the number of voters it requires: succeeding in identifying the correct\noption with probability at least $1 - \\eta$ requires $\\Omega(n^3 \\epsilon^{-2}\n\\log \\eta^{-1})$ voters, where $n$ is the number of options and $\\epsilon$ is a\ndistributional measure of the minimum difference between the options. \n\n"}
{"id": "1111.2456", "contents": "Title: Repeated Games With Intervention: Theory and Applications in\n  Communications Abstract: In communication systems where users share common resources, users' selfish\nbehavior usually results in suboptimal resource utilization. There have been\nextensive works that model communication systems with selfish users as one-shot\ngames and propose incentive schemes to achieve Pareto optimal action profiles\nas non-cooperative equilibria. However, in many communication systems, due to\nstrong negative externalities among users, the sets of feasible payoffs in\none-shot games are nonconvex. Thus, it is possible to expand the set of\nfeasible payoffs by having users choose convex combinations of different\npayoffs. In this paper, we propose a repeated game model generalized by\nintervention. First, we use repeated games to convexify the set of feasible\npayoffs in one-shot games. Second, we combine conventional repeated games with\nintervention, originally proposed for one-shot games, to achieve a larger set\nof equilibrium payoffs and loosen requirements for users' patience to achieve\nit. We study the problem of maximizing a welfare function defined on users'\nequilibrium payoffs, subject to minimum payoff guarantees. Given the optimal\nequilibrium payoff, we derive the minimum intervention capability required and\ndesign corresponding equilibrium strategies. The proposed generalized repeated\ngame model applies to various communication systems, such as power control and\nflow control. \n\n"}
{"id": "1111.2885", "contents": "Title: Privacy Auctions for Recommender Systems Abstract: We study a market for private data in which a data analyst publicly releases\na statistic over a database of private information. Individuals that own the\ndata incur a cost for their loss of privacy proportional to the differential\nprivacy guarantee given by the analyst at the time of the release. The analyst\nincentivizes individuals by compensating them, giving rise to a \\emph{privacy\nauction}. Motivated by recommender systems, the statistic we consider is a\nlinear predictor function with publicly known weights. The statistic can be\nviewed as a prediction of the unknown data of a new individual, based on the\ndata of individuals in the database. We formalize the trade-off between privacy\nand accuracy in this setting, and show that a simple class of estimates\nachieves an order-optimal trade-off. It thus suffices to focus on auction\nmechanisms that output such estimates. We use this observation to design a\ntruthful, individually rational, proportional-purchase mechanism under a fixed\nbudget constraint. We show that our mechanism is 5-approximate in terms of\naccuracy compared to the optimal mechanism, and that no truthful mechanism can\nachieve a $2-\\varepsilon$ approximation, for any $\\varepsilon > 0$. \n\n"}
{"id": "1111.7299", "contents": "Title: Les crashs sont rationnels Abstract: As we show by using notions of equilibrium in infinite sequential games,\ncrashes or financial escalations are rational for economic or environmental\nagents, who have a vision of an infinite world. This contradicts a picture of a\nself-regulating, wise and pacific economic world. In other words, in this\ncontext, equilibrium is not synonymous of stability. We try to draw, from this\nstatement, methodological consequences and new ways of thinking, especially in\neconomic game theory. Among those new paths, coinduction is the basis of our\nreasoning in infinite games. \n\n"}
{"id": "1112.1185", "contents": "Title: Rationality and Escalation in Infinite Extensive Games Abstract: The aim of this of this paper is to study infinite games and to prove\nformally some properties in this framework. As a consequence we show that the\nbehavior (the madness) of people which leads to speculative crashes or\nescalation can be fully rational. Indeed it proceeds from the statement that\nresources are infinite. The reasoning is based on the concept of coinduction\nconceived by computer scientists to model infinite computations and used by\neconomic agents unknowingly. When used consciously, this concept is not as\nsimple as induction and we could paraphrase Newton: \"Modeling the madness of\npeople is more difficult than modeling the motion of planets\". \n\n"}
{"id": "1112.3330", "contents": "Title: Quantum strategies are better than classical in almost any XOR game Abstract: We initiate a study of random instances of nonlocal games. We show that\nquantum strategies are better than classical for almost any 2-player XOR game.\nMore precisely, for large n, the entangled value of a random 2-player XOR game\nwith n questions to every player is at least 1.21... times the classical value,\nfor 1-o(1) fraction of all 2-player XOR games. \n\n"}
{"id": "1112.6361", "contents": "Title: On Multiple Round Sponsored Search Auctions with Budgets Abstract: In a sponsored search auction the advertisement slots on a search result page\nare generally ordered by click-through rate. Bidders have a valuation, which is\nusually assumed to be linear in the click-through rate, a budget constraint,\nand receive at most one slot per search result page (round). We study\nmulti-round sponsored search auctions, where the different rounds are linked\nthrough the budget constraints of the bidders and the valuation of a bidder for\nall rounds is the sum of the valuations for the individual rounds. All\nmechanisms published so far either study one-round sponsored search auctions or\nthe setting where every round has only one slot and all slots have the same\nclick-through rate, which is identical to a multi-item auction.\n  This paper contains the following three results: (1) We give the first\nmechanism for the multi-round sponsored search problem where different slots\nhave different click-through rates. Our mechanism is incentive compatible in\nexpectation, individually rational in expectation, Pareto optimal in\nexpectation, and also ex-post Pareto optimal for each realized outcome. (2)\nAdditionally we study the combinatorial setting, where each bidder is only\ninterested in a subset of the rounds. We give a deterministic, incentive\ncompatible, individually rational, and Pareto optimal mechanism for the setting\nwhere all slots have the same click-through rate. (3) We present an\nimpossibility result for auctions where bidders have diminishing marginal\nvaluations. Specifically, we show that even for the multi-unit (one slot per\nround) setting there is no incentive compatible, individually rational, and\nPareto optimal mechanism for private diminishing marginal valuations and public\nbudgets. \n\n"}
{"id": "1201.0946", "contents": "Title: Cops and Invisible Robbers: the Cost of Drunkenness Abstract: We examine a version of the Cops and Robber (CR) game in which the robber is\ninvisible, i.e., the cops do not know his location until they capture him.\nApparently this game (CiR) has received little attention in the CR literature.\nWe examine two variants: in the first the robber is adversarial (he actively\ntries to avoid capture); in the second he is drunk (he performs a random walk).\nOur goal in this paper is to study the invisible Cost of Drunkenness (iCOD),\nwhich is defined as the ratio ct_i(G)/dct_i(G), with ct_i(G) and dct_i(G) being\nthe expected capture times in the adversarial and drunk CiR variants,\nrespectively. We show that these capture times are well defined, using game\ntheory for the adversarial case and partially observable Markov decision\nprocesses (POMDP) for the drunk case. We give exact asymptotic values of iCOD\nfor several special graph families such as $d$-regular trees, give some bounds\nfor grids, and provide general upper and lower bounds for general classes of\ngraphs. We also give an infinite family of graphs showing that iCOD can be\narbitrarily close to any value in [2,infinty). Finally, we briefly examine one\nmore CiR variant, in which the robber is invisible and \"infinitely fast\"; we\nargue that this variant is significantly different from the Graph Search game,\ndespite several similarities between the two games. \n\n"}
{"id": "1201.6429", "contents": "Title: Bounding the inefficiency of outcomes in generalized second price\n  auctions Abstract: The Generalized Second Price (GSP) auction is the primary auction used for\nmonetizing the use of the Internet. It is well-known that truthtelling is not a\ndominant strategy in this auction and that inefficient equilibria can arise. In\nthis paper we study the space of equilibria in GSP, and quantify the efficiency\nloss that can arise in equilibria under a wide range of sources of uncertainty,\nas well as in the full information setting. The traditional Bayesian game\nmodels uncertainty in the valuations (types) of the participants. The\nGeneralized Second Price (GSP) auction gives rise to a further form of\nuncertainty: the selection of quality factors resulting in uncertainty about\nthe behavior of the underlying ad allocation algorithm. The bounds we obtain\napply to both forms of uncertainty, and are robust in the sense that they apply\nunder various perturbations of the solution concept, extending to models with\ninformation asymmetries and bounded rationality in the form of learning\nstrategies.\n  We present a constant bound (2.927) on the factor of the efficiency loss\n(\\emph{price of anarchy}) of the corresponding game for the Bayesian model of\npartial information about other participants and about ad quality factors. For\nthe full information setting, we prove a surprisingly low upper bound of 1.282\non the price of anarchy over pure Nash equilibria, nearly matching a lower\nbound of 1.259 for the case of three advertisers. Further, we do not require\nthat the system reaches equilibrium, and give similarly low bounds also on the\nquality degradation for any no-regret learning outcome. Our conclusion is that\nthe number of advertisers in the auction has almost no impact on the price of\nanarchy, and that the efficiency of GSP is very robust with respect to the\nbelief and rationality assumptions imposed on the participants. \n\n"}
{"id": "1202.1089", "contents": "Title: Bargaining Dynamics in Exchange Networks Abstract: We consider a dynamical system for computing Nash bargaining solutions on\ngraphs and focus on its rate of convergence. More precisely, we analyze the\nedge-balanced dynamical system by Azar et al and fully specify its convergence\nfor an important class of elementary graph structures that arise in Kleinberg\nand Tardos' procedure for computing a Nash bargaining solution on general\ngraphs. We show that all these dynamical systems are either linear or\neventually become linear and that their convergence times are quadratic in the\nnumber of matched edges. \n\n"}
{"id": "1202.2789", "contents": "Title: The Computational Complexity of Truthfulness in Combinatorial Auctions Abstract: One of the fundamental questions of Algorithmic Mechanism Design is whether\nthere exists an inherent clash between truthfulness and computational\ntractability: in particular, whether polynomial-time truthful mechanisms for\ncombinatorial auctions are provably weaker in terms of approximation ratio than\nnon-truthful ones. This question was very recently answered for universally\ntruthful mechanisms for combinatorial auctions \\cite{D11}, and even for\ntruthful-in-expectation mechanisms \\cite{DughmiV11}. However, both of these\nresults are based on information-theoretic arguments for valuations given by a\nvalue oracle, and leave open the possibility of polynomial-time truthful\nmechanisms for succinctly described classes of valuations.\n  This paper is the first to prove {\\em computational hardness} results for\ntruthful mechanisms for combinatorial auctions with succinctly described\nvaluations. We prove that there is a class of succinctly represented submodular\nvaluations for which no deterministic truthful mechanism provides an\n$m^{1/2-\\epsilon}$-approximation for a constant $\\epsilon>0$, unless $NP=RP$\n($m$ denotes the number of items). Furthermore, we prove that even\ntruthful-in-expectation mechanisms cannot approximate combinatorial auctions\nwith certain succinctly described submodular valuations better than within\n$n^\\gamma$, where $n$ is the number of bidders and $\\gamma>0$ some absolute\nconstant, unless $NP \\subseteq P/poly$. In addition, we prove computational\nhardness results for two related problems. \n\n"}
{"id": "1202.5025", "contents": "Title: The Price of Anarchy for Network Formation in an Adversary Model Abstract: We study network formation with n players and link cost \\alpha > 0. After the\nnetwork is built, an adversary randomly deletes one link according to a certain\nprobability distribution. Cost for player v incorporates the expected number of\nplayers to which v will become disconnected. We show existence of equilibria\nand a price of stability of 1+o(1) under moderate assumptions on the adversary\nand n \\geq 9.\n  As the main result, we prove bounds on the price of anarchy for two special\nadversaries: one removes a link chosen uniformly at random, while the other\nremoves a link that causes a maximum number of player pairs to be separated.\nFor unilateral link formation we show a bound of O(1) on the price of anarchy\nfor both adversaries, the constant being bounded by 10+o(1) and 8+o(1),\nrespectively. For bilateral link formation we show O(1+\\sqrt{n/\\alpha}) for one\nadversary (if \\alpha > 1/2), and \\Theta(n) for the other (if \\alpha > 2\nconsidered constant and n \\geq 9). The latter is the worst that can happen for\nany adversary in this model (if \\alpha = \\Omega(1)). This points out\nsubstantial differences between unilateral and bilateral link formation. \n\n"}
{"id": "1203.0411", "contents": "Title: The Complexity of Online Voter Control in Sequential Elections Abstract: Previous work on voter control, which refers to situations where a chair\nseeks to change the outcome of an election by deleting, adding, or partitioning\nvoters, takes for granted that the chair knows all the voters' preferences and\nthat all votes are cast simultaneously. However, elections are often held\nsequentially and the chair thus knows only the previously cast votes and not\nthe future ones, yet needs to decide instantaneously which control action to\ntake. We introduce a framework that models online voter control in sequential\nelections. We show that the related problems can be much harder than in the\nstandard (non-online) case: For certain election systems, even with efficient\nwinner problems, online control by deleting, adding, or partitioning voters is\nPSPACE-complete, even if there are only two candidates. In addition, we obtain\n(by a new characterization of coNP in terms of weight-bounded alternating\nTuring machines) completeness for coNP in the deleting/adding cases with a\nbounded deletion/addition limit, and we obtain completeness for NP in the\npartition cases with an additional restriction. We also show that for\nplurality, online control by deleting or adding voters is in P, and for\npartitioning voters is coNP-hard. \n\n"}
{"id": "1203.3368", "contents": "Title: Between Arrow and Gibbard-Satterthwaite; A representation theoretic\n  approach Abstract: A central theme in social choice theory is that of impossibility theorems,\nsuch as Arrow's theorem and the Gibbard-Satterthwaite theorem, which state that\nunder certain natural constraints, social choice mechanisms are impossible to\nconstruct. In recent years, beginning in Kalai`01, much work has been done in\nfinding \\textit{robust} versions of these theorems, showing \"approximate\"\nimpossibility remains even when most, but not all, of the constraints are\nsatisfied. We study a spectrum of settings between the case where society\nchooses a single outcome (\\'a-la-Gibbard-Satterthwaite) and the choice of a\ncomplete order (as in Arrow's theorem). We use algebraic techniques,\nspecifically representation theory of the symmetric group, and also prove\nrobust versions of the theorems that we state. Our relaxations of the\nconstraints involve relaxing of a version of \"independence of irrelevant\nalternatives\", rather than relaxing the demand of a transitive outcome, as is\ndone in most other robustness results. \n\n"}
{"id": "1203.3935", "contents": "Title: Distributed Cooperative Q-learning for Power Allocation in Cognitive\n  Femtocell Networks Abstract: In this paper, we propose a distributed reinforcement learning (RL) technique\ncalled distributed power control using Q-learning (DPC-Q) to manage the\ninterference caused by the femtocells on macro-users in the downlink. The DPC-Q\nleverages Q-Learning to identify the sub-optimal pattern of power allocation,\nwhich strives to maximize femtocell capacity, while guaranteeing macrocell\ncapacity level in an underlay cognitive setting. We propose two different\napproaches for the DPC-Q algorithm: namely, independent, and cooperative. In\nthe former, femtocells learn independently from each other while in the latter,\nfemtocells share some information during learning in order to enhance their\nperformance. Simulation results show that the independent approach is capable\nof mitigating the interference generated by the femtocells on macro-users.\nMoreover, the results show that cooperation enhances the performance of the\nfemtocells in terms of speed of convergence, fairness and aggregate femtocell\ncapacity. \n\n"}
{"id": "1203.4523", "contents": "Title: On the Equivalence between Herding and Conditional Gradient Algorithms Abstract: We show that the herding procedure of Welling (2009) takes exactly the form\nof a standard convex optimization algorithm--namely a conditional gradient\nalgorithm minimizing a quadratic moment discrepancy. This link enables us to\ninvoke convergence results from convex optimization and to consider faster\nalternatives for the task of approximating integrals in a reproducing kernel\nHilbert space. We study the behavior of the different variants through\nnumerical simulations. The experiments indicate that while we can improve over\nherding on the task of approximating integrals, the original herding algorithm\ntends to approach more often the maximum entropy distribution, shedding more\nlight on the learning bias behind herding. \n\n"}
{"id": "1204.3283", "contents": "Title: Automated synthesis of reliable and efficient systems through game\n  theory: a case study Abstract: Reactive computer systems bear inherent complexity due to continuous\ninteractions with their environment. While this environment often proves to be\nuncontrollable, we still want to ensure that critical computer systems will not\nfail, no matter what they face. Examples are legion: railway traffic, power\nplants, plane navigation systems, etc. Formal verification of a system may\nensure that it satisfies a given specification, but only applies to an already\nexisting model of a system. In this work, we address the problem of synthesis:\nstarting from a specification of the desired behavior, we show how to build a\nsuitable system controller that will enforce this specification. In particular,\nwe discuss recent developments of that approach for systems that must ensure\nBoolean behaviors (e.g., reachability, liveness) along with quantitative\nrequirements over their execution (e.g., never drop out of fuel, ensure a\nsuitable mean response time). We notably illustrate a powerful, practically\nuseable algorithm for the automated synthesis of provably safe reactive\nsystems. \n\n"}
{"id": "1204.4145", "contents": "Title: Learning From An Optimization Viewpoint Abstract: In this dissertation we study statistical and online learning problems from\nan optimization viewpoint.The dissertation is divided into two parts :\n  I. We first consider the question of learnability for statistical learning\nproblems in the general learning setting. The question of learnability is well\nstudied and fully characterized for binary classification and for real valued\nsupervised learning problems using the theory of uniform convergence. However\nwe show that for the general learning setting uniform convergence theory fails\nto characterize learnability. To fill this void we use stability of learning\nalgorithms to fully characterize statistical learnability in the general\nsetting. Next we consider the problem of online learning. Unlike the\nstatistical learning framework there is a dearth of generic tools that can be\nused to establish learnability and rates for online learning problems in\ngeneral. We provide online analogs to classical tools from statistical learning\ntheory like Rademacher complexity, covering numbers, etc. We further use these\ntools to fully characterize learnability for online supervised learning\nproblems.\n  II. In the second part, for general classes of convex learning problems, we\nprovide appropriate mirror descent (MD) updates for online and statistical\nlearning of these problems. Further, we show that the the MD is near optimal\nfor online convex learning and for most cases, is also near optimal for\nstatistical convex learning. We next consider the problem of convex\noptimization and show that oracle complexity can be lower bounded by the so\ncalled fat-shattering dimension of the associated linear class. Thus we\nestablish a strong connection between offline convex optimization problems and\nstatistical learning problems. We also show that for a large class of high\ndimensional optimization problems, MD is in fact near optimal even for convex\noptimization. \n\n"}
{"id": "1204.5213", "contents": "Title: Solving Weighted Voting Game Design Problems Optimally: Representations,\n  Synthesis, and Enumeration Abstract: We study the inverse power index problem for weighted voting games: the\nproblem of finding a weighted voting game in which the power of the players is\nas close as possible to a certain target distribution. Our goal is to find\nalgorithms that solve this problem exactly. Thereto, we study various\nsubclasses of simple games, and their associated representation methods. We\nsurvey algorithms and impossibility results for the synthesis problem, i.e.,\nconverting a representation of a simple game into another representation.\n  We contribute to the synthesis problem by showing that it is impossible to\ncompute in polynomial time the list of ceiling coalitions (also known as\nshift-maximal losing coalitions) of a game from its list of roof coalitions\n(also known as shift-minimal winning coalitions), and vice versa.\n  Then, we proceed by studying the problem of enumerating the set of weighted\nvoting games. We present first a naive algorithm for this, running in doubly\nexponential time. Using our knowledge of the synthesis problem, we then improve\non this naive algorithm, and we obtain an enumeration algorithm that runs in\nquadratic exponential time (that is, O(2^(n^2) p(n)) for a polynomial p).\nMoreover, we show that this algorithm runs in output-polynomial time, making it\nthe best possible enumeration algorithm up to a polynomial factor.\n  Finally, we propose an exact anytime algorithm for the inverse power index\nproblem that runs in exponential time. This algorithm is straightforward and\ngeneral: it computes the error for each game enumerated, and outputs the game\nthat minimizes this error. By the genericity of our approach, our algorithm can\nbe used to find a weighted voting game that optimizes any exponential time\ncomputable function. We implement our algorithm for the case of the normalized\nBanzhaf index, and we perform experiments in order to study performance and\nerror convergence. \n\n"}
{"id": "1204.5249", "contents": "Title: Does Parrondo Paradox occur in Scale Free Networks? -A simple\n  Consideration- Abstract: Parrondo's paradox occurs in sequences of games in which a winning\nexpectation may be obtained by playing the games in a random order, even though\neach game in the sequence may be lost when played individually. Several\nvariations of Parrondo's games apparently with paradoxical property have been\nintroduced; history dependence, one dimensional line, two dimensional lattice\nand so on. In this article, we examine whether Parrondo's paradox occurs or not\nin scale free networks. This is interesting as an empirical study, since scale\nfree networks are ubiquitous in our real world. First some simulation results\nare given and after that theoretical studies are made. As a result, we mostly\nconfirm that Parrondo's paradox can not occur in the naive case, where the game\nhas the same number of parameters as the original Parrondo's game. \n\n"}
{"id": "1204.6552", "contents": "Title: A Game-Theoretic Model Motivated by the DARPA Network Challenge Abstract: In this paper we propose a game-theoretic model to analyze events similar to\nthe 2009 \\emph{DARPA Network Challenge}, which was organized by the Defense\nAdvanced Research Projects Agency (DARPA) for exploring the roles that the\nInternet and social networks play in incentivizing wide-area collaborations.\nThe challenge was to form a group that would be the first to find the locations\nof ten moored weather balloons across the United States. We consider a model in\nwhich $N$ people (who can form groups) are located in some topology with a\nfixed coverage volume around each person's geographical location. We consider\nvarious topologies where the players can be located such as the Euclidean\n$d$-dimension space and the vertices of a graph. A balloon is placed in the\nspace and a group wins if it is the first one to report the location of the\nballoon. A larger team has a higher probability of finding the balloon, but we\nassume that the prize money is divided equally among the team members. Hence\nthere is a competing tension to keep teams as small as possible.\n  \\emph{Risk aversion} is the reluctance of a person to accept a bargain with\nan uncertain payoff rather than another bargain with a more certain, but\npossibly lower, expected payoff. In our model we consider the \\emph{isoelastic}\nutility function derived from the Arrow-Pratt measure of relative risk\naversion. The main aim is to analyze the structures of the groups in Nash\nequilibria for our model. For the $d$-dimensional Euclidean space ($d\\geq 1$)\nand the class of bounded degree regular graphs we show that in any Nash\nEquilibrium the \\emph{richest} group (having maximum expected utility per\nperson) covers a constant fraction of the total volume. \n\n"}
{"id": "1205.2074", "contents": "Title: A Smooth Transition from Powerlessness to Absolute Power Abstract: We study the phase transition of the coalitional manipulation problem for\ngeneralized scoring rules. Previously it has been shown that, under some\nconditions on the distribution of votes, if the number of manipulators is\n$o(\\sqrt{n})$, where $n$ is the number of voters, then the probability that a\nrandom profile is manipulable by the coalition goes to zero as the number of\nvoters goes to infinity, whereas if the number of manipulators is\n$\\omega(\\sqrt{n})$, then the probability that a random profile is manipulable\ngoes to one. Here we consider the critical window, where a coalition has size\n$c\\sqrt{n}$, and we show that as $c$ goes from zero to infinity, the limiting\nprobability that a random profile is manipulable goes from zero to one in a\nsmooth fashion, i.e., there is a smooth phase transition between the two\nregimes. This result analytically validates recent empirical results, and\nsuggests that deciding the coalitional manipulation problem may be of limited\ncomputational hardness in practice. \n\n"}
{"id": "1205.2152", "contents": "Title: Roughly Weighted Hierarchical Simple Games Abstract: Hierarchical simple games - both disjunctive and conjunctive - are natural\ngeneralizations of simple majority games. They take their origin in the theory\nof secret sharing. Another important generalization of simple majority games\nwith origin in economics and politics are weighted and roughly weighted\nmajority games. In this paper we characterize roughly weighted hierarchical\ngames identifying where the two approaches coincide. \n\n"}
{"id": "1205.6346", "contents": "Title: On (Subgame Perfect) Secure Equilibrium in Quantitative Reachability\n  Games Abstract: We study turn-based quantitative multiplayer non zero-sum games played on\nfinite graphs with reachability objectives. In such games, each player aims at\nreaching his own goal set of states as soon as possible. A previous work on\nthis model showed that Nash equilibria (resp. secure equilibria) are guaranteed\nto exist in the multiplayer (resp. two-player) case. The existence of secure\nequilibria in the multiplayer case remained and is still an open problem. In\nthis paper, we focus our study on the concept of subgame perfect equilibrium, a\nrefinement of Nash equilibrium well-suited in the framework of games played on\ngraphs. We also introduce the new concept of subgame perfect secure\nequilibrium. We prove the existence of subgame perfect equilibria (resp.\nsubgame perfect secure equilibria) in multiplayer (resp. two-player)\nquantitative reachability games. Moreover, we provide an algorithm deciding the\nexistence of secure equilibria in the multiplayer case. \n\n"}
{"id": "1206.0981", "contents": "Title: An Informed Model of Personal Information Release in Social Networking\n  Sites Abstract: The emergence of online social networks and the growing popularity of digital\ncommunication has resulted in an increasingly amount of information about\nindividuals available on the Internet. Social network users are given the\nfreedom to create complex digital identities, and enrich them with truthful or\neven fake personal information. However, this freedom has led to serious\nsecurity and privacy incidents, due to the role users' identities play in\nestablishing social and privacy settings.\n  In this paper, we take a step toward a better understanding of online\ninformation exposure. Based on the detailed analysis of a sample of real-world\ndata, we develop a deception model for online users. The model uses a game\ntheoretic approach to characterizing a user's willingness to release, withhold\nor lie about information depending on the behavior of individuals within the\nuser's circle of friends. In the model, we take into account both the\nheterogeneous nature of users and their different attitudes, as well as the\ndifferent types of information they may expose online. \n\n"}
{"id": "1206.3713", "contents": "Title: Learning the Structure and Parameters of Large-Population Graphical\n  Games from Behavioral Data Abstract: We consider learning, from strictly behavioral data, the structure and\nparameters of linear influence games (LIGs), a class of parametric graphical\ngames introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic\ninference (CSI): Making inferences from causal interventions on stable behavior\nin strategic settings. Applications include the identification of the most\ninfluential individuals in large (social) networks. Such tasks can also support\npolicy-making analysis. Motivated by the computational work on LIGs, we cast\nthe learning problem as maximum-likelihood estimation (MLE) of a generative\nmodel defined by pure-strategy Nash equilibria (PSNE). Our simple formulation\nuncovers the fundamental interplay between goodness-of-fit and model\ncomplexity: good models capture equilibrium behavior within the data while\ncontrolling the true number of equilibria, including those unobserved. We\nprovide a generalization bound establishing the sample complexity for MLE in\nour framework. We propose several algorithms including convex loss minimization\n(CLM) and sigmoidal approximations. We prove that the number of exact PSNE in\nLIGs is small, with high probability; thus, CLM is sound. We illustrate our\napproach on synthetic data and real-world U.S. congressional voting records. We\nbriefly discuss our learning framework's generality and potential applicability\nto general graphical games. \n\n"}
{"id": "1206.4771", "contents": "Title: Bayesian Sequential Auctions Abstract: In many natural settings agents participate in multiple different auctions\nthat are not simultaneous. In such auctions, future opportunities affect\nstrategic considerations of the players. The goal of this paper is to develop a\nquantitative understanding of outcomes of such sequential auctions. In earlier\nwork (Paes Leme et al. 2012) we initiated the study of the price of anarchy in\nsequential auctions. We considered sequential first price auctions in the full\ninformation model, where players are aware of all future opportunities, as well\nas the valuation of all players. In this paper, we study efficiency in\nsequential auctions in the Bayesian environment, relaxing the informational\nassumption on the players. We focus on two environments, both studied in the\nfull information model in Paes Leme et al. 2012, matching markets and matroid\nauctions. In the full information environment, a sequential first price cut\nauction for matroid settings is efficient. In Bayesian environments this is no\nlonger the case, as we show using a simple example with three players. Our main\nresult is a bound of $1+\\frac{e}{e-1}\\approx 2.58$ on the price of anarchy in\nboth matroid auctions and single-value matching markets (even with correlated\ntypes) and a bound of $2\\frac{e}{e-1}\\approx 3.16$ for general matching markets\nwith independent types. To bound the price of anarchy we need to consider\npossible deviations at an equilibrium. In a sequential Bayesian environment the\neffect of deviations is more complex than in one-shot games; early bids allow\nothers to infer information about the player's value. We create effective\ndeviations despite the presence of this difficulty by introducing a bluffing\ntechnique of independent interest. \n\n"}
{"id": "1207.0660", "contents": "Title: No-regret Dynamics and Fictitious Play Abstract: Potential based no-regret dynamics are shown to be related to fictitious\nplay. Roughly, these are epsilon-best reply dynamics where epsilon is the\nmaximal regret, which vanishes with time. This allows for alternative and\nsometimes much shorter proofs of known results on convergence of no-regret\ndynamics to the set of Nash equilibria. \n\n"}
{"id": "1207.0935", "contents": "Title: On the Power of Deterministic Mechanisms for Facility Location Games Abstract: We consider K-Facility Location games, where n strategic agents report their\nlocations in a metric space, and a mechanism maps them to K facilities. Our\nmain result is an elegant characterization of deterministic strategyproof\nmechanisms with a bounded approximation ratio for 2-Facility Location on the\nline. In particular, we show that for instances with n \\geq 5 agents, any such\nmechanism either admits a unique dictator, or always places the facilities at\nthe leftmost and the rightmost location of the instance. As a corollary, we\nobtain that the best approximation ratio achievable by deterministic\nstrategyproof mechanisms for the problem of locating 2 facilities on the line\nto minimize the total connection cost is precisely n-2. Another rather\nsurprising consequence is that the Two-Extremes mechanism of (Procaccia and\nTennenholtz, EC 2009) is the only deterministic anonymous strategyproof\nmechanism with a bounded approximation ratio for 2-Facility Location on the\nline.\n  The proof of the characterization employs several new ideas and technical\ntools, which provide new insights into the behavior of deterministic\nstrategyproof mechanisms for K-Facility Location games, and may be of\nindependent interest. Employing one of these tools, we show that for every K\n\\geq 3, there do not exist any deterministic anonymous strategyproof mechanisms\nwith a bounded approximation ratio for K-Facility Location on the line, even\nfor simple instances with K+1 agents. Moreover, building on the\ncharacterization for the line, we show that there do not exist any\ndeterministic strategyproof mechanisms with a bounded approximation ratio for\n2-Facility Location on more general metric spaces, which is true even for\nsimple instances with 3 agents located in a star. \n\n"}
{"id": "1207.4084", "contents": "Title: Mechanism Design in Large Games: Incentives and Privacy Abstract: We study the problem of implementing equilibria of complete information games\nin settings of incomplete information, and address this problem using\n\"recommender mechanisms.\" A recommender mechanism is one that does not have the\npower to enforce outcomes or to force participation, rather it only has the\npower to suggestion outcomes on the basis of voluntary participation. We show\nthat despite these restrictions, recommender mechanisms can implement\nequilibria of complete information games in settings of incomplete information\nunder the condition that the game is large---i.e. that there are a large number\nof players, and any player's action affects any other's payoff by at most a\nsmall amount.\n  Our result follows from a novel application of differential privacy. We show\nthat any algorithm that computes a correlated equilibrium of a complete\ninformation game while satisfying a variant of differential privacy---which we\ncall joint differential privacy---can be used as a recommender mechanism while\nsatisfying our desired incentive properties. Our main technical result is an\nalgorithm for computing a correlated equilibrium of a large game while\nsatisfying joint differential privacy.\n  Although our recommender mechanisms are designed to satisfy game-theoretic\nproperties, our solution ends up satisfying a strong privacy property as well.\nNo group of players can learn \"much\" about the type of any player outside the\ngroup from the recommendations of the mechanism, even if these players collude\nin an arbitrary way. As such, our algorithm is able to implement equilibria of\ncomplete information games, without revealing information about the realized\ntypes. \n\n"}
{"id": "1207.4747", "contents": "Title: Block-Coordinate Frank-Wolfe Optimization for Structural SVMs Abstract: We propose a randomized block-coordinate variant of the classic Frank-Wolfe\nalgorithm for convex optimization with block-separable constraints. Despite its\nlower iteration cost, we show that it achieves a similar convergence rate in\nduality gap as the full Frank-Wolfe algorithm. We also show that, when applied\nto the dual structural support vector machine (SVM) objective, this yields an\nonline algorithm that has the same low iteration complexity as primal\nstochastic subgradient methods. However, unlike stochastic subgradient methods,\nthe block-coordinate Frank-Wolfe algorithm allows us to compute the optimal\nstep-size and yields a computable duality gap guarantee. Our experiments\nindicate that this simple algorithm outperforms competing structural SVM\nsolvers. \n\n"}
{"id": "1207.5518", "contents": "Title: Optimal Multi-Dimensional Mechanism Design: Reducing Revenue to Welfare\n  Maximization Abstract: We provide a reduction from revenue maximization to welfare maximization in\nmulti-dimensional Bayesian auctions with arbitrary (possibly combinatorial)\nfeasibility constraints and independent bidders with arbitrary (possibly\ncombinatorial) demand constraints, appropriately extending Myerson's result to\nthis setting. We also show that every feasible Bayesian auction can be\nimplemented as a distribution over virtual VCG allocation rules. A virtual VCG\nallocation rule has the following simple form: Every bidder's type t_i is\ntransformed into a virtual type f_i(t_i), via a bidder-specific function. Then,\nthe allocation maximizing virtual welfare is chosen. Using this\ncharacterization, we show how to find and run the revenue-optimal auction given\nonly black box access to an implementation of the VCG allocation rule. We\ngeneralize this result to arbitrarily correlated bidders, introducing the\nnotion of a second-order VCG allocation rule.\n  We obtain our reduction from revenue to welfare optimization via two\nalgorithmic results on reduced forms in settings with arbitrary feasibility and\ndemand constraints. First, we provide a separation oracle for determining\nfeasibility of a reduced form. Second, we provide a geometric algorithm to\ndecompose any feasible reduced form into a distribution over virtual VCG\nallocation rules. In addition, we show how to execute both algorithms given\nonly black box access to an implementation of the VCG allocation rule.\n  Our results are computationally efficient for all multi-dimensional settings\nwhere the bidders are additive. In this case, our mechanisms run in time\npolynomial in the total number of bidder types, but not type profiles. For\ngeneric correlated distributions, this is the natural description complexity of\nthe problem. The runtime can be further improved to poly(#items, #bidders) in\nitem-symmetric settings by making use of recent techniques. \n\n"}
{"id": "1208.5076", "contents": "Title: Opinion Dynamics in Social Networks: A Local Interaction Game with\n  Stubborn Agents Abstract: The process by which new ideas, innovations, and behaviors spread through a\nlarge social network can be thought of as a networked interaction game: Each\nagent obtains information from certain number of agents in his friendship\nneighborhood, and adapts his idea or behavior to increase his benefit. In this\npaper, we are interested in how opinions, about a certain topic, form in social\nnetworks. We model opinions as continuous scalars ranging from 0 to 1 with 1(0)\nrepresenting extremely positive(negative) opinion. Each agent has an initial\nopinion and incurs some cost depending on the opinions of his neighbors, his\ninitial opinion, and his stubbornness about his initial opinion. Agents\niteratively update their opinions based on their own initial opinions and\nobserving the opinions of their neighbors. The iterative update of an agent can\nbe viewed as a myopic cost-minimization response (i.e., the so-called best\nresponse) to the others' actions. We study whether an equilibrium can emerge as\na result of such local interactions and how such equilibrium possibly depends\non the network structure, initial opinions of the agents, and the location of\nstubborn agents and the extent of their stubbornness. We also study the\nconvergence speed to such equilibrium and characterize the convergence time as\na function of aforementioned factors. We also discuss the implications of such\nresults in a few well-known graphs such as Erdos-Renyi random graphs and\nsmall-world graphs. \n\n"}
{"id": "1209.2388", "contents": "Title: On the Complexity of Bandit and Derivative-Free Stochastic Convex\n  Optimization Abstract: The problem of stochastic convex optimization with bandit feedback (in the\nlearning community) or without knowledge of gradients (in the optimization\ncommunity) has received much attention in recent years, in the form of\nalgorithms and performance upper bounds. However, much less is known about the\ninherent complexity of these problems, and there are few lower bounds in the\nliterature, especially for nonlinear functions. In this paper, we investigate\nthe attainable error/regret in the bandit and derivative-free settings, as a\nfunction of the dimension d and the available number of queries T. We provide a\nprecise characterization of the attainable performance for strongly-convex and\nsmooth functions, which also imply a non-trivial lower bound for more general\nproblems. Moreover, we prove that in both the bandit and derivative-free\nsetting, the required number of queries must scale at least quadratically with\nthe dimension. Finally, we show that on the natural class of quadratic\nfunctions, it is possible to obtain a \"fast\" O(1/T) error rate in terms of T,\nunder mild assumptions, even without having access to gradients. To the best of\nour knowledge, this is the first such rate in a derivative-free stochastic\nsetting, and holds despite previous results which seem to imply the contrary. \n\n"}
{"id": "1210.2457", "contents": "Title: Down the Borel Hierarchy: Solving Muller Games via Safety Games Abstract: We transform a Muller game with n vertices into a safety game with (n!)^3\nvertices whose solution allows to determine the winning regions of the Muller\ngame and to compute a finite-state winning strategy for one player. This yields\na novel antichain-based memory structure and a natural notion of permissive\nstrategies for Muller games. Moreover, we generalize our construction by\npresenting a new type of game reduction from infinite games to safety games and\nshow its applicability to several other winning conditions. \n\n"}
{"id": "1210.3539", "contents": "Title: Synthesis from LTL Specifications with Mean-Payoff Objectives Abstract: The classical LTL synthesis problem is purely qualitative: the given LTL\nspecification is realized or not by a reactive system. LTL is not expressive\nenough to formalize the correctness of reactive systems with respect to some\nquantitative aspects. This paper extends the qualitative LTL synthesis setting\nto a quantitative setting. The alphabet of actions is extended with a weight\nfunction ranging over the rational numbers. The value of an infinite word is\nthe mean-payoff of the weights of its letters. The synthesis problem then\namounts to automatically construct (if possible) a reactive system whose\nexecutions all satisfy a given LTL formula and have mean-payoff values greater\nthan or equal to some given threshold. The latter problem is called LTLMP\nsynthesis and the LTLMP realizability problem asks to check whether such a\nsystem exists. We first show that LTLMP realizability is not more difficult\nthan LTL realizability: it is 2ExpTime-Complete. This is done by reduction to\ntwo-player mean-payoff parity games. While infinite memory strategies are\nrequired to realize LTLMP specifications in general, we show that\nepsilon-optimality can be obtained with finite memory strategies, for any\nepsilon > 0. To obtain an efficient algorithm in practice, we define a\nSafraless procedure to decide whether there exists a finite-memory strategy\nthat realizes a given specification for some given threshold. This procedure is\nbased on a reduction to two-player energy safety games which are in turn\nreduced to safety games. Finally, we show that those safety games can be solved\nefficiently by exploiting the structure of their state spaces and by using\nantichains as a symbolic data-structure. All our results extend to\nmulti-dimensional weights. We have implemented an antichain-based procedure and\nwe report on some promising experimental results. \n\n"}
{"id": "1210.3548", "contents": "Title: Multiplayer Cost Games with Simple Nash Equilibria Abstract: Multiplayer games with selfish agents naturally occur in the design of\ndistributed and embedded systems. As the goals of selfish agents are usually\nneither equivalent nor antagonistic to each other, such games are non zero-sum\ngames. We study such games and show that a large class of these games,\nincluding games where the individual objectives are mean- or discounted-payoff,\nor quantitative reachability, and show that they do not only have a solution,\nbut a simple solution. We establish the existence of Nash equilibria that are\ncomposed of k memoryless strategies for each agent in a setting with k agents,\none main and k-1 minor strategies. The main strategy describes what happens\nwhen all agents comply, whereas the minor strategies ensure that all other\nagents immediately start to co-operate against the agent who first deviates\nfrom the plan. This simplicity is important, as rational agents are an\nidealisation. Realistically, agents have to decide on their moves with very\nlimited resources, and complicated strategies that require exponential--or even\nnon-elementary--implementations cannot realistically be implemented. The\nexistence of simple strategies that we prove in this paper therefore holds a\npromise of implementability. \n\n"}
{"id": "1210.7070", "contents": "Title: A Multiscale Framework for Challenging Discrete Optimization Abstract: Current state-of-the-art discrete optimization methods struggle behind when\nit comes to challenging contrast-enhancing discrete energies (i.e., favoring\ndifferent labels for neighboring variables). This work suggests a multiscale\napproach for these challenging problems. Deriving an algebraic representation\nallows us to coarsen any pair-wise energy using any interpolation in a\nprincipled algebraic manner. Furthermore, we propose an energy-aware\ninterpolation operator that efficiently exposes the multiscale landscape of the\nenergy yielding an effective coarse-to-fine optimization scheme. Results on\nchallenging contrast-enhancing energies show significant improvement over\nstate-of-the-art methods. \n\n"}
{"id": "1210.7362", "contents": "Title: Discrete Energy Minimization, beyond Submodularity: Applications and\n  Approximations Abstract: In this thesis I explore challenging discrete energy minimization problems\nthat arise mainly in the context of computer vision tasks. This work motivates\nthe use of such \"hard-to-optimize\" non-submodular functionals, and proposes\nmethods and algorithms to cope with the NP-hardness of their optimization.\nConsequently, this thesis revolves around two axes: applications and\napproximations. The applications axis motivates the use of such\n\"hard-to-optimize\" energies by introducing new tasks. As the energies become\nless constrained and structured one gains more expressive power for the\nobjective function achieving more accurate models. Results show how\nchallenging, hard-to-optimize, energies are more adequate for certain computer\nvision applications. To overcome the resulting challenging optimization tasks\nthe second axis of this thesis proposes approximation algorithms to cope with\nthe NP-hardness of the optimization. Experiments show that these new methods\nyield good results for representative challenging problems. \n\n"}
{"id": "1211.0877", "contents": "Title: Differential Privacy for the Analyst via Private Equilibrium Computation Abstract: We give new mechanisms for answering exponentially many queries from multiple\nanalysts on a private database, while protecting differential privacy both for\nthe individuals in the database and for the analysts. That is, our mechanism's\nanswer to each query is nearly insensitive to changes in the queries asked by\nother analysts. Our mechanism is the first to offer differential privacy on the\njoint distribution over analysts' answers, providing privacy for data analysts\neven if the other data analysts collude or register multiple accounts. In some\nsettings, we are able to achieve nearly optimal error rates (even compared to\nmechanisms which do not offer analyst privacy), and we are able to extend our\ntechniques to handle non-linear queries. Our analysis is based on a novel view\nof the private query-release problem as a two-player zero-sum game, which may\nbe of independent interest. \n\n"}
{"id": "1211.1325", "contents": "Title: Composable and Efficient Mechanisms Abstract: We initiate the study of efficient mechanism design with guaranteed good\nproperties even when players participate in multiple different mechanisms\nsimultaneously or sequentially. We define the class of smooth mechanisms,\nrelated to smooth games defined by Roughgarden, that can be thought of as\nmechanisms that generate approximately market clearing prices. We show that\nsmooth mechanisms result in high quality outcome in equilibrium both in the\nfull information setting and in the Bayesian setting with uncertainty about\nparticipants, as well as in learning outcomes. Our main result is to show that\nsuch mechanisms compose well: smoothness locally at each mechanism implies\nefficiency globally.\n  For mechanisms where good performance requires that bidders do not bid above\ntheir value, we identify the notion of a weakly smooth mechanism. Weakly smooth\nmechanisms, such as the Vickrey auction, are approximately efficient under the\nno-overbidding assumption. Similar to smooth mechanisms, weakly smooth\nmechanisms behave well in composition, and have high quality outcome in\nequilibrium (assuming no overbidding) both in the full information setting and\nin the Bayesian setting, as well as in learning outcomes.\n  In most of the paper we assume participants have quasi-linear valuations. We\nalso extend some of our results to settings where participants have budget\nconstraints. \n\n"}
{"id": "1211.2268", "contents": "Title: Tatonnement in Ongoing Markets of Complementary Goods Abstract: This paper continues the study, initiated by Cole and Fleischer, of the\nbehavior of a tatonnement price update rule in Ongoing Fisher Markets. The\nprior work showed fast convergence toward an equilibrium when the goods\nsatisfied the weak gross substitutes property and had bounded demand and income\nelasticities.\n  The current work shows that fast convergence also occurs for the following\ntypes of markets:\n  - All pairs of goods are complements to each other, and - the demand and\nincome elasticities are suitably bounded.\n  In particular, these conditions hold when all buyers in the market are\nequipped with CES utilities, where all the parameters $\\rho$, one per buyer,\nsatisfy $-1 < \\rho \\le 0$.\n  In addition, we extend the above result to markets in which a mixture of\ncomplements and substitutes occur. This includes characterizing a class of\nnested CES utilities for which fast convergence holds.\n  An interesting technical contribution, which may be of independent interest,\nis an amortized analysis for handling asynchronous events in settings in which\nthere are a mix of continuous changes and discrete events. \n\n"}
{"id": "1211.6244", "contents": "Title: A Computational Model and Convergence Theorem for Rumor Dissemination in\n  Social Networks Abstract: The spread of rumors, which are known as unverified statements of uncertain\norigin, may cause tremendous number of social problems. If it would be possible\nto identify factors affecting spreading a rumor (such as agents' desires, trust\nnetwork, etc.), then this could be used to slowdown or stop its spreading. A\ncomputational model that includes rumor features and the way a rumor is spread\namong society's members, based on their desires, is therefore needed. Our\nresearch is centering on the relation between the homogeneity of the society\nand rumor convergence in it and result shows that the homogeneity of the\nsociety is a necessary condition for convergence of the spreading rumor. \n\n"}
{"id": "1211.6302", "contents": "Title: Duality between subgradient and conditional gradient methods Abstract: Given a convex optimization problem and its dual, there are many possible\nfirst-order algorithms. In this paper, we show the equivalence between mirror\ndescent algorithms and algorithms generalizing the conditional gradient method.\nThis is done through convex duality, and implies notably that for certain\nproblems, such as for supervised machine learning problems with non-smooth\nlosses or problems regularized by non-smooth regularizers, the primal\nsubgradient method and the dual conditional gradient method are formally\nequivalent. The dual interpretation leads to a form of line search for mirror\ndescent, as well as guarantees of convergence for primal-dual certificates. \n\n"}
{"id": "1212.2834", "contents": "Title: Dictionary Subselection Using an Overcomplete Joint Sparsity Model Abstract: Many natural signals exhibit a sparse representation, whenever a suitable\ndescribing model is given. Here, a linear generative model is considered, where\nmany sparsity-based signal processing techniques rely on such a simplified\nmodel. As this model is often unknown for many classes of the signals, we need\nto select such a model based on the domain knowledge or using some exemplar\nsignals. This paper presents a new exemplar based approach for the linear model\n(called the dictionary) selection, for such sparse inverse problems. The\nproblem of dictionary selection, which has also been called the dictionary\nlearning in this setting, is first reformulated as a joint sparsity model. The\njoint sparsity model here differs from the standard joint sparsity model as it\nconsiders an overcompleteness in the representation of each signal, within the\nrange of selected subspaces. The new dictionary selection paradigm is examined\nwith some synthetic and realistic simulations. \n\n"}
{"id": "1212.3782", "contents": "Title: Can Selfish Groups be Self-Enforcing? Abstract: Algorithmic graph theory has thoroughly analyzed how, given a network\ndescribing constraints between various nodes, groups can be formed among these\nso that the resulting configuration optimizes a \\emph{global} metric. In\ncontrast, for various social and economic networks, groups are formed \\emph{de\nfacto} by the choices of selfish players. A fundamental problem in this setting\nis the existence and convergence to a \\emph{self-enforcing} configuration:\nassignment of players into groups such that no player has an incentive to move\ninto another group than hers. Motivated by information sharing on social\nnetworks -- and the difficult tradeoff between its benefits and the associated\nprivacy risk -- we study the possible emergence of such stable configurations\nin a general selfish group formation game.\n  Our paper considers this general game for the first time, and it completes\nits analysis. We show that convergence critically depends on the level of\n\\emph{collusions} among the players -- which allow multiple players to move\nsimultaneously as long as \\emph{all of them} benefit. Solving a previously open\nproblem we exactly show when, depending on collusions, convergence occurs\nwithin polynomial time, non-polynomial time, and when it never occurs. We also\nprove that previously known bounds on convergence time are all loose: by a\nnovel combinatorial analysis of the evolution of this game we are able to\nprovide the first \\emph{asymptotically exact} formula on its convergence.\nMoreover, we extend these results by providing a complete analysis when groups\nmay \\emph{overlap}, and for general utility functions representing\n\\emph{multi-modal} interactions. Finally, we prove that collusions have a\nsignificant and \\emph{positive} effect on the \\emph{efficiency} of the\nequilibrium that is attained. \n\n"}
{"id": "1212.6355", "contents": "Title: Efficient Decomposition of Bimatrix Games Abstract: Exploiting the algebraic structure of the set of bimatrix games, a\ndivide-and-conquer algorithm for finding Nash equilibria is proposed. The\nalgorithm is fixed-parameter tractable with the size of the largest irreducible\ncomponent of a game as parameter. An implementation of the algorithm is shown\nto yield a significant performance increase on inputs with small parameters. \n\n"}
{"id": "1301.1420", "contents": "Title: Is it ever safe to vote strategically? Abstract: There are many situations in which mis-coordinated strategic voting can leave\nstrategic voters worse off than they would have been had they not tried to\nstrategize. We analyse the simplest of such scenarios, in which the set of\nstrategic voters all have the same sincere preferences and all cast the same\nstrategic vote, while all other voters vote sincerely. Most mis-coordinations\nin this framework can be classified as instances of either strategic\novershooting (too many voted strategically) or strategic undershooting (too\nfew). If mis-coordination can result in strategic voters ending up worse off\nthan they would have been had they all just voted sincerely, we call the\nrelevant strategic vote unsafe. We show that under every onto and\nnon-dictatorial social choice rule there exist circumstances where a voter has\nan incentive to cast a safe strategic vote. We extend the Gibbard-Satterthwaite\nTheorem by proving that every onto and non-dictatorial social choice rule can\nbe individually manipulated by a voter casting a safe strategic vote. \n\n"}
{"id": "1301.2533", "contents": "Title: A Novel Analytical Method for Evolutionary Graph Theory Problems Abstract: Evolutionary graph theory studies the evolutionary dynamics of populations\nstructured on graphs. A central problem is determining the probability that a\nsmall number of mutants overtake a population. Currently, Monte Carlo\nsimulations are used for estimating such fixation probabilities on general\ndirected graphs, since no good analytical methods exist. In this paper, we\nintroduce a novel deterministic framework for computing fixation probabilities\nfor strongly connected, directed, weighted evolutionary graphs under neutral\ndrift. We show how this framework can also be used to calculate the expected\nnumber of mutants at a given time step (even if we relax the assumption that\nthe graph is strongly connected), how it can extend to other related models\n(e.g. voter model), how our framework can provide non-trivial bounds for\nfixation probability in the case of an advantageous mutant, and how it can be\nused to find a non-trivial lower bound on the mean time to fixation. We provide\nvarious experimental results determining fixation probabilities and expected\nnumber of mutants on different graphs. Among these, we show that our method\nconsistently outperforms Monte Carlo simulations in speed by several orders of\nmagnitude. Finally we show how our approach can provide insight into synaptic\ncompetition in neurology. \n\n"}
{"id": "1301.4666", "contents": "Title: A Linearly Convergent Conditional Gradient Algorithm with Applications\n  to Online and Stochastic Optimization Abstract: Linear optimization is many times algorithmically simpler than non-linear\nconvex optimization. Linear optimization over matroid polytopes, matching\npolytopes and path polytopes are example of problems for which we have simple\nand efficient combinatorial algorithms, but whose non-linear convex counterpart\nis harder and admits significantly less efficient algorithms. This motivates\nthe computational model of convex optimization, including the offline, online\nand stochastic settings, using a linear optimization oracle. In this\ncomputational model we give several new results that improve over the previous\nstate-of-the-art. Our main result is a novel conditional gradient algorithm for\nsmooth and strongly convex optimization over polyhedral sets that performs only\na single linear optimization step over the domain on each iteration and enjoys\na linear convergence rate. This gives an exponential improvement in convergence\nrate over previous results.\n  Based on this new conditional gradient algorithm we give the first algorithms\nfor online convex optimization over polyhedral sets that perform only a single\nlinear optimization step over the domain while having optimal regret\nguarantees, answering an open question of Kalai and Vempala, and Hazan and\nKale. Our online algorithms also imply conditional gradient algorithms for\nnon-smooth and stochastic convex optimization with the same convergence rates\nas projected (sub)gradient methods. \n\n"}
{"id": "1301.5844", "contents": "Title: Ranking Games that have Competitiveness-based Strategies Abstract: An extensive literature in economics and social science addresses contests,\nin which players compete to outperform each other on some measurable criterion,\noften referred to as a player's score, or output. Players incur costs that are\nan increasing function of score, but receive prizes for obtaining higher score\nthan their competitors. In this paper we study finite games that are\ndiscretized contests, and the problems of computing exact and approximate Nash\nequilibria. Our motivation is the worst-case hardness of Nash equilibrium\ncomputation, and the resulting interest in important classes of games that\nadmit polynomial-time algorithms. For games that have a tie-breaking rule for\nplayers' scores, we present a polynomial-time algorithm for computing an exact\nequilibrium in the 2-player case, and for multiple players, a characterization\nof Nash equilibria that shows an interesting parallel between these games and\nunrestricted 2-player games in normal form. When ties are allowed, via a\nreduction from these games to a subclass of anonymous games, we give\napproximation schemes for two special cases: constant-sized set of strategies,\nand constant number of players. \n\n"}
{"id": "1302.0948", "contents": "Title: Non-monetary fair scheduling---a cooperative game theory approach Abstract: We consider a multi-organizational system in which each organization\ncontributes processors to the global pool but also jobs to be processed on the\ncommon resources. The fairness of the scheduling algorithm is essential for the\nstability and even for the existence of such systems (as organizations may\nrefuse to join an unfair system).\n  We consider on-line, non-clairvoyant scheduling of sequential jobs. The\nstarted jobs cannot be stopped, canceled, preempted, or moved to other\nprocessors. We consider identical processors, but most of our results can be\nextended to related or unrelated processors.\n  We model the fair scheduling problem as a cooperative game and we use the\nShapley value to determine the ideal fair schedule. In contrast to the current\nliterature, we do not use money to assess the relative utilities of jobs.\nInstead, to calculate the contribution of an organization, we determine how the\npresence of this organization influences the performance of other\norganizations. Our approach can be used with arbitrary utility function (e.g.,\nflow time, tardiness, resource utilization), but we argue that the utility\nfunction should be strategy resilient. The organizations should be discouraged\nfrom splitting, merging or delaying their jobs. We present the unique (to\nwithin a multiplicative and additive constants) strategy resilient utility\nfunction.\n  We show that the problem of fair scheduling is NP-hard and hard to\napproximate. However, for unit-size jobs, we present an FPRAS. Also, we show\nthat the problem parametrized with the number of organizations is FPT. Although\nfor the large number of the organizations the problem is computationally hard,\nthe presented exponential algorithm can be used as a fairness benchmark. \n\n"}
{"id": "1302.1669", "contents": "Title: Possible and Necessary Winner Problem in Social Polls Abstract: Social networks are increasingly being used to conduct polls. We introduce a\nsimple model of such social polling. We suppose agents vote sequentially, but\nthe order in which agents choose to vote is not necessarily fixed. We also\nsuppose that an agent's vote is influenced by the votes of their friends who\nhave already voted. Despite its simplicity, this model provides useful insights\ninto a number of areas including social polling, sequential voting, and\nmanipulation. We prove that the number of candidates and the network structure\naffect the computational complexity of computing which candidate necessarily or\npossibly can win in such a social poll. For social networks with bounded\ntreewidth and a bounded number of candidates, we provide polynomial algorithms\nfor both problems. In other cases, we prove that computing which candidates\nnecessarily or possibly win are computationally intractable. \n\n"}
{"id": "1302.2869", "contents": "Title: Searching and Bargaining with Middlemen Abstract: We study decentralized markets with the presence of middlemen, modeled by a\nnon-cooperative bargaining game in trading networks. Our goal is to investigate\nhow the network structure of the market and the role of middlemen influence the\nmarket's efficiency and fairness. We introduce the concept of limit stationary\nequilibrium in a general trading network and use it to analyze how competition\namong middlemen is influenced by the network structure, how endogenous delay\nemerges in trade and how surplus is shared between producers and consumers. \n\n"}
{"id": "1302.4248", "contents": "Title: Looking at Mean-Payoff and Total-Payoff through Windows Abstract: We consider two-player games played on weighted directed graphs with\nmean-payoff and total-payoff objectives, two classical quantitative objectives.\nWhile for single-dimensional games the complexity and memory bounds for both\nobjectives coincide, we show that in contrast to multi-dimensional mean-payoff\ngames that are known to be coNP-complete, multi-dimensional total-payoff games\nare undecidable. We introduce conservative approximations of these objectives,\nwhere the payoff is considered over a local finite window sliding along a play,\ninstead of the whole play. For single dimension, we show that (i) if the window\nsize is polynomial, deciding the winner takes polynomial time, and (ii) the\nexistence of a bounded window can be decided in NP $\\cap$ coNP, and is at least\nas hard as solving mean-payoff games. For multiple dimensions, we show that (i)\nthe problem with fixed window size is EXPTIME-complete, and (ii) there is no\nprimitive-recursive algorithm to decide the existence of a bounded window. \n\n"}
{"id": "1303.2270", "contents": "Title: Penalty-regulated dynamics and robust learning procedures in games Abstract: Starting from a heuristic learning scheme for N-person games, we derive a new\nclass of continuous-time learning dynamics consisting of a replicator-like\ndrift adjusted by a penalty term that renders the boundary of the game's\nstrategy space repelling. These penalty-regulated dynamics are equivalent to\nplayers keeping an exponentially discounted aggregate of their on-going payoffs\nand then using a smooth best response to pick an action based on these\nperformance scores. Owing to this inherent duality, the proposed dynamics\nsatisfy a variant of the folk theorem of evolutionary game theory and they\nconverge to (arbitrarily precise) approximations of Nash equilibria in\npotential games. Motivated by applications to traffic engineering, we exploit\nthis duality further to design a discrete-time, payoff-based learning algorithm\nwhich retains these convergence properties and only requires players to observe\ntheir in-game payoffs: moreover, the algorithm remains robust in the presence\nof stochastic perturbations and observation errors, and it does not require any\nsynchronization between players. \n\n"}
{"id": "1303.2643", "contents": "Title: Revealing Cluster Structure of Graph by Path Following Replicator\n  Dynamic Abstract: In this paper, we propose a path following replicator dynamic, and\ninvestigate its potentials in uncovering the underlying cluster structure of a\ngraph. The proposed dynamic is a generalization of the discrete replicator\ndynamic. The replicator dynamic has been successfully used to extract dense\nclusters of graphs; however, it is often sensitive to the degree distribution\nof a graph, and usually biased by vertices with large degrees, thus may fail to\ndetect the densest cluster. To overcome this problem, we introduce a dynamic\nparameter, called path parameter, into the evolution process. The path\nparameter can be interpreted as the maximal possible probability of a current\ncluster containing a vertex, and it monotonically increases as evolution\nprocess proceeds. By limiting the maximal probability, the phenomenon of some\nvertices dominating the early stage of evolution process is suppressed, thus\nmaking evolution process more robust. To solve the optimization problem with a\nfixed path parameter, we propose an efficient fixed point algorithm. The time\ncomplexity of the path following replicator dynamic is only linear in the\nnumber of edges of a graph, thus it can analyze graphs with millions of\nvertices and tens of millions of edges on a common PC in a few minutes.\nBesides, it can be naturally generalized to hypergraph and graph with edges of\ndifferent orders. We apply it to four important problems: maximum clique\nproblem, densest k-subgraph problem, structure fitting, and discovery of\nhigh-density regions. The extensive experimental results clearly demonstrate\nits advantages, in terms of robustness, scalability and flexility. \n\n"}
{"id": "1304.0539", "contents": "Title: A Real-time Group Auction System for Efficient Allocation of Cloud\n  Internet Applications Abstract: Increasing number of the cloud-based Internet applications demands for\nefficient resource and cost management. This paper proposes a real-time group\nauction system for the cloud instance market. The system is designed based on a\ncombinatorial double auction, and its applicability and effectiveness are\nevaluated in terms of resource efficiency and monetary benefits to auction\nparticipants (e.g., cloud users and providers). The proposed auction system\nassists them to decide when and how providers allocate their resources to which\nusers. Furthermore, we propose a distributed algorithm using a group formation\ngame that determines which users and providers will trade resources by their\ncooperative decisions. To find how to allocate the resources, the utility\noptimization problem is formulated as a binary integer programming problem, and\nthe nearly optimal solution is obtained by a heuristic algorithm with quadratic\ntime complexity. In comparison studies, the proposed real-time group auction\nsystem with cooperation outperforms an individual auction in terms of the\nresource efficiency (e.g., the request acceptance rate for users and resource\nutilization for providers) and monetary benefits (e.g., average payments for\nusers and total profits for providers). \n\n"}
{"id": "1304.1014", "contents": "Title: A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale\n  SVM Training Abstract: Recently, there has been a renewed interest in the machine learning community\nfor variants of a sparse greedy approximation procedure for concave\noptimization known as {the Frank-Wolfe (FW) method}. In particular, this\nprocedure has been successfully applied to train large-scale instances of\nnon-linear Support Vector Machines (SVMs). Specializing FW to SVM training has\nallowed to obtain efficient algorithms but also important theoretical results,\nincluding convergence analysis of training algorithms and new characterizations\nof model sparsity.\n  In this paper, we present and analyze a novel variant of the FW method based\non a new way to perform away steps, a classic strategy used to accelerate the\nconvergence of the basic FW procedure. Our formulation and analysis is focused\non a general concave maximization problem on the simplex. However, the\nspecialization of our algorithm to quadratic forms is strongly related to some\nclassic methods in computational geometry, namely the Gilbert and MDM\nalgorithms.\n  On the theoretical side, we demonstrate that the method matches the\nguarantees in terms of convergence rate and number of iterations obtained by\nusing classic away steps. In particular, the method enjoys a linear rate of\nconvergence, a result that has been recently proved for MDM on quadratic forms.\n  On the practical side, we provide experiments on several classification\ndatasets, and evaluate the results using statistical tests. Experiments show\nthat our method is faster than the FW method with classic away steps, and works\nwell even in the cases in which classic away steps slow down the algorithm.\nFurthermore, these improvements are obtained without sacrificing the predictive\naccuracy of the obtained SVM model. \n\n"}
{"id": "1304.7718", "contents": "Title: A Dynamic Axiomatic Approach to First-Price Auctions Abstract: The first-price auction is popular in practice for its simplicity and\ntransparency. Moreover, its potential virtues grow in complex settings where\nincentive compatible auctions may generate little or no revenue. Unfortunately,\nthe first-price auction is poorly understood in theory because equilibrium is\nnot {\\em a priori} a credible predictor of bidder behavior.\n  We take a dynamic approach to studying first-price auctions: rather than\nbasing performance guarantees solely on static equilibria, we study the\nrepeated setting and show that robust performance guarantees may be derived\nfrom simple axioms of bidder behavior. For example, as long as a loser raises\nher bid quickly, a standard first-price auction will generate at least as much\nrevenue as a second-price auction. We generalize this dynamic technique to\ncomplex pay-your-bid auction settings and show that progressively stronger\nassumptions about bidder behavior imply progressively stronger guarantees about\nthe auction's performance.\n  Along the way, we find that the auctioneer's choice of bidding language is\ncritical when generalizing beyond the single-item setting, and we propose a\nspecific construction called the {\\em utility-target auction} that performs\nwell. The utility-target auction includes a bidder's final utility as an\nadditional parameter, identifying the single dimension along which she wishes\nto compete. This auction is closely related to profit-target bidding in\nfirst-price and ascending proxy package auctions and gives strong revenue\nguarantees for a variety of complex auction environments. Of particular\ninterest, the guaranteed existence of a pure-strategy equilibrium in the\nutility-target auction shows how Overture might have eliminated the cyclic\nbehavior in their generalized first-price sponsored search auction if bidders\ncould have placed more sophisticated bids. \n\n"}
{"id": "1305.0597", "contents": "Title: Prior-Independent Mechanisms for Scheduling Abstract: We study the makespan minimization problem with unrelated selfish machines\nunder the assumption that job sizes are stochastic. We design simple truthful\nmechanisms that under various distributional assumptions provide constant and\nsublogarithmic approximations to expected makespan. Our mechanisms are\nprior-independent in that they do not rely on knowledge of the job size\ndistributions. Prior-independent approximation mechanisms have been previously\nstudied for the objective of revenue maximization [Dhangwatnotai, Roughgarden\nand Yan'10, Devanur, Hartline, Karlin and Nguyen'11, Roughgarden, Talgam-Cohen\nand Yan'12]. In contrast to our results, in prior-free settings no truthful\nanonymous deterministic mechanism for the makespan objective can provide a\nsublinear approximation [Ashlagi, Dobzinski and Lavi'09]. \n\n"}
{"id": "1305.3334", "contents": "Title: Online Learning in a Contract Selection Problem Abstract: In an online contract selection problem there is a seller which offers a set\nof contracts to sequentially arriving buyers whose types are drawn from an\nunknown distribution. If there exists a profitable contract for the buyer in\nthe offered set, i.e., a contract with payoff higher than the payoff of not\naccepting any contracts, the buyer chooses the contract that maximizes its\npayoff. In this paper we consider the online contract selection problem to\nmaximize the sellers profit. Assuming that a structural property called ordered\npreferences holds for the buyer's payoff function, we propose online learning\nalgorithms that have sub-linear regret with respect to the best set of\ncontracts given the distribution over the buyer's type. This problem has many\napplications including spectrum contracts, wireless service provider data plans\nand recommendation systems. \n\n"}
{"id": "1306.0816", "contents": "Title: A Critical Assessment of Cost-Based Nash Methods for Demand Scheduling\n  in Smart Grids Abstract: Demand-side management (DSM) is becoming an increasingly important component\nof the envisioned smart grid. The ability to improve the efficiency of energy\nuse in the power system by altering demand is widely viewed as being not merely\npromising but in fact essential. However, while the advantages of DSM are\nclear, arriving at an efficient implementation has so far proven to be less\nstraightforward. There have recently been many proposals put forth in the\nliterature to tackle the demand scheduling aspect of DSM. One particular\napproach based on a game-theoretic treatment of the day-ahead load-scheduling\nproblem has recently gained tremendous popularity in the DSM literature. In\nthis letter, an assessment of this approach is conducted, and its main result\nis challenged. \n\n"}
{"id": "1306.6278", "contents": "Title: Playing cooperatively with possibly treacherous partner Abstract: We investigate an alternative concept of Nash equilibrium, m-equilibrium,\nwhich slightly resembles Harsanyi-Selten risk dominant equilibrium although it\nis a different notion. M-equilibria provide nontrivial solutions of normal form\ngames as shown by comparison of the Prisoner's Dilemma with the Traveler's\nDilemma. They are also resistant on the deep iterated elimination of dominated\nstrategies. \n\n"}
{"id": "1306.6709", "contents": "Title: A Survey on Metric Learning for Feature Vectors and Structured Data Abstract: The need for appropriate ways to measure the distance or similarity between\ndata is ubiquitous in machine learning, pattern recognition and data mining,\nbut handcrafting such good metrics for specific problems is generally\ndifficult. This has led to the emergence of metric learning, which aims at\nautomatically learning a metric from data and has attracted a lot of interest\nin machine learning and related fields for the past ten years. This survey\npaper proposes a systematic review of the metric learning literature,\nhighlighting the pros and cons of each approach. We pay particular attention to\nMahalanobis distance metric learning, a well-studied and successful framework,\nbut additionally present a wide range of methods that have recently emerged as\npowerful alternatives, including nonlinear metric learning, similarity learning\nand local metric learning. Recent trends and extensions, such as\nsemi-supervised metric learning, metric learning for histogram data and the\nderivation of generalization guarantees, are also covered. Finally, this survey\naddresses metric learning for structured data, in particular edit distance\nlearning, and attempts to give an overview of the remaining challenges in\nmetric learning for the years to come. \n\n"}
{"id": "1307.0241", "contents": "Title: On the steady-state probability of delay and large negative deviations\n  for the $GI/GI/n$ queue in the Halfin-Whitt regime Abstract: We consider the FCFS $GI/GI/n$ queue in the Halfin-Whitt heavy traffic\nregime, and prove bounds for the steady-state probability of delay (s.s.p.d.)\nfor generally distributed processing times. We prove that there exist\n$\\epsilon_1, \\epsilon_2 > 0$, depending on the first three moments of the\ninter-arrival and processing time distributions, such that the s.s.p.d. is\nbounded from above by $\\exp\\big(-\\epsilon_1 B^2\\big)$ as the associated excess\nparameter $B \\rightarrow \\infty$; and by $1 - \\epsilon_2 B$ as $B \\rightarrow\n0$. We also prove that the tail of the steady-state number of idle servers has\na Gaussian decay. We provide explicit bounds in all cases, in terms of the\nfirst three moments of the inter-arrival and service distributions, and use\nknown results to show that our bounds correctly capture various qualitative\nscalings. \\\\\\indent Our main proof technique is the derivation of new\nstochastic comparison bounds for the FCFS $GI/GI/n$ queue, which are of a\nstructural nature, hold for all $n$ and times $t$, and significantly generalize\nthe work of \\citet{GG.10c} (e.g. by providing bounds for the queue length to\nexceed any given level, as opposed to any given level strictly greater than the\nnumber of servers as acheived in \\citet{GG.10c}). Our results do not follow\nfrom simple comparison arguments to e.g. infinite-server systems or loss\nmodels, which would in all cases provide bounds in the opposite direction. \n\n"}
{"id": "1307.2569", "contents": "Title: Mechanism design for resource allocation in networks with intergroup\n  competition and intragroup sharing Abstract: We consider a network where strategic agents, who are contesting for\nallocation of resources, are divided into fixed groups. The network control\nprotocol is such that within each group agents get to share the resource and\nacross groups they contest for it. A prototypical example is the allocation of\ndata rate on a network with multicast/multirate architecture. Compared to the\nunicast architecture (which is a special case), the multicast/multirate\narchitecture can result in substantial bandwidth savings. However, design of a\nmarket mechanism in such a scenario requires dealing with both private and\npublic good problems as opposed to just private goods in unicast.\n  The mechanism proposed in this work ensures that social welfare maximizing\nallocation on such a network is realized at all Nash equilibria (NE) i.e., full\nimplementation in NE. In addition it is individually rational, i.e., agents\nhave an incentive to participate in the mechanism. The mechanism, which is\nconstructed in a quasi-systematic way starting from the dual of the centralized\nproblem, has a number of useful properties. Specifically, due to a novel\nallocation scheme, namely \"radial projection\", the proposed mechanism results\nin feasible allocation even off equilibrium. This is a practical necessity for\nany realistic mechanism since agents have to \"learn\" the NE through a dynamic\nprocess. Finally, it is shown how strong budget balance at equilibrium can be\nachieved with a minimal increase in message space as an add-on to a weakly\nbudget balanced mechanism. \n\n"}
{"id": "1307.3736", "contents": "Title: Prophet Inequalities with Limited Information Abstract: In the classical prophet inequality, a gambler observes a sequence of\nstochastic rewards $V_1,...,V_n$ and must decide, for each reward $V_i$,\nwhether to keep it and stop the game or to forfeit the reward forever and\nreveal the next value $V_i$. The gambler's goal is to obtain a constant\nfraction of the expected reward that the optimal offline algorithm would get.\nRecently, prophet inequalities have been generalized to settings where the\ngambler can choose $k$ items, and, more generally, where he can choose any\nindependent set in a matroid. However, all the existing algorithms require the\ngambler to know the distribution from which the rewards $V_1,...,V_n$ are\ndrawn.\n  The assumption that the gambler knows the distribution from which\n$V_1,...,V_n$ are drawn is very strong. Instead, we work with the much simpler\nassumption that the gambler only knows a few samples from this distribution. We\nconstruct the first single-sample prophet inequalities for many settings of\ninterest, whose guarantees all match the best possible asymptotically,\n\\emph{even with full knowledge of the distribution}. Specifically, we provide a\nnovel single-sample algorithm when the gambler can choose any $k$ elements\nwhose analysis is based on random walks with limited correlation. In addition,\nwe provide a black-box method for converting specific types of solutions to the\nrelated \\emph{secretary problem} to single-sample prophet inequalities, and\napply it to several existing algorithms. Finally, we provide a constant-sample\nprophet inequality for constant-degree bipartite matchings.\n  We apply these results to design the first posted-price and multi-dimensional\nauction mechanisms with limited information in settings with asymmetric\nbidders. \n\n"}
{"id": "1307.3794", "contents": "Title: The Network Improvement Problem for Equilibrium Routing Abstract: In routing games, agents pick their routes through a network to minimize\ntheir own delay. A primary concern for the network designer in routing games is\nthe average agent delay at equilibrium. A number of methods to control this\naverage delay have received substantial attention, including network tolls,\nStackelberg routing, and edge removal.\n  A related approach with arguably greater practical relevance is that of\nmaking investments in improvements to the edges of the network, so that, for a\ngiven investment budget, the average delay at equilibrium in the improved\nnetwork is minimized. This problem has received considerable attention in the\nliterature on transportation research and a number of different algorithms have\nbeen studied. To our knowledge, none of this work gives guarantees on the\noutput quality of any polynomial-time algorithm. We study a model for this\nproblem introduced in transportation research literature, and present both\nhardness results and algorithms that obtain nearly optimal performance\nguarantees.\n  - We first show that a simple algorithm obtains good approximation guarantees\nfor the problem. Despite its simplicity, we show that for affine delays the\napproximation ratio of 4/3 obtained by the algorithm cannot be improved.\n  - To obtain better results, we then consider restricted topologies. For\ngraphs consisting of parallel paths with affine delay functions we give an\noptimal algorithm. However, for graphs that consist of a series of parallel\nlinks, we show the problem is weakly NP-hard.\n  - Finally, we consider the problem in series-parallel graphs, and give an\nFPTAS for this case.\n  Our work thus formalizes the intuition held by transportation researchers\nthat the network improvement problem is hard, and presents topology-dependent\nalgorithms that have provably tight approximation guarantees. \n\n"}
{"id": "1307.4514", "contents": "Title: Supervised Metric Learning with Generalization Guarantees Abstract: The crucial importance of metrics in machine learning algorithms has led to\nan increasing interest in optimizing distance and similarity functions, an area\nof research known as metric learning. When data consist of feature vectors, a\nlarge body of work has focused on learning a Mahalanobis distance. Less work\nhas been devoted to metric learning from structured objects (such as strings or\ntrees), most of it focusing on optimizing a notion of edit distance. We\nidentify two important limitations of current metric learning approaches.\nFirst, they allow to improve the performance of local algorithms such as\nk-nearest neighbors, but metric learning for global algorithms (such as linear\nclassifiers) has not been studied so far. Second, the question of the\ngeneralization ability of metric learning methods has been largely ignored. In\nthis thesis, we propose theoretical and algorithmic contributions that address\nthese limitations. Our first contribution is the derivation of a new kernel\nfunction built from learned edit probabilities. Our second contribution is a\nnovel framework for learning string and tree edit similarities inspired by the\nrecent theory of (e,g,t)-good similarity functions. Using uniform stability\narguments, we establish theoretical guarantees for the learned similarity that\ngive a bound on the generalization error of a linear classifier built from that\nsimilarity. In our third contribution, we extend these ideas to metric learning\nfrom feature vectors by proposing a bilinear similarity learning method that\nefficiently optimizes the (e,g,t)-goodness. Generalization guarantees are\nderived for our approach, highlighting that our method minimizes a tighter\nbound on the generalization error of the classifier. Our last contribution is a\nframework for establishing generalization bounds for a large class of existing\nmetric learning algorithms based on a notion of algorithmic robustness. \n\n"}
{"id": "1307.6134", "contents": "Title: Modeling Human Decision-making in Generalized Gaussian Multi-armed\n  Bandits Abstract: We present a formal model of human decision-making in explore-exploit tasks\nusing the context of multi-armed bandit problems, where the decision-maker must\nchoose among multiple options with uncertain rewards. We address the standard\nmulti-armed bandit problem, the multi-armed bandit problem with transition\ncosts, and the multi-armed bandit problem on graphs. We focus on the case of\nGaussian rewards in a setting where the decision-maker uses Bayesian inference\nto estimate the reward values. We model the decision-maker's prior knowledge\nwith the Bayesian prior on the mean reward. We develop the upper credible limit\n(UCL) algorithm for the standard multi-armed bandit problem and show that this\ndeterministic algorithm achieves logarithmic cumulative expected regret, which\nis optimal performance for uninformative priors. We show how good priors and\ngood assumptions on the correlation structure among arms can greatly enhance\ndecision-making performance, even over short time horizons. We extend to the\nstochastic UCL algorithm and draw several connections to human decision-making\nbehavior. We present empirical data from human experiments and show that human\nperformance is efficiently captured by the stochastic UCL algorithm with\nappropriate parameters. For the multi-armed bandit problem with transition\ncosts and the multi-armed bandit problem on graphs, we generalize the UCL\nalgorithm to the block UCL algorithm and the graphical block UCL algorithm,\nrespectively. We show that these algorithms also achieve logarithmic cumulative\nexpected regret and require a sub-logarithmic expected number of transitions\namong arms. We further illustrate the performance of these algorithms with\nnumerical examples. NB: Appendix G included in this version details minor\nmodifications that correct for an oversight in the previously-published proofs.\nThe remainder of the text reflects the published work. \n\n"}
{"id": "1307.7322", "contents": "Title: Complexity of Manipulation, Bribery, and Campaign Management in Bucklin\n  and Fallback Voting Abstract: A central theme in computational social choice is to study the extent to\nwhich voting systems computationally resist manipulative attacks seeking to\ninfluence the outcome of elections, such as manipulation (i.e., strategic\nvoting), control, and bribery. Bucklin and fallback voting are among the voting\nsystems with the broadest resistance (i.e., NP-hardness) to control attacks.\nHowever, only little is known about their behavior regarding manipulation and\nbribery attacks. We comprehensively investigate the computational resistance of\nBucklin and fallback voting for many of the common manipulation and bribery\nscenarios; we also complement our discussion by considering several campaign\nmanagement problems for Bucklin and fallback. \n\n"}
{"id": "1307.7838", "contents": "Title: Asymmetric-valued Spectrum Auction and Competition in Wireless Broadband\n  Services Abstract: We study bidding and pricing competition between two spiteful mobile network\noperators (MNOs) with considering their existing spectrum holdings. Given\nasymmetric-valued spectrum blocks are auctioned off to them via a first-price\nsealed-bid auction, we investigate the interactions between two spiteful MNOs\nand users as a three-stage dynamic game and characterize the dynamic game's\nequilibria. We show an asymmetric pricing structure and different market share\nbetween two spiteful MNOs. Perhaps counter-intuitively, our results show that\nthe MNO who acquires the less-valued spectrum block always lowers his service\nprice despite providing double-speed LTE service to users. We also show that\nthe MNO who acquires the high-valued spectrum block, despite charing a higher\nprice, still achieves more market share than the other MNO. We further show\nthat the competition between two MNOs leads to some loss of their revenues. By\ninvestigating a cross-over point at which the MNOs' profits are switched, it\nserves as the benchmark of practical auction designs. \n\n"}
{"id": "1308.0990", "contents": "Title: Incentives and Efficiency in Uncertain Collaborative Environments Abstract: We consider collaborative systems where users make contributions across\nmultiple available projects and are rewarded for their contributions in\nindividual projects according to a local sharing of the value produced. This\nserves as a model of online social computing systems such as online Q&A forums\nand of credit sharing in scientific co-authorship settings. We show that the\nmaximum feasible produced value can be well approximated by simple local\nsharing rules where users are approximately rewarded in proportion to their\nmarginal contributions and that this holds even under incomplete information\nabout the player's abilities and effort constraints. For natural instances we\nshow almost 95% optimality at equilibrium. When players incur a cost for their\neffort, we identify a threshold phenomenon: the efficiency is a constant\nfraction of the optimal when the cost is strictly convex and decreases with the\nnumber of players if the cost is linear. \n\n"}
{"id": "1308.2497", "contents": "Title: Bounding the Inefficiency of Altruism Through Social Contribution Games Abstract: We introduce a new class of games, called social contribution games (SCGs),\nwhere each player's individual cost is equal to the cost he induces on society\nbecause of his presence. Our results reveal that SCGs constitute useful\nabstractions of altruistic games when it comes to the analysis of the robust\nprice of anarchy. We first show that SCGs are altruism-independently smooth,\ni.e., the robust price of anarchy of these games remains the same under\narbitrary altruistic extensions. We then devise a general reduction technique\nthat enables us to reduce the problem of establishing smoothness for an\naltruistic extension of a base game to a corresponding SCG. Our reduction\napplies whenever the base game relates to a canonical SCG by satisfying a\nsimple social contribution boundedness property. As it turns out, several\nwell-known games satisfy this property and are thus amenable to our reduction\ntechnique. Examples include min-sum scheduling games, congestion games, second\nprice auctions and valid utility games. Using our technique, we derive mostly\ntight bounds on the robust price of anarchy of their altruistic extensions. For\nthe majority of the mentioned game classes, the results extend to the more\ndifferentiated friendship setting. As we show, our reduction technique covers\nthis model if the base game satisfies three additional natural properties. \n\n"}
{"id": "1308.2655", "contents": "Title: KL-based Control of the Learning Schedule for Surrogate Black-Box\n  Optimization Abstract: This paper investigates the control of an ML component within the Covariance\nMatrix Adaptation Evolution Strategy (CMA-ES) devoted to black-box\noptimization. The known CMA-ES weakness is its sample complexity, the number of\nevaluations of the objective function needed to approximate the global optimum.\nThis weakness is commonly addressed through surrogate optimization, learning an\nestimate of the objective function a.k.a. surrogate model, and replacing most\nevaluations of the true objective function with the (inexpensive) evaluation of\nthe surrogate model. This paper presents a principled control of the learning\nschedule (when to relearn the surrogate model), based on the Kullback-Leibler\ndivergence of the current search distribution and the training distribution of\nthe former surrogate model. The experimental validation of the proposed\napproach shows significant performance gains on a comprehensive set of\nill-conditioned benchmark problems, compared to the best state of the art\nincluding the quasi-Newton high-precision BFGS method. \n\n"}
{"id": "1308.5200", "contents": "Title: Manopt, a Matlab toolbox for optimization on manifolds Abstract: Optimization on manifolds is a rapidly developing branch of nonlinear\noptimization. Its focus is on problems where the smooth geometry of the search\nspace can be leveraged to design efficient numerical algorithms. In particular,\noptimization on manifolds is well-suited to deal with rank and orthogonality\nconstraints. Such structured constraints appear pervasively in machine learning\napplications, including low-rank matrix completion, sensor network\nlocalization, camera network registration, independent component analysis,\nmetric learning, dimensionality reduction and so on. The Manopt toolbox,\navailable at www.manopt.org, is a user-friendly, documented piece of software\ndedicated to simplify experimenting with state of the art Riemannian\noptimization algorithms. We aim particularly at reaching practitioners outside\nour field. \n\n"}
{"id": "1308.5835", "contents": "Title: Backhaul-Aware Interference Management in the Uplink of Wireless Small\n  Cell Networks Abstract: The design of distributed mechanisms for interference management is one of\nthe key challenges in emerging wireless small cell networks whose backhaul is\ncapacity limited and heterogeneous (wired, wireless and a mix thereof). In this\npaper, a novel, backhaul-aware approach to interference management in wireless\nsmall cell networks is proposed. The proposed approach enables macrocell user\nequipments (MUEs) to optimize their uplink performance, by exploiting the\npresence of neighboring small cell base stations. The problem is formulated as\na noncooperative game among the MUEs that seek to optimize their delay-rate\ntradeoff, given the conditions of both the radio access network and the --\npossibly heterogeneous -- backhaul. To solve this game, a novel, distributed\nlearning algorithm is proposed using which the MUEs autonomously choose their\noptimal uplink transmission strategies, given a limited amount of available\ninformation. The convergence of the proposed algorithm is shown and its\nproperties are studied. Simulation results show that, under various types of\nbackhauls, the proposed approach yields significant performance gains, in terms\nof both average throughput and delay for the MUEs, when compared to existing\nbenchmark algorithms. \n\n"}
{"id": "1308.6025", "contents": "Title: Small-Support Approximate Correlated Equilibria Abstract: We prove the existence of approximate correlated equilibrium of support size\npolylogarithmic in the number of players and the number of actions per player.\nIn particular, using the probabilistic method, we show that there exists a\nmultiset of polylogarithmic size such that the uniform distribution over this\nmultiset forms an approximate correlated equilibrium. Along similar lines, we\nestablish the existence of approximate coarse correlated equilibrium with\nlogarithmic support.\n  We complement these results by considering the computational complexity of\ndetermining small-support approximate equilibria. We show that random sampling\ncan be used to efficiently determine an approximate coarse correlated\nequilibrium with logarithmic support. But, such a tight result does not hold\nfor correlated equilibrium, i.e., sampling might generate an approximate\ncorrelated equilibrium of support size \\Omega(m) where m is the number of\nactions per player. Finally, we show that finding an exact correlated\nequilibrium with smallest possible support is NP-hard under Cook reductions,\neven in the case of two-player zero-sum games. \n\n"}
{"id": "1309.1541", "contents": "Title: Projection onto the probability simplex: An efficient algorithm with a\n  simple proof, and an application Abstract: We provide an elementary proof of a simple, efficient algorithm for computing\nthe Euclidean projection of a point onto the probability simplex. We also show\nan application in Laplacian K-modes clustering. \n\n"}
{"id": "1309.2529", "contents": "Title: Limits of Efficiency in Sequential Auctions Abstract: We study the efficiency of sequential first-price item auctions at (subgame\nperfect) equilibrium. This auction format has recently attracted much\nattention, with previous work establishing positive results for unit-demand\nvaluations and negative results for submodular valuations. This leaves a large\ngap in our understanding between these valuation classes. In this work we\nresolve this gap on the negative side. In particular, we show that even in the\nvery restricted case in which each bidder has either an additive valuation or a\nunit-demand valuation, there exist instances in which the inefficiency at\nequilibrium grows linearly with the minimum of the number of items and the\nnumber of bidders. Moreover, these inefficient equilibria persist even under\niterated elimination of weakly dominated strategies. Our main result implies\nlinear inefficiency for many natural settings, including auctions with gross\nsubstitute valuations, capacitated valuations, budget-additive valuations, and\nadditive valuations with hard budget constraints on the payments. Another\nimplication is that the inefficiency in sequential auctions is driven by the\nmaximum number of items contained in any player's optimal set, and this is\ntight. For capacitated valuations, our results imply a lower bound that equals\nthe maximum capacity of any bidder, which is tight following the upper-bound\ntechnique established by Paes Leme et al. \\cite{PaesLeme2012}. \n\n"}
{"id": "1309.4372", "contents": "Title: Faithful Implementations of Distributed Algorithms and Control Laws Abstract: When a distributed algorithm must be executed by strategic agents with\nmisaligned interests, a social leader needs to introduce an appropriate\ntax/subsidy mechanism to incentivize agents to faithfully implement the\nintended algorithm so that a correct outcome is obtained. We discuss the\nincentive issues of implementing economically efficient distributed algorithms\nusing the framework of indirect mechanism design theory. In particular, we show\nthat indirect Groves mechanisms are not only sufficient but also necessary to\nachieve incentive compatibility. This result can be viewed as a generalization\nof the Green-Laffont theorem to indirect mechanisms. Then we introduce the\nnotion of asymptotic incentive compatibility as an appropriate solution concept\nto faithfully implement distributed and iterative optimization algorithms. We\nconsider two special types of optimization algorithms: dual decomposition\nalgorithms for resource allocation and average consensus algorithms. \n\n"}
{"id": "1309.7258", "contents": "Title: Polylogarithmic Supports are required for Approximate Well-Supported\n  Nash Equilibria below 2/3 Abstract: In an epsilon-approximate Nash equilibrium, a player can gain at most epsilon\nin expectation by unilateral deviation. An epsilon well-supported approximate\nNash equilibrium has the stronger requirement that every pure strategy used\nwith positive probability must have payoff within epsilon of the best response\npayoff. Daskalakis, Mehta and Papadimitriou conjectured that every win-lose\nbimatrix game has a 2/3-well-supported Nash equilibrium that uses supports of\ncardinality at most three. Indeed, they showed that such an equilibrium will\nexist subject to the correctness of a graph-theoretic conjecture. Regardless of\nthe correctness of this conjecture, we show that the barrier of a 2/3 payoff\nguarantee cannot be broken with constant size supports; we construct win-lose\ngames that require supports of cardinality at least Omega((log n)^(1/3)) in any\nepsilon-well supported equilibrium with epsilon < 2/3. The key tool in showing\nthe validity of the construction is a proof of a bipartite digraph variant of\nthe well-known Caccetta-Haggkvist conjecture. A probabilistic argument shows\nthat there exist epsilon-well-supported equilibria with supports of cardinality\nO(log n/(epsilon^2)), for any epsilon> 0; thus, the polylogarithmic cardinality\nbound presented cannot be greatly improved. We also show that for any delta >\n0, there exist win-lose games for which no pair of strategies with support\nsizes at most two is a (1-delta)-well-supported Nash equilibrium. In contrast,\nevery bimatrix game with payoffs in [0,1] has a 1/2-approximate Nash\nequilibrium where the supports of the players have cardinality at most two. \n\n"}
{"id": "1309.7824", "contents": "Title: Linear Regression from Strategic Data Sources Abstract: Linear regression is a fundamental building block of statistical data\nanalysis. It amounts to estimating the parameters of a linear model that maps\ninput features to corresponding outputs. In the classical setting where the\nprecision of each data point is fixed, the famous Aitken/Gauss-Markov theorem\nin statistics states that generalized least squares (GLS) is a so-called \"Best\nLinear Unbiased Estimator\" (BLUE). In modern data science, however, one often\nfaces strategic data sources, namely, individuals who incur a cost for\nproviding high-precision data.\n  In this paper, we study a setting in which features are public but\nindividuals choose the precision of the outputs they reveal to an analyst. We\nassume that the analyst performs linear regression on this dataset, and\nindividuals benefit from the outcome of this estimation. We model this scenario\nas a game where individuals minimize a cost comprising two components: (a) an\n(agent-specific) disclosure cost for providing high-precision data; and (b) a\n(global) estimation cost representing the inaccuracy in the linear model\nestimate. In this game, the linear model estimate is a public good that\nbenefits all individuals. We establish that this game has a unique non-trivial\nNash equilibrium. We study the efficiency of this equilibrium and we prove\ntight bounds on the price of stability for a large class of disclosure and\nestimation costs. Finally, we study the estimator accuracy achieved at\nequilibrium. We show that, in general, Aitken's theorem does not hold under\nstrategic data sources, though it does hold if individuals have identical\ndisclosure costs (up to a multiplicative factor). When individuals have\nnon-identical costs, we derive a bound on the improvement of the equilibrium\nestimation cost that can be achieved by deviating from GLS, under mild\nassumptions on the disclosure cost functions. \n\n"}
{"id": "1310.1840", "contents": "Title: Parallel coordinate descent for the Adaboost problem Abstract: We design a randomised parallel version of Adaboost based on previous studies\non parallel coordinate descent. The algorithm uses the fact that the logarithm\nof the exponential loss is a function with coordinate-wise Lipschitz continuous\ngradient, in order to define the step lengths. We provide the proof of\nconvergence for this randomised Adaboost algorithm and a theoretical\nparallelisation speedup factor. We finally provide numerical examples on\nlearning problems of various sizes that show that the algorithm is competitive\nwith concurrent approaches, especially for large scale problems. \n\n"}
{"id": "1310.3593", "contents": "Title: Stability of Mixed-Strategy-Based Iterative Logit Quantal Response\n  Dynamics in Game Theory Abstract: Using the Logit quantal response form as the response function in each step,\nthe original definition of static quantal response equilibrium (QRE) is\nextended into an iterative evolution process. QREs remain as the fixed points\nof the dynamic process. However, depending on whether such fixed points are the\nlong-term solutions of the dynamic process, they can be classified into stable\n(SQREs) and unstable (USQREs) equilibriums. This extension resembles the\nextension from static Nash equilibriums (NEs) to evolutionary stable solutions\nin the framework of evolutionary game theory. The relation between SQREs and\nother solution concepts of games, including NEs and QREs, is discussed. Using\nexperimental data from other published papers, we perform a preliminary\ncomparison between SQREs, NEs, QREs and the observed behavioral outcomes of\nthose experiments. For certain games, we determine that SQREs have better\npredictive power than QREs and NEs. \n\n"}
{"id": "1310.5715", "contents": "Title: Stochastic Gradient Descent, Weighted Sampling, and the Randomized\n  Kaczmarz algorithm Abstract: We obtain an improved finite-sample guarantee on the linear convergence of\nstochastic gradient descent for smooth and strongly convex objectives,\nimproving from a quadratic dependence on the conditioning $(L/\\mu)^2$ (where\n$L$ is a bound on the smoothness and $\\mu$ on the strong convexity) to a linear\ndependence on $L/\\mu$. Furthermore, we show how reweighting the sampling\ndistribution (i.e. importance sampling) is necessary in order to further\nimprove convergence, and obtain a linear dependence in the average smoothness,\ndominating previous results. We also discuss importance sampling for SGD more\nbroadly and show how it can improve convergence also in other scenarios. Our\nresults are based on a connection we make between SGD and the randomized\nKaczmarz algorithm, which allows us to transfer ideas between the separate\nbodies of literature studying each of the two methods. In particular, we recast\nthe randomized Kaczmarz algorithm as an instance of SGD, and apply our results\nto prove its exponential convergence, but to the solution of a weighted least\nsquares problem rather than the original least squares problem. We then present\na modified Kaczmarz algorithm with partially biased sampling which does\nconverge to the original least squares solution with the same exponential\nconvergence rate. \n\n"}
{"id": "1310.7247", "contents": "Title: Optimizing scanning strategies: Selecting scanning bandwidth in\n  adversarial RF environments Abstract: In this paper we investigate the problem of designing a spectrum scanning\nstrategy to detect an intelligent Invader who wants to utilize spectrum\nundetected for his/her unapproved purposes. To deal with this problem we apply\ngame-theoretical tools. We model the situation as a game between a Scanner and\nan Invader where the Invader faces a dilemma: the more bandwidth the Invader\nattempts to use leads to a larger payoff if he is not detected, but at the same\ntime also increases the probability of being detected and thus fined.\nSimilarly, the Scanner faces a dilemma: the wider the bandwidth scanned, the\nhigher the probability of detecting the Invader, but at the expense of\nincreasing the cost of building the scanning system. The equilibrium strategies\nare found explicitly and reveal interesting properties. In particular, we have\nfound a discontinuous dependence of the equilibrium strategies on the network\nparameters, fine and the type of the Invader's award. This discontinuity on\nfine means that the network provider has to take into account a human factor\nsince some threshold values of fine could be very sensible for the Invader,\nwhile in other situations simply increasing the fine has minimal deterrence\nimpact. Also we show how different reward types for the Invader (e.g. motivated\nby using different type of application, say, video-streaming or downloading\nfiles) can be incorporated into scanning strategy to increase its efficiency. \n\n"}
{"id": "1310.7300", "contents": "Title: Relax but stay in control: from value to algorithms for online Markov\n  decision processes Abstract: Online learning algorithms are designed to perform in non-stationary\nenvironments, but generally there is no notion of a dynamic state to model\nconstraints on current and future actions as a function of past actions.\nState-based models are common in stochastic control settings, but commonly used\nframeworks such as Markov Decision Processes (MDPs) assume a known stationary\nenvironment. In recent years, there has been a growing interest in combining\nthe above two frameworks and considering an MDP setting in which the cost\nfunction is allowed to change arbitrarily after each time step. However, most\nof the work in this area has been algorithmic: given a problem, one would\ndevelop an algorithm almost from scratch. Moreover, the presence of the state\nand the assumption of an arbitrarily varying environment complicate both the\ntheoretical analysis and the development of computationally efficient methods.\nThis paper describes a broad extension of the ideas proposed by Rakhlin et al.\nto give a general framework for deriving algorithms in an MDP setting with\narbitrarily changing costs. This framework leads to a unifying view of existing\nmethods and provides a general procedure for constructing new ones. Several new\nmethods are presented, and one of them is shown to have important advantages\nover a similar method developed from scratch via an online version of\napproximate dynamic programming. \n\n"}
{"id": "1310.7419", "contents": "Title: Finding Approximate Nash Equilibria of Bimatrix Games via Payoff Queries Abstract: We study the deterministic and randomized query complexity of finding\napproximate equilibria in bimatrix games. We show that the deterministic query\ncomplexity of finding an $\\epsilon$-Nash equilibrium when $\\epsilon <\n\\frac{1}{2}$ is $\\Omega(k^2)$, even in zero-one constant-sum games. In\ncombination with previous results \\cite{FGGS13}, this provides a complete\ncharacterization of the deterministic query complexity of approximate Nash\nequilibria. We also study randomized querying algorithms. We give a randomized\nalgorithm for finding a $(\\frac{3 - \\sqrt{5}}{2} + \\epsilon)$-Nash equilibrium\nusing $O(\\frac{k \\cdot \\log k}{\\epsilon^2})$ payoff queries, which shows that\nthe $\\frac{1}{2}$ barrier for deterministic algorithms can be broken by\nrandomization. For well-supported Nash equilibria (WSNE), we first give a\nrandomized algorithm for finding an $\\epsilon$-WSNE of a zero-sum bimatrix game\nusing $O(\\frac{k \\cdot \\log k}{\\epsilon^4})$ payoff queries, and we then use\nthis to obtain a randomized algorithm for finding a $(\\frac{2}{3} +\n\\epsilon)$-WSNE in a general bimatrix game using $O(\\frac{k \\cdot \\log\nk}{\\epsilon^4})$ payoff queries. Finally, we initiate the study of lower bounds\nagainst randomized algorithms in the context of bimatrix games, by showing that\nrandomized algorithms require $\\Omega(k^2)$ payoff queries in order to find a\n$\\frac{1}{6k}$-Nash equilibrium, even in zero-one constant-sum games. In\nparticular, this rules out query-efficient randomized algorithms for finding\nexact Nash equilibria. \n\n"}
{"id": "1311.0913", "contents": "Title: Bidding Games and Efficient Allocations Abstract: Richman games are zero-sum games, where in each turn players bid in order to\ndetermine who will play next [Lazarus et al.'99]. We extend the theory to\nimpartial general-sum two player games called \\emph{bidding games}, showing the\nexistence of pure subgame-perfect equilibria (PSPE). In particular, we show\nthat PSPEs form a semilattice, with a unique and natural \\emph{Bottom\nEquilibrium}.\n  Our main result shows that if only two actions available to the players in\neach node, then the Bottom Equilibrium has additional properties: (a) utilities\nare monotone in budget; (b) every outcome is Pareto-efficient; and (c) any\nPareto-efficient outcome is attained for some budget.\n  In the context of combinatorial bargaining, we show that a player with a\nfraction of X% of the total budget prefers her allocation to X% of the possible\nallocations. In addition, we provide a polynomial-time algorithm to compute the\nBottom Equilibrium of a binary bidding game. \n\n"}
{"id": "1311.1644", "contents": "Title: The Maximum Entropy Relaxation Path Abstract: The relaxed maximum entropy problem is concerned with finding a probability\ndistribution on a finite set that minimizes the relative entropy to a given\nprior distribution, while satisfying relaxed max-norm constraints with respect\nto a third observed multinomial distribution. We study the entire relaxation\npath for this problem in detail. We show existence and a geometric description\nof the relaxation path. Specifically, we show that the maximum entropy\nrelaxation path admits a planar geometric description as an increasing,\npiecewise linear function in the inverse relaxation parameter. We derive fast\nalgorithms for tracking the path. In various realistic settings, our algorithms\nrequire $O(n\\log(n))$ operations for probability distributions on $n$ points,\nmaking it possible to handle large problems. Once the path has been recovered,\nwe show that given a validation set, the family of admissible models is reduced\nfrom an infinite family to a small, discrete set. We demonstrate the merits of\nour approach in experiments with synthetic data and discuss its potential for\nthe estimation of compact n-gram language models. \n\n"}
{"id": "1311.2578", "contents": "Title: Primal Beats Dual on Online Packing LPs in the Random-Order Model Abstract: We study packing LPs in an online model where the columns are presented to\nthe algorithm in random order. This natural problem was investigated in various\nrecent studies motivated, e.g., by online ad allocations and yield management\nwhere rows correspond to resources and columns to requests specifying demands\nfor resources. Our main contribution is a $1-O(\\sqrt{(\\log{d})/B})$-competitive\nonline algorithm, where $d$ denotes the column sparsity, i.e., the maximum\nnumber of resources that occur in a single column, and $B$ denotes the capacity\nratio $B$, i.e., the ratio between the capacity of a resource and the maximum\ndemand for this resource. In other words, we achieve a $(1 -\n\\epsilon)$-approximation if the capacity ratio satisfies $B=\\Omega((\\log\nd)/\\epsilon^2)$, which is known to be best-possible for any (randomized) online\nalgorithms.\n  Our result improves exponentially on previous work with respect to the\ncapacity ratio. In contrast to existing results on packing LP problems, our\nalgorithm does not use dual prices to guide the allocation of resources.\nInstead, it simply solves, for each request, a scaled version of the partially\nknown primal program and randomly rounds the obtained fractional solution to\nobtain an integral allocation for this request. We show that this simple\nalgorithmic technique is not restricted to packing LPs with large capacity\nratio: We prove an upper bound on the competitive ratio of\n$\\Omega(d^{-1/(B-1)})$, for any $B \\ge 2$. In addition, we show that our\napproach can be combined with VCG payments and obtain an incentive compatible\n$(1-\\epsilon)$-competitive mechanism for packing LPs with $B=\\Omega((\\log\nm)/\\epsilon^2)$, where $m$ is the number of constraints. Finally, we apply our\ntechnique to the generalized assignment problem for which we obtain the first\nonline algorithm with competitive ratio $O(1)$. \n\n"}
{"id": "1311.3088", "contents": "Title: Endogenous games with goals: side-payments among goal-directed\n  artificial agents Abstract: Artificial agents are typically oriented to the realization of an externally\nassigned task and try to optimize over secondary aspects of plan execution such\ntime lapse or power consumption, technically displaying a quasi-dichotomous\npreference relation. Boolean games have been developed as a paradigm for\nmodelling societies of agents with this type of preference. In boolean games\nagents exercise control over propositional variables and strive to achieve a\ngoal formula whose realization might require the opponents' cooperation.\nRecently, a theory of incentive engineering for such games has been devised,\nwhere an external authority steers the outcome of the game towards certain\ndesirable properties consistent with players' goals, by imposing a taxation\nmechanism on the players that makes the outcomes that do not comply with those\nproperties less appealing to them. The present contribution stems from a\ncomplementary perspective and studies, instead, how games with\nquasi-dichotomous preferences can be transformed from inside, rather than from\noutside, by endowing players with the possibility of sacrificing a part of\ntheir payoff received at a certain outcome in order to convince other players\nto play a certain strategy. Concretely we explore the properties of endogenous\ngames with goals, obtained coupling strategic games with goals, a\ngeneralization of boolean games, with the machinery of endogenous games coming\nfrom game theory. We analyze equilibria in those structures, showing the\npreconditions needed for desirable outcomes to be achieved without external\nintervention. What our results show is that endogenous games with goals display\nspecific irreducible features - with respect to what already known for\nendogenous games - which makes them worth studying in their own sake. \n\n"}
{"id": "1311.3238", "contents": "Title: Doomsday Equilibria for Omega-Regular Games Abstract: Two-player games on graphs provide the theoretical frame- work for many\nimportant problems such as reactive synthesis. While the traditional study of\ntwo-player zero-sum games has been extended to multi-player games with several\nnotions of equilibria, they are decidable only for perfect-information games,\nwhereas several applications require imperfect-information games. In this paper\nwe propose a new notion of equilibria, called doomsday equilibria, which is a\nstrategy profile such that all players satisfy their own objective, and if any\ncoalition of players deviates and violates even one of the players objective,\nthen the objective of every player is violated. We present algorithms and\ncomplexity results for deciding the existence of doomsday equilibria for\nvarious classes of omega-regular objectives, both for imperfect-information\ngames, and for perfect-information games. We provide optimal complexity bounds\nfor imperfect-information games, and in most cases for perfect-information\ngames. \n\n"}
{"id": "1311.4721", "contents": "Title: Economic Efficiency Requires Interaction Abstract: We study the necessity of interaction between individuals for obtaining\napproximately efficient allocations. The role of interaction in markets has\nreceived significant attention in economic thinking, e.g. in Hayek's 1945\nclassic paper.\n  We consider this problem in the framework of simultaneous communication\ncomplexity. We analyze the amount of simultaneous communication required for\nachieving an approximately efficient allocation. In particular, we consider two\nsettings: combinatorial auctions with unit demand bidders (bipartite matching)\nand combinatorial auctions with subadditive bidders. For both settings we first\nshow that non-interactive systems have enormous communication costs relative to\ninteractive ones. On the other hand, we show that limited interaction enables\nus to find approximately efficient allocations. \n\n"}
{"id": "1311.6107", "contents": "Title: Off-policy reinforcement learning for $ H_\\infty $ control design Abstract: The $H_\\infty$ control design problem is considered for nonlinear systems\nwith unknown internal system model. It is known that the nonlinear $ H_\\infty $\ncontrol problem can be transformed into solving the so-called\nHamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial\ndifferential equation that is generally impossible to be solved analytically.\nEven worse, model-based approaches cannot be used for approximately solving HJI\nequation, when the accurate system model is unavailable or costly to obtain in\npractice. To overcome these difficulties, an off-policy reinforcement leaning\n(RL) method is introduced to learn the solution of HJI equation from real\nsystem data instead of mathematical system model, and its convergence is\nproved. In the off-policy RL method, the system data can be generated with\narbitrary policies rather than the evaluating policy, which is extremely\nimportant and promising for practical systems. For implementation purpose, a\nneural network (NN) based actor-critic structure is employed and a least-square\nNN weight update algorithm is derived based on the method of weighted\nresiduals. Finally, the developed NN-based off-policy RL method is tested on a\nlinear F16 aircraft plant, and further applied to a rotational/translational\nactuator system. \n\n"}
{"id": "1311.6230", "contents": "Title: Privacy-Preserving Verifiable Incentive Mechanism for Crowdsourcing\n  Market Applications Abstract: Recently, a novel class of incentive mechanisms is proposed to attract\nextensive users to truthfully participate in crowd sensing applications with a\ngiven budget constraint. The class mechanisms also bring good service quality\nfor the requesters in crowd sensing applications. Although it is so important,\nthere still exists many verification and privacy challenges, including users'\nbids and subtask information privacy and identification privacy, winners' set\nprivacy of the platform, and the security of the payment outcomes. In this\npaper, we present a privacy-preserving verifiable incentive mechanism for crowd\nsensing applications with the budget constraint, not only to explore how to\nprotect the privacies of users and the platform, but also to make the\nverifiable payment correct between the platform and users for crowd sensing\napplications. Results indicate that our privacy-preserving verifiable incentive\nmechanism achieves the same results as the generic one without privacy\npreservation. \n\n"}
{"id": "1312.3797", "contents": "Title: Infinite Games Specified by 2-Tape Automata Abstract: We prove that the determinacy of Gale-Stewart games whose winning sets are\ninfinitary rational relations accepted by 2-tape B\\\"uchi automata is equivalent\nto the determinacy of (effective) analytic Gale-Stewart games which is known to\nbe a large cardinal assumption. Then we prove that winning strategies, when\nthey exist, can be very complex, i.e. highly non-effective, in these games. We\nprove the same results for Gale-Stewart games with winning sets accepted by\nreal-time 1-counter B\\\"uchi automata, then extending previous results obtained\nabout these games. Then we consider the strenghs of determinacy for these\ngames, and we prove that there is a transfinite sequence of 2-tape B\\\"uchi\nautomata (respectively, of real-time 1-counter B\\\"uchi automata) $A_\\alpha$,\nindexed by recursive ordinals, such that the games $G(L(A_\\alpha))$ have\nstrictly increasing strenghs of determinacy. Moreover there is a 2-tape B\\\"uchi\nautomaton (respectively, a real-time 1-counter B\\\"uchi automaton) B such that\nthe determinacy of G(L(B)) is equivalent to the (effective) analytic\ndeterminacy and thus has the maximal strength of determinacy. We show also that\nthe determinacy of Wadge games between two players in charge of infinitary\nrational relations accepted by 2-tape B\\\"uchi automata is equivalent to the\n(effective) analytic determinacy, and thus not provable in ZFC. \n\n"}
{"id": "1312.7853", "contents": "Title: Communication Efficient Distributed Optimization using an Approximate\n  Newton-type Method Abstract: We present a novel Newton-type method for distributed optimization, which is\nparticularly well suited for stochastic optimization and learning problems. For\nquadratic objectives, the method enjoys a linear rate of convergence which\nprovably \\emph{improves} with the data size, requiring an essentially constant\nnumber of iterations under reasonable assumptions. We provide theoretical and\nempirical evidence of the advantages of our method compared to other\napproaches, such as one-shot parameter averaging and ADMM. \n\n"}
{"id": "1401.1760", "contents": "Title: Generalized Proportional Allocation Mechanism Design for Unicast Service\n  on the Internet Abstract: In this report we construct two mechanisms that fully implement social\nwelfare maximising allocation in Nash equilibria for the case of a single\ninfinitely divisible good subject to multiple inequality constraints. The first\nmechanism achieves weak budget balance, while the second is an extension of the\nfirst, and achieves strong budget balance. One important application of this\nmechanism is unicast service on the Internet where a network operator wishes to\nallocate rates among strategic users in such a way that maximise overall user\nsatisfaction while respecting capacity constraints on every link in the\nnetwork. The emphasis of this work is on full implementation, which means that\nall Nash equilibria of the induced game result in the optimal allocations of\nthe centralized allocation problem. \n\n"}
{"id": "1401.2086", "contents": "Title: Actor-Critic Algorithms for Learning Nash Equilibria in N-player\n  General-Sum Games Abstract: We consider the problem of finding stationary Nash equilibria (NE) in a\nfinite discounted general-sum stochastic game. We first generalize a non-linear\noptimization problem from Filar and Vrieze [2004] to a $N$-player setting and\nbreak down this problem into simpler sub-problems that ensure there is no\nBellman error for a given state and an agent. We then provide a\ncharacterization of solution points of these sub-problems that correspond to\nNash equilibria of the underlying game and for this purpose, we derive a set of\nnecessary and sufficient SG-SP (Stochastic Game - Sub-Problem) conditions.\nUsing these conditions, we develop two actor-critic algorithms: OFF-SGSP\n(model-based) and ON-SGSP (model-free). Both algorithms use a critic that\nestimates the value function for a fixed policy and an actor that performs\ndescent in the policy space using a descent direction that avoids local minima.\nWe establish that both algorithms converge, in self-play, to the equilibria of\na certain ordinary differential equation (ODE), whose stable limit points\ncoincide with stationary NE of the underlying general-sum stochastic game. On a\nsingle state non-generic game (see Hart and Mas-Colell [2005]) as well as on a\nsynthetic two-player game setup with $810,000$ states, we establish that\nON-SGSP consistently outperforms NashQ ([Hu and Wellman, 2003] and FFQ\n[Littman, 2001] algorithms. \n\n"}
{"id": "1401.3325", "contents": "Title: Equilibria in multi-player multi-outcome infinite sequential games Abstract: We investigate the existence of certain types of equilibria (Nash,\n$\\varepsilon$-Nash, subgame perfect, $\\varepsilon$-subgame perfect,\nPareto-optimal) in multi-player multi-outcome infinite sequential games. We use\ntwo fundamental approaches: one requires strong topological restrictions on the\ngames, but produces very strong existence results. The other merely requires\nsome very basic determinacy properties to still obtain some existence results.\nBoth results are transfer results: starting with the existence of some\nequilibria for a small class of games, they allow us to conclude the existence\nof some type of equilibria for a larger class.\n  To make the abstract results more concrete, we investigate as a special case\ninfinite sequential games with real-valued payoff functions. Depending on the\nclass of payoff functions (continuous, upper semi-continuous, Borel) and\nwhether the game is zero-sum, we obtain various existence results for\nequilibria.\n  Our results hold for games with two or up to countably many players. \n\n"}
{"id": "1401.4092", "contents": "Title: Redrawing the Boundaries on Purchasing Data from Privacy-Sensitive\n  Individuals Abstract: We prove new positive and negative results concerning the existence of\ntruthful and individually rational mechanisms for purchasing private data from\nindividuals with unbounded and sensitive privacy preferences. We strengthen the\nimpossibility results of Ghosh and Roth (EC 2011) by extending it to a much\nwider class of privacy valuations. In particular, these include privacy\nvaluations that are based on ({\\epsilon}, {\\delta})-differentially private\nmechanisms for non-zero {\\delta}, ones where the privacy costs are measured in\na per-database manner (rather than taking the worst case), and ones that do not\ndepend on the payments made to players (which might not be observable to an\nadversary). To bypass this impossibility result, we study a natural special\nsetting where individuals have mono- tonic privacy valuations, which captures\ncommon contexts where certain values for private data are expected to lead to\nhigher valuations for privacy (e.g. having a particular disease). We give new\nmech- anisms that are individually rational for all players with monotonic\nprivacy valuations, truthful for all players whose privacy valuations are not\ntoo large, and accurate if there are not too many players with too-large\nprivacy valuations. We also prove matching lower bounds showing that in some\nrespects our mechanism cannot be improved significantly. \n\n"}
{"id": "1401.4786", "contents": "Title: Common Information based Markov Perfect Equilibria for Linear-Gaussian\n  Games with Asymmetric Information Abstract: We consider a class of two-player dynamic stochastic nonzero-sum games where\nthe state transition and observation equations are linear, and the primitive\nrandom variables are Gaussian. Each controller acquires possibly different\ndynamic information about the state process and the other controller's past\nactions and observations. This leads to a dynamic game of asymmetric\ninformation among the controllers. Building on our earlier work on finite games\nwith asymmetric information, we devise an algorithm to compute a Nash\nequilibrium by using the common information among the controllers. We call such\nequilibria common information based Markov perfect equilibria of the game,\nwhich can be viewed as a refinement of Nash equilibrium in games with\nasymmetric information. If the players' cost functions are quadratic, then we\nshow that under certain conditions a unique common information based Markov\nperfect equilibrium exists. Furthermore, this equilibrium can be computed by\nsolving a sequence of linear equations. We also show through an example that\nthere could be other Nash equilibria in a game of asymmetric information, not\ncorresponding to common information based Markov perfect equilibria. \n\n"}
{"id": "1401.6523", "contents": "Title: Strategic aspects of the probabilistic serial rule for the allocation of\n  goods Abstract: The probabilistic serial (PS) rule is one of the most prominent randomized\nrules for the assignment problem. It is well-known for its superior fairness\nand welfare properties. However, PS is not immune to manipulative behaviour by\nthe agents. We examine computational and non-computational aspects of\nstrategising under the PS rule. Firstly, we study the computational complexity\nof an agent manipulating the PS rule. We present polynomial-time algorithms for\noptimal manipulation. Secondly, we show that expected utility best responses\ncan cycle. Thirdly, we examine the existence and computation of Nash\nequilibrium profiles under the PS rule. We show that a pure Nash equilibrium is\nguaranteed to exist under the PS rule. For two agents, we identify two\ndifferent types of preference profiles that are not only in Nash equilibrium\nbut can also be computed in linear time. Finally, we conduct experiments to\ncheck the frequency of manipulability of the PS rule under different\ncombinations of the number of agents, objects, and utility functions. \n\n"}
{"id": "1401.7625", "contents": "Title: RES: Regularized Stochastic BFGS Algorithm Abstract: RES, a regularized stochastic version of the Broyden-Fletcher-Goldfarb-Shanno\n(BFGS) quasi-Newton method is proposed to solve convex optimization problems\nwith stochastic objectives. The use of stochastic gradient descent algorithms\nis widespread, but the number of iterations required to approximate optimal\narguments can be prohibitive in high dimensional problems. Application of\nsecond order methods, on the other hand, is impracticable because computation\nof objective function Hessian inverses incurs excessive computational cost.\nBFGS modifies gradient descent by introducing a Hessian approximation matrix\ncomputed from finite gradient differences. RES utilizes stochastic gradients in\nlieu of deterministic gradients for both, the determination of descent\ndirections and the approximation of the objective function's curvature. Since\nstochastic gradients can be computed at manageable computational cost RES is\nrealizable and retains the convergence rate advantages of its deterministic\ncounterparts. Convergence results show that lower and upper bounds on the\nHessian egeinvalues of the sample functions are sufficient to guarantee\nconvergence to optimal arguments. Numerical experiments showcase reductions in\nconvergence time relative to stochastic gradient descent algorithms and\nnon-regularized stochastic versions of BFGS. An application of RES to the\nimplementation of support vector machines is developed. \n\n"}
{"id": "1402.0988", "contents": "Title: The inverse problem for power distributions in committees Abstract: Several power indices have been introduced in the literature in order to\nmeasure the influence of individual committee members on the aggregated\ndecision. Here we ask the inverse question and aim to design voting rules for a\ncommittee such that a given desired power distribution is met as closely as\npossible. We present an exact algorithm for a large class of different power\nindices based on integer linear programming. With respect to negative\napproximation results we generalize the approach of Alon and Edelman who\nstudied power distributions for the Banzhaf index, where most of the power is\nconcentrated on few coordinates. It turned out that each Banzhaf vector of an\nn-member committee that is near to such a desired power distribution, has to be\nalso near to the Banzhaf vector of a k-member committee. We show that such\nAlon-Edelman type results are possible for other power indices like e.g. the\nPublic Good index or the Coleman index to prevent actions, while they are\nprincipally impossible for e.g. the Johnston index. \n\n"}
{"id": "1402.2801", "contents": "Title: An Anti-Folk Theorem for Large Repeated Games with Imperfect Monitoring Abstract: We study infinitely repeated games in settings of imperfect monitoring. We\nfirst prove a family of theorems that show that when the signals observed by\nthe players satisfy a condition known as $(\\epsilon, \\gamma)$-differential\nprivacy, that the folk theorem has little bite: for values of $\\epsilon$ and\n$\\gamma$ sufficiently small, for a fixed discount factor, any equilibrium of\nthe repeated game involve players playing approximate equilibria of the stage\ngame in every period. Next, we argue that in large games ($n$ player games in\nwhich unilateral deviations by single players have only a small impact on the\nutility of other players), many monitoring settings naturally lead to signals\nthat satisfy $(\\epsilon,\\gamma)$-differential privacy, for $\\epsilon$ and\n$\\gamma$ tending to zero as the number of players $n$ grows large. We conclude\nthat in such settings, the set of equilibria of the repeated game collapse to\nthe set of equilibria of the stage game. \n\n"}
{"id": "1402.3426", "contents": "Title: Privacy Games: Optimal User-Centric Data Obfuscation Abstract: In this paper, we design user-centric obfuscation mechanisms that impose the\nminimum utility loss for guaranteeing user's privacy. We optimize utility\nsubject to a joint guarantee of differential privacy (indistinguishability) and\ndistortion privacy (inference error). This double shield of protection limits\nthe information leakage through obfuscation mechanism as well as the posterior\ninference. We show that the privacy achieved through joint\ndifferential-distortion mechanisms against optimal attacks is as large as the\nmaximum privacy that can be achieved by either of these mechanisms separately.\nTheir utility cost is also not larger than what either of the differential or\ndistortion mechanisms imposes. We model the optimization problem as a\nleader-follower game between the designer of obfuscation mechanism and the\npotential adversary, and design adaptive mechanisms that anticipate and protect\nagainst optimal inference algorithms. Thus, the obfuscation mechanism is\noptimal against any inference algorithm. \n\n"}
{"id": "1402.5131", "contents": "Title: Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse\n  Optimization and Noisy Matrix Decomposition Abstract: We propose an efficient ADMM method with guarantees for high-dimensional\nproblems. We provide explicit bounds for the sparse optimization problem and\nthe noisy matrix decomposition problem. For sparse optimization, we establish\nthat the modified ADMM method has an optimal convergence rate of\n$\\mathcal{O}(s\\log d/T)$, where $s$ is the sparsity level, $d$ is the data\ndimension and $T$ is the number of steps. This matches with the minimax lower\nbounds for sparse estimation. For matrix decomposition into sparse and low rank\ncomponents, we provide the first guarantees for any online method, and prove a\nconvergence rate of $\\tilde{\\mathcal{O}}((s+r)\\beta^2(p) /T) +\n\\mathcal{O}(1/p)$ for a $p\\times p$ matrix, where $s$ is the sparsity level,\n$r$ is the rank and $\\Theta(\\sqrt{p})\\leq \\beta(p)\\leq \\Theta(p)$. Our\nguarantees match the minimax lower bound with respect to $s,r$ and $T$. In\naddition, we match the minimax lower bound with respect to the matrix dimension\n$p$, i.e. $\\beta(p)=\\Theta(\\sqrt{p})$, for many important statistical models\nincluding the independent noise model, the linear Bayesian network and the\nlatent Gaussian graphical model under some conditions. Our ADMM method is based\non epoch-based annealing and consists of inexpensive steps which involve\nprojections on to simple norm balls. Experiments show that for both sparse\noptimization and matrix decomposition problems, our algorithm outperforms the\nstate-of-the-art methods. In particular, we reach higher accuracy with same\ntime complexity. \n\n"}
{"id": "1402.6779", "contents": "Title: Resourceful Contextual Bandits Abstract: We study contextual bandits with ancillary constraints on resources, which\nare common in real-world applications such as choosing ads or dynamic pricing\nof items. We design the first algorithm for solving these problems that handles\nconstrained resources other than time, and improves over a trivial reduction to\nthe non-contextual case. We consider very general settings for both contextual\nbandits (arbitrary policy sets, e.g. Dudik et al. (UAI'11)) and bandits with\nresource constraints (bandits with knapsacks, Badanidiyuru et al. (FOCS'13)),\nand prove a regret guarantee with near-optimal statistical properties. \n\n"}
{"id": "1403.3080", "contents": "Title: Statistical Decision Making for Optimal Budget Allocation in Crowd\n  Labeling Abstract: In crowd labeling, a large amount of unlabeled data instances are outsourced\nto a crowd of workers. Workers will be paid for each label they provide, but\nthe labeling requester usually has only a limited amount of the budget. Since\ndata instances have different levels of labeling difficulty and workers have\ndifferent reliability, it is desirable to have an optimal policy to allocate\nthe budget among all instance-worker pairs such that the overall labeling\naccuracy is maximized. We consider categorical labeling tasks and formulate the\nbudget allocation problem as a Bayesian Markov decision process (MDP), which\nsimultaneously conducts learning and decision making. Using the dynamic\nprogramming (DP) recurrence, one can obtain the optimal allocation policy.\nHowever, DP quickly becomes computationally intractable when the size of the\nproblem increases. To solve this challenge, we propose a computationally\nefficient approximate policy, called optimistic knowledge gradient policy. Our\nMDP is a quite general framework, which applies to both pull crowdsourcing\nmarketplaces with homogeneous workers and push marketplaces with heterogeneous\nworkers. It can also incorporate the contextual information of instances when\nthey are available. The experiments on both simulated and real data show that\nthe proposed policy achieves a higher labeling accuracy than other existing\npolicies at the same budget level. \n\n"}
{"id": "1403.3881", "contents": "Title: Complexity of Equilibrium in Diffusion Games on Social Networks Abstract: In this paper, we consider the competitive diffusion game, and study the\nexistence of its pure-strategy Nash equilibrium when defined over general\nundirected networks. We first determine the set of pure-strategy Nash\nequilibria for two special but well-known classes of networks, namely the\nlattice and the hypercube. Characterizing the utility of the players in terms\nof graphical distances of their initial seed placements to other nodes in the\nnetwork, we show that in general networks the decision process on the existence\nof pure-strategy Nash equilibrium is an NP-hard problem. Following this, we\nprovide some necessary conditions for a given profile to be a Nash equilibrium.\nFurthermore, we study players' utilities in the competitive diffusion game over\nErdos-Renyi random graphs and show that as the size of the network grows, the\nutilities of the players are highly concentrated around their expectation, and\nare bounded below by some threshold based on the parameters of the network.\nFinally, we obtain a lower bound for the maximum social welfare of the game\nwith two players, and study sub-modularity of the players' utilities. \n\n"}
{"id": "1403.6530", "contents": "Title: Variance-Constrained Actor-Critic Algorithms for Discounted and Average\n  Reward MDPs Abstract: In many sequential decision-making problems we may want to manage risk by\nminimizing some measure of variability in rewards in addition to maximizing a\nstandard criterion. Variance related risk measures are among the most common\nrisk-sensitive criteria in finance and operations research. However, optimizing\nmany such criteria is known to be a hard problem. In this paper, we consider\nboth discounted and average reward Markov decision processes. For each\nformulation, we first define a measure of variability for a policy, which in\nturn gives us a set of risk-sensitive criteria to optimize. For each of these\ncriteria, we derive a formula for computing its gradient. We then devise\nactor-critic algorithms that operate on three timescales - a TD critic on the\nfastest timescale, a policy gradient (actor) on the intermediate timescale, and\na dual ascent for Lagrange multipliers on the slowest timescale. In the\ndiscounted setting, we point out the difficulty in estimating the gradient of\nthe variance of the return and incorporate simultaneous perturbation approaches\nto alleviate this. The average setting, on the other hand, allows for an actor\nupdate using compatible features to estimate the gradient of the variance. We\nestablish the convergence of our algorithms to locally risk-sensitive optimal\npolicies. Finally, we demonstrate the usefulness of our algorithms in a traffic\nsignal control application. \n\n"}
{"id": "1404.1341", "contents": "Title: Multi-dimensional Virtual Values and Second-degree Price Discrimination Abstract: We consider a multi-dimensional screening problem of selling a product with\nmultiple quality levels and design virtual value functions to derive conditions\nthat imply optimality of only selling highest quality. A challenge of designing\nvirtual values for multi-dimensional agents is that a mechanism that pointwise\noptimizes virtual values resulting from a general application of integration by\nparts is not incentive compatible, and no general methodology is known for\nselecting the right paths for integration by parts. We resolve this issue by\nfirst uniquely solving for paths that satisfy certain necessary conditions that\nthe pointwise optimality of the mechanism imposes on virtual values, and then\nidentifying distributions that ensure the resulting virtual surplus is indeed\npointwise optimized by the mechanism. Our method of solving for virtual values\nis general, and as a second application we use it to derive conditions of\noptimality for selling only the grand bundle of items to an agent with additive\npreferences. \n\n"}
{"id": "1404.1989", "contents": "Title: A Graphical Adversarial Risk Analysis Model for Oil and Gas Drilling\n  Cybersecurity Abstract: Oil and gas drilling is based, increasingly, on operational technology, whose\ncybersecurity is complicated by several challenges. We propose a graphical\nmodel for cybersecurity risk assessment based on Adversarial Risk Analysis to\nface those challenges. We also provide an example of the model in the context\nof an offshore drilling rig. The proposed model provides a more formal and\ncomprehensive analysis of risks, still using the standard business language\nbased on decisions, risks, and value. \n\n"}
{"id": "1404.2188", "contents": "Title: A Convolutional Neural Network for Modelling Sentences Abstract: The ability to accurately represent sentences is central to language\nunderstanding. We describe a convolutional architecture dubbed the Dynamic\nConvolutional Neural Network (DCNN) that we adopt for the semantic modelling of\nsentences. The network uses Dynamic k-Max Pooling, a global pooling operation\nover linear sequences. The network handles input sentences of varying length\nand induces a feature graph over the sentence that is capable of explicitly\ncapturing short and long-range relations. The network does not rely on a parse\ntree and is easily applicable to any language. We test the DCNN in four\nexperiments: small scale binary and multi-class sentiment prediction, six-way\nquestion classification and Twitter sentiment prediction by distant\nsupervision. The network achieves excellent performance in the first three\ntasks and a greater than 25% error reduction in the last task with respect to\nthe strongest baseline. \n\n"}
{"id": "1404.2514", "contents": "Title: Quality Sensitive Price Competition in Spectrum Oligopoly:Part 1 Abstract: We investigate a spectrum oligopoly market where primaries lease their\nchannels to secondaries in lieu of financial remuneration. Transmission quality\nof a channel evolves randomly. Each primary has to select the price it would\nquote without knowing the transmission qualities of its competitors' channels.\nEach secondary buys a channel depending on the price and the transmission\nquality a channel offers. We formulate the price selection problem as a non\nco-operative game with primaries as players. In the one-shot game, we show that\nthere exists a unique symmetric Nash Equilibrium(NE) strategy profile and\nexplicitly compute it. Our analysis reveals that under the NE strategy profile\na primary prices its channel to render high quality channel more preferable to\nthe secondary; this negates the popular belief that prices ought to be selected\nto render channels equally preferable to the secondary regardless of their\nqualities. We show the loss of revenue in the asymptotic limit due to the non\nco-operation of primaries. In the repeated version of the game, we characterize\na subgame perfect NE where a primary can attain a payoff arbitrarily close to\nthe payoff it would obtain when primaries co-operate. \n\n"}
{"id": "1404.2861", "contents": "Title: Distributed Signaling Games Abstract: A recurring theme in recent computer science literature is that proper design\nof signaling schemes is a crucial aspect of effective mechanisms aiming to\noptimize social welfare or revenue. One of the research endeavors of this line\nof work is understanding the algorithmic and computational complexity of\ndesigning efficient signaling schemes. In reality, however, information is\ntypically not held by a central authority, but is distributed among multiple\nsources (third-party \"mediators\"), a fact that dramatically changes the\nstrategic and combinatorial nature of the signaling problem, making it a game\nbetween information providers, as opposed to a traditional mechanism design\nproblem.\n  In this paper we introduce {\\em distributed signaling games}, while using\ndisplay advertising as a canonical example for introducing this foundational\nframework. A distributed signaling game may be a pure coordination game (i.e.,\na distributed optimization task), or a non-cooperative game. In the context of\npure coordination games, we show a wide gap between the computational\ncomplexity of the centralized and distributed signaling problems. On the other\nhand, we show that if the information structure of each mediator is assumed to\nbe \"local\", then there is an efficient algorithm that finds a near-optimal\n($5$-approximation) distributed signaling scheme.\n  In the context of non-cooperative games, the outcome generated by the\nmediators' signals may have different value to each (due to the auctioneer's\ndesire to align the incentives of the mediators with his own by relative\ncompensations). We design a mechanism for this problem via a novel application\nof Shapley's value, and show that it possesses some interesting properties, in\nparticular, it always admits a pure Nash equilibrium, and it never decreases\nthe revenue of the auctioneer. \n\n"}
{"id": "1404.5692", "contents": "Title: Forward - Backward Greedy Algorithms for Atomic Norm Regularization Abstract: In many signal processing applications, the aim is to reconstruct a signal\nthat has a simple representation with respect to a certain basis or frame.\nFundamental elements of the basis known as \"atoms\" allow us to define \"atomic\nnorms\" that can be used to formulate convex regularizations for the\nreconstruction problem. Efficient algorithms are available to solve these\nformulations in certain special cases, but an approach that works well for\ngeneral atomic norms, both in terms of speed and reconstruction accuracy,\nremains to be found. This paper describes an optimization algorithm called\nCoGEnT that produces solutions with succinct atomic representations for\nreconstruction problems, generally formulated with atomic-norm constraints.\nCoGEnT combines a greedy selection scheme based on the conditional gradient\napproach with a backward (or \"truncation\") step that exploits the quadratic\nnature of the objective to reduce the basis size. We establish convergence\nproperties and validate the algorithm via extensive numerical experiments on a\nsuite of signal processing applications. Our algorithm and analysis also allow\nfor inexact forward steps and for occasional enhancements of the current\nrepresentation to be performed. CoGEnT can outperform the basic conditional\ngradient method, and indeed many methods that are tailored to specific\napplications, when the enhancement and truncation steps are defined\nappropriately. We also introduce several novel applications that are enabled by\nthe atomic-norm framework, including tensor completion, moment problems in\nsignal processing, and graph deconvolution. \n\n"}
{"id": "1404.5997", "contents": "Title: One weird trick for parallelizing convolutional neural networks Abstract: I present a new way to parallelize the training of convolutional neural\nnetworks across multiple GPUs. The method scales significantly better than all\nalternatives when applied to modern convolutional neural networks. \n\n"}
{"id": "1404.6003", "contents": "Title: Buying Private Data without Verification Abstract: We consider the problem of designing a survey to aggregate non-verifiable\ninformation from a privacy-sensitive population: an analyst wants to compute\nsome aggregate statistic from the private bits held by each member of a\npopulation, but cannot verify the correctness of the bits reported by\nparticipants in his survey. Individuals in the population are strategic agents\nwith a cost for privacy, \\ie, they not only account for the payments they\nexpect to receive from the mechanism, but also their privacy costs from any\ninformation revealed about them by the mechanism's outcome---the computed\nstatistic as well as the payments---to determine their utilities. How can the\nanalyst design payments to obtain an accurate estimate of the population\nstatistic when individuals strategically decide both whether to participate and\nwhether to truthfully report their sensitive information?\n  We design a differentially private peer-prediction mechanism that supports\naccurate estimation of the population statistic as a Bayes-Nash equilibrium in\nsettings where agents have explicit preferences for privacy. The mechanism\nrequires knowledge of the marginal prior distribution on bits $b_i$, but does\nnot need full knowledge of the marginal distribution on the costs $c_i$,\ninstead requiring only an approximate upper bound. Our mechanism guarantees\n$\\epsilon$-differential privacy to each agent $i$ against any adversary who can\nobserve the statistical estimate output by the mechanism, as well as the\npayments made to the $n-1$ other agents $j\\neq i$. Finally, we show that with\nslightly more structured assumptions on the privacy cost functions of each\nagent, the cost of running the survey goes to $0$ as the number of agents\ndiverges. \n\n"}
{"id": "1405.1481", "contents": "Title: Graphical potential games Abstract: We study the class of potential games that are also graphical games with\nrespect to a given graph $G$ of connections between the players. We show that,\nup to strategic equivalence, this class of games can be identified with the set\nof Markov random fields on $G$.\n  From this characterization, and from the Hammersley-Clifford theorem, it\nfollows that the potentials of such games can be decomposed to local\npotentials. We use this decomposition to strongly bound the number of strategy\nchanges of a single player along a better response path. This result extends to\ngeneralized graphical potential games, which are played on infinite graphs. \n\n"}
{"id": "1405.2798", "contents": "Title: Two-Stage Metric Learning Abstract: In this paper, we present a novel two-stage metric learning algorithm. We\nfirst map each learning instance to a probability distribution by computing its\nsimilarities to a set of fixed anchor points. Then, we define the distance in\nthe input data space as the Fisher information distance on the associated\nstatistical manifold. This induces in the input data space a new family of\ndistance metric with unique properties. Unlike kernelized metric learning, we\ndo not require the similarity measure to be positive semi-definite. Moreover,\nit can also be interpreted as a local metric learning algorithm with well\ndefined distance approximation. We evaluate its performance on a number of\ndatasets. It outperforms significantly other metric learning methods and SVM. \n\n"}
{"id": "1405.5940", "contents": "Title: Bayesian Truthful Mechanisms for Job Scheduling from Bi-criterion\n  Approximation Algorithms Abstract: We provide polynomial-time approximately optimal Bayesian mechanisms for\nmakespan minimization on unrelated machines as well as for max-min fair\nallocations of indivisible goods, with approximation factors of $2$ and\n$\\min\\{m-k+1, \\tilde{O}(\\sqrt{k})\\}$ respectively, matching the approximation\nratios of best known polynomial-time \\emph{algorithms} (for max-min fairness,\nthe latter claim is true for certain ratios of the number of goods $m$ to\npeople $k$). Our mechanisms are obtained by establishing a polynomial-time\napproximation-sensitive reduction from the problem of designing approximately\noptimal {\\em mechanisms} for some arbitrary objective ${\\cal O}$ to that of\ndesigning bi-criterion approximation {\\em algorithms} for the same objective\n${\\cal O}$ plus a linear allocation cost term. Our reduction is itself enabled\nby extending the celebrated \"equivalence of separation and\noptimization\"[GLSS81,KP80] to also accommodate bi-criterion approximations.\nMoreover, to apply the reduction to the specific problems of makespan and\nmax-min fairness we develop polynomial-time bi-criterion approximation\nalgorithms for makespan minimization with costs and max-min fairness with\ncosts, adapting the algorithms of [ST93], [BD05] and [AS07] to the type of\nbi-criterion approximation that is required by the reduction. \n\n"}
{"id": "1405.5960", "contents": "Title: LASS: a simple assignment model with Laplacian smoothing Abstract: We consider the problem of learning soft assignments of $N$ items to $K$\ncategories given two sources of information: an item-category similarity\nmatrix, which encourages items to be assigned to categories they are similar to\n(and to not be assigned to categories they are dissimilar to), and an item-item\nsimilarity matrix, which encourages similar items to have similar assignments.\nWe propose a simple quadratic programming model that captures this intuition.\nWe give necessary conditions for its solution to be unique, define an\nout-of-sample mapping, and derive a simple, effective training algorithm based\non the alternating direction method of multipliers. The model predicts\nreasonable assignments from even a few similarity values, and can be seen as a\ngeneralization of semisupervised learning. It is particularly useful when items\nnaturally belong to multiple categories, as for example when annotating\ndocuments with keywords or pictures with tags, with partially tagged items, or\nwhen the categories have complex interrelations (e.g. hierarchical) that are\nunknown. \n\n"}
{"id": "1405.6146", "contents": "Title: A Simple and Approximately Optimal Mechanism for an Additive Buyer Abstract: We consider a monopolist seller with $n$ heterogeneous items, facing a single\nbuyer. The buyer has a value for each item drawn independently according to\n(non-identical) distributions, and her value for a set of items is additive.\nThe seller aims to maximize his revenue.\n  We suggest using the a-priori better of two simple pricing methods: selling\nthe items separately, each at its optimal price, and bundling together, in\nwhich the entire set of items is sold as one bundle at its optimal price. We\nshow that for any distribution, this mechanism achieves a constant-factor\napproximation to the optimal revenue.\n  Beyond its simplicity, this is the first computationally tractable mechanism\nto obtain a constant-factor approximation for this multi-parameter problem. We\nadditionally discuss extensions to multiple buyers and to valuations that are\ncorrelated across items. \n\n"}
{"id": "1406.0576", "contents": "Title: Welfare and Revenue Guarantees for Competitive Bundling Equilibrium Abstract: We study equilibria of markets with $m$ heterogeneous indivisible goods and\n$n$ consumers with combinatorial preferences. It is well known that a\ncompetitive equilibrium is not guaranteed to exist when valuations are not\ngross substitutes. Given the widespread use of bundling in real-life markets,\nwe study its role as a stabilizing and coordinating device by considering the\nnotion of \\emph{competitive bundling equilibrium}: a competitive equilibrium\nover the market induced by partitioning the goods for sale into fixed bundles.\nCompared to other equilibrium concepts involving bundles, this notion has the\nadvantage of simulatneous succinctness ($O(m)$ prices) and market clearance.\n  Our first set of results concern welfare guarantees. We show that in markets\nwhere consumers care only about the number of goods they receive (known as\nmulti-unit or homogeneous markets), even in the presence of complementarities,\nthere always exists a competitive bundling equilibrium that guarantees a\nlogarithmic fraction of the optimal welfare, and this guarantee is tight. We\nalso establish non-trivial welfare guarantees for general markets, two-consumer\nmarkets, and markets where the consumer valuations are additive up to a fixed\nbudget (budget-additive).\n  Our second set of results concern revenue guarantees. Motivated by the fact\nthat the revenue extracted in a standard competitive equilibrium may be zero\n(even with simple unit-demand consumers), we show that for natural subclasses\nof gross substitutes valuations, there always exists a competitive bundling\nequilibrium that extracts a logarithmic fraction of the optimal welfare, and\nthis guarantee is tight. The notion of competitive bundling equilibrium can\nthus be useful even in markets which possess a standard competitive\nequilibrium. \n\n"}
{"id": "1406.1222", "contents": "Title: Discovering Structure in High-Dimensional Data Through Correlation\n  Explanation Abstract: We introduce a method to learn a hierarchy of successively more abstract\nrepresentations of complex data based on optimizing an information-theoretic\nobjective. Intuitively, the optimization searches for a set of latent factors\nthat best explain the correlations in the data as measured by multivariate\nmutual information. The method is unsupervised, requires no model assumptions,\nand scales linearly with the number of variables which makes it an attractive\napproach for very high dimensional systems. We demonstrate that Correlation\nExplanation (CorEx) automatically discovers meaningful structure for data from\ndiverse sources including personality tests, DNA, and human language. \n\n"}
{"id": "1406.3278", "contents": "Title: An n-to-1 Bidder Reduction for Multi-item Auctions and its Applications Abstract: In this paper, we introduce a novel approach for reducing the $k$-item\n$n$-bidder auction with additive valuation to $k$-item $1$-bidder auctions.\nThis approach, called the \\emph{Best-Guess} reduction, can be applied to\naddress several central questions in optimal revenue auction theory such as the\npower of randomization, and Bayesian versus dominant-strategy implementations.\nFirst, when the items have independent valuation distributions, we present a\ndeterministic mechanism called {\\it Deterministic Best-Guess} that yields at\nleast a constant fraction of the optimal revenue by any randomized mechanism.\nSecond, if all the $nk$ valuation random variables are independent, the optimal\nrevenue achievable in {\\it dominant strategy incentive compatibility} (DSIC) is\nshown to be at least a constant fraction of that achievable in {\\it Bayesian\nincentive compatibility} (BIC). Third, when all the $nk$ values are identically\ndistributed according to a common one-dimensional distribution $F$, the optimal\nrevenue is shown to be expressible in the closed form $\\Theta(k(r+\\int_0^{mr}\n(1-F(x)^n) \\ud x))$ where $r= sup_{x\\geq 0} \\, x(1 - F(x)^n)$ and $m=\\lceil\nk/n\\rceil$; this revenue is achievable by a simple mechanism called\n\\emph{2nd-Price Bundling}. All our results apply to arbitrary distributions,\nregular or irregular. \n\n"}
{"id": "1406.3278", "contents": "Title: An n-to-1 Bidder Reduction for Multi-item Auctions and its Applications Abstract: In this paper, we introduce a novel approach for reducing the $k$-item\n$n$-bidder auction with additive valuation to $k$-item $1$-bidder auctions.\nThis approach, called the \\emph{Best-Guess} reduction, can be applied to\naddress several central questions in optimal revenue auction theory such as the\npower of randomization, and Bayesian versus dominant-strategy implementations.\nFirst, when the items have independent valuation distributions, we present a\ndeterministic mechanism called {\\it Deterministic Best-Guess} that yields at\nleast a constant fraction of the optimal revenue by any randomized mechanism.\nSecond, if all the $nk$ valuation random variables are independent, the optimal\nrevenue achievable in {\\it dominant strategy incentive compatibility} (DSIC) is\nshown to be at least a constant fraction of that achievable in {\\it Bayesian\nincentive compatibility} (BIC). Third, when all the $nk$ values are identically\ndistributed according to a common one-dimensional distribution $F$, the optimal\nrevenue is shown to be expressible in the closed form $\\Theta(k(r+\\int_0^{mr}\n(1-F(x)^n) \\ud x))$ where $r= sup_{x\\geq 0} \\, x(1 - F(x)^n)$ and $m=\\lceil\nk/n\\rceil$; this revenue is achievable by a simple mechanism called\n\\emph{2nd-Price Bundling}. All our results apply to arbitrary distributions,\nregular or irregular. \n\n"}
{"id": "1406.4248", "contents": "Title: On values of repeated games with signals Abstract: We study the existence of different notions of value in two-person zero-sum\nrepeated games where the state evolves and players receive signals. We provide\nsome examples showing that the limsup value (and the uniform value) may not\nexist in general. Then we show the existence of the value for any Borel payoff\nfunction if the players observe a public signal including the actions played.\nWe also prove two other positive results without assumptions on the signaling\nstructure: the existence of the $\\sup$ value in any game and the existence of\nthe uniform value in recursive games with nonnegative payoffs. \n\n"}
{"id": "1406.5370", "contents": "Title: Spectral Ranking using Seriation Abstract: We describe a seriation algorithm for ranking a set of items given pairwise\ncomparisons between these items. Intuitively, the algorithm assigns similar\nrankings to items that compare similarly with all others. It does so by\nconstructing a similarity matrix from pairwise comparisons, using seriation\nmethods to reorder this matrix and construct a ranking. We first show that this\nspectral seriation algorithm recovers the true ranking when all pairwise\ncomparisons are observed and consistent with a total order. We then show that\nranking reconstruction is still exact when some pairwise comparisons are\ncorrupted or missing, and that seriation based spectral ranking is more robust\nto noise than classical scoring methods. Finally, we bound the ranking error\nwhen only a random subset of the comparions are observed. An additional benefit\nof the seriation formulation is that it allows us to solve semi-supervised\nranking problems. Experiments on both synthetic and real datasets demonstrate\nthat seriation based spectral ranking achieves competitive and in some cases\nsuperior performance compared to classical ranking methods. \n\n"}
{"id": "1406.5433", "contents": "Title: The tropical shadow-vertex algorithm solves mean payoff games in\n  polynomial time on average Abstract: We introduce an algorithm which solves mean payoff games in polynomial time\non average, assuming the distribution of the games satisfies a flip invariance\nproperty on the set of actions associated with every state. The algorithm is a\ntropical analogue of the shadow-vertex simplex algorithm, which solves mean\npayoff games via linear feasibility problems over the tropical semiring\n$(\\mathbb{R} \\cup \\{-\\infty\\}, \\max, +)$. The key ingredient in our approach is\nthat the shadow-vertex pivoting rule can be transferred to tropical polyhedra,\nand that its computation reduces to optimal assignment problems through\nPl\\\"ucker relations. \n\n"}
{"id": "1406.6773", "contents": "Title: Approximately Optimal Mechanism Design: Motivation, Examples, and\n  Lessons Learned Abstract: Optimal mechanism design enjoys a beautiful and well-developed theory, and\nalso a number of killer applications. Rules of thumb produced by the field\ninfluence everything from how governments sell wireless spectrum licenses to\nhow the major search engines auction off online advertising. There are,\nhowever, some basic problems for which the traditional optimal mechanism design\napproach is ill-suited --- either because it makes overly strong assumptions,\nor because it advocates overly complex designs. The thesis of this paper is\nthat approximately optimal mechanisms allow us to reason about fundamental\nquestions that seem out of reach of the traditional theory.\n  This survey has three main parts. The first part describes the approximately\noptimal mechanism design paradigm --- how it works, and what we aim to learn by\napplying it. The second and third parts of the survey cover two case studies,\nwhere we instantiate the general design paradigm to investigate two basic\nquestions. In the first example, we consider revenue maximization in a\nsingle-item auction with heterogeneous bidders. Our goal is to understand if\ncomplexity --- in the sense of detailed distributional knowledge --- is an\nessential feature of good auctions for this problem, or alternatively if there\nare simpler auctions that are near-optimal. The second example considers\nwelfare maximization with multiple items. Our goal here is similar in spirit:\nwhen is complexity --- in the form of high-dimensional bid spaces --- an\nessential feature of every auction that guarantees reasonable welfare? Are\nthere interesting cases where low-dimensional bid spaces suffice? \n\n"}
{"id": "1406.7443", "contents": "Title: Efficient Learning in Large-Scale Combinatorial Semi-Bandits Abstract: A stochastic combinatorial semi-bandit is an online learning problem where at\neach step a learning agent chooses a subset of ground items subject to\ncombinatorial constraints, and then observes stochastic weights of these items\nand receives their sum as a payoff. In this paper, we consider efficient\nlearning in large-scale combinatorial semi-bandits with linear generalization,\nand as a solution, propose two learning algorithms called Combinatorial Linear\nThompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both\nalgorithms are computationally efficient as long as the offline version of the\ncombinatorial problem can be solved efficiently. We establish that CombLinTS\nand CombLinUCB are also provably statistically efficient under reasonable\nassumptions, by developing regret bounds that are independent of the problem\nscale (number of items) and sublinear in time. We also evaluate CombLinTS on a\nvariety of problems with thousands of items. Our experiment results demonstrate\nthat CombLinTS is scalable, robust to the choice of algorithm parameters, and\nsignificantly outperforms the best of our baselines. \n\n"}
{"id": "1407.0202", "contents": "Title: SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly\n  Convex Composite Objectives Abstract: In this work we introduce a new optimisation method called SAGA in the spirit\nof SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient\nalgorithms with fast linear convergence rates. SAGA improves on the theory\nbehind SAG and SVRG, with better theoretical convergence rates, and has support\nfor composite objectives where a proximal operator is used on the regulariser.\nUnlike SDCA, SAGA supports non-strongly convex problems directly, and is\nadaptive to any inherent strong convexity of the problem. We give experimental\nresults showing the effectiveness of our method. \n\n"}
{"id": "1407.0420", "contents": "Title: Cooperative Games with Overlapping Coalitions: Charting the Tractability\n  Frontier Abstract: In many multiagent scenarios, agents distribute resources, such as time or\nenergy, among several tasks. Having completed their tasks and generated\nprofits, task payoffs must be divided among the agents in some reasonable\nmanner. Cooperative games with overlapping coalitions (OCF games) are a recent\nframework proposed by Chalkiadakis et al. (2010), generalizing classic\ncooperative games to the case where agents may belong to more than one\ncoalition. Having formed overlapping coalitions and divided profits, some\nagents may feel dissatisfied with their share of the profits, and would like to\ndeviate from the given outcome. However, deviation in OCF games is a\ncomplicated matter: agents may decide to withdraw only some of their weight\nfrom some of the coalitions they belong to; that is, even after deviation, it\nis possible that agents will still be involved in tasks with non-deviators.\nThis means that the desirability of a deviation, and the stability of formed\ncoalitions, is to a great extent determined by the reaction of non-deviators.\nIn this work, we explore algorithmic aspects of OCF games, focusing on the core\nin OCF games. We study the problem of deciding if the core of an OCF game is\nnot empty, and whether a core payoff division can be found in polynomial time;\nmoreover, we identify conditions that ensure that the problem admits polynomial\ntime algorithms. Finally, we introduce and study a natural class of OCF games,\nLinear Bottleneck Games. Interestingly, we show that such games always have a\nnon-empty core, even assuming a highly lenient reaction to deviations. \n\n"}
{"id": "1407.1170", "contents": "Title: Incentive and stability in the Rock-Paper-Scissors game: an experimental\n  investigation Abstract: In a two-person Rock-Paper-Scissors (RPS) game, if we set a loss worth\nnothing and a tie worth 1, and the payoff of winning (the incentive a) as a\nvariable, this game is called as generalized RPS game. The generalized RPS game\nis a representative mathematical model to illustrate the game dynamics,\nappearing widely in textbook. However, how actual motions in these games depend\non the incentive has never been reported quantitatively. Using the data from 7\ngames with different incentives, including 84 groups of 6 subjects playing the\ngame in 300-round, with random-pair tournaments and local information recorded,\nwe find that, both on social and individual level, the actual motions are\nchanging continuously with the incentive. More expressively, some\nrepresentative findings are, (1) in social collective strategy transit views,\nthe forward transition vector field is more and more centripetal as the\nstability of the system increasing; (2) In the individual behavior of strategy\ntransit view, there exists a phase transformation as the stability of the\nsystems increasing, and the phase transformation point being near the standard\nRPS; (3) Conditional response behaviors are structurally changing accompanied\nby the controlled incentive. As a whole, the best response behavior increases\nand the win-stay lose-shift (WSLS) behavior declines with the incentive.\nFurther, the outcome of win, tie, and lose influence the best response behavior\nand WSLS behavior. Both as the best response behavior, the win-stay behavior\ndeclines with the incentive while the lose-left-shift behavior increase with\nthe incentive. And both as the WSLS behavior, the lose-left-shift behavior\nincrease with the incentive, but the lose-right-shift behaviors declines with\nthe incentive. We hope to learn which one in tens of learning models can\ninterpret the empirical observation above. \n\n"}
{"id": "1407.2143", "contents": "Title: Parameterized Algorithmics for Computational Social Choice: Nine\n  Research Challenges Abstract: Computational Social Choice is an interdisciplinary research area involving\nEconomics, Political Science, and Social Science on the one side, and\nMathematics and Computer Science (including Artificial Intelligence and\nMultiagent Systems) on the other side. Typical computational problems studied\nin this field include the vulnerability of voting procedures against attacks,\nor preference aggregation in multi-agent systems. Parameterized Algorithmics is\na subfield of Theoretical Computer Science seeking to exploit meaningful\nproblem-specific parameters in order to identify tractable special cases of in\ngeneral computationally hard problems. In this paper, we propose nine of our\nfavorite research challenges concerning the parameterized complexity of\nproblems appearing in this context. \n\n"}
{"id": "1407.2479", "contents": "Title: Making the Most of Your Samples Abstract: We study the problem of setting a price for a potential buyer with a\nvaluation drawn from an unknown distribution $D$. The seller has \"data\"' about\n$D$ in the form of $m \\ge 1$ i.i.d. samples, and the algorithmic challenge is\nto use these samples to obtain expected revenue as close as possible to what\ncould be achieved with advance knowledge of $D$.\n  Our first set of results quantifies the number of samples $m$ that are\nnecessary and sufficient to obtain a $(1-\\epsilon)$-approximation. For example,\nfor an unknown distribution that satisfies the monotone hazard rate (MHR)\ncondition, we prove that $\\tilde{\\Theta}(\\epsilon^{-3/2})$ samples are\nnecessary and sufficient. Remarkably, this is fewer samples than is necessary\nto accurately estimate the expected revenue obtained by even a single reserve\nprice. We also prove essentially tight sample complexity bounds for regular\ndistributions, bounded-support distributions, and a wide class of irregular\ndistributions. Our lower bound approach borrows tools from differential privacy\nand information theory, and we believe it could find further applications in\nauction theory.\n  Our second set of results considers the single-sample case. For regular\ndistributions, we prove that no pricing strategy is better than\n$\\tfrac{1}{2}$-approximate, and this is optimal by the Bulow-Klemperer theorem.\nFor MHR distributions, we show how to do better: we give a simple pricing\nstrategy that guarantees expected revenue at least $0.589$ times the maximum\npossible. We also prove that no pricing strategy achieves an approximation\nguarantee better than $\\frac{e}{4} \\approx .68$. \n\n"}
{"id": "1407.2576", "contents": "Title: The size of the core in assignment markets Abstract: Assignment markets involve matching with transfers, as in labor markets and\nhousing markets. We consider a two-sided assignment market with agent types and\nstochastic structure similar to models used in empirical studies, and\ncharacterize the size of the core in such markets. Each agent has a randomly\ndrawn productivity with respect to each type of agent on the other side. The\nvalue generated from a match between a pair of agents is the sum of the two\nproductivity terms, each of which depends only on the type but not the identity\nof one of the agents, and a third deterministic term driven by the pair of\ntypes. We allow the number of agents to grow, keeping the number of agent types\nfixed. Let $n$ be the number of agents and $K$ be the number of types on the\nside of the market with more types. We find, under reasonable assumptions, that\nthe relative variation in utility per agent over core outcomes is bounded as\n$O^*(1/n^{1/K})$, where polylogarithmic factors have been suppressed. Further,\nwe show that this bound is tight in worst case. We also provide a tighter bound\nunder more restrictive assumptions. Our results provide partial justification\nfor the typical assumption of a unique core outcome in empirical studies. \n\n"}
{"id": "1407.3028", "contents": "Title: Hidden Stochastic Games and Limit Equilibrium Payoffs Abstract: We consider 2-player stochastic games with perfectly observed actions, and\nstudy the limit, as the discount factor goes to one, of the equilibrium payoffs\nset. In the usual setup where current states are observed by the players, we\nshow that the set of stationary equilibrium payoffs always converges, and\nprovide a simple example where the set of equilibrium payoffs has no limit. We\nthen introduce the more general model of hidden stochastic game, where the\nplayers publicly receive imperfect signals over current states. In this setup\nwe present an example where not only the limit set of equilibrium payoffs does\nnot exist, but there is no converging selection of equilibrium payoffs. This\nsecond example is robust in many aspects, in particular to perturbations of the\npayoffs and to the introduction of correlation or communication devices. \n\n"}
{"id": "1407.5030", "contents": "Title: To Reach or not to Reach? Efficient Algorithms for Total-Payoff Games Abstract: Quantitative games are two-player zero-sum games played on directed weighted\ngraphs. Total-payoff games (that can be seen as a refinement of the\nwell-studied mean-payoff games) are the variant where the payoff of a play is\ncomputed as the sum of the weights. Our aim is to describe the first\npseudo-polynomial time algorithm for total-payoff games in the presence of\narbitrary weights. It consists of a non-trivial application of the value\niteration paradigm. Indeed, it requires to study, as a milestone, a refinement\nof these games, called min-cost reachability games, where we add a reachability\nobjective to one of the players. For these games, we give an efficient value\niteration algorithm to compute the values and optimal strategies (when they\nexist), that runs in pseudo-polynomial time. We also propose heuristics\nallowing one to possibly speed up the computations in both cases. \n\n"}
{"id": "1407.5373", "contents": "Title: On the Complexity of Dynamic Mechanism Design Abstract: We introduce a dynamic mechanism design problem in which the designer wants\nto offer for sale an item to an agent, and another item to the same agent at\nsome point in the future. The agent's joint distribution of valuations for the\ntwo items is known, and the agent knows the valuation for the current item (but\nnot for the one in the future). The designer seeks to maximize expected\nrevenue, and the auction must be deterministic, truthful, and ex post\nindividually rational. The optimum mechanism involves a protocol whereby the\nseller elicits the buyer's current valuation, and based on the bid makes two\ntake-it-or-leave-it offers, one for now and one for the future. We show that\nfinding the optimum deterministic mechanism in this situation - arguably the\nsimplest meaningful dynamic mechanism design problem imaginable - is NP-hard.\nWe also prove several positive results, among them a polynomial linear\nprogramming-based algorithm for the optimum randomized auction (even for many\nbidders and periods), and we show strong separations in revenue between\nnon-adaptive, adaptive, and randomized auctions, even when the valuations in\nthe two periods are uncorrelated. Finally, for the same problem in an\nenvironment in which contracts cannot be enforced, and thus perfection of\nequilibrium is necessary, we show that the optimum randomized mechanism\nrequires multiple rounds of cheap talk-like interactions. \n\n"}
{"id": "1407.5587", "contents": "Title: Weihrauch degrees of finding equilibria in sequential games Abstract: We consider the degrees of non-computability (Weihrauch degrees) of finding\nwinning strategies (or more generally, Nash equilibria) in infinite sequential\ngames with certain winning sets (or more generally, outcome sets). In\nparticular, we show that as the complexity of the winning sets increases in the\ndifference hierarchy, the complexity of constructing winning strategies\nincreases in the effective Borel hierarchy. \n\n"}
{"id": "1407.6731", "contents": "Title: Optimal User-Cell Association for Massive MIMO Wireless Networks Abstract: The use of a very large number of antennas at each base station site\n(referred to as \"Massive MIMO\") is one of the most promising approaches to cope\nwith the predicted wireless data traffic explosion. In combination with Time\nDivision Duplex and with simple per-cell processing, it achieves large\nthroughput per cell, low latency, and attractive power efficiency performance.\nFollowing the current wireless technology trend of moving to higher frequency\nbands and denser small cell deployments, a large number of antennas can be\nimplemented within a small form factor even in small cell base stations. In a\nheterogeneous network formed by large (macro) and small cell BSs, a key system\noptimization problem consists of \"load balancing\", that is, associating users\nto BSs in order to avoid congested hot-spots and/or under-utilized\ninfrastructure. In this paper, we consider the user-BS association problem for\na massive MIMO heterogeneous network. We formulate the problem as a network\nutility maximization, and provide a centralized solution in terms of the\nfraction of transmission resources (time-frequency slots) over which each user\nis served by a given BS. Furthermore, we show that such a solution is\nphysically realizable, i.e., there exists a sequence of integer scheduling\nconfigurations realizing (by time-sharing) the optimal fractions. While this\nsolution is optimal, it requires centralized computation. Then, we also\nconsider decentralized user-centric schemes, formulated as non-cooperative\ngames where each user makes individual selfish association decisions based only\non its local information. We identify a class of schemes such that their Nash\nequilibrium is very close to the global centralized optimum. Hence, these\nuser-centric algorithms are attractive not only for their simplicity and fully\ndecentralized implementation, but also because they operate near the system\n\"social\" optimum. \n\n"}
{"id": "1407.7740", "contents": "Title: Privacy and Truthful Equilibrium Selection for Aggregative Games Abstract: We study a very general class of games --- multi-dimensional aggregative\ngames --- which in particular generalize both anonymous games and weighted\ncongestion games. For any such game that is also large, we solve the\nequilibrium selection problem in a strong sense. In particular, we give an\nefficient weak mediator: a mechanism which has only the power to listen to\nreported types and provide non-binding suggested actions, such that (a) it is\nan asymptotic Nash equilibrium for every player to truthfully report their type\nto the mediator, and then follow its suggested action; and (b) that when\nplayers do so, they end up coordinating on a particular asymptotic pure\nstrategy Nash equilibrium of the induced complete information game. In fact,\ntruthful reporting is an ex-post Nash equilibrium of the mediated game, so our\nsolution applies even in settings of incomplete information, and even when\nplayer types are arbitrary or worst-case (i.e. not drawn from a common prior).\nWe achieve this by giving an efficient differentially private algorithm for\ncomputing a Nash equilibrium in such games. The rates of convergence to\nequilibrium in all of our results are inverse polynomial in the number of\nplayers $n$. We also apply our main results to a multi-dimensional market game.\n  Our results can be viewed as giving, for a rich class of games, a more robust\nversion of the Revelation Principle, in that we work with weaker informational\nassumptions (no common prior), yet provide a stronger solution concept (ex-post\nNash versus Bayes Nash equilibrium). In comparison to previous work, our main\nconceptual contribution is showing that weak mediators are a game theoretic\nobject that exist in a wide variety of games -- previously, they were only\nknown to exist in traffic routing games. \n\n"}
{"id": "1407.8269", "contents": "Title: Justified Representation in Approval-Based Committee Voting Abstract: We consider approval-based committee voting, i.e. the setting where each\nvoter approves a subset of candidates, and these votes are then used to select\na fixed-size set of winners (committee). We propose a natural axiom for this\nsetting, which we call justified representation (JR). This axiom requires that\nif a large enough group of voters exhibits agreement by supporting the same\ncandidate, then at least one voter in this group has an approved candidate in\nthe winning committee. We show that for every list of ballots it is possible to\nselect a committee that provides JR. However, it turns out that several\nprominent approval-based voting rules may fail to output such a committee. In\nparticular, while Proportional Approval Voting (PAV) always outputs a committee\nthat provides JR, Reweighted Approval Voting (RAV), a tractable approximation\nto PAV, does not have this property. We then introduce a stronger version of\nthe JR axiom, which we call extended justified representation (EJR), and show\nthat PAV satisfies EJR, while other rules we consider do not; indeed, EJR can\nbe used to characterize PAV within the class of weighted PAV rules. We also\nconsider several other questions related to JR and EJR, including the\nrelationship between JR/EJR and core stability, and the complexity of the\nassociated algorithmic problems. \n\n"}
{"id": "1408.1211", "contents": "Title: A Unifying Hierarchy of Valuations with Complements and Substitutes Abstract: We introduce a new hierarchy over monotone set functions, that we refer to as\n$\\mathcal{MPH}$ (Maximum over Positive Hypergraphs). Levels of the hierarchy\ncorrespond to the degree of complementarity in a given function. The highest\nlevel of the hierarchy, $\\mathcal{MPH}$-$m$ (where $m$ is the total number of\nitems) captures all monotone functions. The lowest level, $\\mathcal{MPH}$-$1$,\ncaptures all monotone submodular functions, and more generally, the class of\nfunctions known as $\\mathcal{XOS}$. Every monotone function that has a positive\nhypergraph representation of rank $k$ (in the sense defined by Abraham,\nBabaioff, Dughmi and Roughgarden [EC 2012]) is in $\\mathcal{MPH}$-$k$. Every\nmonotone function that has supermodular degree $k$ (in the sense defined by\nFeige and Izsak [ITCS 2013]) is in $\\mathcal{MPH}$-$(k+1)$. In both cases, the\nconverse direction does not hold, even in an approximate sense. We present\nadditional results that demonstrate the expressiveness power of\n$\\mathcal{MPH}$-$k$.\n  One can obtain good approximation ratios for some natural optimization\nproblems, provided that functions are required to lie in low levels of the\n$\\mathcal{MPH}$ hierarchy. We present two such applications. One shows that the\nmaximum welfare problem can be approximated within a ratio of $k+1$ if all\nplayers hold valuation functions in $\\mathcal{MPH}$-$k$. The other is an upper\nbound of $2k$ on the price of anarchy of simultaneous first price auctions.\n  Being in $\\mathcal{MPH}$-$k$ can be shown to involve two requirements -- one\nis monotonicity and the other is a certain requirement that we refer to as\n$\\mathcal{PLE}$ (Positive Lower Envelope). Removing the monotonicity\nrequirement, one obtains the $\\mathcal{PLE}$ hierarchy over all non-negative\nset functions (whether monotone or not), which can be fertile ground for\nfurther research. \n\n"}
{"id": "1408.1906", "contents": "Title: Exploring the evolution of a trade-off between vigilance and foraging in\n  group-living organisms Abstract: Despite the fact that grouping behavior has been actively studied for over a\ncentury, the relative importance of the numerous proposed fitness benefits of\ngrouping remain unclear. We use a digital model of evolving prey under\nsimulated predation to directly explore the evolution of gregarious foraging\nbehavior according to one such benefit, the \"many eyes\" hypothesis. According\nto this hypothesis, collective vigilance allows prey in large groups to detect\npredators more efficiently by making alarm signals or behavioral cues to each\nother, thereby allowing individuals within the group to spend more time\nforaging. Here, we find that collective vigilance is sufficient to select for\ngregarious foraging behavior as long there is not a direct cost for grouping\n(e.g., competition for limited food resources), even when controlling for\nconfounding factors such as the dilution effect. Further, we explore the role\nof the genetic relatedness and reproductive strategy of the prey, and find that\nhighly related groups of prey with a semelparous reproductive strategy are the\nmost likely to evolve gregarious foraging behavior mediated by the benefit of\nvigilance. These findings, combined with earlier studies with evolving digital\norganisms, further sharpen our understanding of the factors favoring grouping\nbehavior. \n\n"}
{"id": "1408.3783", "contents": "Title: Long-term causal effects of economic mechanisms on agent incentives Abstract: Economic mechanisms administer the allocation of resources to interested\nagents based on their self-reported types. One objective in mechanism design is\nto design a strategyproof process so that no agent will have an incentive to\nmisreport its type. However, typical analyses of the incentives properties of\nmechanisms operate under strong, usually untestable assumptions. Empirical,\ndata-oriented approaches are, at best, under-developed. Furthermore,\nmechanism/policy evaluation methods usually ignore the dynamic nature of a\nmulti-agent system and are thus inappropriate for estimating long-term effects.\nWe introduce the problem of estimating the causal effects of mechanisms on\nincentives and frame it under the Rubin causal framework \\citep{rubin74,\nrubin78}. This raises unique technical challenges since the outcome of interest\n(agent truthfulness) is confounded with strategic interactions and,\ninterestingly, is typically never observed under any mechanism. We develop a\nmethodology to estimate such causal effects that using a prior that is based on\na strategic equilibrium model. Working on the domain of kidney exchanges, we\nshow how to apply our methodology to estimate causal effects of kidney\nallocation mechanisms on hospitals' incentives. Our results demonstrate that\nthe use of game-theoretic prior captures the dynamic nature of the kidney\nexchange multiagent system and shrinks the estimates towards long-term effects,\nthus improving upon typical methods that completely ignore agents' strategic\nbehavior. \n\n"}
{"id": "1408.4622", "contents": "Title: A new integral loss function for Bayesian optimization Abstract: We consider the problem of maximizing a real-valued continuous function $f$\nusing a Bayesian approach. Since the early work of Jonas Mockus and Antanas\n\\v{Z}ilinskas in the 70's, the problem of optimization is usually formulated by\nconsidering the loss function $\\max f - M_n$ (where $M_n$ denotes the best\nfunction value observed after $n$ evaluations of $f$). This loss function puts\nemphasis on the value of the maximum, at the expense of the location of the\nmaximizer. In the special case of a one-step Bayes-optimal strategy, it leads\nto the classical Expected Improvement (EI) sampling criterion. This is a\nspecial case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk\nassociated to a certain uncertainty measure (here, the expected loss) on the\nquantity of interest is minimized at each step of the algorithm. In this\narticle, assuming that $f$ is defined over a measure space $(\\mathbb{X},\n\\lambda)$, we propose to consider instead the integral loss function\n$\\int_{\\mathbb{X}} (f - M_n)_{+}\\, d\\lambda$, and we show that this leads, in\nthe case of a Gaussian process prior, to a new numerically tractable sampling\ncriterion that we call $\\rm EI^2$ (for Expected Integrated Expected\nImprovement). A numerical experiment illustrates that a SUR strategy based on\nthis new sampling criterion reduces the error on both the value and the\nlocation of the maximizer faster than the EI-based strategy. \n\n"}
{"id": "1408.4901", "contents": "Title: A Study of Proxies for Shapley Allocations of Transport Costs Abstract: We propose and evaluate a number of solutions to the problem of calculating\nthe cost to serve each location in a single-vehicle transport setting. Such\ncost to serve analysis has application both strategically and operationally in\ntransportation. The problem is formally given by the traveling salesperson game\n(TSG), a cooperative total utility game in which agents correspond to locations\nin a traveling salesperson problem (TSP). The cost to serve a location is an\nallocated portion of the cost of an optimal tour. The Shapley value is one of\nthe most important normative division schemes in cooperative games, giving a\nprincipled and fair allocation both for the TSG and more generally. We consider\na number of direct and sampling-based procedures for calculating the Shapley\nvalue, and present the first proof that approximating the Shapley value of the\nTSG within a constant factor is NP-hard. Treating the Shapley value as an ideal\nbaseline allocation, we then develop six proxies for that value which are\nrelatively easy to compute. We perform an experimental evaluation using\nSynthetic Euclidean games as well as games derived from real-world tours\ncalculated for fast-moving consumer goods scenarios. Our experiments show that\nseveral computationally tractable allocation techniques correspond to good\nproxies for the Shapley value. \n\n"}
{"id": "1408.5093", "contents": "Title: Caffe: Convolutional Architecture for Fast Feature Embedding Abstract: Caffe provides multimedia scientists and practitioners with a clean and\nmodifiable framework for state-of-the-art deep learning algorithms and a\ncollection of reference models. The framework is a BSD-licensed C++ library\nwith Python and MATLAB bindings for training and deploying general-purpose\nconvolutional neural networks and other deep models efficiently on commodity\narchitectures. Caffe fits industry and internet-scale media needs by CUDA GPU\ncomputation, processing over 40 million images a day on a single K40 or Titan\nGPU ($\\approx$ 2.5 ms per image). By separating model representation from\nactual implementation, Caffe allows experimentation and seamless switching\namong platforms for ease of development and deployment from prototyping\nmachines to cloud environments. Caffe is maintained and developed by the\nBerkeley Vision and Learning Center (BVLC) with the help of an active community\nof contributors on GitHub. It powers ongoing research projects, large-scale\nindustrial applications, and startup prototypes in vision, speech, and\nmultimedia. \n\n"}
{"id": "1409.1458", "contents": "Title: Communication-Efficient Distributed Dual Coordinate Ascent Abstract: Communication remains the most significant bottleneck in the performance of\ndistributed optimization algorithms for large-scale machine learning. In this\npaper, we propose a communication-efficient framework, CoCoA, that uses local\ncomputation in a primal-dual setting to dramatically reduce the amount of\nnecessary communication. We provide a strong convergence rate analysis for this\nclass of algorithms, as well as experiments on real-world distributed datasets\nwith implementations in Spark. In our experiments, we find that as compared to\nstate-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoA\nconverges to the same .001-accurate solution quality on average 25x as quickly. \n\n"}
{"id": "1409.1556", "contents": "Title: Very Deep Convolutional Networks for Large-Scale Image Recognition Abstract: In this work we investigate the effect of the convolutional network depth on\nits accuracy in the large-scale image recognition setting. Our main\ncontribution is a thorough evaluation of networks of increasing depth using an\narchitecture with very small (3x3) convolution filters, which shows that a\nsignificant improvement on the prior-art configurations can be achieved by\npushing the depth to 16-19 weight layers. These findings were the basis of our\nImageNet Challenge 2014 submission, where our team secured the first and the\nsecond places in the localisation and classification tracks respectively. We\nalso show that our representations generalise well to other datasets, where\nthey achieve state-of-the-art results. We have made our two best-performing\nConvNet models publicly available to facilitate further research on the use of\ndeep visual representations in computer vision. \n\n"}
{"id": "1409.3741", "contents": "Title: Computing Approximate Nash Equilibria in Polymatrix Games Abstract: In an $\\epsilon$-Nash equilibrium, a player can gain at most $\\epsilon$ by\nunilaterally changing his behaviour. For two-player (bimatrix) games with\npayoffs in $[0,1]$, the best-known$\\epsilon$ achievable in polynomial time is\n0.3393. In general, for $n$-player games an $\\epsilon$-Nash equilibrium can be\ncomputed in polynomial time for an $\\epsilon$ that is an increasing function of\n$n$ but does not depend on the number of strategies of the players. For\nthree-player and four-player games the corresponding values of $\\epsilon$ are\n0.6022 and 0.7153, respectively. Polymatrix games are a restriction of general\n$n$-player games where a player's payoff is the sum of payoffs from a number of\nbimatrix games. There exists a very small but constant $\\epsilon$ such that\ncomputing an $\\epsilon$-Nash equilibrium of a polymatrix game is \\PPAD-hard.\nOur main result is that a $(0.5+\\delta)$-Nash equilibrium of an $n$-player\npolymatrix game can be computed in time polynomial in the input size and\n$\\frac{1}{\\delta}$. Inspired by the algorithm of Tsaknakis and Spirakis, our\nalgorithm uses gradient descent on the maximum regret of the players. We also\nshow that this algorithm can be applied to efficiently find a\n$(0.5+\\delta)$-Nash equilibrium in a two-player Bayesian game. \n\n"}
{"id": "1409.6497", "contents": "Title: Optimal sequence for Parrondo games Abstract: An algorithm based on backward induction is devised in order to compute the\noptimal sequence of games to be played in Parrondo games. The algorithm can be\nused to find the optimal sequence for any finite number of turns or in the\nsteady state, showing that ABABB... is the sequence with the highest steady\nstate average gain. The algorithm can also be generalised to find the optimal\nadaptive strategy in a multi-player version of the games, where a finite number\nof players may choose, at every turn, the game the whole ensemble should play. \n\n"}
{"id": "1409.6765", "contents": "Title: A Generalization of the AL method for Fair Allocation of Indivisible\n  Objects Abstract: We consider the assignment problem in which agents express ordinal\npreferences over $m$ objects and the objects are allocated to the agents based\non the preferences. In a recent paper, Brams, Kilgour, and Klamler (2014)\npresented the AL method to compute an envy-free assignment for two agents. The\nAL method crucially depends on the assumption that agents have strict\npreferences over objects. We generalize the AL method to the case where agents\nmay express indifferences and prove the axiomatic properties satisfied by the\nalgorithm. As a result of the generalization, we also get a $O(m)$ speedup on\nprevious algorithms to check whether a complete envy-free assignment exists or\nnot. Finally, we show that unless P=NP, there can be no polynomial-time\nextension of GAL to the case of arbitrary number of agents. \n\n"}
{"id": "1409.6925", "contents": "Title: A Note on Selling Optimally Two Uniformly Distributed Goods Abstract: We provide a new, much simplified and straightforward proof to a result of\nPavlov [2011] regarding the revenue maximizing mechanism for selling two goods\nwith uniformly i.i.d. valuations over intervals $[c,c+1]$, to an additive\nbuyer. This is done by explicitly defining optimal dual solutions to a relaxed\nversion of the problem, where the convexity requirement for the bidder's\nutility has been dropped. Their optimality comes directly from their structure,\nthrough the use of exact complementarity. For $c=0$ and $c\\geq 0.092$ it turns\nout that the corresponding optimal primal solution is a feasible selling\nmechanism, thus the initial relaxation comes without a loss, and revenue\nmaximality follows. However, for $0<c<0.092$ that's not the case, providing the\nfirst clear example where relaxing convexity provably does not come for free,\neven in a two-item regularly i.i.d. setting. \n\n"}
{"id": "1409.7411", "contents": "Title: A Higher-order Framework for Decision Problems and Games Abstract: We introduce a new unified framework for modelling both decision problems and\nfinite games based on quantifiers and selection functions. We show that the\ncanonical utility maximisation is one special case of a quantifier and that our\nmore abstract framework provides several additional degrees of freedom in\nmodelling. In particular, incomplete preferences, non-maximising heuristics,\nand context-dependent motives can be taken into account when describing an\nagent's goal. We introduce a suitable generalisation of Nash equilibrium for\ngames in terms of quantifiers and selection functions. Moreover, we introduce a\nrefinement of Nash that captures context-dependency of goals. Modelling in our\nframework is compositional as the parts of the game are modular and can be\neasily exchanged. We provide an extended example where we illustrate concepts\nand highlight the benefits of our alternative modelling approach. \n\n"}
{"id": "1409.7595", "contents": "Title: Truthful Multi-unit Procurements with Budgets Abstract: We study procurement games where each seller supplies multiple units of his\nitem, with a cost per unit known only to him. The buyer can purchase any number\nof units from each seller, values different combinations of the items\ndifferently, and has a budget for his total payment.\n  For a special class of procurement games, the {\\em bounded knapsack} problem,\nwe show that no universally truthful budget-feasible mechanism can approximate\nthe optimal value of the buyer within $\\ln n$, where $n$ is the total number of\nunits of all items available. We then construct a polynomial-time mechanism\nthat gives a $4(1+\\ln n)$-approximation for procurement games with {\\em concave\nadditive valuations}, which include bounded knapsack as a special case. Our\nmechanism is thus optimal up to a constant factor. Moreover, for the bounded\nknapsack problem, given the well-known FPTAS, our results imply there is a\nprovable gap between the optimization domain and the mechanism design domain.\n  Finally, for procurement games with {\\em sub-additive valuations}, we\nconstruct a universally truthful budget-feasible mechanism that gives an\n$O(\\frac{\\log^2 n}{\\log \\log n})$-approximation in polynomial time with a\ndemand oracle. \n\n"}
{"id": "1410.0413", "contents": "Title: Risk Dynamics in Trade Networks Abstract: We introduce a new framework to model interactions among agents which seek to\ntrade to minimize their risk with respect to some future outcome. We quantify\nthis risk using the concept of risk measures from finance, and introduce a\nclass of trade dynamics which allow agents to trade contracts contingent upon\nthe future outcome. We then show that these trade dynamics exactly correspond\nto a variant of randomized coordinate descent. By extending the analysis of\nthese coordinate descent methods to account for our more organic setting, we\nare able to show convergence rates for very general trade dynamics, showing\nthat the market or network converges to a unique steady state. Applying these\nresults to prediction markets, we expand on recent results by adding\nconvergence rates and general aggregation properties. Finally, we illustrate\nthe generality of our framework by applying it to agent interactions on a\nscale-free network. \n\n"}
{"id": "1410.1141", "contents": "Title: On the Computational Efficiency of Training Neural Networks Abstract: It is well-known that neural networks are computationally hard to train. On\nthe other hand, in practice, modern day neural networks are trained efficiently\nusing SGD and a variety of tricks that include different activation functions\n(e.g. ReLU), over-specification (i.e., train networks which are larger than\nneeded), and regularization. In this paper we revisit the computational\ncomplexity of training neural networks from a modern perspective. We provide\nboth positive and negative results, some of them yield new provably efficient\nand practical algorithms for training certain types of neural networks. \n\n"}
{"id": "1410.3341", "contents": "Title: Generalization Analysis for Game-Theoretic Machine Learning Abstract: For Internet applications like sponsored search, cautions need to be taken\nwhen using machine learning to optimize their mechanisms (e.g., auction) since\nself-interested agents in these applications may change their behaviors (and\nthus the data distribution) in response to the mechanisms. To tackle this\nproblem, a framework called game-theoretic machine learning (GTML) was recently\nproposed, which first learns a Markov behavior model to characterize agents'\nbehaviors, and then learns the optimal mechanism by simulating agents' behavior\nchanges in response to the mechanism. While GTML has demonstrated practical\nsuccess, its generalization analysis is challenging because the behavior data\nare non-i.i.d. and dependent on the mechanism. To address this challenge,\nfirst, we decompose the generalization error for GTML into the behavior\nlearning error and the mechanism learning error; second, for the behavior\nlearning error, we obtain novel non-asymptotic error bounds for both parametric\nand non-parametric behavior learning methods; third, for the mechanism learning\nerror, we derive a uniform convergence bound based on a new concept called\nnested covering number of the mechanism space and the generalization analysis\ntechniques developed for mixing sequences. To the best of our knowledge, this\nis the first work on the generalization analysis of GTML, and we believe it has\ngeneral implications to the theoretical analysis of other complicated machine\nlearning problems. \n\n"}
{"id": "1410.3688", "contents": "Title: A Game Theoretic Model for Network Virus Protection Abstract: The network virus propagation is influenced by various factors, and some of\nthem are neglected in most of the existed models in the literature. In this\npaper, we study the network virus propagation based on the the epidemiological\nviewpoint. We assume that nodes can be equipped with protection against virus\nand the security of a node depends not only on his protection strategy but also\nby those chosen by other nodes in the network. A crucial aspect is whether\nowners of device, e.g., either smartphones, machines or tablets, are willing to\nbe equipped to protect themselves or to take the risk to be contaminated in\norder to avoid the payment for a new antivirus. We model the interaction\nbetween nodes as a non-cooperative games where the node has two strategies:\neither to update the antivirus or not. To this aim, we provide a full\ncharacterization of the equilibria of the game and we investigate the impact of\nthe price of protection on the equilibrium as well as the efficiency of the\nprotection at equilibrium. Further we consider more realistic scenarios in\nwhich the dynamic of sources that disseminate the virus, evolves as function of\nthe popularity of virus. In this work, the interest in the virus by sources\nevolves under the Influence Linear Threshold (HILT) model. \n\n"}
{"id": "1410.5186", "contents": "Title: On the Hardness of Bribery Variants in Voting with CP-Nets Abstract: We continue previous work by Mattei et al. (Mattei, N., Pini, M., Rossi, F.,\nVenable, K.: Bribery in voting with CP-nets. Ann. of Math. and Artif. Intell.\npp. 1--26 (2013)) in which they study the computational complexity of bribery\nschemes when voters have conditional preferences that are modeled by CP-nets.\nFor most of the cases they considered, they could show that the bribery problem\nis solvable in polynomial time. Some cases remained open---we solve two of them\nand extend the previous results to the case that voters are weighted. Moreover,\nwe consider negative (weighted) bribery in CP-nets, when the briber is not\nallowed to pay voters to vote for his preferred candidate. \n\n"}
{"id": "1410.7172", "contents": "Title: Heteroscedastic Treed Bayesian Optimisation Abstract: Optimising black-box functions is important in many disciplines, such as\ntuning machine learning models, robotics, finance and mining exploration.\nBayesian optimisation is a state-of-the-art technique for the global\noptimisation of black-box functions which are expensive to evaluate. At the\ncore of this approach is a Gaussian process prior that captures our belief\nabout the distribution over functions. However, in many cases a single Gaussian\nprocess is not flexible enough to capture non-stationarity in the objective\nfunction. Consequently, heteroscedasticity negatively affects performance of\ntraditional Bayesian methods. In this paper, we propose a novel prior model\nwith hierarchical parameter learning that tackles the problem of\nnon-stationarity in Bayesian optimisation. Our results demonstrate substantial\nimprovements in a wide range of applications, including automatic machine\nlearning and mining exploration. \n\n"}
{"id": "1410.7472", "contents": "Title: A note on two notions of compliance Abstract: We establish a relation between two models of contracts: binary session\ntypes, and a model based on event structures and game-theoretic notions. In\nparticular, we show that compliance in session types corresponds to the\nexistence of certain winning strategies in game-based contracts. \n\n"}
{"id": "1410.7596", "contents": "Title: Fast Algorithms for Online Stochastic Convex Programming Abstract: We introduce the online stochastic Convex Programming (CP) problem, a very\ngeneral version of stochastic online problems which allows arbitrary concave\nobjectives and convex feasibility constraints. Many well-studied problems like\nonline stochastic packing and covering, online stochastic matching with concave\nreturns, etc. form a special case of online stochastic CP. We present fast\nalgorithms for these problems, which achieve near-optimal regret guarantees for\nboth the i.i.d. and the random permutation models of stochastic inputs. When\napplied to the special case online packing, our ideas yield a simpler and\nfaster primal-dual algorithm for this well studied problem, which achieves the\noptimal competitive ratio. Our techniques make explicit the connection of\nprimal-dual paradigm and online learning to online stochastic CP. \n\n"}
{"id": "1411.0835", "contents": "Title: Variations on the Stochastic Shortest Path Problem Abstract: In this invited contribution, we revisit the stochastic shortest path\nproblem, and show how recent results allow one to improve over the classical\nsolutions: we present algorithms to synthesize strategies with multiple\nguarantees on the distribution of the length of paths reaching a given target,\nrather than simply minimizing its expected value. The concepts and algorithms\nthat we propose here are applications of more general results that have been\nobtained recently for Markov decision processes and that are described in a\nseries of recent papers. \n\n"}
{"id": "1411.0998", "contents": "Title: Jointly Private Convex Programming Abstract: In this paper we present an extremely general method for approximately\nsolving a large family of convex programs where the solution can be divided\nbetween different agents, subject to joint differential privacy. This class\nincludes multi-commodity flow problems, general allocation problems, and\nmulti-dimensional knapsack problems, among other examples. The accuracy of our\nalgorithm depends on the \\emph{number} of constraints that bind between\nindividuals, but crucially, is \\emph{nearly independent} of the number of\nprimal variables and hence the number of agents who make up the problem. As the\nnumber of agents in a problem grows, the error we introduce often becomes\nnegligible.\n  We also consider the setting where agents are strategic and have preferences\nover their part of the solution. For any convex program in this class that\nmaximizes \\emph{social welfare}, there is a generic reduction that makes the\ncorresponding optimization \\emph{approximately dominant strategy truthful} by\ncharging agents prices for resources as a function of the approximately optimal\ndual variables, which are themselves computed under differential privacy. Our\nresults substantially expand the class of problems that are known to be\nsolvable under both privacy and incentive constraints. \n\n"}
{"id": "1411.1134", "contents": "Title: Global Convergence of Stochastic Gradient Descent for Some Non-convex\n  Matrix Problems Abstract: Stochastic gradient descent (SGD) on a low-rank factorization is commonly\nemployed to speed up matrix problems including matrix completion, subspace\ntracking, and SDP relaxation. In this paper, we exhibit a step size scheme for\nSGD on a low-rank least-squares problem, and we prove that, under broad\nsampling conditions, our method converges globally from a random starting point\nwithin $O(\\epsilon^{-1} n \\log n)$ steps with constant probability for\nconstant-rank problems. Our modification of SGD relates it to stochastic power\niteration. We also show experiments to illustrate the runtime and convergence\nof the algorithm. \n\n"}
{"id": "1411.2374", "contents": "Title: Similarity Learning for High-Dimensional Sparse Data Abstract: A good measure of similarity between data points is crucial to many tasks in\nmachine learning. Similarity and metric learning methods learn such measures\nautomatically from data, but they do not scale well respect to the\ndimensionality of the data. In this paper, we propose a method that can learn\nefficiently similarity measure from high-dimensional sparse data. The core idea\nis to parameterize the similarity measure as a convex combination of rank-one\nmatrices with specific sparsity structures. The parameters are then optimized\nwith an approximate Frank-Wolfe procedure to maximally satisfy relative\nsimilarity constraints on the training data. Our algorithm greedily\nincorporates one pair of features at a time into the similarity measure,\nproviding an efficient way to control the number of active features and thus\nreduce overfitting. It enjoys very appealing convergence guarantees and its\ntime and memory complexity depends on the sparsity of the data instead of the\ndimension of the feature space. Our experiments on real-world high-dimensional\ndatasets demonstrate its potential for classification, dimensionality reduction\nand data exploration. \n\n"}
{"id": "1411.3224", "contents": "Title: On TD(0) with function approximation: Concentration bounds and a\n  centered variant with exponential convergence Abstract: We provide non-asymptotic bounds for the well-known temporal difference\nlearning algorithm TD(0) with linear function approximators. These include\nhigh-probability bounds as well as bounds in expectation. Our analysis suggests\nthat a step-size inversely proportional to the number of iterations cannot\nguarantee optimal rate of convergence unless we assume (partial) knowledge of\nthe stationary distribution for the Markov chain underlying the policy\nconsidered. We also provide bounds for the iterate averaged TD(0) variant,\nwhich gets rid of the step-size dependency while exhibiting the optimal rate of\nconvergence. Furthermore, we propose a variant of TD(0) with linear\napproximators that incorporates a centering sequence, and establish that it\nexhibits an exponential rate of convergence in expectation. We demonstrate the\nusefulness of our bounds on two synthetic experimental settings. \n\n"}
{"id": "1411.4000", "contents": "Title: How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets Abstract: The computational complexity of kernel methods has often been a major barrier\nfor applying them to large-scale learning problems. We argue that this barrier\ncan be effectively overcome. In particular, we develop methods to scale up\nkernel models to successfully tackle large-scale learning problems that are so\nfar only approachable by deep learning architectures. Based on the seminal work\nby Rahimi and Recht on approximating kernel functions with features derived\nfrom random projections, we advance the state-of-the-art by proposing methods\nthat can efficiently train models with hundreds of millions of parameters, and\nlearn optimal representations from multiple kernels. We conduct extensive\nempirical studies on problems from image recognition and automatic speech\nrecognition, and show that the performance of our kernel models matches that of\nwell-engineered deep neural nets (DNNs). To the best of our knowledge, this is\nthe first time that a direct comparison between these two methods on\nlarge-scale problems is reported. Our kernel methods have several appealing\nproperties: training with convex optimization, cost for training a single model\ncomparable to DNNs, and significantly reduced total cost due to fewer\nhyperparameters to tune for model selection. Our contrastive study between\nthese two very different but equally competitive models sheds light on\nfundamental questions such as how to learn good representations. \n\n"}
{"id": "1411.4555", "contents": "Title: Show and Tell: A Neural Image Caption Generator Abstract: Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. For instance, while the\ncurrent state-of-the-art BLEU-1 score (the higher the better) on the Pascal\ndataset is 25, our approach yields 59, to be compared to human performance\naround 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66,\nand on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we\nachieve a BLEU-4 of 27.7, which is the current state-of-the-art. \n\n"}
{"id": "1412.0156", "contents": "Title: Constant Step Size Least-Mean-Square: Bias-Variance Trade-offs and\n  Optimal Sampling Distributions Abstract: We consider the least-squares regression problem and provide a detailed\nasymptotic analysis of the performance of averaged constant-step-size\nstochastic gradient descent (a.k.a. least-mean-squares). In the strongly-convex\ncase, we provide an asymptotic expansion up to explicit exponentially decaying\nterms. Our analysis leads to new insights into stochastic approximation\nalgorithms: (a) it gives a tighter bound on the allowed step-size; (b) the\ngeneralization error may be divided into a variance term which is decaying as\nO(1/n), independently of the step-size $\\gamma$, and a bias term that decays as\nO(1/$\\gamma$ 2 n 2); (c) when allowing non-uniform sampling, the choice of a\ngood sampling density depends on whether the variance or bias terms dominate.\nIn particular, when the variance term dominates, optimal sampling densities do\nnot lead to much gain, while when the bias term dominates, we can choose larger\nstep-sizes that leads to significant improvements. \n\n"}
{"id": "1412.3187", "contents": "Title: Revenue Maximization for Selling Multiple Correlated Items Abstract: We study the problem of selling $n$ items to a single buyer with an additive\nvaluation function. We consider the valuation of the items to be correlated,\ni.e., desirabilities of the buyer for the items are not drawn independently.\nIdeally, the goal is to design a mechanism to maximize the revenue. However, it\nhas been shown that a revenue optimal mechanism might be very complicated and\nas a result inapplicable to real-world auctions. Therefore, our focus is on\ndesigning a simple mechanism that achieves a constant fraction of the optimal\nrevenue. Babaioff et al. propose a simple mechanism that achieves a constant\nfraction of the optimal revenue for independent setting with a single additive\nbuyer. However, they leave the following problem as an open question: \"Is there\na simple, approximately optimal mechanism for a single additive buyer whose\nvalue for $n$ items is sampled from a common base-value distribution?\"\n  Babaioff et al. show a constant approximation factor of the optimal revenue\ncan be achieved by either selling the items separately or as a whole bundle in\nthe independent setting. We show a similar result for the correlated setting\nwhen the desirabilities of the buyer are drawn from a common base-value\ndistribution. It is worth mentioning that the core decomposition lemma which is\nmainly the heart of the proofs for efficiency of the mechanisms does not hold\nfor correlated settings. Therefore we propose a modified version of this lemma\nwhich is applicable to the correlated settings as well. Although we apply this\ntechnique to show the proposed mechanism can guarantee a constant fraction of\nthe optimal revenue in a very weak correlation, this method alone can not\ndirectly show the efficiency of the mechanism in stronger correlations. \n\n"}
{"id": "1412.3334", "contents": "Title: Computational Complexity of Competitive Diffusion on (Un)weighted Graphs Abstract: Consider an undirected graph modeling a social network, where the vertices\nrepresent users, and the edges do connections among them. In the competitive\ndiffusion game, each of a number of players chooses a vertex as a seed to\npropagate his/her opinion, and then it spreads along the edges in the graphs.\nThe objective of every player is to maximize the number of vertices the opinion\ninfects. In this paper, we investigate a computational problem of asking\nwhether a pure Nash equilibrium exists in the competitive diffusion game on\nunweighed and weighted graphs, and present several negative and positive\nresults. We first prove that the problem is W[1]-hard when parameterized by the\nnumber of players even for unweighted graphs. We also show that the problem is\nNP-hard even for series-parallel graphs with positive integer weights, and is\nNP-hard even for forests with arbitrary integer weights. Furthermore, we show\nthat the problem for forest of paths with arbitrary weights is solvable in\npseudo-polynomial time; and it is solvable in quadratic time if a given graph\nis unweighted. We also prove that the problem for chain, cochain, and threshold\ngraphs with arbitrary integer weights is solvable in polynomial time. \n\n"}
{"id": "1412.3701", "contents": "Title: How Much Lookahead is Needed to Win Infinite Games? Abstract: Delay games are two-player games of infinite duration in which one player may\ndelay her moves to obtain a lookahead on her opponent's moves. For\n$\\omega$-regular winning conditions it is known that such games can be solved\nin doubly-exponential time and that doubly-exponential lookahead is sufficient.\n  We improve upon both results by giving an exponential time algorithm and an\nexponential upper bound on the necessary lookahead. This is complemented by\nshowing EXPTIME-hardness of the solution problem and tight exponential lower\nbounds on the lookahead. Both lower bounds already hold for safety conditions.\nFurthermore, solving delay games with reachability conditions is shown to be\nPSPACE-complete.\n  This is a corrected version of the paper https://arxiv.org/abs/1412.3701v4\npublished originally on August 26, 2016. \n\n"}
{"id": "1412.3773", "contents": "Title: Distinguishing cause from effect using observational data: methods and\n  benchmarks Abstract: The discovery of causal relationships from purely observational data is a\nfundamental problem in science. The most elementary form of such a causal\ndiscovery problem is to decide whether X causes Y or, alternatively, Y causes\nX, given joint observations of two variables X, Y. An example is to decide\nwhether altitude causes temperature, or vice versa, given only joint\nmeasurements of both variables. Even under the simplifying assumptions of no\nconfounding, no feedback loops, and no selection bias, such bivariate causal\ndiscovery problems are challenging. Nevertheless, several approaches for\naddressing those problems have been proposed in recent years. We review two\nfamilies of such methods: Additive Noise Methods (ANM) and Information\nGeometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs\nthat consists of data for 100 different cause-effect pairs selected from 37\ndatasets from various domains (e.g., meteorology, biology, medicine,\nengineering, economy, etc.) and motivate our decisions regarding the \"ground\ntruth\" causal directions of all pairs. We evaluate the performance of several\nbivariate causal discovery methods on these real-world benchmark data and in\naddition on artificially simulated data. Our empirical results on real-world\ndata indicate that certain methods are indeed able to distinguish cause from\neffect using only purely observational data, although more benchmark data would\nbe needed to obtain statistically significant conclusions. One of the best\nperforming methods overall is the additive-noise method originally proposed by\nHoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of\n0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of\nthis work we prove the consistency of that method. \n\n"}
{"id": "1412.6095", "contents": "Title: Theoretical and Numerical Analysis of Approximate Dynamic Programming\n  with Approximation Errors Abstract: This study is aimed at answering the famous question of how the approximation\nerrors at each iteration of Approximate Dynamic Programming (ADP) affect the\nquality of the final results considering the fact that errors at each iteration\naffect the next iteration. To this goal, convergence of Value Iteration scheme\nof ADP for deterministic nonlinear optimal control problems with undiscounted\ncost functions is investigated while considering the errors existing in\napproximating respective functions. The boundedness of the results around the\noptimal solution is obtained based on quantities which are known in a general\noptimal control problem and assumptions which are verifiable. Moreover, since\nthe presence of the approximation errors leads to the deviation of the results\nfrom optimality, sufficient conditions for stability of the system operated by\nthe result obtained after a finite number of value iterations, along with an\nestimation of its region of attraction, are derived in terms of a calculable\nupper bound of the control approximation error. Finally, the process of\nimplementation of the method on an orbital maneuver problem is investigated\nthrough which the assumptions made in the theoretical developments are verified\nand the sufficient conditions are applied for guaranteeing stability and near\noptimality. \n\n"}
{"id": "1412.6265", "contents": "Title: Inapproximability of Truthful Mechanisms via Generalizations of the VC\n  Dimension Abstract: Algorithmic mechanism design (AMD) studies the delicate interplay between\ncomputational efficiency, truthfulness, and optimality. We focus on AMD's\nparadigmatic problem: combinatorial auctions. We present a new generalization\nof the VC dimension to multivalued collections of functions, which encompasses\nthe classical VC dimension, Natarajan dimension, and Steele dimension. We\npresent a corresponding generalization of the Sauer-Shelah Lemma and harness\nthis VC machinery to establish inapproximability results for deterministic\ntruthful mechanisms. Our results essentially unify all inapproximability\nresults for deterministic truthful mechanisms for combinatorial auctions to\ndate and establish new separation gaps between truthful and non-truthful\nalgorithms. \n\n"}
{"id": "1412.6980", "contents": "Title: Adam: A Method for Stochastic Optimization Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm. \n\n"}
{"id": "1412.8501", "contents": "Title: Formation Games of Reliable Networks Abstract: We establish a network formation game for the Internet's Autonomous System\n(AS) interconnection topology. The game includes different types of players,\naccounting for the heterogeneity of ASs in the Internet. We incorporate\nreliability considerations in the player's utility function, and analyze static\nproperties of the game as well as its dynamic evolution. We provide dynamic\nanalysis of its topological quantities, and explain the prevalence of some\n\"network motifs\" in the Internet graph. We assess our predictions with\nreal-world data. \n\n"}
{"id": "1412.8736", "contents": "Title: Sharing Information Without Regret in Managed Stochastic Games Abstract: This paper considers information sharing in a multi-player repeated game.\nEvery round, each player observes a subset of components of a random vector and\nthen takes a control action. The utility earned by each player depends on the\nfull random vector and on the actions of others. An example is a game where\ndifferent rewards are placed over multiple locations, each player only knows\nthe rewards in a subset of the locations, and players compete to collect the\nrewards. Sharing information can help others, but can also increase competition\nfor desirable locations. Standard Nash equilibrium and correlated equilibrium\nconcepts are inadequate in this scenario. Instead, this paper develops an\nalgorithm where, every round, all players pass their information and intended\nactions to a game manager. The manager provides suggested actions for each\nplayer that, if taken, maximize a concave function of average utilities subject\nto the constraint that each player gets an average utility no worse than it\nwould get without sharing. The algorithm acts online using information given at\neach round and does not require a specific model of random events or player\nactions. Thus, the analytical results of this paper apply in non-ergodic\nsituations with any sequence of actions taken by human players. \n\n"}
{"id": "1501.00882", "contents": "Title: Observing Each Other's Observations in the Electronic Mail Game Abstract: We study a Bayesian coordination game where agents receive private\ninformation on the game's payoff structure. In addition, agents receive private\nsignals on each other's private information. We show that once agents possess\nthese different types of information, there exists a coordination game in the\nevaluation of this information. And even though the precisions of both signal\ntypes is exogenous, the precision with which agents predict each other's\nactions at equilibrium turns out to be endogenous. As a consequence, we find\nthat there exist multiple equilibria if the private signals' precision is high.\nThese equilibria differ with regard to the way that agents weight their private\ninformation to reason about each other's actions. \n\n"}
{"id": "1501.06225", "contents": "Title: Online Optimization : Competing with Dynamic Comparators Abstract: Recent literature on online learning has focused on developing adaptive\nalgorithms that take advantage of a regularity of the sequence of observations,\nyet retain worst-case performance guarantees. A complementary direction is to\ndevelop prediction methods that perform well against complex benchmarks. In\nthis paper, we address these two directions together. We present a fully\nadaptive method that competes with dynamic benchmarks in which regret guarantee\nscales with regularity of the sequence of cost functions and comparators.\nNotably, the regret bound adapts to the smaller complexity measure in the\nproblem environment. Finally, we apply our results to drifting zero-sum,\ntwo-player games where both players achieve no regret guarantees against best\nsequences of actions in hindsight. \n\n"}
{"id": "1502.01157", "contents": "Title: Toward Fully Coordinated Multi-level Multi-carrier Energy Efficient\n  Networks Abstract: Enabling coordination between products from different vendors is a key\ncharacteristic of the design philosophy behind future wireless communication\nnetworks. As an example, different devices may have different implementations,\nleading to different user experiences. A similar story emerges when devices\nrunning different physical and link layer protocols share frequencies in the\nsame spectrum in order to maximize the system-wide spectral efficiency. In such\nsituations, coordinating multiple interfering devices presents a significant\nchallenge not only from an interworking perspective (as a result of reduced\ninfrastructure), but also from an implementation point of view. The following\nquestion may then naturally arise: How to accommodate integrating such\nheterogeneous wireless devices seamlessly? One approach is to coordinate the\nspectrum in a centralized manner. However, the desired autonomous feature of\nfuture wireless systems makes the use of a central authority for spectrum\nmanagement less appealing. Alternately, intelligent spectrum coordination have\nspurred great interest and excitement in the recent years. This paper presents\na multi-level (hierarchical) power control game where users jointly choose\ntheir channel control and power control selfishly in order to maximize their\nindividual energy efficiency. By hierarchical, we mean that some users'\ndecision priority is higher/lower than the others. We propose two simple and\nnearly-optimal algorithms that ensure complete spectrum coordination among\nusers. Interestingly, it turns out that the complexity of the two proposed\nalgorithms is, in the worst case, quadratic in the number of users, whereas the\ncomplexity of the optimal solution (obtained through exhaustive search) is N!. \n\n"}
{"id": "1502.04052", "contents": "Title: Computer-aided verification in mechanism design Abstract: In mechanism design, the gold standard solution concepts are dominant\nstrategy incentive compatibility and Bayesian incentive compatibility. These\nsolution concepts relieve the (possibly unsophisticated) bidders from the need\nto engage in complicated strategizing. While incentive properties are simple to\nstate, their proofs are specific to the mechanism and can be quite complex.\nThis raises two concerns. From a practical perspective, checking a complex\nproof can be a tedious process, often requiring experts knowledgeable in\nmechanism design. Furthermore, from a modeling perspective, if unsophisticated\nagents are unconvinced of incentive properties, they may strategize in\nunpredictable ways.\n  To address both concerns, we explore techniques from computer-aided\nverification to construct formal proofs of incentive properties. Because formal\nproofs can be automatically checked, agents do not need to manually check the\nproperties, or even understand the proof. To demonstrate, we present the\nverification of a sophisticated mechanism: the generic reduction from Bayesian\nincentive compatible mechanism design to algorithm design given by Hartline,\nKleinberg, and Malekian. This mechanism presents new challenges for formal\nverification, including essential use of randomness from both the execution of\nthe mechanism and from the prior type distributions. As an immediate\nconsequence, our work also formalizes Bayesian incentive compatibility for the\nentire family of mechanisms derived via this reduction. Finally, as an\nintermediate step in our formalization, we provide the first formal\nverification of incentive compatibility for the celebrated\nVickrey-Clarke-Groves mechanism. \n\n"}
{"id": "1502.04888", "contents": "Title: Equilibria Under the Probabilistic Serial Rule Abstract: The probabilistic serial (PS) rule is a prominent randomized rule for\nassigning indivisible goods to agents. Although it is well known for its good\nfairness and welfare properties, it is not strategyproof. In view of this, we\naddress several fundamental questions regarding equilibria under PS. Firstly,\nwe show that Nash deviations under the PS rule can cycle. Despite the\npossibilities of cycles, we prove that a pure Nash equilibrium is guaranteed to\nexist under the PS rule. We then show that verifying whether a given profile is\na pure Nash equilibrium is coNP-complete, and computing a pure Nash equilibrium\nis NP-hard. For two agents, we present a linear-time algorithm to compute a\npure Nash equilibrium which yields the same assignment as the truthful profile.\nFinally, we conduct experiments to evaluate the quality of the equilibria that\nexist under the PS rule, finding that the vast majority of pure Nash equilibria\nyield social welfare that is at least that of the truthful profile. \n\n"}
{"id": "1502.05056", "contents": "Title: On Sex, Evolution, and the Multiplicative Weights Update Algorithm Abstract: We consider a recent innovative theory by Chastain et al. on the role of sex\nin evolution [PNAS'14]. In short, the theory suggests that the evolutionary\nprocess of gene recombination implements the celebrated multiplicative weights\nupdates algorithm (MWUA). They prove that the population dynamics induced by\nsexual reproduction can be precisely modeled by genes that use MWUA as their\nlearning strategy in a particular coordination game. The result holds in the\nenvironments of \\emph{weak selection}, under the assumption that the population\nfrequencies remain a product distribution.\n  We revisit the theory, eliminating both the requirement of weak selection and\nany assumption on the distribution of the population. Removing the assumption\nof product distributions is crucial, since as we show, this assumption is\ninconsistent with the population dynamics. We show that the marginal allele\ndistributions induced by the population dynamics precisely match the marginals\ninduced by a multiplicative weights update algorithm in this general setting,\nthereby affirming and substantially generalizing these earlier results.\n  We further revise the implications for convergence and utility or fitness\nguarantees in coordination games. In contrast to the claim of Chastain et\nal.[PNAS'14], we conclude that the sexual evolutionary dynamics does not entail\nany property of the population distribution, beyond those already implied by\nconvergence. \n\n"}
{"id": "1502.05548", "contents": "Title: Facility location with double-peaked preference Abstract: We study the problem of locating a single facility on a real line based on\nthe reports of self-interested agents, when agents have double-peaked\npreferences, with the peaks being on opposite sides of their locations. We\nobserve that double-peaked preferences capture real-life scenarios and thus\ncomplement the well-studied notion of single-peaked preferences. We mainly\nfocus on the case where peaks are equidistant from the agents' locations and\ndiscuss how our results extend to more general settings. We show that most of\nthe results for single-peaked preferences do not directly apply to this\nsetting; this makes the problem essentially more challenging. As our main\ncontribution, we present a simple truthful-in-expectation mechanism that\nachieves an approximation ratio of 1+b/c for both the social and the maximum\ncost, where b is the distance of the agent from the peak and c is the minimum\ncost of an agent. For the latter case, we provide a 3/2 lower bound on the\napproximation ratio of any truthful-in-expectation mechanism. We also study\ndeterministic mechanisms under some natural conditions, proving lower bounds\nand approximation guarantees. We prove that among a large class of reasonable\nmechanisms, there is no deterministic mechanism that outperforms our\ntruthful-in-expectation mechanism. \n\n"}
{"id": "1502.07687", "contents": "Title: Incentive Mechanisms for Participatory Sensing: Survey and Research\n  Challenges Abstract: Participatory sensing is a powerful paradigm which takes advantage of\nsmartphones to collect and analyze data beyond the scale of what was previously\npossible. Given that participatory sensing systems rely completely on the\nusers' willingness to submit up-to-date and accurate information, it is\nparamount to effectively incentivize users' active and reliable participation.\nIn this paper, we survey existing literature on incentive mechanisms for\nparticipatory sensing systems. In particular, we present a taxonomy of existing\nincentive mechanisms for participatory sensing systems, which are subsequently\ndiscussed in depth by comparing and contrasting different approaches. Finally,\nwe discuss an agenda of open research challenges in incentivizing users in\nparticipatory sensing. \n\n"}
{"id": "1502.07823", "contents": "Title: Coalitional Permutation Manipulations in the Gale-Shapley Algorithm Abstract: In this paper, we consider permutation manipulations by any subset of women\nin the men-proposing version of the Gale-Shapley algorithm. This paper is\nmotivated by the college admissions process in China. Our results also answer\nan open problem on what can be achieved by permutation manipulations. We\npresent an efficient algorithm to find a strategy profile such that the induced\nmatching is stable and Pareto-optimal (in the set of all achievable stable\nmatchings) while the strategy profile itself is inconspicuous. Surprisingly, we\nshow that such a strategy profile actually forms a Nash equilibrium of the\nmanipulation game. In the end, we show that it is NP-complete to find a\nmanipulation that is strictly better for all members of the coalition. This\nresult demonstrates a sharp contrast between weakly better off outcomes and\nstrictly better-off outcomes. \n\n"}
{"id": "1503.01488", "contents": "Title: Random Serial Dictatorship versus Probabilistic Serial Rule: A Tale of\n  Two Random Mechanisms Abstract: For assignment problems where agents, specifying ordinal preferences, are\nallocated indivisible objects, two widely studied randomized mechanisms are the\nRandom Serial Dictatorship (RSD) and Probabilistic Serial Rule (PS). These two\nmechanisms both have desirable economic and computational properties, but the\noutcomes they induce can be incomparable in many instances, thus creating\nchallenges in deciding which mechanism to adopt in practice. In this paper we\nfirst look at the space of lexicographic preferences and show that, as opposed\nto the general preference domain, RSD satisfies envyfreeness. Moreover, we show\nthat although under lexicographic preferences PS is strategyproof when the\nnumber of objects is less than or equal agents, it is strictly manipulable when\nthere are more objects than agents. In the space of general preferences, we\nprovide empirical results on the (in)comparability of RSD and PS, analyze\neconomic properties, and provide further insights on the applicability of each\nmechanism in different application domains. \n\n"}
{"id": "1503.02101", "contents": "Title: Escaping From Saddle Points --- Online Stochastic Gradient for Tensor\n  Decomposition Abstract: We analyze stochastic gradient descent for optimizing non-convex functions.\nIn many cases for non-convex functions the goal is to find a reasonable local\nminimum, and the main concern is that gradient updates are trapped in saddle\npoints. In this paper we identify strict saddle property for non-convex problem\nthat allows for efficient optimization. Using this property we show that\nstochastic gradient descent converges to a local minimum in a polynomial number\nof iterations. To the best of our knowledge this is the first work that gives\nglobal convergence guarantees for stochastic gradient descent on non-convex\nfunctions with exponentially many local minima and saddle points. Our analysis\ncan be applied to orthogonal tensor decomposition, which is widely used in\nlearning a rich class of latent variable models. We propose a new optimization\nformulation for the tensor decomposition problem that has strict saddle\nproperty. As a result we get the first online algorithm for orthogonal tensor\ndecomposition with global convergence guarantee. \n\n"}
{"id": "1503.02766", "contents": "Title: Generating Single Peaked Votes Abstract: We discuss how to generate singled peaked votes uniformly from the Impartial\nCulture model. \n\n"}
{"id": "1503.02951", "contents": "Title: Mean Field Games in Nudge Systems for Societal Networks Abstract: We consider the general problem of resource sharing in societal networks,\nconsisting of interconnected communication, transportation, energy and other\nnetworks important to the functioning of society. Participants in such network\nneed to take decisions daily, both on the quantity of resources to use as well\nas the periods of usage. With this in mind, we discuss the problem of\nincentivizing users to behave in such a way that society as a whole benefits.\nIn order to perceive societal level impact, such incentives may take the form\nof rewarding users with lottery tickets based on good behavior, and\nperiodically conducting a lottery to translate these tickets into real rewards.\nWe will pose the user decision problem as a mean field game (MFG), and the\nincentives question as one of trying to select a good mean field equilibrium\n(MFE). In such a framework, each agent (a participant in the societal network)\ntakes a decision based on an assumed distribution of actions of his/her\ncompetitors, and the incentives provided by the social planner. The system is\nsaid to be at MFE if the agent's action is a sample drawn from the assumed\ndistribution. We will show the existence of such an MFE under different\nsettings, and also illustrate how to choose an attractive equilibrium using as\nan example demand-response in energy networks. \n\n"}
{"id": "1503.03517", "contents": "Title: Switching to Learn Abstract: A network of agents attempt to learn some unknown state of the world drawn by\nnature from a finite set. Agents observe private signals conditioned on the\ntrue state, and form beliefs about the unknown state accordingly. Each agent\nmay face an identification problem in the sense that she cannot distinguish the\ntruth in isolation. However, by communicating with each other, agents are able\nto benefit from side observations to learn the truth collectively. Unlike many\ndistributed algorithms which rely on all-time communication protocols, we\npropose an efficient method by switching between Bayesian and non-Bayesian\nregimes. In this model, agents exchange information only when their private\nsignals are not informative enough; thence, by switching between the two\nregimes, agents efficiently learn the truth using only a few rounds of\ncommunications. The proposed algorithm preserves learnability while incurring a\nlower communication cost. We also verify our theoretical findings by simulation\nexamples. \n\n"}
{"id": "1503.04755", "contents": "Title: The Price of Anarchy in Large Games Abstract: Game-theoretic models relevant for computer science applications usually\nfeature a large number of players. The goal of this paper is to develop an\nanalytical framework for bounding the price of anarchy in such models. We\ndemonstrate the wide applicability of our framework through instantiations for\nseveral well-studied models, including simultaneous single-item auctions,\ngreedy combinatorial auctions, and routing games. In all cases, we identify\nconditions under which the POA of large games is much better than that of\nworst-case instances. Our results also give new senses in which simple auctions\ncan perform almost as well as optimal ones in realistic settings. \n\n"}
{"id": "1503.05608", "contents": "Title: Greedy Algorithms make Efficient Mechanisms Abstract: We study mechanisms that use greedy allocation rules and pay-your-bid pricing\nto allocate resources subject to a matroid constraint. We show that all such\nmechanisms obtain a constant fraction of the optimal welfare at any equilibrium\nof bidder behavior, via a smoothness argument. This unifies numerous recent\nresults on the price of anarchy of simple auctions. Our results extend to\npolymatroid and matching constraints, and we discuss extensions to more general\nmatroid intersections. \n\n"}
{"id": "1503.05971", "contents": "Title: Group size effect on cooperation in one-shot social dilemmas II.\n  Curvilinear effect Abstract: In a world in which many pressing global issues require large scale\ncooperation, understanding the group size effect on cooperative behavior is a\ntopic of central importance. Yet, the nature of this effect remains largely\nunknown, with lab experiments insisting that it is either positive or negative\nor null, and field experiments suggesting that it is instead curvilinear. Here\nwe shed light on this apparent contradiction by considering a novel class of\npublic goods games inspired to the realistic scenario in which the natural\noutput limits of the public good imply that the benefit of cooperation\nincreases fast for early contributions and then decelerates. We report on a\nlarge lab experiment providing evidence that, in this case, group size has a\ncurvilinear effect on cooperation, according to which intermediate-size groups\ncooperate more than smaller groups and more than larger groups. In doing so,\nour findings help fill the gap between lab experiments and field experiments\nand suggest concrete ways to promote large scale cooperation among people. \n\n"}
{"id": "1503.05988", "contents": "Title: Algorithmic Bayesian Persuasion Abstract: Persuasion, defined as the act of exploiting an informational advantage in\norder to effect the decisions of others, is ubiquitous. Indeed, persuasive\ncommunication has been estimated to account for almost a third of all economic\nactivity in the US. This paper examines persuasion through a computational\nlens, focusing on what is perhaps the most basic and fundamental model in this\nspace: the celebrated Bayesian persuasion model of Kamenica and Gentzkow. Here\nthere are two players, a sender and a receiver. The receiver must take one of a\nnumber of actions with a-priori unknown payoff, and the sender has access to\nadditional information regarding the payoffs. The sender can commit to\nrevealing a noisy signal regarding the realization of the payoffs of various\nactions, and would like to do so as to maximize her own payoff assuming a\nperfectly rational receiver.\n  We examine the sender's optimization task in three of the most natural input\nmodels for this problem, and essentially pin down its computational complexity\nin each. When the payoff distributions of the different actions are i.i.d. and\ngiven explicitly, we exhibit a polynomial-time (exact) algorithm, and a\n\"simple\" $(1-1/e)$-approximation algorithm. Our optimal scheme for the i.i.d.\nsetting involves an analogy to auction theory, and makes use of Border's\ncharacterization of the space of reduced-forms for single-item auctions. When\naction payoffs are independent but non-identical with marginal distributions\ngiven explicitly, we show that it is #P-hard to compute the optimal expected\nsender utility. Finally, we consider a general (possibly correlated) joint\ndistribution of action payoffs presented by a black box sampling oracle, and\nexhibit a fully polynomial-time approximation scheme (FPTAS) with a bi-criteria\nguarantee. We show that this result is the best possible in the black-box model\nfor information-theoretic reasons. \n\n"}
{"id": "1503.06826", "contents": "Title: Pure Nash Equilibria in Concurrent Deterministic Games Abstract: We study pure-strategy Nash equilibria in multi-player concurrent\ndeterministic games, for a variety of preference relations. We provide a novel\nconstruction, called the suspect game, which transforms a multi-player\nconcurrent game into a two-player turn-based game which turns Nash equilibria\ninto winning strategies (for some objective that depends on the preference\nrelations of the players in the original game). We use that transformation to\ndesign algorithms for computing Nash equilibria in finite games, which in most\ncases have optimal worst-case complexity, for large classes of preference\nrelations. This includes the purely qualitative framework, where each player\nhas a single omega-regular objective that she wants to satisfy, but also the\nlarger class of semi-quantitative objectives, where each player has several\nomega-regular objectives equipped with a preorder (for instance, a player may\nwant to satisfy all her objectives, or to maximise the number of objectives\nthat she achieves.) \n\n"}
{"id": "1503.07426", "contents": "Title: A Survey on Approximation Mechanism Design without Money for Facility\n  Games Abstract: In a facility game one or more facilities are placed in a metric space to\nserve a set of selfish agents whose addresses are their private information. In\na classical facility game, each agent wants to be as close to a facility as\npossible, and the cost of an agent can be defined as the distance between her\nlocation and the closest facility. In an obnoxious facility game, each agent\nwants to be far away from all facilities, and her utility is the distance from\nher location to the facility set. The objective of each agent is to minimize\nher cost or maximize her utility. An agent may lie if, by doing so, more\nbenefit can be obtained. We are interested in social choice mechanisms that do\nnot utilize payments. The game designer aims at a mechanism that is\nstrategy-proof, in the sense that any agent cannot benefit by misreporting her\naddress, or, even better, group strategy-proof, in the sense that any coalition\nof agents cannot all benefit by lying. Meanwhile, it is desirable to have the\nmechanism to be approximately optimal with respect to a chosen objective\nfunction. Several models for such approximation mechanism design without money\nfor facility games have been proposed. In this paper we briefly review these\nmodels and related results for both deterministic and randomized mechanisms,\nand meanwhile we present a general framework for approximation mechanism design\nwithout money for facility games. \n\n"}
{"id": "1504.02627", "contents": "Title: What are Strategies in Delay Games? Borel Determinacy for Games with\n  Lookahead Abstract: We investigate determinacy of delay games with Borel winning conditions,\ninfinite-duration two-player games in which one player may delay her moves to\nobtain a lookahead on her opponent's moves.\n  First, we prove determinacy of such games with respect to a fixed evolution\nof the lookahead. However, strategies in such games may depend on information\nabout the evolution. Thus, we introduce different notions of universal\nstrategies for both players, which are evolution-independent, and determine the\nexact amount of information a universal strategy needs about the history of a\nplay and the evolution of the lookahead to be winning. In particular, we show\nthat delay games with Borel winning conditions are determined with respect to\nuniversal strategies. Finally, we consider decidability problems, e.g., \"Does a\nplayer have a universal winning strategy for delay games with a given winning\ncondition?\", for omega-regular and omega-context-free winning conditions. \n\n"}
{"id": "1504.03903", "contents": "Title: Learning to be green: robust energy efficiency maximization in dynamic\n  MIMO-OFDM systems Abstract: In this paper, we examine the maximization of energy efficiency (EE) in\nnext-generation multi-user MIMO-OFDM networks that evolve dynamically over time\n- e.g. due to user mobility, fluctuations in the wireless medium, modulations\nin the users' load, etc. Contrary to the static/stationary regime, the system\nmay evolve in an arbitrary manner so, targeting a fixed optimum state (either\nstatic or in the mean) becomes obsolete; instead, users must adjust to changes\nin the system \"on the fly\", without being able to predict the state of the\nsystem in advance. To tackle these issues, we propose a simple and distributed\nonline optimization policy that leads to no regret, i.e. it allows users to\nmatch (and typically outperform) even the best fixed transmit policy in\nhindsight, irrespective of how the system varies with time. Moreover, to\naccount for the scarcity of perfect channel state information (CSI) in massive\nMIMO systems, we also study the algorithm's robustness in the presence of\nmeasurement errors and observation noise. Importantly, the proposed policy\nretains its no-regret properties under very mild assumptions on the error\nstatistics and, on average, it enjoys the same performance guarantees as in the\nnoiseless, deterministic case. Our analysis is supplemented by extensive\nnumerical simulations which show that, in realistic network environments, users\ntrack their individually optimum transmit profile even under rapidly changing\nchannel conditions, achieving gains of up to 600% in energy efficiency over\nuniform power allocation policies. \n\n"}
{"id": "1504.06828", "contents": "Title: A bi-convex optimization problem to compute Nash equilibrium in n-player\n  games and an algorithm Abstract: In this paper we present optimization problems with biconvex objective\nfunction and linear constraints such that the set of global minima of the\noptimization problems is the same as the set of Nash equilibria of a n-player\ngeneral-sum normal form game. We further show that the objective function is an\ninvex function and consider a projected gradient descent algorithm. We prove\nthat the projected gradient descent scheme converges to a partial optimum of\nthe objective function. We also present simulation results on certain test\ncases showing convergence to a Nash equilibrium strategy. \n\n"}
{"id": "1504.07342", "contents": "Title: On Potential Equations of Finite Games Abstract: In this paper, some new criteria for detecting whether a finite game is\npotential are proposed by solving potential equations. The verification\nequations with the minimal number for checking a potential game are obtained\nfor the first time. Some connections between the potential equations and the\nexisting characterizations of potential games are established. It is revealed\nthat a finite game is potential if and only if its every bi-matrix sub-game is\npotential. \n\n"}
{"id": "1504.07545", "contents": "Title: Matroids are Immune to Braess Paradox Abstract: The famous Braess paradox describes the following phenomenon: It might happen\nthat the improvement of resources, like building a new street within a\ncongested network, may in fact lead to larger costs for the players in an\nequilibrium. In this paper we consider general nonatomic congestion games and\ngive a characterization of the maximal combinatorial property of strategy\nspaces for which Braess paradox does not occur. In a nutshell, bases of\nmatroids are exactly this maximal structure. We prove our characterization by\ntwo novel sensitivity results for convex separable optimization problems over\npolymatroid base polyhedra which may be of independent interest. \n\n"}
{"id": "1504.08211", "contents": "Title: Multidimensional beyond worst-case and almost-sure problems for\n  mean-payoff objectives Abstract: The beyond worst-case threshold problem (BWC), recently introduced by\nBruy\\`ere et al., asks given a quantitative game graph for the synthesis of a\nstrategy that i) enforces some minimal level of performance against any\nadversary, and ii) achieves a good expectation against a stochastic model of\nthe adversary. They solved the BWC problem for finite-memory strategies and\nunidimensional mean-payoff objectives and they showed membership of the problem\nin NP$\\cap$coNP. They also noted that infinite-memory strategies are more\npowerful than finite-memory ones, but the respective threshold problem was left\nopen. We extend these results in several directions. First, we consider\nmultidimensional mean-payoff objectives. Second, we study both finite-memory\nand infinite-memory strategies. We show that the multidimensional BWC problem\nis coNP-complete in both cases. Third, in the special case when the worst-case\nobjective is unidimensional (but the expectation objective is still\nmultidimensional) we show that the complexity decreases to NP$\\cap$coNP. This\nsolves the infinite-memory threshold problem left open by Bruy\\`ere et al., and\nthis complexity cannot be improved without improving the currently known\ncomplexity of classical mean-payoff games. Finally, we introduce a natural\nrelaxation of the BWC problem, the beyond almost-sure threshold problem (BAS),\nwhich asks for the synthesis of a strategy that ensures some minimal level of\nperformance with probability one and a good expectation against the stochastic\nmodel of the adversary. We show that the multidimensional BAS threshold problem\nis solvable in P. \n\n"}
{"id": "1504.08333", "contents": "Title: Revenue-Maximizing Mechanism Design for Quasi-Proportional Auctions Abstract: In quasi-proportional auctions, each bidder receives a fraction of the\nallocation equal to the weight of their bid divided by the sum of weights of\nall bids, where each bid's weight is determined by a weight function. We study\nthe relationship between the weight function, bidders' private values, number\nof bidders, and the seller's revenue in equilibrium. It has been shown that if\none bidder has a much higher private value than the others, then a nearly flat\nweight function maximizes revenue. Essentially, threatening the bidder who has\nthe highest valuation with having to share the allocation maximizes the\nrevenue. We show that as bidder private values approach parity, steeper weight\nfunctions maximize revenue by making the quasi-proportional auction more like a\nwinner-take-all auction. We also show that steeper weight functions maximize\nrevenue as the number of bidders increases. For flatter weight functions, there\nis known to be a unique pure-strategy Nash equilibrium. We show that a\npure-strategy Nash equilibrium also exists for steeper weight functions, and we\ngive lower bounds for bids at an equilibrium. For a special case that includes\nthe two-bidder auction, we show that the pure-strategy Nash equilibrium is\nunique, and we show how to compute the revenue at equilibrium. We also show\nthat selecting a weight function based on private value ratios and number of\nbidders is necessary for a quasi-proportional auction to produce more revenue\nthan a second-price auction. \n\n"}
{"id": "1505.00828", "contents": "Title: Dynamic Consistency of Conditional Simple Temporal Networks via Mean\n  Payoff Games: a Singly-Exponential Time DC-Checking Abstract: Conditional Simple Temporal Network (CSTN) is a constraint-based\ngraph-formalism for conditional temporal planning. It offers a more flexible\nformalism than the equivalent CSTP model of Tsamardinos, Vidal and Pollack,\nfrom which it was derived mainly as a sound formalization. Three notions of\nconsistency arise for CSTNs and CSTPs: weak, strong, and dynamic. Dynamic\nconsistency is the most interesting notion, but it is also the most challenging\nand it was conjectured to be hard to assess. Tsamardinos, Vidal and Pollack\ngave a doubly-exponential time algorithm for deciding whether a CSTN is\ndynamically-consistent and to produce, in the positive case, a dynamic\nexecution strategy of exponential size. In the present work we offer a proof\nthat deciding whether a CSTN is dynamically-consistent is coNP-hard and provide\nthe first singly-exponential time algorithm for this problem, also producing a\ndynamic execution strategy whenever the input CSTN is dynamically-consistent.\nThe algorithm is based on a novel connection with Mean Payoff Games, a family\nof two-player combinatorial games on graphs well known for having applications\nin model-checking and formal verification. The presentation of such connection\nis mediated by the Hyper Temporal Network model, a tractable generalization of\nSimple Temporal Networks whose consistency checking is equivalent to\ndetermining Mean Payoff Games. In order to analyze the algorithm we introduce a\nrefined notion of dynamic-consistency, named \\epsilon-dynamic-consistency, and\npresent a sharp lower bounding analysis on the critical value of the reaction\ntime \\hat{\\varepsilon} where the CSTN transits from being, to not being,\ndynamically-consistent. The proof technique introduced in this analysis of\n\\hat{\\varepsilon} is applicable more in general when dealing with linear\ndifference constraints which include strict inequalities. \n\n"}
{"id": "1505.04406", "contents": "Title: Hinge-Loss Markov Random Fields and Probabilistic Soft Logic Abstract: A fundamental challenge in developing high-impact machine learning\ntechnologies is balancing the need to model rich, structured domains with the\nability to scale to big data. Many important problem areas are both richly\nstructured and large scale, from social and biological networks, to knowledge\ngraphs and the Web, to images, video, and natural language. In this paper, we\nintroduce two new formalisms for modeling structured data, and show that they\ncan both capture rich structure and scale to big data. The first, hinge-loss\nMarkov random fields (HL-MRFs), is a new kind of probabilistic graphical model\nthat generalizes different approaches to convex inference. We unite three\napproaches from the randomized algorithms, probabilistic graphical models, and\nfuzzy logic communities, showing that all three lead to the same inference\nobjective. We then define HL-MRFs by generalizing this unified objective. The\nsecond new formalism, probabilistic soft logic (PSL), is a probabilistic\nprogramming language that makes HL-MRFs easy to define using a syntax based on\nfirst-order logic. We introduce an algorithm for inferring most-probable\nvariable assignments (MAP inference) that is much more scalable than\ngeneral-purpose convex optimization methods, because it uses message passing to\ntake advantage of sparse dependency structures. We then show how to learn the\nparameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous\ndiscrete models, but much more scalable. Together, these algorithms enable\nHL-MRFs and PSL to model rich, structured data at scales not previously\npossible. \n\n"}
{"id": "1505.07737", "contents": "Title: Aggregation of Votes with Multiple Positions on Each Issue Abstract: We consider the problem of aggregating votes cast by a society on a fixed set\nof issues, where each member of the society may vote for one of several\npositions on each issue, but the combination of votes on the various issues is\nrestricted to a set of feasible voting patterns. We require the aggregation to\nbe supportive, i.e. for every issue $j$ the corresponding component $f_j$ of\nevery aggregator on every issue should satisfy $f_j(x_1, ,\\ldots, x_n) \\in\n\\{x_1, ,\\ldots, x_n\\}$. We prove that, in such a set-up, non-dictatorial\naggregation of votes in a society of some size is possible if and only if\neither non-dictatorial aggregation is possible in a society of only two members\nor a ternary aggregator exists that either on every issue $j$ is a majority\noperation, i.e. the corresponding component satisfies $f_j(x,x,y) = f_j(x,y,x)\n= f_j(y,x,x) =x, \\forall x,y$, or on every issue is a minority operation, i.e.\nthe corresponding component satisfies $f_j(x,x,y) = f_j(x,y,x) = f_j(y,x,x) =y,\n\\forall x,y.$ We then introduce a notion of uniformly non-dictatorial\naggregator, which is defined to be an aggregator that on every issue, and when\nrestricted to an arbitrary two-element subset of the votes for that issue,\ndiffers from all projection functions. We first give a characterization of sets\nof feasible voting patterns that admit a uniformly non-dictatorial aggregator.\nThen making use of Bulatov's dichotomy theorem for conservative constraint\nsatisfaction problems, we connect social choice theory with combinatorial\ncomplexity by proving that if a set of feasible voting patterns $X$ has a\nuniformly non-dictatorial aggregator of some arity then the multi-sorted\nconservative constraint satisfaction problem on $X$, in the sense introduced by\nBulatov and Jeavons, with each issue representing a sort, is tractable;\notherwise it is NP-complete. \n\n"}
{"id": "1506.01900", "contents": "Title: Communication Complexity of Distributed Convex Learning and Optimization Abstract: We study the fundamental limits to communication-efficient distributed\nmethods for convex learning and optimization, under different assumptions on\nthe information available to individual machines, and the types of functions\nconsidered. We identify cases where existing algorithms are already worst-case\noptimal, as well as cases where room for further improvement is still possible.\nAmong other things, our results indicate that without similarity between the\nlocal objective functions (due to statistical data similarity or otherwise)\nmany communication rounds may be required, even if the machines have unbounded\ncomputational power. \n\n"}
{"id": "1506.01972", "contents": "Title: Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives Abstract: Many classical algorithms are found until several years later to outlive the\nconfines in which they were conceived, and continue to be relevant in\nunforeseen settings. In this paper, we show that SVRG is one such method: being\noriginally designed for strongly convex objectives, it is also very robust in\nnon-strongly convex or sum-of-non-convex settings.\n  More precisely, we provide new analysis to improve the state-of-the-art\nrunning times in both settings by either applying SVRG or its novel variant.\nSince non-strongly convex objectives include important examples such as Lasso\nor logistic regression, and sum-of-non-convex objectives include famous\nexamples such as stochastic PCA and is even believed to be related to training\ndeep neural nets, our results also imply better performances in these\napplications. \n\n"}
{"id": "1506.02434", "contents": "Title: Strategy Complexity of Concurrent Stochastic Games with Safety and\n  Reachability Objectives Abstract: We consider finite-state concurrent stochastic games, played by k>=2 players\nfor an infinite number of rounds, where in every round, each player\nsimultaneously and independently of the other players chooses an action,\nwhereafter the successor state is determined by a probability distribution\ngiven by the current state and the chosen actions. We consider reachability\nobjectives that given a target set of states require that some state in the\ntarget is visited, and the dual safety objectives that given a target set\nrequire that only states in the target set are visited. We are interested in\nthe complexity of stationary strategies measured by their patience, which is\ndefined as the inverse of the smallest nonzero probability employed. Our main\nresults are as follows: We show that in two-player zero-sum concurrent\nstochastic games (with reachability objective for one player and the\ncomplementary safety objective for the other player): (i) the optimal bound on\nthe patience of optimal and epsilon-optimal strategies, for both players is\ndoubly exponential; and (ii) even in games with a single nonabsorbing state\nexponential (in the number of actions) patience is necessary. In general we\nstudy the class of non-zero-sum games admitting stationary epsilon-Nash\nequilibria. We show that if there is at least one player with reachability\nobjective, then doubly-exponential patience may be needed for epsilon-Nash\nequilibrium strategies, whereas in contrast if all players have safety\nobjectives, the optimal bound on patience for epsilon-Nash equilibrium\nstrategies is only exponential. \n\n"}
{"id": "1506.03374", "contents": "Title: An efficient algorithm for contextual bandits with knapsacks, and an\n  extension to concave objectives Abstract: We consider a contextual version of multi-armed bandit problem with global\nknapsack constraints. In each round, the outcome of pulling an arm is a scalar\nreward and a resource consumption vector, both dependent on the context, and\nthe global knapsack constraints require the total consumption for each resource\nto be below some pre-fixed budget. The learning agent competes with an\narbitrary set of context-dependent policies. This problem was introduced by\nBadanidiyuru et al. (2014), who gave a computationally inefficient algorithm\nwith near-optimal regret bounds for it. We give a computationally efficient\nalgorithm for this problem with slightly better regret bounds, by generalizing\nthe approach of Agarwal et al. (2014) for the non-constrained version of the\nproblem. The computational time of our algorithm scales logarithmically in the\nsize of the policy space. This answers the main open question of Badanidiyuru\net al. (2014). We also extend our results to a variant where there are no\nknapsack constraints but the objective is an arbitrary Lipschitz concave\nfunction of the sum of outcome vectors. \n\n"}
{"id": "1506.03378", "contents": "Title: On the Prior Sensitivity of Thompson Sampling Abstract: The empirically successful Thompson Sampling algorithm for stochastic bandits\nhas drawn much interest in understanding its theoretical properties. One\nimportant benefit of the algorithm is that it allows domain knowledge to be\nconveniently encoded as a prior distribution to balance exploration and\nexploitation more effectively. While it is generally believed that the\nalgorithm's regret is low (high) when the prior is good (bad), little is known\nabout the exact dependence. In this paper, we fully characterize the\nalgorithm's worst-case dependence of regret on the choice of prior, focusing on\na special yet representative case. These results also provide insights into the\ngeneral sensitivity of the algorithm to the choice of priors. In particular,\nwith $p$ being the prior probability mass of the true reward-generating model,\nwe prove $O(\\sqrt{T/p})$ and $O(\\sqrt{(1-p)T})$ regret upper bounds for the\nbad- and good-prior cases, respectively, as well as \\emph{matching} lower\nbounds. Our proofs rely on the discovery of a fundamental property of Thompson\nSampling and make heavy use of martingale theory, both of which appear novel in\nthe literature, to the best of our knowledge. \n\n"}
{"id": "1506.03489", "contents": "Title: Truthful Linear Regression Abstract: We consider the problem of fitting a linear model to data held by individuals\nwho are concerned about their privacy. Incentivizing most players to truthfully\nreport their data to the analyst constrains our design to mechanisms that\nprovide a privacy guarantee to the participants; we use differential privacy to\nmodel individuals' privacy losses. This immediately poses a problem, as\ndifferentially private computation of a linear model necessarily produces a\nbiased estimation, and existing approaches to design mechanisms to elicit data\nfrom privacy-sensitive individuals do not generalize well to biased estimators.\nWe overcome this challenge through an appropriate design of the computation and\npayment scheme. \n\n"}
{"id": "1506.03662", "contents": "Title: Variance Reduced Stochastic Gradient Descent with Neighbors Abstract: Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its\nslow convergence can be a computational bottleneck. Variance reduction\ntechniques such as SAG, SVRG and SAGA have been proposed to overcome this\nweakness, achieving linear convergence. However, these methods are either based\non computations of full gradients at pivot points, or on keeping per data point\ncorrections in memory. Therefore speed-ups relative to SGD may need a minimal\nnumber of epochs in order to materialize. This paper investigates algorithms\nthat can exploit neighborhood structure in the training data to share and\nre-use information about past stochastic gradients across data points, which\noffers advantages in the transient optimization phase. As a side-product we\nprovide a unified convergence analysis for a family of variance reduction\nalgorithms, which we call memorization algorithms. We provide experimental\nresults supporting our theory. \n\n"}
{"id": "1506.04047", "contents": "Title: Approximation Algorithm for the Binary-Preference Capacitated Selfish\n  Replication Game and a Tight Bound on its Price of Anarchy Abstract: We consider the capacitated selfish replication (CSR) game with binary\npreferences, over general undirected networks. We first show that such games\nhave an associated ordinary potential function, and hence always admit a\npure-strategy Nash equilibrium (NE). Further, when the minimum degree of the\nnetwork and the number of resources are of the same order, there exists an\nexact polynomial time algorithm which can find a NE. Following this, we study\nthe price of anarchy of such games, and show that it is bounded above by 3; we\nfurther provide some instances for which the price of anarchy is at least 2. We\ndevelop a quasi-polynomial algorithm O(n^2D^{ln n}), where n is the number of\nplayers and D is the diameter of the network, which can find, in a distributed\nmanner, an allocation profile that is within a constant factor of the optimal\nallocation, and hence of any pure-strategy NE of the game. Proof of this result\nuses a novel potential function. \n\n"}
{"id": "1506.04641", "contents": "Title: Strategy Recovery for Stochastic Mean Payoff Games Abstract: We prove that to find optimal positional strategies for stochastic mean\npayoff games when the value of every state of the game is known, in general, is\nas hard as solving such games tout court. This answers a question posed by\nDaniel Andersson and Peter Bro Miltersen. \n\n"}
{"id": "1506.04838", "contents": "Title: Spectral Sparsification and Regret Minimization Beyond Matrix\n  Multiplicative Updates Abstract: In this paper, we provide a novel construction of the linear-sized spectral\nsparsifiers of Batson, Spielman and Srivastava [BSS14]. While previous\nconstructions required $\\Omega(n^4)$ running time [BSS14, Zou12], our\nsparsification routine can be implemented in almost-quadratic running time\n$O(n^{2+\\varepsilon})$.\n  The fundamental conceptual novelty of our work is the leveraging of a strong\nconnection between sparsification and a regret minimization problem over\ndensity matrices. This connection was known to provide an interpretation of the\nrandomized sparsifiers of Spielman and Srivastava [SS11] via the application of\nmatrix multiplicative weight updates (MWU) [CHS11, Vis14]. In this paper, we\nexplain how matrix MWU naturally arises as an instance of the\nFollow-the-Regularized-Leader framework and generalize this approach to yield a\nlarger class of updates. This new class allows us to accelerate the\nconstruction of linear-sized spectral sparsifiers, and give novel insights on\nthe motivation behind Batson, Spielman and Srivastava [BSS14]. \n\n"}
{"id": "1506.06438", "contents": "Title: Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms Abstract: Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of\nmachine learning problems. Researchers and industry have developed several\ntechniques to optimize SGD's runtime performance, including asynchronous\nexecution and reduced precision. Our main result is a martingale-based analysis\nthat enables us to capture the rich noise models that may arise from such\ntechniques. Specifically, we use our new analysis in three ways: (1) we derive\nconvergence rates for the convex case (Hogwild!) with relaxed assumptions on\nthe sparsity of the problem; (2) we analyze asynchronous SGD algorithms for\nnon-convex matrix problems including matrix completion; and (3) we design and\nanalyze an asynchronous SGD algorithm, called Buckwild!, that uses\nlower-precision arithmetic. We show experimentally that our algorithms run\nefficiently for a variety of problems on modern hardware. \n\n"}
{"id": "1506.07942", "contents": "Title: A Comprehensive Survey of Potential Game Approaches to Wireless Networks Abstract: Potential games form a class of non-cooperative games where unilateral\nimprovement dynamics are guaranteed to converge in many practical cases. The\npotential game approach has been applied to a wide range of wireless network\nproblems, particularly to a variety of channel assignment problems. In this\npaper, the properties of potential games are introduced, and games in wireless\nnetworks that have been proven to be potential games are comprehensively\ndiscussed. \n\n"}
{"id": "1507.00576", "contents": "Title: Flip the Cloud: Cyber-Physical Signaling Games in the Presence of\n  Advanced Persistent Threats Abstract: Access to the cloud has the potential to provide scalable and cost effective\nenhancements of physical devices through the use of advanced computational\nprocesses run on apparently limitless cyber infrastructure. On the other hand,\ncyber-physical systems and cloud-controlled devices are subject to numerous\ndesign challenges; among them is that of security. In particular, recent\nadvances in adversary technology pose Advanced Persistent Threats (APTs) which\nmay stealthily and completely compromise a cyber system. In this paper, we\ndesign a framework for the security of cloud-based systems that specifies when\na device should trust commands from the cloud which may be compromised. This\ninteraction can be considered as a game between three players: a cloud\ndefender/administrator, an attacker, and a device. We use traditional signaling\ngames to model the interaction between the cloud and the device, and we use the\nrecently proposed FlipIt game to model the struggle between the defender and\nattacker for control of the cloud. Because attacks upon the cloud can occur\nwithout knowledge of the defender, we assume that strategies in both games are\npicked according to prior commitment. This framework requires a new equilibrium\nconcept, which we call Gestalt Equilibrium, a fixed-point that expresses the\ninterdependence of the signaling and FlipIt games. We present the solution to\nthis fixed-point problem under certain parameter cases, and illustrate an\nexample application of cloud control of an unmanned vehicle. Our results\ncontribute to the growing understanding of cloud-controlled systems. \n\n"}
{"id": "1507.01423", "contents": "Title: Abstract Interpretation of Supermodular Games Abstract: Supermodular games find significant applications in a variety of models,\nespecially in operations research and economic applications of noncooperative\ngame theory, and feature pure strategy Nash equilibria characterized as fixed\npoints of multivalued functions on complete lattices. Pure strategy Nash\nequilibria of supermodular games are here approximated by resorting to the\ntheory of abstract interpretation, a well established and known framework used\nfor designing static analyses of programming languages. This is obtained by\nextending the theory of abstract interpretation in order to handle\napproximations of multivalued functions and by providing some methods for\nabstracting supermodular games, in order to obtain approximate Nash equilibria\nwhich are shown to be correct within the abstract interpretation framework. \n\n"}
{"id": "1507.02746", "contents": "Title: Low-Risk Mechanisms for the Kidney Exchange Game Abstract: In this paper we consider the pairwise kidney exchange game. This game\nnaturally appears in situations that some service providers benefit from\npairwise allocations on a network, such as the kidney exchanges between\nhospitals.\n  Ashlagi et al. present a $2$-approximation randomized truthful mechanism for\nthis problem. This is the best known result in this setting with multiple\nplayers. However, we note that the variance of the utility of an agent in this\nmechanism may be as large as $\\Omega(n^2)$, which is not desirable in a real\napplication. In this paper we resolve this issue by providing a\n$2$-approximation randomized truthful mechanism in which the variance of the\nutility of each agent is at most $2+\\epsilon$.\n  Interestingly, we could apply our technique to design a deterministic\nmechanism such that, if an agent deviates from the mechanism, she does not gain\nmore than $2\\lceil \\log_2 m\\rceil$. We call such a mechanism an almost truthful\nmechanism. Indeed, in a practical scenario, an almost truthful mechanism is\nlikely to imply a truthful mechanism. We believe that our approach can be used\nto design low risk or almost truthful mechanisms for other problems. \n\n"}
{"id": "1507.06738", "contents": "Title: Linear Contextual Bandits with Knapsacks Abstract: We consider the linear contextual bandit problem with resource consumption,\nin addition to reward generation. In each round, the outcome of pulling an arm\nis a reward as well as a vector of resource consumptions. The expected values\nof these outcomes depend linearly on the context of that arm. The\nbudget/capacity constraints require that the total consumption doesn't exceed\nthe budget for each resource. The objective is once again to maximize the total\nreward. This problem turns out to be a common generalization of classic linear\ncontextual bandits (linContextual), bandits with knapsacks (BwK), and the\nonline stochastic packing problem (OSPP). We present algorithms with\nnear-optimal regret bounds for this problem. Our bounds compare favorably to\nresults on the unstructured version of the problem where the relation between\nthe contexts and the outcomes could be arbitrary, but the algorithm only\ncompetes against a fixed set of policies accessible through an optimization\noracle. We combine techniques from the work on linContextual, BwK, and OSPP in\na nontrivial manner while also tackling new difficulties that are not present\nin any of these special cases. \n\n"}
{"id": "1507.07045", "contents": "Title: The Square Root Agreement Rule for Incentivizing Truthful Feedback on\n  Online Platforms Abstract: A major challenge in obtaining evaluations of products or services on\ne-commerce platforms is eliciting informative responses in the absence of\nverifiability. This paper proposes the Square Root Agreement Rule (SRA): a\nsimple reward mechanism that incentivizes truthful responses to objective\nevaluations on such platforms. In this mechanism, an agent gets a reward for an\nevaluation only if her answer matches that of her peer, where this reward is\ninversely proportional to a popularity index of the answer. This index is\ndefined to be the square root of the empirical frequency at which any two\nagents performing the same evaluation agree on the particular answer across\nevaluations of similar entities operating on the platform. Rarely agreed-upon\nanswers thus earn a higher reward than answers for which agreements are\nrelatively more common.\n  We show that in the many tasks regime, the truthful equilibrium under SRA is\nstrictly payoff-dominant across large classes of natural equilibria that could\narise in these settings, thus increasing the likelihood of its adoption. While\nthere exist other mechanisms achieving such guarantees, they either impose\nadditional assumptions on the response distribution that are not generally\nsatisfied for objective evaluations or they incentivize truthful behavior only\nif each agent performs a prohibitively large number of evaluations and commits\nto using the same strategy for each evaluation. SRA is the first known\nincentive mechanism satisfying such guarantees without imposing any such\nrequirements. Moreover, our empirical findings demonstrate the robustness of\nthe incentive properties of SRA in the presence of mild subjectivity or\nobservational biases in the responses. These properties make SRA uniquely\nattractive for administering reward-based incentive schemes (e.g., rebates,\ndiscounts, reputation scores, etc.) on online platforms. \n\n"}
{"id": "1507.07064", "contents": "Title: Strategyproof Quota Mechanisms for Multiple Assignment Problems Abstract: We study the problem of allocating multiple objects to agents without\ntransferable utilities, where each agent may receive more than one object\naccording to a quota. Under lexicographic preferences, we characterize the set\nof strategyproof, non-bossy, and neutral quota mechanisms and show that under a\nmild Pareto efficiency condition, serial dictatorship quota mechanisms are the\nonly mechanisms satisfying these properties. Dropping the neutrality\nrequirement, this class of quota mechanisms further expands to sequential\ndictatorship quota mechanisms. We then extend quota mechanisms to randomized\nsettings, and show that the random serial dictatorship quota mechanisms (RSDQ)\nare envyfree, strategyproof, and ex post efficient for any number of agents and\nobjects and any quota system, proving that the well-studied Random Serial\nDictatorship (RSD) satisfies envyfreeness when preferences are lexicographic. \n\n"}
{"id": "1507.07688", "contents": "Title: Belief and Truth in Hypothesised Behaviours Abstract: There is a long history in game theory on the topic of Bayesian or \"rational\"\nlearning, in which each player maintains beliefs over a set of alternative\nbehaviours, or types, for the other players. This idea has gained increasing\ninterest in the artificial intelligence (AI) community, where it is used as a\nmethod to control a single agent in a system composed of multiple agents with\nunknown behaviours. The idea is to hypothesise a set of types, each specifying\na possible behaviour for the other agents, and to plan our own actions with\nrespect to those types which we believe are most likely, given the observed\nactions of the agents. The game theory literature studies this idea primarily\nin the context of equilibrium attainment. In contrast, many AI applications\nhave a focus on task completion and payoff maximisation. With this perspective\nin mind, we identify and address a spectrum of questions pertaining to belief\nand truth in hypothesised types. We formulate three basic ways to incorporate\nevidence into posterior beliefs and show when the resulting beliefs are\ncorrect, and when they may fail to be correct. Moreover, we demonstrate that\nprior beliefs can have a significant impact on our ability to maximise payoffs\nin the long-term, and that they can be computed automatically with consistent\nperformance effects. Furthermore, we analyse the conditions under which we are\nable complete our task optimally, despite inaccuracies in the hypothesised\ntypes. Finally, we show how the correctness of hypothesised types can be\nascertained during the interaction via an automated statistical analysis. \n\n"}
{"id": "1508.01818", "contents": "Title: Designing Incentive Schemes For Privacy-Sensitive Users Abstract: Businesses (retailers) often wish to offer personalized advertisements\n(coupons) to individuals (consumers), but run the risk of strong reactions from\nconsumers who want a customized shopping experience but feel their privacy has\nbeen violated. Existing models for privacy such as differential privacy or\ninformation theory try to quantify privacy risk but do not capture the\nsubjective experience and heterogeneous expression of privacy-sensitivity. We\npropose a Markov decision process (MDP) model to capture (i) different consumer\nprivacy sensitivities via a time-varying state; (ii) different coupon types\n(action set) for the retailer; and (iii) the action-and-state-dependent cost\nfor perceived privacy violations. For the simple case with two states (\"Normal\"\nand \"Alerted\"), two coupons (targeted and untargeted) model, and consumer\nbehavior statistics known to the retailer, we show that a stationary\nthreshold-based policy is the optimal coupon-offering strategy for a retailer\nthat wishes to minimize its expected discounted cost. The threshold is a\nfunction of all model parameters; the retailer offers a targeted coupon if\ntheir belief that the consumer is in the \"Alerted\" state is below the\nthreshold. We extend this two-state model to consumers with multiple\nprivacy-sensitivity states as well as coupon-dependent state transition\nprobabilities. Furthermore, we study the case with imperfect (noisy) cost\nfeedback from consumers and uncertain initial belief state. \n\n"}
{"id": "1508.02440", "contents": "Title: Energy Structure of Optimal Positional Strategies in Mean Payoff Games Abstract: This note studies structural aspects concerning Optimal Positional Strategies\n(OPSs) in Mean Payoff Games (MPGs), it is a contribution to understanding the\nrelationship between OPSs in MPGs and Small Energy-Progress Measures (SEPMs) in\nreweighted Energy Games (EGs). Firstly, it is observed that the space of all\nOPSs, $\\texttt{opt}_{\\Gamma}\\Sigma^M_0$, admits a unique complete decomposition\nin terms of so-called extremal-SEPM{s} in reweighted EG{s}; this points out\nwhat we called the \"Energy-Lattice $\\mathcal{X}^*_{\\Gamma}$ of\n$\\texttt{opt}_{\\Gamma}\\Sigma^M_0$\". Secondly, it is offered a pseudo-polynomial\ntotal-time recursive procedure for enumerating (w/o repetitions) all the\nelements of $\\mathcal{X}^*_{\\Gamma}$, and for computing the corresponding\npartitioning of $\\texttt{opt}_{\\Gamma}\\Sigma^M_0$. It is observed that the\ncorresponding recursion tree defines an additional lattice\n$\\mathcal{B}^*_{\\Gamma}$, whose elements are certain subgames $\\Gamma'\\subseteq\n\\Gamma$ that we call basic subgames. The extremal-SEPMs of a given \\MPG\n$\\Gamma$ coincide with the least-SEPMs of the basic subgames of $\\Gamma$; so,\n$\\mathcal{X}^*_{\\Gamma}$ is the energy-lattice comprising all and only the\nleast-SEPMs of the \\emph{basic} subgames of $\\Gamma$. The complexity of the\nproposed enumeration for both $\\mathcal{B}^*_{\\Gamma}$ and\n$\\mathcal{X}^*_{\\Gamma}$ is $O(|V|^3|E|W |\\mathcal{B}^*_{\\Gamma}|)$ total time\nand $O(|V||E|)+\\Theta\\big(|E| \\mathcal{B}^*_{\\Gamma}|\\big)$ working space.\nFinally, it is constructed an \\MPG $\\Gamma$ for which $|\\mathcal{B}^*_{\\Gamma}|\n> |\\mathcal{X}^*_\\Gamma|$, this proves that $\\mathcal{B}^*_{\\Gamma}$ and\n$\\mathcal{X}^*_\\Gamma$ are not isomorphic. \n\n"}
{"id": "1508.03420", "contents": "Title: A strategic timing of arrivals to a linear slowdown processor sharing\n  system Abstract: We consider a discrete population of users with homogeneous service demand\nwho need to decide when to arrive to a system in which the service rate\ndeteriorates linearly with the number of users in the system. The users have\nheterogeneous desired departure times from the system, and their goal is to\nminimize a weighted sum of the travel time and square deviation from the\ndesired departure times. Users join the system sequentially, according to the\norder of their desired departure times. We model this scenario as a\nnon-cooperative game in which each user selects his actual arrival time. We\npresent explicit equilibria solutions for a two-user example, namely the\nsubgame perfect and Nash equilibria and show that multiple equilibria may\nexist. We further explain why a general solution for any number of users is\ncomputationally challenging. The difficulty lies in the fact that the objective\nfunctions are piecewise convex, i.e., non-smooth and non-convex. As a result,\nthe minimization of the costs relies on checking all arrival and departure\norder permutations, which is exponentially large with respect to the population\nsize. Instead we propose an iterated best-response algorithm which can be\nefficiently studied numerically. Finally, we compare the equilibrium arrival\nprofiles to a socially optimal solution and discuss the implications. \n\n"}
{"id": "1508.07370", "contents": "Title: Large Market Games with Near Optimal Efficiency Abstract: As is well known, many classes of markets have efficient equilibria, but this\ndepends on agents being non-strategic, i.e. that they declare their true\ndemands when offered goods at particular prices, or in other words, that they\nare price-takers. An important question is how much the equilibria degrade in\nthe face of strategic behavior, i.e. what is the Price of Anarchy (PoA) of the\nmarket viewed as a mechanism?\n  Often, PoA bounds are modest constants such as 4/3 or 2. Nonetheless, in\npractice a guarantee that no more than 25% or 50% of the economic value is lost\nmay be unappealing. This paper asks whether significantly better bounds are\npossible under plausible assumptions. In particular, we look at how these worst\ncase guarantees improve in the following large settings.\n  Large Walrasian auctions: These are auctions with many copies of each item\nand many agents. We show that the PoA tends to 1 as the market size increases,\nunder suitable conditions, mainly that there is some uncertainty about the\nnumbers of copies of each good and demands obey the gross substitutes\ncondition. We also note that some such assumption is unavoidable.\n  Large Fisher markets: Fisher markets are a class of economies that has\nreceived considerable attention in the computer science literature. A large\nmarket is one in which at equilibrium, each buyer makes only a small fraction\nof the total purchases; the smaller the fraction, the larger the market. Here\nthe main condition is that demands are based on homogeneous monotone utility\nfunctions that satisfy the gross substitutes condition. Again, the PoA tends to\n1 as the market size increases.\n  Furthermore, in each setting, we quantify the tradeoff between market size\nand the PoA. \n\n"}
{"id": "1508.07370", "contents": "Title: Large Market Games with Near Optimal Efficiency Abstract: As is well known, many classes of markets have efficient equilibria, but this\ndepends on agents being non-strategic, i.e. that they declare their true\ndemands when offered goods at particular prices, or in other words, that they\nare price-takers. An important question is how much the equilibria degrade in\nthe face of strategic behavior, i.e. what is the Price of Anarchy (PoA) of the\nmarket viewed as a mechanism?\n  Often, PoA bounds are modest constants such as 4/3 or 2. Nonetheless, in\npractice a guarantee that no more than 25% or 50% of the economic value is lost\nmay be unappealing. This paper asks whether significantly better bounds are\npossible under plausible assumptions. In particular, we look at how these worst\ncase guarantees improve in the following large settings.\n  Large Walrasian auctions: These are auctions with many copies of each item\nand many agents. We show that the PoA tends to 1 as the market size increases,\nunder suitable conditions, mainly that there is some uncertainty about the\nnumbers of copies of each good and demands obey the gross substitutes\ncondition. We also note that some such assumption is unavoidable.\n  Large Fisher markets: Fisher markets are a class of economies that has\nreceived considerable attention in the computer science literature. A large\nmarket is one in which at equilibrium, each buyer makes only a small fraction\nof the total purchases; the smaller the fraction, the larger the market. Here\nthe main condition is that demands are based on homogeneous monotone utility\nfunctions that satisfy the gross substitutes condition. Again, the PoA tends to\n1 as the market size increases.\n  Furthermore, in each setting, we quantify the tradeoff between market size\nand the PoA. \n\n"}
{"id": "1509.01314", "contents": "Title: Exponential Weight Functions for Quasi-Proportional Auctions Abstract: In quasi-proportional auctions, the allocation is shared among bidders in\nproportion to their weighted bids. The auctioneer selects a bid weight\nfunction, and bidders know the weight function when they bid. In this note, we\nanalyze how weight functions that are exponential in the bid affect bidder\nbehavior. We show that exponential weight functions have a pure-strategy Nash\nequilibrium, we characterize bids at an equilibrium, and we compare it to an\nequilibrium for power weight functions. \n\n"}
{"id": "1509.01404", "contents": "Title: Coordinate Descent Methods for Symmetric Nonnegative Matrix\n  Factorization Abstract: Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix\nfactorization (symNMF) is the problem of finding a nonnegative matrix $H$,\nusually with much fewer columns than $A$, such that $A \\approx HH^T$. SymNMF\ncan be used for data analysis and in particular for various clustering tasks.\nIn this paper, we propose simple and very efficient coordinate descent schemes\nto solve this problem, and that can handle large and sparse input matrices. The\neffectiveness of our methods is illustrated on synthetic and real-world data\nsets, and we show that they perform favorably compared to recent\nstate-of-the-art methods. \n\n"}
{"id": "1509.02333", "contents": "Title: Precise Complexity of the Core in Dichotomous and Additive Hedonic Games Abstract: Hedonic games provide a general model of coalition formation, in which a set\nof agents is partitioned into coalitions, with each agent having preferences\nover which other players are in her coalition. We prove that with additively\nseparable preferences, it is $\\Sigma_2^p$-complete to decide whether a core- or\nstrict-core-stable partition exists, extending a result of Woeginger (2013).\nOur result holds even if valuations are symmetric and non-zero only for a\nconstant number of other agents. We also establish $\\Sigma_2^p$-completeness of\ndeciding non-emptiness of the strict core for hedonic games with dichotomous\npreferences. Such results establish that the core is much less tractable than\nsolution concepts such as individual stability. \n\n"}
{"id": "1509.03327", "contents": "Title: Optimal Strategy in \"Guess Who?\": Beyond Binary Search Abstract: \"Guess Who?\" is a popular two player game where players ask \"Yes\"/\"No\"\nquestions to search for their opponent's secret identity from a pool of\npossible candidates. This is modeled as a simple stochastic game. Using this\nmodel, the optimal strategy is explicitly found. Contrary to popular belief,\nperforming a binary search is \\emph{not} always optimal. Instead, the optimal\nstrategy for the player who trails is to make certain bold plays in an attempt\ncatch up. This is discovered by first analyzing a continuous version of the\ngame where players play indefinitely and the winner is never decided after\nfinitely many rounds. \n\n"}
{"id": "1509.04344", "contents": "Title: Stable Nash Equilibria in the Gale-Shapley Matching Game Abstract: In this article we study the stable marriage game induced by the\nmen-proposing Gale-Shapley algorithm. Our setting is standard: all the lists\nare complete and the matching mechanism is the men-proposing Gale-Shapley\nalgorithm. It is well known that in this setting, men cannot cheat, but women\ncan. In fact, Teo, Sethuraman and Tan \\cite{TST01}, show that there is a\npolynomial time algorithm to obtain, for a given strategy (the set of all\nlists) $Q$ and a woman $w$, the best partner attainable by changing her list.\nHowever, what if the resulting matching is not stable with respect to $Q$?\nObviously, such a matching would be vulnerable to further manipulation, but is\nnot mentioned in \\cite{TST01}. In this paper, we consider (safe) manipulation\nthat implies a stable matching in a most general setting. Specifically, our\ngoal is to decide for a given $Q$, if w can manipulate her list to obtain a\nstrictly better partner with respect to the true strategy $P$ (which may be\ndifferent from $Q$), and also the outcome is a stable matching for $P$. \n\n"}
{"id": "1509.05497", "contents": "Title: Quadratic Gaussian Privacy Games Abstract: A game-theoretic model for analysing the effects of privacy on strategic\ncommunication between agents is devised. In the model, a sender wishes to\nprovide an accurate measurement of the state to a receiver while also\nprotecting its private information (which is correlated with the state) private\nfrom a malicious agent that may eavesdrop on its communications with the\nreceiver. A family of nontrivial equilibria, in which the communicated messages\ncarry information, is constructed and its properties are studied. \n\n"}
{"id": "1509.07495", "contents": "Title: Unbounded Lookahead in WMSO+U Games Abstract: Delay games are two-player games of infinite duration in which one player may\ndelay her moves to obtain a lookahead on her opponent's moves. We consider\ndelay games with winning conditions expressed in weak monadic second order\nlogic with the unbounding quantifier (WMSO+U), which is able to express\n(un)boundedness properties. It is decidable whether the delaying player is able\nto win such a game with bounded lookahead, i.e., if she only skips a finite\nnumber of moves.\n  However, bounded lookahead is not always sufficient: we present a game that\ncan be won with unbounded lookahead, but not with bounded lookahead. Then, we\nconsider WMSO+U delay games with unbounded lookahead and show that the exact\nevolution of the lookahead is irrelevant: the winner is always the same, as\nlong as the initial lookahead is large enough and the lookahead tends to\ninfinity. \n\n"}
{"id": "1509.09002", "contents": "Title: Convergence of Stochastic Gradient Descent for PCA Abstract: We consider the problem of principal component analysis (PCA) in a streaming\nstochastic setting, where our goal is to find a direction of approximate\nmaximal variance, based on a stream of i.i.d. data points in $\\reals^d$. A\nsimple and computationally cheap algorithm for this is stochastic gradient\ndescent (SGD), which incrementally updates its estimate based on each new data\npoint. However, due to the non-convex nature of the problem, analyzing its\nperformance has been a challenge. In particular, existing guarantees rely on a\nnon-trivial eigengap assumption on the covariance matrix, which is intuitively\nunnecessary. In this paper, we provide (to the best of our knowledge) the first\neigengap-free convergence guarantees for SGD in the context of PCA. This also\npartially resolves an open problem posed in \\cite{hardt2014noisy}. Moreover,\nunder an eigengap assumption, we show that the same techniques lead to new SGD\nconvergence guarantees with better dependence on the eigengap. \n\n"}
{"id": "1510.00295", "contents": "Title: Welfare and Rationality Guarantees for the Simultaneous Multiple-Round\n  Ascending Auction Abstract: The simultaneous multiple-round auction (SMRA) and the combinatorial clock\nauction (CCA) are the two primary mechanisms used to sell bandwidth. Under\ntruthful bidding, the SMRA is known to output a Walrasian equilibrium that\nmaximizes social welfare provided the bidder valuation functions satisfy the\ngross substitutes property. Recently, it was shown that the combinatorial clock\nauction (CCA) provides good welfare guarantees for general classes of valuation\nfunctions. This motivates the question of whether similar welfare guarantees\nhold for the SMRA in the case of general valuation functions.\n  We show the answer is no. But we prove that good welfare guarantees still\narise if the degree of complementarities in the bidder valuations are bounded.\nIn particular, if bidder valuations functions are $\\alpha$-near-submodular\nthen, under truthful bidding, the SMRA has a welfare ratio (the worst case\nratio between the social welfare of the optimal allocation and the auction\nallocation) of at most $(1+\\alpha)$. The special case of submodular valuations,\nnamely $\\alpha=1$, and produces individually rational solutions. However, for\n$\\alpha>1$, this is a bicriteria guarantee, to obtain good welfare under\ntruthful bidding requires relaxing individual rationality.\n  Finally, we examine what strategies are required to ensure individual\nrationality in the SMRA with general valuation functions. First, we provide a\nweak characterization, namely \\emph{secure bidding}, for individual\nrationality. We then show that if the bidders use a profit-maximizing secure\nbidding strategy the welfare ratio is at most $1+\\alpha$. Consequently, by\nbidding securely, it is possible to obtain the same welfare guarantees as\ntruthful bidding without the loss of individual rationality. \n\n"}
{"id": "1510.02045", "contents": "Title: Budget Constraints in Prediction Markets Abstract: We give a detailed characterization of optimal trades under budget\nconstraints in a prediction market with a cost-function-based automated market\nmaker. We study how the budget constraints of individual traders affect their\nability to impact the market price. As a concrete application of our\ncharacterization, we give sufficient conditions for a property we call budget\nadditivity: two traders with budgets B and B' and the same beliefs would have a\ncombined impact equal to a single trader with budget B+B'. That way, even if a\nsingle trader cannot move the market much, a crowd of like-minded traders can\nhave the same desired effect. When the set of payoff vectors associated with\noutcomes, with coordinates corresponding to securities, is affinely\nindependent, we obtain that a generalization of the heavily-used logarithmic\nmarket scoring rule is budget additive, but the quadratic market scoring rule\nis not. Our results may be used both descriptively, to understand if a\nparticular market maker is affected by budget constraints or not, and\nprescriptively, as a recipe to construct markets. \n\n"}
{"id": "1510.02067", "contents": "Title: Asymptotically tight bounds for inefficiency in risk-averse selfish\n  routing Abstract: We consider a nonatomic selfish routing model with independent stochastic\ntravel times, represented by mean and variance latency functions for each edge\nthat depend on their flows. In an effort to decouple the effect of risk-averse\nplayer preferences from selfish behavior on the degradation of system\nperformance, Nikolova and Stier- Moses [16] defined the concept of the price of\nrisk aversion as the worst-case ratio of the cost of an equilibrium with\nrisk-averse players and that of an equilibrium with risk-neutral users. For\nrisk-averse users who seek to minimize the mean plus variance of travel time on\na path, they proved an upper bound on the price of risk aversion, which is\nindependent of the latency functions, and grows linearly with the size of the\ngraph and players' risk-aversion. In this follow-up paper, we provide a\nmatching lower bound for graphs with number of vertices equal to powers of two,\nvia the construction of a graph family inductively generated from the Braess\ngraph. We also provide conceptually different bounds, which we call functional,\nthat depend on the class of mean latency functions and provide\ncharacterizations that are independent of the network topology (first derived,\nin a more complicated way, by Meir and Parkes [10] in a different context with\ndifferent techniques). We also supplement the upper bound with a new\nasymptotically-tight lower bound. Our third contribution is a tight bound on\nthe price of risk aversion for a family of graphs that generalize\nseries-parallel graphs which applies to users minimizing the mean plus standard\ndeviation of a path, a much more complex model of risk-aversion due to the cost\nof a path being non-additive over edge costs. This is a refinement of previous\nresults in [16] that characterized the price of risk-aversion for\nseries-parallel graphs and for the Braess graph. \n\n"}
{"id": "1510.03903", "contents": "Title: Fair Cake-Cutting among Families Abstract: We study the fair division of a continuous resource, such as a land-estate or\na time-interval, among pre-specified groups of agents, such as families. Each\nfamily is given a piece of the resource and this piece is used simultaneously\nby all family members, while different members may have different value\nfunctions. Three ways to assess the fairness of such a division are examined.\n(a) Average Fairness means that each family's share is fair according to the\n\"family value function\", defined as the arithmetic mean of the value functions\nof the family members. (b) Unanimous Fairness means that all members in all\nfamilies feel that their family's share is fair according to their personal\nvalue function. (c) Democratic Fairness means that in each family, at least a\nfixed fraction (e.g. a half) of the members feel that their family's share is\nfair. We compare these criteria based on the number of connected components in\nthe resulting division and on their compatibility with Pareto-efficiency. \n\n"}
{"id": "1510.04991", "contents": "Title: Honest signaling in zero-sum games is hard, and lying is even harder Abstract: We prove that, assuming the exponential time hypothesis, finding an\n\\epsilon-approximately optimal symmetric signaling scheme in a two-player\nzero-sum game requires quasi-polynomial time. This is tight by [Cheng et al.,\nFOCS'15] and resolves an open question of [Dughmi, FOCS'14]. We also prove that\nfinding a multiplicative approximation is NP-hard.\n  We also introduce a new model where a dishonest signaler may publicly commit\nto use one scheme, but post signals according to a different scheme. For this\nmodel, we prove that even finding a (1-2^{-n})-approximately optimal scheme is\nNP-hard. \n\n"}
{"id": "1510.05417", "contents": "Title: Piecewise-Linear Approximation for Feature Subset Selection in a\n  Sequential Logit Model Abstract: This paper concerns a method of selecting a subset of features for a\nsequential logit model. Tanaka and Nakagawa (2014) proposed a mixed integer\nquadratic optimization formulation for solving the problem based on a quadratic\napproximation of the logistic loss function. However, since there is a\nsignificant gap between the logistic loss function and its quadratic\napproximation, their formulation may fail to find a good subset of features. To\novercome this drawback, we apply a piecewise-linear approximation to the\nlogistic loss function. Accordingly, we frame the feature subset selection\nproblem of minimizing an information criterion as a mixed integer linear\noptimization problem. The computational results demonstrate that our\npiecewise-linear approximation approach found a better subset of features than\nthe quadratic approximation approach. \n\n"}
{"id": "1510.06567", "contents": "Title: Generalized conditional gradient: analysis of convergence and\n  applications Abstract: The objectives of this technical report is to provide additional results on\nthe generalized conditional gradient methods introduced by Bredies et al.\n[BLM05]. Indeed , when the objective function is smooth, we provide a novel\ncertificate of optimality and we show that the algorithm has a linear\nconvergence rate. Applications of this algorithm are also discussed. \n\n"}
{"id": "1511.01132", "contents": "Title: Liquid Price of Anarchy Abstract: Incorporating budget constraints into the analysis of auctions has become\nincreasingly important, as they model practical settings more accurately. The\nsocial welfare function, which is the standard measure of efficiency in\nauctions, is inadequate for settings with budgets, since there may be a large\ndisconnect between the value a bidder derives from obtaining an item and what\ncan be liquidated from her. The Liquid Welfare objective function has been\nsuggested as a natural alternative for settings with budgets. Simple auctions,\nlike simultaneous item auctions, are evaluated by their performance at\nequilibrium using the Price of Anarchy (PoA) measure -- the ratio of the\nobjective function value of the optimal outcome to the worst equilibrium.\nAccordingly, we evaluate the performance of simultaneous item auctions in\nbudgeted settings by the Liquid Price of Anarchy (LPoA) measure -- the ratio of\nthe optimal Liquid Welfare to the Liquid Welfare obtained in the worst\nequilibrium.\n  Our main result is that the LPoA for mixed Nash equilibria is bounded by a\nconstant when bidders are additive and items can be divided into sufficiently\nmany discrete parts. Our proofs are robust, and can be extended to achieve\nsimilar bounds for simultaneous second price auctions as well as Bayesian Nash\nequilibria. For pure Nash equilibria, we establish tight bounds on the LPoA for\nthe larger class of fractionally-subadditive valuations. To derive our results,\nwe develop a new technique in which some bidders deviate (surprisingly) toward\na non-optimal solution. In particular, this technique does not fit into the\nsmoothness framework. \n\n"}
{"id": "1511.01169", "contents": "Title: adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs Abstract: Recurrent Neural Networks (RNNs) are powerful models that achieve exceptional\nperformance on several pattern recognition problems. However, the training of\nRNNs is a computationally difficult task owing to the well-known\n\"vanishing/exploding\" gradient problem. Algorithms proposed for training RNNs\neither exploit no (or limited) curvature information and have cheap\nper-iteration complexity, or attempt to gain significant curvature information\nat the cost of increased per-iteration cost. The former set includes\ndiagonally-scaled first-order methods such as ADAGRAD and ADAM, while the\nlatter consists of second-order algorithms like Hessian-Free Newton and K-FAC.\nIn this paper, we present adaQN, a stochastic quasi-Newton algorithm for\ntraining RNNs. Our approach retains a low per-iteration cost while allowing for\nnon-diagonal scaling through a stochastic L-BFGS updating scheme. The method\nuses a novel L-BFGS scaling initialization scheme and is judicious in storing\nand retaining L-BFGS curvature pairs. We present numerical experiments on two\nlanguage modeling tasks and show that adaQN is competitive with popular RNN\ntraining algorithms. \n\n"}
{"id": "1511.01411", "contents": "Title: Learning in Auctions: Regret is Hard, Envy is Easy Abstract: A line of recent work provides welfare guarantees of simple combinatorial\nauction formats, such as selling m items via simultaneous second price auctions\n(SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et\nal. 2013). These guarantees hold even when the auctions are repeatedly executed\nand players use no-regret learning algorithms. Unfortunately, off-the-shelf\nno-regret algorithms for these auctions are computationally inefficient as the\nnumber of actions is exponential. We show that this obstacle is insurmountable:\nthere are no polynomial-time no-regret algorithms for SiSPAs, unless\nRP$\\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises\nthe question of how good outcomes polynomially-bounded bidders may discover in\nsuch auctions.\n  To answer this question, we propose a novel concept of learning in auctions,\ntermed \"no-envy learning.\" This notion is founded upon Walrasian equilibrium,\nand we show that it is both efficiently implementable and results in\napproximately optimal welfare, even when the bidders have fractionally\nsubadditive (XOS) valuations (assuming demand oracles) or coverage valuations\n(without demand oracles). No-envy learning outcomes are a relaxation of\nno-regret outcomes, which maintain their approximate welfare optimality while\nendowing them with computational tractability. Our results extend to other\nauction formats that have been studied in the literature via the smoothness\nparadigm.\n  Our results for XOS valuations are enabled by a novel\nFollow-The-Perturbed-Leader algorithm for settings where the number of experts\nis infinite, and the payoff function of the learner is non-linear. This\nalgorithm has applications outside of auction settings, such as in security\ngames. Our result for coverage valuations is based on a novel use of convex\nrounding schemes and a reduction to online convex optimization. \n\n"}
{"id": "1511.06017", "contents": "Title: Rate of Price Discovery in Iterative Combinatorial Auctions Abstract: We study a class of iterative combinatorial auctions which can be viewed as\nsubgradient descent methods for the problem of pricing bundles to balance\nsupply and demand. We provide concrete convergence rates for auctions in this\nclass, bounding the number of auction rounds needed to reach clearing prices.\nOur analysis allows for a variety of pricing schemes, including item, bundle,\nand polynomial pricing, and the respective convergence rates confirm that more\nexpressive pricing schemes come at the cost of slower convergence. We consider\ntwo models of bidder behavior. In the first model, bidders behave\nstochastically according to a random utility model, which includes standard\nbest-response bidding as a special case. In the second model, bidders behave\narbitrarily (even adversarially), and meaningful convergence relies on properly\ndesigned activity rules. \n\n"}
{"id": "1511.08141", "contents": "Title: Reinstating Combinatorial Protections for Manipulation and Bribery in\n  Single-Peaked and Nearly Single-Peaked Electorates Abstract: Understanding when and how computational complexity can be used to protect\nelections against different manipulative actions has been a highly active\nresearch area over the past two decades. A recent body of work, however, has\nshown that many of the NP-hardness shields, previously obtained, vanish when\nthe electorate has single-peaked or nearly single-peaked preferences. In light\nof these results, we investigate whether it is possible to reimpose NP-hardness\nshields for such electorates by allowing the voters to specify partial\npreferences instead of insisting they cast complete ballots. In particular, we\nshow that in single-peaked and nearly single-peaked electorates, if voters are\nallowed to submit top-truncated ballots, then the complexity of manipulation\nand bribery for many voting rules increases from being in P to being\nNP-complete. \n\n"}
{"id": "1511.08591", "contents": "Title: On Game-Theoretic Risk Management (Part Two) -- Algorithms to Compute\n  Nash-Equilibria in Games with Distributions as Payoffs Abstract: The game-theoretic risk management framework put forth in the precursor work\n\"Towards a Theory of Games with Payoffs that are Probability-Distributions\"\n(arXiv:1506.07368 [q-fin.EC]) is herein extended by algorithmic details on how\nto compute equilibria in games where the payoffs are probability distributions.\nOur approach is \"data driven\" in the sense that we assume empirical data\n(measurements, simulation, etc.) to be available that can be compiled into\ndistribution models, which are suitable for efficient decisions about\npreferences, and setting up and solving games using these as payoffs. While\npreferences among distributions turn out to be quite simple if nonparametric\nmethods (kernel density estimates) are used, computing Nash-equilibria in games\nusing such models is discovered as inefficient (if not impossible). In fact, we\ngive a counterexample in which fictitious play fails to converge for the\n(specifically unfortunate) choice of payoff distributions in the game, and\nintroduce a suitable tail approximation of the payoff densities to tackle the\nissue. The overall procedure is essentially a modified version of fictitious\nplay, and is herein described for standard and multicriteria games, to\niteratively deliver an (approximate) Nash-equilibrium. An exact method using\nlinear programming is also given. \n\n"}
{"id": "1512.01274", "contents": "Title: MXNet: A Flexible and Efficient Machine Learning Library for\n  Heterogeneous Distributed Systems Abstract: MXNet is a multi-language machine learning (ML) library to ease the\ndevelopment of ML algorithms, especially for deep neural networks. Embedded in\nthe host language, it blends declarative symbolic expression with imperative\ntensor computation. It offers auto differentiation to derive gradients. MXNet\nis computation and memory efficient and runs on various heterogeneous systems,\nranging from mobile devices to distributed GPU clusters.\n  This paper describes both the API design and the system implementation of\nMXNet, and explains how embedding of both symbolic expression and tensor\noperation is handled in a unified fashion. Our preliminary experiments reveal\npromising results on large scale deep neural network applications using\nmultiple GPU machines. \n\n"}
{"id": "1512.01764", "contents": "Title: Fast Algorithms for Game-Theoretic Centrality Measures Abstract: In this dissertation, we analyze the computational properties of\ngame-theoretic centrality measures. The key idea behind game-theoretic approach\nto network analysis is to treat nodes as players in a cooperative game, where\nthe value of each coalition of nodes is determined by certain graph properties.\nNext, the centrality of any individual node is determined by a chosen\ngame-theoretic solution concept (notably, the Shapley value) in the same way as\nthe payoff of a player in a cooperative game. On one hand, the advantage of\ngame-theoretic centrality measures is that nodes are ranked not only according\nto their individual roles but also according to how they contribute to the role\nplayed by all possible subsets of nodes. On the other hand, the disadvantage is\nthat the game-theoretic solution concepts are typically computationally\nchallenging. The main contribution of this dissertation is that we show that a\nwide variety of game-theoretic solution concepts on networks can be computed in\npolynomial time. Our focus is on centralities based on the Shapley value and\nits various extensions, such as the Semivalues and Coalitional Semivalues.\nFurthermore, we prove #P-hardness of computing the Shapley value in\nconnectivity games and propose an algorithm to compute it. Finally, we analyse\ncomputational properties of generalized version of cooperative games in which\norder of player matters. We propose a new representation for such games, called\ngeneralized marginal contribution networks, that allows for polynomial\ncomputation in the size of the representation of two dedicated extensions of\nthe Shapley value to this class of games. \n\n"}
{"id": "1512.03223", "contents": "Title: Robust Probability Updating Abstract: This paper discusses an alternative to conditioning that may be used when the\nprobability distribution is not fully specified. It does not require any\nassumptions (such as CAR: coarsening at random) on the unknown distribution.\nThe well-known Monty Hall problem is the simplest scenario where neither naive\nconditioning nor the CAR assumption suffice to determine an updated probability\ndistribution. This paper thus addresses a generalization of that problem to\narbitrary distributions on finite outcome spaces, arbitrary sets of `messages',\nand (almost) arbitrary loss functions, and provides existence and\ncharacterization theorems for robust probability updating strategies. We find\nthat for logarithmic loss, optimality is characterized by an elegant condition,\nwhich we call RCAR (reverse coarsening at random). Under certain conditions,\nthe same condition also characterizes optimality for a much larger class of\nloss functions, and we obtain an objective and general answer to how one should\nupdate probabilities in the light of new information. \n\n"}
{"id": "1512.03385", "contents": "Title: Deep Residual Learning for Image Recognition Abstract: Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation. \n\n"}
{"id": "1512.05665", "contents": "Title: Probabilistic Programming with Gaussian Process Memoization Abstract: Gaussian Processes (GPs) are widely used tools in statistics, machine\nlearning, robotics, computer vision, and scientific computation. However,\ndespite their popularity, they can be difficult to apply; all but the simplest\nclassification or regression applications require specification and inference\nover complex covariance functions that do not admit simple analytical\nposteriors. This paper shows how to embed Gaussian processes in any\nhigher-order probabilistic programming language, using an idiom based on\nmemoization, and demonstrates its utility by implementing and extending classic\nand state-of-the-art GP applications. The interface to Gaussian processes,\ncalled gpmem, takes an arbitrary real-valued computational process as input and\nreturns a statistical emulator that automatically improve as the original\nprocess is invoked and its input-output behavior is recorded. The flexibility\nof gpmem is illustrated via three applications: (i) robust GP regression with\nhierarchical hyper-parameter learning, (ii) discovering symbolic expressions\nfrom time-series data by fully Bayesian structure learning over kernels\ngenerated by a stochastic grammar, and (iii) a bandit formulation of Bayesian\noptimization with automatic inference and action selection. All applications\nshare a single 50-line Python library and require fewer than 20 lines of\nprobabilistic code each. \n\n"}
{"id": "1512.05868", "contents": "Title: On Voting and Facility Location Abstract: We study mechanisms for candidate selection that seek to minimize the social\ncost, where voters and candidates are associated with points in some underlying\nmetric space. The social cost of a candidate is the sum of its distances to\neach voter. Some of our work assumes that these points can be modeled on a real\nline, but other results of ours are more general.\n  A question closely related to candidate selection is that of minimizing the\nsum of distances for facility location. The difference is that in our setting\nthere is a fixed set of candidates, whereas the large body of work on facility\nlocation seems to consider every point in the metric space to be a possible\ncandidate. This gives rise to three types of mechanisms which differ in the\ngranularity of their input space (voting, ranking and location mechanisms). We\nstudy the relationships between these three classes of mechanisms.\n  While it may seem that Black's 1948 median algorithm is optimal for candidate\nselection on the line, this is not the case. We give matching upper and lower\nbounds for a variety of settings. In particular, when candidates and voters are\non the line, our universally truthful spike mechanism gives a [tight]\napproximation of two. When assessing candidate selection mechanisms, we seek\nseveral desirable properties: (a) efficiency (minimizing the social cost) (b)\ntruthfulness (dominant strategy incentive compatibility) and (c) simplicity (a\nsmaller input space). We quantify the effect that truthfulness and simplicity\nimpose on the efficiency. \n\n"}
{"id": "1512.05974", "contents": "Title: Improved Balanced Flow Computation Using Parametric Flow Abstract: We present a new algorithm for computing balanced flows in equality networks\narising in market equilibrium computations. The current best time bound for\ncomputing balanced flows in such networks requires $O(n)$ maxflow computations,\nwhere $n$ is the number of nodes in the network [Devanur et al. 2008]. Our\nalgorithm requires only a single parametric flow computation. The best\nalgorithm for computing parametric flows [Gallo et al. 1989] is only by a\nlogarithmic factor slower than the best algorithms for computing maxflows.\nHence, the running time of the algorithms in [Devanur et al. 2008] and [Duan\nand Mehlhorn 2015] for computing market equilibria in linear Fisher and\nArrow-Debreu markets improve by almost a factor of $n$. \n\n"}
{"id": "1601.03095", "contents": "Title: Submodular Optimization under Noise Abstract: We consider the problem of maximizing a monotone submodular function under\nnoise. There has been a great deal of work on optimization of submodular\nfunctions under various constraints, resulting in algorithms that provide\ndesirable approximation guarantees. In many applications, however, we do not\nhave access to the submodular function we aim to optimize, but rather to some\nerroneous or noisy version of it. This raises the question of whether provable\nguarantees are obtainable in presence of error and noise. We provide initial\nanswers, by focusing on the question of maximizing a monotone submodular\nfunction under a cardinality constraint when given access to a noisy oracle of\nthe function. We show that:\n  - For a cardinality constraint $k \\geq 2$, there is an approximation\nalgorithm whose approximation ratio is arbitrarily close to $1-1/e$;\n  - For $k=1$ there is an algorithm whose approximation ratio is arbitrarily\nclose to $1/2$. No randomized algorithm can obtain an approximation ratio\nbetter than $1/2+o(1)$;\n  -If the noise is adversarial, no non-trivial approximation guarantee can be\nobtained. \n\n"}
{"id": "1601.07267", "contents": "Title: Evolutionary stability implies asymptotic stability under multiplicative\n  weights Abstract: We show that evolutionarily stable states in general (nonlinear) population\ngames (which can be viewed as continuous vector fields constrained on a\npolytope) are asymptotically stable under a multiplicative weights dynamic\n(under appropriate choices of a parameter called the learning rate or step\nsize, which we demonstrate to be crucial to achieve convergence, as otherwise\neven chaotic behavior is possible to manifest). Our result implies that\nevolutionary theories based on multiplicative weights are compatible (in\nprinciple, more general) with those based on the notion of evolutionary\nstability. However, our result further establishes multiplicative weights as a\nnonlinear programming primitive (on par with standard nonlinear programming\nmethods) since various nonlinear optimization problems, such as finding\nNash/Wardrop equilibria in nonatomic congestion games, which are well-known to\nbe equipped with a convex potential function, and finding strict local maxima\nof quadratic programming problems, are special cases of the problem of\ncomputing evolutionarily stable states in nonlinear population games. \n\n"}
{"id": "1602.02283", "contents": "Title: Importance Sampling for Minibatches Abstract: Minibatching is a very well studied and highly popular technique in\nsupervised learning, used by practitioners due to its ability to accelerate\ntraining through better utilization of parallel processing power and reduction\nof stochastic variance. Another popular technique is importance sampling -- a\nstrategy for preferential sampling of more important examples also capable of\naccelerating the training process. However, despite considerable effort by the\ncommunity in these areas, and due to the inherent technical difficulty of the\nproblem, there is no existing work combining the power of importance sampling\nwith the strength of minibatching. In this paper we propose the first {\\em\nimportance sampling for minibatches} and give simple and rigorous complexity\nanalysis of its performance. We illustrate on synthetic problems that for\ntraining data of certain properties, our sampling can lead to several orders of\nmagnitude improvement in training time. We then test the new sampling on\nseveral popular datasets, and show that the improvement can reach an order of\nmagnitude. \n\n"}
{"id": "1602.02338", "contents": "Title: Stratified Bayesian Optimization Abstract: We consider derivative-free black-box global optimization of expensive noisy\nfunctions, when most of the randomness in the objective is produced by a few\ninfluential scalar random inputs. We present a new Bayesian global optimization\nalgorithm, called Stratified Bayesian Optimization (SBO), which uses this\nstrong dependence to improve performance. Our algorithm is similar in spirit to\nstratification, a technique from simulation, which uses strong dependence on a\ncategorical representation of the random input to reduce variance. We\ndemonstrate in numerical experiments that SBO outperforms state-of-the-art\nBayesian optimization benchmarks that do not leverage this dependence. \n\n"}
{"id": "1602.05237", "contents": "Title: FPTAS for Mixed-Strategy Nash Equilibria in Tree Graphical Games and\n  Their Generalizations Abstract: We provide the first fully polynomial time approximation scheme (FPTAS) for\ncomputing an approximate mixed-strategy Nash equilibrium in tree-structured\ngraphical multi-hypermatrix games (GMhGs). GMhGs are generalizations of\nnormal-form games, graphical games, graphical polymatrix games, and\nhypergraphical games. Computing an exact mixed-strategy Nash equilibria in\ngraphical polymatrix games is PPAD-complete and thus generally believed to be\nintractable. In contrast, to the best of our knowledge, we are the first to\nestablish an FPTAS for tree polymatrix games as well as tree graphical games\nwhen the number of actions is bounded by a constant. As a corollary, we give a\nquasi-polynomial time approximation scheme (quasi-PTAS) when the number of\nactions is bounded by the logarithm of the number of players. \n\n"}
{"id": "1602.05310", "contents": "Title: Large Scale Kernel Learning using Block Coordinate Descent Abstract: We demonstrate that distributed block coordinate descent can quickly solve\nkernel regression and classification problems with millions of data points.\nArmed with this capability, we conduct a thorough comparison between the full\nkernel, the Nystr\\\"om method, and random features on three large classification\ntasks from various domains. Our results suggest that the Nystr\\\"om method\ngenerally achieves better statistical accuracy than random features, but can\nrequire significantly more iterations of optimization. Lastly, we derive new\nrates for block coordinate descent which support our experimental findings when\nspecialized to kernel methods. \n\n"}
{"id": "1602.06063", "contents": "Title: Pricing and Resource Allocation via Game Theory for a Small-Cell Video\n  Caching System Abstract: Evidence indicates that downloading on-demand videos accounts for a dramatic\nincrease in data traffic over cellular networks. Caching popular videos in the\nstorage of small-cell base stations (SBS), namely, small-cell caching, is an\nefficient technology for reducing the transmission latency whilst mitigating\nthe redundant transmissions of popular videos over back-haul channels. In this\npaper, we consider a commercialized small-cell caching system consisting of a\nnetwork service provider (NSP), several video retailers (VR), and mobile users\n(MU). The NSP leases its SBSs to the VRs for the purpose of making profits, and\nthe VRs, after storing popular videos in the rented SBSs, can provide faster\nlocal video transmissions to the MUs, thereby gaining more profits. We conceive\nthis system within the framework of Stackelberg game by treating the SBSs as a\nspecific type of resources. We first model the MUs and SBSs as two independent\nPoisson point processes, and develop, via stochastic geometry theory, the\nprobability of the specific event that an MU obtains the video of its choice\ndirectly from the memory of an SBS. Then, based on the probability derived, we\nformulate a Stackelberg game to jointly maximize the average profit of both the\nNSP and the VRs. Also, we investigate the Stackelberg equilibrium by solving a\nnon-convex optimization problem. With the aid of this game theoretic framework,\nwe shed light on the relationship between four important factors: the optimal\npricing of leasing an SBS, the SBSs allocation among the VRs, the storage size\nof the SBSs, and the popularity distribution of the VRs. Monte-Carlo\nsimulations show that our stochastic geometry-based analytical results closely\nmatch the empirical ones. Numerical results are also provided for quantifying\nthe proposed game-theoretic framework by showing its efficiency on pricing and\nresource allocation. \n\n"}
{"id": "1602.06940", "contents": "Title: Complexity of Manipulating Sequential Allocation Abstract: Sequential allocation is a simple allocation mechanism in which agents are\ngiven pre-specified turns and each agents gets the most preferred item that is\nstill available. It has long been known that sequential allocation is not\nstrategyproof.\n  Bouveret and Lang (2014) presented a polynomial-time algorithm to compute a\nbest response of an agent with respect to additively separable utilities and\nclaimed that (1) their algorithm correctly finds a best response, and (2) each\nbest response results in the same allocation for the manipulator. We show that\nboth claims are false via an example. We then show that in fact the problem of\ncomputing a best response is NP-complete. On the other hand, the insights and\nresults of Bouveret and Lang (2014) for the case of two agents still hold. \n\n"}
{"id": "1602.07570", "contents": "Title: Bayesian Exploration: Incentivizing Exploration in Bayesian Games Abstract: We consider a ubiquitous scenario in the Internet economy when individual\ndecision-makers (henceforth, agents) both produce and consume information as\nthey make strategic choices in an uncertain environment. This creates a\nthree-way tradeoff between exploration (trying out insufficiently explored\nalternatives to help others in the future), exploitation (making optimal\ndecisions given the information discovered by other agents), and incentives of\nthe agents (who are myopically interested in exploitation, while preferring the\nothers to explore). We posit a principal who controls the flow of information\nfrom agents that came before, and strives to coordinate the agents towards a\nsocially optimal balance between exploration and exploitation, not using any\nmonetary transfers. The goal is to design a recommendation policy for the\nprincipal which respects agents' incentives and minimizes a suitable notion of\nregret.\n  We extend prior work in this direction to allow the agents to interact with\none another in a shared environment: at each time step, multiple agents arrive\nto play a Bayesian game, receive recommendations, choose their actions, receive\ntheir payoffs, and then leave the game forever. The agents now face two sources\nof uncertainty: the actions of the other agents and the parameters of the\nuncertain game environment.\n  Our main contribution is to show that the principal can achieve constant\nregret when the utilities are deterministic (where the constant depends on the\nprior distribution, but not on the time horizon), and logarithmic regret when\nthe utilities are stochastic. As a key technical tool, we introduce the concept\nof explorable actions, the actions which some incentive-compatible policy can\nrecommend with non-zero probability. We show how the principal can identify\n(and explore) all explorable actions, and use the revealed information to\nperform optimally. \n\n"}
{"id": "1602.07844", "contents": "Title: Fast Nonsmooth Regularized Risk Minimization with Continuation Abstract: In regularized risk minimization, the associated optimization problem becomes\nparticularly difficult when both the loss and regularizer are nonsmooth.\nExisting approaches either have slow or unclear convergence properties, are\nrestricted to limited problem subclasses, or require careful setting of a\nsmoothing parameter. In this paper, we propose a continuation algorithm that is\napplicable to a large class of nonsmooth regularized risk minimization\nproblems, can be flexibly used with a number of existing solvers for the\nunderlying smoothed subproblem, and with convergence results on the whole\nalgorithm rather than just one of its subproblems. In particular, when\naccelerated solvers are used, the proposed algorithm achieves the fastest known\nrates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex\nproblems. Experiments on nonsmooth classification and regression tasks\ndemonstrate that the proposed algorithm outperforms the state-of-the-art. \n\n"}
{"id": "1602.08109", "contents": "Title: Recognising Multidimensional Euclidean Preferences Abstract: Euclidean preferences are a widely studied preference model, in which\ndecision makers and alternatives are embedded in d-dimensional Euclidean space.\nDecision makers prefer those alternatives closer to them. This model, also\nknown as multidimensional unfolding, has applications in economics,\npsychometrics, marketing, and many other fields. We study the problem of\ndeciding whether a given preference profile is d-Euclidean. For the\none-dimensional case, polynomial-time algorithms are known. We show that, in\ncontrast, for every other fixed dimension d > 1, the recognition problem is\nequivalent to the existential theory of the reals (ETR), and so in particular\nNP-hard. We further show that some Euclidean preference profiles require\nexponentially many bits in order to specify any Euclidean embedding, and prove\nthat the domain of d-Euclidean preferences does not admit a finite forbidden\nminor characterisation for any d > 1. We also study dichotomous preferencesand\nthe behaviour of other metrics, and survey a variety of related work. \n\n"}
{"id": "1602.08771", "contents": "Title: Investigating practical linear temporal difference learning Abstract: Off-policy reinforcement learning has many applications including: learning\nfrom demonstration, learning multiple goal seeking policies in parallel, and\nrepresenting predictive knowledge. Recently there has been an proliferation of\nnew policy-evaluation algorithms that fill a longstanding algorithmic void in\nreinforcement learning: combining robustness to off-policy sampling, function\napproximation, linear complexity, and temporal difference (TD) updates. This\npaper contains two main contributions. First, we derive two new hybrid TD\npolicy-evaluation algorithms, which fill a gap in this collection of\nalgorithms. Second, we perform an empirical comparison to elicit which of these\nnew linear TD methods should be preferred in different situations, and make\nconcrete suggestions about practical use. \n\n"}
{"id": "1602.09088", "contents": "Title: To Give or Not to Give: Fair Division for Single Minded Valuations Abstract: Single minded agents have strict preferences, in which a bundle is acceptable\nonly if it meets a certain demand. Such preferences arise naturally in\nscenarios such as allocating computational resources among users, where the\ngoal is to fairly serve as many requests as possible. In this paper we study\nthe fair division problem for such agents, which is harder to handle due to\ndiscontinuity and complementarities of the preferences.\n  Our solution concept---the competitive allocation from equal incomes\n(CAEI)---is inspired from market equilibria and implements fair outcomes\nthrough a pricing mechanism. We study the existence and computation of CAEI for\nmultiple divisible goods, cake cutting, and multiple discrete goods. For the\nfirst two scenarios we show that existence of CAEI solutions is guaranteed,\nwhile for the third we give a succinct characterization of instances that admit\nthis solution; then we give an efficient algorithm to find one in all three\ncases. Maximizing social welfare turns out to be NP-hard in general, however we\nobtain efficient algorithms for (i) divisible and discrete goods when the\nnumber of different \\emph{types} of players is a constant, (ii) cake cutting\nwith contiguous demands, for which we establish an interesting connection with\ninterval scheduling, and (iii) cake cutting with a constant number of players\nwith arbitrary demands.\n  Our solution is useful more generally, when the players have a target set of\ndesired goods, and very small positive values for any bundle not containing\ntheir target set. \n\n"}
{"id": "1602.09118", "contents": "Title: Easy Monotonic Policy Iteration Abstract: A key problem in reinforcement learning for control with general function\napproximators (such as deep neural networks and other nonlinear functions) is\nthat, for many algorithms employed in practice, updates to the policy or\n$Q$-function may fail to improve performance---or worse, actually cause the\npolicy performance to degrade. Prior work has addressed this for policy\niteration by deriving tight policy improvement bounds; by optimizing the lower\nbound on policy improvement, a better policy is guaranteed. However, existing\napproaches suffer from bounds that are hard to optimize in practice because\nthey include sup norm terms which cannot be efficiently estimated or\ndifferentiated. In this work, we derive a better policy improvement bound where\nthe sup norm of the policy divergence has been replaced with an average\ndivergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that\ngenerates sequences of policies with guaranteed non-decreasing returns and is\neasy to implement in a sample-based framework. \n\n"}
{"id": "1603.00119", "contents": "Title: From Duels to Battefields: Computing Equilibria of Blotto and Other\n  Games Abstract: We study the problem of computing Nash equilibria of zero-sum games. Many\nnatural zero-sum games have exponentially many strategies, but highly\nstructured payoffs. For example, in the well-studied Colonel Blotto game\n(introduced by Borel in 1921), players must divide a pool of troops among a set\nof battlefields with the goal of winning (i.e., having more troops in) a\nmajority. The Colonel Blotto game is commonly used for analyzing a wide range\nof applications from the U.S presidential election, to innovative technology\ncompetitions, to advertisement, to sports. However, because of the size of the\nstrategy space, standard methods for computing equilibria of zero-sum games\nfail to be computationally feasible. Indeed, despite its importance, only a few\nsolutions for special variants of the problem are known.\n  In this paper we show how to compute equilibria of Colonel Blotto games.\n  Moreover, our approach takes the form of a general reduction: to find a Nash\nequilibrium of a zero-sum game, it suffices to design a separation oracle for\nthe strategy polytope of any bilinear game that is payoff-equivalent. We then\napply this technique to obtain the first polytime algorithms for a variety of\ngames. In addition to Colonel Blotto, we also show how to compute equilibria in\nan infinite-strategy variant called the General Lotto game; this involves\nshowing how to prune the strategy space to a finite subset before applying our\nreduction. We also consider the class of dueling games, first introduced by\nImmorlica et al. (2011). We show that our approach provably extends the class\nof dueling games for which equilibria can be computed: we introduce a new\ndueling game, the matching duel, on which prior methods fail to be\ncomputationally feasible but upon which our reduction can be applied. \n\n"}
{"id": "1603.00749", "contents": "Title: Non-additive Security Games Abstract: We have investigated the security game under non-additive utility functions. \n\n"}
{"id": "1603.01257", "contents": "Title: New Convex Programs for Fisher's Market Model and its Generalizations Abstract: We present the following results pertaining to Fisher's market model:\n  -We give two natural generalizations of Fisher's market model: In model M_1,\nsellers can declare an upper bound on the money they wish to earn (and take\nback their unsold good), and in model M_2, buyers can declare an upper bound on\nthe amount to utility they wish to derive (and take back the unused part of\ntheir money).\n  -We derive convex programs for the linear case of these two models by\ngeneralizing a convex program due to Shmyrev and the Eisenberg-Gale program,\nrespectively.\n  -We generalize the Arrow-Hurwicz theorem to the linear case of these two\nmodels, hence deriving alternate convex programs.\n  -For the special class of convex programs having convex objective functions\nand linear constraints, we derive a simple set of rules for constructing the\ndual program (as simple as obtaining the dual of an LP). Using these rules we\nshow a formal relationship between the two seemingly different convex programs\nfor linear Fisher markets, due to Eisenberg-Gale and Shmyrev; the duals of\nthese are the same, upto a change of variables. \n\n"}
{"id": "1603.01315", "contents": "Title: Ecology-Based DoS Attack in Cognitive Radio Networks Abstract: Cognitive radio technology, which is designed to enhance spectrum\nutilization, depends on the success of opportunistic access, where secondary\nusers (SUs) exploit spectrum void unoccupied by primary users (PUs) for\ntransmissions. We note that the system behaviors are very similar to the\ninteractions among different species coexisting in an ecosystem. However, SUs\nof a selfish nature or of misleading information may make concurrent\ntransmissions with PUs for additional incentives, and thus disrupt the entire\necosystem. By exploiting this vulnerability, this paper proposes a novel\ndistributed denial-of-service (DoS) attack where invasive species, i.e.,\nmalicious users (MUs), induce originally normal-behaved SUs to execute\nconcurrent transmissions with PUs and thus collapse the cognitive radio\nnetwork. We adopt stochastic geometry to model the spatial distributions of\nPUs, SUs, and MUs for the analysis of the mutual interference among them. The\naccess strategy of each SU in the spectrum sharing ecosystem, which evolves\nwith the experienced payoffs and interference, is modeled by an evolutionary\ngame. Based on the evolutionary stable strategy concept, we could efficiently\nidentify the fragile operating region at which normal-behaved SUs are\neventually evolved to conduct concurrent transmissions and thus to cause the\nruin of the network. \n\n"}
{"id": "1603.01318", "contents": "Title: Efficiently characterizing games consistent with perturbed equilibrium\n  observations Abstract: We study the problem of characterizing the set of games that are consistent\nwith observed equilibrium play. Our contribution is to develop and analyze a\nnew methodology based on convex optimization to address this problem for many\nclasses of games and observation models of interest. Our approach provides a\nsharp, computationally efficient characterization of the extent to which a\nparticular set of observations constrains the space of games that could have\ngenerated them. This allows us to solve a number of variants of this problem as\nwell as to quantify the power of games from particular classes (e.g., zero-sum,\npotential, linearly parameterized) to explain player behavior. We illustrate\nour approach with numerical simulations. \n\n"}
{"id": "1603.02074", "contents": "Title: Optimal dictionary for least squares representation Abstract: Dictionaries are collections of vectors used for representations of random\nvectors in Euclidean spaces. Recent research on optimal dictionaries is focused\non constructing dictionaries that offer sparse representations, i.e.,\n$\\ell_0$-optimal representations. Here we consider the problem of finding\noptimal dictionaries with which representations of samples of a random vector\nare optimal in an $\\ell_2$-sense: optimality of representation is defined as\nattaining the minimal average $\\ell_2$-norm of the coefficients used to\nrepresent the random vector. With the help of recent results on rank-$1$\ndecompositions of symmetric positive semidefinite matrices, we provide an\nexplicit description of $\\ell_2$-optimal dictionaries as well as their\nalgorithmic constructions in polynomial time. \n\n"}
{"id": "1603.03236", "contents": "Title: Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic\n  Differentiation Abstract: Optimization on manifolds is a class of methods for optimization of an\nobjective function, subject to constraints which are smooth, in the sense that\nthe set of points which satisfy the constraints admits the structure of a\ndifferentiable manifold. While many optimization problems are of the described\nform, technicalities of differential geometry and the laborious calculation of\nderivatives pose a significant barrier for experimenting with these methods.\n  We introduce Pymanopt (available at https://pymanopt.github.io), a toolbox\nfor optimization on manifolds, implemented in Python, that---similarly to the\nManopt Matlab toolbox---implements several manifold geometries and optimization\nalgorithms. Moreover, we lower the barriers to users further by using automated\ndifferentiation for calculating derivative information, saving users time and\nsaving them from potential calculation and implementation errors. \n\n"}
{"id": "1603.03682", "contents": "Title: Ultra Dense Small Cell Networks: Turning Density into Energy Efficiency Abstract: In this paper, a novel approach for joint power control and user scheduling\nis proposed for optimizing energy efficiency (EE), in terms of bits per unit\nenergy, in ultra dense small cell networks (UDNs). Due to severe coupling in\ninterference, this problem is formulated as a dynamic stochastic game (DSG)\nbetween small cell base stations (SBSs). This game enables to capture the\ndynamics of both the queues and channel states of the system. To solve this\ngame, assuming a large homogeneous UDN deployment, the problem is cast as a\nmean-field game (MFG) in which the MFG equilibrium is analyzed with the aid of\nlow-complexity tractable partial differential equations. Exploiting the\nstochastic nature of the problem, user scheduling is formulated as a stochastic\noptimization problem and solved using the drift plus penalty (DPP) approach in\nthe framework of Lyapunov optimization. Remarkably, it is shown that by weaving\nnotions from Lyapunov optimization and mean-field theory, the proposed solution\nyields an equilibrium control policy per SBS which maximizes the network\nutility while ensuring users' quality-of-service. Simulation results show that\nthe proposed approach achieves up to 70.7% gains in EE and 99.5% reductions in\nthe network's outage probabilities compared to a baseline model which focuses\non improving EE while attempting to satisfy the users' instantaneous\nquality-of-service requirements. \n\n"}
{"id": "1603.04319", "contents": "Title: Learning Network of Multivariate Hawkes Processes: A Time Series\n  Approach Abstract: Learning the influence structure of multiple time series data is of great\ninterest to many disciplines. This paper studies the problem of recovering the\ncausal structure in network of multivariate linear Hawkes processes. In such\nprocesses, the occurrence of an event in one process affects the probability of\noccurrence of new events in some other processes. Thus, a natural notion of\ncausality exists between such processes captured by the support of the\nexcitation matrix. We show that the resulting causal influence network is\nequivalent to the Directed Information graph (DIG) of the processes, which\nencodes the causal factorization of the joint distribution of the processes.\nFurthermore, we present an algorithm for learning the support of excitation\nmatrix (or equivalently the DIG). The performance of the algorithm is evaluated\non synthesized multivariate Hawkes networks as well as a stock market and\nMemeTracker real-world dataset. \n\n"}
{"id": "1603.04641", "contents": "Title: Compositional game theory Abstract: We introduce open games as a compositional foundation of economic game\ntheory. A compositional approach potentially allows methods of game theory and\ntheoretical computer science to be applied to large-scale economic models for\nwhich standard economic tools are not practical. An open game represents a game\nplayed relative to an arbitrary environment and to this end we introduce the\nconcept of coutility, which is the utility generated by an open game and\nreturned to its environment. Open games are the morphisms of a symmetric\nmonoidal category and can therefore be composed by categorical composition into\nsequential move games and by monoidal products into simultaneous move games.\nOpen games can be represented by string diagrams which provide an intuitive but\nformal visualisation of the information flows. We show that a variety of games\ncan be faithfully represented as open games in the sense of having the same\nNash equilibria and off-equilibrium best responses. \n\n"}
{"id": "1603.06422", "contents": "Title: Parity Game Reductions Abstract: Parity games play a central role in model checking and satisfiability\nchecking. Solving parity games is computationally expensive, among others due\nto the size of the games, which, for model checking problems, can easily\ncontain $10^9$ vertices or beyond. Equivalence relations can be used to reduce\nthe size of a parity game, thereby potentially alleviating part of the\ncomputational burden. We reconsider (governed) bisimulation and (governed)\nstuttering bisimulation, and we give detailed proofs that these relations are\nequivalences, have unique quotients and they approximate the winning regions of\nparity games. Furthermore, we present game-based characterisations of these\nrelations. Using these characterisations our equivalences are compared to\nrelations for parity games that can be found in the literature, such as direct\nsimulation equivalence and delayed simulation equivalence. To complete the\noverview we develop coinductive characterisations of direct- and delayed\nsimulation equivalence and we establish a lattice of equivalences for parity\ngames. \n\n"}
{"id": "1603.07210", "contents": "Title: Computing Equilibria in Markets with Budget-Additive Utilities Abstract: We present the first analysis of Fisher markets with buyers that have\nbudget-additive utility functions. Budget-additive utilities are elementary\nconcave functions with numerous applications in online adword markets and\nrevenue optimization problems. They extend the standard case of linear\nutilities and have been studied in a variety of other market models. In\ncontrast to the frequently studied CES utilities, they have a global satiation\npoint which can imply multiple market equilibria with quite different\ncharacteristics. Our main result is an efficient combinatorial algorithm to\ncompute a market equilibrium with a Pareto-optimal allocation of goods. It\nrelies on a new descending-price approach and, as a special case, also implies\na novel combinatorial algorithm for computing a market equilibrium in linear\nFisher markets. We complement these positive results with a number of hardness\nresults for related computational questions. We prove that it is NP-hard to\ncompute a market equilibrium that maximizes social welfare, and it is PPAD-hard\nto find any market equilibrium with utility functions with separate satiation\npoints for each buyer and each good. \n\n"}
{"id": "1603.09620", "contents": "Title: Online Optimization with Costly and Noisy Measurements using Random\n  Fourier Expansions Abstract: This paper analyzes DONE, an online optimization algorithm that iteratively\nminimizes an unknown function based on costly and noisy measurements. The\nalgorithm maintains a surrogate of the unknown function in the form of a random\nFourier expansion (RFE). The surrogate is updated whenever a new measurement is\navailable, and then used to determine the next measurement point. The algorithm\nis comparable to Bayesian optimization algorithms, but its computational\ncomplexity per iteration does not depend on the number of measurements. We\nderive several theoretical results that provide insight on how the\nhyper-parameters of the algorithm should be chosen. The algorithm is compared\nto a Bayesian optimization algorithm for a benchmark problem and three\napplications, namely, optical coherence tomography, optical beam-forming\nnetwork tuning, and robot arm control. It is found that the DONE algorithm is\nsignificantly faster than Bayesian optimization in the discussed problems,\nwhile achieving a similar or better performance. \n\n"}
{"id": "1604.00299", "contents": "Title: Stochastic Control Approach to Reputation Games Abstract: Through a stochastic control theoretic approach, we analyze reputation games\nwhere a strategic long-lived player acts in a sequential repeated game against\na collection of short-lived players. The key assumption in our model is that\nthe information of the short-lived players is nested in that of the long-lived\nplayer. This nested information structure is obtained through an appropriate\nmonitoring structure. Under this monitoring structure, we show that, given mild\nassumptions, the set of Perfect Bayesian Equilibrium payoffs coincide with\nMarkov Perfect Equilibrium payoffs, and hence a dynamic programming formulation\ncan be obtained for the computation of equilibrium strategies of the strategic\nlong-lived player in the discounted setup. We also consider the undiscounted\naverage-payoff setup where we obtain an optimal equilibrium strategy of the\nstrategic long-lived player under further technical conditions. We then use\nthis optimal strategy in the undiscounted setup as a tool to obtain a tight\nupper payoff bound for the arbitrarily patient long-lived player in the\ndiscounted setup. Finally, by using measure concentration techniques, we obtain\na refined lower payoff bound on the value of reputation in the discounted\nsetup. We also study the continuity of equilibrium payoffs in the prior\nbeliefs. \n\n"}
{"id": "1604.00981", "contents": "Title: Revisiting Distributed Synchronous SGD Abstract: Distributed training of deep learning models on large-scale training data is\ntypically conducted with asynchronous stochastic optimization to maximize the\nrate of updates, at the cost of additional noise introduced from asynchrony. In\ncontrast, the synchronous approach is often thought to be impractical due to\nidle time wasted on waiting for straggling workers. We revisit these\nconventional beliefs in this paper, and examine the weaknesses of both\napproaches. We demonstrate that a third approach, synchronous optimization with\nbackup workers, can avoid asynchronous noise while mitigating for the worst\nstragglers. Our approach is empirically validated and shown to converge faster\nand to better test accuracies. \n\n"}
{"id": "1604.01952", "contents": "Title: Deep Online Convex Optimization with Gated Games Abstract: Methods from convex optimization are widely used as building blocks for deep\nlearning algorithms. However, the reasons for their empirical success are\nunclear, since modern convolutional networks (convnets), incorporating\nrectifier units and max-pooling, are neither smooth nor convex. Standard\nguarantees therefore do not apply. This paper provides the first convergence\nrates for gradient descent on rectifier convnets. The proof utilizes the\nparticular structure of rectifier networks which consists in binary\nactive/inactive gates applied on top of an underlying linear network. The\napproach generalizes to max-pooling, dropout and maxout. In other words, to\nprecisely the neural networks that perform best empirically. The key step is to\nintroduce gated games, an extension of convex games with similar convergence\nproperties that capture the gating function of rectifiers. The main result is\nthat rectifier convnets converge to a critical point at a rate controlled by\nthe gated-regret of the units in the network. Corollaries of the main result\ninclude: (i) a game-theoretic description of the representations learned by a\nneural network; (ii) a logarithmic-regret algorithm for training neural nets;\nand (iii) a formal setting for analyzing conditional computation in neural nets\nthat can be applied to recently developed models of attention. \n\n"}
{"id": "1604.02606", "contents": "Title: A General Retraining Framework for Scalable Adversarial Classification Abstract: Traditional classification algorithms assume that training and test data come\nfrom similar distributions. This assumption is violated in adversarial\nsettings, where malicious actors modify instances to evade detection. A number\nof custom methods have been developed for both adversarial evasion attacks and\nrobust learning. We propose the first systematic and general-purpose retraining\nframework which can: a) boost robustness of an \\emph{arbitrary} learning\nalgorithm, in the face of b) a broader class of adversarial models than any\nprior methods. We show that, under natural conditions, the retraining framework\nminimizes an upper bound on optimal adversarial risk, and show how to extend\nthis result to account for approximations of evasion attacks. Extensive\nexperimental evaluation demonstrates that our retraining methods are nearly\nindistinguishable from state-of-the-art algorithms for optimizing adversarial\nrisk, but are more general and far more scalable. The experiments also confirm\nthat without retraining, our adversarial framework dramatically reduces the\neffectiveness of learning. In contrast, retraining significantly boosts\nrobustness to evasion attacks without significantly compromising overall\naccuracy. \n\n"}
{"id": "1604.02737", "contents": "Title: Correlated Equilibria for Approximate Variational Inference in MRFs Abstract: Almost all of the work in graphical models for game theory has mirrored\nprevious work in probabilistic graphical models. Our work considers the\nopposite direction: Taking advantage of recent advances in equilibrium\ncomputation for probabilistic inference. We present formulations of inference\nproblems in Markov random fields (MRFs) as computation of equilibria in a\ncertain class of game-theoretic graphical models. We concretely establishes the\nprecise connection between variational probabilistic inference in MRFs and\ncorrelated equilibria. No previous work exploits recent theoretical and\nempirical results from the literature on algorithmic and computational game\ntheory on the tractable, polynomial-time computation of exact or approximate\ncorrelated equilibria in graphical games with arbitrary, loopy graph structure.\nWe discuss how to design new algorithms with equally tractable guarantees for\nthe computation of approximate variational inference in MRFs. Also, inspired by\na previously stated game-theoretic view of state-of-the-art tree-reweighed\n(TRW) message-passing techniques for belief inference as zero-sum game, we\npropose a different, general-sum potential game to design approximate\nfictitious-play techniques. We perform synthetic experiments evaluating our\nproposed approximation algorithms with standard methods and TRW on several\nclasses of classical Ising models (i.e., with binary random variables). We also\nevaluate the algorithms using Ising models learned from the MNIST dataset. Our\nexperiments show that our global approach is competitive, particularly shinning\nin a class of Ising models with constant, \"highly attractive\" edge-weights, in\nwhich it is often better than all other alternatives we evaluated. With a\nnotable exception, our more local approach was not as effective. Yet, in\nfairness, almost all of the alternatives are often no better than a simple\nbaseline: estimate 0.5. \n\n"}
{"id": "1604.03178", "contents": "Title: Incentives for Truthful Peer Grading Abstract: Peer grading systems work well only if users have incentives to grade\ntruthfully. An example of non-truthful grading, that we observed in classrooms,\nconsists in students assigning the maximum grade to all submissions. With a\nnaive grading scheme, such as averaging the assigned grades, all students would\nreceive the maximum grade. In this paper, we develop three grading schemes that\nprovide incentives for truthful peer grading. In the first scheme, the\ninstructor grades a fraction p of the submissions, and penalizes students whose\ngrade deviates from the instructor grade. We provide lower bounds on p to\nensure truthfulness, and conclude that these schemes work only for moderate\nclass sizes, up to a few hundred students. To overcome this limitation, we\npropose a hierarchical extension of this supervised scheme, and we show that it\ncan handle classes of any size with bounded (and little) instructor work, and\nis therefore applicable to Massive Open Online Courses (MOOCs). Finally, we\npropose unsupervised incentive schemes, in which the student incentive is based\non statistical properties of the grade distribution, without any grading\nrequired by the instructor. We show that the proposed unsupervised schemes\nprovide incentives to truthful grading, at the price of being possibly unfair\nto individual students. \n\n"}
{"id": "1604.03632", "contents": "Title: Strategyproof Peer Selection using Randomization, Partitioning, and\n  Apportionment Abstract: Peer reviews, evaluations, and selections are a fundamental aspect of modern\nscience. Funding bodies the world over employ experts to review and select the\nbest proposals from those submitted for funding. The problem of peer selection,\nhowever, is much more general: a professional society may want to give a subset\nof its members awards based on the opinions of all members; an instructor for a\nMassive Open Online Course (MOOC) or an online course may want to crowdsource\ngrading; or a marketing company may select ideas from group brainstorming\nsessions based on peer evaluation.\n  We make three fundamental contributions to the study of peer selection, a\nspecific type of group decision-making problem, studied in computer science,\neconomics, and political science. First, we propose a novel mechanism that is\nstrategyproof, i.e., agents cannot benefit by reporting insincere valuations.\nSecond, we demonstrate the effectiveness of our mechanism by a comprehensive\nsimulation-based comparison with a suite of mechanisms found in the literature.\nFinally, our mechanism employs a randomized rounding technique that is of\nindependent interest, as it solves the apportionment problem that arises in\nvarious settings where discrete resources such as parliamentary representation\nslots need to be divided proportionally. \n\n"}
{"id": "1604.03655", "contents": "Title: A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of\n  Agents Abstract: We consider the well-studied cake cutting problem in which the goal is to\nfind an envy-free allocation based on queries from $n$ agents. The problem has\nreceived attention in computer science, mathematics, and economics. It has been\na major open problem whether there exists a discrete and bounded envy-free\nprotocol. We resolve the problem by proposing a discrete and bounded envy-free\nprotocol for any number of agents. The maximum number of queries required by\nthe protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do\nnot run our protocol to completion, it can find in at most $n^3{(n^2)}^n$\nqueries a partial allocation of the cake that achieves proportionality (each\nagent gets at least $1/n$ of the value of the whole cake) and envy-freeness.\nFinally we show that an envy-free partial allocation can be computed in at most\n$n^3{(n^2)}^n$ queries such that each agent gets a connected piece that gives\nthe agent at least $1/(3n)$ of the value of the whole cake. \n\n"}
{"id": "1604.03930", "contents": "Title: Efficient Algorithms for Large-scale Generalized Eigenvector Computation\n  and Canonical Correlation Analysis Abstract: This paper considers the problem of canonical-correlation analysis (CCA)\n(Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a\npair of symmetric matrices. These are two fundamental problems in data analysis\nand scientific computing with numerous applications in machine learning and\nstatistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).\n  We provide simple iterative algorithms, with improved runtimes, for solving\nthese problems that are globally linearly convergent with moderate dependencies\non the condition numbers and eigenvalue gaps of the matrices involved.\n  We obtain our results by reducing CCA to the top-$k$ generalized eigenvector\nproblem. We solve this problem through a general framework that simply requires\nblack box access to an approximate linear system solver. Instantiating this\nframework with accelerated gradient descent we obtain a running time of\n$O(\\frac{z k \\sqrt{\\kappa}}{\\rho} \\log(1/\\epsilon) \\log\n\\left(k\\kappa/\\rho\\right))$ where $z$ is the total number of nonzero entries,\n$\\kappa$ is the condition number and $\\rho$ is the relative eigenvalue gap of\nthe appropriate matrices.\n  Our algorithm is linear in the input size and the number of components $k$ up\nto a $\\log(k)$ factor. This is essential for handling large-scale matrices that\nappear in practice. To the best of our knowledge this is the first such\nalgorithm with global linear convergence. We hope that our results prompt\nfurther research and ultimately improve the practical running time for\nperforming these important data analysis procedures on large data sets. \n\n"}
{"id": "1604.05972", "contents": "Title: Optimal online escape path against a certificate Abstract: More than fifty years ago, Bellman asked for the best escape path within a\nknown forest but for an unknown starting position. This deterministic finite\npath is the shortest path that leads out of a given environment from any\nstarting point. There are some worst case positions where the full path length\nis required. Up to now such a fixed ultimate optimal escape path for a known\nshape for any starting position is only known for some special convex shapes\n(i.e., circles, strips of a given width, fat convex bodies, some isosceles\ntriangles). Therefore, we introduce a different, simple and intuitive escape\npath, the so-called certificate path. This escape path depends on the starting\nposition s and takes the distances from s to the outer boundary of the\nenvironment into account. Due to the additional information, the certificate\npath always (for any position s) leaves the environment earlier than the\nultimate escape path, in the above convex examples. Next we assume that fewer\ninformation is available. Neither the precise shape of the envir- onment, nor\nthe location of the starting point is known. For a class of environments\n(convex shapes and shapes with kernel positions), we design an online strategy\nthat always leaves the environment. We show that the path length for leaving\nthe environment is always shorter than 3.318764 the length of the corresponding\ncertificate path. We also give a lower bound of 3.313126, which shows that for\nthe above class of environments the factor 3.318764 is (almost) tight. \n\n"}
{"id": "1604.06210", "contents": "Title: Double Auctions in Markets for Multiple Kinds of Goods Abstract: Motivated by applications such as stock exchanges and spectrum auctions,\nthere is a growing interest in mechanisms for arranging trade in two-sided\nmarkets. Existing mechanisms are either not truthful, or do not guarantee an\nasymptotically-optimal gain-from-trade, or rely on a prior on the traders'\nvaluations, or operate in limited settings such as a single kind of good. We\nextend the random market-halving technique used in earlier works to markets\nwith multiple kinds of goods, where traders have gross-substitute valuations.\nWe present MIDA: a Multi Item-kind Double-Auction mechanism. It is prior-free,\ntruthful, strongly-budget-balanced, and guarantees near-optimal gain from trade\nwhen market sizes of all goods grow to $\\infty$ at a similar rate. \n\n"}
{"id": "1604.07070", "contents": "Title: Stochastic Variance-Reduced ADMM Abstract: The alternating direction method of multipliers (ADMM) is a powerful\noptimization solver in machine learning. Recently, stochastic ADMM has been\nintegrated with variance reduction methods for stochastic gradient, leading to\nSAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration\ncomplexities. However, their space requirements can still be high. In this\npaper, we propose an integration of ADMM with the method of stochastic variance\nreduced gradient (SVRG). Unlike another recent integration attempt called\nSCAS-ADMM, the proposed algorithm retains the fast convergence benefits of\nSAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage\nrequirement is very low, even independent of the sample size $n$. We also\nextend the proposed method for nonconvex problems, and obtain a convergence\nrate of $O(1/T)$. Experimental results demonstrate that it is as fast as\nSAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much\nbigger data sets. \n\n"}
{"id": "1604.08179", "contents": "Title: Strategic Formation of Heterogeneous Networks Abstract: We establish a network formation game for the Internet's Autonomous System\n(AS) interconnection topology. The game includes different types of players,\naccounting for the heterogeneity of ASs in the Internet. In this network\nformation game, the utility of a player depends on the network structure, e.g.,\nthe distances between nodes and the cost of links. We also consider the case\nwhere utility (or monetary) transfers are allowed between the players. We\nincorporate reliability considerations in the player's utility function, and\nanalyze static properties of the game as well as its dynamic evolution. We\nprovide dynamic analysis of topological quantities, and explain the prevalence\nof some \"network motifs\" in the Internet graph. We assess our predictions with\nreal-world data. \n\n"}
{"id": "1604.08191", "contents": "Title: Modeling Single-Peakedness for Votes with Ties Abstract: Single-peakedness is one of the most important and well-known domain\nrestrictions on preferences. The computational study of single-peaked\nelectorates has largely been restricted to elections with tie-free votes, and\nrecent work that studies the computational complexity of manipulative attacks\nfor single-peaked elections for votes with ties has been restricted to\nnonstandard models of single-peaked preferences for top orders. We study the\ncomputational complexity of manipulation for votes with ties for the standard\nmodel of single-peaked preferences and for single-plateaued preferences. We\nshow that these models avoid the anomalous complexity behavior exhibited by the\nother models. We also state a surprising result on the relation between the\nsocietal axis and the complexity of manipulation for single-peaked preferences. \n\n"}
{"id": "1605.02330", "contents": "Title: Robust Energy Harvesting Based on a Stackelberg Game Abstract: We study a Stackelberg game between a base station and a multi-antenna power\nbeacon for wireless energy harvesting in a multiple sensor node scenario.\nAssuming imperfect CSI between the sensor nodes and the power beacon, we\npropose a utility function that is based on throughput non-outage probability\nat the base station. We provide an analytical solution for the equilibrium in\ncase of a single sensor node. For the general case consisting of multiple\nsensor nodes, we provide upper and lower bounds on the power and price\n(players' strategies). We compare the bounds with solutions resulting from an\nexhaustive search and a relaxed semidefinite program, and find the upper bound\nto be tight. \n\n"}
{"id": "1605.02402", "contents": "Title: Game-theoretic Demand-side Management Robust to Non-Ideal Consumer\n  Behavior in Smart Grid Abstract: This paper investigates effects of realistic, non-ideal, decisions of energy\nusers as to whether to participate in an energy trading system proposed for\ndemand-side management of a residential community. The energy trading system\nadopts a non-cooperative Stackelberg game between a community energy storage\n(CES) device and users with rooftop photovoltaic panels where the CES operator\nis the leader and the users are the followers. Participating users determine\ntheir optimal energy trading starting time to minimize their personal daily\nenergy costs while subjectively viewing their opponents' actions. Following a\nnon-cooperative game, we study the subjective behavior of users when they\ndecide on energy trading starting time using prospect theory. We show that\ndepending on the decisions of participating-time, the proposed energy trading\nsystem has a unique Stackelberg equilibrium at which the CES operator maximizes\ntheir revenue while users minimize their personal energy costs attaining a Nash\nequilibrium. Simulation results confirm that the benefits of the energy trading\nsystem are robust to decisions of participating-time that significantly deviate\nfrom complete rationality. \n\n"}
{"id": "1605.02711", "contents": "Title: Nonconvex Sparse Learning via Stochastic Optimization with Progressive\n  Variance Reduction Abstract: We propose a stochastic variance reduced optimization algorithm for solving\nsparse learning problems with cardinality constraints. Sufficient conditions\nare provided, under which the proposed algorithm enjoys strong linear\nconvergence guarantees and optimal estimation accuracy in high dimensions. We\nfurther extend the proposed algorithm to an asynchronous parallel variant with\na near linear speedup. Numerical experiments demonstrate the efficiency of our\nalgorithm in terms of both parameter estimation and computational performance. \n\n"}
{"id": "1605.03468", "contents": "Title: A constrained L1 minimization approach for estimating multiple Sparse\n  Gaussian or Nonparanormal Graphical Models Abstract: Identifying context-specific entity networks from aggregated data is an\nimportant task, arising often in bioinformatics and neuroimaging.\nComputationally, this task can be formulated as jointly estimating multiple\ndifferent, but related, sparse Undirected Graphical Models (UGM) from\naggregated samples across several contexts. Previous joint-UGM studies have\nmostly focused on sparse Gaussian Graphical Models (sGGMs) and can't identify\ncontext-specific edge patterns directly. We, therefore, propose a novel\napproach, SIMULE (detecting Shared and Individual parts of MULtiple graphs\nExplicitly) to learn multi-UGM via a constrained L1 minimization. SIMULE\nautomatically infers both specific edge patterns that are unique to each\ncontext and shared interactions preserved among all the contexts. Through the\nL1 constrained formulation, this problem is cast as multiple independent\nsubtasks of linear programming that can be solved efficiently in parallel. In\naddition to Gaussian data, SIMULE can also handle multivariate Nonparanormal\ndata that greatly relaxes the normality assumption that many real-world\napplications do not follow. We provide a novel theoretical proof showing that\nSIMULE achieves a consistent result at the rate O(log(Kp)/n_{tot}). On multiple\nsynthetic datasets and two biomedical datasets, SIMULE shows significant\nimprovement over state-of-the-art multi-sGGM and single-UGM baselines. \n\n"}
{"id": "1605.03838", "contents": "Title: An Experimental Evaluation of Regret-Based Econometrics Abstract: Using data obtained in a controlled ad-auction experiment that we ran, we\nevaluate the regret-based approach to econometrics that was recently suggested\nby Nekipelov, Syrgkanis, and Tardos (EC 2015). We found that despite the weak\nregret-based assumptions, the results were (at least) as accurate as those\nobtained using classic equilibrium-based assumptions. En route we studied to\nwhat extent humans actually minimize regret in our ad auction, and found a\nsignificant difference between the \"high types\" (players with a high valuation)\nwho indeed rationally minimized regret and the \"low types\" who significantly\noverbid. We suggest that correcting for these biases and adjusting the\nregret-based econometric method may improve the accuracy of estimated values. \n\n"}
{"id": "1605.04638", "contents": "Title: Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online\n  Learning with True and Noisy Gradient Abstract: This work focuses on dynamic regret of online convex optimization that\ncompares the performance of online learning to a clairvoyant who knows the\nsequence of loss functions in advance and hence selects the minimizer of the\nloss function at each step. By assuming that the clairvoyant moves slowly\n(i.e., the minimizers change slowly), we present several improved\nvariation-based upper bounds of the dynamic regret under the true and noisy\ngradient feedback, which are {\\it optimal} in light of the presented lower\nbounds. The key to our analysis is to explore a regularity metric that measures\nthe temporal changes in the clairvoyant's minimizers, to which we refer as {\\it\npath variation}. Firstly, we present a general lower bound in terms of the path\nvariation, and then show that under full information or gradient feedback we\nare able to achieve an optimal dynamic regret. Secondly, we present a lower\nbound with noisy gradient feedback and then show that we can achieve optimal\ndynamic regrets under a stochastic gradient feedback and two-point bandit\nfeedback. Moreover, for a sequence of smooth loss functions that admit a small\nvariation in the gradients, our dynamic regret under the two-point bandit\nfeedback matches what is achieved with full information. \n\n"}
{"id": "1605.05114", "contents": "Title: Growth of Dimension in Complete Simple Games Abstract: The concept of dimension in simple games was introduced as a measure of the\nremoteness of a given game from a weighted game. Taylor and Zwicker (1993)\ndemonstrated that the dimension of a simple game can grow exponentially in the\nnumber of players. However, the problem of worst-case growth of the dimension\nin complete games was left open. Freixas and Puente (2008) showed that complete\ngames of arbitrary dimension exist and, in particular, their examples\ndemonstrate that the worst-case growth of dimension in complete games is at\nleast linear. In this paper, using a novel technique of Kurz and Napel (2016),\nwe demonstrate that the worst-case growth of dimension in complete simple games\nis exponential in the number of players. \n\n"}
{"id": "1605.05273", "contents": "Title: Learning Convolutional Neural Networks for Graphs Abstract: Numerous important problems can be framed as learning from graph data. We\npropose a framework for learning convolutional neural networks for arbitrary\ngraphs. These graphs may be undirected, directed, and with both discrete and\ncontinuous node and edge attributes. Analogous to image-based convolutional\nnetworks that operate on locally connected regions of the input, we present a\ngeneral approach to extracting locally connected regions from graphs. Using\nestablished benchmark data sets, we demonstrate that the learned feature\nrepresentations are competitive with state of the art graph kernels and that\ntheir computation is highly efficient. \n\n"}
{"id": "1605.06181", "contents": "Title: Variational hybridization and transformation for large inaccurate\n  noisy-or networks Abstract: Variational inference provides approximations to the computationally\nintractable posterior distribution in Bayesian networks. A prominent medical\napplication of noisy-or Bayesian network is to infer potential diseases given\nobserved symptoms. Previous studies focus on approximating a handful of\ncomplicated pathological cases using variational transformation. Our goal is to\nuse variational transformation as part of a novel hybridized inference for\nserving reliable and real time diagnosis at web scale. We propose a hybridized\ninference that allows variational parameters to be estimated without disease\nposteriors or priors, making the inference faster and much of its computation\nrecyclable. In addition, we propose a transformation ranking algorithm that is\nvery stable to large variances in network prior probabilities, a common issue\nthat arises in medical applications of Bayesian networks. In experiments, we\nperform comparative study on a large real life medical network and scalability\nstudy on a much larger (36,000x) synthesized network. \n\n"}
{"id": "1605.07950", "contents": "Title: On Fast Convergence of Proximal Algorithms for SQRT-Lasso Optimization:\n  Don't Worry About Its Nonsmooth Loss Function Abstract: Many machine learning techniques sacrifice convenient computational\nstructures to gain estimation robustness and modeling flexibility. However, by\nexploring the modeling structures, we find these \"sacrifices\" do not always\nrequire more computational efforts. To shed light on such a \"free-lunch\"\nphenomenon, we study the square-root-Lasso (SQRT-Lasso) type regression\nproblem. Specifically, we show that the nonsmooth loss functions of SQRT-Lasso\ntype regression ease tuning effort and gain adaptivity to inhomogeneous noise,\nbut is not necessarily more challenging than Lasso in computation. We can\ndirectly apply proximal algorithms (e.g. proximal gradient descent, proximal\nNewton, and proximal Quasi-Newton algorithms) without worrying the\nnonsmoothness of the loss function. Theoretically, we prove that the proximal\nalgorithms combined with the pathwise optimization scheme enjoy fast\nconvergence guarantees with high probability. Numerical results are provided to\nsupport our theory. \n\n"}
{"id": "1605.08062", "contents": "Title: A PAC RL Algorithm for Episodic POMDPs Abstract: Many interesting real world domains involve reinforcement learning (RL) in\npartially observable environments. Efficient learning in such domains is\nimportant, but existing sample complexity bounds for partially observable RL\nare at least exponential in the episode length. We give, to our knowledge, the\nfirst partially observable RL algorithm with a polynomial bound on the number\nof episodes on which the algorithm may not achieve near-optimal performance.\nOur algorithm is suitable for an important class of episodic POMDPs. Our\napproach builds on recent advances in method of moments for latent variable\nmodel estimation. \n\n"}
{"id": "1605.08370", "contents": "Title: Provable Efficient Online Matrix Completion via Non-convex Stochastic\n  Gradient Descent Abstract: Matrix completion, where we wish to recover a low rank matrix by observing a\nfew entries from it, is a widely studied problem in both theory and practice\nwith wide applications. Most of the provable algorithms so far on this problem\nhave been restricted to the offline setting where they provide an estimate of\nthe unknown matrix using all observations simultaneously. However, in many\napplications, the online version, where we observe one entry at a time and\ndynamically update our estimate, is more appealing. While existing algorithms\nare efficient for the offline setting, they could be highly inefficient for the\nonline setting.\n  In this paper, we propose the first provable, efficient online algorithm for\nmatrix completion. Our algorithm starts from an initial estimate of the matrix\nand then performs non-convex stochastic gradient descent (SGD). After every\nobservation, it performs a fast update involving only one row of two tall\nmatrices, giving near linear total runtime. Our algorithm can be naturally used\nin the offline setting as well, where it gives competitive sample complexity\nand runtime to state of the art algorithms. Our proofs introduce a general\nframework to show that SGD updates tend to stay away from saddle surfaces and\ncould be of broader interests for other non-convex problems to prove tight\nrates. \n\n"}
{"id": "1605.08840", "contents": "Title: Optimal dynamic mechanisms with ex-post IR via bank accounts Abstract: Lately, the problem of designing multi-stage dynamic mechanisms has been\nshown to be both theoretically challenging and practically important. In this\npaper, we consider the problem of designing revenue optimal dynamic mechanism\nfor a setting where an auctioneer sells a set of items to a buyer in multiple\nstages. At each stage, there could be multiple items for sale but each item can\nonly appear in one stage. The type of the buyer at each stage is thus a\nmulti-dimensional vector characterizing the buyer's valuations of the items at\nthat stage and is assumed to be stage-wise independent.\n  In particular, we propose a novel class of mechanisms called bank account\nmechanisms. Roughly, a bank account mechanism is no different from any\nstage-wise individual mechanism except for an augmented structure called bank\naccount, a real number for each node that summarizes the history so far. We\nfirst establish that the optimal revenue from any dynamic mechanism in this\nsetting can be achieved by a bank account mechanism, and we provide a simple\ncharacterization of the set of incentive compatible and ex-post individually\nrational bank account mechanisms. Based on these characterizations, we then\ninvestigate the problem of finding the (approximately) optimal bank account\nmechanisms. We prove that there exists a simple, randomized bank account\nmechanism that approximates optimal revenue up to a constant factor. Our result\nis general and can accommodate previous approximation results in single-shot\nmulti-dimensional mechanism design. Based on the previous mechanism, we further\nshow that there exists a deterministic bank account mechanism that achieves\nconstant-factor approximation as well. Finally, we consider the problem of\ncomputing optimal mechanisms when the type space is discrete and provide an\nFPTAS via linear and dynamic programming. \n\n"}
{"id": "1605.09593", "contents": "Title: Adaptive Learning Rate via Covariance Matrix Based Preconditioning for\n  Deep Neural Networks Abstract: Adaptive learning rate algorithms such as RMSProp are widely used for\ntraining deep neural networks. RMSProp offers efficient training since it uses\nfirst order gradients to approximate Hessian-based preconditioning. However,\nsince the first order gradients include noise caused by stochastic\noptimization, the approximation may be inaccurate. In this paper, we propose a\nnovel adaptive learning rate algorithm called SDProp. Its key idea is effective\nhandling of the noise by preconditioning based on covariance matrix. For\nvarious neural networks, our approach is more efficient and effective than\nRMSProp and its variant. \n\n"}
{"id": "1605.09733", "contents": "Title: Condorcet-Consistent and Approximately Strategyproof Tournament Rules Abstract: We consider the manipulability of tournament rules for round-robin\ntournaments of $n$ competitors. Specifically, $n$ competitors are competing for\na prize, and a tournament rule $r$ maps the result of all $\\binom{n}{2}$\npairwise matches (called a tournament, $T$) to a distribution over winners.\nRule $r$ is Condorcet-consistent if whenever $i$ wins all $n-1$ of her matches,\n$r$ selects $i$ with probability $1$.\n  We consider strategic manipulation of tournaments where player $j$ might\nthrow their match to player $i$ in order to increase the likelihood that one of\nthem wins the tournament. Regardless of the reason why $j$ chooses to do this,\nthe potential for manipulation exists as long as $\\Pr[r(T) = i]$ increases by\nmore than $\\Pr[r(T) = j]$ decreases. Unfortunately, it is known that every\nCondorcet-consistent rule is manipulable (Altman and Kleinberg). In this work,\nwe address the question of how manipulable Condorcet-consistent rules must\nnecessarily be - by trying to minimize the difference between the increase in\n$\\Pr[r(T) = i]$ and decrease in $\\Pr[r(T) = j]$ for any potential manipulating\npair.\n  We show that every Condorcet-consistent rule is in fact $1/3$-manipulable,\nand that selecting a winner according to a random single elimination bracket is\nnot $\\alpha$-manipulable for any $\\alpha > 1/3$. We also show that many\npreviously studied tournament formats are all $1/2$-manipulable, and the\npopular class of Copeland rules (any rule that selects a player with the most\nwins) are all in fact $1$-manipulable, the worst possible. Finally, we consider\nextensions to match-fixing among sets of more than two players. \n\n"}
{"id": "1606.02355", "contents": "Title: Active Long Term Memory Networks Abstract: Continual Learning in artificial neural networks suffers from interference\nand forgetting when different tasks are learned sequentially. This paper\nintroduces the Active Long Term Memory Networks (A-LTM), a model of sequential\nmulti-task deep learning that is able to maintain previously learned\nassociation between sensory input and behavioral output while acquiring knew\nknowledge. A-LTM exploits the non-convex nature of deep neural networks and\nactively maintains knowledge of previously learned, inactive tasks using a\ndistillation loss. Distortions of the learned input-output map are penalized\nbut hidden layers are free to transverse towards new local optima that are more\nfavorable for the multi-task objective. We re-frame the McClelland's seminal\nHippocampal theory with respect to Catastrophic Inference (CI) behavior\nexhibited by modern deep architectures trained with back-propagation and\ninhomogeneous sampling of latent factors across epochs. We present empirical\nresults of non-trivial CI during continual learning in Deep Linear Networks\ntrained on the same task, in Convolutional Neural Networks when the task shifts\nfrom predicting semantic to graphical factors and during domain adaptation from\nsimple to complex environments. We present results of the A-LTM model's ability\nto maintain viewpoint recognition learned in the highly controlled iLab-20M\ndataset with 10 object categories and 88 camera viewpoints, while adapting to\nthe unstructured domain of Imagenet with 1,000 object categories. \n\n"}
{"id": "1606.03062", "contents": "Title: Procrastination with variable present bias Abstract: Individuals working towards a goal often exhibit time inconsistent behavior,\nmaking plans and then failing to follow through. One well-known model of such\nbehavioral anomalies is present-bias discounting: individuals over-weight\npresent costs by a bias factor. This model explains many time-inconsistent\nbehaviors, but can make stark predictions in many settings: individuals either\nfollow the most efficient plan for reaching their goal or procrastinate\nindefinitely.\n  We propose a modification in which the present-bias parameter can vary over\ntime, drawn independently each step from a fixed distribution. Following\nKleinberg and Oren (2014), we use a weighted task graph to model task planning,\nand measure the cost of procrastination as the relative expected cost of the\nchosen path versus the optimal path. We use a novel connection to optimal\npricing theory to describe the structure of the worst-case task graph for any\npresent-bias distribution. We then leverage this structure to derive conditions\non the bias distribution under which the worst-case ratio is exponential (in\ntime) or constant. We also examine conditions on the task graph that lead to\nimproved procrastination ratios: graphs with a uniformly bounded distance to\nthe goal, and graphs in which the distance to the goal monotonically decreases\non any path. \n\n"}
{"id": "1606.04145", "contents": "Title: Sample Complexity of Automated Mechanism Design Abstract: The design of revenue-maximizing combinatorial auctions, i.e. multi-item\nauctions over bundles of goods, is one of the most fundamental problems in\ncomputational economics, unsolved even for two bidders and two items for sale.\nIn the traditional economic models, it is assumed that the bidders' valuations\nare drawn from an underlying distribution and that the auction designer has\nperfect knowledge of this distribution. Despite this strong and oftentimes\nunrealistic assumption, it is remarkable that the revenue-maximizing\ncombinatorial auction remains unknown. In recent years, automated mechanism\ndesign has emerged as one of the most practical and promising approaches to\ndesigning high-revenue combinatorial auctions. The most scalable automated\nmechanism design algorithms take as input samples from the bidders' valuation\ndistribution and then search for a high-revenue auction in a rich auction\nclass. In this work, we provide the first sample complexity analysis for the\nstandard hierarchy of deterministic combinatorial auction classes used in\nautomated mechanism design. In particular, we provide tight sample complexity\nbounds on the number of samples needed to guarantee that the empirical revenue\nof the designed mechanism on the samples is close to its expected revenue on\nthe underlying, unknown distribution over bidder valuations, for each of the\nauction classes in the hierarchy. In addition to helping set automated\nmechanism design on firm foundations, our results also push the boundaries of\nlearning theory. In particular, the hypothesis functions used in our contexts\nare defined through multi-stage combinatorial optimization procedures, rather\nthan simple decision boundaries, as are common in machine learning. \n\n"}
{"id": "1606.04487", "contents": "Title: Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs Abstract: We study the factors affecting training time in multi-device deep learning\nsystems. Given a specification of a convolutional neural network, our goal is\nto minimize the time to train this model on a cluster of commodity CPUs and\nGPUs. We first focus on the single-node setting and show that by using standard\nbatching and data-parallel techniques, throughput can be improved by at least\n5.5x over state-of-the-art systems on CPUs. This ensures an end-to-end training\nspeed directly proportional to the throughput of a device regardless of its\nunderlying hardware, allowing each node in the cluster to be treated as a black\nbox. Our second contribution is a theoretical and empirical study of the\ntradeoffs affecting end-to-end training time in a multiple-device setting. We\nidentify the degree of asynchronous parallelization as a key factor affecting\nboth hardware and statistical efficiency. We see that asynchrony can be viewed\nas introducing a momentum term. Our results imply that tuning momentum is\ncritical in asynchronous parallel configurations, and suggest that published\nresults that have not been fully tuned might report suboptimal performance for\nsome configurations. For our third contribution, we use our novel understanding\nof the interaction between system and optimization dynamics to provide an\nefficient hyperparameter optimizer. Our optimizer involves a predictive model\nfor the total time to convergence and selects an allocation of resources to\nminimize that time. We demonstrate that the most popular distributed deep\nlearning systems fall within our tradeoff space, but do not optimize within the\nspace. By doing this optimization, our prototype runs 1.9x to 12x faster than\nthe fastest state-of-the-art systems. \n\n"}
{"id": "1606.04991", "contents": "Title: A Class of Parallel Doubly Stochastic Algorithms for Large-Scale\n  Learning Abstract: We consider learning problems over training sets in which both, the number of\ntraining examples and the dimension of the feature vectors, are large. To solve\nthese problems we propose the random parallel stochastic algorithm (RAPSA). We\ncall the algorithm random parallel because it utilizes multiple parallel\nprocessors to operate on a randomly chosen subset of blocks of the feature\nvector. We call the algorithm stochastic because processors choose training\nsubsets uniformly at random. Algorithms that are parallel in either of these\ndimensions exist, but RAPSA is the first attempt at a methodology that is\nparallel in both the selection of blocks and the selection of elements of the\ntraining set. In RAPSA, processors utilize the randomly chosen functions to\ncompute the stochastic gradient component associated with a randomly chosen\nblock. The technical contribution of this paper is to show that this minimally\ncoordinated algorithm converges to the optimal classifier when the training\nobjective is convex. Moreover, we present an accelerated version of RAPSA\n(ARAPSA) that incorporates the objective function curvature information by\npremultiplying the descent direction by a Hessian approximation matrix. We\nfurther extend the results for asynchronous settings and show that if the\nprocessors perform their updates without any coordination the algorithms are\nstill convergent to the optimal argument. RAPSA and its extensions are then\nnumerically evaluated on a linear estimation problem and a binary image\nclassification task using the MNIST handwritten digit dataset. \n\n"}
{"id": "1606.05725", "contents": "Title: An Efficient Large-scale Semi-supervised Multi-label Classifier Capable\n  of Handling Missing labels Abstract: Multi-label classification has received considerable interest in recent\nyears. Multi-label classifiers have to address many problems including:\nhandling large-scale datasets with many instances and a large set of labels,\ncompensating missing label assignments in the training set, considering\ncorrelations between labels, as well as exploiting unlabeled data to improve\nprediction performance. To tackle datasets with a large set of labels,\nembedding-based methods have been proposed which seek to represent the label\nassignments in a low-dimensional space. Many state-of-the-art embedding-based\nmethods use a linear dimensionality reduction to represent the label\nassignments in a low-dimensional space. However, by doing so, these methods\nactually neglect the tail labels - labels that are infrequently assigned to\ninstances. We propose an embedding-based method that non-linearly embeds the\nlabel vectors using an stochastic approach, thereby predicting the tail labels\nmore accurately. Moreover, the proposed method have excellent mechanisms for\nhandling missing labels, dealing with large-scale datasets, as well as\nexploiting unlabeled data. With the best of our knowledge, our proposed method\nis the first multi-label classifier that simultaneously addresses all of the\nmentioned challenges. Experiments on real-world datasets show that our method\noutperforms stateof-the-art multi-label classifiers by a large margin, in terms\nof prediction performance, as well as training time. \n\n"}
{"id": "1606.06271", "contents": "Title: Dynamic Programming for One-Sided Partially Observable Pursuit-Evasion\n  Games Abstract: Pursuit-evasion scenarios appear widely in robotics, security domains, and\nmany other real-world situations. We focus on two-player pursuit-evasion games\nwith concurrent moves, infinite horizon, and discounted rewards. We assume that\nthe players have a partial observability, however, the evader is given an\nadvantage of knowing the current position of the units of the pursuer. This\nsetting is particularly interesting for security domains where a robust\nstrategy, designed to maximize the utility in the worst-case scenario, is often\ndesirable. We provide, to the best of our knowledge, the first algorithm that\nprovably converges to the value of a partially observable pursuit-evasion game\nwith infinite horizon. Our algorithm extends well-known value iteration\nalgorithm by exploiting that (1) the value functions of our game depend only on\nposition of the pursuer and the belief he has about the current position of the\nevader, and (2) that these functions are piecewise linear and convex in the\nbelief space. \n\n"}
{"id": "1606.07042", "contents": "Title: Incentivizing Evaluation via Limited Access to Ground Truth:\n  Peer-Prediction Makes Things Worse Abstract: In many settings, an effective way of evaluating objects of interest is to\ncollect evaluations from dispersed individuals and to aggregate these\nevaluations together. Some examples are categorizing online content and\nevaluating student assignments via peer grading. For this data science problem,\none challenge is to motivate participants to conduct such evaluations carefully\nand to report them honestly, particularly when doing so is costly. Existing\napproaches, notably peer-prediction mechanisms, can incentivize truth telling\nin equilibrium. However, they also give rise to equilibria in which agents do\nnot pay the costs required to evaluate accurately, and hence fail to elicit\nuseful information. We show that this problem is unavoidable whenever agents\nare able to coordinate using low-cost signals about the items being evaluated\n(e.g., text labels or pictures). We then consider ways of circumventing this\nproblem by comparing agents' reports to ground truth, which is available in\npractice when there exist trusted evaluators---such as teaching assistants in\nthe peer grading scenario---who can perform a limited number of unbiased (but\nnoisy) evaluations. Of course, when such ground truth is available, a simpler\napproach is also possible: rewarding each agent based on agreement with ground\ntruth with some probability, and unconditionally rewarding the agent otherwise.\nSurprisingly, we show that the simpler mechanism achieves stronger incentive\nguarantees given less access to ground truth than a large set of\npeer-prediction mechanisms. \n\n"}
{"id": "1606.09424", "contents": "Title: Variance Allocation and Shapley Value Abstract: Motivated by the problem of utility allocation in a portfolio under a\nMarkowitz mean-variance choice paradigm, we propose an allocation criterion for\nthe variance of the sum of $n$ possibly dependent random variables. This\ncriterion, the Shapley value, requires to translate the problem into a\ncooperative game. The Shapley value has nice properties, but, in general, is\ncomputationally demanding. The main result of this paper shows that in our\nparticular case the Shapley value has a very simple form that can be easily\ncomputed. The same criterion is used also to allocate the standard deviation of\nthe sum of $n$ random variables and a conjecture about the relation of the\nvalues in the two games is formulated. \n\n"}
{"id": "1607.01517", "contents": "Title: Envy-Free Cake-Cutting among Families Abstract: This paper extends the classic cake-cutting problem to a situation in which\nthe \"cake\" is divided among families. Each piece of cake is owned and used\nsimultaneously by all members of the family. A typical example of such a cake\nis land. We examine three ways to assess the fairness of such a division, based\non the classic no-envy criterion: (a) Average envy-freeness means that for each\nfamily, the average value of its share (averaged over all family members) is\nweakly larger than the average value of any other share; (b) Unanimous\nenvy-freeness means that in each family, each member values the family's share\nweakly more than any other share; (c) Democratic envy-freeness means that in\neach family, at least half the members value the family's share weakly more\nthan any other share. We study each of these definitions from both an\nexistential and a computational perspective. \n\n"}
{"id": "1607.02959", "contents": "Title: From Behavior to Sparse Graphical Games: Efficient Recovery of\n  Equilibria Abstract: In this paper we study the problem of exact recovery of the pure-strategy\nNash equilibria (PSNE) set of a graphical game from noisy observations of joint\nactions of the players alone. We consider sparse linear influence games --- a\nparametric class of graphical games with linear payoffs, and represented by\ndirected graphs of n nodes (players) and in-degree of at most k. We present an\n$\\ell_1$-regularized logistic regression based algorithm for recovering the\nPSNE set exactly, that is both computationally efficient --- i.e. runs in\npolynomial time --- and statistically efficient --- i.e. has logarithmic sample\ncomplexity. Specifically, we show that the sufficient number of samples\nrequired for exact PSNE recovery scales as $\\mathcal{O}(\\mathrm{poly}(k) \\log\nn)$. We also validate our theoretical results using synthetic experiments. \n\n"}
{"id": "1607.03084", "contents": "Title: Kernel-based methods for bandit convex optimization Abstract: We consider the adversarial convex bandit problem and we build the first\n$\\mathrm{poly}(T)$-time algorithm with $\\mathrm{poly}(n) \\sqrt{T}$-regret for\nthis problem. To do so we introduce three new ideas in the derivative-free\noptimization literature: (i) kernel methods, (ii) a generalization of Bernoulli\nconvolutions, and (iii) a new annealing schedule for exponential weights (with\nincreasing learning rate). The basic version of our algorithm achieves\n$\\tilde{O}(n^{9.5} \\sqrt{T})$-regret, and we show that a simple variant of this\nalgorithm can be run in $\\mathrm{poly}(n \\log(T))$-time per step at the cost of\nan additional $\\mathrm{poly}(n) T^{o(1)}$ factor in the regret. These results\nimprove upon the $\\tilde{O}(n^{11} \\sqrt{T})$-regret and\n$\\exp(\\mathrm{poly}(T))$-time result of the first two authors, and the\n$\\log(T)^{\\mathrm{poly}(n)} \\sqrt{T}$-regret and\n$\\log(T)^{\\mathrm{poly}(n)}$-time result of Hazan and Li. Furthermore we\nconjecture that another variant of the algorithm could achieve\n$\\tilde{O}(n^{1.5} \\sqrt{T})$-regret, and moreover that this regret is\nunimprovable (the current best lower bound being $\\Omega(n \\sqrt{T})$ and it is\nachieved with linear functions). For the simpler situation of zeroth order\nstochastic convex optimization this corresponds to the conjecture that the\noptimal query complexity is of order $n^3 / \\epsilon^2$. \n\n"}
{"id": "1607.03084", "contents": "Title: Kernel-based methods for bandit convex optimization Abstract: We consider the adversarial convex bandit problem and we build the first\n$\\mathrm{poly}(T)$-time algorithm with $\\mathrm{poly}(n) \\sqrt{T}$-regret for\nthis problem. To do so we introduce three new ideas in the derivative-free\noptimization literature: (i) kernel methods, (ii) a generalization of Bernoulli\nconvolutions, and (iii) a new annealing schedule for exponential weights (with\nincreasing learning rate). The basic version of our algorithm achieves\n$\\tilde{O}(n^{9.5} \\sqrt{T})$-regret, and we show that a simple variant of this\nalgorithm can be run in $\\mathrm{poly}(n \\log(T))$-time per step at the cost of\nan additional $\\mathrm{poly}(n) T^{o(1)}$ factor in the regret. These results\nimprove upon the $\\tilde{O}(n^{11} \\sqrt{T})$-regret and\n$\\exp(\\mathrm{poly}(T))$-time result of the first two authors, and the\n$\\log(T)^{\\mathrm{poly}(n)} \\sqrt{T}$-regret and\n$\\log(T)^{\\mathrm{poly}(n)}$-time result of Hazan and Li. Furthermore we\nconjecture that another variant of the algorithm could achieve\n$\\tilde{O}(n^{1.5} \\sqrt{T})$-regret, and moreover that this regret is\nunimprovable (the current best lower bound being $\\Omega(n \\sqrt{T})$ and it is\nachieved with linear functions). For the simpler situation of zeroth order\nstochastic convex optimization this corresponds to the conjecture that the\noptimal query complexity is of order $n^3 / \\epsilon^2$. \n\n"}
{"id": "1607.03356", "contents": "Title: Extending Finite Memory Determinacy to Multiplayer Games Abstract: We show that under some general conditions the finite memory determinacy of a\nclass of two-player win/lose games played on finite graphs implies the\nexistence of a Nash equilibrium built from finite memory strategies for the\ncorresponding class of multi-player multi-outcome games. This generalizes a\nprevious result by Brihaye, De Pril and Schewe. For most of our conditions we\nprovide counterexamples showing that they cannot be dispensed with.\n  Our proofs are generally constructive, that is, provide upper bounds for the\nmemory required, as well as algorithms to compute the relevant winning\nstrategies. \n\n"}
{"id": "1607.03760", "contents": "Title: Distributed Games and Strategies Abstract: A summary of work on distributed games and strategies done within the first\nthree years of the ERC project ECSYM is presented. \n\n"}
{"id": "1607.07684", "contents": "Title: The Price of Anarchy in Auctions Abstract: This survey outlines a general and modular theory for proving approximation\nguarantees for equilibria of auctions in complex settings. This theory\ncomplements traditional economic techniques, which generally focus on exact and\noptimal solutions and are accordingly limited to relatively stylized settings.\n  We highlight three user-friendly analytical tools: smoothness-type\ninequalities, which immediately yield approximation guarantees for many auction\nformats of interest in the special case of complete information and\ndeterministic strategies; extension theorems, which extend such guarantees to\nrandomized strategies, no-regret learning outcomes, and incomplete-information\nsettings; and composition theorems, which extend such guarantees from simpler\nto more complex auctions. Combining these tools yields tight worst-case\napproximation guarantees for the equilibria of many widely-used auction\nformats. \n\n"}
{"id": "1607.08863", "contents": "Title: Exponentially fast convergence to (strict) equilibrium via hedging Abstract: Motivated by applications to data networks where fast convergence is\nessential, we analyze the problem of learning in generic N-person games that\nadmit a Nash equilibrium in pure strategies. Specifically, we consider a\nscenario where players interact repeatedly and try to learn from past\nexperience by small adjustments based on local - and possibly imperfect -\npayoff information. For concreteness, we focus on the so-called \"hedge\" variant\nof the exponential weights algorithm where players select an action with\nprobability proportional to the exponential of the action's cumulative payoff\nover time. When players have perfect information on their mixed payoffs, the\nalgorithm converges locally to a strict equilibrium and the rate of convergence\nis exponentially fast - of the order of\n$\\mathcal{O}(\\exp(-a\\sum_{j=1}^{t}\\gamma_{j}))$ where $a>0$ is a constant and\n$\\gamma_{j}$ is the algorithm's step-size. In the presence of uncertainty,\nconvergence requires a more conservative step-size policy, but with high\nprobability, the algorithm remains locally convergent and achieves an\nexponential convergence rate. \n\n"}
{"id": "1608.01264", "contents": "Title: Fast and Simple Optimization for Poisson Likelihood Models Abstract: Poisson likelihood models have been prevalently used in imaging, social\nnetworks, and time series analysis. We propose fast, simple,\ntheoretically-grounded, and versatile, optimization algorithms for Poisson\nlikelihood modeling. The Poisson log-likelihood is concave but not\nLipschitz-continuous. Since almost all gradient-based optimization algorithms\nrely on Lipschitz-continuity, optimizing Poisson likelihood models with a\nguarantee of convergence can be challenging, especially for large-scale\nproblems.\n  We present a new perspective allowing to efficiently optimize a wide range of\npenalized Poisson likelihood objectives. We show that an appropriate saddle\npoint reformulation enjoys a favorable geometry and a smooth structure.\nTherefore, we can design a new gradient-based optimization algorithm with\n$O(1/t)$ convergence rate, in contrast to the usual $O(1/\\sqrt{t})$ rate of\nnon-smooth minimization alternatives. Furthermore, in order to tackle problems\nwith large samples, we also develop a randomized block-decomposition variant\nthat enjoys the same convergence rate yet more efficient iteration cost.\n  Experimental results on several point process applications including social\nnetwork estimation and temporal recommendation show that the proposed algorithm\nand its randomized block variant outperform existing methods both on synthetic\nand real-world datasets. \n\n"}
{"id": "1608.01875", "contents": "Title: Sample Complexity for Non-Truthful Mechanisms Abstract: This paper considers the design of non-truthful mechanisms from samples. We\nidentify a parameterized family of mechanisms with strategically simple\nwinner-pays-bid, all-pay, and truthful payment formats. In general (not\nnecessarily downward-closed) single-parameter feasibility environments we prove\nthat the family has low representation and generalization error. Specifically,\npolynomially many bid samples suffice to identify and run a mechanism that is\n$\\epsilon$-close in Bayes-Nash equilibrium revenue or welfare to that of the\noptimal truthful mechanism with high probability. \n\n"}
{"id": "1608.04943", "contents": "Title: Demystifying Competition and Cooperation Dynamics of the Aerial mmWave\n  Access Market Abstract: Cellular has always relied on static deployments for providing wireless\naccess. However, even the emerging fifth-generation (5G) networks may face\ndifficulty in supporting the increased traffic demand with rigid, fixed\ninfrastructure without substantial over-provisioning. This is particularly true\nfor spontaneous large-scale events that require service providers to augment\ncapacity of their networks quickly. Today, the use of aerial devices equipped\nwith high-rate radio access capabilities has the potential to offer the much\nneeded \"on-demand\" capacity boost. Conversely, it also threatens to rattle the\nlong-standing business strategies of wireless operators, especially as the\n\"gold rush\" for cheaper millimeter wave (mmWave) spectrum lowers the market\nentry barriers. However, the intricate structure of this new market presently\nremains a mystery. This paper sheds light on competition and cooperation\nbehavior of dissimilar aerial mmWave access suppliers, concurrently employing\nlicensed and license-exempt frequency bands, by modeling it as a vertically\ndifferentiated market where customers have varying preferences in price and\nquality. To understand viable service provider strategies, we begin with\nconstructing the Nash equilibrium for the initial market competition by\nemploying the Bertrand and Cournot games. We then conduct a unique assessment\nof short-term market dynamics, where two licensed-band service providers may\ncooperate to improve their competition positions against the unlicensed-band\ncounterpart intruding the market. Our unprecedented analysis studies the\neffects of various market interactions, price-driven demand evolution, and\ndynamic profit balance in this novel type of ecosystem. \n\n"}
{"id": "1608.06819", "contents": "Title: Pricing and Optimization in Shared Vehicle Systems: An Approximation\n  Framework Abstract: Optimizing shared vehicle systems (bike/scooter/car/ride-sharing) is more\nchallenging compared to traditional resource allocation settings due to the\npresence of \\emph{complex network externalities} -- changes in the\ndemand/supply at any location affect future supply throughout the system within\nshort timescales. These externalities are well captured by steady-state\nMarkovian models, which are therefore widely used to analyze such systems.\nHowever, using such models to design pricing and other control policies is\ncomputationally difficult since the resulting optimization problems are\nhigh-dimensional and non-convex.\n  To this end, we develop a \\emph{rigorous approximation framework} for shared\nvehicle systems, providing a unified approach for a wide range of controls\n(pricing, matching, rebalancing), objective functions (throughput, revenue,\nwelfare), and system constraints (travel-times, welfare benchmarks,\nposted-price constraints). Our approach is based on the analysis of natural\nconvex relaxations, and obtains as special cases existing approximate-optimal\npolicies for limited settings, asymptotic-optimality results, and heuristic\npolicies. The resulting guarantees are non-asymptotic and parametric, and\nprovide operational insights into the design of real-world systems. In\nparticular, for any shared vehicle system with $n$ stations and $m$ vehicles,\nour framework obtains an approximation ratio of $1+(n-1)/m$, which is\nparticularly meaningful when $m/n$, the average number of vehicles per station,\nis large, as is often the case in practice. \n\n"}
{"id": "1608.07886", "contents": "Title: Incentives for Truthful Evaluations Abstract: We consider crowdsourcing problems where the users are asked to provide\nevaluations for items; the user evaluations are then used directly, or\naggregated into a consensus value. Lacking an incentive scheme, users have no\nmotive in making effort in completing the evaluations, providing inaccurate\nanswers instead. We propose incentive schemes that are truthful and cheap:\ntruthful as the optimal user behavior consists in providing accurate\nevaluations, and cheap because the truthfulness is achieved with little\noverhead cost. We consider both discrete evaluation tasks, where an evaluation\ncan be done either correctly, or incorrectly, with no degrees of approximation\nin between, and quantitative evaluation tasks, where evaluations are real\nnumbers, and the error is measured as distance from the correct value. For both\ntypes of tasks, we propose hierarchical incentive schemes that can be effected\nwith a small amount of additional evaluations, and that scale to arbitrarily\nlarge crowd sizes: they have the property that the strength of the incentive\ndoes not weaken with increasing hierarchy depth. Interestingly, we show that\nfor these schemes to work, the only requisite is that workers know their place\nin the hierarchy in advance. \n\n"}
{"id": "1609.04221", "contents": "Title: Structured Perfect Bayesian Equilibrium in Infinite Horizon Dynamic\n  Games with Asymmetric Information Abstract: In dynamic games with asymmetric information structure, the widely used\nconcept of equilibrium is perfect Bayesian equilibrium (PBE). This is expressed\nas a strategy and belief pair that simultaneously satisfy sequential\nrationality and belief consistency. Unlike symmetric information dynamic games,\nwhere subgame perfect equilibrium (SPE) is the natural equilibrium concept, to\ndate there does not exist a universal algorithm that decouples the\ninterdependence of strategies and beliefs over time in calculating PBE. In this\npaper we find a subset of PBE for an infinite horizon discounted reward\nasymmetric information dynamic game. We refer to it as Structured PBE or SPBE;\nin SPBE, any agents' strategy depends on the public history only through a\ncommon public belief and on private history only through the respective agents'\nlatest private information (his private type). The public belief acts as a\nsummary of all the relevant past information and it's dimension does not\nincrease with time. The motivation for this comes the common information\napproach proposed in Nayyar et al. (2013) for solving decentralized team\n(non-strategic) resource allocation problems with asymmetric information. We\ncalculate SPBE by solving a single-shot fixed-point equation and a\ncorresponding forward recursive algorithm. We demonstrate our methodology by\nmeans of a public goods example. \n\n"}
{"id": "1609.04508", "contents": "Title: Column Networks for Collective Classification Abstract: Relational learning deals with data that are characterized by relational\nstructures. An important task is collective classification, which is to jointly\nclassify networked objects. While it holds a great promise to produce a better\naccuracy than non-collective classifiers, collective classification is\ncomputational challenging and has not leveraged on the recent breakthroughs of\ndeep learning. We present Column Network (CLN), a novel deep learning model for\ncollective classification in multi-relational domains. CLN has many desirable\ntheoretical properties: (i) it encodes multi-relations between any two\ninstances; (ii) it is deep and compact, allowing complex functions to be\napproximated at the network level with a small set of free parameters; (iii)\nlocal and relational features are learned simultaneously; (iv) long-range,\nhigher-order dependencies between instances are supported naturally; and (v)\ncrucially, learning and inference are efficient, linear in the size of the\nnetwork and the number of relations. We evaluate CLN on multiple real-world\napplications: (a) delay prediction in software projects, (b) PubMed Diabetes\npublication classification and (c) film genre classification. In all\napplications, CLN demonstrates a higher accuracy than state-of-the-art rivals. \n\n"}
{"id": "1609.05370", "contents": "Title: The Maximin Support Method: An Extension of the D'Hondt Method to\n  Approval-Based Multiwinner Elections Abstract: We propose the maximin support method, a novel extension of the D'Hondt\napportionment method to approval-based multiwinner elections. The maximin\nsupport method is based on maximizing the support of the least supported\nelected candidate. It can be computed efficiently and satisfies (adjusted\nversions of) the main properties of the original D'Hondt method: house\nmonotonicity, population monotonicity, and proportional representation. We also\nestablish a close relationship between the maximin support method and\nPhragm\\'{e}n's voting rules. \n\n"}
{"id": "1610.00571", "contents": "Title: Dynamic Complexity of Parity Games with Bounded Tree-Width Abstract: Dynamic complexity is concerned with updating the output of a problem when\nthe input is slightly changed. We study the dynamic complexity of two-player\nparity games over graphs of bounded tree-width, where updates may add or delete\nedges, or change the owner or color of states. We show that this problem is in\nDynFO (with LOGSPACE precomputation); this is achieved by a reduction to a\nDyck-path problem on an acyclic automaton. \n\n"}
{"id": "1610.01443", "contents": "Title: Efficiency and Budget Balance in General Quasi-linear Domains Abstract: We study efficiency and budget balance for designing mechanisms in general\nquasi-linear domains. Green and Laffont (1979) proved that one cannot\ngenerically achieve both. We consider strategyproof budget-balanced mechanisms\nthat are approximately efficient. For deterministic mechanisms, we show that a\nstrategyproof and budget-balanced mechanism must have a sink agent whose\nvaluation function is ignored in selecting an alternative, and she is\ncompensated with the payments made by the other agents. We assume the\nvaluations of the agents come from a bounded open interval. Using this result,\nwe find a tight lower bound on the inefficiencies of strategyproof,\nbudget-balanced mechanisms in this domain. The bound shows that the\ninefficiency asymptotically disappears when the number of agents is large---a\nresult close in spirit to Green and Laffont (1979, Theorem 9.4). However, our\nresults provide worst-case bounds and the best possible rate of convergence.\n  Next, we consider minimizing any convex combination of inefficiency and\nbudget imbalance. We show that if the valuations are unrestricted, no\ndeterministic mechanism can do asymptotically better than minimizing\ninefficiency alone.\n  Finally, we investigate randomized mechanisms and provide improved lower\nbounds on expected inefficiency. We give a tight lower bound for an interesting\nclass of strategyproof, budget-balanced, randomized mechanisms. We also use an\noptimization-based approach---in the spirit of automated mechanism design---to\nprovide a lower bound on the minimum achievable inefficiency of any randomized\nmechanism.\n  Experiments with real data from two applications show that the inefficiency\nfor a simple randomized mechanism is 5--100 times smaller than the worst case.\nThis relative difference increases with the number of agents. \n\n"}
{"id": "1610.01900", "contents": "Title: Distance rationalization of anonymous and homogeneous voting rules Abstract: The concept of distance rationalizability of voting rules has been explored\nin recent years by several authors. Roughly speaking, we first choose a\nconsensus set of elections (defined via preferences of voters over candidates)\nfor which the result is specified a priori (intuitively, these are elections on\nwhich all voters can easily agree on the result). We also choose a measure of\ndistance between elections. The result of an election outside the consensus set\nis defined to be the result of the closest consensual election under the\ndistance measure.\n  Most previous work has dealt with a definition in terms of preference\nprofiles. However, most voting rules in common use are anonymous and\nhomogeneous. In this case there is a much more succinct representation (using\nthe voting simplex) of the inputs to the rule. This representation has been\nwidely used in the voting literature, but rarely in the context of distance\nrationalizability.\n  We show exactly how to connect distance rationalizability on profiles for\nanonymous and homogeneous rules to geometry in the simplex. We develop the\nconnection for the important special case of votewise distances, recently\nintroduced and studied by Elkind, Faliszewski and Slinko in several papers.\nThis yields a direct interpretation in terms of well-developed mathematical\nconcepts not seen before in the voting literature, namely Kantorovich (also\ncalled Wasserstein) distances and the geometry of Minkowski spaces.\n  As an application of this approach, we prove some positive and some negative\nresults about the decisiveness of distance rationalizable anonymous and\nhomogeneous rules. The positive results connect with the recent theory of\nhyperplane rules, while the negative ones deal with distances that are not\nmetrics, controversial notions of consensus, and the fact that the\n$\\ell^1$-norm is not strictly convex. \n\n"}
{"id": "1610.01982", "contents": "Title: Quantum Game Theory for Beam Alignment in Millimeter Wave\n  Device-to-Device Communications Abstract: In this paper, the problem of optimized beam alignment for wearable\ndevice-to-device (D2D) communications over millimeter wave (mmW) frequencies is\nstudied. In particular, a noncooperative game is formulated between wearable\ncommunication pairs that engage in D2D communications. In this game, wearable\ndevices acting as transmitters autonomously select the directions of their\nbeams so as to maximize the data rate to their receivers. To solve the game, an\nalgorithm based on best response dynamics is proposed that allows the\ntransmitters to reach a Nash equilibrium in a distributed manner. To further\nimprove the performance of mmW D2D communications, a novel quantum game model\nis formulated to enable the wearable devices to exploit new quantum directions\nduring their beam alignment so as to further enhance their data rate.\nSimulation results show that the proposed game-theoretic approach improves the\nperformance, in terms of data rate, of about 75% compared to a uniform beam\nalignment. The results also show that the quantum game model can further yield\nup to 20% improvement in data rates, relative to the classical game approach. \n\n"}
{"id": "1610.03013", "contents": "Title: Display Advertising with Real-Time Bidding (RTB) and Behavioural\n  Targeting Abstract: The most significant progress in recent years in online display advertising\nis what is known as the Real-Time Bidding (RTB) mechanism to buy and sell ads.\nRTB essentially facilitates buying an individual ad impression in real time\nwhile it is still being generated from a user's visit. RTB not only scales up\nthe buying process by aggregating a large amount of available inventories\nacross publishers but, most importantly, enables direct targeting of individual\nusers. As such, RTB has fundamentally changed the landscape of digital\nmarketing. Scientifically, the demand for automation, integration and\noptimisation in RTB also brings new research opportunities in information\nretrieval, data mining, machine learning and other related fields. In this\nmonograph, an overview is given of the fundamental infrastructure, algorithms,\nand technical solutions of this new frontier of computational advertising. The\ncovered topics include user response prediction, bid landscape forecasting,\nbidding algorithms, revenue optimisation, statistical arbitrage, dynamic\npricing, and ad fraud detection. \n\n"}
{"id": "1610.03045", "contents": "Title: Sketching Meets Random Projection in the Dual: A Provable Recovery\n  Algorithm for Big and High-dimensional Data Abstract: Sketching techniques have become popular for scaling up machine learning\nalgorithms by reducing the sample size or dimensionality of massive data sets,\nwhile still maintaining the statistical power of big data. In this paper, we\nstudy sketching from an optimization point of view: we first show that the\niterative Hessian sketch is an optimization process with preconditioning, and\ndevelop accelerated iterative Hessian sketch via the searching the conjugate\ndirection; we then establish primal-dual connections between the Hessian sketch\nand dual random projection, and apply the preconditioned conjugate gradient\napproach on the dual problem, which leads to the accelerated iterative dual\nrandom projection methods. Finally to tackle the challenges from both large\nsample size and high-dimensionality, we propose the primal-dual sketch, which\niteratively sketches the primal and dual formulations. We show that using a\nlogarithmic number of calls to solvers of small scale problem, primal-dual\nsketch is able to recover the optimum of the original problem up to arbitrary\nprecision. The proposed algorithms are validated via extensive experiments on\nsynthetic and real data sets which complements our theoretical results. \n\n"}
{"id": "1610.03474", "contents": "Title: The Core of the Participatory Budgeting Problem Abstract: In participatory budgeting, communities collectively decide on the allocation\nof public tax dollars for local public projects. In this work, we consider the\nquestion of fairly aggregating the preferences of community members to\ndetermine an allocation of funds to projects. This problem is different from\nstandard fair resource allocation because of public goods: The allocated goods\nbenefit all users simultaneously. Fairness is crucial in participatory decision\nmaking, since generating equitable outcomes is an important goal of these\nprocesses. We argue that the classic game theoretic notion of core captures\nfairness in the setting. To compute the core, we first develop a novel\ncharacterization of a public goods market equilibrium called the Lindahl\nequilibrium, which is always a core solution. We then provide the first (to our\nknowledge) polynomial time algorithm for computing such an equilibrium for a\nbroad set of utility functions; our algorithm also generalizes (in a\nnon-trivial way) the well-known concept of proportional fairness. We use our\ntheoretical insights to perform experiments on real participatory budgeting\nvoting data. We empirically show that the core can be efficiently computed for\nutility functions that naturally model our practical setting, and examine the\nrelation of the core with the familiar welfare objective. Finally, we address\nconcerns of incentives and mechanism design by developing a randomized\napproximately dominant-strategy truthful mechanism building on the exponential\nmechanism from differential privacy. \n\n"}
{"id": "1610.03745", "contents": "Title: Dividing goods and bads under additive utilities Abstract: When utilities are additive, we uncovered in our previous paper (Bogomolnaia\net al. \"Dividing Goods or Bads under Additive Utilities\") many similarities but\nalso surprising differences in the behavior of the familiar Competitive rule\n(with equal incomes), when we divide (private) goods or bads. The rule picks in\nboth cases the critical points of the product of utilities (or disutilities) on\nthe efficiency frontier, but there is only one such point if we share goods,\nwhile there can be exponentially many in the case of bads.\n  We extend this analysis to the fair division of mixed items: each item can be\nviewed by some participants as a good and by others as a bad, with\ncorresponding positive or negative marginal utilities. We find that the\ndivision of mixed items boils down, normatively as well as computationally, to\na variant of an all goods problem, or of an all bads problem: in particular the\ntask of dividing the non disposable items must be either good news for\neveryone, or bad news for everyone.\n  If at least one feasible utility profile is positive, the Competitive rule\npicks the unique maximum of the product of (positive) utilities. If no feasible\nutility profile is positive, this rule picks all critical points of the product\nof disutilities on the efficient frontier. \n\n"}
{"id": "1610.07858", "contents": "Title: Bounding Average-energy Games Abstract: We consider average-energy games, where the goal is to minimize the long-run\naverage of the accumulated energy. While several results have been obtained on\nthese games recently, decidability of average-energy games with a lower-bound\nconstraint on the energy level (but no upper bound) remained open; in\nparticular, so far there was no known upper bound on the memory that is\nrequired for winning strategies.\n  By reducing average-energy games with lower-bounded energy to infinite-state\nmean-payoff games and analyzing the density of low-energy configurations, we\nshow an almost tight doubly-exponential upper bound on the necessary memory,\nand that the winner of average-energy games with lower-bounded energy can be\ndetermined in doubly-exponential time. We also prove EXPSPACE-hardness of this\nproblem.\n  Finally, we consider multi-dimensional extensions of all types of\naverage-energy games: without bounds, with only a lower bound, and with both a\nlower and an upper bound on the energy. We show that the fully-bounded version\nis the only case to remain decidable in multiple dimensions. \n\n"}
{"id": "1610.08936", "contents": "Title: Learning Scalable Deep Kernels with Recurrent Structure Abstract: Many applications in speech, robotics, finance, and biology deal with\nsequential data, where ordering matters and recurrent structures are common.\nHowever, this structure cannot be easily captured by standard kernel functions.\nTo model such structure, we propose expressive closed-form kernel functions for\nGaussian processes. The resulting model, GP-LSTM, fully encapsulates the\ninductive biases of long short-term memory (LSTM) recurrent networks, while\nretaining the non-parametric probabilistic advantages of Gaussian processes. We\nlearn the properties of the proposed kernels by optimizing the Gaussian process\nmarginal likelihood using a new provably convergent semi-stochastic gradient\nprocedure and exploit the structure of these kernels for scalable training and\nprediction. This approach provides a practical representation for Bayesian\nLSTMs. We demonstrate state-of-the-art performance on several benchmarks, and\nthoroughly investigate a consequential autonomous driving application, where\nthe predictive uncertainties provided by GP-LSTM are uniquely valuable. \n\n"}
{"id": "1610.09300", "contents": "Title: Globally Optimal Training of Generalized Polynomial Neural Networks with\n  Nonlinear Spectral Methods Abstract: The optimization problem behind neural networks is highly non-convex.\nTraining with stochastic gradient descent and variants requires careful\nparameter tuning and provides no guarantee to achieve the global optimum. In\ncontrast we show under quite weak assumptions on the data that a particular\nclass of feedforward neural networks can be trained globally optimal with a\nlinear convergence rate with our nonlinear spectral method. Up to our knowledge\nthis is the first practically feasible method which achieves such a guarantee.\nWhile the method can in principle be applied to deep networks, we restrict\nourselves for simplicity in this paper to one and two hidden layer networks.\nOur experiments confirm that these models are rich enough to achieve good\nperformance on a series of real-world datasets. \n\n"}
{"id": "1611.01688", "contents": "Title: Oracle-Efficient Online Learning and Auction Design Abstract: We consider the design of computationally efficient online learning\nalgorithms in an adversarial setting in which the learner has access to an\noffline optimization oracle. We present an algorithm called Generalized\nFollow-the-Perturbed-Leader and provide conditions under which it is\noracle-efficient while achieving vanishing regret. Our results make significant\nprogress on an open problem raised by Hazan and Koren, who showed that\noracle-efficient algorithms do not exist in general and asked whether one can\nidentify properties under which oracle-efficient online learning may be\npossible.\n  Our auction-design framework considers an auctioneer learning an optimal\nauction for a sequence of adversarially selected valuations with the goal of\nachieving revenue that is almost as good as the optimal auction in hindsight,\namong a class of auctions. We give oracle-efficient learning results for: (1)\nVCG auctions with bidder-specific reserves in single-parameter settings, (2)\nenvy-free item pricing in multi-item auctions, and (3) s-level auctions of\nMorgenstern and Roughgarden for single-item settings. The last result leads to\nan approximation of the overall optimal Myerson auction when bidders'\nvaluations are drawn according to a fast-mixing Markov process, extending prior\nwork that only gave such guarantees for the i.i.d. setting.\n  Finally, we derive various extensions, including: (1) oracle-efficient\nalgorithms for the contextual learning setting in which the learner has access\nto side information (such as bidder demographics), (2) learning with\napproximate oracles such as those based on Maximal-in-Range algorithms, and (3)\nno-regret bidding in simultaneous auctions, resolving an open problem of\nDaskalakis and Syrgkanis. \n\n"}
{"id": "1611.02053", "contents": "Title: Reinforcement-based Simultaneous Algorithm and its Hyperparameters\n  Selection Abstract: Many algorithms for data analysis exist, especially for classification\nproblems. To solve a data analysis problem, a proper algorithm should be\nchosen, and also its hyperparameters should be selected. In this paper, we\npresent a new method for the simultaneous selection of an algorithm and its\nhyperparameters. In order to do so, we reduced this problem to the multi-armed\nbandit problem. We consider an algorithm as an arm and algorithm\nhyperparameters search during a fixed time as the corresponding arm play. We\nalso suggest a problem-specific reward function. We performed the experiments\non 10 real datasets and compare the suggested method with the existing one\nimplemented in Auto-WEKA. The results show that our method is significantly\nbetter in most of the cases and never worse than the Auto-WEKA. \n\n"}
{"id": "1611.03991", "contents": "Title: A Further Step Towards an Understanding of the Tournament Equilibrium\n  Set Abstract: We study some problems pertaining to the tournament equilibrium set (TEQ for\nshort). A tournament $H$ is a TEQ-retentive tournament if there is a tournament\n$T$ which has a minimal TEQ-retentive set $R$ such that $T[R]$ is isomorphic to\n$H$. We study TEQ-retentive tournaments and achieve many significant results.\nIn particular, we prove that there are no TEQ-retentive tournaments of size 4,\nonly 2 non-isomorphic TEQ-retentive tournaments of sizes 5 and 6, respectively,\nand 26 non-isomorphic TEQ-retentive tournaments of size 7. For three\ntournaments $H_1, H_2$ and $T$, we say $T$ is a $(H_1,H_2)$-TEQ-retentive\ntournament if $T$ has two minimal TEQ-retentive sets $R_1$ and $R_2$ such that\n$T[R_1]$ and $T[R_2]$ are isomorphic to $H_1$ and $H_2$, respectively. We show\nthat there are no $(H_1,H_2)$-retentive tournaments for $H_1$ and $H_2$ being\nsmall tournaments. Our results imply that Schwartz's Conjecture holds in all\ntournaments of size at most 14. Finally, we study Schwartz's Conjecture in\nseveral classes of tournaments. To achieve these results, we study the relation\nbetween (directed) domination graphs of tournaments and TEQ-retentive sets, and\nderive a number of properties on minimal TEQ-retentive sets. \n\n"}
{"id": "1611.04034", "contents": "Title: Fair Public Decision Making Abstract: We generalize the classic problem of fairly allocating indivisible goods to\nthe problem of \\emph{fair public decision making}, in which a decision must be\nmade on several social issues simultaneously, and, unlike the classic setting,\na decision can provide positive utility to multiple players. We extend the\npopular fairness notion of proportionality (which is not guaranteeable) to our\nmore general setting, and introduce three novel relaxations ---\n\\emph{proportionality up to one issue, round robin share, and pessimistic\nproportional share} --- that are also interesting in the classic goods\nallocation setting. We show that the Maximum Nash Welfare solution, which is\nknown to satisfy appealing fairness properties in the classic setting,\nsatisfies or approximates all three relaxations in our framework. We also\nprovide polynomial time algorithms and hardness results for finding allocations\nsatisfying these axioms, with or without insisting on Pareto optimality. \n\n"}
{"id": "1611.04831", "contents": "Title: The Power of Normalization: Faster Evasion of Saddle Points Abstract: A commonly used heuristic in non-convex optimization is Normalized Gradient\nDescent (NGD) - a variant of gradient descent in which only the direction of\nthe gradient is taken into account and its magnitude ignored. We analyze this\nheuristic and show that with carefully chosen parameters and noise injection,\nthis method can provably evade saddle points. We establish the convergence of\nNGD to a local minimum, and demonstrate rates which improve upon the fastest\nknown first order algorithm due to Ge e al. (2015).\n  The effectiveness of our method is demonstrated via an application to the\nproblem of online tensor decomposition; a task for which saddle point evasion\nis known to result in convergence to global minima. \n\n"}
{"id": "1611.05342", "contents": "Title: Approximately Efficient Two-Sided Combinatorial Auctions Abstract: Mechanism design for one-sided markets has been investigated for several\ndecades in economics and in computer science. More recently, there has been an\nincreased attention on mechanisms for two-sided markets, in which buyers and\nsellers act strategically. For two-sided markets, an impossibility result of\nMyerson and Satterthwaite states that no mechanism can simultaneously satisfy\nindividual rationality (IR), incentive compatibility (IC), strong\nbudget-balance (SBB), and be efficient. On the other hand, important\napplications to web advertisement, stock exchange, and frequency spectrum\nallocation, require us to consider two-sided combinatorial auctions in which\nbuyers have preferences on subsets of items, and sellers may offer multiple\nheterogeneous items. No efficient mechanism was known so far for such two-sided\ncombinatorial markets. This work provides the first IR, IC and SBB mechanisms\nthat provides an O(1)-approximation to the optimal social welfare for two-sided\nmarkets. An initial construction yields such a mechanism, but exposes a\nconceptual problem in the traditional SBB notion. This leads us to define the\nstronger notion of direct trade strong budget balance (DSBB). We then proceed\nto design mechanisms that are IR, IC, DSBB, and again provide an\nO(1)-approximation to the optimal social welfare. Our mechanisms work for any\nnumber of buyers with XOS valuations - a class in between submodular and\nsubadditive functions - and any number of sellers. We provide a mechanism that\nis dominant strategy incentive compatible (DSIC) if the sellers each have one\nitem for sale, and one that is bayesian incentive compatible (BIC) if sellers\nhold multiple items and have additive valuations over them. Finally, we present\na DSIC mechanism for the case that the valuation functions of all buyers and\nsellers are additive. \n\n"}
{"id": "1611.05763", "contents": "Title: Learning to reinforcement learn Abstract: In recent years deep reinforcement learning (RL) systems have attained\nsuperhuman performance in a number of challenging task domains. However, a\nmajor limitation of such applications is their demand for massive amounts of\ntraining data. A critical present objective is thus to develop deep RL methods\nthat can adapt rapidly to new tasks. In the present work we introduce a novel\napproach to this challenge, which we refer to as deep meta-reinforcement\nlearning. Previous work has shown that recurrent networks can support\nmeta-learning in a fully supervised context. We extend this approach to the RL\nsetting. What emerges is a system that is trained using one RL algorithm, but\nwhose recurrent dynamics implement a second, quite separate RL procedure. This\nsecond, learned RL algorithm can differ from the original one in arbitrary\nways. Importantly, because it is learned, it is configured to exploit structure\nin the training domain. We unpack these points in a series of seven\nproof-of-concept experiments, each of which examines a key aspect of deep\nmeta-RL. We consider prospects for extending and scaling up the approach, and\nalso point out some potentially important implications for neuroscience. \n\n"}
{"id": "1611.08691", "contents": "Title: Multiwinner Approval Rules as Apportionment Methods Abstract: We establish a link between multiwinner elections and apportionment problems\nby showing how approval-based multiwinner election rules can be interpreted as\nmethods of apportionment. We consider several multiwinner rules and observe\nthat they induce apportionment methods that are well-established in the\nliterature on proportional representation. For instance, we show that\nProportional Approval Voting induces the D'Hondt method and that Monroe's rule\ninduces the largest reminder method. We also consider properties of\napportionment methods and exhibit multiwinner rules that induce apportionment\nmethods satisfying these properties. \n\n"}
{"id": "1611.09928", "contents": "Title: Proportional Justified Representation Abstract: The goal of multi-winner elections is to choose a fixed-size committee based\non voters' preferences. An important concern in this setting is representation:\nlarge groups of voters with cohesive preferences should be adequately\nrepresented by the election winners. Recently, Aziz et al. (2015a;2017)\nproposed two axioms that aim to capture this idea: justified representation\n(JR) and its strengthening extended justified representation (EJR). In this\npaper, we extend the work of Aziz et al. in several directions. First, we\nanswer an open question of Aziz et al., by showing that Reweighted Approval\nVoting satisfies JR for $k=3, 4, 5$, but fails it for $k\\ge 6$. Second, we\nobserve that EJR is incompatible with the Perfect Representation criterion,\nwhich is important for many applications of multi-winner voting, and propose a\nrelaxation of EJR, which we call Proportional Justified Representation (PJR).\nPJR is more demanding than JR, but, unlike EJR, it is compatible with perfect\nrepresentation, and a committee that provides PJR can be computed in polynomial\ntime if the committee size divides the number of voters. Moreover, just like\nEJR, PJR can be used to characterize the classic PAV rule in the class of\nweighted PAV rules. On the other hand, we show that EJR provides stronger\nguarantees with respect to average voter satisfaction than PJR does. \n\n"}
{"id": "1612.00367", "contents": "Title: Large-scale Validation of Counterfactual Learning Methods: A Test-Bed Abstract: The ability to perform effective off-policy learning would revolutionize the\nprocess of building better interactive systems, such as search engines and\nrecommendation systems for e-commerce, computational advertising and news.\nRecent approaches for off-policy evaluation and learning in these settings\nappear promising. With this paper, we provide real-world data and a\nstandardized test-bed to systematically investigate these algorithms using data\nfrom display advertising. In particular, we consider the problem of filling a\nbanner ad with an aggregate of multiple products the user may want to purchase.\nThis paper presents our test-bed, the sanity checks we ran to ensure its\nvalidity, and shows results comparing state-of-the-art off-policy learning\nmethods like doubly robust optimization, POEM, and reductions to supervised\nlearning using regression baselines. Our results show experimental evidence\nthat recent off-policy learning methods can improve upon state-of-the-art\nsupervised learning techniques on a large-scale real-world data set. \n\n"}
{"id": "1612.00414", "contents": "Title: Distributed Nash Equilibrium Seeking via the Alternating Direction\n  Method of Multipliers Abstract: In this paper, the problem of finding a Nash equilibrium of a multi-player\ngame is considered. The players are only aware of their own cost functions as\nwell as the action space of all players. We develop a relatively fast algorithm\nwithin the framework of inexact-ADMM. It requires a communication graph for the\ninformation exchange between the players as well as a few mild assumptions on\ncost functions. The convergence proof of the algorithm to a Nash equilibrium of\nthe game is then provided. Moreover, the convergence rate is investigated via\nsimulations. \n\n"}
{"id": "1612.00796", "contents": "Title: Overcoming catastrophic forgetting in neural networks Abstract: The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially. \n\n"}
{"id": "1612.02803", "contents": "Title: The Physical Systems Behind Optimization Algorithms Abstract: We use differential equations based approaches to provide some {\\it\n\\textbf{physics}} insights into analyzing the dynamics of popular optimization\nalgorithms in machine learning. In particular, we study gradient descent,\nproximal gradient descent, coordinate gradient descent, proximal coordinate\ngradient, and Newton's methods as well as their Nesterov's accelerated variants\nin a unified framework motivated by a natural connection of optimization\nalgorithms to physical systems. Our analysis is applicable to more general\nalgorithms and optimization problems {\\it \\textbf{beyond}} convexity and strong\nconvexity, e.g. Polyak-\\L ojasiewicz and error bound conditions (possibly\nnonconvex). \n\n"}
{"id": "1612.02879", "contents": "Title: Learning Representations by Stochastic Meta-Gradient Descent in Neural\n  Networks Abstract: Representations are fundamental to artificial intelligence. The performance\nof a learning system depends on the type of representation used for\nrepresenting the data. Typically, these representations are hand-engineered\nusing domain knowledge. More recently, the trend is to learn these\nrepresentations through stochastic gradient descent in multi-layer neural\nnetworks, which is called backprop. Learning the representations directly from\nthe incoming data stream reduces the human labour involved in designing a\nlearning system. More importantly, this allows in scaling of a learning system\nfor difficult tasks. In this paper, we introduce a new incremental learning\nalgorithm called crossprop, which learns incoming weights of hidden units based\non the meta-gradient descent approach, that was previously introduced by Sutton\n(1992) and Schraudolph (1999) for learning step-sizes. The final update\nequation introduces an additional memory parameter for each of these weights\nand generalizes the backprop update equation. From our experiments, we show\nthat crossprop learns and reuses its feature representation while tackling new\nand unseen tasks whereas backprop relearns a new feature representation. \n\n"}
{"id": "1612.04485", "contents": "Title: Re-incentivizing Discovery: Mechanisms for Partial-Progress Sharing in\n  Research Abstract: An essential primitive for an efficient research ecosystem is\n\\emph{partial-progress sharing} (PPS) -- whereby a researcher shares\ninformation immediately upon making a breakthrough. This helps prevent\nduplication of work; however there is evidence that existing reward structures\nin research discourage partial-progress sharing. Ensuring PPS is especially\nimportant for new online collaborative-research platforms, which involve many\nresearchers working on large, multi-stage problems.\n  We study the problem of incentivizing information-sharing in research, under\na stylized model: non-identical agents work independently on subtasks of a\nlarge project, with dependencies between subtasks captured via an acyclic\nsubtask-network. Each subtask carries a reward, given to the first agent who\npublicly shares its solution. Agents can choose which subtasks to work on, and\nmore importantly, when to reveal solutions to completed subtasks. Under this\nmodel, we uncover the strategic rationale behind certain anecdotal phenomena.\nMoreover, for any acyclic subtask-network, and under a general model of\nagent-subtask completion times, we give sufficient conditions that ensure PPS\nis incentive-compatible for all agents.\n  One surprising finding is that rewards which are approximately proportional\nto perceived task-difficulties are sufficient to ensure PPS in all acyclic\nsubtask-networks. The fact that there is no tension between local fairness and\nglobal information-sharing in multi-stage projects is encouraging, as it\nsuggests practical mechanisms for real-world settings. Finally, we show that\nPPS is necessary, and in many cases, sufficient, to ensure a high rate of\nprogress in research. \n\n"}
{"id": "1612.05356", "contents": "Title: Projected Semi-Stochastic Gradient Descent Method with Mini-Batch Scheme\n  under Weak Strong Convexity Assumption Abstract: We propose a projected semi-stochastic gradient descent method with\nmini-batch for improving both the theoretical complexity and practical\nperformance of the general stochastic gradient descent method (SGD). We are\nable to prove linear convergence under weak strong convexity assumption. This\nrequires no strong convexity assumption for minimizing the sum of smooth convex\nfunctions subject to a compact polyhedral set, which remains popular across\nmachine learning community. Our PS2GD preserves the low-cost per iteration and\nhigh optimization accuracy via stochastic gradient variance-reduced technique,\nand admits a simple parallel implementation with mini-batches. Moreover, PS2GD\nis also applicable to dual problem of SVM with hinge loss. \n\n"}
{"id": "1612.06340", "contents": "Title: Computing Human-Understandable Strategies Abstract: Algorithms for equilibrium computation generally make no attempt to ensure\nthat the computed strategies are understandable by humans. For instance the\nstrategies for the strongest poker agents are represented as massive binary\nfiles. In many situations, we would like to compute strategies that can\nactually be implemented by humans, who may have computational limitations and\nmay only be able to remember a small number of features or components of the\nstrategies that have been computed. We study poker games where private\ninformation distributions can be arbitrary. We create a large training set of\ngame instances and solutions, by randomly selecting the information\nprobabilities, and present algorithms that learn from the training instances in\norder to perform well in games with unseen information distributions. We are\nable to conclude several new fundamental rules about poker strategy that can be\neasily implemented by humans. \n\n"}
{"id": "1612.06347", "contents": "Title: On-demand or Spot? Selling the cloud to risk-averse customers Abstract: In Amazon EC2, cloud resources are sold through a combination of an on-demand\nmarket, in which customers buy resources at a fixed price, and a spot market,\nin which customers bid for an uncertain supply of excess resources. Standard\nmarket environments suggest that an optimal design uses just one type of\nmarket. We show the prevalence of a dual market system can be explained by\nheterogeneous risk attitudes of customers. In our stylized model, we consider\nunit demand risk-averse bidders. We show the model admits a unique equilibrium,\nwith higher revenue and higher welfare than using only spot markets.\nFurthermore, as risk aversion increases, the usage of the on-demand market\nincreases. We conclude that risk attitudes are an important factor in cloud\nresource allocation and should be incorporated into models of cloud markets. \n\n"}
{"id": "1612.06476", "contents": "Title: Computational Complexity of Testing Proportional Justified\n  Representation Abstract: We consider a committee voting setting in which each voter approves of a\nsubset of candidates and based on the approvals, a target number of candidates\nare selected. Aziz et al. (2015) proposed two representation axioms called\njustified representation and extended justified representation. Whereas the\nformer can be tested as well as achieved in polynomial time, the latter\nproperty is coNP-complete to test and no polynomial-time algorithm is known to\nachieve it. Interestingly, S{\\'a}nchez-Fern{\\'a}ndez et~al. (2016) proposed an\nintermediate property called proportional justified representation that admits\na polynomial-time algorithm to achieve. The complexity of testing proportional\njustified representation has remained an open problem. In this paper, we settle\nthe complexity by proving that testing proportional justified representation is\ncoNP-complete. We complement the complexity result by showing that the problem\nadmits efficient algorithms if any of the following parameters are bounded: (1)\nnumber of voters (2) number of candidates (3) maximum number of candidates\napproved by a voter (4) maximum number of voters approving a given candidate. \n\n"}
{"id": "1612.09171", "contents": "Title: A Unified Approach to Analyzing Asynchronous Coordinate Descent and\n  Tatonnement Abstract: This paper concerns asynchrony in iterative processes, focusing on gradient\ndescent and tatonnement, a fundamental price dynamic.\n  Gradient descent is an important class of iterative algorithms for minimizing\nconvex functions. Classically, gradient descent has been a sequential and\nsynchronous process, although distributed and asynchronous variants have been\nstudied since the 1980s. Coordinate descent is a commonly studied version of\ngradient descent. In this paper, we focus on asynchronous coordinate descent on\nconvex functions $F:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ of the form $F(x) = f(x)\n+ \\sum_{k=1}^n \\Psi_k(x_k)$, where $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ is a\nsmooth convex function, and each $\\Psi_k:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a\nunivariate and possibly non-smooth convex function. Such functions occur in\nmany data analysis and machine learning problems.\n  We give new analyses of cyclic coordinate descent, a parallel asynchronous\nstochastic coordinate descent, and a rather general worst-case parallel\nasynchronous coordinate descent. For all of these, we either obtain sharply\nimproved bounds, or provide the first analyses. Our analyses all use a common\namortized framework. The application of this framework to the asynchronous\nstochastic version requires some new ideas, for it is not obvious how to ensure\na uniform distribution where it is needed in the face of asynchronous actions\nthat may undo uniformity. We believe that our approach may well be applicable\nto the analysis of other iterative asynchronous stochastic processes.\n  We extend the framework to show that an asynchronous version of tatonnement,\na fundamental price dynamic widely studied in general equilibrium theory,\nconverges toward a market equilibrium for Fisher markets with CES utilities or\nLeontief utilities, for which tatonnement is equivalent to coordinate descent. \n\n"}
{"id": "1612.09296", "contents": "Title: Symmetry, Saddle Points, and Global Optimization Landscape of Nonconvex\n  Matrix Factorization Abstract: We propose a general theory for studying the \\xl{landscape} of nonconvex\n\\xl{optimization} with underlying symmetric structures \\tz{for a class of\nmachine learning problems (e.g., low-rank matrix factorization, phase\nretrieval, and deep linear neural networks)}. In specific, we characterize the\nlocations of stationary points and the null space of Hessian matrices \\xl{of\nthe objective function} via the lens of invariant groups\\removed{for associated\noptimization problems, including low-rank matrix factorization, phase\nretrieval, and deep linear neural networks}. As a major motivating example, we\napply the proposed general theory to characterize the global \\xl{landscape} of\nthe \\xl{nonconvex optimization in} low-rank matrix factorization problem. In\nparticular, we illustrate how the rotational symmetry group gives rise to\ninfinitely many nonisolated strict saddle points and equivalent global minima\nof the objective function. By explicitly identifying all stationary points, we\ndivide the entire parameter space into three regions: ($\\cR_1$) the region\ncontaining the neighborhoods of all strict saddle points, where the objective\nhas negative curvatures; ($\\cR_2$) the region containing neighborhoods of all\nglobal minima, where the objective enjoys strong convexity along certain\ndirections; and ($\\cR_3$) the complement of the above regions, where the\ngradient has sufficiently large magnitudes. We further extend our result to the\nmatrix sensing problem. Such global landscape implies strong global convergence\nguarantees for popular iterative algorithms with arbitrary initial solutions. \n\n"}
{"id": "1701.00529", "contents": "Title: Truthful Facility Location with Additive Errors Abstract: We address the problem of locating facilities on the $[0,1]$ interval based\non reports from strategic agents. The cost of each agent is her distance to the\nclosest facility, and the global objective is to minimize either the maximum\ncost of an agent or the social cost.\n  As opposed to the extensive literature on facility location which considers\nthe multiplicative error, we focus on minimizing the worst-case additive error.\nMinimizing the additive error incentivizes mechanisms to adapt to the size of\nthe instance. I.e., mechanisms can sacrifice little efficiency in small\ninstances (location profiles in which all agents are relatively close to one\nanother), in order to gain more [absolute] efficiency in large instances. We\nargue that this measure is better suited for many manifestations of the\nfacility location problem in various domains.\n  We present tight bounds for mechanisms locating a single facility in both\ndeterministic and randomized cases. We further provide several extensions for\nlocating multiple facilities. \n\n"}
{"id": "1701.01302", "contents": "Title: Toward negotiable reinforcement learning: shifting priorities in Pareto\n  optimal sequential decision-making Abstract: Existing multi-objective reinforcement learning (MORL) algorithms do not\naccount for objectives that arise from players with differing beliefs.\nConcretely, consider two players with different beliefs and utility functions\nwho may cooperate to build a machine that takes actions on their behalf. A\nrepresentation is needed for how much the machine's policy will prioritize each\nplayer's interests over time. Assuming the players have reached common\nknowledge of their situation, this paper derives a recursion that any Pareto\noptimal policy must satisfy. Two qualitative observations can be made from the\nrecursion: the machine must (1) use each player's own beliefs in evaluating how\nwell an action will serve that player's utility function, and (2) shift the\nrelative priority it assigns to each player's expected utilities over time, by\na factor proportional to how well that player's beliefs predict the machine's\ninputs. Observation (2) represents a substantial divergence from na\\\"{i}ve\nlinear utility aggregation (as in Harsanyi's utilitarian theorem, and existing\nMORL algorithms), which is shown here to be inadequate for Pareto optimal\nsequential decision-making on behalf of players with different beliefs. \n\n"}
{"id": "1701.01722", "contents": "Title: Follow the Compressed Leader: Faster Online Learning of Eigenvectors and\n  Faster MMWU Abstract: The online problem of computing the top eigenvector is fundamental to machine\nlearning. In both adversarial and stochastic settings, previous results (such\nas matrix multiplicative weight update, follow the regularized leader, follow\nthe compressed leader, block power method) either achieve optimal regret but\nrun slow, or run fast at the expense of loosing a $\\sqrt{d}$ factor in total\nregret where $d$ is the matrix dimension.\n  We propose a $\\textit{follow-the-compressed-leader (FTCL)}$ framework which\nachieves optimal regret without sacrificing the running time. Our idea is to\n\"compress\" the matrix strategy to dimension 3 in the adversarial setting, or\ndimension 1 in the stochastic setting. These respectively resolve two open\nquestions regarding the design of optimal and efficient algorithms for the\nonline eigenvector problem. \n\n"}
{"id": "1701.02058", "contents": "Title: Coupled Compound Poisson Factorization Abstract: We present a general framework, the coupled compound Poisson factorization\n(CCPF), to capture the missing-data mechanism in extremely sparse data sets by\ncoupling a hierarchical Poisson factorization with an arbitrary data-generating\nmodel. We derive a stochastic variational inference algorithm for the resulting\nmodel and, as examples of our framework, implement three different\ndata-generating models---a mixture model, linear regression, and factor\nanalysis---to robustly model non-random missing data in the context of\nclustering, prediction, and matrix factorization. In all three cases, we test\nour framework against models that ignore the missing-data mechanism on large\nscale studies with non-random missing data, and we show that explicitly\nmodeling the missing-data mechanism substantially improves the quality of the\nresults, as measured using data log likelihood on a held-out test set. \n\n"}
{"id": "1701.02433", "contents": "Title: Managing Risk of Bidding in Display Advertising Abstract: In this paper, we deal with the uncertainty of bidding for display\nadvertising. Similar to the financial market trading, real-time bidding (RTB)\nbased display advertising employs an auction mechanism to automate the\nimpression level media buying; and running a campaign is no different than an\ninvestment of acquiring new customers in return for obtaining additional\nconverted sales. Thus, how to optimally bid on an ad impression to drive the\nprofit and return-on-investment becomes essential. However, the large\nrandomness of the user behaviors and the cost uncertainty caused by the auction\ncompetition may result in a significant risk from the campaign performance\nestimation. In this paper, we explicitly model the uncertainty of user\nclick-through rate estimation and auction competition to capture the risk. We\nborrow an idea from finance and derive the value at risk for each ad display\nopportunity. Our formulation results in two risk-aware bidding strategies that\npenalize risky ad impressions and focus more on the ones with higher expected\nreturn and lower risk. The empirical study on real-world data demonstrates the\neffectiveness of our proposed risk-aware bidding strategies: yielding profit\ngains of 15.4% in offline experiments and up to 17.5% in an online A/B test on\na commercial RTB platform over the widely applied bidding strategies. \n\n"}
{"id": "1701.03974", "contents": "Title: An Online Convex Optimization Approach to Dynamic Network Resource\n  Allocation Abstract: Existing approaches to online convex optimization (OCO) make sequential\none-slot-ahead decisions, which lead to (possibly adversarial) losses that\ndrive subsequent decision iterates. Their performance is evaluated by the\nso-called regret that measures the difference of losses between the online\nsolution and the best yet fixed overall solution in hindsight. The present\npaper deals with online convex optimization involving adversarial loss\nfunctions and adversarial constraints, where the constraints are revealed after\nmaking decisions, and can be tolerable to instantaneous violations but must be\nsatisfied in the long term. Performance of an online algorithm in this setting\nis assessed by: i) the difference of its losses relative to the best dynamic\nsolution with one-slot-ahead information of the loss function and the\nconstraint (that is here termed dynamic regret); and, ii) the accumulated\namount of constraint violations (that is here termed dynamic fit). In this\ncontext, a modified online saddle-point (MOSP) scheme is developed, and proved\nto simultaneously yield sub-linear dynamic regret and fit, provided that the\naccumulated variations of per-slot minimizers and constraints are sub-linearly\ngrowing with time. MOSP is also applied to the dynamic network resource\nallocation task, and it is compared with the well-known stochastic dual\ngradient method. Under various scenarios, numerical experiments demonstrate the\nperformance gain of MOSP relative to the state-of-the-art. \n\n"}
{"id": "1701.05948", "contents": "Title: Sponsored Search Auctions with Rich Ads Abstract: The generalized second price (GSP) auction has served as the core selling\nmechanism for sponsored search ads for over a decade. However, recent trends\nexpanding the set of allowed ad formats---to include a variety of sizes,\ndecorations, and other distinguishing features---have raised critical problems\nfor GSP-based platforms. Alternatives such as the Vickrey-Clarke-Groves (VCG)\nauction raise different complications because they fundamentally change the way\nprices are computed. In this paper we report on our efforts to redesign a\nsearch ad selling system from the ground up in this new context, proposing a\nmechanism that optimizes an entire slate of ads globally and computes prices\nthat achieve properties analogous to those held by GSP in the original, simpler\nsetting of uniform ads. A careful algorithmic coupling of\nallocation-optimization and pricing-computation allows our auction to operate\nwithin the strict timing constraints inherent in real-time ad auctions. We\nreport performance results of the auction in Yahoo's Gemini Search platform. \n\n"}
{"id": "1701.08644", "contents": "Title: Security Game with Non-additive Utilities and Multiple Attacker\n  Resources Abstract: There has been significant interest in studying security games for modeling\nthe interplay of attacks and defenses on various systems involving critical\ninfrastructure, financial system security, political campaigns, and civil\nsafeguarding. However, existing security game models typically either assume\nadditive utility functions, or that the attacker can attack only one target.\nSuch assumptions lead to tractable analysis, but miss key inherent dependencies\nthat exist among different targets in current complex networks. In this paper,\nwe generalize the classical security game models to allow for non-additive\nutility functions. We also allow attackers to be able to attack multiple\ntargets. We examine such a general security game from a theoretical perspective\nand provide a unified view. In particular, we show that each security game is\nequivalent to a combinatorial optimization problem over a set system\n$\\varepsilon$, which consists of defender's pure strategy space. The key\ntechnique we use is based on the transformation, projection of a polytope, and\nthe elipsoid method. This work settles several open questions in security game\ndomain and significantly extends the state of-the-art of both the polynomial\nsolvable and NP-hard class of the security game. \n\n"}
{"id": "1702.02113", "contents": "Title: Rare Nash Equilibria and the Price of Anarchy in Large Static Games Abstract: We study a static game played by a finite number of agents, in which agents\nare assigned independent and identically distributed random types and each\nagent minimizes its objective function by choosing from a set of admissible\nactions that depends on its type. The game is anonymous in the sense that the\nobjective function of each agent depends on the actions of other agents only\nthrough the empirical distribution of their type-action pairs. We study the\nasymptotic behavior of Nash equilibria, as the number of agents tends to\ninfinity, first by deriving laws of large numbers characterizes almost sure\nlimit points of Nash equilibria in terms of so-called Cournot-Nash equilibria\nof an associated nonatomic game. Our main results are large deviation\nprinciples that characterize the probability of rare Nash equilibria and\nassociated conditional limit theorems describing the behavior of equilibria\nconditioned on a rare event. The results cover situations when neither the\nfinite-player game nor the associated nonatomic game has a unique equilibrium.\nIn addition, we study the asymptotic behavior of the price of anarchy,\ncomplementing existing worst-case bounds with new probabilistic bounds in the\ncontext of congestion games, which are used to model traffic routing in\nnetworks. \n\n"}
{"id": "1702.04849", "contents": "Title: Theoretical and Practical Advances on Smoothing for Extensive-Form Games Abstract: Sparse iterative methods, in particular first-order methods, are known to be\namong the most effective in solving large-scale two-player zero-sum\nextensive-form games. The convergence rates of these methods depend heavily on\nthe properties of the distance-generating function that they are based on. We\ninvestigate the acceleration of first-order methods for solving extensive-form\ngames through better design of the dilated entropy function---a class of\ndistance-generating functions related to the domains associated with the\nextensive-form games. By introducing a new weighting scheme for the dilated\nentropy function, we develop the first distance-generating function for the\nstrategy spaces of sequential games that has no dependence on the branching\nfactor of the player. This result improves the convergence rate of several\nfirst-order methods by a factor of $\\Omega(b^dd)$, where $b$ is the branching\nfactor of the player, and $d$ is the depth of the game tree.\n  Thus far, counterfactual regret minimization methods have been faster in\npractice, and more popular, than first-order methods despite their\ntheoretically inferior convergence rates. Using our new weighting scheme and\npractical tuning we show that, for the first time, the excessive gap technique\ncan be made faster than the fastest counterfactual regret minimization\nalgorithm, CFR+, in practice. \n\n"}
{"id": "1702.05472", "contents": "Title: Threshold Constraints with Guarantees for Parity Objectives in Markov\n  Decision Processes Abstract: The beyond worst-case synthesis problem was introduced recently by Bruy\\`ere\net al. [BFRR14]: it aims at building system controllers that provide strict\nworst-case performance guarantees against an antagonistic environment while\nensuring higher expected performance against a stochastic model of the\nenvironment. Our work extends the framework of [BFRR14] and follow-up papers,\nwhich focused on quantitative objectives, by addressing the case of\n$\\omega$-regular conditions encoded as parity objectives, a natural way to\nrepresent functional requirements of systems.\n  We build strategies that satisfy a main parity objective on all plays, while\nensuring a secondary one with sufficient probability. This setting raises new\nchallenges in comparison to quantitative objectives, as one cannot easily mix\ndifferent strategies without endangering the functional properties of the\nsystem. We establish that, for all variants of this problem, deciding the\nexistence of a strategy lies in ${\\sf NP} \\cap {\\sf coNP}$, the same complexity\nclass as classical parity games. Hence, our framework provides additional\nmodeling power while staying in the same complexity class.\n  [BFRR14] V\\'eronique Bruy\\`ere, Emmanuel Filiot, Mickael Randour, and\nJean-Fran\\c{c}ois Raskin. Meet your expectations with guarantees: Beyond\nworst-case synthesis in quantitative games. In Ernst W. Mayr and Natacha\nPortier, editors, 31st International Symposium on Theoretical Aspects of\nComputer Science, STACS 2014, March 5-8, 2014, Lyon, France, volume 25 of\nLIPIcs, pages 199-213. Schloss Dagstuhl - Leibniz - Zentrum fuer Informatik,\n2014. \n\n"}
{"id": "1702.05575", "contents": "Title: A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics Abstract: We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for\nnon-convex optimization. The algorithm performs stochastic gradient descent,\nwhere in each step it injects appropriately scaled Gaussian noise to the\nupdate. We analyze the algorithm's hitting time to an arbitrary subset of the\nparameter space. Two results follow from our general theory: First, we prove\nthat for empirical risk minimization, if the empirical risk is point-wise close\nto the (smooth) population risk, then the algorithm achieves an approximate\nlocal minimum of the population risk in polynomial time, escaping suboptimal\nlocal minima that only exist in the empirical risk. Second, we show that SGLD\nimproves on one of the best known learnability results for learning linear\nclassifiers under the zero-one loss. \n\n"}
{"id": "1702.05594", "contents": "Title: Riemannian stochastic variance reduced gradient algorithm with\n  retraction and vector transport Abstract: In recent years, stochastic variance reduction algorithms have attracted\nconsiderable attention for minimizing the average of a large but finite number\nof loss functions. This paper proposes a novel Riemannian extension of the\nEuclidean stochastic variance reduced gradient (R-SVRG) algorithm to a manifold\nsearch space. The key challenges of averaging, adding, and subtracting multiple\ngradients are addressed with retraction and vector transport. For the proposed\nalgorithm, we present a global convergence analysis with a decaying step size\nas well as a local convergence rate analysis with a fixed step size under some\nnatural assumptions. In addition, the proposed algorithm is applied to the\ncomputation problem of the Riemannian centroid on the symmetric positive\ndefinite (SPD) manifold as well as the principal component analysis and\nlow-rank matrix completion problems on the Grassmann manifold. The results show\nthat the proposed algorithm outperforms the standard Riemannian stochastic\ngradient descent algorithm in each case. \n\n"}
{"id": "1702.06917", "contents": "Title: Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe Abstract: We consider the problem of bandit optimization, inspired by stochastic\noptimization and online learning problems with bandit feedback. In this\nproblem, the objective is to minimize a global loss function of all the\nactions, not necessarily a cumulative loss. This framework allows us to study a\nvery general class of problems, with applications in statistics, machine\nlearning, and other fields. To solve this problem, we analyze the\nUpper-Confidence Frank-Wolfe algorithm, inspired by techniques for bandits and\nconvex optimization. We give theoretical guarantees for the performance of this\nalgorithm over various classes of functions, and discuss the optimality of\nthese results. \n\n"}
{"id": "1702.07309", "contents": "Title: Bounding the inefficiency of compromise in opinion formation Abstract: Social networks on the Internet have seen an enormous growth recently and\nplay a crucial role in different aspects of today's life. They have facilitated\ninformation dissemination in ways that have been beneficial for their users but\nthey are often used strategically in order to spread information that only\nserves the objectives of particular users. These properties have inspired a\nrevision of classical opinion formation models from sociology using\ngame-theoretic notions and tools. We follow the same modeling approach,\nfocusing on scenarios where the opinion expressed by each user is a compromise\nbetween her internal belief and the opinions of a small number of neighbors\namong her social acquaintances. We formulate simple games that capture this\nbehavior and quantify the inefficiency of equilibria using the well-known\nnotion of the price of anarchy. Our results indicate that compromise comes at a\ncost that strongly depends on the neighborhood size. \n\n"}
{"id": "1702.07444", "contents": "Title: Bandits with Movement Costs and Adaptive Pricing Abstract: We extend the model of Multi-armed Bandit with unit switching cost to\nincorporate a metric between the actions. We consider the case where the metric\nover the actions can be modeled by a complete binary tree, and the distance\nbetween two leaves is the size of the subtree of their least common ancestor,\nwhich abstracts the case that the actions are points on the continuous interval\n$[0,1]$ and the switching cost is their distance. In this setting, we give a\nnew algorithm that establishes a regret of $\\widetilde{O}(\\sqrt{kT} + T/k)$,\nwhere $k$ is the number of actions and $T$ is the time horizon. When the set of\nactions corresponds to whole $[0,1]$ interval we can exploit our method for the\ntask of bandit learning with Lipschitz loss functions, where our algorithm\nachieves an optimal regret rate of $\\widetilde{\\Theta}(T^{2/3})$, which is the\nsame rate one obtains when there is no penalty for movements. As our main\napplication, we use our new algorithm to solve an adaptive pricing problem.\nSpecifically, we consider the case of a single seller faced with a stream of\npatient buyers. Each buyer has a private value and a window of time in which\nthey are interested in buying, and they buy at the lowest price in the window,\nif it is below their value. We show that with an appropriate discretization of\nthe prices, the seller can achieve a regret of $\\widetilde{O}(T^{2/3})$\ncompared to the best fixed price in hindsight, which outperform the previous\nregret bound of $\\widetilde{O}(T^{3/4})$ for the problem. \n\n"}
{"id": "1702.07665", "contents": "Title: Truthful Mechanisms for Delivery with Mobile Agents Abstract: We study the game-theoretic task of selecting mobile agents to deliver\nmultiple items on a network. An instance is given by $m$ messages (physical\nobjects) which have to be transported between specified source-target pairs in\na weighted undirected graph, and $k$ mobile heterogeneous agents, each being\nable to transport one message at a time. Following a recent model by\n[B\\\"artschi et al. 2016], each agent $i$ consumes energy proportional to the\ndistance it travels in the graph, where the different rates of energy\nconsumption are given by weight factors $w_i$. We are interested in optimizing\nor approximating the total energy consumption over all selected agents.\n  Unlike previous research, we assume the weights to be private values known\nonly to the respective agents. We present three different mechanisms which\nselect, route and pay the agents in a truthful way that guarantees voluntary\nparticipation of the agents, while approximating the optimum energy consumption\nby a constant factor. To this end we analyze a previous structural result and\nan approximation algorithm given by [B\\\"artschi et al. 2017]. Finally, we show\nthat for some instances in the case of a single package, the sum of the\npayments can be bounded in terms of the optimum. \n\n"}
{"id": "1702.07902", "contents": "Title: Approval Voting with Intransitive Preferences Abstract: We extend Approval voting to the settings where voters may have intransitive\npreferences. The major obstacle to applying Approval voting in these settings\nis that voters are not able to clearly determine who they should approve or\ndisapprove, due to the intransitivity of their preferences. An approach to\naddress this issue is to apply tournament solutions to help voters make the\ndecision. We study a class of voting systems where first each voter casts a\nvote defined as a tournament, then a well-defined tournament solution is\napplied to select the candidates who are assumed to be approved by the voter.\nWinners are the ones receiving the most approvals. We study axiomatic\nproperties of this class of voting systems and complexity of control and\nbribery problems for these voting systems. \n\n"}
{"id": "1702.07932", "contents": "Title: The role of quantum correlations in Cop and Robber game Abstract: We introduce and study quantized versions of Cop and Robber game. We achieve\nthis by using graph-preserving quantum operations, which are the quantum\nanalogues of stochastic operations preserving the graph. We provide the tight\nbound for the number of operations required to reach the given state. By\nextending them to the controlled operations, we define a quantum-controlled Cop\nand Robber game, which expands the classical Cop and Robber game, as well as\nthe classically controlled quantum Cop and Robber game. In contrast to the\ntypical scheme for introducing quantum games, we assume that both parties can\nutilise full information about the opponent's strategy. We show that the\nutilisation of the full knowledge about the opponent's state does not provide\nthe advantage. Moreover, the chances of catching the Robber decrease for\nclassical cop-win graphs. This result does not depend on the chosen model of\nevolution. On the other hand, the possibility to execute controlled quantum\noperations allows catching the Robber on almost all classical cop-win graphs.\nBy this, we demonstrate that it is necessary to enrich the structure of\ncorrelations between the players' systems to provide a non-trivial quantized\nCop and Robber game. Thus the quantum controlled operations offer a significant\nadvantage over the classically controlled quantum operations. \n\n"}
{"id": "1702.08134", "contents": "Title: Dropping Convexity for More Efficient and Scalable Online Multiview\n  Learning Abstract: Multiview representation learning is very popular for latent factor analysis.\nIt naturally arises in many data analysis, machine learning, and information\nretrieval applications to model dependent structures among multiple data\nsources. For computational convenience, existing approaches usually formulate\nthe multiview representation learning as convex optimization problems, where\nglobal optima can be obtained by certain algorithms in polynomial time.\nHowever, many pieces of evidence have corroborated that heuristic nonconvex\napproaches also have good empirical computational performance and convergence\nto the global optima, although there is a lack of theoretical justification.\nSuch a gap between theory and practice motivates us to study a nonconvex\nformulation for multiview representation learning, which can be efficiently\nsolved by a simple stochastic gradient descent (SGD) algorithm. We first\nillustrate the geometry of the nonconvex formulation; Then, we establish\nasymptotic global rates of convergence to the global optima by diffusion\napproximations. Numerical experiments are provided to support our theory. \n\n"}
{"id": "1702.08533", "contents": "Title: Competing Bandits: Learning under Competition Abstract: Most modern systems strive to learn from interactions with users, and many\nengage in exploration: making potentially suboptimal choices for the sake of\nacquiring new information. We initiate a study of the interplay between\nexploration and competition--how such systems balance the exploration for\nlearning and the competition for users. Here the users play three distinct\nroles: they are customers that generate revenue, they are sources of data for\nlearning, and they are self-interested agents which choose among the competing\nsystems. In our model, we consider competition between two multi-armed bandit\nalgorithms faced with the same bandit instance. Users arrive one by one and\nchoose among the two algorithms, so that each algorithm makes progress if and\nonly if it is chosen. We ask whether and to what extent competition\nincentivizes the adoption of better bandit algorithms. We investigate this\nissue for several models of user response, as we vary the degree of rationality\nand competitiveness in the model. Our findings are closely related to the\n\"competition vs. innovation\" relationship, a well-studied theme in economics. \n\n"}
{"id": "1702.08670", "contents": "Title: On architectural choices in deep learning: From network structure to\n  gradient convergence and parameter estimation Abstract: We study mechanisms to characterize how the asymptotic convergence of\nbackpropagation in deep architectures, in general, is related to the network\nstructure, and how it may be influenced by other design choices including\nactivation type, denoising and dropout rate. We seek to analyze whether network\narchitecture and input data statistics may guide the choices of learning\nparameters and vice versa. Given the broad applicability of deep architectures,\nthis issue is interesting both from theoretical and a practical standpoint.\nUsing properties of general nonconvex objectives (with first-order\ninformation), we first build the association between structural, distributional\nand learnability aspects of the network vis-\\`a-vis their interaction with\nparameter convergence rates. We identify a nice relationship between feature\ndenoising and dropout, and construct families of networks that achieve the same\nlevel of convergence. We then derive a workflow that provides systematic\nguidance regarding the choice of network sizes and learning parameters often\nmediated4 by input statistics. Our technical results are corroborated by an\nextensive set of evaluations, presented in this paper as well as independent\nempirical observations reported by other groups. We also perform experiments\nshowing the practical implications of our framework for choosing the best\nfully-connected design for a given problem. \n\n"}
{"id": "1702.08862", "contents": "Title: Proportional Representation in Vote Streams Abstract: We consider elections where the voters come one at a time, in a streaming\nfashion, and devise space-efficient algorithms which identify an approximate\nwinning committee with respect to common multiwinner proportional\nrepresentation voting rules; specifically, we consider the Approval-based and\nthe Borda-based variants of both the Chamberlin-- ourant rule and the Monroe\nrule. We complement our algorithms with lower bounds. Somewhat surprisingly,\nour results imply that, using space which does not depend on the number of\nvoters it is possible to efficiently identify an approximate representative\ncommittee of fixed size over vote streams with huge number of voters. \n\n"}
{"id": "1703.00320", "contents": "Title: Investigating the Characteristics of One-Sided Matching Mechanisms Under\n  Various Preferences and Risk Attitudes Abstract: One-sided matching mechanisms are fundamental for assigning a set of\nindivisible objects to a set of self-interested agents when monetary transfers\nare not allowed. Two widely-studied randomized mechanisms in multiagent\nsettings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial\nRule (PS). Both mechanisms require only that agents specify ordinal preferences\nand have a number of desirable economic and computational properties. However,\nthe induced outcomes of the mechanisms are often incomparable and thus there\nare challenges when it comes to deciding which mechanism to adopt in practice.\nIn this paper, we first consider the space of general ordinal preferences and\nprovide empirical results on the (in)comparability of RSD and PS. We analyze\ntheir respective economic properties under general and lexicographic\npreferences. We then instantiate utility functions with the goal of gaining\ninsights on the manipulability, efficiency, and envyfreeness of the mechanisms\nunder different risk-attitude models. Our results hold under various preference\ndistribution models, which further confirm the broad use of RSD in most\npractical applications. \n\n"}
{"id": "1703.00381", "contents": "Title: The Statistical Recurrent Unit Abstract: Sophisticated gated recurrent neural network architectures like LSTMs and\nGRUs have been shown to be highly effective in a myriad of applications. We\ndevelop an un-gated unit, the statistical recurrent unit (SRU), that is able to\nlearn long term dependencies in data by only keeping moving averages of\nstatistics. The SRU's architecture is simple, un-gated, and contains a\ncomparable number of parameters to LSTMs; yet, SRUs perform favorably to more\nsophisticated LSTM and GRU alternatives, often outperforming one or both in\nvarious tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an\nunbiased manner by optimizing respective architectures' hyperparameters in a\nBayesian optimization scheme for both synthetic and real-world tasks. \n\n"}
{"id": "1703.00663", "contents": "Title: Introduction to Nonnegative Matrix Factorization Abstract: In this paper, we introduce and provide a short overview of nonnegative\nmatrix factorization (NMF). Several aspects of NMF are discussed, namely, the\napplication in hyperspectral imaging, geometry and uniqueness of NMF solutions,\ncomplexity, algorithms, and its link with extended formulations of polyhedra.\nIn order to put NMF into perspective, the more general problem class of\nconstrained low-rank matrix approximation problems is first briefly introduced. \n\n"}
{"id": "1703.01138", "contents": "Title: Multiplicative Weights Update with Constant Step-Size in Congestion\n  Games: Convergence, Limit Cycles and Chaos Abstract: The Multiplicative Weights Update (MWU) method is a ubiquitous meta-algorithm\nthat works as follows: A distribution is maintained on a certain set, and at\neach step the probability assigned to element $\\gamma$ is multiplied by $(1\n-\\epsilon C(\\gamma))>0$ where $C(\\gamma)$ is the \"cost\" of element $\\gamma$ and\nthen rescaled to ensure that the new values form a distribution. We analyze MWU\nin congestion games where agents use \\textit{arbitrary admissible constants} as\nlearning rates $\\epsilon$ and prove convergence to \\textit{exact Nash\nequilibria}. Our proof leverages a novel connection between MWU and the\nBaum-Welch algorithm, the standard instantiation of the\nExpectation-Maximization (EM) algorithm for hidden Markov models (HMM).\nInterestingly, this convergence result does not carry over to the nearly\nhomologous MWU variant where at each step the probability assigned to element\n$\\gamma$ is multiplied by $(1 -\\epsilon)^{C(\\gamma)}$ even for the most\ninnocuous case of two-agent, two-strategy load balancing games, where such\ndynamics can provably lead to limit cycles or even chaotic behavior. \n\n"}
{"id": "1703.01851", "contents": "Title: Approximation Algorithms for Maximin Fair Division Abstract: We consider the problem of allocating indivisible goods fairly among n agents\nwho have additive and submodular valuations for the goods. Our fairness\nguarantees are in terms of the maximin share, that is defined to be the maximum\nvalue that an agent can ensure for herself, if she were to partition the goods\ninto n bundles, and then receive a minimum valued bundle. Since maximin fair\nallocations (i.e., allocations in which each agent gets at least her maximin\nshare) do not always exist, prior work has focused on approximation results\nthat aim to find allocations in which the value of the bundle allocated to each\nagent is (multiplicatively) as close to her maximin share as possible. In\nparticular, Procaccia and Wang (2014) along with Amanatidis et al. (2015) have\nshown that under additive valuations a 2/3-approximate maximin fair allocation\nalways exists and can be found in polynomial time. We complement these results\nby developing a simple and efficient algorithm that achieves the same\napproximation guarantee.\n  Furthermore, we initiate the study of approximate maximin fair division under\nsubmodular valuations. Specifically, we show that when the valuations of the\nagents are nonnegative, monotone, and submodular, then a 0.21-approximate\nmaximin fair allocation is guaranteed to exist. In fact, we show that such an\nallocation can be efficiently found by using a simple round-robin algorithm. A\ntechnical contribution of the paper is to analyze the performance of this\ncombinatorial algorithm by employing the concept of multilinear extensions. \n\n"}
{"id": "1703.02567", "contents": "Title: Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity\n  Auctions Abstract: We study the online learning problem of a bidder who participates in repeated\nauctions. With the goal of maximizing his T-period payoff, the bidder\ndetermines the optimal allocation of his budget among his bids for $K$ goods at\neach period. As a bidding strategy, we propose a polynomial-time algorithm,\ninspired by the dynamic programming approach to the knapsack problem. The\nproposed algorithm, referred to as dynamic programming on discrete set (DPDS),\nachieves a regret order of $O(\\sqrt{T\\log{T}})$. By showing that the regret is\nlower bounded by $\\Omega(\\sqrt{T})$ for any strategy, we conclude that DPDS is\norder optimal up to a $\\sqrt{\\log{T}}$ term. We evaluate the performance of\nDPDS empirically in the context of virtual trading in wholesale electricity\nmarkets by using historical data from the New York market. Empirical results\nshow that DPDS consistently outperforms benchmark heuristic methods that are\nderived from machine learning and online learning approaches. \n\n"}
{"id": "1703.03111", "contents": "Title: Statistical Cost Sharing Abstract: We study the cost sharing problem for cooperative games in situations where\nthe cost function $C$ is not available via oracle queries, but must instead be\nderived from data, represented as tuples $(S, C(S))$, for different subsets $S$\nof players. We formalize this approach, which we call statistical cost sharing,\nand consider the computation of the core and the Shapley value, when the tuples\nare drawn from some distribution $\\mathcal{D}$.\n  Previous work by Balcan et al. in this setting showed how to compute cost\nshares that satisfy the core property with high probability for limited classes\nof functions. We expand on their work and give an algorithm that computes such\ncost shares for any function with a non-empty core. We complement these results\nby proving an inapproximability lower bound for a weaker relaxation.\n  We then turn our attention to the Shapley value. We first show that when cost\nfunctions come from the family of submodular functions with bounded curvature,\n$\\kappa$, the Shapley value can be approximated from samples up to a $\\sqrt{1 -\n\\kappa}$ factor, and that the bound is tight. We then define statistical\nanalogues of the Shapley axioms, and derive a notion of statistical Shapley\nvalue. We show that these can always be approximated arbitrarily well for\ngeneral functions over any distribution $\\mathcal{D}$. \n\n"}
{"id": "1703.04225", "contents": "Title: New algorithms for matching problems Abstract: The standard two-sided and one-sided matching problems, and the closely\nrelated school choice problem, have been widely studied from an axiomatic\nviewpoint. A small number of algorithms dominate the literature. For two-sided\nmatching, the Gale-Shapley algorithm; for one-sided matching, (random) Serial\nDictatorship and Probabilistic Serial rule; for school choice, Gale-Shapley and\nthe Boston mechanisms.\n  The main reason for the dominance of these algorithms is their good\n(worst-case) axiomatic behaviour with respect to notions of efficiency and\nstrategyproofness. However if we shift the focus to fairness, social welfare,\ntradeoffs between incompatible axioms, and average-case analysis, it is far\nfrom clear that these algorithms are optimal.\n  We investigate new algorithms several of which have not appeared (to our\nknowledge) in the literature before. We give a unified presentation in which\nalgorithms for 2-sided matching yield 1-sided matching algorithms in a\nsystematic way. In addition to axiomatic properties, we investigate agent\nwelfare using both theoretical and computational approaches. We find that some\nof the new algorithms are worthy of consideration for certain applications. In\nparticular, when considering welfare under truthful preferences, some of the\nnew algorithms outperform the classic ones. \n\n"}
{"id": "1703.08636", "contents": "Title: Informational Substitutes Abstract: We propose definitions of substitutes and complements for pieces of\ninformation (\"signals\") in the context of a decision or optimization problem,\nwith game-theoretic and algorithmic applications. In a game-theoretic context,\nsubstitutes capture diminishing marginal value of information to a rational\ndecision maker. We use the definitions to address the question of how and when\ninformation is aggregated in prediction markets. Substitutes characterize\n\"best-possible\" equilibria with immediate information aggregation, while\ncomplements characterize \"worst-possible\", delayed aggregation. Game-theoretic\napplications also include settings such as crowdsourcing contests and Q\\&A\nforums. In an algorithmic context, where substitutes capture diminishing\nmarginal improvement of information to an optimization problem, substitutes\nimply efficient approximation algorithms for a very general class of (adaptive)\ninformation acquisition problems.\n  In tandem with these broad applications, we examine the structure and design\nof informational substitutes and complements. They have equivalent, intuitive\ndefinitions from disparate perspectives: submodularity, geometry, and\ninformation theory. We also consider the design of scoring rules or\noptimization problems so as to encourage substitutability or complementarity,\nwith positive and negative results. Taken as a whole, the results give some\nevidence that, in parallel with substitutable items, informational substitutes\nplay a natural conceptual and formal role in game theory and algorithms. \n\n"}
{"id": "1703.09083", "contents": "Title: The weighted stable matching problem Abstract: We study the stable matching problem in non-bipartite graphs with incomplete\nbut strict preference lists, where the edges have weights and the goal is to\ncompute a stable matching of minimum or maximum weight. This problem is known\nto be NP-hard in general. Our contribution is two fold: a polyhedral\ncharacterization and an approximation algorithm. Previously Chen et al. have\nshown that the stable matching polytope is integral if and only if the subgraph\nobtained after running phase one of Irving's algorithm is bipartite. We improve\nupon this result by showing that there are instances where this subgraph might\nnot be bipartite but one can further eliminate some edges and arrive at a\nbipartite subgraph. Our elimination procedure ensures that the set of stable\nmatchings remains the same, and thus the stable matching polytope of the final\nsubgraph contains the incidence vectors of all stable matchings of our original\ngraph. This allows us to characterize a larger class of instances for which the\nweighted stable matching problem is polynomial-time solvable. We also show that\nour edge elimination procedure is best possible, meaning that if the subgraph\nwe arrive at is not bipartite, then there is no bipartite subgraph that has the\nsame set of stable matchings as the original graph. We complement these results\nwith a $2$-approximation algorithm for the minimum weight stable matching\nproblem for instances where each agent has at most two possible partners in any\nstable matching. This is the first approximation result for any class of\ninstances with general weights. \n\n"}
{"id": "1703.10897", "contents": "Title: Multi-unit Assignment under Dichotomous Preferences Abstract: I study the problem of allocating objects among agents without using money.\nAgents can receive several objects and have dichotomous preferences, meaning\nthat they either consider objects to be acceptable or not. In this setup, the\negalitarian solution is more appealing than the competitive equilibrium with\nequal incomes because it is Lorenz dominant, unique in utilities, and group\nstrategy-proof. Moreover, it can be adapted to satisfy a new fairness axiom\nthat arises naturally in this context. Both solutions are disjoint. \n\n"}
{"id": "1704.00222", "contents": "Title: Fair Allocation of Indivisible Goods: Improvement and Generalization Abstract: We study the problem of fair allocation for indivisible goods. We use the the\nmaxmin share paradigm introduced by Budish as a measure for fairness. Procaccia\nand Wang (EC'14) were first to investigate this fundamental problem in the\nadditive setting. In contrast to what real-world experiments suggest, they show\nthat a maxmin guarantee (1-MMS allocation) is not always possible even when the\nnumber of agents is limited to 3. While the existence of an approximation\nsolution (e.g. a $1/2$-MMS allocation) is quite straightforward, improving the\nguarantee becomes subtler for larger constants. Procaccia provide a proof for\nexistence of a $2/3$-MMS allocation and leave the question open for better\nguarantees.\n  Our main contribution is an answer to the above question. We improve the\nresult of [Procaccia and Wang] to a $3/4$ factor in the additive setting. The\nmain idea for our $3/4$-MMS allocation method is clustering the agents. To this\nend, we introduce three notions and techniques, namely reducibility, matching\nallocation, and cycle-envy-freeness, and prove the approximation guarantee of\nour algorithm via non-trivial applications of these techniques. Our analysis\ninvolves coloring and double counting arguments that might be of independent\ninterest.\n  One major shortcoming of the current studies on fair allocation is the\nadditivity assumption on the valuations. We alleviate this by extending our\nresults to the case of submodular, fractionally subadditive, and subadditive\nsettings. More precisely, we give constant approximation guarantees for\nsubmodular and XOS agents, and a logarithmic approximation for the case of\nsubadditive agents. Furthermore, we complement our results by providing close\nupper bounds for each class of valuation functions. Finally, we present\nalgorithms to find such allocations for additive, submodular, and XOS settings\nin polynomial time. \n\n"}
{"id": "1704.00708", "contents": "Title: No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified\n  Geometric Analysis Abstract: In this paper we develop a new framework that captures the common landscape\nunderlying the common non-convex low-rank matrix problems including matrix\nsensing, matrix completion and robust PCA. In particular, we show for all above\nproblems (including asymmetric cases): 1) all local minima are also globally\noptimal; 2) no high-order saddle points exists. These results explain why\nsimple algorithms such as stochastic gradient descent have global converge, and\nefficiently optimize these non-convex objective functions in practice. Our\nframework connects and simplifies the existing analyses on optimization\nlandscapes for matrix sensing and symmetric matrix completion. The framework\nnaturally leads to new results for asymmetric matrix completion and robust PCA. \n\n"}
{"id": "1704.00726", "contents": "Title: Fairly Dividing a Cake after Some Parts Were Burnt in the Oven Abstract: There is a heterogeneous resource that contains both good parts and bad\nparts, for example, a cake with some parts burnt, a land-estate with some parts\nheavily taxed, or a chore with some parts fun to do. The resource has to be\ndivided fairly among $n$ agents with different preferences, each of whom has a\npersonal value-density function on the resource. The value-density functions\ncan accept any real value --- positive, negative or zero. Each agent should\nreceive a connected piece and no agent should envy another agent. We prove that\nsuch a division exists for 3 agents and present preliminary positive results\nfor larger numbers of agents. \n\n"}
{"id": "1704.01195", "contents": "Title: Statistical Estimation with Strategic Data Sources in Competitive\n  Settings Abstract: In this paper, we introduce a preliminary model for interactions in the data\nmarket. Recent research has shown ways in which a data aggregator can design\nmechanisms for users to ensure the quality of data, even in situations where\nthe users are effort-averse (i.e. prefer to submit lower-quality estimates) and\nthe data aggregator cannot observe the effort exerted by the users (i.e. the\ncontract suffers from the principal-agent problem). However, we have shown that\nthese mechanisms often break down in more realistic models, where multiple data\naggregators are in competition. Under minor assumptions on the properties of\nthe statistical estimators in use by data aggregators, we show that there is\neither no Nash equilibrium, or there is an infinite number of Nash equilibrium.\nIn the latter case, there is a fundamental ambiguity in who bears the burden of\nincentivizing different data sources. We are also able to calculate the price\nof anarchy, which measures how much social welfare is lost between the Nash\nequilibrium and the social optimum, i.e. between non-cooperative strategic play\nand cooperation. \n\n"}
{"id": "1704.02598", "contents": "Title: A Sample Complexity Measure with Applications to Learning Optimal\n  Auctions Abstract: We introduce a new sample complexity measure, which we refer to as\nsplit-sample growth rate. For any hypothesis $H$ and for any sample $S$ of size\n$m$, the split-sample growth rate $\\hat{\\tau}_H(m)$ counts how many different\nhypotheses can empirical risk minimization output on any sub-sample of $S$ of\nsize $m/2$. We show that the expected generalization error is upper bounded by\n$O\\left(\\sqrt{\\frac{\\log(\\hat{\\tau}_H(2m))}{m}}\\right)$. Our result is enabled\nby a strengthening of the Rademacher complexity analysis of the expected\ngeneralization error. We show that this sample complexity measure, greatly\nsimplifies the analysis of the sample complexity of optimal auction design, for\nmany auction classes studied in the literature. Their sample complexity can be\nderived solely by noticing that in these auction classes, ERM on any sample or\nsub-sample will pick parameters that are equal to one of the points in the\nsample. \n\n"}
{"id": "1704.04010", "contents": "Title: ZigZag: A new approach to adaptive online learning Abstract: We develop a novel family of algorithms for the online learning setting with\nregret against any data sequence bounded by the empirical Rademacher complexity\nof that sequence. To develop a general theory of when this type of adaptive\nregret bound is achievable we establish a connection to the theory of\ndecoupling inequalities for martingales in Banach spaces. When the hypothesis\nclass is a set of linear functions bounded in some norm, such a regret bound is\nachievable if and only if the norm satisfies certain decoupling inequalities\nfor martingales. Donald Burkholder's celebrated geometric characterization of\ndecoupling inequalities (1984) states that such an inequality holds if and only\nif there exists a special function called a Burkholder function satisfying\ncertain restricted concavity properties. Our online learning algorithms are\nefficient in terms of queries to this function.\n  We realize our general theory by giving novel efficient algorithms for\nclasses including lp norms, Schatten p-norms, group norms, and reproducing\nkernel Hilbert spaces. The empirical Rademacher complexity regret bound implies\n--- when used in the i.i.d. setting --- a data-dependent complexity bound for\nexcess risk after online-to-batch conversion. To showcase the power of the\nempirical Rademacher complexity regret bound, we derive improved rates for a\nsupervised learning generalization of the online learning with low rank experts\ntask and for the online matrix prediction task.\n  In addition to obtaining tight data-dependent regret bounds, our algorithms\nenjoy improved efficiency over previous techniques based on Rademacher\ncomplexity, automatically work in the infinite horizon setting, and are\nscale-free. To obtain such adaptive methods, we introduce novel machinery, and\nthe resulting algorithms are not based on the standard tools of online convex\noptimization. \n\n"}
{"id": "1704.04720", "contents": "Title: Understanding Norm Change: An Evolutionary Game-Theoretic Approach\n  (Extended Version) Abstract: Human societies around the world interact with each other by developing and\nmaintaining social norms, and it is critically important to understand how such\nnorms emerge and change. In this work, we define an evolutionary game-theoretic\nmodel to study how norms change in a society, based on the idea that different\nstrength of norms in societies translate to different game-theoretic\ninteraction structures and incentives. We use this model to study, both\nanalytically and with extensive agent-based simulations, the evolutionary\nrelationships of the need for coordination in a society (which is related to\nits norm strength) with two key aspects of norm change: cultural inertia\n(whether or how quickly the population responds when faced with conditions that\nmake a norm change desirable), and exploration rate (the willingness of agents\nto try out new strategies). Our results show that a high need for coordination\nleads to both high cultural inertia and a low exploration rate, while a low\nneed for coordination leads to low cultural inertia and high exploration rate.\nThis is the first work, to our knowledge, on understanding the evolutionary\ncausal relationships among these factors. \n\n"}
{"id": "1704.05027", "contents": "Title: Optimal Multi-Unit Mechanisms with Private Demands Abstract: In the multi-unit pricing problem, multiple units of a single item are for\nsale. A buyer's valuation for $n$ units of the item is $v \\min \\{ n, d\\} $,\nwhere the per unit valuation $v$ and the capacity $d$ are private information\nof the buyer. We consider this problem in the Bayesian setting, where the pair\n$(v,d)$ is drawn jointly from a given probability distribution. In the\n\\emph{unlimited supply} setting, the optimal (revenue maximizing) mechanism is\na pricing problem, i.e., it is a menu of lotteries. In this paper we show that\nunder a natural regularity condition on the probability distributions, which we\ncall \\emph{decreasing marginal revenue}, the optimal pricing is in fact\n\\emph{deterministic}. It is a price curve, offering $i$ units of the item for a\nprice of $p_i$, for every integer $i$. Further, we show that the revenue as a\nfunction of the prices $p_i$ is a \\emph{concave} function, which implies that\nthe optimum price curve can be found in polynomial time. This gives a rare\nexample of a natural multi-parameter setting where we can show such a clean\ncharacterization of the optimal mechanism. We also give a more detailed\ncharacterization of the optimal prices for the case where there are only two\npossible demands. \n\n"}
{"id": "1704.06304", "contents": "Title: k-Majority Digraphs and the Hardness of Voting with a Constant Number of\n  Voters Abstract: Many hardness results in computational social choice make use of the fact\nthat every directed graph may be induced as the pairwise majority relation of\nsome preference profile. However, this fact requires a number of voters that is\nalmost linear in the number of alternatives. It is therefore unclear whether\nthese results remain intact when the number of voters is bounded, as is, for\nexample, typically the case in search engine aggregation settings. In this\npaper, we provide a systematic study of majority digraphs for a constant number\nof voters resulting in analytical, experimental, and complexity-theoretic\ninsights. First, we characterize the set of digraphs that can be induced by two\nand three voters, respectively, and give sufficient conditions for larger\nnumbers of voters. Second, we present a surprisingly efficient implementation\nvia SAT solving for computing the minimal number of voters that is required to\ninduce a given digraph and experimentally evaluate how many voters are required\nto induce the majority digraphs of real-world and generated preference\nprofiles. Finally, we leverage our sufficient conditions to show that the\nwinner determination problem of various well-known voting rules remains hard\neven when there is only a small constant number of voters. In particular, we\nshow that Kemeny's rule is hard to evaluate for 7 voters, while previous\nmethods could only establish such a result for constant even numbers of voters. \n\n"}
{"id": "1704.08754", "contents": "Title: Bifurcation Mechanism Design -- From Optimal Flat Taxes to Improved\n  Cancer Treatments Abstract: Small changes to the parameters of a system can lead to abrupt qualitative\nchanges of its behavior, a phenomenon known as bifurcation. Such instabilities\nare typically considered problematic, however, we show that their power can be\nleveraged to design novel types of mechanisms. Hysteresis mechanisms use\ntransient changes of system parameters to induce a permanent improvement to its\nperformance via optimal equilibrium selection. Optimal control mechanisms\ninduce convergence to states whose performance is better than even the best\nequilibrium. We apply these mechanisms in two different settings that\nillustrate the versatility of bifurcation mechanism design. In the first one we\nexplore how introducing flat taxation can improve social welfare, despite\ndecreasing agent \"rationality\", by destabilizing inefficient equilibria. From\nthere we move on to consider a well known game of tumor metabolism and use our\napproach to derive novel cancer treatment strategies. \n\n"}
{"id": "1705.00243", "contents": "Title: Generalization Guarantees for Multi-item Profit Maximization: Pricing,\n  Auctions, and Randomized Mechanisms Abstract: We study multi-item profit maximization when there is an underlying\ndistribution over buyers' values. In practice, a full description of the\ndistribution is typically unavailable, so we study the setting where the\nmechanism designer only has samples from the distribution. If the designer uses\nthe samples to optimize over a complex mechanism class -- such as the set of\nall multi-item, multi-buyer mechanisms -- a mechanism may have high average\nprofit over the samples but low expected profit. This raises the central\nquestion of this paper: how many samples are sufficient to ensure that a\nmechanism's average profit is close to its expected profit? To answer this\nquestion, we uncover structure shared by many pricing, auction, and lottery\nmechanisms: for any set of buyers' values, profit is piecewise linear in the\nmechanism's parameters. Using this structure, we prove new bounds for mechanism\nclasses not yet studied in the sample-based mechanism design literature and\nmatch or improve over the best-known guarantees for many classes. \n\n"}
{"id": "1705.01736", "contents": "Title: Of the People: Voting Is More Effective with Representative Candidates Abstract: In light of the classic impossibility results of Arrow and Gibbard and\nSatterthwaite regarding voting with ordinal rules, there has been recent\ninterest in characterizing how well common voting rules approximate the social\noptimum. In order to quantify the quality of approximation, it is natural to\nconsider the candidates and voters as embedded within a common metric space,\nand to ask how much further the chosen candidate is from the population as\ncompared to the socially optimal one. We use this metric preference model to\nexplore a fundamental and timely question: does the social welfare of a\npopulation improve when candidates are representative of the population? If so,\nthen by how much, and how does the answer depend on the complexity of the\nmetric space?\n  We restrict attention to the most fundamental and common social choice\nsetting: a population of voters, two independently drawn candidates, and a\nmajority rule election. When candidates are not representative of the\npopulation, it is known that the candidate selected by the majority rule can be\nthrice as far from the population as the socially optimal one. We examine how\nthis ratio improves when candidates are drawn independently from the population\nof voters. Our results are two-fold: When the metric is a line, the ratio\nimproves from $3$ to $4-2\\sqrt{2}$, roughly $1.1716$; this bound is tight. When\nthe metric is arbitrary, we show a lower bound of $1.5$ and a constant upper\nbound strictly better than $2$ on the approximation ratio of the majority rule.\n  The positive result depends in part on the assumption that candidates are\nindependent and identically distributed. However, we show that independence\nalone is not enough to achieve the upper bound: even when candidates are drawn\nindependently, if the population of candidates can be different from the\nvoters, then an upper bound of $2$ on the approximation is tight. \n\n"}
{"id": "1705.02266", "contents": "Title: Computing Constrained Approximate Equilibria in Polymatrix Games Abstract: This paper is about computing constrained approximate Nash equilibria in\npolymatrix games, which are succinctly represented many-player games defined by\nan interaction graph between the players. In a recent breakthrough, Rubinstein\nshowed that there exists a small constant $\\epsilon$, such that it is\nPPAD-complete to find an (unconstrained) $\\epsilon$-Nash equilibrium of a\npolymatrix game. In the first part of the paper, we show that is NP-hard to\ndecide if a polymatrix game has a constrained approximate equilibrium for 9\nnatural constraints and any non-trivial approximation guarantee. These results\nhold even for planar bipartite polymatrix games with degree 3 and at most 7\nstrategies per player, and all non-trivial approximation guarantees. These\nresults stand in contrast to similar results for bimatrix games, which\nobviously need a non-constant number of actions, and which rely on stronger\ncomplexity-theoretic conjectures such as the exponential time hypothesis. In\nthe second part, we provide a deterministic QPTAS for interaction graphs with\nbounded treewidth and with logarithmically many actions per player that can\ncompute constrained approximate equilibria for a wide family of constraints\nthat cover many of the constraints dealt with in the first part. \n\n"}
{"id": "1705.02946", "contents": "Title: The Query Complexity of Cake Cutting Abstract: We study the query complexity of cake cutting and give lower and upper bounds\nfor computing approximately envy-free, perfect, and equitable allocations with\nthe minimum number of cuts. The lower bounds are tight for computing connected\nenvy-free allocations among n=3 players and for computing perfect and equitable\nallocations with minimum number of cuts between n=2 players.\n  We also formalize moving knife procedures and show that a large subclass of\nthis family, which captures all the known moving knife procedures, can be\nsimulated efficiently with arbitrarily small error in the Robertson-Webb query\nmodel. \n\n"}
{"id": "1705.03805", "contents": "Title: Smart Routing of Electric Vehicles for Load Balancing in Smart Grids Abstract: Electric vehicles (EVs) are expected to be a major component of the smart\ngrid. The rapid proliferation of EVs will introduce an unprecedented load on\nthe existing electric grid due to the charging/discharging behavior of the EVs,\nthus motivating the need for novel approaches for routing EVs across the grid.\nIn this paper, a novel gametheoretic framework for smart routing of EVs within\nthe smart grid is proposed. The goal of this framework is to balance the\nelectricity load across the grid while taking into account the traffic\ncongestion and the waiting time at charging stations. The EV routing problem is\nformulated as a noncooperative game. For this game, it is shown that selfish\nbehavior of EVs will result in a pure-strategy Nash equilibrium with the price\nof anarchy upper bounded by the variance of the ground load induced by the\nresidential, industrial, or commercial users. Moreover, the results are\nextended to capture the stochastic nature of induced ground load as well as the\nsubjective behavior of the owners of EVs as captured by using notions from the\nbehavioral framework of prospect theory. Simulation results provide new\ninsights on more efficient energy pricing at charging stations and under more\nrealistic grid conditions. \n\n"}
{"id": "1705.05035", "contents": "Title: Discrete Sequential Prediction of Continuous Actions for Deep RL Abstract: It has long been assumed that high dimensional continuous control problems\ncannot be solved effectively by discretizing individual dimensions of the\naction space due to the exponentially large number of bins over which policies\nwould have to be learned. In this paper, we draw inspiration from the recent\nsuccess of sequence-to-sequence models for structured prediction problems to\ndevelop policies over discretized spaces. Central to this method is the\nrealization that complex functions over high dimensional spaces can be modeled\nby neural networks that predict one dimension at a time. Specifically, we show\nhow Q-values and policies over continuous spaces can be modeled using a next\nstep prediction model over discretized dimensions. With this parameterization,\nit is possible to both leverage the compositional structure of action spaces\nduring learning, as well as compute maxima over action spaces (approximately).\nOn a simple example task we demonstrate empirically that our method can perform\nglobal search, which effectively gets around the local optimization issues that\nplague DDPG. We apply the technique to off-policy (Q-learning) methods and show\nthat our method can achieve the state-of-the-art for off-policy methods on\nseveral continuous control tasks. \n\n"}
{"id": "1705.07120", "contents": "Title: VAE with a VampPrior Abstract: Many different methods to train deep generative models have been introduced\nin the past. In this paper, we propose to extend the variational auto-encoder\n(VAE) framework with a new type of prior which we call \"Variational Mixture of\nPosteriors\" prior, or VampPrior for short. The VampPrior consists of a mixture\ndistribution (e.g., a mixture of Gaussians) with components given by\nvariational posteriors conditioned on learnable pseudo-inputs. We further\nextend this prior to a two layer hierarchical model and show that this\narchitecture with a coupled prior and posterior, learns significantly better\nmodels. The model also avoids the usual local optima issues related to useless\nlatent dimensions that plague VAEs. We provide empirical studies on six\ndatasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,\nFrey Faces and Histopathology patches, and show that applying the hierarchical\nVampPrior delivers state-of-the-art results on all datasets in the unsupervised\npermutation invariant setting and the best results or comparable to SOTA\nmethods for the approach with convolutional networks. \n\n"}
{"id": "1705.07215", "contents": "Title: On Convergence and Stability of GANs Abstract: We propose studying GAN training dynamics as regret minimization, which is in\ncontrast to the popular view that there is consistent minimization of a\ndivergence between real and generated distributions. We analyze the convergence\nof GAN training from this new point of view to understand why mode collapse\nhappens. We hypothesize the existence of undesirable local equilibria in this\nnon-convex game to be responsible for mode collapse. We observe that these\nlocal equilibria often exhibit sharp gradients of the discriminator function\naround some real data points. We demonstrate that these degenerate local\nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show\nthat DRAGAN enables faster training, achieves improved stability with fewer\nmode collapses, and leads to generator networks with better modeling\nperformance across a variety of architectures and objective functions. \n\n"}
{"id": "1705.07795", "contents": "Title: Training Deep Networks without Learning Rates Through Coin Betting Abstract: Deep learning methods achieve state-of-the-art performance in many\napplication scenarios. Yet, these methods require a significant amount of\nhyperparameters tuning in order to achieve the best results. In particular,\ntuning the learning rates in the stochastic optimization process is still one\nof the main bottlenecks. In this paper, we propose a new stochastic gradient\ndescent procedure for deep networks that does not require any learning rate\nsetting. Contrary to previous methods, we do not adapt the learning rates nor\nwe make use of the assumed curvature of the objective function. Instead, we\nreduce the optimization process to a game of betting on a coin and propose a\nlearning-rate-free optimal algorithm for this scenario. Theoretical convergence\nis proven for convex and quasi-convex functions and empirical evidence shows\nthe advantage of our algorithm over popular stochastic gradient algorithms. \n\n"}
{"id": "1705.07881", "contents": "Title: Online Factorization and Partition of Complex Networks From Random Walks Abstract: Finding the reduced-dimensional structure is critical to understanding\ncomplex networks. Existing approaches such as spectral clustering are\napplicable only when the full network is explicitly observed. In this paper, we\nfocus on the online factorization and partition of implicit large-scale\nnetworks based on observations from an associated random walk. We formulate\nthis into a nonconvex stochastic factorization problem and propose an efficient\nand scalable stochastic generalized Hebbian algorithm. The algorithm is able to\nprocess dependent state-transition data dynamically generated by the underlying\nnetwork and learn a low-dimensional representation for each vertex. By applying\na diffusion approximation analysis, we show that the continuous-time limiting\nprocess of the stochastic algorithm converges globally to the \"principal\ncomponents\" of the Markov chain and achieves a nearly optimal sample\ncomplexity. Once given the learned low-dimensional representations, we further\napply clustering techniques to recover the network partition. We show that when\nthe associated Markov process is lumpable, one can recover the partition\nexactly with high probability. We apply the proposed approach to model the\ntraffic flow of Manhattan as city-wide random walks. By using our algorithm to\nanalyze the taxi trip data, we discover a latent partition of the Manhattan\ncity that closely matches the traffic dynamics. \n\n"}
{"id": "1705.07993", "contents": "Title: Fair Allocation based on Diminishing Differences Abstract: Ranking alternatives is a natural way for humans to explain their\npreferences. It is being used in many settings, such as school choice, course\nallocations and residency matches. In some cases, several `items' are given to\neach participant. Without having any information on the underlying cardinal\nutilities, arguing about fairness of allocation requires extending the ordinal\nitem ranking to ordinal bundle ranking. The most commonly used such extension\nis stochastic dominance (SD), where a bundle X is preferred over a bundle Y if\nits score is better according to all additive score functions. SD is a very\nconservative extension, by which few allocations are necessarily fair while\nmany allocations are possibly fair. We propose to make a natural assumption on\nthe underlying cardinal utilities of the players, namely that the difference\nbetween two items at the top is larger than the difference between two items at\nthe bottom. This assumption implies a preference extension which we call\ndiminishing differences (DD), where X is preferred over Y if its score is\nbetter according to all additive score functions satisfying the DD assumption.\nWe give a full characterization of allocations that are\nnecessarily-proportional or possibly-proportional according to this assumption.\nBased on this characterization, we present a polynomial-time algorithm for\nfinding a necessarily-DD-proportional allocation if it exists. Using\nsimulations, we show that with high probability, a necessarily-proportional\nallocation does not exist but a necessarily-DD-proportional allocation exists,\nand moreover, that allocation is proportional according to the underlying\ncardinal utilities. We also consider chore allocation under the analogous\ncondition --- increasing-differences. \n\n"}
{"id": "1705.08711", "contents": "Title: Non-orthogonal Multiple Access for High-reliable and Low-latency V2X\n  Communications Abstract: In this paper, we consider a dense vehicular communication network where each\nvehicle broadcasts its safety information to its neighborhood in each\ntransmission period. Such applications require low latency and high\nreliability, and thus, we propose a non-orthogonal multiple access scheme to\nreduce the latency and to improve the packet reception probability. In the\nproposed scheme, the BS performs the semi-persistent scheduling to optimize the\ntime scheduling and allocate frequency resources in a non-orthogonal manner\nwhile the vehicles autonomously perform distributed power control. We formulate\nthe centralized scheduling and resource allocation problem as equivalent to a\nmulti-dimensional stable roommate matching problem, in which the users and\ntime/frequency resources are considered as disjoint sets of players to be\nmatched with each other. We then develop a novel rotation matching algorithm,\nwhich converges to a q-exchange stable matching after a limited number of\niterations. Simulation results show that the proposed scheme outperforms the\ntraditional orthogonal multiple access scheme in terms of the latency and\nreliability. \n\n"}
{"id": "1705.09644", "contents": "Title: Learning Causal Structures Using Regression Invariance Abstract: We study causal inference in a multi-environment setting, in which the\nfunctional relations for producing the variables from their direct causes\nremain the same across environments, while the distribution of exogenous noises\nmay vary. We introduce the idea of using the invariance of the functional\nrelations of the variables to their causes across a set of environments. We\ndefine a notion of completeness for a causal inference algorithm in this\nsetting and prove the existence of such algorithm by proposing the baseline\nalgorithm. Additionally, we present an alternate algorithm that has\nsignificantly improved computational and sample complexity compared to the\nbaseline algorithm. The experiment results show that the proposed algorithm\noutperforms the other existing algorithms. \n\n"}
{"id": "1705.09684", "contents": "Title: Multiple Source Domain Adaptation with Adversarial Training of Neural\n  Networks Abstract: While domain adaptation has been actively researched in recent years, most\ntheoretical results and algorithms focus on the single-source-single-target\nadaptation setting. Naive application of such algorithms on multiple source\ndomain adaptation problem may lead to suboptimal solutions. As a step toward\nbridging the gap, we propose a new generalization bound for domain adaptation\nwhen there are multiple source domains with labeled instances and one target\ndomain with unlabeled instances. Compared with existing bounds, the new bound\ndoes not require expert knowledge about the target distribution, nor the\noptimal combination rule for multisource domains. Interestingly, our theory\nalso leads to an efficient learning strategy using adversarial neural networks:\nwe show how to interpret it as learning feature representations that are\ninvariant to the multiple domain shifts while still being discriminative for\nthe learning task. To this end, we propose two models, both of which we call\nmultisource domain adversarial networks (MDANs): the first model optimizes\ndirectly our bound, while the second model is a smoothed approximation of the\nfirst one, leading to a more data-efficient and task-adaptive model. The\noptimization tasks of both models are minimax saddle point problems that can be\noptimized by adversarial training. To demonstrate the effectiveness of MDANs,\nwe conduct extensive experiments showing superior adaptation performance on\nthree real-world datasets: sentiment analysis, digit classification, and\nvehicle counting. \n\n"}
{"id": "1705.10116", "contents": "Title: Fractional Hedonic Games Abstract: The work we present in this paper initiated the formal study of fractional\nhedonic games, coalition formation games in which the utility of a player is\nthe average value he ascribes to the members of his coalition. Among other\nsettings, this covers situations in which players only distinguish between\nfriends and non-friends and desire to be in a coalition in which the fraction\nof friends is maximal. Fractional hedonic games thus not only constitute a\nnatural class of succinctly representable coalition formation games, but also\nprovide an interesting framework for network clustering. We propose a number of\nconditions under which the core of fractional hedonic games is non-empty and\nprovide algorithms for computing a core stable outcome. By contrast, we show\nthat the core may be empty in other cases, and that it is computationally hard\nin general to decide non-emptiness of the core. \n\n"}
{"id": "1706.00652", "contents": "Title: Computer aided synthesis: a game theoretic approach Abstract: In this invited contribution, we propose a comprehensive introduction to game\ntheory applied in computer aided synthesis. In this context, we give some\nclassical results on two-player zero-sum games and then on multi-player non\nzero-sum games. The simple case of one-player games is strongly related to\nautomata theory on infinite words. All along the article, we focus on general\napproaches to solve the studied problems, and we provide several illustrative\nexamples as well as intuitions on the proofs. \n\n"}
{"id": "1706.00849", "contents": "Title: A Game of Nontransitive Dice Abstract: We consider a two player simultaneous-move game where the two players each\nselect any permissible $n$-sided die for a fixed integer $n$. A player wins if\nthe outcome of his roll is greater than that of his opponent. Remarkably, for\n$n>3$, there is a unique Nash Equilibrium in pure strategies. The unique Nash\nEquilibrium is for each player to throw the Standard $n$-sided die, where each\nside has a different number. Our proof of uniqueness is constructive. We\nintroduce an algorithm with which, for any nonstandard die, we may generate\nanother die that beats it. \n\n"}
{"id": "1706.01350", "contents": "Title: Emergence of Invariance and Disentanglement in Deep Representations Abstract: Using established principles from Statistics and Information Theory, we show\nthat invariance to nuisance factors in a deep neural network is equivalent to\ninformation minimality of the learned representation, and that stacking layers\nand injecting noise during training naturally bias the network towards learning\ninvariant representations. We then decompose the cross-entropy loss used during\ntraining and highlight the presence of an inherent overfitting term. We propose\nregularizing the loss by bounding such a term in two equivalent ways: One with\na Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other\nusing the information in the weights as a measure of complexity of a learned\nmodel, yielding a novel Information Bottleneck for the weights. Finally, we\nshow that invariance and independence of the components of the representation\nlearned by the network are bounded above and below by the information in the\nweights, and therefore are implicitly optimized during training. The theory\nenables us to quantify and predict sharp phase transitions between underfitting\nand overfitting of random labels when using our regularized loss, which we\nverify in experiments, and sheds light on the relation between the geometry of\nthe loss function, invariance properties of the learned representation, and\ngeneralization error. \n\n"}
{"id": "1706.02677", "contents": "Title: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour Abstract: Deep learning thrives with large neural networks and large datasets. However,\nlarger networks and larger datasets result in longer training times that impede\nresearch and development progress. Distributed synchronous SGD offers a\npotential solution to this problem by dividing SGD minibatches over a pool of\nparallel workers. Yet to make this scheme efficient, the per-worker workload\nmust be large, which implies nontrivial growth in the SGD minibatch size. In\nthis paper, we empirically show that on the ImageNet dataset large minibatches\ncause optimization difficulties, but when these are addressed the trained\nnetworks exhibit good generalization. Specifically, we show no loss of accuracy\nwhen training with large minibatch sizes up to 8192 images. To achieve this\nresult, we adopt a hyper-parameter-free linear scaling rule for adjusting\nlearning rates as a function of minibatch size and develop a new warmup scheme\nthat overcomes optimization challenges early in training. With these simple\ntechniques, our Caffe2-based system trains ResNet-50 with a minibatch size of\n8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using\ncommodity hardware, our implementation achieves ~90% scaling efficiency when\nmoving from 8 to 256 GPUs. Our findings enable training visual recognition\nmodels on internet-scale data with high efficiency. \n\n"}
{"id": "1706.03459", "contents": "Title: Optimal Auctions through Deep Learning: Advances in Differentiable\n  Economics Abstract: Designing an incentive compatible auction that maximizes expected revenue is\nan intricate task. The single-item case was resolved in a seminal piece of work\nby Myerson in 1981, but more than 40 years later a full analytical\nunderstanding of the optimal design still remains elusive for settings with two\nor more items. In this work, we initiate the exploration of the use of tools\nfrom deep learning for the automated design of optimal auctions. We model an\nauction as a multi-layer neural network, frame optimal auction design as a\nconstrained learning problem, and show how it can be solved using standard\nmachine learning pipelines. In addition to providing generalization bounds, we\npresent extensive experimental results, recovering essentially all known\nsolutions that come from the theoretical analysis of optimal auction design\nproblems and obtaining novel mechanisms for settings in which the optimal\nmechanism is unknown. \n\n"}
{"id": "1706.04972", "contents": "Title: Device Placement Optimization with Reinforcement Learning Abstract: The past few years have witnessed a growth in size and computational\nrequirements for training and inference with neural networks. Currently, a\ncommon approach to address these requirements is to use a heterogeneous\ndistributed environment with a mixture of hardware devices such as CPUs and\nGPUs. Importantly, the decision of placing parts of the neural models on\ndevices is often made by human experts based on simple heuristics and\nintuitions. In this paper, we propose a method which learns to optimize device\nplacement for TensorFlow computational graphs. Key to our method is the use of\na sequence-to-sequence model to predict which subsets of operations in a\nTensorFlow graph should run on which of the available devices. The execution\ntime of the predicted placements is then used as the reward signal to optimize\nthe parameters of the sequence-to-sequence model. Our main result is that on\nInception-V3 for ImageNet classification, and on RNN LSTM, for language\nmodeling and neural machine translation, our model finds non-trivial device\nplacements that outperform hand-crafted heuristics and traditional algorithmic\nmethods. \n\n"}
{"id": "1706.05081", "contents": "Title: Approximate Best-Response Dynamics in Random Interference Games Abstract: In this paper we develop a novel approach to the convergence of Best-Response\nDynamics for the family of interference games. Interference games represent the\nfundamental resource allocation conflict between users of the radio spectrum.\nIn contrast to congestion games, interference games are generally not potential\ngames. Therefore, proving the convergence of the best-response dynamics to a\nNash equilibrium in these games requires new techniques. We suggest a model for\nrandom interference games, based on the long term fading governed by the\nplayers' geometry. Our goal is to prove convergence of the approximate\nbest-response dynamics with high probability with respect to the randomized\ngame. We embrace the asynchronous model in which the acting player is chosen at\neach stage at random. In our approximate best-response dynamics, the action of\na deviating player is chosen at random among all the approximately best ones.\nWe show that with high probability, with respect to the players' geometry and\nasymptotically with the number of players, each action increases the expected\nsocial-welfare (sum of achievable rates). Hence, the induced sum-rate process\nis a submartingale. Based on the Martingale Convergence Theorem, we prove\nconvergence of the strategy profile to an approximate Nash equilibrium with\ngood performance for asymptotically almost all interference games. We use the\nMarkovity of the induced sum-rate process to provide probabilistic bounds on\nthe convergence time. Finally, we demonstrate our results in simulated\nexamples. \n\n"}
{"id": "1706.05598", "contents": "Title: On the Optimization Landscape of Tensor Decompositions Abstract: Non-convex optimization with local search heuristics has been widely used in\nmachine learning, achieving many state-of-art results. It becomes increasingly\nimportant to understand why they can work for these NP-hard problems on typical\ndata. The landscape of many objective functions in learning has been\nconjectured to have the geometric property that \"all local optima are\n(approximately) global optima\", and thus they can be solved efficiently by\nlocal search algorithms. However, establishing such property can be very\ndifficult.\n  In this paper, we analyze the optimization landscape of the random\nover-complete tensor decomposition problem, which has many applications in\nunsupervised learning, especially in learning latent variable models. In\npractice, it can be efficiently solved by gradient ascent on a non-convex\nobjective. We show that for any small constant $\\epsilon > 0$, among the set of\npoints with function values $(1+\\epsilon)$-factor larger than the expectation\nof the function, all the local maxima are approximate global maxima.\nPreviously, the best-known result only characterizes the geometry in small\nneighborhoods around the true components. Our result implies that even with an\ninitialization that is barely better than the random guess, the gradient ascent\nalgorithm is guaranteed to solve this problem.\n  Our main technique uses Kac-Rice formula and random matrix theory. To our\nbest knowledge, this is the first time when Kac-Rice formula is successfully\napplied to counting the number of local minima of a highly-structured random\npolynomial with dependent coefficients. \n\n"}
{"id": "1706.06348", "contents": "Title: Frank-Wolfe Optimization for Symmetric-NMF under Simplicial Constraint Abstract: Symmetric nonnegative matrix factorization has found abundant applications in\nvarious domains by providing a symmetric low-rank decomposition of nonnegative\nmatrices. In this paper we propose a Frank-Wolfe (FW) solver to optimize the\nsymmetric nonnegative matrix factorization problem under a simplicial\nconstraint, which has recently been proposed for probabilistic clustering.\nCompared with existing solutions, this algorithm is simple to implement, and\nhas no hyperparameters to be tuned. Building on the recent advances of FW\nalgorithms in nonconvex optimization, we prove an $O(1/\\varepsilon^2)$\nconvergence rate to $\\varepsilon$-approximate KKT points, via a tight bound\n$\\Theta(n^2)$ on the curvature constant, which matches the best known result in\nunconstrained nonconvex setting using gradient methods. Numerical results\ndemonstrate the effectiveness of our algorithm. As a side contribution, we\nconstruct a simple nonsmooth convex problem where the FW algorithm fails to\nconverge to the optimum. This result raises an interesting question about\nnecessary conditions of the success of the FW algorithm on convex problems. \n\n"}
{"id": "1706.06569", "contents": "Title: A Unified Approach to Adaptive Regularization in Online and Stochastic\n  Optimization Abstract: We describe a framework for deriving and analyzing online optimization\nalgorithms that incorporate adaptive, data-dependent regularization, also\ntermed preconditioning. Such algorithms have been proven useful in stochastic\noptimization by reshaping the gradients according to the geometry of the data.\nOur framework captures and unifies much of the existing literature on adaptive\nonline methods, including the AdaGrad and Online Newton Step algorithms as well\nas their diagonal versions. As a result, we obtain new convergence proofs for\nthese algorithms that are substantially simpler than previous analyses. Our\nframework also exposes the rationale for the different preconditioned updates\nused in common stochastic optimization methods. \n\n"}
{"id": "1706.09869", "contents": "Title: Approximate Maximin Shares for Groups of Agents Abstract: We investigate the problem of fairly allocating indivisible goods among\ninterested agents using the concept of maximin share. Procaccia and Wang showed\nthat while an allocation that gives every agent at least her maximin share does\nnot necessarily exist, one that gives every agent at least $2/3$ of her share\nalways does. In this paper, we consider the more general setting where we\nallocate the goods to groups of agents. The agents in each group share the same\nset of goods even though they may have conflicting preferences. For two groups,\nwe characterize the cardinality of the groups for which a constant factor\napproximation of the maximin share is possible regardless of the number of\ngoods. We also show settings where an approximation is possible or impossible\nwhen there are several groups. \n\n"}
{"id": "1707.00208", "contents": "Title: Reconciling Selfish Routing with Social Good Abstract: Selfish routing is a central problem in algorithmic game theory, with one of\nthe principal applications being that of routing in road networks. Inspired by\nthe emergence of routing technologies and autonomous driving, we revisit\nselfish routing and consider three possible outcomes of it: (i)\n$\\theta$-Positive Nash Equilibrium flow, where every path that has non-zero\nflow on all of its edges has cost no greater than $\\theta$ times the cost of\nany other path, (ii) $\\theta$-Used Nash Equilibrium flow, where every used path\nthat appears in the path flow decomposition has cost no greater than $\\theta$\ntimes the cost of any other path, and (iii) $\\theta$-Envy Free flow, where\nevery path that appears in the path flow decomposition has cost no greater than\n$\\theta$ times the cost of any other path in the path flow decomposition. We\nfirst examine the relations of these outcomes among each other and then measure\ntheir possible impact on the network's performance. Afterwards, we examine the\ncomputational complexity of finding such flows of minimum social cost and give\na range for $\\theta$ for which this task is easy and a range for $\\theta$ for\nwhich this task is NP-hard. Finally, we propose deterministic strategies which,\nin a worst case approach, can be used by a central planner in order to provide\ngood such flows, and further introduce a natural idea for randomly routing\nplayers after giving them specific guarantees about their costs in the\nrandomized routing, as a tool for the central planner to implement a desired\nflow. \n\n"}
{"id": "1707.00524", "contents": "Title: Hashing over Predicted Future Frames for Informed Exploration of Deep\n  Reinforcement Learning Abstract: In deep reinforcement learning (RL) tasks, an efficient exploration mechanism\nshould be able to encourage an agent to take actions that lead to less frequent\nstates which may yield higher accumulative future return. However, both knowing\nabout the future and evaluating the frequentness of states are non-trivial\ntasks, especially for deep RL domains, where a state is represented by\nhigh-dimensional image frames. In this paper, we propose a novel informed\nexploration framework for deep RL, where we build the capability for an RL\nagent to predict over the future transitions and evaluate the frequentness for\nthe predicted future frames in a meaningful manner. To this end, we train a\ndeep prediction model to predict future frames given a state-action pair, and a\nconvolutional autoencoder model to hash over the seen frames. In addition, to\nutilize the counts derived from the seen frames to evaluate the frequentness\nfor the predicted frames, we tackle the challenge of matching the predicted\nfuture frames and their corresponding seen frames at the latent feature level.\nIn this way, we derive a reliable metric for evaluating the novelty of the\nfuture direction pointed by each action, and hence inform the agent to explore\nthe least frequent one. \n\n"}
{"id": "1707.01068", "contents": "Title: Maintaining cooperation in complex social dilemmas using deep\n  reinforcement learning Abstract: Social dilemmas are situations where individuals face a temptation to\nincrease their payoffs at a cost to total welfare. Building artificially\nintelligent agents that achieve good outcomes in these situations is important\nbecause many real world interactions include a tension between selfish\ninterests and the welfare of others. We show how to modify modern reinforcement\nlearning methods to construct agents that act in ways that are simple to\nunderstand, nice (begin by cooperating), provokable (try to avoid being\nexploited), and forgiving (try to return to mutual cooperation). We show both\ntheoretically and experimentally that such agents can maintain cooperation in\nMarkov social dilemmas. Our construction does not require training methods\nbeyond a modification of self-play, thus if an environment is such that good\nstrategies can be constructed in the zero-sum case (eg. Atari) then we can\nconstruct agents that solve social dilemmas in this environment. \n\n"}
{"id": "1707.01590", "contents": "Title: Fairness at Equilibrium in the Labor Market Abstract: Recent literature on computational notions of fairness has been broadly\ndivided into two distinct camps, supporting interventions that address either\nindividual-based or group-based fairness. Rather than privilege a single\ndefinition, we seek to resolve both within the particular domain of employment\ndiscrimination. To this end, we construct a dual labor market model composed of\na Temporary Labor Market, in which firm strategies are constrained to ensure\ngroup-level fairness, and a Permanent Labor Market, in which individual worker\nfairness is guaranteed. We show that such restrictions on hiring practices\ninduces an equilibrium that Pareto-dominates those arising from strategies that\nemploy statistical discrimination or a \"group-blind\" criterion. Individual\nworker reputations produce externalities for collective reputation, generating\na feedback loop termed a \"self-fulfilling prophecy.\" Our model produces its own\nfeedback loop, raising the collective reputation of an initially disadvantaged\ngroup via a fairness intervention that need not be permanent. Moreover, we show\nthat, contrary to popular assumption, the asymmetric equilibria resulting from\nhiring practices that disregard group-fairness may be immovable without\ntargeted intervention. The enduring nature of such equilibria that are both\ninequitable and Pareto inefficient suggest that fairness interventions are of\ncritical importance in moving the labor market to be more socially just and\nefficient. \n\n"}
{"id": "1707.01625", "contents": "Title: Optimal Vehicle Dispatching Schemes via Dynamic Pricing Abstract: Over the past few years, ride-sharing has emerged as an effective way to\nrelieve traffic congestion. A key problem for these platforms is to come up\nwith a revenue-optimal (or GMV-optimal) pricing scheme and an induced vehicle\ndispatching policy that incorporate geographic and temporal information. In\nthis paper, we aim to tackle this problem via an economic approach.\n  Modeled naively, the underlying optimization problem may be non-convex and\nthus hard to compute. To this end, we use a so-called \"ironing\" technique to\nconvert the problem into an equivalent convex optimization one via a clean\nMarkov decision process (MDP) formulation, where the states are the driver\ndistributions and the decision variables are the prices for each pair of\nlocations. Our main finding is an efficient algorithm that computes the exact\nrevenue-optimal (or GMV-optimal) randomized pricing schemes. We characterize\nthe optimal solution of the MDP by a primal-dual analysis of a corresponding\nconvex program. We also conduct empirical evaluations of our solution through\nreal data of a major ride-sharing platform and show its advantages over fixed\npricing schemes as well as several prevalent surge-based pricing schemes. \n\n"}
{"id": "1707.02444", "contents": "Title: Global optimality conditions for deep neural networks Abstract: We study the error landscape of deep linear and nonlinear neural networks\nwith the squared error loss. Minimizing the loss of a deep linear neural\nnetwork is a nonconvex problem, and despite recent progress, our understanding\nof this loss surface is still incomplete. For deep linear networks, we present\nnecessary and sufficient conditions for a critical point of the risk function\nto be a global minimum. Surprisingly, our conditions provide an efficiently\ncheckable test for global optimality, while such tests are typically\nintractable in nonconvex optimization. We further extend these results to deep\nnonlinear neural networks and prove similar sufficient conditions for global\noptimality, albeit in a more limited function space setting. \n\n"}
{"id": "1707.04428", "contents": "Title: Satiation in Fisher Markets and Approximation of Nash Social Welfare Abstract: We study linear Fisher markets with satiation. In these markets, sellers have\nearning limits and buyers have utility limits. Beyond natural applications in\neconomics, these markets arise in the context of maximizing Nash social welfare\nwhen allocating indivisible items to agents. In contrast to markets with either\nearning or utility limits, markets with both limits have not been studied\nbefore. They turn out to have fundamentally different properties.\n  In general, the existence of competitive equilibria is not guaranteed. We\nidentify a natural property of markets (termed money clearing) that implies\nexistence. We show that the set of equilibria is not always convex, answering a\nquestion of Cole et al. [EC'17]. We design an FPTAS to compute an approximate\nequilibrium and prove that the problem of computing an exact equilibrium lies\nin the intersection of complexity classes PLS and PPAD. For a constant number\nof buyers or goods, we give a polynomial-time algorithm to compute an exact\nequilibrium.\n  We show how (approximate) equilibria can be rounded and provide the first\nconstant-factor approximation algorithm (with a factor of 2.404) for maximizing\nNash social welfare when agents have budget-additive valuations. Finally, we\nsignificantly improve the approximation hardness for additive valuations to\n\\sqrt{8/7} > 1.069 (over 1.00008 by Lee [IPL'17]). \n\n"}
{"id": "1707.05729", "contents": "Title: Robust Bayesian Optimization with Student-t Likelihood Abstract: Bayesian optimization has recently attracted the attention of the automatic\nmachine learning community for its excellent results in hyperparameter tuning.\nBO is characterized by the sample efficiency with which it can optimize\nexpensive black-box functions. The efficiency is achieved in a similar fashion\nto the learning to learn methods: surrogate models (typically in the form of\nGaussian processes) learn the target function and perform intelligent sampling.\nThis surrogate model can be applied even in the presence of noise; however, as\nwith most regression methods, it is very sensitive to outlier data. This can\nresult in erroneous predictions and, in the case of BO, biased and inefficient\nexploration. In this work, we present a GP model that is robust to outliers\nwhich uses a Student-t likelihood to segregate outliers and robustly conduct\nBayesian optimization. We present numerical results evaluating the proposed\nmethod in both artificial functions and real problems. \n\n"}
{"id": "1707.05947", "contents": "Title: Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical\n  Viewpoints Abstract: Algorithm-dependent generalization error bounds are central to statistical\nlearning theory. A learning algorithm may use a large hypothesis space, but the\nlimited number of iterations controls its model capacity and generalization\nerror. The impacts of stochastic gradient methods on generalization error for\nnon-convex learning problems not only have important theoretical consequences,\nbut are also critical to generalization errors of deep learning.\n  In this paper, we study the generalization errors of Stochastic Gradient\nLangevin Dynamics (SGLD) with non-convex objectives. Two theories are proposed\nwith non-asymptotic discrete-time analysis, using Stability and PAC-Bayesian\nresults respectively. The stability-based theory obtains a bound of\n$O\\left(\\frac{1}{n}L\\sqrt{\\beta T_k}\\right)$, where $L$ is uniform Lipschitz\nparameter, $\\beta$ is inverse temperature, and $T_k$ is aggregated step sizes.\nFor PAC-Bayesian theory, though the bound has a slower $O(1/\\sqrt{n})$ rate,\nthe contribution of each step is shown with an exponentially decaying factor by\nimposing $\\ell^2$ regularization, and the uniform Lipschitz constant is also\nreplaced by actual norms of gradients along trajectory. Our bounds have no\nimplicit dependence on dimensions, norms or other capacity measures of\nparameter, which elegantly characterizes the phenomenon of \"Fast Training\nGuarantees Generalization\" in non-convex settings. This is the first\nalgorithm-dependent result with reasonable dependence on aggregated step sizes\nfor non-convex learning, and has important implications to statistical learning\naspects of stochastic gradient methods in complicated models such as deep\nlearning. \n\n"}
{"id": "1707.06203", "contents": "Title: Imagination-Augmented Agents for Deep Reinforcement Learning Abstract: We introduce Imagination-Augmented Agents (I2As), a novel architecture for\ndeep reinforcement learning combining model-free and model-based aspects. In\ncontrast to most existing model-based reinforcement learning and planning\nmethods, which prescribe how a model should be used to arrive at a policy, I2As\nlearn to interpret predictions from a learned environment model to construct\nimplicit plans in arbitrary ways, by using the predictions as additional\ncontext in deep policy networks. I2As show improved data efficiency,\nperformance, and robustness to model misspecification compared to several\nbaselines. \n\n"}
{"id": "1707.06920", "contents": "Title: Evolution Reinforces Cooperation with the Emergence of Self-Recognition\n  Mechanisms: an empirical study of the Moran process for the iterated\n  Prisoner's dilemma Abstract: We present insights and empirical results from an extensive numerical study\nof the evolutionary dynamics of the iterated prisoner's dilemma. Fixation\nprobabilities for Moran processes are obtained for all pairs of 164 different\nstrategies including classics such as TitForTat, zero determinant strategies,\nand many more sophisticated strategies. Players with long memories and\nsophisticated behaviours outperform many strategies that perform well in a two\nplayer setting. Moreover we introduce several strategies trained with\nevolutionary algorithms to excel at the Moran process. These strategies are\nexcellent invaders and resistors of invasion and in some cases naturally evolve\nhandshaking mechanisms to resist invasion. The best invaders were those trained\nto maximize total payoff while the best resistors invoke handshake mechanisms.\nThis suggests that while maximizing individual payoff can lead to the evolution\nof cooperation through invasion, the relatively weak invasion resistance of\npayoff maximizing strategies are not as evolutionarily stable as strategies\nemploying handshake mechanisms. \n\n"}
{"id": "1707.08761", "contents": "Title: Self-confirming Games: Unawareness, Discovery, and Equilibrium Abstract: Equilibrium notions for games with unawareness in the literature cannot be\ninterpreted as steady-states of a learning process because players may discover\nnovel actions during play. In this sense, many games with unawareness are\n\"self-destroying\" as a player's representation of the game must change after\nplaying it once. We define discovery processes where at each state there is an\nextensive-form game with unawareness that together with the players' play\ndetermines the transition to possibly another extensive-form games with\nunawareness in which players are now aware of actions that they have previously\ndiscovered. A discovery process is rationalizable if players play\nextensive-form rationalizable strategies in each game with unawareness. We show\nthat for any game with unawareness there is a rationalizable discovery process\nthat leads to a self-confirming game that possesses an extensive-form\nrationalizable self-confirming equilibrium. This notion of equilibrium can be\ninterpreted as steady-state of a learning and discovery process. \n\n"}
{"id": "1707.09132", "contents": "Title: Network Formation in the Sky: Unmanned Aerial Vehicles for Multi-hop\n  Wireless Backhauling Abstract: To reap the benefits of dense small base station (SBS) deployment, innovative\nbackhaul solutions are needed in order to manage scenarios in which high-speed\nground backhaul links are either unavailable or limited in capacity. In this\npaper, a novel backhaul scheme that utilizes unmanned aerial vehicles (UAVs) as\nan on-demand flying network linking ground SBSs and the core network is\nproposed. The design of the aerial backhaul scheme is formulated as a network\nformation game among UAVs that seek to form a multi-hop backhaul network in the\nair. To solve this game, a myopic network formation algorithm which reaches a\npairwise stable network upon convergence, is introduced. The proposed network\nformation algorithm enables the UAVs to form the necessary multi-hop backhaul\nnetwork in a decentralized manner thus adapting the backhaul architecture to\nthe dynamics of the network. Simulation results show that the proposed network\nformation algorithm achieves substantial performance gains in terms of both\nrate and delay reaching, respectively, up to 40% and 41% compared to the\nformation of direct communication links with the gateway node (for a network\nwith 15 UAVs). \n\n"}
{"id": "1708.01208", "contents": "Title: Semantic Augmented Reality Environment with Material-Aware Physical\n  Interactions Abstract: In Augmented Reality (AR) environment, realistic interactions between the\nvirtual and real objects play a crucial role in user experience. Much of recent\nadvances in AR has been largely focused on developing geometry-aware\nenvironment, but little has been done in dealing with interactions at the\nsemantic level. High-level scene understanding and semantic descriptions in AR\nwould allow effective design of complex applications and enhanced user\nexperience. In this paper, we present a novel approach and a prototype system\nthat enables the deeper understanding of semantic properties of the real world\nenvironment, so that realistic physical interactions between the real and the\nvirtual objects can be generated. A material-aware AR environment has been\ncreated based on the deep material learning using a fully convolutional network\n(FCN). The state-of-the-art dense Simultaneous Localisation and Mapping (SLAM)\nhas been used for the semantic mapping. Together with efficient accelerated 3D\nray casting, natural and realistic physical interactions are generated for\ninteractive AR games. Our approach has significant impact on the future\ndevelopment of advanced AR systems and applications. \n\n"}
{"id": "1708.01384", "contents": "Title: Variance-Reduced Stochastic Learning by Networked Agents under Random\n  Reshuffling Abstract: A new amortized variance-reduced gradient (AVRG) algorithm was developed in\n\\cite{ying2017convergence}, which has constant storage requirement in\ncomparison to SAGA and balanced gradient computations in comparison to SVRG.\nOne key advantage of the AVRG strategy is its amenability to decentralized\nimplementations. In this work, we show how AVRG can be extended to the network\ncase where multiple learning agents are assumed to be connected by a graph\ntopology. In this scenario, each agent observes data that is spatially\ndistributed and all agents are only allowed to communicate with direct\nneighbors. Moreover, the amount of data observed by the individual agents may\ndiffer drastically. For such situations, the balanced gradient computation\nproperty of AVRG becomes a real advantage in reducing idle time caused by\nunbalanced local data storage requirements, which is characteristic of other\nreduced-variance gradient algorithms. The resulting diffusion-AVRG algorithm is\nshown to have linear convergence to the exact solution, and is much more memory\nefficient than other alternative algorithms. In addition, we propose a\nmini-batch strategy to balance the communication and computation efficiency for\ndiffusion-AVRG. When a proper batch size is employed, it is observed in\nsimulations that diffusion-AVRG is more computationally efficient than exact\ndiffusion or EXTRA while maintaining almost the same communication efficiency. \n\n"}
{"id": "1708.02105", "contents": "Title: Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls Abstract: We propose a rank-$k$ variant of the classical Frank-Wolfe algorithm to solve\nconvex optimization over a trace-norm ball. Our algorithm replaces the top\nsingular-vector computation ($1$-SVD) in Frank-Wolfe with a top-$k$\nsingular-vector computation ($k$-SVD), which can be done by repeatedly applying\n$1$-SVD $k$ times. Alternatively, our algorithm can be viewed as a rank-$k$\nrestricted version of projected gradient descent. We show that our algorithm\nhas a linear convergence rate when the objective function is smooth and\nstrongly convex, and the optimal solution has rank at most $k$. This improves\nthe convergence rate and the total time complexity of the Frank-Wolfe method\nand its variants. \n\n"}
{"id": "1708.03735", "contents": "Title: Sparse Coding and Autoencoders Abstract: In \"Dictionary Learning\" one tries to recover incoherent matrices $A^* \\in\n\\mathbb{R}^{n \\times h}$ (typically overcomplete and whose columns are assumed\nto be normalized) and sparse vectors $x^* \\in \\mathbb{R}^h$ with a small\nsupport of size $h^p$ for some $0 <p < 1$ while having access to observations\n$y \\in \\mathbb{R}^n$ where $y = A^*x^*$. In this work we undertake a rigorous\nanalysis of whether gradient descent on the squared loss of an autoencoder can\nsolve the dictionary learning problem. The \"Autoencoder\" architecture we\nconsider is a $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ mapping with a single\nReLU activation layer of size $h$.\n  Under very mild distributional assumptions on $x^*$, we prove that the norm\nof the expected gradient of the standard squared loss function is\nasymptotically (in sparse code dimension) negligible for all points in a small\nneighborhood of $A^*$. This is supported with experimental evidence using\nsynthetic data. We also conduct experiments to suggest that $A^*$ is a local\nminimum. Along the way we prove that a layer of ReLU gates can be set up to\nautomatically recover the support of the sparse codes. This property holds\nindependent of the loss function. We believe that it could be of independent\ninterest. \n\n"}
{"id": "1708.04357", "contents": "Title: Graph Classification via Deep Learning with Virtual Nodes Abstract: Learning representation for graph classification turns a variable-size graph\ninto a fixed-size vector (or matrix). Such a representation works nicely with\nalgebraic manipulations. Here we introduce a simple method to augment an\nattributed graph with a virtual node that is bidirectionally connected to all\nexisting nodes. The virtual node represents the latent aspects of the graph,\nwhich are not immediately available from the attributes and local connectivity\nstructures. The expanded graph is then put through any node representation\nmethod. The representation of the virtual node is then the representation of\nthe entire graph. In this paper, we use the recently introduced Column Network\nfor the expanded graph, resulting in a new end-to-end graph classification\nmodel dubbed Virtual Column Network (VCN). The model is validated on two tasks:\n(i) predicting bio-activity of chemical compounds, and (ii) finding software\nvulnerability from source code. Results demonstrate that VCN is competitive\nagainst well-established rivals. \n\n"}
{"id": "1708.04699", "contents": "Title: Mechanism Redesign Abstract: This paper develops the theory of mechanism redesign by which an auctioneer\ncan reoptimize an auction based on bid data collected from previous iterations\nof the auction on bidders from the same market. We give a direct method for\nestimation of the revenue of a counterfactual auction from the bids in the\ncurrent auction. The estimator is a simple weighted order statistic of the bids\nand has the optimal error rate. Two applications of our estimator are A/B\ntesting (a.k.a., randomized controlled trials) and instrumented optimization\n(i.e., revenue optimization subject to being able to do accurate inference of\nany counterfactual auction revenue). \n\n"}
{"id": "1708.04733", "contents": "Title: Geometric Enclosing Networks Abstract: Training model to generate data has increasingly attracted research attention\nand become important in modern world applications. We propose in this paper a\nnew geometry-based optimization approach to address this problem. Orthogonal to\ncurrent state-of-the-art density-based approaches, most notably VAE and GAN, we\npresent a fresh new idea that borrows the principle of minimal enclosing ball\nto train a generator G\\left(\\bz\\right) in such a way that both training and\ngenerated data, after being mapped to the feature space, are enclosed in the\nsame sphere. We develop theory to guarantee that the mapping is bijective so\nthat its inverse from feature space to data space results in expressive\nnonlinear contours to describe the data manifold, hence ensuring data generated\nare also lying on the data manifold learned from training data. Our model\nenjoys a nice geometric interpretation, hence termed Geometric Enclosing\nNetworks (GEN), and possesses some key advantages over its rivals, namely\nsimple and easy-to-control optimization formulation, avoidance of mode\ncollapsing and efficiently learn data manifold representation in a completely\nunsupervised manner. We conducted extensive experiments on synthesis and\nreal-world datasets to illustrate the behaviors, strength and weakness of our\nproposed GEN, in particular its ability to handle multi-modal data and quality\nof generated data. \n\n"}
{"id": "1708.04801", "contents": "Title: Weighted parallel SGD for distributed unbalanced-workload training\n  system Abstract: Stochastic gradient descent (SGD) is a popular stochastic optimization method\nin machine learning. Traditional parallel SGD algorithms, e.g., SimuParallel\nSGD, often require all nodes to have the same performance or to consume equal\nquantities of data. However, these requirements are difficult to satisfy when\nthe parallel SGD algorithms run in a heterogeneous computing environment;\nlow-performance nodes will exert a negative influence on the final result. In\nthis paper, we propose an algorithm called weighted parallel SGD (WP-SGD).\nWP-SGD combines weighted model parameters from different nodes in the system to\nproduce the final output. WP-SGD makes use of the reduction in standard\ndeviation to compensate for the loss from the inconsistency in performance of\nnodes in the cluster, which means that WP-SGD does not require that all nodes\nconsume equal quantities of data. We also analyze the theoretical feasibility\nof running two other parallel SGD algorithms combined with WP-SGD in a\nheterogeneous environment. The experimental results show that WP-SGD\nsignificantly outperforms the traditional parallel SGD algorithms on\ndistributed training systems with an unbalanced workload. \n\n"}
{"id": "1708.05978", "contents": "Title: Stochastic Primal-Dual Proximal ExtraGradient Descent for Compositely\n  Regularized Optimization Abstract: We consider a wide range of regularized stochastic minimization problems with\ntwo regularization terms, one of which is composed with a linear function. This\noptimization model abstracts a number of important applications in artificial\nintelligence and machine learning, such as fused Lasso, fused logistic\nregression, and a class of graph-guided regularized minimization. The\ncomputational challenges of this model are in two folds. On one hand, the\nclosed-form solution of the proximal mapping associated with the composed\nregularization term or the expected objective function is not available. On the\nother hand, the calculation of the full gradient of the expectation in the\nobjective is very expensive when the number of input data samples is\nconsiderably large. To address these issues, we propose a stochastic variant of\nextra-gradient type methods, namely \\textsf{Stochastic Primal-Dual Proximal\nExtraGradient descent (SPDPEG)}, and analyze its convergence property for both\nconvex and strongly convex objectives. For general convex objectives, the\nuniformly average iterates generated by \\textsf{SPDPEG} converge in expectation\nwith $O(1/\\sqrt{t})$ rate. While for strongly convex objectives, the uniformly\nand non-uniformly average iterates generated by \\textsf{SPDPEG} converge with\n$O(\\log(t)/t)$ and $O(1/t)$ rates, respectively. The order of the rate of the\nproposed algorithm is known to match the best convergence rate for first-order\nstochastic algorithms. Experiments on fused logistic regression and\ngraph-guided regularized logistic regression problems show that the proposed\nalgorithm performs very efficiently and consistently outperforms other\ncompeting algorithms. \n\n"}
{"id": "1708.07580", "contents": "Title: The Expanding Approvals Rule: Improving Proportional Representation and\n  Monotonicity Abstract: Proportional representation (PR) is often discussed in voting settings as a\nmajor desideratum. For the past century or so, it is common both in practice\nand in the academic literature to jump to single transferable vote (STV) as the\nsolution for achieving PR. Some of the most prominent electoral reform\nmovements around the globe are pushing for the adoption of STV. It has been\ntermed a major open problem to design a voting rule that satisfies the same PR\nproperties as STV and better monotonicity properties. In this paper, we first\npresent a taxonomy of proportional representation axioms for general weak order\npreferences, some of which generalise and strengthen previously introduced\nconcepts. We then present a rule called Expanding Approvals Rule (EAR) that\nsatisfies properties stronger than the central PR axiom satisfied by STV, can\nhandle indifferences in a convenient and computationally efficient manner, and\nalso satisfies better candidate monotonicity properties. In view of this, our\nproposed rule seems to be a compelling solution for achieving proportional\nrepresentation in voting settings. \n\n"}
{"id": "1708.08819", "contents": "Title: Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields Abstract: Generative adversarial networks (GANs) evolved into one of the most\nsuccessful unsupervised techniques for generating realistic images. Even though\nit has recently been shown that GAN training converges, GAN models often end up\nin local Nash equilibria that are associated with mode collapse or otherwise\nfail to model the target distribution. We introduce Coulomb GANs, which pose\nthe GAN learning problem as a potential field of charged particles, where\ngenerated samples are attracted to training set samples but repel each other.\nThe discriminator learns a potential field while the generator decreases the\nenergy by moving its samples along the vector (force) field determined by the\ngradient of the potential field. Through decreasing the energy, the GAN model\nlearns to generate samples according to the whole target distribution and does\nnot only cover some of its modes. We prove that Coulomb GANs possess only one\nNash equilibrium which is optimal in the sense that the model distribution\nequals the target distribution. We show the efficacy of Coulomb GANs on a\nvariety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of\nthe art and produce a previously unseen variety of different samples. \n\n"}
{"id": "1708.09441", "contents": "Title: Incorporating Feedback into Tree-based Anomaly Detection Abstract: Anomaly detectors are often used to produce a ranked list of statistical\nanomalies, which are examined by human analysts in order to extract the actual\nanomalies of interest. Unfortunately, in realworld applications, this process\ncan be exceedingly difficult for the analyst since a large fraction of\nhigh-ranking anomalies are false positives and not interesting from the\napplication perspective. In this paper, we aim to make the analyst's job easier\nby allowing for analyst feedback during the investigation process. Ideally, the\nfeedback influences the ranking of the anomaly detector in a way that reduces\nthe number of false positives that must be examined before discovering the\nanomalies of interest. In particular, we introduce a novel technique for\nincorporating simple binary feedback into tree-based anomaly detectors. We\nfocus on the Isolation Forest algorithm as a representative tree-based anomaly\ndetector, and show that we can significantly improve its performance by\nincorporating feedback, when compared with the baseline algorithm that does not\nincorporate feedback. Our technique is simple and scales well as the size of\nthe data increases, which makes it suitable for interactive discovery of\nanomalies in large datasets. \n\n"}
{"id": "1709.02100", "contents": "Title: Dynamics and Coalitions in Sequential Games Abstract: We consider N-player non-zero sum games played on finite trees (i.e.,\nsequential games), in which the players have the right to repeatedly update\ntheir respective strategies (for instance, to improve the outcome wrt to the\ncurrent strategy profile). This generates a dynamics in the game which may\neventually stabilise to a Nash Equilibrium (as with Kukushkin's lazy\nimprovement), and we argue that it is interesting to study the conditions that\nguarantee such a dynamics to terminate.\n  We build on the works of Le Roux and Pauly who have studied extensively one\nsuch dynamics, namely the Lazy Improvement Dynamics. We extend these works by\nfirst defining a turn-based dynamics, proving that it terminates on subgame\nperfect equilibria, and showing that several variants do not terminate. Second,\nwe define a variant of Kukushkin's lazy improvement where the players may now\nform coalitions to change strategies. We show how properties of the players'\npreferences on the outcomes affect the termination of this dynamics, and we\nthereby characterise classes of games where it always terminates (in particular\ntwo-player games). \n\n"}
{"id": "1709.02726", "contents": "Title: A Modular Analysis of Adaptive (Non-)Convex Optimization: Optimism,\n  Composite Objectives, and Variational Bounds Abstract: Recently, much work has been done on extending the scope of online learning\nand incremental stochastic optimization algorithms. In this paper we contribute\nto this effort in two ways: First, based on a new regret decomposition and a\ngeneralization of Bregman divergences, we provide a self-contained, modular\nanalysis of the two workhorses of online learning: (general) adaptive versions\nof Mirror Descent (MD) and the Follow-the-Regularized-Leader (FTRL) algorithms.\nThe analysis is done with extra care so as not to introduce assumptions not\nneeded in the proofs and allows to combine, in a straightforward way, different\nalgorithmic ideas (e.g., adaptivity, optimism, implicit updates) and learning\nsettings (e.g., strongly convex or composite objectives). This way we are able\nto reprove, extend and refine a large body of the literature, while keeping the\nproofs concise. The second contribution is a byproduct of this careful\nanalysis: We present algorithms with improved variational bounds for smooth,\ncomposite objectives, including a new family of optimistic MD algorithms with\nonly one projection step per round. Furthermore, we provide a simple extension\nof adaptive regret bounds to practically relevant non-convex problem settings\nwith essentially no extra effort. \n\n"}
{"id": "1709.04176", "contents": "Title: Computing the Shapley Value in Allocation Problems: Approximations and\n  Bounds, with an Application to the Italian VQR Research Assessment Program Abstract: In allocation problems, a given set of goods are assigned to agents in such a\nway that the social welfare is maximised, that is, the largest possible global\nworth is achieved. When goods are indivisible, it is possible to use money\ncompensation to perform a fair allocation taking into account the actual\ncontribution of all agents to the social welfare. Coalitional games provide a\nformal mathematical framework to model such problems, in particular the Shapley\nvalue is a solution concept widely used for assigning worths to agents in a\nfair way. Unfortunately, computing this value is a $\\#{\\rm P}$-hard problem, so\nthat applying this good theoretical notion is often quite difficult in\nreal-world problems.\n  We describe useful properties that allow us to greatly simplify the instances\nof allocation problems, without affecting the Shapley value of any player.\nMoreover, we propose algorithms for computing lower bounds and upper bounds of\nthe Shapley value, which in some cases provide the exact result and that can be\ncombined with approximation algorithms.\n  The proposed techniques have been implemented and tested on a real-world\napplication of allocation problems, namely, the Italian research assessment\nprogram, known as VQR. For the large university considered in the experiments,\nthe problem involves thousands of agents and goods (here, researchers and their\nresearch products). The algorithms described in the paper are able to compute\nthe Shapley value for most of those agents, and to get a good approximation of\nthe Shapley value for all of them. \n\n"}
{"id": "1709.04326", "contents": "Title: Learning with Opponent-Learning Awareness Abstract: Multi-agent settings are quickly gathering importance in machine learning.\nThis includes a plethora of recent work on deep multi-agent reinforcement\nlearning, but also can be extended to hierarchical RL, generative adversarial\nnetworks and decentralised optimisation. In all these settings the presence of\nmultiple learning agents renders the training problem non-stationary and often\nleads to unstable training or undesired final results. We present Learning with\nOpponent-Learning Awareness (LOLA), a method in which each agent shapes the\nanticipated learning of the other agents in the environment. The LOLA learning\nrule includes a term that accounts for the impact of one agent's policy on the\nanticipated parameter update of the other agents. Results show that the\nencounter of two LOLA agents leads to the emergence of tit-for-tat and\ntherefore cooperation in the iterated prisoners' dilemma, while independent\nlearning does not. In this domain, LOLA also receives higher payouts compared\nto a naive learner, and is robust against exploitation by higher order\ngradient-based methods. Applied to repeated matching pennies, LOLA agents\nconverge to the Nash equilibrium. In a round robin tournament we show that LOLA\nagents successfully shape the learning of a range of multi-agent learning\nalgorithms from literature, resulting in the highest average returns on the\nIPD. We also show that the LOLA update rule can be efficiently calculated using\nan extension of the policy gradient estimator, making the method suitable for\nmodel-free RL. The method thus scales to large parameter and input spaces and\nnonlinear function approximators. We apply LOLA to a grid world task with an\nembedded social dilemma using recurrent policies and opponent modelling. By\nexplicitly considering the learning of the other agent, LOLA agents learn to\ncooperate out of self-interest. The code is at github.com/alshedivat/lola. \n\n"}
{"id": "1709.04555", "contents": "Title: Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network Abstract: The prediction of organic reaction outcomes is a fundamental problem in\ncomputational chemistry. Since a reaction may involve hundreds of atoms, fully\nexploring the space of possible transformations is intractable. The current\nsolution utilizes reaction templates to limit the space, but it suffers from\ncoverage and efficiency issues. In this paper, we propose a template-free\napproach to efficiently explore the space of product molecules by first\npinpointing the reaction center -- the set of nodes and edges where graph edits\noccur. Since only a small number of atoms contribute to reaction center, we can\ndirectly enumerate candidate products. The generated candidates are scored by a\nWeisfeiler-Lehman Difference Network that models high-order interactions\nbetween changes occurring at nodes across the molecule. Our framework\noutperforms the top-performing template-based approach with a 10\\% margin,\nwhile running orders of magnitude faster. Finally, we demonstrate that the\nmodel accuracy rivals the performance of domain experts. \n\n"}
{"id": "1709.04569", "contents": "Title: REMOTEGATE: Incentive-Compatible Remote Configuration of Security\n  Gateways Abstract: Imagine that a malicious hacker is trying to attack a server over the\nInternet and the server wants to block the attack packets as close to their\npoint of origin as possible. However, the security gateway ahead of the source\nof attack is untrusted. How can the server block the attack packets through\nthis gateway? In this paper, we introduce REMOTEGATE, a trustworthy mechanism\nfor allowing any party (server) on the Internet to configure a security gateway\nowned by a second party, at a certain agreed upon reward that the former pays\nto the latter for its service. We take an interactive incentive-compatible\napproach, for the case when both the server and the gateway are rational, to\ndevise a protocol that will allow the server to help the security gateway\ngenerate and deploy a policy rule that filters the attack packets before they\nreach the server. The server will reward the gateway only when the latter can\nsuccessfully verify that it has generated and deployed the correct rule for the\nissue. This mechanism will enable an Internet-scale approach to improving\nsecurity and privacy, backed by digital payment incentives. \n\n"}
{"id": "1709.06293", "contents": "Title: Sparse Markov Decision Processes with Causal Sparse Tsallis Entropy\n  Regularization for Reinforcement Learning Abstract: In this paper, a sparse Markov decision process (MDP) with novel causal\nsparse Tsallis entropy regularization is proposed.The proposed policy\nregularization induces a sparse and multi-modal optimal policy distribution of\na sparse MDP. The full mathematical analysis of the proposed sparse MDP is\nprovided.We first analyze the optimality condition of a sparse MDP. Then, we\npropose a sparse value iteration method which solves a sparse MDP and then\nprove the convergence and optimality of sparse value iteration using the Banach\nfixed point theorem. The proposed sparse MDP is compared to soft MDPs which\nutilize causal entropy regularization. We show that the performance error of a\nsparse MDP has a constant bound, while the error of a soft MDP increases\nlogarithmically with respect to the number of actions, where this performance\nerror is caused by the introduced regularization term. In experiments, we apply\nsparse MDPs to reinforcement learning problems. The proposed method outperforms\nexisting methods in terms of the convergence speed and performance. \n\n"}
{"id": "1709.07955", "contents": "Title: On the Competition Complexity of Dynamic Mechanism Design Abstract: The Competition Complexity of an auction measures how much competition is\nneeded for the revenue of a simple auction to surpass the optimal revenue. A\nclassic result from auction theory by Bulow and Klemperer [9], states that the\nCompetition Complexity of VCG, in the case of n i.i.d. buyers and a single\nitem, is 1, i.e., it is better to recruit one extra buyer and run a second\nprice auction than to learn exactly the buyers' underlying distribution and run\nthe revenue-maximizing auction tailored to this distribution.\n  In this paper we study the Competition Complexity of dynamic auctions.\nConsider the following setting: a monopolist is auctioning off m items in m\nconsecutive stages to n interested buyers. A buyer realizes her value for item\nk in the beginning of stage k. We prove that the Competition Complexity of\ndynamic auctions is at most 3n, and at least linear in n, even when the buyers'\nvalues are correlated across stages, under a monotone hazard rate assumption on\nthe stage (marginal) distributions. We also prove results on the number of\nadditional buyers necessary for VCG at every stage to be an\n{\\alpha}-approximation of the optimal revenue; we term this number the\n{\\alpha}-approximate Competition Complexity. As a corollary we provide the\nfirst results on prior-independent dynamic auctions. This is, to the best of\nour knowledge, the first non-trivial positive guarantees for simple ex-post IR\ndynamic auctions for correlated stages.\n  A key step towards proving bounds on the Competition Complexity is getting a\ngood benchmark/upper bound to the optimal revenue. To this end, we extend the\nrecent duality framework of Cai et al. [12] to dynamic settings. As an aside to\nour approach we obtain a revenue non-monotonicity lemma for dynamic auctions,\nwhich may be of independent interest. \n\n"}
{"id": "1709.08318", "contents": "Title: Hodge decomposition and the Shapley value of a cooperative game Abstract: We show that a cooperative game may be decomposed into a sum of component\ngames, one for each player, using the combinatorial Hodge decomposition on a\ngraph. This decomposition is shown to satisfy certain efficiency, null-player,\nsymmetry, and linearity properties. Consequently, we obtain a new\ncharacterization of the classical Shapley value as the value of the grand\ncoalition in each player's component game. We also relate this decomposition to\na least-squares problem involving inessential games (in a similar spirit to\nprevious work on least-squares and minimum-norm solution concepts) and to the\ngraph Laplacian. Finally, we generalize this approach to games with weights\nand/or constraints on coalition formation. \n\n"}
{"id": "1709.08441", "contents": "Title: Uncertainty in Multi-Commodity Routing Networks: When does it help? Abstract: We study the equilibrium behavior in a multi-commodity selfish routing game\nwith many types of uncertain users where each user over- or under-estimates\ntheir congestion costs by a multiplicative factor. Surprisingly, we find that\nuncertainties in different directions have qualitatively distinct impacts on\nequilibria. Namely, contrary to the usual notion that uncertainty increases\ninefficiencies, network congestion actually decreases when users over-estimate\ntheir costs. On the other hand, under-estimation of costs leads to increased\ncongestion. We apply these results to urban transportation networks, where\ndrivers have different estimates about the cost of congestion. In light of the\ndynamic pricing policies aimed at tackling congestion, our results indicate\nthat users' perception of these prices can significantly impact the policy's\nefficacy, and \"caution in the face of uncertainty\" leads to favorable network\nconditions. \n\n"}
{"id": "1709.08991", "contents": "Title: Reachability Switching Games Abstract: We study the problem of deciding the winner of reachability switching games\nfor zero-, one-, and two-player variants. Switching games provide a\ndeterministic analogue of stochastic games. We show that the zero-player case\nis NL-hard, the one-player case is NP-complete, and that the two-player case is\nPSPACE-hard and in EXPTIME. For the zero-player case, we also show P-hardness\nfor a succinctly-represented model that maintains the upper bound of NP $\\cap$\ncoNP. For the one- and two-player cases, our results hold in both the natural,\nexplicit model and succinctly-represented model. Our results show that the\nswitching variant of a game is harder in complexity-theoretic terms than the\ncorresponding stochastic version. \n\n"}
{"id": "1709.10122", "contents": "Title: Mechanism Design for Demand Response Programs with financial and\n  non-monetary (social) Incentives Abstract: Most demand management approaches with non-mandatory policies assume full\nusers' cooperation, which may not be the case given users' beliefs, needs and\npreferences. In this paper we propose a mechanism for demand management\nincluding incentives both with and without money. The mechanism is validated by\nmeans of simulation, modeling the consumers as a finite multiagent system which\nevolves until a stable state, and social incentives diffusion using opinion\ndynamics. \n\n"}
{"id": "1710.00996", "contents": "Title: Equilibrium Computation and Robust Optimization in Zero Sum Games with\n  Submodular Structure Abstract: We define a class of zero-sum games with combinatorial structure, where the\nbest response problem of one player is to maximize a submodular function. For\nexample, this class includes security games played on networks, as well as the\nproblem of robustly optimizing a submodular function over the worst case from a\nset of scenarios. The challenge in computing equilibria is that both players'\nstrategy spaces can be exponentially large. Accordingly, previous algorithms\nhave worst-case exponential runtime and indeed fail to scale up on practical\ninstances. We provide a pseudopolynomial-time algorithm which obtains a\nguaranteed $(1 - 1/e)^2$-approximate mixed strategy for the maximizing player.\nOur algorithm only requires access to a weakened version of a best response\noracle for the minimizing player which runs in polynomial time. Experimental\nresults for network security games and a robust budget allocation problem\nconfirm that our algorithm delivers near-optimal solutions and scales to much\nlarger instances than was previously possible. \n\n"}
{"id": "1710.01782", "contents": "Title: On the Tree Conjecture for the Network Creation Game Abstract: Selfish Network Creation focuses on modeling real world networks from a\ngame-theoretic point of view. One of the classic models by Fabrikant et al.\n[PODC'03] is the network creation game, where agents correspond to nodes in a\nnetwork which buy incident edges for the price of $\\alpha$ per edge to minimize\ntheir total distance to all other nodes. The model is well-studied but still\nhas intriguing open problems. The most famous conjectures state that the price\nof anarchy is constant for all $\\alpha$ and that for $\\alpha \\geq n$ all\nequilibrium networks are trees.\n  We introduce a novel technique for analyzing stable networks for high\nedge-price $\\alpha$ and employ it to improve on the best known bounds for both\nconjectures. In particular we show that for $\\alpha > 4n-13$ all equilibrium\nnetworks must be trees, which implies a constant price of anarchy for this\nrange of $\\alpha$. Moreover, we also improve the constant upper bound on the\nprice of anarchy for equilibrium trees. \n\n"}
{"id": "1710.03830", "contents": "Title: Inference on Auctions with Weak Assumptions on Information Abstract: Given a sample of bids from independent auctions, this paper examines the\nquestion of inference on auction fundamentals (e.g. valuation distributions,\nwelfare measures) under weak assumptions on information structure. The question\nis important as it allows us to learn about the valuation distribution in a\nrobust way, i.e., without assuming that a particular information structure\nholds across observations. We leverage the recent contributions of\n\\cite{Bergemann2013} in the robust mechanism design literature that exploit the\nlink between Bayesian Correlated Equilibria and Bayesian Nash Equilibria in\nincomplete information games to construct an econometrics framework for\nlearning about auction fundamentals using observed data on bids. We showcase\nour construction of identified sets in private value and common value auctions.\nOur approach for constructing these sets inherits the computational simplicity\nof solving for correlated equilibria: checking whether a particular valuation\ndistribution belongs to the identified set is as simple as determining whether\na {\\it linear} program is feasible. A similar linear program can be used to\nconstruct the identified set on various welfare measures and counterfactual\nobjects. For inference and to summarize statistical uncertainty, we propose\nnovel finite sample methods using tail inequalities that are used to construct\nconfidence regions on sets. We also highlight methods based on Bayesian\nbootstrap and subsampling. A set of Monte Carlo experiments show adequate\nfinite sample properties of our inference procedures. We illustrate our methods\nusing data from OCS auctions. \n\n"}
{"id": "1710.05739", "contents": "Title: On the Hardness of Inventory Management with Censored Demand Data Abstract: We consider a repeated newsvendor problem where the inventory manager has no\nprior information about the demand, and can access only censored/sales data. In\nanalogy to multi-armed bandit problems, the manager needs to simultaneously\n\"explore\" and \"exploit\" with her inventory decisions, in order to minimize the\ncumulative cost. We make no probabilistic assumptions---importantly,\nindependence or time stationarity---regarding the mechanism that creates the\ndemand sequence. Our goal is to shed light on the hardness of the problem, and\nto develop policies that perform well with respect to the regret criterion,\nthat is, the difference between the cumulative cost of a policy and that of the\nbest fixed action/static inventory decision in hindsight, uniformly over all\nfeasible demand sequences. We show that a simple randomized policy, termed the\nExponentially Weighted Forecaster, combined with a carefully designed cost\nestimator, achieves optimal scaling of the expected regret (up to logarithmic\nfactors) with respect to all three key primitives: the number of time periods,\nthe number of inventory decisions available, and the demand support. Through\nthis result, we derive an important insight: the benefit from \"information\nstalking\" as well as the cost of censoring are both negligible in this dynamic\nlearning problem, at least with respect to the regret criterion. Furthermore,\nwe modify the proposed policy in order to perform well in terms of the tracking\nregret, that is, using as benchmark the best sequence of inventory decisions\nthat switches a limited number of times. Numerical experiments suggest that the\nproposed approach outperforms existing ones (that are tailored to, or\nfacilitated by, time stationarity) on nonstationary demand models. Finally, we\nextend the proposed approach and its analysis to a \"combinatorial\" version of\nthe repeated newsvendor problem. \n\n"}
{"id": "1710.05895", "contents": "Title: Spectral Algorithms for Computing Fair Support Vector Machines Abstract: Classifiers and rating scores are prone to implicitly codifying biases, which\nmay be present in the training data, against protected classes (i.e., age,\ngender, or race). So it is important to understand how to design classifiers\nand scores that prevent discrimination in predictions. This paper develops\ncomputationally tractable algorithms for designing accurate but fair support\nvector machines (SVM's). Our approach imposes a constraint on the covariance\nmatrices conditioned on each protected class, which leads to a nonconvex\nquadratic constraint in the SVM formulation. We develop iterative algorithms to\ncompute fair linear and kernel SVM's, which solve a sequence of relaxations\nconstructed using a spectral decomposition of the nonconvex constraint. Its\neffectiveness in achieving high prediction accuracy while ensuring fairness is\nshown through numerical experiments on several data sets. \n\n"}
{"id": "1710.06026", "contents": "Title: Targeting Interventions in Networks Abstract: We study games in which a network mediates strategic spillovers and\nexternalities among the players. How does a planner optimally target\ninterventions that change individuals' private returns to investment? We\nanalyze this question by decomposing any intervention into orthogonal principal\ncomponents, which are determined by the network and are ordered according to\ntheir associated eigenvalues. There is a close connection between the nature of\nspillovers and the representation of various principal components in the\noptimal intervention. In games of strategic complements (substitutes),\ninterventions place more weight on the top (bottom) principal components, which\nreflect more global (local) network structure. For large budgets, optimal\ninterventions are simple -- they involve a single principal component. \n\n"}
{"id": "1710.07328", "contents": "Title: Online Monotone Games Abstract: Algorithmic game theory (AGT) focuses on the design and analysis of\nalgorithms for interacting agents, with interactions rigorously formalized\nwithin the framework of games. Results from AGT find applications in domains\nsuch as online bidding auctions for web advertisements and network routing\nprotocols. Monotone games are games where agent strategies naturally converge\nto an equilibrium state. Previous results in AGT have been obtained for convex,\nsocially-convex, or smooth games, but not monotone games. Our primary\ntheoretical contributions are defining the monotone game setting and its\nextension to the online setting, a new notion of regret for this setting, and\naccompanying algorithms that achieve sub-linear regret. We demonstrate the\nutility of online monotone game theory on a variety of problem domains\nincluding variational inequalities, reinforcement learning, and generative\nadversarial networks. \n\n"}
{"id": "1710.07365", "contents": "Title: An extension of the Moran process using type-specific connection graphs Abstract: The Moran process, as studied by [Lieberman, E., Hauert, C. and Nowak, M.\nEvolutionary dynamics on graphs. Nature 433, pp. 312-316 (2005)], is a\nstochastic process modeling the spread of genetic mutations in populations. In\nthis process, agents of a two-type population (i.e. mutants and residents) are\nassociated with the vertices of a graph. Initially, only one vertex chosen\nuniformly at random is a mutant, with fitness $r > 0$, while all other\nindividuals are residents, with fitness $1$. In every step, an individual is\nchosen with probability proportional to its fitness, and its state (mutant or\nresident) is passed on to a neighbor which is chosen uniformly at random. In\nthis paper, we introduce and study a generalization of the model of Lieberman\net al. by assuming that different types of individuals perceive the population\nthrough different graphs, namely $G_R(V,E_R)$ for residents and $G_M(V,E_M)$\nfor mutants. In this model, we study the fixation probability, i.e. the\nprobability that eventually only mutants remain in the population, for various\npairs of graphs. First, we transfer known results from the original\nsingle-graph model of Lieberman et al. to our 2-graph model. Among them, we\nprovide a generalization of the Isothermal Theorem of Lieberman et al., that\ngives sufficient conditions for a pair of graphs to have the same fixation\nprobability as a pair of cliques. Next, we give a 2-player strategic game view\nof the process where player payoffs correspond to fixation and/or extinction\nprobabilities. In this setting, we attempt to identify best responses for each\nplayer and give evidence that the clique is the most beneficial graph for both\nplayers. Finally, we examine the possibility of efficient approximation of the\nfixation probability and provide a FPRAS for the special case where the mutant\ngraph is complete. \n\n"}
{"id": "1710.07735", "contents": "Title: ADA: A Game-Theoretic Perspective on Data Augmentation for Object\n  Detection Abstract: The use of random perturbations of ground truth data, such as random\ntranslation or scaling of bounding boxes, is a common heuristic used for data\naugmentation that has been shown to prevent overfitting and improve\ngeneralization. Since the design of data augmentation is largely guided by\nreported best practices, it is difficult to understand if those design choices\nare optimal. To provide a more principled perspective, we develop a\ngame-theoretic interpretation of data augmentation in the context of object\ndetection. We aim to find an optimal adversarial perturbations of the ground\ntruth data (i.e., the worst case perturbations) that forces the object bounding\nbox predictor to learn from the hardest distribution of perturbed examples for\nbetter test-time performance. We establish that the game theoretic solution,\nthe Nash equilibrium, provides both an optimal predictor and optimal data\naugmentation distribution. We show that our adversarial method of training a\npredictor can significantly improve test time performance for the task of\nobject detection. On the ImageNet object detection task, our adversarial\napproach improves performance by over 16\\% compared to the best performing data\naugmentation method \n\n"}
{"id": "1710.07783", "contents": "Title: A Novel Stochastic Stratified Average Gradient Method: Convergence Rate\n  and Its Complexity Abstract: SGD (Stochastic Gradient Descent) is a popular algorithm for large scale\noptimization problems due to its low iterative cost. However, SGD can not\nachieve linear convergence rate as FGD (Full Gradient Descent) because of the\ninherent gradient variance. To attack the problem, mini-batch SGD was proposed\nto get a trade-off in terms of convergence rate and iteration cost. In this\npaper, a general CVI (Convergence-Variance Inequality) equation is presented to\nstate formally the interaction of convergence rate and gradient variance. Then\na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is\nintroduced to reduce gradient variance based on two techniques, stratified\nsampling and averaging over iterations that is a key idea in SAG (Stochastic\nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of\n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs,\nwhere $C\\geq 2$ is the category number of training data. This convergence rate\ndepends mainly on the variance between classes, but not on the variance within\nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's\nconvergence rate is much better than SAG's convergence rate of $\\mathcal\n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG\nand many other algorithms. \n\n"}
{"id": "1710.10117", "contents": "Title: Reality-Aware Social Choice Abstract: Social Choice theory generalizes voting on one proposal to ranking multiple\nproposals. Yet, while a vote on a single proposal has the status quo (Reality)\nas a default, Reality has been forsaken during this generalization. Here, we\npropose to restore this default social state and to incorporate Reality\nexplicitly into Social Choice. We show that doing so gives rise to a new\ntheory, complete with its domain restrictions, voting rules with their\nReality-aware axiomatic properties, and certain game-theoretic aspects. In\nparticular, we show how Reality can be used in a principled way to break\nCondorcet cycles and develop an efficient Reality-aware Condorcet-consistent\nagenda. We then discuss several applications of Reality-Aware Social Choice. \n\n"}
{"id": "1710.10595", "contents": "Title: Social Welfare Maximization Auction in Edge Computing Resource\n  Allocation for Mobile Blockchain Abstract: Blockchain, an emerging decentralized security system, has been applied in\nmany applications, such as bitcoin, smart grid, and Internet-of-Things.\nHowever, running the mining process may cost too much energy consumption and\ncomputing resource usage on handheld devices, which restricts the use of\nblockchain in mobile environments. In this paper, we consider deploying edge\ncomputing service to support the mobile blockchain. We propose an auction-based\nedge computing resource market of the edge computing service provider. Since\nthere is competition among miners, the allocative externalities (positive and\nnegative) are taken into account in the model. In our auction mechanism, we\nmaximize the social welfare while guaranteeing the truthfulness, individual\nrationality and computational efficiency. Based on blockchain mining experiment\nresults, we define a hash power function that characterizes the probability of\nsuccessfully mining a block. Through extensive simulations, we evaluate the\nperformance of our auction mechanism which shows that our edge computing\nresources market model can efficiently solve the social welfare maximization\nproblem for the edge computing service provider. \n\n"}
{"id": "1710.10772", "contents": "Title: Tensorizing Generative Adversarial Nets Abstract: Generative Adversarial Network (GAN) and its variants exhibit\nstate-of-the-art performance in the class of generative models. To capture\nhigher-dimensional distributions, the common learning procedure requires high\ncomputational complexity and a large number of parameters. The problem of\nemploying such massive framework arises when deploying it on a platform with\nlimited computational power such as mobile phones. In this paper, we present a\nnew generative adversarial framework by representing each layer as a tensor\nstructure connected by multilinear operations, aiming to reduce the number of\nmodel parameters by a large factor while preserving the generative performance\nand sample quality. To learn the model, we employ an efficient algorithm which\nalternatively optimizes both discriminator and generator. Experimental outcomes\ndemonstrate that our model can achieve high compression rate for model\nparameters up to $35$ times when compared to the original GAN for MNIST\ndataset. \n\n"}
{"id": "1710.11238", "contents": "Title: Prototype Matching Networks for Large-Scale Multi-label Genomic Sequence\n  Classification Abstract: One of the fundamental tasks in understanding genomics is the problem of\npredicting Transcription Factor Binding Sites (TFBSs). With more than hundreds\nof Transcription Factors (TFs) as labels, genomic-sequence based TFBS\nprediction is a challenging multi-label classification task. There are two\nmajor biological mechanisms for TF binding: (1) sequence-specific binding\npatterns on genomes known as \"motifs\" and (2) interactions among TFs known as\nco-binding effects. In this paper, we propose a novel deep architecture, the\nPrototype Matching Network (PMN) to mimic the TF binding mechanisms. Our PMN\nmodel automatically extracts prototypes (\"motif\"-like features) for each TF\nthrough a novel prototype-matching loss. Borrowing ideas from few-shot matching\nmodels, we use the notion of support set of prototypes and an LSTM to learn how\nTFs interact and bind to genomic sequences. On a reference TFBS dataset with\n$2.1$ $million$ genomic sequences, PMN significantly outperforms baselines and\nvalidates our design choices empirically. To our knowledge, this is the first\ndeep learning architecture that introduces prototype learning and considers\nTF-TF interactions for large-scale TFBS prediction. Not only is the proposed\narchitecture accurate, but it also models the underlying biology. \n\n"}
{"id": "1711.00108", "contents": "Title: Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer\n  Ordering Abstract: Existing deep multitask learning (MTL) approaches align layers shared between\ntasks in a parallel ordering. Such an organization significantly constricts the\ntypes of shared structure that can be learned. The necessity of parallel\nordering for deep MTL is first tested by comparing it with permuted ordering of\nshared layers. The results indicate that a flexible ordering can enable more\neffective sharing, thus motivating the development of a soft ordering approach,\nwhich learns how shared layers are applied in different ways for different\ntasks. Deep MTL with soft ordering outperforms parallel ordering methods across\na series of domains. These results suggest that the power of deep MTL comes\nfrom learning highly general building blocks that can be assembled to meet the\ndemands of each task. \n\n"}
{"id": "1711.00141", "contents": "Title: Training GANs with Optimism Abstract: We address the issue of limit cycling behavior in training Generative\nAdversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for\ntraining Wasserstein GANs. Recent theoretical results have shown that\noptimistic mirror decent (OMD) can enjoy faster regret rates in the context of\nzero-sum games. WGANs is exactly a context of solving a zero-sum game with\nsimultaneous no-regret dynamics. Moreover, we show that optimistic mirror\ndecent addresses the limit cycling problem in training WGANs. We formally show\nthat in the case of bi-linear zero-sum games the last iterate of OMD dynamics\nconverges to an equilibrium, in contrast to GD dynamics which are bound to\ncycle. We also portray the huge qualitative difference between GD and OMD\ndynamics with toy examples, even when GD is modified with many adaptations\nproposed in the recent literature, such as gradient penalty or momentum. We\napply OMD WGAN training to a bioinformatics problem of generating DNA\nsequences. We observe that models trained with OMD achieve consistently smaller\nKL divergence with respect to the true underlying distribution, than models\ntrained with GD variants. Finally, we introduce a new algorithm, Optimistic\nAdam, which is an optimistic variant of Adam. We apply it to WGAN training on\nCIFAR10 and observe improved performance in terms of inception score as\ncompared to Adam. \n\n"}
{"id": "1711.00201", "contents": "Title: Credimus Abstract: We believe that economic design and computational complexity---while already\nimportant to each other---should become even more important to each other with\neach passing year. But for that to happen, experts in on the one hand such\nareas as social choice, economics, and political science and on the other hand\ncomputational complexity will have to better understand each other's\nworldviews.\n  This article, written by two complexity theorists who also work in\ncomputational social choice theory, focuses on one direction of that process by\npresenting a brief overview of how most computational complexity theorists view\nthe world. Although our immediate motivation is to make the lens through which\ncomplexity theorists see the world be better understood by those in the social\nsciences, we also feel that even within computer science it is very important\nfor nontheoreticians to understand how theoreticians think, just as it is\nequally important within computer science for theoreticians to understand how\nnontheoreticians think. \n\n"}
{"id": "1711.00501", "contents": "Title: Learning One-hidden-layer Neural Networks with Landscape Design Abstract: We consider the problem of learning a one-hidden-layer neural network: we\nassume the input $x\\in \\mathbb{R}^d$ is from Gaussian distribution and the\nlabel $y = a^\\top \\sigma(Bx) + \\xi$, where $a$ is a nonnegative vector in\n$\\mathbb{R}^m$ with $m\\le d$, $B\\in \\mathbb{R}^{m\\times d}$ is a full-rank\nweight matrix, and $\\xi$ is a noise vector. We first give an analytic formula\nfor the population risk of the standard squared loss and demonstrate that it\nimplicitly attempts to decompose a sequence of low-rank tensors simultaneously.\n  Inspired by the formula, we design a non-convex objective function $G(\\cdot)$\nwhose landscape is guaranteed to have the following properties: 1. All local\nminima of $G$ are also global minima.\n  2. All global minima of $G$ correspond to the ground truth parameters.\n  3. The value and gradient of $G$ can be estimated using samples.\n  With these properties, stochastic gradient descent on $G$ provably converges\nto the global minimum and learn the ground-truth parameters. We also prove\nfinite sample complexity result and validate the results by simulations. \n\n"}
{"id": "1711.01761", "contents": "Title: AdaBatch: Efficient Gradient Aggregation Rules for Sequential and\n  Parallel Stochastic Gradient Methods Abstract: We study a new aggregation operator for gradients coming from a mini-batch\nfor stochastic gradient (SG) methods that allows a significant speed-up in the\ncase of sparse optimization problems. We call this method AdaBatch and it only\nrequires a few lines of code change compared to regular mini-batch SGD\nalgorithms. We provide a theoretical insight to understand how this new class\nof algorithms is performing and show that it is equivalent to an implicit\nper-coordinate rescaling of the gradients, similarly to what Adagrad methods\ncan do. In theory and in practice, this new aggregation allows to keep the same\nsample efficiency of SG methods while increasing the batch size.\nExperimentally, we also show that in the case of smooth convex optimization,\nour procedure can even obtain a better loss when increasing the batch size for\na fixed number of samples. We then apply this new algorithm to obtain a\nparallelizable stochastic gradient method that is synchronous but allows\nspeed-up on par with Hogwild! methods as convergence does not deteriorate with\nthe increase of the batch size. The same approach can be used to make\nmini-batch provably efficient for variance-reduced SG methods such as SVRG. \n\n"}
{"id": "1711.02303", "contents": "Title: Iterative Computation of Security Strategies of Matrix Games with\n  Growing Action Set Abstract: This paper studies how to efficiently update the saddle-point strategy, or\nsecurity strategy of one player in a matrix game when the other player develops\nnew actions in the game. It is well known that the saddle-point strategy of one\nplayer can be computed by solving a linear program. Developing a new action\nwill add a new constraint to the existing LP. Therefore, our problem becomes\nhow to solve the new LP with a new constraint efficiently. Considering the\npotentially huge number of constraints, which corresponds to the large size of\nthe other player's action set, we use shadow vertex simplex method, whose\ncomputational complexity is lower than linear with respect to the size of the\nconstraints, as the basis of our iterative algorithm. We first rebuild the main\ntheorems in shadow vertex method with relaxed assumption to make sure such\nmethod works well in our model, then analyze the probability that the old\noptimum remains optimal in the new LP, and finally provides the iterative\nshadow vertex method whose computational complexity is shown to be strictly\nless than that of shadow vertex method. The simulation results demonstrates our\nmain results about the probability of re-computing the optimum and the\ncomputational complexity of the iterative shadow vertex method. \n\n"}
{"id": "1711.02308", "contents": "Title: Security Strategies of Both Players in Asymmetric Information Zero-Sum\n  Stochastic Games with an Informed Controller Abstract: This paper considers a zero-sum two-player asymmetric information stochastic\ngame where only one player knows the system state, and the transition law is\ncontrolled by the informed player only. For the informed player, it has been\nshown that the security strategy only depends on the belief and the current\nstage. We provide LP formulations whose size is only linear in the size of the\nuninformed player's action set to compute both history based and belief based\nsecurity strategies. For the uninformed player, we focus on the regret, the\ndifference between 0 and the future payoff guaranteed by the uninformed player\nin every possible state. Regret is a real vector of the same size as the\nbelief, and depends only on the action of the informed player and the strategy\nof the uninformed player. This paper shows that the uninformed player has a\nsecurity strategy that only depends on the regret and the current stage. LP\nformulations are then given to compute the history based security strategy, the\nregret at every stage, and the regret based security strategy. The size of the\nLP formulations are again linear in the size of the uninformed player action\nset. Finally, an intrusion detection problem is studied to demonstrate the main\nresults in this paper. \n\n"}
{"id": "1711.02309", "contents": "Title: Learning Overcomplete HMMs Abstract: We study the problem of learning overcomplete HMMs---those that have many\nhidden states but a small output alphabet. Despite having significant practical\nimportance, such HMMs are poorly understood with no known positive or negative\nresults for efficient learning. In this paper, we present several new\nresults---both positive and negative---which help define the boundaries between\nthe tractable and intractable settings. Specifically, we show positive results\nfor a large subclass of HMMs whose transition matrices are sparse,\nwell-conditioned, and have small probability mass on short cycles. On the other\nhand, we show that learning is impossible given only a polynomial number of\nsamples for HMMs with a small output alphabet and whose transition matrices are\nrandom regular graphs with large degree. We also discuss these results in the\ncontext of learning HMMs which can capture long-term dependencies. \n\n"}
{"id": "1711.02515", "contents": "Title: Continuous DR-submodular Maximization: Structure and Algorithms Abstract: DR-submodular continuous functions are important objectives with wide\nreal-world applications spanning MAP inference in determinantal point processes\n(DPPs), and mean-field inference for probabilistic submodular models, amongst\nothers. DR-submodularity captures a subclass of non-convex functions that\nenables both exact minimization and approximate maximization in polynomial\ntime.\n  In this work we study the problem of maximizing non-monotone DR-submodular\ncontinuous functions under general down-closed convex constraints. We start by\ninvestigating geometric properties that underlie such objectives, e.g., a\nstrong relation between (approximately) stationary points and global optimum is\nproved. These properties are then used to devise two optimization algorithms\nwith provable guarantees. Concretely, we first devise a \"two-phase\" algorithm\nwith $1/4$ approximation guarantee. This algorithm allows the use of existing\nmethods for finding (approximately) stationary points as a subroutine, thus,\nharnessing recent progress in non-convex optimization. Then we present a\nnon-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and\nsublinear convergence rate. Finally, we extend our approach to a broader class\nof generalized DR-submodular continuous functions, which captures a wider\nspectrum of applications. Our theoretical findings are validated on synthetic\nand real-world problem instances. \n\n"}
{"id": "1711.02782", "contents": "Title: Block-Sparse Recurrent Neural Networks Abstract: Recurrent Neural Networks (RNNs) are used in state-of-the-art models in\ndomains such as speech recognition, machine translation, and language\nmodelling. Sparsity is a technique to reduce compute and memory requirements of\ndeep learning models. Sparse RNNs are easier to deploy on devices and high-end\nserver processors. Even though sparse operations need less compute and memory\nrelative to their dense counterparts, the speed-up observed by using sparse\noperations is less than expected on different hardware platforms. In order to\naddress this issue, we investigate two different approaches to induce block\nsparsity in RNNs: pruning blocks of weights in a layer and using group lasso\nregularization to create blocks of weights with zeros. Using these techniques,\nwe demonstrate that we can create block-sparse RNNs with sparsity ranging from\n80% to 90% with small loss in accuracy. This allows us to reduce the model size\nby roughly 10x. Additionally, we can prune a larger dense network to recover\nthis loss in accuracy while maintaining high block sparsity and reducing the\noverall parameter count. Our technique works with a variety of block sizes up\nto 32x32. Block-sparse RNNs eliminate overheads related to data storage and\nirregular memory accesses while increasing hardware efficiency compared to\nunstructured sparsity. \n\n"}
{"id": "1711.02844", "contents": "Title: Optimal Auction For Edge Computing Resource Management in Mobile\n  Blockchain Networks: A Deep Learning Approach Abstract: Blockchain has recently been applied in many applications such as bitcoin,\nsmart grid, and Internet of Things (IoT) as a public ledger of transactions.\nHowever, the use of blockchain in mobile environments is still limited because\nthe mining process consumes too much computing and energy resources on mobile\ndevices. Edge computing offered by the Edge Computing Service Provider can be\nadopted as a viable solution for offloading the mining tasks from the mobile\ndevices, i.e., miners, in the mobile blockchain environment. However, a\nmechanism needs to be designed for edge resource allocation to maximize the\nrevenue for the Edge Computing Service Provider and to ensure incentive\ncompatibility and individual rationality is still open. In this paper, we\ndevelop an optimal auction based on deep learning for the edge resource\nallocation. Specifically, we construct a multi-layer neural network\narchitecture based on an analytical solution of the optimal auction. The neural\nnetworks first perform monotone transformations of the miners' bids. Then, they\ncalculate allocation and conditional payment rules for the miners. We use\nvaluations of the miners as the data training to adjust parameters of the\nneural networks so as to optimize the loss function which is the expected,\nnegated revenue of the Edge Computing Service Provider. We show the\nexperimental results to confirm the benefits of using the deep learning for\nderiving the optimal auction for mobile blockchain with high revenue \n\n"}
{"id": "1711.02974", "contents": "Title: Clustering with feature selection using alternating minimization,\n  Application to computational biology Abstract: This paper deals with unsupervised clustering with feature selection. The\nproblem is to estimate both labels and a sparse projection matrix of weights.\nTo address this combinatorial non-convex problem maintaining a strict control\non the sparsity of the matrix of weights, we propose an alternating\nminimization of the Frobenius norm criterion. We provide a new efficient\nalgorithm named K-sparse which alternates k-means with projection-gradient\nminimization. The projection-gradient step is a method of splitting type, with\nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of\nthe gradient-projection step is addressed, and a preliminary analysis of the\nalternating minimization is made. The Frobenius norm criterion converges as the\nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on\nSingle Cell RNA sequencing datasets show that our method significantly improves\nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and\nachieves a relevant selection of genes. The complexity of K-sparse is linear in\nthe number of samples (cells), so that the method scales up to large datasets. \n\n"}
{"id": "1711.03067", "contents": "Title: Learning K-way D-dimensional Discrete Code For Compact Embedding\n  Representations Abstract: Embedding methods such as word embedding have become pillars for many\napplications containing discrete structures. Conventional embedding methods\ndirectly associate each symbol with a continuous embedding vector, which is\nequivalent to applying linear transformation based on \"one-hot\" encoding of the\ndiscrete symbols. Despite its simplicity, such approach yields number of\nparameters that grows linearly with the vocabulary size and can lead to\noverfitting. In this work we propose a much more compact K-way D-dimensional\ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\",\neach symbol is represented by a $D$-dimensional code, and each of its dimension\nhas a cardinality of $K$. The final symbol embedding vector can be generated by\ncomposing the code embedding vectors. To learn the semantically meaningful\ncode, we derive a relaxed discrete optimization technique based on stochastic\ngradient descent. By adopting the new coding system, the efficiency of\nparameterization can be significantly improved (from linear to logarithmic),\nand this can also mitigate the over-fitting problem. In our experiments with\nlanguage modeling, the number of embedding parameters can be reduced by 97\\%\nwhile achieving similar or better performance. \n\n"}
{"id": "1711.03466", "contents": "Title: On Strong Equilibria and Improvement Dynamics in Network Creation Games Abstract: We study strong equilibria in network creation games. These form a classical\nand well-studied class of games where a set of players form a network by buying\nedges to their neighbors at a cost of a fixed parameter $\\alpha$. The cost of a\nplayer is defined to be the cost of the bought edges plus the sum of distances\nto all the players in the resulting graph.\n  We identify and characterize various structural properties of strong\nequilibria, which lead to a characterization of the set of strong equilibria\nfor all $\\alpha$ in the range $(0,2)$. For $\\alpha > 2$, Andelman et al. (2009)\nprove that a star graph in which every leaf buys one edge to the center node is\na strong equilibrium, and conjecture that in fact any star is a strong\nequilibrium. We resolve this conjecture in the affirmative. Additionally, we\nshow that when $\\alpha$ is large enough ($\\geq 2n$) there exist non-star trees\nthat are strong equilibria. For the strong price of anarchy, we provide precise\nexpressions when $\\alpha$ is in the range $(0,2)$, and we prove a lower bound\nof $3/2$ when $\\alpha \\geq 2$.\n  Lastly, we aim to characterize under which conditions (coalitional)\nimprovement dynamics may converge to a strong equilibrium. To this end, we\nstudy the (coalitional) finite improvement property and (coalitional) weak\nacyclicity property. We prove various conditions under which these properties\ndo and do not hold. Some of these results also hold for the class of pure Nash\nequilibria. \n\n"}
{"id": "1711.04024", "contents": "Title: How fragile are information cascades? Abstract: It is well known that sequential decision making may lead to information\ncascades. That is, when agents make decisions based on their private\ninformation, as well as observing the actions of those before them, then it\nmight be rational to ignore their private signal and imitate the action of\nprevious individuals. If the individuals are choosing between a right and a\nwrong state, and the initial actions are wrong, then the whole cascade will be\nwrong. This issue is due to the fact that cascades can be based on very little\ninformation.\n  We show that if agents occasionally disregard the actions of others and base\ntheir action only on their private information, then wrong cascades can be\navoided. Moreover, we study the optimal asymptotic rate at which the error\nprobability at time $t$ can go to zero. The optimal policy is for the player at\ntime $t$ to follow their private information with probability $p_{t} = c/t$,\nleading to a learning rate of $c'/t$, where the constants $c$ and $c'$ are\nexplicit. \n\n"}
{"id": "1711.04066", "contents": "Title: Communication Complexity of Discrete Fair Division Abstract: We initiate the study of the communication complexity of fair division with\nindivisible goods. We focus on some of the most well-studied fairness notions\n(envy-freeness, proportionality, and approximations thereof) and valuation\nclasses (submodular, subadditive and unrestricted). Within these parameters,\nour results completely resolve whether the communication complexity of\ncomputing a fair allocation (or determining that none exist) is polynomial or\nexponential (in the number of goods), for every combination of fairness notion,\nvaluation class, and number of players, for both deterministic and randomized\nprotocols. \n\n"}
{"id": "1711.04484", "contents": "Title: Stable project allocation under distributional constraints Abstract: In a two-sided matching market when agents on both sides have preferences the\nstability of the solution is typically the most important requirement. However,\nwe may also face some distributional constraints with regard to the minimum\nnumber of assignees or the distribution of the assignees according to their\ntypes. These two requirements can be challenging to reconcile in practice. In\nthis paper we describe two real applications, a project allocation problem and\na workshop assignment problem, both involving some distributional constraints.\nWe used integer programming techniques to find reasonably good solutions with\nregard to the stability and the distributional constraints. Our approach can be\nuseful in a variety of different applications, such as resident allocation with\nlower quotas, controlled school choice or college admissions with affirmative\naction. \n\n"}
{"id": "1711.04503", "contents": "Title: Consensus Halving is PPA-Complete Abstract: We show that the computational problem CONSENSUS-HALVING is PPA-complete, the\nfirst PPA-completeness result for a problem whose definition does not involve\nan explicit circuit. We also show that an approximate version of this problem\nis polynomial-time equivalent to NECKLACE SPLITTING, which establishes\nPPAD-hardness for NECKLACE SPLITTING, and suggests that it is also\nPPA-complete. \n\n"}
{"id": "1711.04520", "contents": "Title: Fair Knapsack Abstract: We study the following multiagent variant of the knapsack problem. We are\ngiven a set of items, a set of voters, and a value of the budget; each item is\nendowed with a cost and each voter assigns to each item a certain value. The\ngoal is to select a subset of items with the total cost not exceeding the\nbudget, in a way that is consistent with the voters' preferences. Since the\npreferences of the voters over the items can vary significantly, we need a way\nof aggregating these preferences, in order to select the socially best valid\nknapsack. We study three approaches to aggregating voters' preferences, which\nare motivated by the literature on multiwinner elections and fair allocation.\nThis way we introduce the concepts of individually best, diverse, and fair\nknapsack. We study the computational complexity (including parameterized\ncomplexity, and complexity under restricted domains) of the aforementioned\nmultiagent variants of knapsack. \n\n"}
{"id": "1711.04965", "contents": "Title: Near-optimal sample complexity for convex tensor completion Abstract: We analyze low rank tensor completion (TC) using noisy measurements of a\nsubset of the tensor. Assuming a rank-$r$, order-$d$, $N \\times N \\times \\cdots\n\\times N$ tensor where $r=O(1)$, the best sampling complexity that was achieved\nis $O(N^{\\frac{d}{2}})$, which is obtained by solving a tensor nuclear-norm\nminimization problem. However, this bound is significantly larger than the\nnumber of free variables in a low rank tensor which is $O(dN)$. In this paper,\nwe show that by using an atomic-norm whose atoms are rank-$1$ sign tensors, one\ncan obtain a sample complexity of $O(dN)$. Moreover, we generalize the matrix\nmax-norm definition to tensors, which results in a max-quasi-norm (max-qnorm)\nwhose unit ball has small Rademacher complexity. We prove that solving a\nconstrained least squares estimation using either the convex atomic-norm or the\nnonconvex max-qnorm results in optimal sample complexity for the problem of\nlow-rank tensor completion. Furthermore, we show that these bounds are nearly\nminimax rate-optimal. We also provide promising numerical results for max-qnorm\nconstrained tensor completion, showing improved recovery results compared to\nmatricization and alternating least squares. \n\n"}
{"id": "1711.06030", "contents": "Title: Sub-committee Approval Voting and Generalised Justified Representation\n  Axioms Abstract: Social choice is replete with various settings including single-winner\nvoting, multi-winner voting, probabilistic voting, multiple referenda, and\npublic decision making. We study a general model of social choice called\nSub-Committee Voting (SCV) that simultaneously generalizes these settings. We\nthen focus on sub-committee voting with approvals and propose extensions of the\njustified representation axioms that have been considered for proportional\nrepresentation in approval-based committee voting. We study the properties and\nrelations of these axioms. For each of the axioms, we analyse whether a\nrepresentative committee exists and also examine the complexity of computing\nand verifying such a committee. \n\n"}
{"id": "1711.06350", "contents": "Title: Towards Deep Learning Models for Psychological State Prediction using\n  Smartphone Data: Challenges and Opportunities Abstract: There is an increasing interest in exploiting mobile sensing technologies and\nmachine learning techniques for mental health monitoring and intervention.\nResearchers have effectively used contextual information, such as mobility,\ncommunication and mobile phone usage patterns for quantifying individuals' mood\nand wellbeing. In this paper, we investigate the effectiveness of neural\nnetwork models for predicting users' level of stress by using the location\ninformation collected by smartphones. We characterize the mobility patterns of\nindividuals using the GPS metrics presented in the literature and employ these\nmetrics as input to the network. We evaluate our approach on the open-source\nStudentLife dataset. Moreover, we discuss the challenges and trade-offs\ninvolved in building machine learning models for digital mental health and\nhighlight potential future work in this direction. \n\n"}
{"id": "1711.06527", "contents": "Title: Multiwinner Elections with Diversity Constraints Abstract: We develop a model of multiwinner elections that combines performance-based\nmeasures of the quality of the committee (such as, e.g., Borda scores of the\ncommittee members) with diversity constraints. Specifically, we assume that the\ncandidates have certain attributes (such as being a male or a female, being\njunior or senior, etc.) and the goal is to elect a committee that, on the one\nhand, has as high a score regarding a given performance measure, but that, on\nthe other hand, meets certain requirements (e.g., of the form \"at least $30\\%$\nof the committee members are junior candidates and at least $40\\%$ are\nfemales\"). We analyze the computational complexity of computing winning\ncommittees in this model, obtaining polynomial-time algorithms (exact and\napproximate) and NP-hardness results. We focus on several natural classes of\nvoting rules and diversity constraints. \n\n"}
{"id": "1711.07600", "contents": "Title: On the Distortion of Voting with Multiple Representative Candidates Abstract: We study positional voting rules when candidates and voters are embedded in a\ncommon metric space, and cardinal preferences are naturally given by distances\nin the metric space. In a positional voting rule, each candidate receives a\nscore from each ballot based on the ballot's rank order; the candidate with the\nhighest total score wins the election. The cost of a candidate is his sum of\ndistances to all voters, and the distortion of an election is the ratio between\nthe cost of the elected candidate and the cost of the optimum candidate. We\nconsider the case when candidates are representative of the population, in the\nsense that they are drawn i.i.d. from the population of the voters, and analyze\nthe expected distortion of positional voting rules.\n  Our main result is a clean and tight characterization of positional voting\nrules that have constant expected distortion (independent of the number of\ncandidates and the metric space). Our characterization result immediately\nimplies constant expected distortion for Borda Count and elections in which\neach voter approves a constant fraction of all candidates. On the other hand,\nwe obtain super-constant expected distortion for Plurality, Veto, and approving\na constant number of candidates. These results contrast with previous results\non voting with metric preferences: When the candidates are chosen\nadversarially, all of the preceding voting rules have distortion linear in the\nnumber of candidates or voters. Thus, the model of representative candidates\nallows us to distinguish voting rules which seem equally bad in the worst case. \n\n"}
{"id": "1711.07621", "contents": "Title: Groupwise Maximin Fair Allocation of Indivisible Goods Abstract: We study the problem of allocating indivisible goods among n agents in a fair\nmanner. For this problem, maximin share (MMS) is a well-studied solution\nconcept which provides a fairness threshold. Specifically, maximin share is\ndefined as the minimum utility that an agent can guarantee for herself when\nasked to partition the set of goods into n bundles such that the remaining\n(n-1) agents pick their bundles adversarially. An allocation is deemed to be\nfair if every agent gets a bundle whose valuation is at least her maximin\nshare.\n  Even though maximin shares provide a natural benchmark for fairness, it has\nits own drawbacks and, in particular, it is not sufficient to rule out\nunsatisfactory allocations. Motivated by these considerations, in this work we\ndefine a stronger notion of fairness, called groupwise maximin share guarantee\n(GMMS). In GMMS, we require that the maximin share guarantee is achieved not\njust with respect to the grand bundle, but also among all the subgroups of\nagents. Hence, this solution concept strengthens MMS and provides an ex-post\nfairness guarantee. We show that in specific settings, GMMS allocations always\nexist. We also establish the existence of approximate GMMS allocations under\nadditive valuations, and develop a polynomial-time algorithm to find such\nallocations. Moreover, we establish a scale of fairness wherein we show that\nGMMS implies approximate envy freeness.\n  Finally, we empirically demonstrate the existence of GMMS allocations in a\nlarge set of randomly generated instances. For the same set of instances, we\nadditionally show that our algorithm achieves an approximation factor better\nthan the established, worst-case bound. \n\n"}
{"id": "1711.08226", "contents": "Title: Proportionally Representative Participatory Budgeting: Axioms and\n  Algorithms Abstract: Participatory budgeting is one of the exciting developments in deliberative\ngrassroots democracy. We concentrate on approval elections and propose\nproportional representation axioms in participatory budgeting, by generalizing\nrelevant axioms for approval-based multi-winner elections. We observe a rich\nlandscape with respect to the computational complexity of identifying\nproportional budgets and computing such, and present budgeting methods that\nsatisfy these axioms by identifying budgets that are representative to the\ndemands of vast segments of the voters. \n\n"}
{"id": "1711.08534", "contents": "Title: Safer Classification by Synthesis Abstract: The discriminative approach to classification using deep neural networks has\nbecome the de-facto standard in various fields. Complementing recent\nreservations about safety against adversarial examples, we show that\nconventional discriminative methods can easily be fooled to provide incorrect\nlabels with very high confidence to out of distribution examples. We posit that\na generative approach is the natural remedy for this problem, and propose a\nmethod for classification using generative models. At training time, we learn a\ngenerative model for each class, while at test time, given an example to\nclassify, we query each generator for its most similar generation, and select\nthe class corresponding to the most similar one. Our approach is general and\ncan be used with expressive models such as GANs and VAEs. At test time, our\nmethod accurately \"knows when it does not know,\" and provides resilience to out\nof distribution examples while maintaining competitive performance for standard\nexamples. \n\n"}
{"id": "1711.09012", "contents": "Title: Mobile Edge Computation Offloading Using Game Theory and Reinforcement\n  Learning Abstract: Due to the ever-increasing popularity of resource-hungry and\ndelay-constrained mobile applications, the computation and storage capabilities\nof remote cloud has partially migrated towards the mobile edge, giving rise to\nthe concept known as Mobile Edge Computing (MEC). While MEC servers enjoy the\nclose proximity to the end-users to provide services at reduced latency and\nlower energy costs, they suffer from limitations in computational and radio\nresources, which calls for fair efficient resource management in the MEC\nservers. The problem is however challenging due to the ultra-high density,\ndistributed nature, and intrinsic randomness of next generation wireless\nnetworks. In this article, we focus on the application of game theory and\nreinforcement learning for efficient distributed resource management in MEC, in\nparticular, for computation offloading. We briefly review the cutting-edge\nresearch and discuss future challenges. Furthermore, we develop a\ngame-theoretical model for energy-efficient distributed edge server activation\nand study several learning techniques. Numerical results are provided to\nillustrate the performance of these distributed learning techniques. Also, open\nresearch issues in the context of resource management in MEC servers are\ndiscussed. \n\n"}
{"id": "1711.09681", "contents": "Title: Butterfly Effect: Bidirectional Control of Classification Performance by\n  Small Additive Perturbation Abstract: This paper proposes a new algorithm for controlling classification results by\ngenerating a small additive perturbation without changing the classifier\nnetwork. Our work is inspired by existing works generating adversarial\nperturbation that worsens classification performance. In contrast to the\nexisting methods, our work aims to generate perturbations that can enhance\noverall classification performance. To solve this performance enhancement\nproblem, we newly propose a perturbation generation network (PGN) influenced by\nthe adversarial learning strategy. In our problem, the information in a large\nexternal dataset is summarized by a small additive perturbation, which helps to\nimprove the performance of the classifier trained with the target dataset. In\naddition to this performance enhancement problem, we show that the proposed PGN\ncan be adopted to solve the classical adversarial problem without utilizing the\ninformation on the target classifier. The mentioned characteristics of our\nmethod are verified through extensive experiments on publicly available visual\ndatasets. \n\n"}
{"id": "1711.10102", "contents": "Title: A Game-theoretic Framework for Revenue Sharing in Edge-Cloud Computing\n  System Abstract: We introduce a game-theoretic framework to ex- plore revenue sharing in an\nEdge-Cloud computing system, in which computing service providers at the edge\nof the Internet (edge providers) and computing service providers at the cloud\n(cloud providers) co-exist and collectively provide computing resources to\nclients (e.g., end users or applications) at the edge. Different from\ntraditional cloud computing, the providers in an Edge-Cloud system are\nindependent and self-interested. To achieve high system-level efficiency, the\nmanager of the system adopts a task distribution mechanism to maximize the\ntotal revenue received from clients and also adopts a revenue sharing mechanism\nto split the received revenue among computing servers (and hence service\nproviders). Under those system-level mechanisms, service providers attempt to\ngame with the system in order to maximize their own utilities, by strategically\nallocating their resources (e.g., computing servers).\n  Our framework models the competition among the providers in an Edge-Cloud\nsystem as a non-cooperative game. Our simulations and experiments on an\nemulation system have shown the existence of Nash equilibrium in such a game.\nWe find that revenue sharing mechanisms have a significant impact on the\nsystem-level efficiency at Nash equilibria, and surprisingly the revenue\nsharing mechanism based directly on actual contributions can result in\nsignificantly worse system efficiency than Shapley value sharing mechanism and\nOrtmann proportional sharing mechanism. Our framework provides an effective\neconomics approach to understanding and designing efficient Edge-Cloud\ncomputing systems. \n\n"}
{"id": "1711.10279", "contents": "Title: Reinforcement Mechanism Design, with Applications to Dynamic Pricing in\n  Sponsored Search Auctions Abstract: In this study, we apply reinforcement learning techniques and propose what we\ncall reinforcement mechanism design to tackle the dynamic pricing problem in\nsponsored search auctions. In contrast to previous game-theoretical approaches\nthat heavily rely on rationality and common knowledge among the bidders, we\ntake a data-driven approach, and try to learn, over repeated interactions, the\nset of optimal reserve prices. We implement our approach within the current\nsponsored search framework of a major search engine: we first train a buyer\nbehavior model, via a real bidding data set, that accurately predicts bids\ngiven information that bidders are aware of, including the game parameters\ndisclosed by the search engine, as well as the bidders' KPI data from previous\nrounds. We then put forward a reinforcement/MDP (Markov Decision Process) based\nalgorithm that optimizes reserve prices over time, in a GSP-like auction. Our\nsimulations demonstrate that our framework outperforms static optimization\nstrategies including the ones that are currently in use, as well as several\nother dynamic ones. \n\n"}
{"id": "1711.10456", "contents": "Title: Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient\n  Descent Abstract: Nesterov's accelerated gradient descent (AGD), an instance of the general\nfamily of \"momentum methods\", provably achieves faster convergence rate than\ngradient descent (GD) in the convex setting. However, whether these methods are\nsuperior to GD in the nonconvex setting remains open. This paper studies a\nsimple variant of AGD, and shows that it escapes saddle points and finds a\nsecond-order stationary point in $\\tilde{O}(1/\\epsilon^{7/4})$ iterations,\nfaster than the $\\tilde{O}(1/\\epsilon^{2})$ iterations required by GD. To the\nbest of our knowledge, this is the first Hessian-free algorithm to find a\nsecond-order stationary point faster than GD, and also the first single-loop\nalgorithm with a faster rate than GD even in the setting of finding a\nfirst-order stationary point. Our analysis is based on two key ideas: (1) the\nuse of a simple Hamiltonian function, inspired by a continuous-time\nperspective, which AGD monotonically decreases per step even for nonconvex\nfunctions, and (2) a novel framework called improve or localize, which is\nuseful for tracking the long-term behavior of gradient-based optimization\nalgorithms. We believe that these techniques may deepen our understanding of\nboth acceleration algorithms and nonconvex optimization. \n\n"}
{"id": "1711.10789", "contents": "Title: Efficient exploration with Double Uncertain Value Networks Abstract: This paper studies directed exploration for reinforcement learning agents by\ntracking uncertainty about the value of each available action. We identify two\nsources of uncertainty that are relevant for exploration. The first originates\nfrom limited data (parametric uncertainty), while the second originates from\nthe distribution of the returns (return uncertainty). We identify methods to\nlearn these distributions with deep neural networks, where we estimate\nparametric uncertainty with Bayesian drop-out, while return uncertainty is\npropagated through the Bellman equation as a Gaussian distribution. Then, we\nidentify that both can be jointly estimated in one network, which we call the\nDouble Uncertain Value Network. The policy is directly derived from the learned\ndistributions based on Thompson sampling. Experimental results show that both\ntypes of uncertainty may vastly improve learning in domains with a strong\nexploration challenge. \n\n"}
{"id": "1711.10922", "contents": "Title: Generalizing Virtual Values to Multidimensional Auctions: a\n  Non-Myersonian Approach Abstract: We consider the revenue maximization problem of a monopolist via a\nnon-Myersonian approach that could generalize to multiple items and multiple\nbuyers. Although such an approach does not lead to any closed-form solution of\nthe problem, it does provide some insights into this problem from different\nangles. In particular, we consider both Bayesian (Bayesian Incentive Compatible\n+ Bayesian Individually Rational) and Dominant-Strategy (Dominant-Strategy\nIncentive Compatible + ex-post Individually Rational) implementations, where\nall the buyers have additive valuations and quasi-linear utilities and all the\nvaluations are independent across buyers (not necessarily independent across\nitems).\n  The main technique of our approach is to formulate the problem as an LP\n(probably with exponential size) and apply primal-dual analysis. We observe\nthat any optimal solution of the dual program naturally defines the virtual\nvalue functions for the primal revenue maximization problem in the sense that\nany revenue-maximizing auction must be a virtual welfare maximizer (cf.\nMyerson's auction for a single item [Myerson, 1981]).\n  Based on this observation, we characterize a sufficient and necessary\ncondition for BIC = DSIC, i.e., the optimal revenue of Bayesian implementations\nequals to the optimal revenue of dominant-strategy implementations (BRev =\nDRev). The condition is if and only if the optimal DSIC revenue DRev can be\nachieved by a DSIC and ex-post IR virtual welfare maximizer with\nbuyer-independent virtual value functions (buyer i's virtual value is\nindependent of other buyers' valuations). In light of the characterization, we\nfurther show that when all the valuations are i.i.d., it is further equivalent\nto that separate-selling is optimal. In particular, it respects one result from\nthe recent breakthrough work on the exact optimal solutions in the multi-item\nmulti-buyer setting by Yao [2016]. \n\n"}
{"id": "1711.10938", "contents": "Title: Extreme Dimension Reduction for Handling Covariate Shift Abstract: In the covariate shift learning scenario, the training and test covariate\ndistributions differ, so that a predictor's average loss over the training and\ntest distributions also differ. In this work, we explore the potential of\nextreme dimension reduction, i.e. to very low dimensions, in improving the\nperformance of importance weighting methods for handling covariate shift, which\nfail in high dimensions due to potentially high train/test covariate divergence\nand the inability to accurately estimate the requisite density ratios. We first\nformulate and solve a problem optimizing over linear subspaces a combination of\ntheir predictive utility and train/test divergence within. Applying it to\nsimulated and real data, we show extreme dimension reduction helps sometimes\nbut not always, due to a bias introduced by dimension reduction. \n\n"}
{"id": "1711.11392", "contents": "Title: Two-sided Facility Location Abstract: Recent years have witnessed the rise of many successful e-commerce\nmarketplace platforms like the Amazon marketplace, AirBnB, Uber/Lyft, and\nUpwork, where a central platform mediates economic transactions between buyers\nand sellers. Motivated by these platforms, we formulate a set of facility\nlocation problems that we term Two-sided Facility location. In our model,\nagents arrive at nodes in an underlying metric space, where the metric distance\nbetween any buyer and seller captures the quality of the corresponding match.\nThe platform posts prices and wages at the nodes, and opens a set of facilities\nto route the agents to. The agents at any facility are assumed to be matched.\nThe platform ensures high match quality by imposing a distance constraint\nbetween a node and the facilities it is routed to. It ensures high service\navailability by ensuring flow to the facility is at least a pre-specified lower\nbound. Subject to these constraints, the goal of the platform is to maximize\nthe social surplus (or gains from trade) subject to weak budget balance, i.e.,\nprofit being non-negative.\n  We present an approximation algorithm for this problem that yields a $(1 +\n\\epsilon)$ approximation to surplus for any constant $\\epsilon > 0$, while\nrelaxing the match quality (i.e., maximum distance of any match) by a constant\nfactor. We use an LP rounding framework that easily extends to other objectives\nsuch as maximizing volume of trade or profit.\n  We justify our models by considering a dynamic marketplace setting where\nagents arrive according to a stochastic process and have finite patience (or\ndeadlines) for being matched. We perform queueing analysis to show that for\npolicies that route agents to facilities and match them, ensuring a low\nabandonment probability of agents reduces to ensuring sufficient flow arrives\nat each facility. \n\n"}
{"id": "1712.01252", "contents": "Title: An Equivalence of Fully Connected Layer and Convolutional Layer Abstract: This article demonstrates that convolutional operation can be converted to\nmatrix multiplication, which has the same calculation way with fully connected\nlayer. The article is helpful for the beginners of the neural network to\nunderstand how fully connected layer and the convolutional layer work in the\nbackend. To be concise and to make the article more readable, we only consider\nthe linear case. It can be extended to the non-linear case easily through\nplugging in a non-linear encapsulation to the values like this $\\sigma(x)$\ndenoted as $x^{\\prime}$. \n\n"}
{"id": "1712.03518", "contents": "Title: A Note on Approximate Revenue Maximization with Two Items Abstract: We consider the problem of maximizing revenue when selling 2 items to a\nsingle buyer with known valuation distributions. Hart and Nisan showed that\nselling each item separately using the optimal Myerson's price, gains at least\nhalf of the revenue attainable by optimal auction for two items. We show that\nin case the items have different revenues when sold separately the bound can be\ntightened. \n\n"}
{"id": "1712.04581", "contents": "Title: Potential-Function Proofs for First-Order Methods Abstract: This note discusses proofs for convergence of first-order methods based on\nsimple potential-function arguments. We cover methods like gradient descent\n(for both smooth and non-smooth settings), mirror descent, and some accelerated\nvariants. \n\n"}
{"id": "1712.05385", "contents": "Title: Equilibria in the Tangle Abstract: We analyse the Tangle --- a DAG-valued stochastic process where new vertices\nget attached to the graph at Poissonian times, and the attachment's locations\nare chosen by means of random walks on that graph. These new vertices, also\nthought of as \"transactions\", are issued by many players (which are the nodes\nof the network), independently. The main application of this model is that it\nis used as a base for the IOTA cryptocurrency system (www.iota.org). We prove\nexistence of \"almost symmetric\" Nash equilibria for the system where a part of\nplayers tries to optimize their attachment strategies. Then, we also present\nsimulations that show that the \"selfish\" players will nevertheless cooperate\nwith the network by choosing attachment strategies that are similar to the\n\"recommended\" one. \n\n"}
{"id": "1712.05723", "contents": "Title: Perfect Prediction in Normal Form: Superrational Thinking Extended to\n  Non-Symmetric Games Abstract: This paper introduces a new solution concept for non-cooperative games in\nnormal form with no ties and pure strategies: the Perfectly Transparent\nEquilibrium. The players are rational in all possible worlds and know each\nother's strategies in all possible worlds, which together we refer to as\nPerfect Prediction. The anticipation of a player's decision by their opponents\nis counterfactually dependent on the decision, unlike in Nash Equilibra where\nthe decisions are made independently. The equilibrium, when it exists, is\nunique and is Pareto optimal.\n  This equilibrium is the normal-form counterpart of the Perfect Prediction\nEquilibrium; the prediction happens \"in another room\" rather than in the past.\nThe equilibrium can also be seen as a natural extension of Hofstadter's\nsuperrationality to non-symmetric games.\n  Algorithmically, an iterated elimination of non-individually-rational\nstrategy profiles is performed until at most one remains. An equilibrium is a\nstrategy profile that is immune against knowledge of strategies in all possible\nworlds and rationality in all possible worlds, a stronger concept than common\nknowledge of rationality but also than common counterfactual belief of\nrationality.\n  We formalize and contrast the Non-Nashian Decision Theory paradigm, common to\nthis and several other papers, with Causal Decision Theory and Evidential\nDecision Theory. We define the Perfectly Transparent Equilibrium\nalgorithmically and, when it exists, prove its uniqueness, its\nPareto-optimality, and that it coincides with the Hofstadter's Superrationality\non symmetric games.\n  We also relate to concepts found in literature such as Individual\nRationality, Minimax-Rationalizability, the Correlated Equilibrium.\n  Finally, we specifically discuss inclusion relationships on the special case\nof symmetric games and contrast them with asymmetric games. \n\n"}
{"id": "1712.06050", "contents": "Title: Wasserstein Distributionally Robust Optimization and Variation\n  Regularization Abstract: Wasserstein distributionally robust optimization (DRO) has recently achieved\nempirical success for various applications in operations research and machine\nlearning, owing partly to its regularization effect. Although connection\nbetween Wasserstein DRO and regularization has been established in several\nsettings, existing results often require restrictive assumptions, such as\nsmoothness or convexity, that are not satisfied for many problems. In this\npaper, we develop a general theory on the variation regularization effect of\nthe Wasserstein DRO - a new form of regularization that generalizes\ntotal-variation regularization, Lipschitz regularization and gradient\nregularization. Our results cover possibly non-convex and non-smooth losses and\nlosses on non-Euclidean spaces. Examples include multi-item newsvendor,\nportfolio selection, linear prediction, neural networks, manifold learning, and\nintensity estimation for Poisson processes, etc. As an application of our\ntheory of variation regularization, we derive new generalization guarantees for\nadversarial robust learning. \n\n"}
{"id": "1712.06358", "contents": "Title: The Saga of KPR: Theoretical and Experimental developments Abstract: In this article, we present a brief narration of the origin and the overview\nof the recent developments done on the Kolkata Paise Restaurant (KPR) problem,\nwhich can serve as a prototype for a broader class of resource allocation\nproblems in the presence of a large number of competing agents, typically\nstudied using coordination and anti-coordination games. We discuss the KPR and\nits several extensions, as well as its applications in many economic and social\nphenomena. We end the article with some discussions on our ongoing experimental\nanalysis of the same problem. We demonstrate that this provides an interesting\npicture of how people analyze complex situations, and design their strategies\nor react to them. \n\n"}
{"id": "1712.06488", "contents": "Title: Invincible Strategies of Iterated Prisoner's Dilemma Abstract: Iterated Prisoner's Dilemma(IPD) is a well-known benchmark for studying the\nlong term behaviors of rational agents, such as how cooperation can emerge\namong selfish and unrelated agents that need to co-exist over long term. Many\nwell-known strategies have been studied, from the simple tit-for-tat(TFT)\nstrategy made famous by Axelrod after his influential tournaments to more\ninvolved ones like zero determinant strategies studied recently by Press and\nDyson. In this paper, following Press and Dyson, we consider one memory\nprobabilistic strategies. We consider that we call invincible strategies: a\nstrategy is invincible if it never loses against any other strategy in terms of\naverage payoff in the limit, if the limit exists. We show a strategy is\ninvincible for infinite repeated iterated prisoner's dilemma. \n\n"}
{"id": "1712.06924", "contents": "Title: Safe Policy Improvement with Baseline Bootstrapping Abstract: This paper considers Safe Policy Improvement (SPI) in Batch Reinforcement\nLearning (Batch RL): from a fixed dataset and without direct access to the true\nenvironment, train a policy that is guaranteed to perform at least as well as\nthe baseline policy used to collect the data. Our approach, called SPI with\nBaseline Bootstrapping (SPIBB), is inspired by the knows-what-it-knows\nparadigm: it bootstraps the trained policy with the baseline when the\nuncertainty is high. Our first algorithm, $\\Pi_b$-SPIBB, comes with SPI\ntheoretical guarantees. We also implement a variant, $\\Pi_{\\leq b}$-SPIBB, that\nis even more efficient in practice. We apply our algorithms to a motivational\nstochastic gridworld domain and further demonstrate on randomly generated MDPs\nthe superiority of SPIBB with respect to existing algorithms, not only in\nsafety but also in mean performance. Finally, we implement a model-free version\nof SPIBB and show its benefits on a navigation task with deep RL implementation\ncalled SPIBB-DQN, which is, to the best of our knowledge, the first RL\nalgorithm relying on a neural network representation able to train efficiently\nand reliably from batch data, without any interaction with the environment. \n\n"}
{"id": "1712.07296", "contents": "Title: Block-diagonal Hessian-free Optimization for Training Neural Networks Abstract: Second-order methods for neural network optimization have several advantages\nover methods based on first-order gradient descent, including better scaling to\nlarge mini-batch sizes and fewer updates needed for convergence. But they are\nrarely applied to deep learning in practice because of high computational cost\nand the need for model-dependent algorithmic variations. We introduce a variant\nof the Hessian-free method that leverages a block-diagonal approximation of the\ngeneralized Gauss-Newton matrix. Our method computes the curvature\napproximation matrix only for pairs of parameters from the same layer or block\nof the neural network and performs conjugate gradient updates independently for\neach block. Experiments on deep autoencoders, deep convolutional networks, and\nmultilayer LSTMs demonstrate better convergence and generalization compared to\nthe original Hessian-free approach and the Adam method. \n\n"}
{"id": "1712.07464", "contents": "Title: Selfishness need not be bad Abstract: We investigate the price of anarchy (PoA) in non-atomic congestion games when\nthe total demand $T$ gets very large.\n  First results in this direction have recently been obtained by\n\\cite{Colini2016On, Colini2017WINE, Colini2017arxiv} for routing games and show\nthat the PoA converges to 1 when the growth of the total demand $T$ satisfies\ncertain regularity conditions. We extend their results by developing a\n\\Wuuu{new} framework for the limit analysis of \\Wuuuu{the PoA that offers\nstrong techniques such as the limit of games and applies to arbitrary growth\npatterns of $T$.} \\Wuuu{We} show that the PoA converges to 1 in the limit game\nregardless of the type of growth of $T$ for a large class of cost functions\nthat contains all polynomials and all regularly varying functions.\n  %\n  For routing games with BPR \\Wuu{cost} functions, we show in addition that\nsocially optimal strategy profiles converge to \\Wuu{equilibria} in the limit\ngame, and that PoA$=1+o(T^{-\\beta})$, where $\\beta>0$ is the degree of the\n\\Wuu{BPR} functions. However, the precise convergence rate depends crucially on\nthe the growth of $T$, which shows that a conjecture proposed by\n\\cite{O2016Mechanisms} need not hold. \n\n"}
{"id": "1712.07742", "contents": "Title: Mechanism Design for Demand Response Programs Abstract: Demand Response (DR) programs serve to reduce the consumption of electricity\nat times when the supply is scarce and expensive. The utility informs the\naggregator of an anticipated DR event. The aggregator calls on a subset of its\npool of recruited agents to reduce their electricity use. Agents are paid for\nreducing their energy consumption from contractually established baselines.\nBaselines are counter-factual consumption estimates of the energy an agent\nwould have consumed if they were not participating in the DR program. Baselines\nare used to determine payments to agents. This creates an incentive for agents\nto inflate their baselines. We propose a novel self-reported baseline mechanism\n(SRBM) where each agent reports its baseline and marginal utility. These\nreports are strategic and need not be truthful. Based on the reported\ninformation, the aggregator selects or calls on agents to meet the load\nreduction target. Called agents are paid for observed reductions from their\nself-reported baselines. Agents who are not called face penalties for\nconsumption shortfalls below their baselines. The mechanism is specified by the\nprobability with which agents are called, reward prices for called agents, and\npenalty prices for agents who are not called. Under SRBM, we show that truthful\nreporting of baseline consumption and marginal utility is a dominant strategy.\nThus, SRBM eliminates the incentive for agents to inflate baselines. SRBM is\nassured to meet the load reduction target. SRBM is also nearly efficient since\nit selects agents with the smallest marginal utilities, and each called agent\ncontributes maximally to the load reduction target. Finally, we show that SRBM\nis almost optimal in the metric of average cost of DR provision faced by the\naggregator. \n\n"}
{"id": "1712.09203", "contents": "Title: Algorithmic Regularization in Over-parameterized Matrix Sensing and\n  Neural Networks with Quadratic Activations Abstract: We show that the gradient descent algorithm provides an implicit\nregularization effect in the learning of over-parameterized matrix\nfactorization models and one-hidden-layer neural networks with quadratic\nactivations. Concretely, we show that given $\\tilde{O}(dr^{2})$ random linear\nmeasurements of a rank $r$ positive semidefinite matrix $X^{\\star}$, we can\nrecover $X^{\\star}$ by parameterizing it by $UU^\\top$ with $U\\in \\mathbb\nR^{d\\times d}$ and minimizing the squared loss, even if $r \\ll d$. We prove\nthat starting from a small initialization, gradient descent recovers\n$X^{\\star}$ in $\\tilde{O}(\\sqrt{r})$ iterations approximately. The results\nsolve the conjecture of Gunasekar et al.'17 under the restricted isometry\nproperty. The technique can be applied to analyzing neural networks with\none-hidden-layer quadratic activations with some technical modifications. \n\n"}
{"id": "1712.10050", "contents": "Title: Kernel Robust Bias-Aware Prediction under Covariate Shift Abstract: Under covariate shift, training (source) data and testing (target) data\ndiffer in input space distribution, but share the same conditional label\ndistribution. This poses a challenging machine learning task. Robust Bias-Aware\n(RBA) prediction provides the conditional label distribution that is robust to\nthe worstcase logarithmic loss for the target distribution while matching\nfeature expectation constraints from the source distribution. However,\nemploying RBA with insufficient feature constraints may result in high\ncertainty predictions for much of the source data, while leaving too much\nuncertainty for target data predictions. To overcome this issue, we extend the\nrepresenter theorem to the RBA setting, enabling minimization of regularized\nexpected target risk by a reweighted kernel expectation under the source\ndistribution. By applying kernel methods, we establish consistency guarantees\nand demonstrate better performance of the RBA classifier than competing methods\non synthetically biased UCI datasets as well as datasets that have natural\ncovariate shift. \n\n"}
{"id": "1801.00218", "contents": "Title: Game-theoretic Network Centrality: A Review Abstract: Game-theoretic centrality is a flexible and sophisticated approach to\nidentify the most important nodes in a network. It builds upon the methods from\ncooperative game theory and network theory. The key idea is to treat nodes as\nplayers in a cooperative game, where the value of each coalition is determined\nby certain graph-theoretic properties. Using solution concepts from cooperative\ngame theory, it is then possible to measure how responsible each node is for\nthe worth of the network.\n  The literature on the topic is already quite large, and is scattered among\ngame-theoretic and computer science venues. We review the main game-theoretic\nnetwork centrality measures from both bodies of literature and organize them\ninto two categories: those that are more focused on the connectivity of nodes,\nand those that are more focused on the synergies achieved by nodes in groups.\nWe present and explain each centrality, with a focus on algorithms and\ncomplexity. \n\n"}
{"id": "1801.01290", "contents": "Title: Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement\n  Learning with a Stochastic Actor Abstract: Model-free deep reinforcement learning (RL) algorithms have been demonstrated\non a range of challenging decision making and control tasks. However, these\nmethods typically suffer from two major challenges: very high sample complexity\nand brittle convergence properties, which necessitate meticulous hyperparameter\ntuning. Both of these challenges severely limit the applicability of such\nmethods to complex, real-world domains. In this paper, we propose soft\nactor-critic, an off-policy actor-critic deep RL algorithm based on the maximum\nentropy reinforcement learning framework. In this framework, the actor aims to\nmaximize expected reward while also maximizing entropy. That is, to succeed at\nthe task while acting as randomly as possible. Prior deep RL methods based on\nthis framework have been formulated as Q-learning methods. By combining\noff-policy updates with a stable stochastic actor-critic formulation, our\nmethod achieves state-of-the-art performance on a range of continuous control\nbenchmark tasks, outperforming prior on-policy and off-policy methods.\nFurthermore, we demonstrate that, in contrast to other off-policy algorithms,\nour approach is very stable, achieving very similar performance across\ndifferent random seeds. \n\n"}
{"id": "1801.01989", "contents": "Title: The impact of bundling licensed and unlicensed wireless service Abstract: Unlicensed spectrum has been viewed as a way to increase competition in\nwireless access and promote innovation in new technologies and business models.\nHowever, several recent papers have shown that the openness of such spectrum\ncan also lead to it becoming over congested when used by competing wireless\nservice providers (SPs). This in turn can result in the SPs making no profit\nand may deter them from entering the market. However, this prior work assumes\nthat unlicensed access is a separate service from any service offered using\nlicensed spectrum. Here, we instead consider the more common case were service\nproviders bundle both licensed and unlicensed spectrum as a single service and\noffer this with a single price. We analyze a model for such a market and show\nthat in this case SPs are able to gain higher profit than the case without\nbundling. It is also possible to get higher social welfare with bundling.\nMoreover, we explore the case where SPs are allowed to manage the customers'\naverage percentage of time they receive service on unlicensed spectrum and\ncharacterize the social welfare gap between the profit maximizing and social\nwelfare maximizing setting. \n\n"}
{"id": "1801.02042", "contents": "Title: Learning from Neighbors about a Changing State Abstract: Agents learn about a changing state using private signals and their\nneighbors' past estimates of the state. We present a model in which Bayesian\nagents in equilibrium use neighbors' estimates simply by taking weighted sums\nwith time-invariant weights. The dynamics thus parallel those of the tractable\nDeGroot model of learning in networks, but arise as an equilibrium outcome\nrather than a behavioral assumption. We examine whether information aggregation\nis nearly optimal as neighborhoods grow large. A key condition for this is\nsignal diversity: each individual's neighbors have private signals that not\nonly contain independent information, but also have sufficiently different\ndistributions. Without signal diversity $\\unicode{x2013}$ e.g., if private\nsignals are i.i.d. $\\unicode{x2013}$ learning is suboptimal in all networks and\nhighly inefficient in some. Turning to social influence, we find it is much\nmore sensitive to one's signal quality than to one's number of neighbors, in\ncontrast to standard models with exogenous updating rules. \n\n"}
{"id": "1801.02044", "contents": "Title: Multilabeled versions of Sperner's and Fan's lemmas and applications Abstract: We propose a general technique related to the polytopal Sperner lemma for\nproving old and new multilabeled versions of Sperner's lemma. A notable\napplication of this technique yields a cake-cutting theorem where the number of\nplayers and the number of pieces can be independently chosen. We also prove\nmultilabeled versions of Fan's lemma, a combinatorial analogue of the\nBorsuk-Ulam theorem, and exhibit applications to fair division and graph\ncoloring. \n\n"}
{"id": "1801.02982", "contents": "Title: How To Make the Gradients Small Stochastically: Even Faster Convex and\n  Nonconvex SGD Abstract: Stochastic gradient descent (SGD) gives an optimal convergence rate when\nminimizing convex stochastic objectives $f(x)$. However, in terms of making the\ngradients small, the original SGD does not give an optimal rate, even when\n$f(x)$ is convex.\n  If $f(x)$ is convex, to find a point with gradient norm $\\varepsilon$, we\ndesign an algorithm SGD3 with a near-optimal rate\n$\\tilde{O}(\\varepsilon^{-2})$, improving the best known rate\n$O(\\varepsilon^{-8/3})$ of [18].\n  If $f(x)$ is nonconvex, to find its $\\varepsilon$-approximate local minimum,\nwe design an algorithm SGD5 with rate $\\tilde{O}(\\varepsilon^{-3.5})$, where\npreviously SGD variants only achieve $\\tilde{O}(\\varepsilon^{-4})$ [6, 15, 33].\nThis is no slower than the best known stochastic version of Newton's method in\nall parameter regimes [30]. \n\n"}
{"id": "1801.03137", "contents": "Title: Convergence Analysis of Gradient Descent Algorithms with Proportional\n  Updates Abstract: The rise of deep learning in recent years has brought with it increasingly\nclever optimization methods to deal with complex, non-linear loss functions.\nThese methods are often designed with convex optimization in mind, but have\nbeen shown to work well in practice even for the highly non-convex optimization\nassociated with neural networks. However, one significant drawback of these\nmethods when they are applied to deep learning is that the magnitude of the\nupdate step is sometimes disproportionate to the magnitude of the weights (much\nsmaller or larger), leading to training instabilities such as vanishing and\nexploding gradients. An idea to combat this issue is gradient descent with\nproportional updates. Gradient descent with proportional updates was introduced\nin 2017. It was independently developed by You et al (Layer-wise Adaptive Rate\nScaling (LARS) algorithm) and by Abu-El-Haija (PercentDelta algorithm). The\nbasic idea of both of these algorithms is to make each step of the gradient\ndescent proportional to the current weight norm and independent of the gradient\nmagnitude. It is common in the context of new optimization methods to prove\nconvergence or derive regret bounds under the assumption of Lipschitz\ncontinuity and convexity. However, even though LARS and PercentDelta were shown\nto work well in practice, there is no theoretical analysis of the convergence\nproperties of these algorithms. Thus it is not clear if the idea of gradient\ndescent with proportional updates is used in the optimal way, or if it could be\nimproved by using a different norm or specific learning rate schedule, for\nexample. Moreover, it is not clear if these algorithms can be extended to other\nproblems, besides neural networks. We attempt to answer these questions by\nestablishing the theoretical analysis of gradient descent with proportional\nupdates, and verifying this analysis with empirical examples. \n\n"}
{"id": "1801.03164", "contents": "Title: Paranom: A Parallel Anomaly Dataset Generator Abstract: In this paper, we present Paranom, a parallel anomaly dataset generator. We\ndiscuss its design and provide brief experimental results demonstrating its\nusefulness in improving the classification correctness of LSTM-AD, a\nstate-of-the-art anomaly detection model. \n\n"}
{"id": "1801.03459", "contents": "Title: Sequential decomposition of repeated games with asymmetric information\n  and dependent states Abstract: We consider a finite horizon repeated game with $N$ selfish players who\nobserve their types privately and take actions, which are publicly observed.\nTheir actions and types jointly determine their instantaneous rewards. In each\nperiod, players jointly observe actions of each other with delay 1, and private\nobservations of the state of the system, and get an instantaneous reward which\nis a function of the state and everyone's actions. The players' types are\nstatic and are potentially correlated among players.\n  An appropriate notion of equilibrium for such games is Perfect Bayesian\nEquilibrium (PBE) which consists of a strategy and a belief profile of the\nplayers which is coupled across time and as a result, the complexity of finding\nsuch equilibria grows double-exponentially in time. We present a sequential\ndecomposition methodology to compute \\emph{structured perfect Bayesian\nequilibria} (SPBE) of this game, introduced in~\\cite{VaAn15arxiv}, where\nequilibrium policy of a player is a function of a common belief and a private\nstate. This methodology computes SPBE in linear time. In general, the SPBE of\nthe game problem exhibit \\textit{signaling} behavior, i.e. players' actions\nreveal part of their private information that is payoff relevant to other\nplayers. \n\n"}
{"id": "1801.04342", "contents": "Title: Combining Symbolic Expressions and Black-box Function Evaluations in\n  Neural Programs Abstract: Neural programming involves training neural networks to learn programs,\nmathematics, or logic from data. Previous works have failed to achieve good\ngeneralization performance, especially on problems and programs with high\ncomplexity or on large domains. This is because they mostly rely either on\nblack-box function evaluations that do not capture the structure of the\nprogram, or on detailed execution traces that are expensive to obtain, and\nhence the training data has poor coverage of the domain under consideration. We\npresent a novel framework that utilizes black-box function evaluations, in\nconjunction with symbolic expressions that define relationships between the\ngiven functions. We employ tree LSTMs to incorporate the structure of the\nsymbolic expression trees. We use tree encoding for numbers present in function\nevaluation data, based on their decimal representation. We present an\nevaluation benchmark for this task to demonstrate our proposed model combines\nsymbolic reasoning and function evaluation in a fruitful manner, obtaining high\naccuracies in our experiments. Our framework generalizes significantly better\nto expressions of higher depth and is able to fill partial equations with valid\ncompletions. \n\n"}
{"id": "1801.04569", "contents": "Title: Towards Realistic Threat Modeling: Attack Commodification, Irrelevant\n  Vulnerabilities, and Unrealistic Assumptions Abstract: Current threat models typically consider all possible ways an attacker can\npenetrate a system and assign probabilities to each path according to some\nmetric (e.g. time-to-compromise). In this paper we discuss how this view\nhinders the realness of both technical (e.g. attack graphs) and strategic (e.g.\ngame theory) approaches of current threat modeling, and propose to steer away\nby looking more carefully at attack characteristics and attacker environment.\nWe use a toy threat model for ICS attacks to show how a realistic view of\nattack instances can emerge from a simple analysis of attack phases and\nattacker limitations. \n\n"}
{"id": "1801.05394", "contents": "Title: Time Series Segmentation through Automatic Feature Learning Abstract: Internet of things (IoT) applications have become increasingly popular in\nrecent years, with applications ranging from building energy monitoring to\npersonal health tracking and activity recognition. In order to leverage these\ndata, automatic knowledge extraction - whereby we map from observations to\ninterpretable states and transitions - must be done at scale. As such, we have\nseen many recent IoT data sets include annotations with a human expert\nspecifying states, recorded as a set of boundaries and associated labels in a\ndata sequence. These data can be used to build automatic labeling algorithms\nthat produce labels as an expert would. Here, we refer to human-specified\nboundaries as breakpoints. Traditional changepoint detection methods only look\nfor statistically-detectable boundaries that are defined as abrupt variations\nin the generative parameters of a data sequence. However, we observe that\nbreakpoints occur on more subtle boundaries that are non-trivial to detect with\nthese statistical methods. In this work, we propose a new unsupervised\napproach, based on deep learning, that outperforms existing techniques and\nlearns the more subtle, breakpoint boundaries with a high accuracy. Through\nextensive experiments on various real-world data sets - including\nhuman-activity sensing data, speech signals, and electroencephalogram (EEG)\nactivity traces - we demonstrate the effectiveness of our algorithm for\npractical applications. Furthermore, we show that our approach achieves\nsignificantly better performance than previous methods. \n\n"}
{"id": "1801.07215", "contents": "Title: Get Your Workload in Order: Game Theoretic Prioritization of Database\n  Auditing Abstract: For enhancing the privacy protections of databases, where the increasing\namount of detailed personal data is stored and processed, multiple mechanisms\nhave been developed, such as audit logging and alert triggers, which notify\nadministrators about suspicious activities; however, the two main limitations\nin common are: 1) the volume of such alerts is often substantially greater than\nthe capabilities of resource-constrained organizations, and 2) strategic\nattackers may disguise their actions or carefully choosing which records they\ntouch, making incompetent the statistical detection models. For solving them,\nwe introduce a novel approach to database auditing that explicitly accounts for\nadversarial behavior by 1) prioritizing the order in which types of alerts are\ninvestigated and 2) providing an upper bound on how much resource to allocate\nfor each type. We model the interaction between a database auditor and\npotential attackers as a Stackelberg game in which the auditor chooses an\nauditing policy and attackers choose which records to target. A corresponding\napproach combining linear programming, column generation, and heuristic search\nis proposed to derive an auditing policy. For testing the policy-searching\nperformance, a publicly available credit card application dataset are adopted,\non which it shows that our methods produce high-quality mixed strategies as\ndatabase audit policies, and our general approach significantly outperforms\nnon-game-theoretic baselines. \n\n"}
{"id": "1801.07384", "contents": "Title: Hybrid Gradient Boosting Trees and Neural Networks for Forecasting\n  Operating Room Data Abstract: Time series data constitutes a distinct and growing problem in machine\nlearning. As the corpus of time series data grows larger, deep models that\nsimultaneously learn features and classify with these features can be\nintractable or suboptimal. In this paper, we present feature learning via long\nshort term memory (LSTM) networks and prediction via gradient boosting trees\n(XGB). Focusing on the consequential setting of electronic health record data,\nwe predict the occurrence of hypoxemia five minutes into the future based on\npast features. We make two observations: 1) long short term memory networks are\neffective at capturing long term dependencies based on a single feature and 2)\ngradient boosting trees are capable of tractably combining a large number of\nfeatures including static features like height and weight. With these\nobservations in mind, we generate features by performing \"supervised\"\nrepresentation learning with LSTM networks. Augmenting the original XGB model\nwith these features gives significantly better performance than either\nindividual method. \n\n"}
{"id": "1801.09346", "contents": "Title: Representing the Insincere: Strategically Robust Proportional\n  Representation Abstract: Proportional representation (PR) is a fundamental principle of many\ndemocracies world-wide which employ PR-based voting rules to elect their\nrepresentatives. The normative properties of these voting rules however, are\noften only understood in the context of sincere voting.\n  In this paper we consider PR in the presence of strategic voters. We\nconstruct a voting rule such that for every preference profile there exists at\nleast one costly voting equilibrium satisfying PR with respect to voters'\nprivate and unrevealed preferences - such a voting rule is said to be\nstrategically robust. In contrast, a commonly applied voting rule is shown not\nbe strategically robust. Furthermore, we prove a limit on `how strategically\nrobust' a PR-based voting rule can be; we show that there is no PR-based voting\nrule which ensures that every equilibrium satisfies PR. Collectively, our\nresults highlight the possibility and limit of achieving PR in the presence of\nstrategic voters and a positive role for mechanisms, such as pre-election\npolls, which coordinate voter behaviour towards equilibria which satisfy PR. \n\n"}
{"id": "1802.00899", "contents": "Title: Learning Parametric Closed-Loop Policies for Markov Potential Games Abstract: Multiagent systems where agents interact among themselves and with a\nstochastic environment can be formalized as stochastic games. We study a\nsubclass named Markov potential games (MPGs) that appear often in economic and\nengineering applications when the agents share a common resource. We consider\nMPGs with continuous state-action variables, coupled constraints and nonconvex\nrewards. Previous analysis followed a variational approach that is only valid\nfor very simple cases (convex rewards, invertible dynamics, and no coupled\nconstraints); or considered deterministic dynamics and provided open-loop (OL)\nanalysis, studying strategies that consist in predefined action sequences,\nwhich are not optimal for stochastic environments. We present a closed-loop\n(CL) analysis for MPGs and consider parametric policies that depend on the\ncurrent state. We provide easily verifiable, sufficient and necessary\nconditions for a stochastic game to be an MPG, even for complex parametric\nfunctions (e.g., deep neural networks); and show that a closed-loop Nash\nequilibrium (NE) can be found (or at least approximated) by solving a related\noptimal control problem (OCP). This is useful since solving an OCP--which is a\nsingle-objective problem--is usually much simpler than solving the original set\nof coupled OCPs that form the game--which is a multiobjective control problem.\nThis is a considerable improvement over the previously standard approach for\nthe CL analysis of MPGs, which gives no approximate solution if no NE belongs\nto the chosen parametric family, and which is practical only for simple\nparametric forms. We illustrate the theoretical contributions with an example\nby applying our approach to a noncooperative communications engineering game.\nWe then solve the game with a deep reinforcement learning algorithm that learns\npolicies that closely approximates an exact variational NE of the game. \n\n"}
{"id": "1802.02618", "contents": "Title: A Diversity-based Substation Cyber Defense Strategy utilizing Coloring\n  Games Abstract: Growing cybersecurity risks in the power grid require that utilities\nimplement a variety of security mechanism (SM) composed mostly of VPNs,\nfirewalls, or other custom security components. While they provide some\nprotection, they might contain software vulnerabilities which can lead to a\ncyber-attack. In this paper, the severity of a cyber-attack has been decreased\nby employing a diverse set of SM that reduce repetition of a single\nvulnerability. This paper focuses on the allocation of diverse SM and tries to\nincrease the security of the cyber assets located within the electronic\nsecurity perimeter(ESP) of a substation. We have used a graph-based coloring\ngame in a distributed manner to allocate diverse SM for protecting the cyber\nassets. The vulnerability assessment for power grid network is also analyzed\nusing this game theoretic method. An improved, diversified SMs for worst-case\nscenario has been demonstrated by reaching the Nash equilibrium of graph\ncoloring game. As a case study, we analyze the IEEE-14 and IEEE-118 bus system,\nobserve the different distributed coloring algorithm for allocating diverse SM\nand calculating the overall network criticality. \n\n"}
{"id": "1802.02751", "contents": "Title: Monopoly pricing with buyer search Abstract: In many shopping scenarios, e.g., in online shopping, customers have a large\nmenu of options to choose from. However, most of the buyers do not browse all\nthe options and make decision after considering only a small part of the menu.\nTo study such buyer's behavior we consider the standard Bayesian monopoly\nproblem for a unit-demand buyer, where the monopolist displays the menu\ndynamically page after a page to the buyer. The seller aims to maximize the\nexpected revenue over the distribution of buyer's values which we assume are\ni.i.d. The buyer incurs a fixed cost for browsing through one menu page and\nwould stop if that cost exceeds the increase in her utility. We observe that\nthe optimal posted price mechanism in our dynamic setting may have quite\ndifferent structure than in the classic static scenario. We find a (relatively)\nsimple and approximately optimal mechanism, that uses part of the items as a\n\"bait\" to keep the buyer interested for multiple rounds with low prices, while\nat the same time showing many other expensive items. \n\n"}
{"id": "1802.02850", "contents": "Title: Detection Games Under Fully Active Adversaries Abstract: We study a binary hypothesis testing problem in which a defender must decide\nwhether or not a test sequence has been drawn from a given memoryless source\n$P_0$ whereas, an attacker strives to impede the correct detection. With\nrespect to previous works, the adversarial setup addressed in this paper\nconsiders an attacker who is active under both hypotheses, namely, a fully\nactive attacker, as opposed to a partially active attacker who is active under\none hypothesis only. In the fully active setup, the attacker distorts sequences\ndrawn both from $P_0$ and from an alternative memoryless source $P_1$, up to a\ncertain distortion level, which is possibly different under the two hypotheses,\nin order to maximize the confusion in distinguishing between the two sources,\ni.e., to induce both false positive and false negative errors at the detector,\nalso referred to as the defender. We model the defender-attacker interaction as\na game and study two versions of this game, the Neyman-Pearson game and the\nBayesian game. Our main result is in the characterization of an attack strategy\nthat is asymptotically both dominant (i.e., optimal no matter what the\ndefender's strategy is) and universal, i.e., independent of $P_0$ and $P_1$.\nFrom the analysis of the equilibrium payoff, we also derive the best achievable\nperformance of the defender, by relaxing the requirement on the exponential\ndecay rate of the false positive error probability in the Neyman--Pearson setup\nand the tradeoff between the error exponents in the Bayesian setup. Such\nanalysis permits to characterize the conditions for the distinguishability of\nthe two sources given the distortion levels. \n\n"}
{"id": "1802.03337", "contents": "Title: Large Scale Constrained Linear Regression Revisited: Faster Algorithms\n  via Preconditioning Abstract: In this paper, we revisit the large-scale constrained linear regression\nproblem and propose faster methods based on some recent developments in\nsketching and optimization. Our algorithms combine (accelerated) mini-batch SGD\nwith a new method called two-step preconditioning to achieve an approximate\nsolution with a time complexity lower than that of the state-of-the-art\ntechniques for the low precision case. Our idea can also be extended to the\nhigh precision case, which gives an alternative implementation to the Iterative\nHessian Sketch (IHS) method with significantly improved time complexity.\nExperiments on benchmark and synthetic datasets suggest that our methods indeed\noutperform existing ones considerably in both the low and high precision cases. \n\n"}
{"id": "1802.03487", "contents": "Title: Small nonlinearities in activation functions create bad local minima in\n  neural networks Abstract: We investigate the loss surface of neural networks. We prove that even for\none-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks\nhave spurious local minima in most cases. Our results thus indicate that in\ngeneral \"no spurious local minima\" is a property limited to deep linear\nnetworks, and insights obtained from linear networks may not be robust.\nSpecifically, for ReLU(-like) networks we constructively prove that for almost\nall practical datasets there exist infinitely many local minima. We also\npresent a counterexample for more general activations (sigmoid, tanh, arctan,\nReLU, etc.), for which there exists a bad local minimum. Our results make the\nleast restrictive assumptions relative to existing results on spurious local\noptima in neural networks. We complete our discussion by presenting a\ncomprehensive characterization of global optimality for deep linear networks,\nwhich unifies other results on this topic. \n\n"}
{"id": "1802.03765", "contents": "Title: Convex Formulations for Fair Principal Component Analysis Abstract: Though there is a growing body of literature on fairness for supervised\nlearning, the problem of incorporating fairness into unsupervised learning has\nbeen less well-studied. This paper studies fairness in the context of principal\ncomponent analysis (PCA). We first present a definition of fairness for\ndimensionality reduction, and our definition can be interpreted as saying that\na reduction is fair if information about a protected class (e.g., race or\ngender) cannot be inferred from the dimensionality-reduced data points. Next,\nwe develop convex optimization formulations that can improve the fairness (with\nrespect to our definition) of PCA and kernel PCA. These formulations are\nsemidefinite programs (SDP's), and we demonstrate the effectiveness of our\nformulations using several datasets. We conclude by showing how our approach\ncan be used to perform a fair (with respect to age) clustering of health data\nthat may be used to set health insurance rates. \n\n"}
{"id": "1802.03774", "contents": "Title: On Kernel Method-Based Connectionist Models and Supervised Deep Learning\n  Without Backpropagation Abstract: We propose a novel family of connectionist models based on kernel machines\nand consider the problem of learning layer-by-layer a compositional hypothesis\nclass, i.e., a feedforward, multilayer architecture, in a supervised setting.\nIn terms of the models, we present a principled method to \"kernelize\" (partly\nor completely) any neural network (NN). With this method, we obtain a\ncounterpart of any given NN that is powered by kernel machines instead of\nneurons. In terms of learning, when learning a feedforward deep architecture in\na supervised setting, one needs to train all the components simultaneously\nusing backpropagation (BP) since there are no explicit targets for the hidden\nlayers (Rumelhart86). We consider without loss of generality the two-layer case\nand present a general framework that explicitly characterizes a target for the\nhidden layer that is optimal for minimizing the objective function of the\nnetwork. This characterization then makes possible a purely greedy training\nscheme that learns one layer at a time, starting from the input layer. We\nprovide realizations of the abstract framework under certain architectures and\nobjective functions. Based on these realizations, we present a layer-wise\ntraining algorithm for an l-layer feedforward network for classification, where\nl>=2 can be arbitrary. This algorithm can be given an intuitive geometric\ninterpretation that makes the learning dynamics transparent. Empirical results\nare provided to complement our theory. We show that the kernelized networks,\ntrained layer-wise, compare favorably with classical kernel machines as well as\nother connectionist models trained by BP. We also visualize the inner workings\nof the greedy kernelized models to validate our claim on the transparency of\nthe layer-wise algorithm. \n\n"}
{"id": "1802.03788", "contents": "Title: Influence-Directed Explanations for Deep Convolutional Networks Abstract: We study the problem of explaining a rich class of behavioral properties of\ndeep neural networks. Distinctively, our influence-directed explanations\napproach this problem by peering inside the network to identify neurons with\nhigh influence on a quantity and distribution of interest, using an\naxiomatically-justified influence measure, and then providing an interpretation\nfor the concepts these neurons represent. We evaluate our approach by\ndemonstrating a number of its unique capabilities on convolutional neural\nnetworks trained on ImageNet. Our evaluation demonstrates that\ninfluence-directed explanations (1) identify influential concepts that\ngeneralize across instances, (2) can be used to extract the \"essence\" of what\nthe network learned about a class, and (3) isolate individual features the\nnetwork uses to make decisions and distinguish related classes. \n\n"}
{"id": "1802.03866", "contents": "Title: Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex\n  Optimization Abstract: The problem of minimizing sum-of-nonconvex functions (i.e., convex functions\nthat are average of non-convex ones) is becoming increasingly important in\nmachine learning, and is the core machinery for PCA, SVD, regularized Newton's\nmethod, accelerated non-convex optimization, and more.\n  We show how to provably obtain an accelerated stochastic algorithm for\nminimizing sum-of-nonconvex functions, by $\\textit{adding one additional line}$\nto the well-known SVRG method. This line corresponds to momentum, and shows how\nto directly apply momentum to the finite-sum stochastic minimization of\nsum-of-nonconvex functions. As a side result, our method enjoys linear parallel\nspeed-up using mini-batch. \n\n"}
{"id": "1802.03900", "contents": "Title: Q-learning with Nearest Neighbors Abstract: We consider model-free reinforcement learning for infinite-horizon discounted\nMarkov Decision Processes (MDPs) with a continuous state space and unknown\ntransition kernel, when only a single sample path under an arbitrary policy of\nthe system is available. We consider the Nearest Neighbor Q-Learning (NNQL)\nalgorithm to learn the optimal Q function using nearest neighbor regression\nmethod. As the main contribution, we provide tight finite sample analysis of\nthe convergence rate. In particular, for MDPs with a $d$-dimensional state\nspace and the discounted factor $\\gamma \\in (0,1)$, given an arbitrary sample\npath with \"covering time\" $ L $, we establish that the algorithm is guaranteed\nto output an $\\varepsilon$-accurate estimate of the optimal Q-function using\n$\\tilde{O}\\big(L/(\\varepsilon^3(1-\\gamma)^7)\\big)$ samples. For instance, for a\nwell-behaved MDP, the covering time of the sample path under the purely random\npolicy scales as $ \\tilde{O}\\big(1/\\varepsilon^d\\big),$ so the sample\ncomplexity scales as $\\tilde{O}\\big(1/\\varepsilon^{d+3}\\big).$ Indeed, we\nestablish a lower bound that argues that the dependence of $\n\\tilde{\\Omega}\\big(1/\\varepsilon^{d+2}\\big)$ is necessary. \n\n"}
{"id": "1802.04942", "contents": "Title: Isolating Sources of Disentanglement in Variational Autoencoders Abstract: We decompose the evidence lower bound to show the existence of a term\nmeasuring the total correlation between latent variables. We use this to\nmotivate our $\\beta$-TCVAE (Total Correlation Variational Autoencoder), a\nrefinement of the state-of-the-art $\\beta$-VAE objective for learning\ndisentangled representations, requiring no additional hyperparameters during\ntraining. We further propose a principled classifier-free measure of\ndisentanglement called the mutual information gap (MIG). We perform extensive\nquantitative and qualitative experiments, in both restricted and non-restricted\nsettings, and show a strong relation between total correlation and\ndisentanglement, when the latent variables model is trained using our\nframework. \n\n"}
{"id": "1802.04966", "contents": "Title: Destination Choice Game: A Spatial Interaction Theory on Human Mobility Abstract: With remarkable significance in migration prediction, global disease\nmitigation, urban planning and many others, an arresting challenge is to\npredict human mobility fluxes between any two locations. A number of methods\nhave been proposed against the above challenge, including the gravity model,\nthe intervening opportunity model, the radiation model, the population-weighted\nopportunity model, and so on. Despite their theoretical elegance, all models\nignored an intuitive and important ingredient in individual decision about\nwhere to go, that is, the possible congestion on the way and the possible\ncrowding in the destination. Here we propose a microscopic mechanism underlying\nmobility decisions, named destination choice game (DCG), which takes into\naccount the crowding effects resulted from spatial interactions among\nindividuals. In comparison with the state-of-the-art models, the present one\nshows more accurate prediction on mobility fluxes across wide scales from\nintracity trips to intercity travels, and further to internal migrations. The\nwell-known gravity model is proved to be the equilibrium solution of a\ndegenerated DCG neglecting the crowding effects in the destinations. \n\n"}
{"id": "1802.05027", "contents": "Title: Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical\n  Care Abstract: Patients in the intensive care unit (ICU) require constant and close\nsupervision. To assist clinical staff in this task, hospitals use monitoring\nsystems that trigger audiovisual alarms if their algorithms indicate that a\npatient's condition may be worsening. However, current monitoring systems are\nextremely sensitive to movement artefacts and technical errors. As a result,\nthey typically trigger hundreds to thousands of false alarms per patient per\nday - drowning the important alarms in noise and adding to the exhaustion of\nclinical staff. In this setting, data is abundantly available, but obtaining\ntrustworthy annotations by experts is laborious and expensive. We frame the\nproblem of false alarm reduction from multivariate time series as a\nmachine-learning task and address it with a novel multitask network\narchitecture that utilises distant supervision through multiple related\nauxiliary tasks in order to reduce the number of expensive labels required for\ntraining. We show that our approach leads to significant improvements over\nseveral state-of-the-art baselines on real-world ICU data and provide new\ninsights on the importance of task selection and architectural choices in\ndistantly supervised multitask learning. \n\n"}
{"id": "1802.05155", "contents": "Title: A Diffusion Approximation Theory of Momentum SGD in Nonconvex\n  Optimization Abstract: Momentum Stochastic Gradient Descent (MSGD) algorithm has been widely applied\nto many nonconvex optimization problems in machine learning, e.g., training\ndeep neural networks, variational Bayesian inference, and etc. Despite its\nempirical success, there is still a lack of theoretical understanding of\nconvergence properties of MSGD. To fill this gap, we propose to analyze the\nalgorithmic behavior of MSGD by diffusion approximations for nonconvex\noptimization problems with strict saddle points and isolated local optima. Our\nstudy shows that the momentum helps escape from saddle points, but hurts the\nconvergence within the neighborhood of optima (if without the step size\nannealing or momentum annealing). Our theoretical discovery partially\ncorroborates the empirical success of MSGD in training deep neural networks. \n\n"}
{"id": "1802.05249", "contents": "Title: Distributionally Robust Submodular Maximization Abstract: Submodular functions have applications throughout machine learning, but in\nmany settings, we do not have direct access to the underlying function $f$. We\nfocus on stochastic functions that are given as an expectation of functions\nover a distribution $P$. In practice, we often have only a limited set of\nsamples $f_i$ from $P$. The standard approach indirectly optimizes $f$ by\nmaximizing the sum of $f_i$. However, this ignores generalization to the true\n(unknown) distribution. In this paper, we achieve better performance on the\nactual underlying function $f$ by directly optimizing a combination of bias and\nvariance. Algorithmically, we accomplish this by showing how to carry out\ndistributionally robust optimization (DRO) for submodular functions, providing\nefficient algorithms backed by theoretical guarantees which leverage several\nnovel contributions to the general theory of DRO. We also show compelling\nempirical evidence that DRO improves generalization to the unknown stochastic\nsubmodular function. \n\n"}
{"id": "1802.05584", "contents": "Title: Convolutional Analysis Operator Learning: Acceleration and Convergence Abstract: Convolutional operator learning is gaining attention in many signal\nprocessing and computer vision applications. Learning kernels has mostly relied\non so-called patch-domain approaches that extract and store many overlapping\npatches across training signals. Due to memory demands, patch-domain methods\nhave limitations when learning kernels from large datasets -- particularly with\nmulti-layered structures, e.g., convolutional neural networks -- or when\napplying the learned kernels to high-dimensional signal recovery problems. The\nso-called convolution approach does not store many overlapping patches, and\nthus overcomes the memory problems particularly with careful algorithmic\ndesigns; it has been studied within the \"synthesis\" signal model, e.g.,\nconvolutional dictionary learning. This paper proposes a new convolutional\nanalysis operator learning (CAOL) framework that learns an analysis sparsifying\nregularizer with the convolution perspective, and develops a new convergent\nBlock Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve\nthe corresponding block multi-nonconvex problems. To learn diverse filters\nwithin the CAOL framework, this paper introduces an orthogonality constraint\nthat enforces a tight-frame filter condition, and a regularizer that promotes\ndiversity between filters. Numerical experiments show that, with sharp\nmajorizers, BPEG-M significantly accelerates the CAOL convergence rate compared\nto the state-of-the-art block proximal gradient (BPG) method. Numerical\nexperiments for sparse-view computational tomography show that a convolutional\nsparsifying regularizer learned via CAOL significantly improves reconstruction\nquality compared to a conventional edge-preserving regularizer. Using more and\nwider kernels in a learned regularizer better preserves edges in reconstructed\nimages. \n\n"}
{"id": "1802.05757", "contents": "Title: Stochastic Wasserstein Barycenters Abstract: We present a stochastic algorithm to compute the barycenter of a set of\nprobability distributions under the Wasserstein metric from optimal transport.\nUnlike previous approaches, our method extends to continuous input\ndistributions and allows the support of the barycenter to be adjusted in each\niteration. We tackle the problem without regularization, allowing us to recover\na sharp output whose support is contained within the support of the true\nbarycenter. We give examples where our algorithm recovers a more meaningful\nbarycenter than previous work. Our method is versatile and can be extended to\napplications such as generating super samples from a given distribution and\nrecovering blue noise approximations. \n\n"}
{"id": "1802.06132", "contents": "Title: Interaction Matters: A Note on Non-asymptotic Local Convergence of\n  Generative Adversarial Networks Abstract: Motivated by the pursuit of a systematic computational and algorithmic\nunderstanding of Generative Adversarial Networks (GANs), we present a simple\nyet unified non-asymptotic local convergence theory for smooth two-player\ngames, which subsumes several discrete-time gradient-based saddle point\ndynamics. The analysis reveals the surprising nature of the off-diagonal\ninteraction term as both a blessing and a curse. On the one hand, this\ninteraction term explains the origin of the slow-down effect in the convergence\nof Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other\nhand, for the unstable equilibria, exponential convergence can be proved thanks\nto the interaction term, for four modified dynamics proposed to stabilize GAN\ntraining: Optimistic Mirror Descent (OMD), Consensus Optimization (CO),\nImplicit Updates (IU) and Predictive Method (PM). The analysis uncovers the\nintimate connections among these stabilizing techniques, and provides detailed\ncharacterization on the choice of learning rate. As a by-product, we present a\nnew analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017]\nwith improved rates. \n\n"}
{"id": "1802.06293", "contents": "Title: Black-Box Reductions for Parameter-free Online Learning in Banach Spaces Abstract: We introduce several new black-box reductions that significantly improve the\ndesign of adaptive and parameter-free online learning algorithms by simplifying\nanalysis, improving regret guarantees, and sometimes even improving runtime. We\nreduce parameter-free online learning to online exp-concave optimization, we\nreduce optimization in a Banach space to one-dimensional optimization, and we\nreduce optimization over a constrained domain to unconstrained optimization.\nAll of our reductions run as fast as online gradient descent. We use our new\ntechniques to improve upon the previously best regret bounds for parameter-free\nlearning, and do so for arbitrary norms. \n\n"}
{"id": "1802.06476", "contents": "Title: Simultaneous Modeling of Multiple Complications for Risk Profiling in\n  Diabetes Care Abstract: Type 2 diabetes mellitus (T2DM) is a chronic disease that often results in\nmultiple complications. Risk prediction and profiling of T2DM complications is\ncritical for healthcare professionals to design personalized treatment plans\nfor patients in diabetes care for improved outcomes. In this paper, we study\nthe risk of developing complications after the initial T2DM diagnosis from\nlongitudinal patient records. We propose a novel multi-task learning approach\nto simultaneously model multiple complications where each task corresponds to\nthe risk modeling of one complication. Specifically, the proposed method\nstrategically captures the relationships (1) between the risks of multiple T2DM\ncomplications, (2) between the different risk factors, and (3) between the risk\nfactor selection patterns. The method uses coefficient shrinkage to identify an\ninformative subset of risk factors from high-dimensional data, and uses a\nhierarchical Bayesian framework to allow domain knowledge to be incorporated as\npriors. The proposed method is favorable for healthcare applications because in\nadditional to improved prediction performance, relationships among the\ndifferent risks and risk factors are also identified. Extensive experimental\nresults on a large electronic medical claims database show that the proposed\nmethod outperforms state-of-the-art models by a significant margin.\nFurthermore, we show that the risk associations learned and the risk factors\nidentified lead to meaningful clinical insights. \n\n"}
{"id": "1802.07384", "contents": "Title: Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic\n  Corrections Abstract: We present a new algorithm to generate minimal, stable, and symbolic\ncorrections to an input that will cause a neural network with ReLU activations\nto change its output. We argue that such a correction is a useful way to\nprovide feedback to a user when the network's output is different from a\ndesired output. Our algorithm generates such a correction by solving a series\nof linear constraint satisfaction problems. The technique is evaluated on three\nneural network models: one predicting whether an applicant will pay a mortgage,\none predicting whether a first-order theorem can be proved efficiently by a\nsolver using certain heuristics, and the final one judging whether a drawing is\nan accurate rendition of a canonical drawing of a cat. \n\n"}
{"id": "1802.07712", "contents": "Title: Condition numbers of stochastic mean payoff games and what they say\n  about nonarchimedean semidefinite programming Abstract: Semidefinite programming can be considered over any real closed field,\nincluding fields of Puiseux series equipped with their nonarchimedean\nvaluation. Nonarchimedean semidefinite programs encode parametric families of\nclassical semidefinite programs, for sufficiently large values of the\nparameter. Recently, a correspondence has been established between\nnonarchimedean semidefinite programs and stochastic mean payoff games with\nperfect information. This correspondence relies on tropical geometry. It allows\none to solve generic nonarchimedean semidefinite feasibility problems, of large\nscale, by means of stochastic game algorithms. In this paper, we show that the\nmean payoff of these games can be interpreted as a condition number for the\ncorresponding nonarchimedean feasibility problems. This number measures how\nclose a feasible instance is from being infeasible, and vice versa. We show\nthat it coincides with the maximal radius of a ball in Hilbert's projective\nmetric, that is included in the feasible set. The geometric interpretation of\nthe condition number relies in particular on a duality theorem for tropical\nsemidefinite feasibility programs. Then, we bound the complexity of the\nfeasibility problem in terms of the condition number. We finally give explicit\nbounds for this condition number, in terms of the characteristics of the\nstochastic game. As a consequence, we show that the simplest algorithm to\ndecide whether a stochastic mean payoff game is winning, namely value\niteration, has a pseudopolynomial complexity when the number of random\npositions is fixed. \n\n"}
{"id": "1802.07814", "contents": "Title: Learning to Explain: An Information-Theoretic Perspective on Model\n  Interpretation Abstract: We introduce instancewise feature selection as a methodology for model\ninterpretation. Our method is based on learning a function to extract a subset\nof features that are most informative for each given example. This feature\nselector is trained to maximize the mutual information between selected\nfeatures and the response variable, where the conditional distribution of the\nresponse variable given the input is the model to be explained. We develop an\nefficient variational approximation to the mutual information, and show the\neffectiveness of our method on a variety of synthetic and real data sets using\nboth quantitative metrics and human evaluation. \n\n"}
{"id": "1802.07833", "contents": "Title: Variational Inference for Policy Gradient Abstract: Inspired by the seminal work on Stein Variational Inference and Stein\nVariational Policy Gradient, we derived a method to generate samples from the\nposterior variational parameter distribution by \\textit{explicitly} minimizing\nthe KL divergence to match the target distribution in an amortize fashion.\nConsequently, we applied this varational inference technique into vanilla\npolicy gradient, TRPO and PPO with Bayesian Neural Network parameterizations\nfor reinforcement learning problems. \n\n"}
{"id": "1802.08235", "contents": "Title: Vector Field Based Neural Networks Abstract: A novel Neural Network architecture is proposed using the mathematically and\nphysically rich idea of vector fields as hidden layers to perform nonlinear\ntransformations in the data. The data points are interpreted as particles\nmoving along a flow defined by the vector field which intuitively represents\nthe desired movement to enable classification. The architecture moves the data\npoints from their original configuration to anew one following the streamlines\nof the vector field with the objective of achieving a final configuration where\nclasses are separable. An optimization problem is solved through gradient\ndescent to learn this vector field. \n\n"}
{"id": "1802.08249", "contents": "Title: On the Convergence and Robustness of Training GANs with Regularized\n  Optimal Transport Abstract: Generative Adversarial Networks (GANs) are one of the most practical methods\nfor learning data distributions. A popular GAN formulation is based on the use\nof Wasserstein distance as a metric between probability distributions.\nUnfortunately, minimizing the Wasserstein distance between the data\ndistribution and the generative model distribution is a computationally\nchallenging problem as its objective is non-convex, non-smooth, and even hard\nto compute. In this work, we show that obtaining gradient information of the\nsmoothed Wasserstein GAN formulation, which is based on regularized Optimal\nTransport (OT), is computationally effortless and hence one can apply first\norder optimization methods to minimize this objective. Consequently, we\nestablish theoretical convergence guarantee to stationarity for a proposed\nclass of GAN optimization algorithms. Unlike the original non-smooth\nformulation, our algorithm only requires solving the discriminator to\napproximate optimality. We apply our method to learning MNIST digits as well as\nCIFAR-10images. Our experiments show that our method is computationally\nefficient and generates images comparable to the state of the art algorithms\ngiven the same architecture and computational power. \n\n"}
{"id": "1802.08334", "contents": "Title: Learning Without Mixing: Towards A Sharp Analysis of Linear System\n  Identification Abstract: We prove that the ordinary least-squares (OLS) estimator attains nearly\nminimax optimal performance for the identification of linear dynamical systems\nfrom a single observed trajectory. Our upper bound relies on a generalization\nof Mendelson's small-ball method to dependent data, eschewing the use of\nstandard mixing-time arguments. Our lower bounds reveal that these upper bounds\nmatch up to logarithmic factors. In particular, we capture the correct\nsignal-to-noise behavior of the problem, showing that more unstable linear\nsystems are easier to estimate. This behavior is qualitatively different from\narguments which rely on mixing-time calculations that suggest that unstable\nsystems are more difficult to estimate. We generalize our technique to provide\nbounds for a more general class of linear response time-series. \n\n"}
{"id": "1802.08352", "contents": "Title: Learning to Make Predictions on Graphs with Autoencoders Abstract: We examine two fundamental tasks associated with graph representation\nlearning: link prediction and semi-supervised node classification. We present a\nnovel autoencoder architecture capable of learning a joint representation of\nboth local graph structure and available node features for the multi-task\nlearning of link prediction and node classification. Our autoencoder\narchitecture is efficiently trained end-to-end in a single learning stage to\nsimultaneously perform link prediction and node classification, whereas\nprevious related methods require multiple training steps that are difficult to\noptimize. We provide a comprehensive empirical evaluation of our models on nine\nbenchmark graph-structured datasets and demonstrate significant improvement\nover related methods for graph representation learning. Reference code and data\nare available at https://github.com/vuptran/graph-representation-learning \n\n"}
{"id": "1802.09001", "contents": "Title: The Complexity of the Possible Winner Problem over Partitioned\n  Preferences Abstract: The Possible-Winner problem asks, given an election where the voters'\npreferences over the set of candidates is partially specified, whether a\ndistinguished candidate can become a winner. In this work, we consider the\ncomputational complexity of Possible-Winner under the assumption that the voter\npreferences are $partitioned$. That is, we assume that every voter provides a\ncomplete order over sets of incomparable candidates (e.g., candidates are\nranked by their level of education). We consider elections with partitioned\nprofiles over positional scoring rules, with an unbounded number of candidates,\nand unweighted voters. Our first result is a polynomial time algorithm for\nvoting rules with $2$ distinct values, which include the well-known\n$k$-approval voting rule. We then go on to prove NP-hardness for a class of\nrules that contain all voting rules that produce scoring vectors with at least\n$4$ distinct values. \n\n"}
{"id": "1802.09128", "contents": "Title: Averaging Stochastic Gradient Descent on Riemannian Manifolds Abstract: We consider the minimization of a function defined on a Riemannian manifold\n$\\mathcal{M}$ accessible only through unbiased estimates of its gradients. We\ndevelop a geometric framework to transform a sequence of slowly converging\niterates generated from stochastic gradient descent (SGD) on $\\mathcal{M}$ to\nan averaged iterate sequence with a robust and fast $O(1/n)$ convergence rate.\nWe then present an application of our framework to geodesically-strongly-convex\n(and possibly Euclidean non-convex) problems. Finally, we demonstrate how these\nideas apply to the case of streaming $k$-PCA, where we show how to accelerate\nthe slow rate of the randomized power method (without requiring knowledge of\nthe eigengap) into a robust algorithm achieving the optimal rate of\nconvergence. \n\n"}
{"id": "1802.09158", "contents": "Title: Surrogate Scoring Rules Abstract: Strictly proper scoring rules (SPSR) are incentive compatible for eliciting\ninformation about random variables from strategic agents when the principal can\nreward agents after the realization of the random variables. They also quantify\nthe quality of elicited information, with more accurate predictions receiving\nhigher scores in expectation. In this paper, we extend such scoring rules to\nsettings where a principal elicits private probabilistic beliefs but only has\naccess to agents' reports. We name our solution \\emph{Surrogate Scoring Rules}\n(SSR). SSR build on a bias correction step and an error rate estimation\nprocedure for a reference answer defined using agents' reports. We show that,\nwith a single bit of information about the prior distribution of the random\nvariables, SSR in a multi-task setting recover SPSR in expectation, as if\nhaving access to the ground truth. Therefore, a salient feature of SSR is that\nthey quantify the quality of information despite the lack of ground truth, just\nas SPSR do for the setting \\emph{with} ground truth. As a by-product, SSR\ninduce \\emph{dominant truthfulness} in reporting. Our method is verified both\ntheoretically and empirically using data collected from real human forecasters. \n\n"}
{"id": "1802.10140", "contents": "Title: Towards a Socially Optimal Multi-Modal Routing Platform Abstract: The increasing rate of urbanization has added pressure on the already\nconstrained transportation networks in our communities. Ride-sharing platforms\nsuch as Uber and Lyft are becoming a more commonplace, particularly in urban\nenvironments. While such services may be deemed more convenient than riding\npublic transit due to their on-demand nature, reports show that they do not\nnecessarily decrease the congestion in major cities. One of the key problems is\nthat typically mobility decision support systems focus on individual utility\nand react only after congestion appears. In this paper, we propose socially\nconsiderate multi-modal routing algorithms that are proactive and consider, via\npredictions, the shared effect of riders on the overall efficacy of mobility\nservices. We have adapted the MATSim simulator framework to incorporate the\nproposed algorithms present a simulation analysis of a case study in Nashville,\nTennessee that assesses the effects of our routing models on the traffic\ncongestion for different levels of penetration and adoption of socially\nconsiderate routes. Our results indicate that even at a low penetration (social\nratio), we are able to achieve an improvement in system-level performance. \n\n"}
{"id": "1802.10465", "contents": "Title: Leakage and Protocol Composition in a Game-Theoretic Perspective Abstract: In the inference attacks studied in Quantitative Information Flow (QIF), the\nadversary typically tries to interfere with the system in the attempt to\nincrease its leakage of secret information. The defender, on the other hand,\ntypically tries to decrease leakage by introducing some controlled noise. This\nnoise introduction can be modeled as a type of protocol composition, i.e., a\nprobabilistic choice among different protocols, and its effect on the amount of\nleakage depends heavily on whether or not this choice is visible to the\nadversary. In this work we consider operators for modeling visible and\ninvisible choice in protocol composition, and we study their algebraic\nproperties. We then formalize the interplay between defender and adversary in a\ngame-theoretic framework adapted to the specific issues of QIF, where the\npayoff is information leakage. We consider various kinds of leakage games,\ndepending on whether players act simultaneously or sequentially, and on whether\nor not the choices of the defender are visible to the adversary. Finally, we\nestablish a hierarchy of these games in terms of their information leakage, and\nprovide methods for finding optimal strategies (at the points of equilibrium)\nfor both attacker and defender in the various cases. The full version of this\npaper can be found in arXiv:1803.10042 \n\n"}
{"id": "1802.10551", "contents": "Title: A Variational Inequality Perspective on Generative Adversarial Networks Abstract: Generative adversarial networks (GANs) form a generative modeling approach\nknown for producing appealing samples, but they are notably difficult to train.\nOne common way to tackle this issue has been to propose new formulations of the\nGAN objective. Yet, surprisingly few studies have looked at optimization\nmethods designed for this adversarial training. In this work, we cast GAN\noptimization problems in the general variational inequality framework. Tapping\ninto the mathematical programming literature, we counter some common\nmisconceptions about the difficulties of saddle point optimization and propose\nto extend techniques designed for variational inequalities to the training of\nGANs. We apply averaging, extrapolation and a computationally cheaper variant\nthat we call extrapolation from the past to the stochastic gradient method\n(SGD) and Adam. \n\n"}
{"id": "1803.00101", "contents": "Title: Model-Based Value Estimation for Efficient Model-Free Reinforcement\n  Learning Abstract: Recent model-free reinforcement learning algorithms have proposed\nincorporating learned dynamics models as a source of additional data with the\nintention of reducing sample complexity. Such methods hold the promise of\nincorporating imagined data coupled with a notion of model uncertainty to\naccelerate the learning of continuous control tasks. Unfortunately, they rely\non heuristics that limit usage of the dynamics model. We present model-based\nvalue expansion, which controls for uncertainty in the model by only allowing\nimagination to fixed depth. By enabling wider use of learned dynamics models\nwithin a model-free reinforcement learning algorithm, we improve value\nestimation, which, in turn, reduces the sample complexity of learning. \n\n"}
{"id": "1803.00144", "contents": "Title: Learning Longer-term Dependencies in RNNs with Auxiliary Losses Abstract: Despite recent advances in training recurrent neural networks (RNNs),\ncapturing long-term dependencies in sequences remains a fundamental challenge.\nMost approaches use backpropagation through time (BPTT), which is difficult to\nscale to very long sequences. This paper proposes a simple method that improves\nthe ability to capture long term dependencies in RNNs by adding an unsupervised\nauxiliary loss to the original objective. This auxiliary loss forces RNNs to\neither reconstruct previous events or predict next events in a sequence, making\ntruncated backpropagation feasible for long sequences and also improving full\nBPTT. We evaluate our method on a variety of settings, including pixel-by-pixel\nimage classification with sequence lengths up to 16\\,000, and a real document\nclassification benchmark. Our results highlight good performance and resource\nefficiency of this approach over competitive baselines, including other\nrecurrent models and a comparable sized Transformer. Further analyses reveal\nbeneficial effects of the auxiliary loss on optimization and regularization, as\nwell as extreme cases where there is little to no backpropagation. \n\n"}
{"id": "1803.00158", "contents": "Title: Modeling reverse thinking for machine learning Abstract: Human inertial thinking schemes can be formed through learning, which are\nthen applied to quickly solve similar problems later. However, when problems\nare significantly different, inertial thinking generally presents the solutions\nthat are definitely imperfect. In such cases, people will apply creative\nthinking, such as reverse thinking, to solve problems. Similarly, machine\nlearning methods also form inertial thinking schemes through learning the\nknowledge from a large amount of data. However, when the testing data are\nvastly difference, the formed inertial thinking schemes will inevitably\ngenerate errors. This kind of inertial thinking is called illusion inertial\nthinking. Because all machine learning methods do not consider illusion\ninertial thinking, in this paper we propose a new method that uses reverse\nthinking to correct illusion inertial thinking, which increases the\ngeneralization ability of machine learning methods. Experimental results on\nbenchmark datasets are used to validate the proposed method. \n\n"}
{"id": "1803.00162", "contents": "Title: Towards Cooperation in Sequential Prisoner's Dilemmas: a Deep Multiagent\n  Reinforcement Learning Approach Abstract: The Iterated Prisoner's Dilemma has guided research on social dilemmas for\ndecades. However, it distinguishes between only two atomic actions: cooperate\nand defect. In real-world prisoner's dilemmas, these choices are temporally\nextended and different strategies may correspond to sequences of actions,\nreflecting grades of cooperation. We introduce a Sequential Prisoner's Dilemma\n(SPD) game to better capture the aforementioned characteristics. In this work,\nwe propose a deep multiagent reinforcement learning approach that investigates\nthe evolution of mutual cooperation in SPD games. Our approach consists of two\nphases. The first phase is offline: it synthesizes policies with different\ncooperation degrees and then trains a cooperation degree detection network. The\nsecond phase is online: an agent adaptively selects its policy based on the\ndetected degree of opponent cooperation. The effectiveness of our approach is\ndemonstrated in two representative SPD 2D games: the Apple-Pear game and the\nFruit Gathering game. Experimental results show that our strategy can avoid\nbeing exploited by exploitative opponents and achieve cooperation with\ncooperative opponents. \n\n"}
{"id": "1803.00494", "contents": "Title: Robust Repeated Auctions under Heterogeneous Buyer Behavior Abstract: We study revenue optimization in a repeated auction between a single seller\nand a single buyer. Traditionally, the design of repeated auctions requires\nstrong modeling assumptions about the bidder behavior, such as it being myopic,\ninfinite lookahead, or some specific form of learning behavior. Is it possible\nto design mechanisms which are simultaneously optimal against a multitude of\npossible buyer behaviors? We answer this question by designing a simple\nstate-based mechanism that is simultaneously approximately optimal against a\n$k$-lookahead buyer for all $k$, a buyer who is a no-regret learner, and a\nbuyer who is a policy-regret learner. Against each type of buyer our mechanism\nattains a constant fraction of the optimal revenue attainable against that type\nof buyer. We complement our positive results with almost tight impossibility\nresults, showing that the revenue approximation tradeoffs achieved by our\nmechanism for different lookahead attitudes are near-optimal. \n\n"}
{"id": "1803.00590", "contents": "Title: Hierarchical Imitation and Reinforcement Learning Abstract: We study how to effectively leverage expert feedback to learn sequential\ndecision-making policies. We focus on problems with sparse rewards and long\ntime horizons, which typically pose significant challenges in reinforcement\nlearning. We propose an algorithmic framework, called hierarchical guidance,\nthat leverages the hierarchical structure of the underlying problem to\nintegrate different modes of expert interaction. Our framework can incorporate\ndifferent combinations of imitation learning (IL) and reinforcement learning\n(RL) at different levels, leading to dramatic reductions in both expert effort\nand cost of exploration. Using long-horizon benchmarks, including Montezuma's\nRevenge, we demonstrate that our approach can learn significantly faster than\nhierarchical RL, and be significantly more label-efficient than standard IL. We\nalso theoretically analyze labeling cost for certain instantiations of our\nframework. \n\n"}
{"id": "1803.00607", "contents": "Title: Optimization-Based Algorithm for Evolutionarily Stable Strategies\n  against Pure Mutations Abstract: Evolutionarily stable strategy (ESS) is an important solution concept in game\ntheory which has been applied frequently to biological models. Informally an\nESS is a strategy that if followed by the population cannot be taken over by a\nmutation strategy that is initially rare. Finding such a strategy has been\nshown to be difficult from a theoretical complexity perspective. We present an\nalgorithm for the case where mutations are restricted to pure strategies, and\npresent experiments on several game classes including random and a\nrecently-proposed cancer model. Our algorithm is based on a mixed-integer\nnon-convex feasibility program formulation, which constitutes the first general\noptimization formulation for this problem. It turns out that the vast majority\nof the games included in the experiments contain ESS with small support, and\nour algorithm is outperformed by a support-enumeration based approach. However\nwe suspect our algorithm may be useful in the future as games are studied that\nhave ESS with potentially larger and unknown support size. \n\n"}
{"id": "1803.00916", "contents": "Title: Deep Learning for Signal Authentication and Security in Massive Internet\n  of Things Systems Abstract: Secure signal authentication is arguably one of the most challenging problems\nin the Internet of Things (IoT) environment, due to the large-scale nature of\nthe system and its susceptibility to man-in-the-middle and eavesdropping\nattacks. In this paper, a novel deep learning method is proposed for dynamic\nauthentication of IoT signals to detect cyber attacks. The proposed learning\nframework, based on a long short-term memory (LSTM) structure, enables the IoT\ndevices (IoTDs) to extract a set of stochastic features from their generated\nsignal and dynamically watermark these features into the signal. This method\nenables the cloud, which collects signals from the IoT devices, to effectively\nauthenticate the reliability of the signals. Moreover, in massive IoT\nscenarios, since the cloud cannot authenticate all the IoTDs simultaneously due\nto computational limitations, a game-theoretic framework is proposed to improve\nthe cloud's decision making process by predicting vulnerable IoTDs. The\nmixed-strategy Nash equilibrium (MSNE) for this game is derived and the\nuniqueness of the expected utility at the equilibrium is proven. In the massive\nIoT system, due to a large set of available actions for the cloud, it is shown\nthat analytically deriving the MSNE is challenging and, thus, a learning\nalgorithm proposed that converges to the MSNE. Moreover, in order to cope with\nthe incomplete information case in which the cloud cannot access the state of\nthe unauthenticated IoTDs, a deep reinforcement learning algorithm is proposed\nto dynamically predict the state of unauthenticated IoTDs and allow the cloud\nto decide on which IoTDs to authenticate. Simulation results show that, with an\nattack detection delay of under 1 second the messages can be transmitted from\nIoT devices with an almost 100% reliability. \n\n"}
{"id": "1803.02312", "contents": "Title: Dimensionality Reduction for Stationary Time Series via Stochastic\n  Nonconvex Optimization Abstract: Stochastic optimization naturally arises in machine learning. Efficient\nalgorithms with provable guarantees, however, are still largely missing, when\nthe objective function is nonconvex and the data points are dependent. This\npaper studies this fundamental challenge through a streaming PCA problem for\nstationary time series data. Specifically, our goal is to estimate the\nprinciple component of time series data with respect to the covariance matrix\nof the stationary distribution. Computationally, we propose a variant of Oja's\nalgorithm combined with downsampling to control the bias of the stochastic\ngradient caused by the data dependency. Theoretically, we quantify the\nuncertainty of our proposed stochastic algorithm based on diffusion\napproximations. This allows us to prove the asymptotic rate of convergence and\nfurther implies near optimal asymptotic sample complexity. Numerical\nexperiments are provided to support our analysis. \n\n"}
{"id": "1803.03289", "contents": "Title: Deep Neural Network Compression with Single and Multiple Level\n  Quantization Abstract: Network quantization is an effective solution to compress deep neural\nnetworks for practical usage. Existing network quantization methods cannot\nsufficiently exploit the depth information to generate low-bit compressed\nnetwork. In this paper, we propose two novel network quantization approaches,\nsingle-level network quantization (SLQ) for high-bit quantization and\nmulti-level network quantization (MLQ) for extremely low-bit quantization\n(ternary).We are the first to consider the network quantization from both width\nand depth level. In the width level, parameters are divided into two parts: one\nfor quantization and the other for re-training to eliminate the quantization\nloss. SLQ leverages the distribution of the parameters to improve the width\nlevel. In the depth level, we introduce incremental layer compensation to\nquantize layers iteratively which decreases the quantization loss in each\niteration. The proposed approaches are validated with extensive experiments\nbased on the state-of-the-art neural networks including AlexNet, VGG-16,\nGoogleNet and ResNet-18. Both SLQ and MLQ achieve impressive results. \n\n"}
{"id": "1803.03451", "contents": "Title: Comparative Statics via Stochastic Orderings in a Two-Echelon Market\n  with Upstream Demand Uncertainty Abstract: We revisit the classic Cournot model and extend it to a two-echelon supply\nchain with an upstream supplier who operates under demand uncertainty and\nmultiple downstream retailers who compete over quantity. The supplier's belief\nabout retail demand is modeled via a continuous probability distribution\nfunction F. If F has the decreasing generalized mean residual life (DGMRL)\nproperty, then the supplier's optimal pricing policy exists and is the unique\nfixed point of the mean residual life (MRL) function. This closed form\nrepresentation of the supplier's equilibrium strategy facilitates a transparent\ncomparative statics and sensitivity analysis. We utilize the theory of\nstochastic orderings to study the response of the equilibrium fundamentals -\nwholesale price, retail price and quantity - to different demand distribution\nparameters. We examine supply chain performance, in terms of the distribution\nof profits, supply chain efficiency, in terms of the Price of Anarchy, and\ncomplement our findings with numerical results. \n\n"}
{"id": "1803.04848", "contents": "Title: Soft-Robust Actor-Critic Policy-Gradient Abstract: Robust Reinforcement Learning aims to derive optimal behavior that accounts\nfor model uncertainty in dynamical systems. However, previous studies have\nshown that by considering the worst case scenario, robust policies can be\noverly conservative. Our soft-robust framework is an attempt to overcome this\nissue. In this paper, we present a novel Soft-Robust Actor-Critic algorithm\n(SR-AC). It learns an optimal policy with respect to a distribution over an\nuncertainty set and stays robust to model uncertainty but avoids the\nconservativeness of robust strategies. We show the convergence of SR-AC and\ntest the efficiency of our approach on different domains by comparing it\nagainst regular learning methods and their robust formulations. \n\n"}
{"id": "1803.05501", "contents": "Title: Max-Min Greedy Matching Abstract: A bipartite graph $G(U,V;E)$ that admits a perfect matching is given. One\nplayer imposes a permutation $\\pi$ over $V$, the other player imposes a\npermutation $\\sigma$ over $U$. In the greedy matching algorithm, vertices of\n$U$ arrive in order $\\sigma$ and each vertex is matched to the lowest (under\n$\\pi$) yet unmatched neighbor in $V$ (or left unmatched, if all its neighbors\nare already matched). The obtained matching is maximal, thus matches at least a\nhalf of the vertices. The max-min greedy matching problem asks: suppose the\nfirst (max) player reveals $\\pi$, and the second (min) player responds with the\nworst possible $\\sigma$ for $\\pi$, does there exist a permutation $\\pi$\nensuring to match strictly more than a half of the vertices? Can such a\npermutation be computed in polynomial time?\n  The main result of this paper is an affirmative answer for this question: we\nshow that there exists a polytime algorithm to compute $\\pi$ for which for\nevery $\\sigma$ at least $\\rho > 0.51$ fraction of the vertices of $V$ are\nmatched. We provide additional lower and upper bounds for special families of\ngraphs, including regular and Hamiltonian. Interestingly, even for regular\ngraphs with arbitrarily large degree (implying a large number of disjoint\nperfect matchings), there is no $\\pi$ ensuring to match more than a fraction\n$8/9$ of the vertices.\n  The max-min greedy matching problem solves an open problem regarding the\nwelfare guarantees attainable by pricing in sequential markets with binary\nunit-demand valuations. In addition, it has implications for the size of the\nunique stable matching in markets with global preferences, subject to the graph\nstructure. \n\n"}
{"id": "1803.05999", "contents": "Title: Escaping Saddles with Stochastic Gradients Abstract: We analyze the variance of stochastic gradients along negative curvature\ndirections in certain non-convex machine learning models and show that\nstochastic gradients exhibit a strong component along these directions.\nFurthermore, we show that - contrary to the case of isotropic noise - this\nvariance is proportional to the magnitude of the corresponding eigenvalues and\nnot decreasing in the dimensionality. Based upon this observation we propose a\nnew assumption under which we show that the injection of explicit, isotropic\nnoise usually applied to make gradient descent escape saddle points can\nsuccessfully be replaced by a simple SGD step. Additionally - and under the\nsame condition - we derive the first convergence rate for plain SGD to a\nsecond-order stationary point in a number of iterations that is independent of\nthe problem dimension. \n\n"}
{"id": "1803.06707", "contents": "Title: An Improved Welfare Guarantee for First Price Auctions Abstract: This paper proves that the welfare of the first price auction in Bayes-Nash\nequilibrium is at least a $.743$-fraction of the welfare of the optimal\nmechanism assuming agents' values are independently distributed. The previous\nbest bound was $1-1/e \\approx .63$, derived in Syrgkanis and Tardos (2013)\nusing smoothness, the standard technique for reasoning about welfare of games\nin equilibrium. In the worst known example (from Hartline et al. (2014)), the\nfirst price auction achieves a $\\approx .869$-fraction of the optimal welfare,\nfar better than the theoretical guarantee. Despite this large gap, it was\nunclear whether the $1-1/e \\approx .63$ bound was tight. We prove that it is\nnot. Our analysis eschews smoothness, and instead uses the independence\nassumption on agents' value distributions to give a more careful accounting of\nthe welfare contribution of agents who win despite not having the highest\nvalue. \n\n"}
{"id": "1803.07246", "contents": "Title: Variance Reduction for Policy Gradient with Action-Dependent Factorized\n  Baselines Abstract: Policy gradient methods have enjoyed great success in deep reinforcement\nlearning but suffer from high variance of gradient estimates. The high variance\nproblem is particularly exasperated in problems with long horizons or\nhigh-dimensional action spaces. To mitigate this issue, we derive a bias-free\naction-dependent baseline for variance reduction which fully exploits the\nstructural form of the stochastic policy itself and does not make any\nadditional assumptions about the MDP. We demonstrate and quantify the benefit\nof the action-dependent baseline through both theoretical analysis as well as\nnumerical results, including an analysis of the suboptimality of the optimal\nstate-dependent baseline. The result is a computationally efficient policy\ngradient algorithm, which scales to high-dimensional control problems, as\ndemonstrated by a synthetic 2000-dimensional target matching task. Our\nexperimental results indicate that action-dependent baselines allow for faster\nlearning on standard reinforcement learning benchmarks and high-dimensional\nhand manipulation and synthetic tasks. Finally, we show that the general idea\nof including additional information in baselines for improved variance\nreduction can be extended to partially observed and multi-agent tasks. \n\n"}
{"id": "1803.07300", "contents": "Title: Risk and parameter convergence of logistic regression Abstract: Gradient descent, when applied to the task of logistic regression, outputs\niterates which are biased to follow a unique ray defined by the data. The\ndirection of this ray is the maximum margin predictor of a maximal linearly\nseparable subset of the data; the gradient descent iterates converge to this\nray in direction at the rate $\\mathcal{O}(\\ln\\ln t / \\ln t)$. The ray does not\npass through the origin in general, and its offset is the bounded global\noptimum of the risk over the remaining data; gradient descent recovers this\noffset at a rate $\\mathcal{O}((\\ln t)^2 / \\sqrt{t})$. \n\n"}
{"id": "1803.07482", "contents": "Title: Natural Gradient Deep Q-learning Abstract: We present a novel algorithm to train a deep Q-learning agent using\nnatural-gradient techniques. We compare the original deep Q-network (DQN)\nalgorithm to its natural-gradient counterpart, which we refer to as NGDQN, on a\ncollection of classic control domains. Without employing target networks, NGDQN\nsignificantly outperforms DQN without target networks, and performs no worse\nthan DQN with target networks, suggesting that NGDQN stabilizes training and\ncan help reduce the need for additional hyperparameter tuning. We also find\nthat NGDQN is less sensitive to hyperparameter optimization relative to DQN.\nTogether these results suggest that natural-gradient techniques can improve\nvalue-function optimization in deep reinforcement learning. \n\n"}
{"id": "1803.07484", "contents": "Title: Collective Schedules: Scheduling Meets Computational Social Choice Abstract: When scheduling public works or events in a shared facility one needs to\naccommodate preferences of a population. We formalize this problem by\nintroducing the notion of a collective schedule. We show how to extend\nfundamental tools from social choice theory---positional scoring rules, the\nKemeny rule and the Condorcet principle---to collective scheduling. We study\nthe computational complexity of finding collective schedules. We also\nexperimentally demonstrate that optimal collective schedules can be found for\ninstances with realistic sizes. \n\n"}
{"id": "1803.07617", "contents": "Title: Online Learning: Sufficient Statistics and the Burkholder Method Abstract: We uncover a fairly general principle in online learning: If regret can be\n(approximately) expressed as a function of certain \"sufficient statistics\" for\nthe data sequence, then there exists a special Burkholder function that 1) can\nbe used algorithmically to achieve the regret bound and 2) only depends on\nthese sufficient statistics, not the entire data sequence, so that the online\nstrategy is only required to keep the sufficient statistics in memory. This\ncharacterization is achieved by bringing the full power of the Burkholder\nMethod --- originally developed for certifying probabilistic martingale\ninequalities --- to bear on the online learning setting.\n  To demonstrate the scope and effectiveness of the Burkholder method, we\ndevelop a novel online strategy for matrix prediction that attains a regret\nbound corresponding to the variance term in matrix concentration inequalities.\nWe also present a linear-time/space prediction strategy for parameter free\nsupervised learning with linear classes and general smooth norms. \n\n"}
{"id": "1803.07964", "contents": "Title: Stochastic Learning under Random Reshuffling with Constant Step-sizes Abstract: In empirical risk optimization, it has been observed that stochastic gradient\nimplementations that rely on random reshuffling of the data achieve better\nperformance than implementations that rely on sampling the data uniformly.\nRecent works have pursued justifications for this behavior by examining the\nconvergence rate of the learning process under diminishing step-sizes. This\nwork focuses on the constant step-size case and strongly convex loss function.\nIn this case, convergence is guaranteed to a small neighborhood of the\noptimizer albeit at a linear rate. The analysis establishes analytically that\nrandom reshuffling outperforms uniform sampling by showing explicitly that\niterates approach a smaller neighborhood of size $O(\\mu^2)$ around the\nminimizer rather than $O(\\mu)$. Furthermore, we derive an analytical expression\nfor the steady-state mean-square-error performance of the algorithm, which\nhelps clarify in greater detail the differences between sampling with and\nwithout replacement. We also explain the periodic behavior that is observed in\nrandom reshuffling implementations. \n\n"}
{"id": "1803.09357", "contents": "Title: On the Local Minima of the Empirical Risk Abstract: Population risk is always of primary interest in machine learning; however,\nlearning algorithms only have access to the empirical risk. Even for\napplications with nonconvex nonsmooth losses (such as modern deep networks),\nthe population risk is generally significantly more well-behaved from an\noptimization point of view than the empirical risk. In particular, sampling can\ncreate many spurious local minima. We consider a general framework which aims\nto optimize a smooth nonconvex function $F$ (population risk) given only access\nto an approximation $f$ (empirical risk) that is pointwise close to $F$ (i.e.,\n$\\|F-f\\|_{\\infty} \\le \\nu$). Our objective is to find the\n$\\epsilon$-approximate local minima of the underlying function $F$ while\navoiding the shallow local minima---arising because of the tolerance\n$\\nu$---which exist only in $f$. We propose a simple algorithm based on\nstochastic gradient descent (SGD) on a smoothed version of $f$ that is\nguaranteed to achieve our goal as long as $\\nu \\le O(\\epsilon^{1.5}/d)$. We\nalso provide an almost matching lower bound showing that our algorithm achieves\noptimal error tolerance $\\nu$ among all algorithms making a polynomial number\nof queries of $f$. As a concrete example, we show that our results can be\ndirectly used to give sample complexities for learning a ReLU unit. \n\n"}
{"id": "1803.09370", "contents": "Title: Popular Matching in Roommates Setting is NP-hard Abstract: An input to the Popular Matching problem, in the roommates setting, consists\nof a graph $G$ and each vertex ranks its neighbors in strict order, known as\nits preference. In the Popular Matching problem the objective is to test\nwhether there exists a matching $M^\\star$ such that there is no matching $M$\nwhere more people are happier with $M$ than with $M^\\star$. In this paper we\nsettle the computational complexity of the Popular Matching problem in the\nroommates setting by showing that the problem is NP-complete. Thus, we resolve\nan open question that has been repeatedly, explicitly asked over the last\ndecade. \n\n"}
{"id": "1803.09928", "contents": "Title: Entropy based Independent Learning in Anonymous Multi-Agent Settings Abstract: Efficient sequential matching of supply and demand is a problem of interest\nin many online to offline services. For instance, Uber, Lyft, Grab for matching\ntaxis to customers; Ubereats, Deliveroo, FoodPanda etc for matching restaurants\nto customers. In these online to offline service problems, individuals who are\nresponsible for supply (e.g., taxi drivers, delivery bikes or delivery van\ndrivers) earn more by being at the \"right\" place at the \"right\" time. We are\ninterested in developing approaches that learn to guide individuals to be in\nthe \"right\" place at the \"right\" time (to maximize revenue) in the presence of\nother similar \"learning\" individuals and only local aggregated observation of\nother agents states (e.g., only number of other taxis in same zone as current\nagent).\n  A key characteristic of the domains of interest is that the interactions\nbetween individuals are anonymous, i.e., the outcome of an interaction\n(competing for demand) is dependent only on the number and not on the identity\nof the agents. We model these problems using the Anonymous MARL (AyMARL) model.\nThe key contribution of this paper is in employing principle of maximum entropy\nto provide a general framework of independent learning that is both empirically\neffective (even with only local aggregated information of agent population\ndistribution) and theoretically justified.\n  Finally, our approaches provide a significant improvement with respect to\njoint and individual revenue on a generic simulator for online to offline\nservices and a real world taxi problem over existing approaches. More\nimportantly, this is achieved while having the least variance in revenues\nearned by the learning individuals, an indicator of fairness. \n\n"}
{"id": "1803.10131", "contents": "Title: The algebra of predicting agents Abstract: The category of open games, which provides a strongly compositional\nfoundation of economic game theory, is intermediate between symmetric monoidal\nand compact closed. More precisely it has counits with no corresponding units,\nand a partially defined duality. There exist open games with the same types as\nunit maps, given by agents with the strategic goal of predicting a future\nvalue. Such agents appear in earlier work on selection functions. We explore\nthe algebraic properties of these agents via the symmetric monoidal bicategory\nwhose 2-cells are morphisms between open games, and show how the resulting\nstructure approximates a compact closed category with a family of lax\ncommutative bialgebras. \n\n"}
{"id": "1803.10846", "contents": "Title: Non-Convex Matrix Completion Against a Semi-Random Adversary Abstract: Matrix completion is a well-studied problem with many machine learning\napplications. In practice, the problem is often solved by non-convex\noptimization algorithms. However, the current theoretical analysis for\nnon-convex algorithms relies heavily on the assumption that every entry is\nobserved with exactly the same probability $p$, which is not realistic in\npractice.\n  In this paper, we investigate a more realistic semi-random model, where the\nprobability of observing each entry is at least $p$. Even with this mild\nsemi-random perturbation, we can construct counter-examples where existing\nnon-convex algorithms get stuck in bad local optima.\n  In light of the negative results, we propose a pre-processing step that tries\nto re-weight the semi-random input, so that it becomes \"similar\" to a random\ninput. We give a nearly-linear time algorithm for this problem, and show that\nafter our pre-processing, all the local minima of the non-convex objective can\nbe used to approximately recover the underlying ground-truth matrix. \n\n"}
{"id": "1803.11030", "contents": "Title: Exploiting Weak Supermodularity for Coalition-Proof Mechanisms Abstract: Under the incentive-compatible Vickrey-Clarke-Groves mechanism, coalitions of\nparticipants can influence the auction outcome to obtain higher collective\nprofit. These manipulations were proven to be eliminated if and only if the\nmarket objective is supermodular. Nevertheless, several auctions do not satisfy\nthe stringent conditions for supermodularity. These auctions include\nelectricity markets, which are the main motivation of our study. To\ncharacterize nonsupermodular functions, we introduce the supermodularity ratio\nand the weak supermodularity. We show that these concepts provide us with tight\nbounds on the profitability of collusion and shill bidding. We then derive an\nanalytical lower bound on the supermodularity ratio. Our results are verified\nwith case studies based on the IEEE test systems. \n\n"}
{"id": "1803.11060", "contents": "Title: COBRAS: Fast, Iterative, Active Clustering with Pairwise Constraints Abstract: Constraint-based clustering algorithms exploit background knowledge to\nconstruct clusterings that are aligned with the interests of a particular user.\nThis background knowledge is often obtained by allowing the clustering system\nto pose pairwise queries to the user: should these two elements be in the same\ncluster or not? Active clustering methods aim to minimize the number of queries\nneeded to obtain a good clustering by querying the most informative pairs\nfirst. Ideally, a user should be able to answer a couple of these queries,\ninspect the resulting clustering, and repeat these two steps until a\nsatisfactory result is obtained. We present COBRAS, an approach to active\nclustering with pairwise constraints that is suited for such an interactive\nclustering process. A core concept in COBRAS is that of a super-instance: a\nlocal region in the data in which all instances are assumed to belong to the\nsame cluster. COBRAS constructs such super-instances in a top-down manner to\nproduce high-quality results early on in the clustering process, and keeps\nrefining these super-instances as more pairwise queries are given to get more\ndetailed clusterings later on. We experimentally demonstrate that COBRAS\nproduces good clusterings at fast run times, making it an excellent candidate\nfor the iterative clustering scenario outlined above. \n\n"}
{"id": "1803.11115", "contents": "Title: Deep Reinforcement Learning for Traffic Light Control in Vehicular\n  Networks Abstract: Existing inefficient traffic light control causes numerous problems, such as\nlong delay and waste of energy. To improve efficiency, taking real-time traffic\ninformation as an input and dynamically adjusting the traffic light duration\naccordingly is a must. In terms of how to dynamically adjust traffic signals'\nduration, existing works either split the traffic signal into equal duration or\nextract limited traffic information from the real data. In this paper, we study\nhow to decide the traffic signals' duration based on the collected data from\ndifferent sensors and vehicular networks. We propose a deep reinforcement\nlearning model to control the traffic light. In the model, we quantify the\ncomplex traffic scenario as states by collecting data and dividing the whole\nintersection into small grids. The timing changes of a traffic light are the\nactions, which are modeled as a high-dimension Markov decision process. The\nreward is the cumulative waiting time difference between two cycles. To solve\nthe model, a convolutional neural network is employed to map the states to\nrewards. The proposed model is composed of several components to improve the\nperformance, such as dueling network, target network, double Q-learning\nnetwork, and prioritized experience replay. We evaluate our model via\nsimulation in the Simulation of Urban MObility (SUMO) in a vehicular network,\nand the simulation results show the efficiency of our model in controlling\ntraffic lights. \n\n"}
{"id": "1803.11130", "contents": "Title: Incentive Design in a Distributed Problem with Strategic Agents Abstract: In this paper, we consider a general distributed system with multiple agents\nwho select and then implement actions in the system. The system has an operator\nwith a centralized objective. The agents, on the other hand, are selfinterested\nand strategic in the sense that each agent optimizes its own individual\nobjective. The operator aims to mitigate this misalignment by designing an\nincentive scheme for the agents. The problem is difficult due to the cost\nfunctions of the agents being coupled, the objective of the operator not being\nsocial welfare, and the operator having no direct control over actions being\nimplemented by the agents. This problem has been studied in many fields,\nparticularly in mechanism design and cost allocation. However, mechanism design\ntypically assumes that the operator has knowledge of the cost functions of the\nagents and the actions being implemented by the operator. On the other hand,\ncost allocation classically assumes that agents do not anticipate the effect of\ntheir actions on the incentive that they obtain. We remove these assumptions\nand present an incentive rule for this setup by bridging the gap between\nmechanism design and classical cost allocation. We analyze whether the proposed\ndesign satisfies various desirable properties such as social optimality, budget\nbalance, participation constraint, and so on. We also analyze which of these\nproperties can be satisfied if the assumptions of cost functions of the agents\nbeing private and the agents being anticipatory are relaxed. \n\n"}
{"id": "1804.00846", "contents": "Title: Learning to Search via Retrospective Imitation Abstract: We study the problem of learning a good search policy for combinatorial\nsearch spaces. We propose retrospective imitation learning, which, after\ninitial training by an expert, improves itself by learning from\n\\textit{retrospective inspections} of its own roll-outs. That is, when the\npolicy eventually reaches a feasible solution in a combinatorial search tree\nafter making mistakes and backtracks, it retrospectively constructs an improved\nsearch trace to the solution by removing backtracks, which is then used to\nfurther train the policy. A key feature of our approach is that it can\niteratively scale up, or transfer, to larger problem sizes than those solved by\nthe initial expert demonstrations, thus dramatically expanding its\napplicability beyond that of conventional imitation learning. We showcase the\neffectiveness of our approach on a range of tasks, including synthetic maze\nsolving and combinatorial problems expressed as integer programs. \n\n"}
{"id": "1804.01874", "contents": "Title: A Human Mixed Strategy Approach to Deep Reinforcement Learning Abstract: In 2015, Google's DeepMind announced an advancement in creating an autonomous\nagent based on deep reinforcement learning (DRL) that could beat a professional\nplayer in a series of 49 Atari games. However, the current manifestation of DRL\nis still immature, and has significant drawbacks. One of DRL's imperfections is\nits lack of \"exploration\" during the training process, especially when working\nwith high-dimensional problems. In this paper, we propose a mixed strategy\napproach that mimics behaviors of human when interacting with environment, and\ncreate a \"thinking\" agent that allows for more efficient exploration in the DRL\ntraining process. The simulation results based on the Breakout game show that\nour scheme achieves a higher probability of obtaining a maximum score than does\nthe baseline DRL algorithm, i.e., the asynchronous advantage actor-critic\nmethod. The proposed scheme therefore can be applied effectively to solving a\ncomplicated task in a real-world application. \n\n"}
{"id": "1804.02268", "contents": "Title: Social Choice with Non Quasi-linear Utilities Abstract: Without monetary payments, the Gibbard-Satterthwaite theorem proves that\nunder mild requirements all truthful social choice mechanisms must be\ndictatorships. When payments are allowed, the Vickrey-Clarke-Groves (VCG)\nmechanism implements the value-maximizing choice, and has many other good\nproperties: it is strategy-proof, onto, deterministic, individually rational,\nand does not make positive transfers to the agents. By Roberts' theorem, with\nthree or more alternatives, the weighted VCG mechanisms are essentially unique\nfor domains with quasi-linear utilities. The goal of this paper is to\ncharacterize domains of non-quasi-linear utilities where \"reasonable\"\nmechanisms (with VCG-like properties) exist. Our main result is a tight\ncharacterization of the maximal non quasi-linear utility domain, which we call\nthe largest parallel domain. We extend Roberts' theorem to parallel domains,\nand use the generalized theorem to prove two impossibility results. First, any\nreasonable mechanism must be dictatorial when the utility domain is\nquasi-linear together with any single non-parallel type. Second, for richer\nutility domains that still differ very slightly from quasi-linearity, every\nstrategy-proof, onto and deterministic mechanism must be a dictatorship. \n\n"}
{"id": "1804.03081", "contents": "Title: Routing Game on Parallel Networks: the Convergence of Atomic to\n  Nonatomic Abstract: We consider an instance of a nonatomic routing game. We assume that the\nnetwork is parallel, that is, constituted of only two nodes, an origin and a\ndestination. We consider infinitesimal players that have a symmetric network\ncost, but are heterogeneous through their set of feasible strategies and their\nindividual utilities. We show that if an atomic routing game instance is\ncorrectly defined to approximate the nonatomic instance, then an atomic Nash\nEquilibrium will approximate the nonatomic Wardrop Equilibrium. We give\nexplicit bounds on the distance between the equilibria according to the\nparameters of the atomic instance. This approximation gives a method to compute\nthe Wardrop equilibrium at an arbitrary precision. \n\n"}
{"id": "1804.03178", "contents": "Title: Power of Bonus in Pricing for Crowdsourcing Abstract: We consider a simple form of pricing for a crowdsourcing system, where\npricing policy is published a priori, and workers then decide their task\nacceptance. Such a pricing form is widely adopted in practice for its\nsimplicity, e.g., Amazon Mechanical Turk, although additional sophistication to\npricing rule can enhance budget efficiency. With the goal of designing\nefficient and simple pricing rules, we study the impact of the following two\ndesign features in pricing policies: (i) personalization tailoring policy\nworker-by-worker and (ii) bonus payment to qualified task completion. In the\nBayesian setting, where the only prior distribution of workers' profiles is\navailable, we first study the Price of Agnosticism (PoA) that quantifies the\nutility gap between personalized and common pricing policies. We show that PoA\nis bounded within a constant factor under some mild conditions, and the impact\nof bonus is essential in common pricing. These analytic results imply that\ncomplex personalized pricing can be replaced by simple common pricing once it\nis equipped with a proper bonus payment. To provide insights on efficient\ncommon pricing, we then study the efficient mechanisms of bonus payment for\nseveral profile distribution regimes which may exist in practice. We provide\nprimitive experiments on Amazon Mechanical Turk, which support our analytical\nfindings. \n\n"}
{"id": "1804.03450", "contents": "Title: End of Potential Line Abstract: We introduce the problem EndOfPotentialLine and the corresponding complexity\nclass EOPL of all problems that can be reduced to it in polynomial time. This\nclass captures problems that admit a single combinatorial proof of their joint\nmembership in the complexity classes PPAD of fixpoint problems and PLS of local\nsearch problems. EOPL is a combinatorially-defined alternative to the class CLS\n(for Continuous Local Search), which was introduced in with the goal of\ncapturing the complexity of some well-known problems in PPAD $\\cap$ PLS that\nhave resisted, in some cases for decades, attempts to put them in polynomial\ntime. Two of these are Contraction, the problem of finding a fixpoint of a\ncontraction map, and P-LCP, the problem of solving a P-matrix Linear\nComplementarity Problem.\n  We show that EndOfPotentialLine is in CLS via a two-way reduction to\nEndOfMeteredLine. The latter was defined in to show query and cryptographic\nlower bounds for CLS. Our two main results are to show that both PL-Contraction\n(Piecewise-Linear Contraction) and P-LCP are in EOPL. Our reductions imply that\nthe promise versions of PL-Contraction and P-LCP are in the promise class\nUniqueEOPL, which corresponds to the case of a single potential line. This also\nshows that simple-stochastic, discounted, mean-payoff, and parity games are in\nEOPL.\n  Using the insights from our reduction for PL-Contraction, we obtain the first\npolynomial-time algorithms for finding fixed points of contraction maps in\nfixed dimension for any $\\ell_p$ norm, where previously such algorithms were\nonly known for the $\\ell_2$ and $\\ell_\\infty$ norms. Our reduction from P-LCP\nto EndOfPotentialLine allows a technique of Aldous to be applied, which in turn\ngives the fastest-known randomized algorithm for the P-LCP. \n\n"}
{"id": "1804.04235", "contents": "Title: Adafactor: Adaptive Learning Rates with Sublinear Memory Cost Abstract: In several recently proposed stochastic optimization methods (e.g. RMSProp,\nAdam, Adadelta), parameter updates are scaled by the inverse square roots of\nexponential moving averages of squared past gradients. Maintaining these\nper-parameter second-moment estimators requires memory equal to the number of\nparameters. For the case of neural network weight matrices, we propose\nmaintaining only the per-row and per-column sums of these moving averages, and\nestimating the per-parameter second moments based on these sums. We demonstrate\nempirically that this method produces similar results to the baseline.\nSecondly, we show that adaptive methods can produce larger-than-desired updates\nwhen the decay rate of the second moment accumulator is too slow. We propose\nupdate clipping and a gradually increasing decay rate scheme as remedies.\nCombining these methods and dropping momentum, we achieve comparable results to\nthe published Adam regime in training the Transformer model on the WMT 2014\nEnglish-German machine translation task, while using very little auxiliary\nstorage in the optimizer. Finally, we propose scaling the parameter updates\nbased on the scale of the parameters themselves. \n\n"}
{"id": "1804.04372", "contents": "Title: Infinite-Duration Poorman-Bidding Games Abstract: In two-player games on graphs, the players move a token through a graph to\nproduce an infinite path, which determines the winner or payoff of the game.\nSuch games are central in formal verification since they model the interaction\nbetween a non-terminating system and its environment. We study {\\em bidding\ngames} in which the players bid for the right to move the token. Two bidding\nrules have been defined. In {\\em Richman} bidding, in each round, the players\nsimultaneously submit bids, and the higher bidder moves the token and pays the\nother player. {\\em Poorman} bidding is similar except that the winner of the\nbidding pays the \"bank\" rather than the other player. While poorman\nreachability games have been studied before, we present, for the first time,\nresults on {\\em infinite-duration} poorman games. A central quantity in these\ngames is the {\\em ratio} between the two players' initial budgets. The\nquestions we study concern a necessary and sufficient ratio with which a player\ncan achieve a goal. For reachability objectives, such {\\em threshold ratios}\nare known to exist for both bidding rules. We show that the properties of\npoorman reachability games extend to complex qualitative objectives such as\nparity, similarly to the Richman case. Our most interesting results concern\nquantitative poorman games, namely poorman mean-payoff games, where we\nconstruct optimal strategies depending on the initial ratio, by showing a\nconnection with {\\em random-turn based games}. The connection in itself is\ninteresting, because it does not hold for reachability poorman games. We also\nsolve the complexity problems that arise in poorman bidding games. \n\n"}
{"id": "1804.04421", "contents": "Title: Regularized Greedy Column Subset Selection Abstract: The Column Subset Selection Problem provides a natural framework for\nunsupervised feature selection. Despite being a hard combinatorial optimization\nproblem, there exist efficient algorithms that provide good approximations. The\ndrawback of the problem formulation is that it incorporates no form of\nregularization, and is therefore very sensitive to noise when presented with\nscarce data. In this paper we propose a regularized formulation of this\nproblem, and derive a correct greedy algorithm that is similar in efficiency to\nexisting greedy methods for the unregularized problem. We study its adequacy\nfor feature selection and propose suitable formulations. Additionally, we\nderive a lower bound for the error of the proposed problems. Through various\nnumerical experiments on real and synthetic data, we demonstrate the\nsignificantly increased robustness and stability of our method, as well as the\nimproved conditioning of its output, all while remaining efficient for\npractical use. \n\n"}
{"id": "1804.05040", "contents": "Title: Stable Outcomes in Modified Fractional Hedonic Games Abstract: In coalition formation games self-organized coalitions are created as a\nresult of the strategic interactions of independent agents. For each couple of\nagents $(i,j)$, weight $w_{i,j}=w_{j,i}$ reflects how much agents $i$ and $j$\nbenefit from belonging to the same coalition. We consider the modified\nfractional hedonic game, that is a coalition formation game in which agents'\nutilities are such that the total benefit of agent $i$ belonging to a coalition\n(given by the sum of $w_{i,j}$ over all other agents $j$ belonging to the same\ncoalition) is averaged over all the other members of that coalition, i.e.,\nexcluding herself. Modified fractional hedonic games constitute a class of\nsuccinctly representable hedonic games.\n  We are interested in the scenario in which agents, individually or jointly,\nchoose to form a new coalition or to join an existing one, until a stable\noutcome is reached. To this aim, we consider common stability notions, leading\nto strong Nash stable outcomes, Nash stable outcomes or core stable outcomes:\nwe study their existence, complexity and performance, both in the case of\ngeneral weights and in the case of 0-1 weights. In particular, we completely\ncharacterize the existence of the considered stable outcomes and show many\ntight or asymptotically tight results on the performance of these natural\nstable outcomes for modified fractional hedonic games, also highlighting the\ndifferences with respect to the model of fractional hedonic games, in which the\ntotal benefit of an agent in a coalition is averaged over all members of that\ncoalition, i.e., including herself. \n\n"}
{"id": "1804.05271", "contents": "Title: Adaptive Federated Learning in Resource Constrained Edge Computing\n  Systems Abstract: Emerging technologies and applications including Internet of Things (IoT),\nsocial networking, and crowd-sourcing generate large amounts of data at the\nnetwork edge. Machine learning models are often built from the collected data,\nto enable the detection, classification, and prediction of future events. Due\nto bandwidth, storage, and privacy concerns, it is often impractical to send\nall the data to a centralized location. In this paper, we consider the problem\nof learning model parameters from data distributed across multiple edge nodes,\nwithout sending raw data to a centralized place. Our focus is on a generic\nclass of machine learning models that are trained using gradient-descent based\napproaches. We analyze the convergence bound of distributed gradient descent\nfrom a theoretical point of view, based on which we propose a control algorithm\nthat determines the best trade-off between local update and global parameter\naggregation to minimize the loss function under a given resource budget. The\nperformance of the proposed algorithm is evaluated via extensive experiments\nwith real datasets, both on a networked prototype system and in a larger-scale\nsimulated environment. The experimentation results show that our proposed\napproach performs near to the optimum with various machine learning models and\ndifferent data distributions. \n\n"}
{"id": "1804.05560", "contents": "Title: Deep Bayesian Trust : A Dominant and Fair Incentive Mechanism for Crowd Abstract: An important class of game-theoretic incentive mechanisms for eliciting\neffort from a crowd are the peer based mechanisms, in which workers are paid by\nmatching their answers with one another. The other classic mechanism is to have\nthe workers solve some gold standard tasks and pay them according to their\naccuracy on gold tasks. This mechanism ensures stronger incentive compatibility\nthan the peer based mechanisms but assigning gold tasks to all workers becomes\ninefficient at large scale. We propose a novel mechanism that assigns gold\ntasks to only a few workers and exploits transitivity to derive accuracy of the\nrest of the workers from their peers' accuracy. We show that the resulting\nmechanism ensures a dominant notion of incentive compatibility and fairness. \n\n"}
{"id": "1804.05834", "contents": "Title: CytonRL: an Efficient Reinforcement Learning Open-source Toolkit\n  Implemented in C++ Abstract: This paper presents an open-source enforcement learning toolkit named CytonRL\n(https://github.com/arthurxlw/cytonRL). The toolkit implements four recent\nadvanced deep Q-learning algorithms from scratch using C++ and NVIDIA's\nGPU-accelerated libraries. The code is simple and elegant, owing to an\nopen-source general-purpose neural network library named CytonLib. Benchmark\nshows that the toolkit achieves competitive performances on the popular Atari\ngame of Breakout. \n\n"}
{"id": "1804.05929", "contents": "Title: UCBoost: A Boosting Approach to Tame Complexity and Optimality for\n  Stochastic Bandits Abstract: In this work, we address the open problem of finding low-complexity\nnear-optimal multi-armed bandit algorithms for sequential decision making\nproblems. Existing bandit algorithms are either sub-optimal and computationally\nsimple (e.g., UCB1) or optimal and computationally complex (e.g., kl-UCB). We\npropose a boosting approach to Upper Confidence Bound based algorithms for\nstochastic bandits, that we call UCBoost. Specifically, we propose two types of\nUCBoost algorithms. We show that UCBoost($D$) enjoys $O(1)$ complexity for each\narm per round as well as regret guarantee that is $1/e$-close to that of the\nkl-UCB algorithm. We propose an approximation-based UCBoost algorithm,\nUCBoost($\\epsilon$), that enjoys a regret guarantee $\\epsilon$-close to that of\nkl-UCB as well as $O(\\log(1/\\epsilon))$ complexity for each arm per round.\nHence, our algorithms provide practitioners a practical way to trade optimality\nwith computational complexity. Finally, we present numerical results which show\nthat UCBoost($\\epsilon$) can achieve the same regret performance as the\nstandard kl-UCB while incurring only $1\\%$ of the computational cost of kl-UCB. \n\n"}
{"id": "1804.06021", "contents": "Title: Model-Free Linear Quadratic Control via Reduction to Expert Prediction Abstract: Model-free approaches for reinforcement learning (RL) and continuous control\nfind policies based only on past states and rewards, without fitting a model of\nthe system dynamics. They are appealing as they are general purpose and easy to\nimplement; however, they also come with fewer theoretical guarantees than\nmodel-based RL. In this work, we present a new model-free algorithm for\ncontrolling linear quadratic (LQ) systems, and show that its regret scales as\n$O(T^{\\xi+2/3})$ for any small $\\xi>0$ if time horizon satisfies $T>C^{1/\\xi}$\nfor a constant $C$. The algorithm is based on a reduction of control of Markov\ndecision processes to an expert prediction problem. In practice, it corresponds\nto a variant of policy iteration with forced exploration, where the policy in\neach phase is greedy with respect to the average of all previous value\nfunctions. This is the first model-free algorithm for adaptive control of LQ\nsystems that provably achieves sublinear regret and has a polynomial\ncomputation cost. Empirically, our algorithm dramatically outperforms standard\npolicy iteration, but performs worse than a model-based approach. \n\n"}
{"id": "1804.06461", "contents": "Title: An Adaptive Clipping Approach for Proximal Policy Optimization Abstract: Very recently proximal policy optimization (PPO) algorithms have been\nproposed as first-order optimization methods for effective reinforcement\nlearning. While PPO is inspired by the same learning theory that justifies\ntrust region policy optimization (TRPO), PPO substantially simplifies algorithm\ndesign and improves data efficiency by performing multiple epochs of\n\\emph{clipped policy optimization} from sampled data. Although clipping in PPO\nstands for an important new mechanism for efficient and reliable policy update,\nit may fail to adaptively improve learning performance in accordance with the\nimportance of each sampled state. To address this issue, a new surrogate\nlearning objective featuring an adaptive clipping mechanism is proposed in this\npaper, enabling us to develop a new algorithm, known as PPO-$\\lambda$.\nPPO-$\\lambda$ optimizes policies repeatedly based on a theoretical target for\nadaptive policy improvement. Meanwhile, destructively large policy update can\nbe effectively prevented through both clipping and adaptive control of a\nhyperparameter $\\lambda$ in PPO-$\\lambda$, ensuring high learning reliability.\nPPO-$\\lambda$ enjoys the same simple and efficient design as PPO. Empirically\non several Atari game playing tasks and benchmark control tasks, PPO-$\\lambda$\nalso achieved clearly better performance than PPO. \n\n"}
{"id": "1804.06496", "contents": "Title: A Capacity-Price Game for Uncertain Renewables Resources Abstract: Renewable resources are starting to constitute a growing portion of the total\ngeneration mix of the power system. A key difference between renewables and\ntraditional generators is that many renewable resources are managed by\nindividuals, especially in the distribution system. In this paper, we study the\ncapacity investment and pricing problem, where multiple renewable producers\ncompete in a decentralized market. It is known that most deterministic capacity\ngames tend to result in very inefficient equilibria, even when there are a\nlarge number of similar players. In contrast, we show that due to the inherent\nrandomness of renewable resources, the equilibria in our capacity game becomes\nefficient as the number of players grows and coincides with the centralized\ndecision from the social planner's problem. This result provides a new\nperspective on how to look at the positive influence of randomness in a game\nframework as well as its contribution to resource planning, scheduling, and\nbidding. We validate our results by simulation studies using real world data. \n\n"}
{"id": "1804.06836", "contents": "Title: Delayed Blockchain Protocols Abstract: Given the parallels between game theory and consensus, it makes sense to\nintelligently design blockchain or DAG protocols with an\nincentive-compatible-first mentality. To that end, we propose a new blockchain\nor DAG protocol enhancement based on delayed rewards. We devise a new method\nfor imposing slashing conditions on miner behavior, using their delayed rewards\nas stake in a Proof of Work system. Using fraud proofs, we can slash malicious\nminer behavior and reward long-lived, honest behavior. \n\n"}
{"id": "1804.06867", "contents": "Title: Optimal Deterministic Mechanisms for an Additive Buyer Abstract: We study revenue maximization by deterministic mechanisms for the simplest\ncase for which Myerson's characterization does not hold: a single seller\nselling two items, with independently distributed values, to a single additive\nbuyer. We prove that optimal mechanisms are submodular and hence monotone.\nFurthermore, we show that in the IID case, optimal mechanisms are symmetric.\nOur characterizations are surprisingly non-trivial, and we show that they fail\nto extend in several natural ways, e.g. for correlated distributions or more\nthan two items. In particular, this shows that the optimality of symmetric\nmechanisms does not follow from the symmetry of the IID distribution. \n\n"}
{"id": "1804.07193", "contents": "Title: Lipschitz Continuity in Model-based Reinforcement Learning Abstract: We examine the impact of learning Lipschitz continuous models in the context\nof model-based reinforcement learning. We provide a novel bound on multi-step\nprediction error of Lipschitz models where we quantify the error using the\nWasserstein metric. We go on to prove an error bound for the value-function\nestimate arising from Lipschitz models and show that the estimated value\nfunction is itself Lipschitz. We conclude with empirical results that show the\nbenefits of controlling the Lipschitz constant of neural-network models. \n\n"}
{"id": "1804.07406", "contents": "Title: Tipping Points for Norm Change in Human Cultures Abstract: Humans interact with each other on a daily basis by developing and\nmaintaining various social norms and it is critical to form a deeper\nunderstanding of how such norms develop, how they change, and how fast they\nchange. In this work, we develop an evolutionary game-theoretic model based on\nresearch in cultural psychology that shows that humans in various cultures\ndiffer in their tendencies to conform with those around them. Using this model,\nwe analyze the evolutionary relationships between the tendency to conform and\nhow quickly a population reacts when conditions make a change in norm\ndesirable. Our analysis identifies conditions when a tipping point is reached\nin a population, causing norms to change rapidly. \n\n"}
{"id": "1804.07451", "contents": "Title: Bayesian Auctions with Efficient Queries Abstract: Generating good revenue is one of the most important problems in Bayesian\nauction design, and many (approximately) optimal dominant-strategy incentive\ncompatible (DSIC) Bayesian mechanisms have been constructed for various auction\nsettings. However, most existing studies do not consider the complexity for the\nseller to carry out the mechanism. It is assumed that the seller knows \"each\nsingle bit\" of the distributions and is able to optimize perfectly based on the\nentire distributions. Unfortunately, this is a strong assumption and may not\nhold in reality: for example, when the value distributions have exponentially\nlarge supports or do not have succinct representations.\n  In this work we consider, for the first time, the query complexity of\nBayesian mechanisms. We only allow the seller to have limited oracle accesses\nto the players' value distributions, via quantile queries and value queries.\nFor a large class of auction settings, we prove logarithmic lower-bounds for\nthe query complexity for any DSIC Bayesian mechanism to be of any constant\napproximation to the optimal revenue. For single-item auctions and multi-item\nauctions with unit-demand or additive valuation functions, we prove tight\nupper-bounds via efficient query schemes, without requiring the distributions\nto be regular or have monotone hazard rate. Thus, in those auction settings the\nseller needs to access much less than the full distributions in order to\nachieve approximately optimal revenue. \n\n"}
{"id": "1804.07605", "contents": "Title: Distributed, Private, and Derandomized Allocation Algorithm for EV\n  Charging Abstract: Efficient resource allocation is challenging when privacy of users is\nimportant. Distributed approaches have recently been used extensively to find a\nsolution for such problems. In this work, the efficiency of distributed AIMD\nalgorithm for allocation of subsidized goods is studied. First, a suitable\nutility function is assigned to each user describing the amount of satisfaction\nthat it has from allocated resource. Then the resource allocation is defined as\na total utilitarianism problem that is an optimization problem of sum of users\nutility functions subjected to capacity constraint. Recently, a stochastic\nstate-dependent variant of AIMD algorithm is used for allocation of common\ngoods among users with strictly increasing and concave utility functions. Here,\nthe stochastic AIMD algorithm is derandomized and its efficiency is compared\nwith the stochastic version. Moreover, the algorithm is improved to allocate\nsubsidized goods to users with concave and non-monotone utility functions as\nwell as users with Sigmoidal utility functions. To illustrate the effectiveness\nof the proposed solutions, simulation results is presented for a public\nrenewable-energy powered charging station in which the electric vehicles (EV)\ncompete to be recharged. \n\n"}
{"id": "1804.07759", "contents": "Title: A Self-paced Regularization Framework for Partial-Label Learning Abstract: Partial label learning (PLL) aims to solve the problem where each training\ninstance is associated with a set of candidate labels, one of which is the\ncorrect label. Most PLL algorithms try to disambiguate the candidate label set,\nby either simply treating each candidate label equally or iteratively\nidentifying the true label. Nonetheless, existing algorithms usually treat all\nlabels and instances equally, and the complexities of both labels and instances\nare not taken into consideration during the learning stage. Inspired by the\nsuccessful application of self-paced learning strategy in machine learning\nfield, we integrate the self-paced regime into the partial label learning\nframework and propose a novel Self-Paced Partial-Label Learning (SP-PLL)\nalgorithm, which could control the learning process to alleviate the problem by\nranking the priorities of the training examples together with their candidate\nlabels during each learning iteration. Extensive experiments and comparisons\nwith other baseline methods demonstrate the effectiveness and robustness of the\nproposed method. \n\n"}
{"id": "1804.08017", "contents": "Title: Tracing Equilibrium in Dynamic Markets via Distributed Adaptation Abstract: Competitive equilibrium is a central concept in economics with numerous\napplications beyond markets, such as scheduling, fair allocation of goods, or\nbandwidth distribution in networks. Computation of competitive equilibria has\nreceived a significant amount of interest in algorithmic game theory, mainly\nfor the prominent case of Fisher markets. Natural and decentralized processes\nlike tatonnement and proportional response dynamics (PRD) converge quickly\ntowards equilibrium in large classes of Fisher markets. Almost all of the\nliterature assumes that the market is a static environment and that the\nparameters of agents and goods do not change over time. In contrast, many large\nreal-world markets are subject to frequent and dynamic changes. In this paper,\nwe provide the first provable performance guarantees of discrete-time\ntatonnement and PRD in markets that are subject to perturbation over time. We\nanalyze the prominent class of Fisher markets with CES utilities and quantify\nthe impact of changes in supplies of goods, budgets of agents, and utility\nfunctions of agents on the convergence of tatonnement to market equilibrium.\nSince the equilibrium becomes a dynamic object and will rarely be reached, our\nanalysis provides bounds expressing the distance to equilibrium that will be\nmaintained via tatonnement and PRD updates. Our results indicate that in many\ncases, tatonnement and PRD follow the equilibrium rather closely and quickly\nrecover conditions of approximate market clearing. Our approach can be\ngeneralized to analyzing a general class of Lyapunov dynamical systems with\nchanging system parameters, which might be of independent interest. \n\n"}
{"id": "1804.08597", "contents": "Title: Towards Symbolic Reinforcement Learning with Common Sense Abstract: Deep Reinforcement Learning (deep RL) has made several breakthroughs in\nrecent years in applications ranging from complex control tasks in unmanned\nvehicles to game playing. Despite their success, deep RL still lacks several\nimportant capacities of human intelligence, such as transfer learning,\nabstraction and interpretability. Deep Symbolic Reinforcement Learning (DSRL)\nseeks to incorporate such capacities to deep Q-networks (DQN) by learning a\nrelevant symbolic representation prior to using Q-learning. In this paper, we\npropose a novel extension of DSRL, which we call Symbolic Reinforcement\nLearning with Common Sense (SRL+CS), offering a better balance between\ngeneralization and specialization, inspired by principles of common sense when\nassigning rewards and aggregating Q-values. Experiments reported in this paper\nshow that SRL+CS learns consistently faster than Q-learning and DSRL, achieving\nalso a higher accuracy. In the hardest case, where agents were trained in a\ndeterministic environment and tested in a random environment, SRL+CS achieves\nnearly 100% average accuracy compared to DSRL's 70% and DQN's 50% accuracy. To\nthe best of our knowledge, this is the first case of near perfect zero-shot\ntransfer learning using Reinforcement Learning. \n\n"}
{"id": "1804.08607", "contents": "Title: Benchmarking projective simulation in navigation problems Abstract: Projective simulation (PS) is a model for intelligent agents with a\ndeliberation capacity that is based on episodic memory. The model has been\nshown to provide a flexible framework for constructing reinforcement-learning\nagents, and it allows for quantum mechanical generalization, which leads to a\nspeed-up in deliberation time. PS agents have been applied successfully in the\ncontext of complex skill learning in robotics, and in the design of\nstate-of-the-art quantum experiments. In this paper, we study the performance\nof projective simulation in two benchmarking problems in navigation, namely the\ngrid world and the mountain car problem. The performance of PS is compared to\nstandard tabular reinforcement learning approaches, Q-learning and SARSA. Our\ncomparison demonstrates that the performance of PS and standard learning\napproaches are qualitatively and quantitatively similar, while it is much\neasier to choose optimal model parameters in case of projective simulation,\nwith a reduced computational effort of one to two orders of magnitude. Our\nresults show that the projective simulation model stands out for its simplicity\nin terms of the number of model parameters, which makes it simple to set up the\nlearning agent in unknown task environments. \n\n"}
{"id": "1804.08619", "contents": "Title: State Distribution-aware Sampling for Deep Q-learning Abstract: A critical and challenging problem in reinforcement learning is how to learn\nthe state-action value function from the experience replay buffer and\nsimultaneously keep sample efficiency and faster convergence to a high quality\nsolution. In prior works, transitions are uniformly sampled at random from the\nreplay buffer or sampled based on their priority measured by\ntemporal-difference (TD) error. However, these approaches do not fully take\ninto consideration the intrinsic characteristics of transition distribution in\nthe state space and could result in redundant and unnecessary TD updates,\nslowing down the convergence of the learning procedure. To overcome this\nproblem, we propose a novel state distribution-aware sampling method to balance\nthe replay times for transitions with skew distribution, which takes into\naccount both the occurrence frequencies of transitions and the uncertainty of\nstate-action values. Consequently, our approach could reduce the unnecessary TD\nupdates and increase the TD updates for state-action value with more\nuncertainty, making the experience replay more effective and efficient.\nExtensive experiments are conducted on both classic control tasks and Atari\n2600 games based on OpenAI gym platform and the experimental results\ndemonstrate the effectiveness of our approach in comparison with the standard\nDQN approach. \n\n"}
{"id": "1804.08645", "contents": "Title: Individual Sensitivity Preprocessing for Data Privacy Abstract: The sensitivity metric in differential privacy, which is informally defined\nas the largest marginal change in output between neighboring databases, is of\nsubstantial significance in determining the accuracy of private data analyses.\nTechniques for improving accuracy when the average sensitivity is much smaller\nthan the worst-case sensitivity have been developed within the differential\nprivacy literature, including tools such as smooth sensitivity,\nSample-and-Aggregate, Propose-Test-Release, and Lipschitz extensions.\n  In this work, we provide a new and general Sensitivity-Preprocessing\nframework for reducing sensitivity, where efficient application gives\nstate-of-the-art accuracy for privately outputting the important statistical\nmetrics median and mean when no underlying assumptions are made about the\ndatabase. In particular, our framework compares favorably to smooth sensitivity\nfor privately outputting median, in terms of both running time and accuracy.\nFurthermore, because our framework is a preprocessing step, it can also be\ncomplementary to smooth sensitivity and any other private mechanism, where\napplying both can achieve further gains in accuracy.\n  We additionally introduce a new notion of individual sensitivity and show\nthat it is an important metric in the variant definition of personalized\ndifferential privacy. We show that our algorithm can extend to this context and\nserve as a useful tool for this variant definition and its applications in\nmarkets for privacy. \n\n"}
{"id": "1804.09238", "contents": "Title: Semi-Supervised Learning with Declaratively Specified Entropy\n  Constraints Abstract: We propose a technique for declaratively specifying strategies for\nsemi-supervised learning (SSL). The proposed method can be used to specify\nensembles of semi-supervised learning, as well as agreement constraints and\nentropic regularization constraints between these learners, and can be used to\nmodel both well-known heuristics such as co-training and novel domain-specific\nheuristics. In addition to representing individual SSL heuristics, we show that\nmultiple heuristics can also be automatically combined using Bayesian\noptimization methods. We show consistent improvements on a suite of\nwell-studied SSL benchmarks, including a new state-of-the-art result on a\ndifficult relation extraction task. \n\n"}
{"id": "1804.09468", "contents": "Title: Price of Anarchy for Mechanisms with Risk-Averse Agents Abstract: We study the price of anarchy of mechanisms in the presence of risk-averse\nagents. Previous work has focused on agents with quasilinear utilities,\npossibly with a budget. Our model subsumes this as a special case but also\ncaptures that agents might be less sensitive to payments than in the\nrisk-neutral model. We show that many positive price-of-anarchy results proved\nin the smoothness framework continue to hold in the more general risk-averse\nsetting. A sufficient condition is that agents can never end up with negative\nquasilinear utility after playing an undominated strategy. This is true, e.g.,\nfor first-price and second-price auctions. For all-pay auctions, similar\nresults do not hold: We show that there are Bayes-Nash equilibria with\narbitrarily bad social welfare compared to the optimum. \n\n"}
{"id": "1804.09521", "contents": "Title: Fair Division Under Cardinality Constraints Abstract: We consider the problem of fairly allocating indivisible goods, among agents,\nunder cardinality constraints and additive valuations. In this setting, we are\ngiven a partition of the entire set of goods---i.e., the goods are\ncategorized---and a limit is specified on the number of goods that can be\nallocated from each category to any agent. The objective here is to find a fair\nallocation in which the subset of goods assigned to any agent satisfies the\ngiven cardinality constraints. This problem naturally captures a number of\nresource-allocation applications, and is a generalization of the well-studied\n(unconstrained) fair division problem.\n  The two central notions of fairness, in the context of fair division of\nindivisible goods, are envy freeness up to one good (EF1) and the (approximate)\nmaximin share guarantee (MMS). We show that the existence and algorithmic\nguarantees established for these solution concepts in the unconstrained setting\ncan essentially be achieved under cardinality constraints. Specifically, we\ndevelop efficient algorithms which compute EF1 and approximately MMS\nallocations in the constrained setting.\n  Furthermore, focusing on the case wherein all the agents have the same\nadditive valuation, we establish that EF1 allocations exist and can be computed\nefficiently even under laminar matroid constraints. \n\n"}
{"id": "1804.10328", "contents": "Title: Scalable Bilinear $\\pi$ Learning Using State and Action Features Abstract: Approximate linear programming (ALP) represents one of the major algorithmic\nfamilies to solve large-scale Markov decision processes (MDP). In this work, we\nstudy a primal-dual formulation of the ALP, and develop a scalable, model-free\nalgorithm called bilinear $\\pi$ learning for reinforcement learning when a\nsampling oracle is provided. This algorithm enjoys a number of advantages.\nFirst, it adopts (bi)linear models to represent the high-dimensional value\nfunction and state-action distributions, using given state and action features.\nIts run-time complexity depends on the number of features, not the size of the\nunderlying MDPs. Second, it operates in a fully online fashion without having\nto store any sample, thus having minimal memory footprint. Third, we prove that\nit is sample-efficient, solving for the optimal policy to high precision with a\nsample complexity linear in the dimension of the parameter space. \n\n"}
{"id": "1804.10512", "contents": "Title: Probabilistic Verification for Obviously Strategyproof Mechanisms Abstract: Obviously strategyproof (OSP) mechanisms maintain the incentive compatibility\nof agents that are not fully rational. They have been object of a number of\nstudies since their recent definition. A research agenda, initiated in\n[Ferraioli&Ventre, AAAI 2017], is to find a small (possibly, the smallest) set\nof conditions allowing to implement an OSP mechanism. To this aim, we define a\nmodel of probabilistic verification wherein agents are caught misbehaving with\na certain probability, and show how OSP mechanisms can implement every social\nchoice function at the cost of either imposing very large fines for lies or\nverifying a linear number of agents. \n\n"}
{"id": "1804.10689", "contents": "Title: Decoupling Dynamics and Reward for Transfer Learning Abstract: Current reinforcement learning (RL) methods can successfully learn single\ntasks but often generalize poorly to modest perturbations in task domain or\ntraining procedure. In this work, we present a decoupled learning strategy for\nRL that creates a shared representation space where knowledge can be robustly\ntransferred. We separate learning the task representation, the forward\ndynamics, the inverse dynamics and the reward function of the domain, and show\nthat this decoupling improves performance within the task, transfers well to\nchanges in dynamics and reward, and can be effectively used for online\nplanning. Empirical results show good performance in both continuous and\ndiscrete RL domains. \n\n"}
{"id": "1804.10850", "contents": "Title: Drug Similarity Integration Through Attentive Multi-view Graph\n  Auto-Encoders Abstract: Drug similarity has been studied to support downstream clinical tasks such as\ninferring novel properties of drugs (e.g. side effects, indications,\ninteractions) from known properties. The growing availability of new types of\ndrug features brings the opportunity of learning a more comprehensive and\naccurate drug similarity that represents the full spectrum of underlying drug\nrelations. However, it is challenging to integrate these heterogeneous, noisy,\nnonlinear-related information to learn accurate similarity measures especially\nwhen labels are scarce. Moreover, there is a trade-off between accuracy and\ninterpretability. In this paper, we propose to learn accurate and interpretable\nsimilarity measures from multiple types of drug features. In particular, we\nmodel the integration using multi-view graph auto-encoders, and add attentive\nmechanism to determine the weights for each view with respect to corresponding\ntasks and features for better interpretability. Our model has flexible design\nfor both semi-supervised and unsupervised settings. Experimental results\ndemonstrated significant predictive accuracy improvement. Case studies also\nshowed better model capacity (e.g. embed node features) and interpretability. \n\n"}
{"id": "1804.11221", "contents": "Title: Adversarial Task Assignment Abstract: The problem of assigning tasks to workers is of long-standing fundamental\nimportance. Examples of this include the classical problem of assigning\ncomputing tasks to nodes in a distributed computing environment, assigning jobs\nto robots, and crowdsourcing. Extensive research into this problem generally\naddresses important issues such as uncertainty and incentives. However, the\nproblem of adversarial tampering with the task assignment process has not\nreceived as much attention.\n  We are concerned with a particular adversarial setting where an attacker may\ntarget a set of workers in order to prevent the tasks assigned to these workers\nfrom being completed. When all tasks are homogeneous, we provide an efficient\nalgorithm for computing the optimal assignment. When tasks are heterogeneous,\nwe show that the adversarial assignment problem is NP-Hard, and present an\nalgorithm for solving it approximately. Our theoretical results are accompanied\nby extensive experiments showing the effectiveness of our algorithms. \n\n"}
{"id": "1805.02338", "contents": "Title: Implementation of Stochastic Quasi-Newton's Method in PyTorch Abstract: In this paper, we implement the Stochastic Damped LBFGS (SdLBFGS) for\nstochastic non-convex optimization. We make two important modifications to the\noriginal SdLBFGS algorithm. First, by initializing the Hessian at each step\nusing an identity matrix, the algorithm converges better than original\nalgorithm. Second, by performing direction normalization we could gain stable\noptimization procedure without line search. Experiments on minimizing a 2D\nnon-convex function shows that our improved algorithm converges better than\noriginal algorithm, and experiments on the CIFAR10 and MNIST datasets show that\nour improved algorithm works stably and gives comparable or even better testing\naccuracies than first order optimizers SGD, Adagrad, and second order\noptimizers LBFGS in PyTorch. \n\n"}
{"id": "1805.02777", "contents": "Title: What game are we playing? End-to-end learning in normal and extensive\n  form games Abstract: Although recent work in AI has made great progress in solving large,\nzero-sum, extensive-form games, the underlying assumption in most past work is\nthat the parameters of the game itself are known to the agents. This paper\ndeals with the relatively under-explored but equally important \"inverse\"\nsetting, where the parameters of the underlying game are not known to all\nagents, but must be learned through observations. We propose a differentiable,\nend-to-end learning framework for addressing this task. In particular, we\nconsider a regularized version of the game, equivalent to a particular form of\nquantal response equilibrium, and develop 1) a primal-dual Newton method for\nfinding such equilibrium points in both normal and extensive form games; and 2)\na backpropagation method that lets us analytically compute gradients of all\nrelevant game parameters through the solution itself. This ultimately lets us\nlearn the game by training in an end-to-end fashion, effectively by integrating\na \"differentiable game solver\" into the loop of larger deep network\narchitectures. We demonstrate the effectiveness of the learning method in\nseveral settings including poker and security game tasks. \n\n"}
{"id": "1805.03270", "contents": "Title: Continuous-time integral dynamics for monotone aggregative games with\n  coupling constraints Abstract: We consider continuous-time equilibrium seeking in monotone aggregative games\nwith coupling constraints. We propose semi-decentralized integral dynamics and\nprove their global convergence to a variational generalized aggregative or Nash\nequilibrium. The proof is based on Lyapunov arguments and invariance techniques\nfor differential inclusions. \n\n"}
{"id": "1805.03359", "contents": "Title: Reward Estimation for Variance Reduction in Deep Reinforcement Learning Abstract: Reinforcement Learning (RL) agents require the specification of a reward\nsignal for learning behaviours. However, introduction of corrupt or stochastic\nrewards can yield high variance in learning. Such corruption may be a direct\nresult of goal misspecification, randomness in the reward signal, or\ncorrelation of the reward with external factors that are not known to the\nagent. Corruption or stochasticity of the reward signal can be especially\nproblematic in robotics, where goal specification can be particularly difficult\nfor complex tasks. While many variance reduction techniques have been studied\nto improve the robustness of the RL process, handling such stochastic or\ncorrupted reward structures remains difficult. As an alternative for handling\nthis scenario in model-free RL methods, we suggest using an estimator for both\nrewards and value functions. We demonstrate that this improves performance\nunder corrupted stochastic rewards in both the tabular and non-linear function\napproximation settings for a variety of noise types and environments. The use\nof reward estimation is a robust and easy-to-implement improvement for handling\ncorrupted reward signals in model-free RL. \n\n"}
{"id": "1805.03586", "contents": "Title: Policy Optimization with Second-Order Advantage Information Abstract: Policy optimization on high-dimensional continuous control tasks exhibits its\ndifficulty caused by the large variance of the policy gradient estimators. We\npresent the action subspace dependent gradient (ASDG) estimator which\nincorporates the Rao-Blackwell theorem (RB) and Control Variates (CV) into a\nunified framework to reduce the variance. To invoke RB, our proposed algorithm\n(POSA) learns the underlying factorization structure among the action space\nbased on the second-order advantage information. POSA captures the quadratic\ninformation explicitly and efficiently by utilizing the wide & deep\narchitecture. Empirical studies show that our proposed approach demonstrates\nthe performance improvements on high-dimensional synthetic settings and OpenAI\nGym's MuJoCo continuous control tasks. \n\n"}
{"id": "1805.04190", "contents": "Title: On the approximation guarantee of obviously strategyproof mechanisms Abstract: Catering to the incentives of people with limited rationality is a\nchallenging research direction that requires novel paradigms to design\nmechanisms and approximation algorithms. Obviously strategyproof (OSP)\nmechanisms have recently emerged as the concept of interest to this research\nagenda. However, the majority of the literature in the area has either\nhighlighted the shortcomings of OSP or focused on the \"right\" definition rather\nthan on the construction of these mechanisms.\n  We here give the first set of tight results on the approximation guarantee of\nOSP mechanisms for scheduling related machines and a characterization of\noptimal OSP mechanisms for set system problems. By extending the well-known\ncycle monotonicity technique, we are able to concentrate on the algorithmic\ncomponent of OSP mechanisms and provide some novel paradigms for their design. \n\n"}
{"id": "1805.04436", "contents": "Title: Capturing Complementarity in Set Functions by Going Beyond\n  Submodularity/Subadditivity Abstract: We introduce two new \"degree of complementarity\" measures, which we refer to,\nrespectively, as supermodular width and superadditive width. Both are\nformulated based on natural witnesses of complementarity. We show that both\nmeasures are robust by proving that they, respectively, characterize the gap of\nmonotone set functions from being submodular and subadditive. Thus, they define\ntwo new hierarchies over monotone set functions, which we will refer to as\nSupermodular Width (SMW) hierarchy and Superadditive Width (SAW) hierarchy,\nwith level 0 of the hierarchies resting exactly on submodular and subadditive\nfunctions, respectively.\n  We present a comprehensive comparative analysis of the SMW hierarchy and the\nSupermodular Degree (SD) hierarchy, defined by Feige and Izsak. We prove that\nthe SMW hierarchy is strictly more expressive than the SD hierarchy. We show\nthat previous results regarding approximation guarantees for welfare and\nconstrained maximization as well as regarding the Price of Anarchy (PoA) of\nsimple auctions can be extended without any loss from the SD hierarchy to the\nSMW hierarchy. We also establish almost matching information-theoretical lower\nbounds. The combination of these approximation and hardness results illustrate\nthat the SMW hierarchy provides an accurate characterization of \"near\nsubmodularity\" needed for maximization approximation. While SD and SMW\nhierarchies support nontrivial bounds on the PoA of simple auctions, we show\nthat our SAW hierarchy seems to capture more intrinsic properties needed to\nrealize the efficiency of simple auctions. So far, the SAW hierarchy provides\nthe best dependency for the PoA of Single-bid Auction, and is nearly as\ncompetitive as the Maximum over Positive Hypergraphs (MPH) hierarchy for\nSimultaneous Item First Price Auction (SIA). We provide almost tight lower\nbounds for the PoA of both auctions with respect to the SAW hierarchy. \n\n"}
{"id": "1805.04514", "contents": "Title: Metatrace Actor-Critic: Online Step-size Tuning by Meta-gradient Descent\n  for Reinforcement Learning Control Abstract: Reinforcement learning (RL) has had many successes in both \"deep\" and\n\"shallow\" settings. In both cases, significant hyperparameter tuning is often\nrequired to achieve good performance. Furthermore, when nonlinear function\napproximation is used, non-stationarity in the state representation can lead to\nlearning instability. A variety of techniques exist to combat this --- most\nnotably large experience replay buffers or the use of multiple parallel actors.\nThese techniques come at the cost of moving away from the online RL problem as\nit is traditionally formulated (i.e., a single agent learning online without\nmaintaining a large database of training examples). Meta-learning can\npotentially help with both these issues by tuning hyperparameters online and\nallowing the algorithm to more robustly adjust to non-stationarity in a\nproblem. This paper applies meta-gradient descent to derive a set of step-size\ntuning algorithms specifically for online RL control with eligibility traces.\nOur novel technique, Metatrace, makes use of an eligibility trace analogous to\nmethods like $TD(\\lambda)$. We explore tuning both a single scalar step-size\nand a separate step-size for each learned parameter. We evaluate Metatrace\nfirst for control with linear function approximation in the classic mountain\ncar problem and then in a noisy, non-stationary version. Finally, we apply\nMetatrace for control with nonlinear function approximation in 5 games in the\nArcade Learning Environment where we explore how it impacts learning speed and\nrobustness to initial step-size choice. Results show that the meta-step-size\nparameter of Metatrace is easy to set, Metatrace can speed learning, and\nMetatrace can allow an RL algorithm to deal with non-stationarity in the\nlearning task. \n\n"}
{"id": "1805.05094", "contents": "Title: Prophets and Secretaries with Overbooking Abstract: The prophet and secretary problems demonstrate online scenarios involving the\noptimal stopping theory. In a typical prophet or secretary problem, selection\ndecisions are assumed to be immediate and irrevocable. However, many online\nsettings accommodate some degree of revocability. To study such scenarios, we\nintroduce the $\\ell-out-of-k$ setting, where the decision maker can select up\nto $k$ elements immediately and irrevocably, but her performance is measured by\nthe top $\\ell$ elements in the selected set. Equivalently, the decision makes\ncan hold up to $\\ell$ elements at any given point in time, but can make up to\n$k-\\ell$ returns as new elements arrive.\n  We give upper and lower bounds on the competitive ratio of $\\ell$-out-of-$k$\nprophet and secretary scenarios. These include a single-sample prophet\nalgorithm that gives a competitive ratio of $1-\\ell\\cdot\ne^{-\\Theta\\left(\\frac{\\left(k-\\ell\\right)^2}{k}\\right)}$, which is\nasymptotically tight for $k-\\ell=\\Theta(\\ell)$. For secretary settings, we\ndevise an algorithm that obtains a competitive ratio of $1-\\ell\ne^{-\\frac{k-8\\ell}{2+2\\ln \\ell}} - e^{-k/6}$, and show that no secretary\nalgorithm obtains a better ratio than $1-e^{-k}$ (up to negligible terms). In\npassing, our results lead to an improvement of the results of Assaf et al.\n[2000] for $1-out-of-k$ prophet scenarios.\n  Beyond the contribution to online algorithms and optimal stopping theory, our\nresults have implications to mechanism design. In particular, we use our\nprophet algorithms to derive {\\em overbooking} mechanisms with good welfare and\nrevenue guarantees; these are mechanisms that sell more items than the seller's\ncapacity, then allocate to the agents with the highest values among the\nselected agents. \n\n"}
{"id": "1805.05456", "contents": "Title: Wearable Audio and IMU Based Shot Detection in Racquet Sports Abstract: Wearables like smartwatches which are embedded with sensors and powerful\nprocessors, provide a strong platform for development of analytics solutions in\nsports domain. To analyze players' games, while motion sensor based shot\ndetection has been extensively studied in sports like Tennis, Golf, Baseball;\nTable Tennis and Badminton are relatively less explored due to possible less\nintense hand motion during shots. In our paper, we propose a novel,\ncomputationally inexpensive and real-time system for shot detection in table\ntennis, based on fusion of Inertial Measurement Unit (IMU) and audio sensor\ndata embedded in a wrist-worn wearable. The system builds upon our presented\nmethodology for synchronizing IMU and audio sensor input in time using detected\nshots and achieves 95.6% accuracy. To our knowledge, it is the first\nfusion-based solution for sports analysis in wearables. Shot detectors for\nother racquet sports as well as further analytics to provide features like shot\nclassification, rally analysis and recommendations, can easily be built over\nour proposed solution. \n\n"}
{"id": "1805.05751", "contents": "Title: Local Saddle Point Optimization: A Curvature Exploitation Approach Abstract: Gradient-based optimization methods are the most popular choice for finding\nlocal optima for classical minimization and saddle point problems. Here, we\nhighlight a systemic issue of gradient dynamics that arise for saddle point\nproblems, namely the presence of undesired stable stationary points that are no\nlocal optima. We propose a novel optimization approach that exploits curvature\ninformation in order to escape from these undesired stationary points. We prove\nthat different optimization methods, including gradient method and Adagrad,\nequipped with curvature exploitation can escape non-optimal stationary points.\nWe also provide empirical results on common saddle point problems which confirm\nthe advantage of using curvature exploitation. \n\n"}
{"id": "1805.06191", "contents": "Title: Fair Allocation of Indivisible Items With Externalities Abstract: One of the important yet insufficiently studied subjects in fair allocation\nis the externality effect among agents. For a resource allocation problem,\nexternalities imply that a bundle allocated to an agent may affect the\nutilities of other agents.\n  In this paper, we conduct a study of fair allocation of indivisible goods\nwhen the externalities are not negligible. We present a simple and natural\nmodel, namely \\emph{network externalities}, to capture the externalities. To\nevaluate fairness in the network externalities model, we generalize the idea\nbehind the notion of maximin-share ($\\MMS$) to achieve a new criterion, namely,\n\\emph{extended-maximin-share} ($\\EMMS$). Next, we consider two problems\nconcerning our model.\n  First, we discuss the computational aspects of finding the value of $\\EMMS$\nfor every agent. For this, we introduce a generalized form of partitioning\nproblem that includes many famous partitioning problems such as maximin,\nminimax, and leximin partitioning problems. We show that a $1/2$-approximation\nalgorithm exists for this partitioning problem.\n  Next, we investigate on finding approximately optimal $\\EMMS$ allocations.\nThat is, allocations that guarantee every agent a utility of at least a\nfraction of his extended-maximin-share. We show that under a natural assumption\nthat the agents are $\\alpha$-self-reliant, an $\\alpha/2$-$\\EMMS$ allocation\nalways exists. The combination of this with the former result yields a\npolynomial-time $\\alpha/4$-$\\EMMS$ allocation algorithm. \n\n"}
{"id": "1805.06387", "contents": "Title: Near-Optimal Communication Lower Bounds for Approximate Nash Equilibria Abstract: We prove an $N^{2-o(1)}$ lower bound on the randomized communication\ncomplexity of finding an $\\epsilon$-approximate Nash equilibrium (for constant\n$\\epsilon>0$) in a two-player $N\\times N$ game. \n\n"}
{"id": "1805.06523", "contents": "Title: End-to-end Learning of a Convolutional Neural Network via Deep Tensor\n  Decomposition Abstract: In this paper we study the problem of learning the weights of a deep\nconvolutional neural network. We consider a network where convolutions are\ncarried out over non-overlapping patches with a single kernel in each layer. We\ndevelop an algorithm for simultaneously learning all the kernels from the\ntraining data. Our approach dubbed Deep Tensor Decomposition (DeepTD) is based\non a rank-1 tensor decomposition. We theoretically investigate DeepTD under a\nrealizable model for the training data where the inputs are chosen i.i.d. from\na Gaussian distribution and the labels are generated according to planted\nconvolutional kernels. We show that DeepTD is data-efficient and provably works\nas soon as the sample size exceeds the total number of convolutional weights in\nthe network. We carry out a variety of numerical experiments to investigate the\neffectiveness of DeepTD and verify our theoretical findings. \n\n"}
{"id": "1805.06822", "contents": "Title: DNN or k-NN: That is the Generalize vs. Memorize Question Abstract: This paper studies the relationship between the classification performed by\ndeep neural networks (DNNs) and the decision of various classical classifiers,\nnamely k-nearest neighbours (k-NN), support vector machines (SVM) and logistic\nregression (LR), at various layers of the network. This comparison provides us\nwith new insights as to the ability of neural networks to both memorize the\ntraining data and generalize to new data at the same time, where k-NN serves as\nthe ideal estimator that perfectly memorizes the data. We show that\nmemorization of non-generalizing networks happens only at the last layers.\nMoreover, the behavior of DNNs compared to the linear classifiers SVM and LR is\nquite the same on the training and test data regardless of whether the network\ngeneralizes. On the other hand, the similarity to k-NN holds only at the\nabsence of overfitting. Our results suggests that k-NN behavior of the network\non new data is a sign of generalization. Moreover, it shows that memorization\nand generalization, which are traditionally considered to be contradicting to\neach other, are compatible and complementary. \n\n"}
{"id": "1805.06962", "contents": "Title: Counterexample-Guided Data Augmentation Abstract: We present a novel framework for augmenting data sets for machine learning\nbased on counterexamples. Counterexamples are misclassified examples that have\nimportant properties for retraining and improving the model. Key components of\nour framework include a counterexample generator, which produces data items\nthat are misclassified by the model and error tables, a novel data structure\nthat stores information pertaining to misclassifications. Error tables can be\nused to explain the model's vulnerabilities and are used to efficiently\ngenerate counterexamples for augmentation. We show the efficacy of the proposed\nframework by comparing it to classical augmentation techniques on a case study\nof object detection in autonomous driving based on deep neural networks. \n\n"}
{"id": "1805.07075", "contents": "Title: Trusted Neural Networks for Safety-Constrained Autonomous Control Abstract: We propose Trusted Neural Network (TNN) models, which are deep neural network\nmodels that satisfy safety constraints critical to the application domain. We\ninvestigate different mechanisms for incorporating rule-based knowledge in the\nform of first-order logic constraints into a TNN model, where rules that encode\nsafety are accompanied by weights indicating their relative importance. This\nframework allows the TNN model to learn from knowledge available in form of\ndata as well as logical rules. We propose multiple approaches for solving this\nproblem: (a) a multi-headed model structure that allows trade-off between\nsatisfying logical constraints and fitting training data in a unified training\nframework, and (b) creating a constrained optimization problem and solving it\nin dual formulation by posing a new constrained loss function and using a\nproximal gradient descent algorithm. We demonstrate the efficacy of our TNN\nframework through experiments using the open-source TORCS~\\cite{BernhardCAA15}\n3D simulator for self-driving cars. Experiments using our first approach of a\nmulti-headed TNN model, on a dataset generated by a customized version of\nTORCS, show that (1) adding safety constraints to a neural network model\nresults in increased performance and safety, and (2) the improvement increases\nwith increasing importance of the safety constraints. Experiments were also\nperformed using the second approach of proximal algorithm for constrained\noptimization --- they demonstrate how the proposed method ensures that (1) the\noverall TNN model satisfies the constraints even when the training data\nviolates some of the constraints, and (2) the proximal gradient descent\nalgorithm on the constrained objective converges faster than the unconstrained\nversion. \n\n"}
{"id": "1805.07476", "contents": "Title: Two geometric input transformation methods for fast online reinforcement\n  learning with neural nets Abstract: We apply neural nets with ReLU gates in online reinforcement learning. Our\ngoal is to train these networks in an incremental manner, without the\ncomputationally expensive experience replay. By studying how individual neural\nnodes behave in online training, we recognize that the global nature of ReLU\ngates can cause undesirable learning interference in each node's learning\nbehavior. We propose reducing such interferences with two efficient input\ntransformation methods that are geometric in nature and match well the\ngeometric property of ReLU gates. The first one is tile coding, a classic\nbinary encoding scheme originally designed for local generalization based on\nthe topological structure of the input space. The second one (EmECS) is a new\nmethod we introduce; it is based on geometric properties of convex sets and\ntopological embedding of the input space into the boundary of a convex set. We\ndiscuss the behavior of the network when it operates on the transformed inputs.\nWe also compare it experimentally with some neural nets that do not use the\nsame input transformations, and with the classic algorithm of tile coding plus\na linear function approximator, and on several online reinforcement learning\ntasks, we show that the neural net with tile coding or EmECS can achieve not\nonly faster learning but also more accurate approximations. Our results\nstrongly suggest that geometric input transformation of this type can be\neffective for interference reduction and takes us a step closer to fully\nincremental reinforcement learning with neural nets. \n\n"}
{"id": "1805.07708", "contents": "Title: A Lyapunov-based Approach to Safe Reinforcement Learning Abstract: In many real-world reinforcement learning (RL) problems, besides optimizing\nthe main objective function, an agent must concurrently avoid violating a\nnumber of constraints. In particular, besides optimizing performance it is\ncrucial to guarantee the safety of an agent during training as well as\ndeployment (e.g. a robot should avoid taking actions - exploratory or not -\nwhich irrevocably harm its hardware). To incorporate safety in RL, we derive\nalgorithms under the framework of constrained Markov decision problems (CMDPs),\nan extension of the standard Markov decision problems (MDPs) augmented with\nconstraints on expected cumulative costs. Our approach hinges on a novel\n\\emph{Lyapunov} method. We define and present a method for constructing\nLyapunov functions, which provide an effective way to guarantee the global\nsafety of a behavior policy during training via a set of local, linear\nconstraints. Leveraging these theoretical underpinnings, we show how to use the\nLyapunov approach to systematically transform dynamic programming (DP) and RL\nalgorithms into their safe counterparts. To illustrate their effectiveness, we\nevaluate these algorithms in several CMDP planning and decision-making tasks on\na safety benchmark domain. Our results show that our proposed method\nsignificantly outperforms existing baselines in balancing constraint\nsatisfaction and performance. \n\n"}
{"id": "1805.08013", "contents": "Title: Incentive-Compatible Diffusion Abstract: Our work bridges the literature on incentive-compatible mechanism design and\nthe literature on diffusion algorithms. We introduce the study of finding an\nincentive-compatible (strategy-proof) mechanism for selecting an influential\nvertex in a directed graph (e.g. Twitter's network). The goal is to devise a\nmechanism with a bounded ratio between the maximal influence and the influence\nof the selected user, and in which no user can improve its probability of being\nselected by following or unfollowing other users. We introduce the `Two Path'\nmechanism which is based on the idea of selecting the vertex that is the first\nintersection of two independent random walks in the network. The Two Path\nmechanism is incentive compatible on directed acyclic graphs (DAGs), and has a\nfinite approximation ratio on natural subfamilies of DAGs. Simulations indicate\nthat this mechanism is suitable for practical uses. \n\n"}
{"id": "1805.08195", "contents": "Title: Depth-Limited Solving for Imperfect-Information Games Abstract: A fundamental challenge in imperfect-information games is that states do not\nhave well-defined values. As a result, depth-limited search algorithms used in\nsingle-agent settings and perfect-information games do not apply. This paper\nintroduces a principled way to conduct depth-limited solving in\nimperfect-information games by allowing the opponent to choose among a number\nof strategies for the remainder of the game at the depth limit. Each one of\nthese strategies results in a different set of values for leaf nodes. This\nforces an agent to be robust to the different strategies an opponent may\nemploy. We demonstrate the effectiveness of this approach by building a\nmaster-level heads-up no-limit Texas hold'em poker AI that defeats two prior\ntop agents using only a 4-core CPU and 16 GB of memory. Developing such a\npowerful agent would have previously required a supercomputer. \n\n"}
{"id": "1805.08877", "contents": "Title: Adversarial Label Learning Abstract: We consider the task of training classifiers without labels. We propose a\nweakly supervised method---adversarial label learning---that trains classifiers\nto perform well against an adversary that chooses labels for training data. The\nweak supervision constrains what labels the adversary can choose. The method\ntherefore minimizes an upper bound of the classifier's error rate using\nprojected primal-dual subgradient descent. Minimizing this bound protects\nagainst bias and dependencies in the weak supervision. Experiments on three\nreal datasets show that our method can train without labels and outperforms\nother approaches for weakly supervised learning. \n\n"}
{"id": "1805.08882", "contents": "Title: Multi-task Maximum Entropy Inverse Reinforcement Learning Abstract: Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring\nmultiple reward functions from expert demonstrations. Prior work, built on\nBayesian IRL, is unable to scale to complex environments due to computational\nconstraints. This paper contributes a formulation of multi-task IRL in the more\ncomputationally efficient Maximum Causal Entropy (MCE) IRL framework.\nExperiments show our approach can perform one-shot imitation learning in a\ngridworld environment that single-task IRL algorithms need hundreds of\ndemonstrations to solve. We outline preliminary work using meta-learning to\nextend our method to the function approximator setting of modern MCE IRL\nalgorithms. Evaluating on multi-task variants of common simulated robotics\nbenchmarks, we discover serious limitations of these IRL algorithms, and\nconclude with suggestions for further work. \n\n"}
{"id": "1805.08890", "contents": "Title: Step Size Matters in Deep Learning Abstract: Training a neural network with the gradient descent algorithm gives rise to a\ndiscrete-time nonlinear dynamical system. Consequently, behaviors that are\ntypically observed in these systems emerge during training, such as convergence\nto an orbit but not to a fixed point or dependence of convergence on the\ninitialization. Step size of the algorithm plays a critical role in these\nbehaviors: it determines the subset of the local optima that the algorithm can\nconverge to, and it specifies the magnitude of the oscillations if the\nalgorithm converges to an orbit. To elucidate the effects of the step size on\ntraining of neural networks, we study the gradient descent algorithm as a\ndiscrete-time dynamical system, and by analyzing the Lyapunov stability of\ndifferent solutions, we show the relationship between the step size of the\nalgorithm and the solutions that can be obtained with this algorithm. The\nresults provide an explanation for several phenomena observed in practice,\nincluding the deterioration in the training error with increased depth, the\nhardness of estimating linear mappings with large singular values, and the\ndistinct performance of deep residual networks. \n\n"}
{"id": "1805.08948", "contents": "Title: Scalable Coordinated Exploration in Concurrent Reinforcement Learning Abstract: We consider a team of reinforcement learning agents that concurrently operate\nin a common environment, and we develop an approach to efficient coordinated\nexploration that is suitable for problems of practical scale. Our approach\nbuilds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value\nfunction learning (Osband et al., 2016). We demonstrate that, for simple\ntabular contexts, the approach is competitive with previously proposed tabular\nmodel learning methods (Dimakopoulou and Van Roy, 2018). With a\nhigher-dimensional problem and a neural network value function representation,\nthe approach learns quickly with far fewer agents than alternative exploration\nschemes. \n\n"}
{"id": "1805.09267", "contents": "Title: Reinforcement Learning for Heterogeneous Teams with PALO Bounds Abstract: We introduce reinforcement learning for heterogeneous teams in which rewards\nfor an agent are additively factored into local costs, stimuli unique to each\nagent, and global rewards, those shared by all agents in the domain. Motivating\ndomains include coordination of varied robotic platforms, which incur different\ncosts for the same action, but share an overall goal. We present two templates\nfor learning in this setting with factored rewards: a generalization of\nPerkins' Monte Carlo exploring starts for POMDPs to canonical MPOMDPs, with a\nsingle policy mapping joint observations of all agents to joint actions\n(MCES-MP); and another with each agent individually mapping joint observations\nto their own action (MCES-FMP). We use probably approximately local optimal\n(PALO) bounds to analyze sample complexity, instantiating these templates to\nPALO learning. We promote sample efficiency by including a policy space pruning\ntechnique, and evaluate the approaches on three domains of heterogeneous agents\ndemonstrating that MCES-FMP yields improved policies in less samples compared\nto MCES-MP and a previous benchmark. \n\n"}
{"id": "1805.09293", "contents": "Title: Learning to Optimize Contextually Constrained Problems for Real-Time\n  Decision-Generation Abstract: The topic of learning to solve optimization problems has received interest\nfrom both the operations research and machine learning communities. In this\nwork, we combine techniques from both fields to address the problem of learning\nto generate decisions to instances of continuous optimization problems where\nthe feasible set varies with contextual features. We propose a novel framework\nfor training a generative model to estimate optimal decisions by combining\ninterior point methods and adversarial learning, which we further embed within\nan data generation algorithm. Decisions generated by our model satisfy\nin-sample and out-of-sample optimality guarantees. Finally, we investigate case\nstudies in portfolio optimization and personalized treatment design,\ndemonstrating that our approach yields advantages over predict-then-optimize\nand supervised deep learning techniques, respectively. \n\n"}
{"id": "1805.09386", "contents": "Title: Predictive Local Smoothness for Stochastic Gradient Methods Abstract: Stochastic gradient methods are dominant in nonconvex optimization especially\nfor deep models but have low asymptotical convergence due to the fixed\nsmoothness. To address this problem, we propose a simple yet effective method\nfor improving stochastic gradient methods named predictive local smoothness\n(PLS). First, we create a convergence condition to build a learning rate which\nvaries adaptively with local smoothness. Second, the local smoothness can be\npredicted by the latest gradients. Third, we use the adaptive learning rate to\nupdate the stochastic gradients for exploring linear convergence rates. By\napplying the PLS method, we implement new variants of three popular algorithms:\nPLS-stochastic gradient descent (PLS-SGD), PLS-accelerated SGD (PLS-AccSGD),\nand PLS-AMSGrad. Moreover, we provide much simpler proofs to ensure their\nlinear convergence. Empirical results show that the variants have better\nperformance gains than the popular algorithms, such as, faster convergence and\nalleviating explosion and vanish of gradients. \n\n"}
{"id": "1805.09388", "contents": "Title: Regret Bounds for Robust Adaptive Control of the Linear Quadratic\n  Regulator Abstract: We consider adaptive control of the Linear Quadratic Regulator (LQR), where\nan unknown linear system is controlled subject to quadratic costs. Leveraging\nrecent developments in the estimation of linear systems and in robust\ncontroller synthesis, we present the first provably polynomial time algorithm\nthat provides high probability guarantees of sub-linear regret on this problem.\nWe further study the interplay between regret minimization and parameter\nestimation by proving a lower bound on the expected regret in terms of the\nexploration schedule used by any algorithm. Finally, we conduct a numerical\nstudy comparing our robust adaptive algorithm to other methods from the\nadaptive LQR literature, and demonstrate the flexibility of our proposed method\nby extending it to a demand forecasting problem subject to state constraints. \n\n"}
{"id": "1805.09480", "contents": "Title: Optimal Algorithms for Continuous Non-monotone Submodular and\n  DR-Submodular Maximization Abstract: In this paper we study the fundamental problems of maximizing a continuous\nnon-monotone submodular function over the hypercube, both with and without\ncoordinate-wise concavity. This family of optimization problems has several\napplications in machine learning, economics, and communication systems. Our\nmain result is the first $\\frac{1}{2}$-approximation algorithm for continuous\nsubmodular function maximization; this approximation factor of $\\frac{1}{2}$ is\nthe best possible for algorithms that only query the objective function at\npolynomially many points. For the special case of DR-submodular maximization,\ni.e. when the submodular functions is also coordinate wise concave along all\ncoordinates, we provide a different $\\frac{1}{2}$-approximation algorithm that\nruns in quasilinear time. Both of these results improve upon prior work [Bian\net al, 2017, Soma and Yoshida, 2017].\n  Our first algorithm uses novel ideas such as reducing the guaranteed\napproximation problem to analyzing a zero-sum game for each coordinate, and\nincorporates the geometry of this zero-sum game to fix the value at this\ncoordinate. Our second algorithm exploits coordinate-wise concavity to identify\na monotone equilibrium condition sufficient for getting the required\napproximation guarantee, and hunts for the equilibrium point using binary\nsearch. We further run experiments to verify the performance of our proposed\nalgorithms in related machine learning applications. \n\n"}
{"id": "1805.10005", "contents": "Title: Finite Sample Analysis of LSTD with Random Projections and Eligibility\n  Traces Abstract: Policy evaluation with linear function approximation is an important problem\nin reinforcement learning. When facing high-dimensional feature spaces, such a\nproblem becomes extremely hard considering the computation efficiency and\nquality of approximations. We propose a new algorithm, LSTD($\\lambda$)-RP,\nwhich leverages random projection techniques and takes eligibility traces into\nconsideration to tackle the above two challenges. We carry out theoretical\nanalysis of LSTD($\\lambda$)-RP, and provide meaningful upper bounds of the\nestimation error, approximation error and total generalization error. These\nresults demonstrate that LSTD($\\lambda$)-RP can benefit from random projection\nand eligibility traces strategies, and LSTD($\\lambda$)-RP can achieve better\nperformances than prior LSTD-RP and LSTD($\\lambda$) algorithms. \n\n"}
{"id": "1805.10115", "contents": "Title: On incremental deployability Abstract: Motivated by the difficulty of effecting fundamental change in the\narchitecture of the Internet, in this paper, we study from a theoretical\nperspective the question of how individuals can join forces toward collective\nventures. To that end, we draw on an elementary concept in Internet systems\nengineering, namely, that of incremental deployability, which we study\nmathematically and computationally. For example, we show that incremental\ndeployability is at least as general a concept as the Nash equilibrium (in that\nthe latter can be derived from the former). We then draw on this foundation to\ndesign and analyze institutional mechanisms that are not only promising to\nbootstrap emerging Internet architectures but they also have broader\napplications in social organization beyond its predominant market (and\nfinance)-based character. \n\n"}
{"id": "1805.10662", "contents": "Title: Fingerprint Policy Optimisation for Robust Reinforcement Learning Abstract: Policy gradient methods ignore the potential value of adjusting environment\nvariables: unobservable state features that are randomly determined by the\nenvironment in a physical setting, but are controllable in a simulator. This\ncan lead to slow learning, or convergence to suboptimal policies, if the\nenvironment variable has a large impact on the transition dynamics. In this\npaper, we present fingerprint policy optimisation (FPO), which finds a policy\nthat is optimal in expectation across the distribution of environment\nvariables. The central idea is to use Bayesian optimisation (BO) to actively\nselect the distribution of the environment variable that maximises the\nimprovement generated by each iteration of the policy gradient method. To make\nthis BO practical, we contribute two easy-to-compute low-dimensional\nfingerprints of the current policy. Our experiments show that FPO can\nefficiently learn policies that are robust to significant rare events, which\nare unlikely to be observable under random sampling, but are key to learning\ngood policies. \n\n"}
{"id": "1805.10693", "contents": "Title: Strategyproof Linear Regression in High Dimensions Abstract: This paper is part of an emerging line of work at the intersection of machine\nlearning and mechanism design, which aims to avoid noise in training data by\ncorrectly aligning the incentives of data sources. Specifically, we focus on\nthe ubiquitous problem of linear regression, where strategyproof mechanisms\nhave previously been identified in two dimensions. In our setting, agents have\nsingle-peaked preferences and can manipulate only their response variables. Our\nmain contribution is the discovery of a family of group strategyproof linear\nregression mechanisms in any number of dimensions, which we call generalized\nresistant hyperplane mechanisms. The game-theoretic properties of these\nmechanisms -- and, in fact, their very existence -- are established through a\nconnection to a discrete version of the Ham Sandwich Theorem. \n\n"}
{"id": "1805.10817", "contents": "Title: GPGPU Linear Complexity t-SNE Optimization Abstract: The t-distributed Stochastic Neighbor Embedding (tSNE) algorithm has become\nin recent years one of the most used and insightful techniques for the\nexploratory data analysis of high-dimensional data. tSNE reveals clusters of\nhigh-dimensional data points at different scales while it requires only minimal\ntuning of its parameters. Despite these advantages, the computational\ncomplexity of the algorithm limits its application to relatively small\ndatasets. To address this problem, several evolutions of tSNE have been\ndeveloped in recent years, mainly focusing on the scalability of the similarity\ncomputations between data points. However, these contributions are insufficient\nto achieve interactive rates when visualizing the evolution of the tSNE\nembedding for large datasets. In this work, we present a novel approach to the\nminimization of the tSNE objective function that heavily relies on modern\ngraphics hardware and has linear computational complexity. Our technique does\nnot only beat the state of the art, but can even be executed on the client side\nin a browser. We propose to approximate the repulsion forces between data\npoints using adaptive-resolution textures that are drawn at every iteration\nwith WebGL. This approximation allows us to reformulate the tSNE minimization\nproblem as a series of tensor operation that are computed with TensorFlow.js, a\nJavaScript library for scalable tensor computations. \n\n"}
{"id": "1805.11088", "contents": "Title: Deep Reinforcement Learning in Ice Hockey for Context-Aware Player\n  Evaluation Abstract: A variety of machine learning models have been proposed to assess the\nperformance of players in professional sports. However, they have only a\nlimited ability to model how player performance depends on the game context.\nThis paper proposes a new approach to capturing game context: we apply Deep\nReinforcement Learning (DRL) to learn an action-value Q function from 3M\nplay-by-play events in the National Hockey League (NHL). The neural network\nrepresentation integrates both continuous context signals and game history,\nusing a possession-based LSTM. The learned Q-function is used to value players'\nactions under different game contexts. To assess a player's overall\nperformance, we introduce a novel Game Impact Metric (GIM) that aggregates the\nvalues of the player's actions. Empirical Evaluation shows GIM is consistent\nthroughout a play season, and correlates highly with standard success measures\nand future salary. \n\n"}
{"id": "1805.11447", "contents": "Title: Virtuously Safe Reinforcement Learning Abstract: We show that when a third party, the adversary, steps into the two-party\nsetting (agent and operator) of safely interruptible reinforcement learning, a\ntrade-off has to be made between the probability of following the optimal\npolicy in the limit, and the probability of escaping a dangerous situation\ncreated by the adversary. So far, the work on safely interruptible agents has\nassumed a perfect perception of the agent about its environment (no adversary),\nand therefore implicitly set the second probability to zero, by explicitly\nseeking a value of one for the first probability. We show that (1) agents can\nbe made both interruptible and adversary-resilient, and (2) the\ninterruptibility can be made safe in the sense that the agent itself will not\nseek to avoid it. We also solve the problem that arises when the agent does not\ngo completely greedy, i.e. issues with safe exploration in the limit.\nResilience to perturbed perception, safe exploration in the limit, and safe\ninterruptibility are the three pillars of what we call \\emph{virtuously safe\nreinforcement learning}. \n\n"}
{"id": "1805.11450", "contents": "Title: Model-based Pricing for Machine Learning in a Data Marketplace Abstract: Data analytics using machine learning (ML) has become ubiquitous in science,\nbusiness intelligence, journalism and many other domains. While a lot of work\nfocuses on reducing the training cost, inference runtime and storage cost of ML\nmodels, little work studies how to reduce the cost of data acquisition, which\npotentially leads to a loss of sellers' revenue and buyers' affordability and\nefficiency.\n  In this paper, we propose a model-based pricing (MBP) framework, which\ninstead of pricing the data, directly prices ML model instances. We first\nformally describe the desired properties of the MBP framework, with a focus on\navoiding arbitrage. Next, we show a concrete realization of the MBP framework\nvia a noise injection approach, which provably satisfies the desired formal\nproperties. Based on the proposed framework, we then provide algorithmic\nsolutions on how the seller can assign prices to models under different market\nscenarios (such as to maximize revenue). Finally, we conduct extensive\nexperiments, which validate that the MBP framework can provide high revenue to\nthe seller, high affordability to the buyer, and also operate on low runtime\ncost. \n\n"}
{"id": "1805.11711", "contents": "Title: Depth and nonlinearity induce implicit exploration for RL Abstract: The question of how to explore, i.e., take actions with uncertain outcomes to\nlearn about possible future rewards, is a key question in reinforcement\nlearning (RL). Here, we show a surprising result: We show that Q-learning with\nnonlinear Q-function and no explicit exploration (i.e., a purely greedy policy)\ncan learn several standard benchmark tasks, including mountain car, equally\nwell as, or better than, the most commonly-used $\\epsilon$-greedy exploration.\nWe carefully examine this result and show that both the depth of the Q-network\nand the type of nonlinearity are important to induce such deterministic\nexploration. \n\n"}
{"id": "1805.12559", "contents": "Title: The Complexity of Splitting Necklaces and Bisecting Ham Sandwiches Abstract: We resolve the computational complexity of two problems known as\nNECKLACE-SPLITTING and DISCRETE HAM SANDWICH, showing that they are\nPPA-complete. For NECKLACE SPLITTING, this result is specific to the important\nspecial case in which two thieves share the necklace. We do this via a\nPPA-completeness result for an approximate version of the CONSENSUS-HALVING\nproblem, strengthening our recent result that the problem is PPA-complete for\ninverse-exponential precision. At the heart of our construction is a smooth\nembedding of the high-dimensional M\\\"obius strip in the CONSENSUS-HALVING\nproblem. These results settle the status of PPA as a class that captures the\ncomplexity of \"natural\" problems whose definitions do not incorporate a\ncircuit. \n\n"}
{"id": "1806.00413", "contents": "Title: Global linear convergence of Newton's method without strong-convexity or\n  Lipschitz gradients Abstract: We show that Newton's method converges globally at a linear rate for\nobjective functions whose Hessians are stable. This class of problems includes\nmany functions which are not strongly convex, such as logistic regression. Our\nlinear convergence result is (i) affine-invariant, and holds even if an (ii)\napproximate Hessian is used, and if the subproblems are (iii) only solved\napproximately. Thus we theoretically demonstrate the superiority of Newton's\nmethod over first-order methods, which would only achieve a sublinear\n$O(1/t^2)$ rate under similar conditions. \n\n"}
{"id": "1806.00499", "contents": "Title: Backpropagation for Implicit Spectral Densities Abstract: Most successful machine intelligence systems rely on gradient-based learning,\nwhich is made possible by backpropagation. Some systems are designed to aid us\nin interpreting data when explicit goals cannot be provided. These unsupervised\nsystems are commonly trained by backpropagating through a likelihood function.\nWe introduce a tool that allows us to do this even when the likelihood is not\nexplicitly set, by instead using the implicit likelihood of the model.\nExplicitly defining the likelihood often entails making heavy-handed\nassumptions that impede our ability to solve challenging tasks. On the other\nhand, the implicit likelihood of the model is accessible without the need for\nsuch assumptions. Our tool, which we call spectral backpropagation, allows us\nto optimize it in much greater generality than what has been attempted before.\nGANs can also be viewed as a technique for optimizing implicit likelihoods. We\nstudy them using spectral backpropagation in order to demonstrate robustness\nfor high-dimensional problems, and identify two novel properties of the\ngenerator G: (1) there exist aberrant, nonsensical outputs to which G assigns\nvery high likelihood, and (2) the eigenvectors of the metric induced by G over\nlatent space correspond to quasi-disentangled explanatory factors. \n\n"}
{"id": "1806.00900", "contents": "Title: Algorithmic Regularization in Learning Deep Homogeneous Models: Layers\n  are Automatically Balanced Abstract: We study the implicit regularization imposed by gradient descent for learning\nmulti-layer homogeneous functions including feed-forward fully connected and\nconvolutional deep neural networks with linear, ReLU or Leaky ReLU activation.\nWe rigorously prove that gradient flow (i.e. gradient descent with\ninfinitesimal step size) effectively enforces the differences between squared\nnorms across different layers to remain invariant without any explicit\nregularization. This result implies that if the weights are initially small,\ngradient flow automatically balances the magnitudes of all layers. Using a\ndiscretization argument, we analyze gradient descent with positive step size\nfor the non-convex low-rank asymmetric matrix factorization problem without any\nregularization. Inspired by our findings for gradient flow, we prove that\ngradient descent with step sizes $\\eta_t = O\\left(t^{-\\left(\n\\frac12+\\delta\\right)} \\right)$ ($0<\\delta\\le\\frac12$) automatically balances\ntwo low-rank factors and converges to a bounded global optimum. Furthermore,\nfor rank-$1$ asymmetric matrix factorization we give a finer analysis showing\ngradient descent with constant step size converges to the global minimum at a\nglobally linear rate. We believe that the idea of examining the invariance\nimposed by first order algorithms in learning homogeneous models could serve as\na fundamental building block for studying optimization for learning deep\nmodels. \n\n"}
{"id": "1806.01242", "contents": "Title: Graph networks as learnable physics engines for inference and control Abstract: Understanding and interacting with everyday physical scenes requires rich\nknowledge about the structure of the world, represented either implicitly in a\nvalue or policy function, or explicitly in a transition model. Here we\nintroduce a new class of learnable models--based on graph networks--which\nimplement an inductive bias for object- and relation-centric representations of\ncomplex, dynamical systems. Our results show that as a forward model, our\napproach supports accurate predictions from real and simulated data, and\nsurprisingly strong and efficient generalization, across eight distinct\nphysical systems which we varied parametrically and structurally. We also found\nthat our inference model can perform system identification. Our models are also\ndifferentiable, and support online planning via gradient-based trajectory\noptimization, as well as offline policy optimization. Our framework offers new\nopportunities for harnessing and exploiting rich knowledge about the world, and\ntakes a key step toward building machines with more human-like representations\nof the world. \n\n"}
{"id": "1806.01261", "contents": "Title: Relational inductive biases, deep learning, and graph networks Abstract: Artificial intelligence (AI) has undergone a renaissance recently, making\nmajor progress in key domains such as vision, language, control, and\ndecision-making. This has been due, in part, to cheap data and cheap compute\nresources, which have fit the natural strengths of deep learning. However, many\ndefining characteristics of human intelligence, which developed under much\ndifferent pressures, remain out of reach for current approaches. In particular,\ngeneralizing beyond one's experiences--a hallmark of human intelligence from\ninfancy--remains a formidable challenge for modern AI.\n  The following is part position paper, part review, and part unification. We\nargue that combinatorial generalization must be a top priority for AI to\nachieve human-like abilities, and that structured representations and\ncomputations are key to realizing this objective. Just as biology uses nature\nand nurture cooperatively, we reject the false choice between\n\"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an\napproach which benefits from their complementary strengths. We explore how\nusing relational inductive biases within deep learning architectures can\nfacilitate learning about entities, relations, and rules for composing them. We\npresent a new building block for the AI toolkit with a strong relational\ninductive bias--the graph network--which generalizes and extends various\napproaches for neural networks that operate on graphs, and provides a\nstraightforward interface for manipulating structured knowledge and producing\nstructured behaviors. We discuss how graph networks can support relational\nreasoning and combinatorial generalization, laying the foundation for more\nsophisticated, interpretable, and flexible patterns of reasoning. As a\ncompanion to this paper, we have released an open-source software library for\nbuilding graph networks, with demonstrations of how to use them in practice. \n\n"}
{"id": "1806.01347", "contents": "Title: Importance Sampling Policy Evaluation with an Estimated Behavior Policy Abstract: We consider the problem of off-policy evaluation in Markov decision\nprocesses. Off-policy evaluation is the task of evaluating the expected return\nof one policy with data generated by a different, behavior policy. Importance\nsampling is a technique for off-policy evaluation that re-weights off-policy\nreturns to account for differences in the likelihood of the returns between the\ntwo policies. In this paper, we study importance sampling with an estimated\nbehavior policy where the behavior policy estimate comes from the same set of\ndata used to compute the importance sampling estimate. We find that this\nestimator often lowers the mean squared error of off-policy evaluation compared\nto importance sampling with the true behavior policy or using a behavior policy\nthat is estimated from a separate data set. Intuitively, estimating the\nbehavior policy in this way corrects for error due to sampling in the\naction-space. Our empirical results also extend to other popular variants of\nimportance sampling and show that estimating a non-Markovian behavior policy\ncan further lower large-sample mean squared error even when the true behavior\npolicy is Markovian. \n\n"}
{"id": "1806.01488", "contents": "Title: A Primer on Causal Analysis Abstract: We provide a conceptual map to navigate causal analysis problems. Focusing on\nthe case of discrete random variables, we consider the case of causal effect\nestimation from observational data. The presented approaches apply also to\ncontinuous variables, but the issue of estimation becomes more complex. We then\nintroduce the four schools of thought for causal analysis \n\n"}
{"id": "1806.02322", "contents": "Title: Learning Kolmogorov Models for Binary Random Variables Abstract: We summarize our recent findings, where we proposed a framework for learning\na Kolmogorov model, for a collection of binary random variables. More\nspecifically, we derive conditions that link outcomes of specific random\nvariables, and extract valuable relations from the data. We also propose an\nalgorithm for computing the model and show its first-order optimality, despite\nthe combinatorial nature of the learning problem. We apply the proposed\nalgorithm to recommendation systems, although it is applicable to other\nscenarios. We believe that the work is a significant step toward interpretable\nmachine learning. \n\n"}
{"id": "1806.02639", "contents": "Title: Path-Level Network Transformation for Efficient Architecture Search Abstract: We introduce a new function-preserving transformation for efficient neural\narchitecture search. This network transformation allows reusing previously\ntrained networks and existing successful architectures that improves sample\nefficiency. We aim to address the limitation of current network transformation\noperations that can only perform layer-level architecture modifications, such\nas adding (pruning) filters or inserting (removing) a layer, which fails to\nchange the topology of connection paths. Our proposed path-level transformation\noperations enable the meta-controller to modify the path topology of the given\nnetwork while keeping the merits of reusing weights, and thus allow efficiently\ndesigning effective structures with complex path topologies like Inception\nmodels. We further propose a bidirectional tree-structured reinforcement\nlearning meta-controller to explore a simple yet highly expressive\ntree-structured architecture space that can be viewed as a generalization of\nmulti-branch architectures. We experimented on the image classification\ndatasets with limited computational resources (about 200 GPU-hours), where we\nobserved improved parameter efficiency and better test results (97.70% test\naccuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet\nin the mobile setting), demonstrating the effectiveness and transferability of\nour designed architectures. \n\n"}
{"id": "1806.02643", "contents": "Title: Re-evaluating Evaluation Abstract: Progress in machine learning is measured by careful evaluation on problems of\noutstanding common interest. However, the proliferation of benchmark suites and\nenvironments, adversarial attacks, and other complications has diluted the\nbasic evaluation model by overwhelming researchers with choices. Deliberate or\naccidental cherry picking is increasingly likely, and designing well-balanced\nevaluation suites requires increasing effort. In this paper we take a step back\nand propose Nash averaging. The approach builds on a detailed analysis of the\nalgebraic structure of evaluation in two basic scenarios: agent-vs-agent and\nagent-vs-task. The key strength of Nash averaging is that it automatically\nadapts to redundancies in evaluation data, so that results are not biased by\nthe incorporation of easy tasks or weak agents. Nash averaging thus encourages\nmaximally inclusive evaluation -- since there is no harm (computational cost\naside) from including all available tasks and agents. \n\n"}
{"id": "1806.02813", "contents": "Title: Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement\n  Learning with Trajectory Embeddings Abstract: In this work, we take a representation learning perspective on hierarchical\nreinforcement learning, where the problem of learning lower layers in a\nhierarchy is transformed into the problem of learning trajectory-level\ngenerative models. We show that we can learn continuous latent representations\nof trajectories, which are effective in solving temporally extended and\nmulti-stage problems. Our proposed model, SeCTAR, draws inspiration from\nvariational autoencoders, and learns latent representations of trajectories. A\nkey component of this method is to learn both a latent-conditioned policy and a\nlatent-conditioned model which are consistent with each other. Given the same\nlatent, the policy generates a trajectory which should match the trajectory\npredicted by the model. This model provides a built-in prediction mechanism, by\npredicting the outcome of closed loop policy behavior. We propose a novel\nalgorithm for performing hierarchical RL with this model, combining model-based\nplanning in the learned latent space with an unsupervised exploration\nobjective. We show that our model is effective at reasoning over long horizons\nwith sparse rewards for several simulated tasks, outperforming standard\nreinforcement learning methods and prior methods for hierarchical reasoning,\nmodel-based planning, and exploration. \n\n"}
{"id": "1806.02927", "contents": "Title: Lightweight Stochastic Optimization for Minimizing Finite Sums with\n  Infinite Data Abstract: Variance reduction has been commonly used in stochastic optimization. It\nrelies crucially on the assumption that the data set is finite. However, when\nthe data are imputed with random noise as in data augmentation, the perturbed\ndata set be- comes essentially infinite. Recently, the stochastic MISO (S-MISO)\nalgorithm is introduced to address this expected risk minimization problem.\nThough it converges faster than SGD, a significant amount of memory is\nrequired. In this pa- per, we propose two SGD-like algorithms for expected risk\nminimization with random perturbation, namely, stochastic sample average\ngradient (SSAG) and stochastic SAGA (S-SAGA). The memory cost of SSAG does not\ndepend on the sample size, while that of S-SAGA is the same as those of\nvariance reduction methods on un- perturbed data. Theoretical analysis and\nexperimental results on logistic regression and AUC maximization show that SSAG\nhas faster convergence rate than SGD with comparable space requirement, while\nS-SAGA outperforms S-MISO in terms of both iteration complexity and storage. \n\n"}
{"id": "1806.03108", "contents": "Title: Ergodic Mean-Payoff Games for the Analysis of Attacks in\n  Crypto-Currencies Abstract: Crypto-currencies are digital assets designed to work as a medium of\nexchange, e.g., Bitcoin, but they are susceptible to attacks (dishonest\nbehavior of participants). A framework for the analysis of attacks in\ncrypto-currencies requires (a) modeling of game-theoretic aspects to analyze\nincentives for deviation from honest behavior; (b) concurrent interactions\nbetween participants; and (c) analysis of long-term monetary gains. Traditional\ngame-theoretic approaches for the analysis of security protocols consider\neither qualitative temporal properties such as safety and termination, or the\nvery special class of one-shot (stateless) games. However, to analyze general\nattacks on protocols for crypto-currencies, both stateful analysis and\nquantitative objectives are necessary. In this work our main contributions are\nas follows: (a) we show how a class of concurrent mean-payoff games, namely\nergodic games, can model various attacks that arise naturally in\ncrypto-currencies; (b) we present the first practical implementation of\nalgorithms for ergodic games that scales to model realistic problems for\ncrypto-currencies; and (c) we present experimental results showing that our\nframework can handle games with thousands of states and millions of\ntransitions. \n\n"}
{"id": "1806.04067", "contents": "Title: Adaptive Mechanism Design: Learning to Promote Cooperation Abstract: In the future, artificial learning agents are likely to become increasingly\nwidespread in our society. They will interact with both other learning agents\nand humans in a variety of complex settings including social dilemmas. We\nconsider the problem of how an external agent can promote cooperation between\nartificial learners by distributing additional rewards and punishments based on\nobserving the learners' actions. We propose a rule for automatically learning\nhow to create right incentives by considering the players' anticipated\nparameter updates. Using this learning rule leads to cooperation with high\nsocial welfare in matrix games in which the agents would otherwise learn to\ndefect with high probability. We show that the resulting cooperative outcome is\nstable in certain games even if the planning agent is turned off after a given\nnumber of episodes, while other games require ongoing intervention to maintain\nmutual cooperation. However, even in the latter case, the amount of necessary\nadditional incentives decreases over time. \n\n"}
{"id": "1806.04242", "contents": "Title: The Potential of the Return Distribution for Exploration in RL Abstract: This paper studies the potential of the return distribution for exploration\nin deterministic reinforcement learning (RL) environments. We study network\nlosses and propagation mechanisms for Gaussian, Categorical and Gaussian\nmixture distributions. Combined with exploration policies that leverage this\nreturn distribution, we solve, for example, a randomized Chain task of length\n100, which has not been reported before when learning with neural networks. \n\n"}
{"id": "1806.04640", "contents": "Title: Unsupervised Meta-Learning for Reinforcement Learning Abstract: Meta-learning algorithms use past experience to learn to quickly solve new\ntasks. In the context of reinforcement learning, meta-learning algorithms\nacquire reinforcement learning procedures to solve new problems more\nefficiently by utilizing experience from prior tasks. The performance of\nmeta-learning algorithms depends on the tasks available for meta-training: in\nthe same way that supervised learning generalizes best to test points drawn\nfrom the same distribution as the training points, meta-learning methods\ngeneralize best to tasks from the same distribution as the meta-training tasks.\nIn effect, meta-reinforcement learning offloads the design burden from\nalgorithm design to task design. If we can automate the process of task design\nas well, we can devise a meta-learning algorithm that is truly automated. In\nthis work, we take a step in this direction, proposing a family of unsupervised\nmeta-learning algorithms for reinforcement learning. We motivate and describe a\ngeneral recipe for unsupervised meta-reinforcement learning, and present an\ninstantiation of this approach. Our conceptual and theoretical contributions\nconsist of formulating the unsupervised meta-reinforcement learning problem and\ndescribing how task proposals based on mutual information can be used to train\noptimal meta-learners. Our experimental results indicate that unsupervised\nmeta-reinforcement learning effectively acquires accelerated reinforcement\nlearning procedures without the need for manual task design and these\nprocedures exceed the performance of learning from scratch. \n\n"}
{"id": "1806.05151", "contents": "Title: On Landscape of Lagrangian Functions and Stochastic Search for\n  Constrained Nonconvex Optimization Abstract: We study constrained nonconvex optimization problems in machine learning,\nsignal processing, and stochastic control. It is well-known that these problems\ncan be rewritten to a minimax problem in a Lagrangian form. However, due to the\nlack of convexity, their landscape is not well understood and how to find the\nstable equilibria of the Lagrangian function is still unknown. To bridge the\ngap, we study the landscape of the Lagrangian function. Further, we define a\nspecial class of Lagrangian functions. They enjoy two properties: 1.Equilibria\nare either stable or unstable (Formal definition in Section 2); 2.Stable\nequilibria correspond to the global optima of the original problem. We show\nthat a generalized eigenvalue (GEV) problem, including canonical correlation\nanalysis and other problems, belongs to the class. Specifically, we\ncharacterize its stable and unstable equilibria by leveraging an invariant\ngroup and symmetric property (more details in Section 3). Motivated by these\nneat geometric structures, we propose a simple, efficient, and stochastic\nprimal-dual algorithm solving the online GEV problem. Theoretically, we provide\nsufficient conditions, based on which we establish an asymptotic convergence\nrate and obtain the first sample complexity result for the online GEV problem\nby diffusion approximations, which are widely used in applied probability and\nstochastic control. Numerical results are provided to support our theory. \n\n"}
{"id": "1806.05544", "contents": "Title: Constrained existence problem for weak subgame perfect equilibria with\n  omega-regular Boolean objectives (full version) Abstract: We study multiplayer turn-based games played on a finite directed graph such\nthat each player aims at satisfying an omega-regular Boolean objective. Instead\nof the well-known notions of Nash equilibrium (NE) and subgame perfect\nequilibrium (SPE), we focus on the recent notion of weak subgame perfect\nequilibrium (weak SPE), a refinement of SPE. In this setting, players who\ndeviate can only use the subclass of strategies that differ from the original\none on a finite number of histories. We are interested in the constrained\nexistence problem for weak SPEs. We provide a complete characterization of the\ncomputational complexity of this problem: it is P-complete for Explicit Muller\nobjectives, NP-complete for Co-B\\\"uchi, Parity, Muller, Rabin, and Streett\nobjectives, and PSPACE-complete for Reachability and Safety objectives (we only\nprove NP-membership for B\\\"uchi objectives). We also show that the constrained\nexistence problem is fixed parameter tractable and is polynomial when the\nnumber of players is fixed. All these results are based on a fine analysis of a\nfixpoint algorithm that computes the set of possible payoff profiles underlying\nweak SPEs. \n\n"}
{"id": "1806.05799", "contents": "Title: CIA-Towards a Unified Marketing Optimization Framework for e-Commerce\n  Sponsored Search Abstract: As the largest e-commerce platform, Taobao helps advertisers reach billions\nof search queries each day via sponsored search, which has also contributed\nconsiderable revenue to the platform. An efficient bidding strategy to cater to\ndiverse advertiser demands while balancing platform revenue and consumer\nexperience is significant to a healthy and sustainable marketing ecosystem. In\nthis paper we propose \\emph{Customer Intelligent Agent (CIA)}, a bidding\noptimization framework which implements an impression-level bidding to reflect\nadvertisers' conversion willingness and budget control. In this way, CIA is\ncapable of fulfilling various e-commerce advertiser demands on different\nlevels, such as Gross Merchandise Volume optimization, style comparison etc.\nAdditionally, a replay based simulation system is designed to predict the\nperformance of different take-rate. CIA unifies the benefits of three parties\nin the marketing ecosystem without changing the Generalized Second Price\nmechanism. Our extensive offline simulations and large-scale online experiments\non \\emph{Taobao Search Advertising (TSA)} platform verify the high\neffectiveness of the CIA framework. Moreover, CIA has been deployed online as a\nmajor bidding tool in TSA. \n\n"}
{"id": "1806.06230", "contents": "Title: Nonsmooth Aggregative Games with Coupling Constraints and Infinitely\n  Many Classes of Players Abstract: After defining a pure-action profile in a nonatomic aggregative game, where\nplayers have specific compact convex pure-action sets and nonsmooth convex cost\nfunctions, as a square-integrable function, we characterize a Wardrop\nequilibrium as a solution to an infinite-dimensional generalized variational\ninequality. We show the existence of Wardrop equilibrium and variational\nWardrop equilibrium, a concept of equilibrium adapted to the presence of\ncoupling constraints, in monotone nonatomic aggregative games. The uniqueness\nof (variational) Wardrop equilibrium is proved for strictly or aggregatively\nstrictly monotone nonatomic aggregative games. We then show that, for a\nsequence of finite-player aggregative games with aggregative constraints, if\nthe players' pure-action sets converge to those of a strongly (resp.\naggregatively strongly) monotone nonatomic aggregative game, and the\naggregative constraints in the finite-player games converge to the aggregative\nconstraint of the nonatomic game, then a sequence of so-called variational Nash\nequilibria in these finite-player games converge to the variational Wardrop\nequilibrium in pure-action profile (resp. aggregate-action profile). In\nparticular, it allows the construction of an auxiliary sequence of games with\nfinite-dimensional equilibria to approximate the infinite-dimensional\nequilibrium in such a nonatomic game. Finally, we show how to construct\nauxiliary finite-player games for two general classes of nonatomic games. \n\n"}
{"id": "1806.06266", "contents": "Title: On Strategyproof Conference Peer Review Abstract: We consider peer review in a conference setting where there is typically an\noverlap between the set of reviewers and the set of authors. This overlap can\nincentivize strategic reviews to influence the final ranking of one's own\npapers. In this work, we address this problem through the lens of social\nchoice, and present a theoretical framework for strategyproof and efficient\npeer review. We first present and analyze an algorithm for reviewer-assignment\nand aggregation that guarantees strategyproofness and a natural efficiency\nproperty called unanimity, when the authorship graph satisfies a simple\nproperty. Our algorithm is based on the so-called partitioning method, and can\nbe thought as a generalization of this method to conference peer review\nsettings. We then empirically show that the requisite property on the\nauthorship graph is indeed satisfied in the submission data from the ICLR\nconference, and further demonstrate a simple trick to make the partitioning\nmethod more practically appealing for conference peer review. Finally, we\ncomplement our positive results with negative theoretical results where we\nprove that under various ways of strengthening the requirements, it is\nimpossible for any algorithm to be strategyproof and efficient. \n\n"}
{"id": "1806.07129", "contents": "Title: Instance-Level Explanations for Fraud Detection: A Case Study Abstract: Fraud detection is a difficult problem that can benefit from predictive\nmodeling. However, the verification of a prediction is challenging; for a\nsingle insurance policy, the model only provides a prediction score. We present\na case study where we reflect on different instance-level model explanation\ntechniques to aid a fraud detection team in their work. To this end, we\ndesigned two novel dashboards combining various state-of-the-art explanation\ntechniques. These enable the domain expert to analyze and understand\npredictions, dramatically speeding up the process of filtering potential fraud\ncases. Finally, we discuss the lessons learned and outline open research\nissues. \n\n"}
{"id": "1806.07268", "contents": "Title: Beyond Local Nash Equilibria for Adversarial Networks Abstract: Save for some special cases, current training methods for Generative\nAdversarial Networks (GANs) are at best guaranteed to converge to a `local Nash\nequilibrium` (LNE). Such LNEs, however, can be arbitrarily far from an actual\nNash equilibrium (NE), which implies that there are no guarantees on the\nquality of the found generator or classifier. This paper proposes to model GANs\nexplicitly as finite games in mixed strategies, thereby ensuring that every LNE\nis an NE. With this formulation, we propose a solution method that is proven to\nmonotonically converge to a resource-bounded Nash equilibrium (RB-NE): by\nincreasing computational resources we can find better solutions. We empirically\ndemonstrate that our method is less prone to typical GAN problems such as mode\ncollapse, and produces solutions that are less exploitable than those produced\nby GANs and MGANs, and closely resemble theoretical predictions about NEs. \n\n"}
{"id": "1806.07343", "contents": "Title: Quantum Nash equilibrium in the thermodynamic limit Abstract: The quantum Nash equilibrium in the thermodynamic limit is studied for games\nlike quantum Prisoner's dilemma and the quantum game of chicken. A phase\ntransition is seen in both games as a function of the entanglement in the game.\nWe observe that for maximal entanglement irrespective of the classical payoffs,\na majority of players choose Quantum strategy over Defect in the thermodynamic\nlimit. \n\n"}
{"id": "1806.07811", "contents": "Title: Stochastic Nested Variance Reduction for Nonconvex Optimization Abstract: We study finite-sum nonconvex optimization problems, where the objective\nfunction is an average of $n$ nonconvex functions. We propose a new stochastic\ngradient descent algorithm based on nested variance reduction. Compared with\nconventional stochastic variance reduced gradient (SVRG) algorithm that uses\ntwo reference points to construct a semi-stochastic gradient with diminishing\nvariance in each iteration, our algorithm uses $K+1$ nested reference points to\nbuild a semi-stochastic gradient to further reduce its variance in each\niteration. For smooth nonconvex functions, the proposed algorithm converges to\nan $\\epsilon$-approximate first-order stationary point (i.e., $\\|\\nabla\nF(\\mathbf{x})\\|_2\\leq \\epsilon$) within $\\tilde O(n\\land\n\\epsilon^{-2}+\\epsilon^{-3}\\land n^{1/2}\\epsilon^{-2})$ number of stochastic\ngradient evaluations. This improves the best known gradient complexity of SVRG\n$O(n+n^{2/3}\\epsilon^{-2})$ and that of SCSG $O(n\\land\n\\epsilon^{-2}+\\epsilon^{-10/3}\\land n^{2/3}\\epsilon^{-2})$. For gradient\ndominated functions, our algorithm also achieves better gradient complexity\nthan the state-of-the-art algorithms. Thorough experimental results on\ndifferent nonconvex optimization problems back up our theory. \n\n"}
{"id": "1806.08340", "contents": "Title: Interpretable Discovery in Large Image Data Sets Abstract: Automated detection of new, interesting, unusual, or anomalous images within\nlarge data sets has great value for applications from surveillance (e.g.,\nairport security) to science (observations that don't fit a given theory can\nlead to new discoveries). Many image data analysis systems are turning to\nconvolutional neural networks (CNNs) to represent image content due to their\nsuccess in achieving high classification accuracy rates. However, CNN\nrepresentations are notoriously difficult for humans to interpret. We describe\na new strategy that combines novelty detection with CNN image features to\nachieve rapid discovery with interpretable explanations of novel image content.\nWe applied this technique to familiar images from ImageNet as well as to a\nscientific image collection from planetary science. \n\n"}
{"id": "1806.09605", "contents": "Title: Many-Goals Reinforcement Learning Abstract: All-goals updating exploits the off-policy nature of Q-learning to update all\npossible goals an agent could have from each transition in the world, and was\nintroduced into Reinforcement Learning (RL) by Kaelbling (1993). In prior work\nthis was mostly explored in small-state RL problems that allowed tabular\nrepresentations and where all possible goals could be explicitly enumerated and\nlearned separately. In this paper we empirically explore 3 different extensions\nof the idea of updating many (instead of all) goals in the context of RL with\ndeep neural networks (or DeepRL for short). First, in a direct adaptation of\nKaelbling's approach we explore if many-goals updating can be used to achieve\nmastery in non-tabular visual-observation domains. Second, we explore whether\nmany-goals updating can be used to pre-train a network to subsequently learn\nfaster and better on a single main task of interest. Third, we explore whether\nmany-goals updating can be used to provide auxiliary task updates in training a\nnetwork to learn faster and better on a single main task of interest. We\nprovide comparisons to baselines for each of the 3 extensions. \n\n"}
{"id": "1806.09777", "contents": "Title: On the Implicit Bias of Dropout Abstract: Algorithmic approaches endow deep learning systems with implicit bias that\nhelps them generalize even in over-parametrized settings. In this paper, we\nfocus on understanding such a bias induced in learning through dropout, a\npopular technique to avoid overfitting in deep learning. For single\nhidden-layer linear neural networks, we show that dropout tends to make the\nnorm of incoming/outgoing weight vectors of all the hidden nodes equal. In\naddition, we provide a complete characterization of the optimization landscape\ninduced by dropout. \n\n"}
{"id": "1806.10019", "contents": "Title: Adversarial Active Exploration for Inverse Dynamics Model Learning Abstract: We present an adversarial active exploration for inverse dynamics model\nlearning, a simple yet effective learning scheme that incentivizes exploration\nin an environment without any human intervention. Our framework consists of a\ndeep reinforcement learning (DRL) agent and an inverse dynamics model\ncontesting with each other. The former collects training samples for the\nlatter, with an objective to maximize the error of the latter. The latter is\ntrained with samples collected by the former, and generates rewards for the\nformer when it fails to predict the actual action taken by the former. In such\na competitive setting, the DRL agent learns to generate samples that the\ninverse dynamics model fails to predict correctly, while the inverse dynamics\nmodel learns to adapt to the challenging samples. We further propose a reward\nstructure that ensures the DRL agent to collect only moderately hard samples\nbut not overly hard ones that prevent the inverse model from predicting\neffectively. We evaluate the effectiveness of our method on several robotic arm\nand hand manipulation tasks against multiple baseline models. Experimental\nresults show that our method is comparable to those directly trained with\nexpert demonstrations, and superior to the other baselines even without any\nhuman priors. \n\n"}
{"id": "1806.10071", "contents": "Title: Learning Existing Social Conventions via Observationally Augmented\n  Self-Play Abstract: In order for artificial agents to coordinate effectively with people, they\nmust act consistently with existing conventions (e.g. how to navigate in\ntraffic, which language to speak, or how to coordinate with teammates). A\ngroup's conventions can be viewed as a choice of equilibrium in a coordination\ngame. We consider the problem of an agent learning a policy for a coordination\ngame in a simulated environment and then using this policy when it enters an\nexisting group. When there are multiple possible conventions we show that\nlearning a policy via multi-agent reinforcement learning (MARL) is likely to\nfind policies which achieve high payoffs at training time but fail to\ncoordinate with the real group into which the agent enters. We assume access to\na small number of samples of behavior from the true convention and show that we\ncan augment the MARL objective to help it find policies consistent with the\nreal group's convention. In three environments from the literature - traffic,\ncommunication, and team coordination - we observe that augmenting MARL with a\nsmall amount of imitation learning greatly increases the probability that the\nstrategy found by MARL fits well with the existing social convention. We show\nthat this works even in an environment where standard training methods very\nrarely find the true convention of the agent's partners. \n\n"}
{"id": "1806.10270", "contents": "Title: Optimal Piecewise Local-Linear Approximations Abstract: Existing works on \"black-box\" model interpretation use local-linear\napproximations to explain the predictions made for each data instance in terms\nof the importance assigned to the different features for arriving at the\nprediction. These works provide instancewise explanations and thus give a local\nview of the model. To be able to trust the model it is important to understand\nthe global model behavior and there are relatively fewer works which do the\nsame. Piecewise local-linear models provide a natural way to extend\nlocal-linear models to explain the global behavior of the model. In this work,\nwe provide a dynamic programming based framework to obtain piecewise\napproximations of the black-box model. We also provide provable fidelity, i.e.,\nhow well the explanations reflect the black-box model, guarantees. We carry out\nsimulations on synthetic and real datasets to show the utility of the proposed\napproach. At the end, we show that the ideas developed for our framework can\nalso be used to address the problem of clustering for one-dimensional data. We\ngive a polynomial time algorithm and prove that it achieves optimal clustering. \n\n"}
{"id": "1806.10729", "contents": "Title: Illuminating Generalization in Deep Reinforcement Learning through\n  Procedural Level Generation Abstract: Deep reinforcement learning (RL) has shown impressive results in a variety of\ndomains, learning directly from high-dimensional sensory streams. However, when\nneural networks are trained in a fixed environment, such as a single level in a\nvideo game, they will usually overfit and fail to generalize to new levels.\nWhen RL models overfit, even slight modifications to the environment can result\nin poor agent performance. This paper explores how procedurally generated\nlevels during training can increase generality. We show that for some games\nprocedural level generation enables generalization to new levels within the\nsame distribution. Additionally, it is possible to achieve better performance\nwith less data by manipulating the difficulty of the levels in response to the\nperformance of the agent. The generality of the learned behaviors is also\nevaluated on a set of human-designed levels. The results suggest that the\nability to generalize to human-designed levels highly depends on the design of\nthe level generators. We apply dimensionality reduction and clustering\ntechniques to visualize the generators' distributions of levels and analyze to\nwhat degree they can produce levels similar to those designed by a human. \n\n"}
{"id": "1806.10758", "contents": "Title: A Benchmark for Interpretability Methods in Deep Neural Networks Abstract: We propose an empirical measure of the approximate accuracy of feature\nimportance estimates in deep neural networks. Our results across several\nlarge-scale image classification datasets show that many popular\ninterpretability methods produce estimates of feature importance that are not\nbetter than a random designation of feature importance. Only certain ensemble\nbased approaches---VarGrad and SmoothGrad-Squared---outperform such a random\nassignment of importance. The manner of ensembling remains critical, we show\nthat some approaches do no better then the underlying method but carry a far\nhigher computational burden. \n\n"}
{"id": "1806.10952", "contents": "Title: Amortized Analysis of Asynchronous Price Dynamics Abstract: We extend a recently developed framework for analyzing asynchronous\ncoordinate descent algorithms to show that an asynchronous version of\ntatonnement, a fundamental price dynamic widely studied in general equilibrium\ntheory, converges toward a market equilibrium for Fisher markets with CES\nutilities or Leontief utilities, for which tatonnement is equivalent to\ncoordinate descent. \n\n"}
{"id": "1807.01389", "contents": "Title: Efficient Rational Proofs with Strong Utility-Gap Guarantees Abstract: As modern computing moves towards smaller devices and powerful cloud\nplatforms, more and more computation is being delegated to powerful service\nproviders. Interactive proofs are a widely-used model to design efficient\nprotocols for verifiable computation delegation. Rational proofs are\npayment-based interactive proofs. The payments are designed to incentivize the\nprovers to give correct answers. If the provers misreport the answer then they\nincur a payment loss of at least 1/u, where u is the utility gap of the\nprotocol.\n  In this work, we tightly characterize the power of rational proofs that are\nsuper efficient, that is, require only logarithmic time and communication for\nverification. We also characterize the power of single-round rational protocols\nthat require only logarithmic space and randomness for verification. Our\nprotocols have strong (that is, polynomial, logarithmic, and even constant)\nutility gap. Finally, we show when and how rational protocols can be converted\nto give the completeness and soundness guarantees of classical interactive\nproofs. \n\n"}
{"id": "1807.01659", "contents": "Title: MIXGAN: Learning Concepts from Different Domains for Mixture Generation Abstract: In this work, we present an interesting attempt on mixture generation:\nabsorbing different image concepts (e.g., content and style) from different\ndomains and thus generating a new domain with learned concepts. In particular,\nwe propose a mixture generative adversarial network (MIXGAN). MIXGAN learns\nconcepts of content and style from two domains respectively, and thus can join\nthem for mixture generation in a new domain, i.e., generating images with\ncontent from one domain and style from another. MIXGAN overcomes the limitation\nof current GAN-based models which either generate new images in the same domain\nas they observed in training stage, or require off-the-shelf content templates\nfor transferring or translation. Extensive experimental results demonstrate the\neffectiveness of MIXGAN as compared to related state-of-the-art GAN-based\nmodels. \n\n"}
{"id": "1807.01672", "contents": "Title: Ranked Reward: Enabling Self-Play Reinforcement Learning for\n  Combinatorial Optimization Abstract: Adversarial self-play in two-player games has delivered impressive results\nwhen used with reinforcement learning algorithms that combine deep neural\nnetworks and tree search. Algorithms like AlphaZero and Expert Iteration learn\ntabula-rasa, producing highly informative training data on the fly. However,\nthe self-play training strategy is not directly applicable to single-player\ngames. Recently, several practically important combinatorial optimisation\nproblems, such as the travelling salesman problem and the bin packing problem,\nhave been reformulated as reinforcement learning problems, increasing the\nimportance of enabling the benefits of self-play beyond two-player games. We\npresent the Ranked Reward (R2) algorithm which accomplishes this by ranking the\nrewards obtained by a single agent over multiple games to create a relative\nperformance metric. Results from applying the R2 algorithm to instances of a\ntwo-dimensional and three-dimensional bin packing problems show that it\noutperforms generic Monte Carlo tree search, heuristic algorithms and integer\nprogramming solvers. We also present an analysis of the ranked reward\nmechanism, in particular, the effects of problem instances with varying\ndifficulty and different ranking thresholds. \n\n"}
{"id": "1807.01675", "contents": "Title: Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value\n  Expansion Abstract: Integrating model-free and model-based approaches in reinforcement learning\nhas the potential to achieve the high performance of model-free algorithms with\nlow sample complexity. However, this is difficult because an imperfect dynamics\nmodel can degrade the performance of the learning algorithm, and in\nsufficiently complex environments, the dynamics model will almost always be\nimperfect. As a result, a key challenge is to combine model-based approaches\nwith model-free learning in such a way that errors in the model do not degrade\nperformance. We propose stochastic ensemble value expansion (STEVE), a novel\nmodel-based technique that addresses this issue. By dynamically interpolating\nbetween model rollouts of various horizon lengths for each individual example,\nSTEVE ensures that the model is only utilized when doing so does not introduce\nsignificant errors. Our approach outperforms model-free baselines on\nchallenging continuous control benchmarks with an order-of-magnitude increase\nin sample efficiency, and in contrast to previous model-based approaches,\nperformance does not degrade in complex environments. \n\n"}
{"id": "1807.01736", "contents": "Title: Transfer with Model Features in Reinforcement Learning Abstract: A key question in Reinforcement Learning is which representation an agent can\nlearn to efficiently reuse knowledge between different tasks. Recently the\nSuccessor Representation was shown to have empirical benefits for transferring\nknowledge between tasks with shared transition dynamics. This paper presents\nModel Features: a feature representation that clusters behaviourally equivalent\nstates and that is equivalent to a Model-Reduction. Further, we present a\nSuccessor Feature model which shows that learning Successor Features is\nequivalent to learning a Model-Reduction. A novel optimization objective is\ndeveloped and we provide bounds showing that minimizing this objective results\nin an increasingly improved approximation of a Model-Reduction. Further, we\nprovide transfer experiments on randomly generated MDPs which vary in their\ntransition and reward functions but approximately preserve behavioural\nequivalence between states. These results demonstrate that Model Features are\nsuitable for transfer between tasks with varying transition and reward\nfunctions. \n\n"}
{"id": "1807.01798", "contents": "Title: Regularizing Autoencoder-Based Matrix Completion Models via Manifold\n  Learning Abstract: Autoencoders are popular among neural-network-based matrix completion models\ndue to their ability to retrieve potential latent factors from the partially\nobserved matrices. Nevertheless, when training data is scarce their performance\nis significantly degraded due to overfitting. In this paper, we mit- igate\noverfitting with a data-dependent regularization technique that relies on the\nprinciples of multi-task learning. Specifically, we propose an\nautoencoder-based matrix completion model that performs prediction of the\nunknown matrix values as a main task, and manifold learning as an auxiliary\ntask. The latter acts as an inductive bias, leading to solutions that\ngeneralize better. The proposed model outperforms the existing\nautoencoder-based models designed for matrix completion, achieving high\nreconstruction accuracy in well-known datasets. \n\n"}
{"id": "1807.01830", "contents": "Title: Per-decision Multi-step Temporal Difference Learning with Control\n  Variates Abstract: Multi-step temporal difference (TD) learning is an important approach in\nreinforcement learning, as it unifies one-step TD learning with Monte Carlo\nmethods in a way where intermediate algorithms can outperform either extreme.\nThey address a bias-variance trade off between reliance on current estimates,\nwhich could be poor, and incorporating longer sampled reward sequences into the\nupdates. Especially in the off-policy setting, where the agent aims to learn\nabout a policy different from the one generating its behaviour, the variance in\nthe updates can cause learning to diverge as the number of sampled rewards used\nin the estimates increases. In this paper, we introduce per-decision control\nvariates for multi-step TD algorithms, and compare them to existing methods.\nOur results show that including the control variates can greatly improve\nperformance on both on and off-policy multi-step temporal difference learning\ntasks. \n\n"}
{"id": "1807.02037", "contents": "Title: TFLMS: Large Model Support in TensorFlow by Graph Rewriting Abstract: While accelerators such as GPUs have limited memory, deep neural networks are\nbecoming larger and will not fit with the memory limitation of accelerators for\ntraining. We propose an approach to tackle this problem by rewriting the\ncomputational graph of a neural network, in which swap-out and swap-in\noperations are inserted to temporarily store intermediate results on CPU\nmemory. In particular, we first revise the concept of a computational graph by\ndefining a concrete semantics for variables in a graph. We then formally show\nhow to derive swap-out and swap-in operations from an existing graph and\npresent rules to optimize the graph. To realize our approach, we developed a\nmodule in TensorFlow, named TFLMS. TFLMS is published as a pull request in the\nTensorFlow repository for contributing to the TensorFlow community. With TFLMS,\nwe were able to train ResNet-50 and 3DUnet with 4.7x and 2x larger batch size,\nrespectively. In particular, we were able to train 3DUNet using images of size\nof $192^3$ for image segmentation, which, without TFLMS, had been done only by\ndividing the images to smaller images, which affects the accuracy. \n\n"}
{"id": "1807.02963", "contents": "Title: Jointly learning relevant subgraph patterns and nonlinear models of\n  their indicators Abstract: Classification and regression in which the inputs are graphs of arbitrary\nsize and shape have been paid attention in various fields such as computational\nchemistry and bioinformatics. Subgraph indicators are often used as the most\nfundamental features, but the number of possible subgraph patterns are\nintractably large due to the combinatorial explosion. We propose a novel\nefficient algorithm to jointly learn relevant subgraph patterns and nonlinear\nmodels of their indicators. Previous methods for such joint learning of\nsubgraph features and models are based on search for single best subgraph\nfeatures with specific pruning and boosting procedures of adding their\nindicators one by one, which result in linear models of subgraph indicators. In\ncontrast, the proposed approach is based on directly learning regression trees\nfor graph inputs using a newly derived bound of the total sum of squares for\ndata partitions by a given subgraph feature, and thus can learn nonlinear\nmodels through standard gradient boosting. An illustrative example we call the\nGraph-XOR problem to consider nonlinearity, numerical experiments with real\ndatasets, and scalability comparisons to naive approaches using explicit\npattern enumeration are also presented. \n\n"}
{"id": "1807.03571", "contents": "Title: A Game-Based Approximate Verification of Deep Neural Networks with\n  Provable Guarantees Abstract: Despite the improved accuracy of deep neural networks, the discovery of\nadversarial examples has raised serious safety concerns. In this paper, we\nstudy two variants of pointwise robustness, the maximum safe radius problem,\nwhich for a given input sample computes the minimum distance to an adversarial\nexample, and the feature robustness problem, which aims to quantify the\nrobustness of individual features to adversarial perturbations. We demonstrate\nthat, under the assumption of Lipschitz continuity, both problems can be\napproximated using finite optimisation by discretising the input space, and the\napproximation has provable guarantees, i.e., the error is bounded. We then show\nthat the resulting optimisation problems can be reduced to the solution of\ntwo-player turn-based games, where the first player selects features and the\nsecond perturbs the image within the feature. While the second player aims to\nminimise the distance to an adversarial example, depending on the optimisation\nobjective the first player can be cooperative or competitive. We employ an\nanytime approach to solve the games, in the sense of approximating the value of\na game by monotonically improving its upper and lower bounds. The Monte Carlo\ntree search algorithm is applied to compute upper bounds for both games, and\nthe Admissible A* and the Alpha-Beta Pruning algorithms are, respectively, used\nto compute lower bounds for the maximum safety radius and feature robustness\ngames. When working on the upper bound of the maximum safe radius problem, our\ntool demonstrates competitive performance against existing adversarial example\ncrafting algorithms. Furthermore, we show how our framework can be deployed to\nevaluate pointwise robustness of neural networks in safety-critical\napplications such as traffic sign recognition in self-driving cars. \n\n"}
{"id": "1807.04457", "contents": "Title: Query-Efficient Hard-label Black-box Attack:An Optimization-based\n  Approach Abstract: We study the problem of attacking a machine learning model in the hard-label\nblack-box setting, where no model information is revealed except that the\nattacker can make queries to probe the corresponding hard-label decisions. This\nis a very challenging problem since the direct extension of state-of-the-art\nwhite-box attacks (e.g., CW or PGD) to the hard-label black-box setting will\nrequire minimizing a non-continuous step function, which is combinatorial and\ncannot be solved by a gradient-based optimizer. The only current approach is\nbased on random walk on the boundary, which requires lots of queries and lacks\nconvergence guarantees. We propose a novel way to formulate the hard-label\nblack-box attack as a real-valued optimization problem which is usually\ncontinuous and can be solved by any zeroth order optimization algorithm. For\nexample, using the Randomized Gradient-Free method, we are able to bound the\nnumber of iterations needed for our algorithm to achieve stationary points. We\ndemonstrate that our proposed method outperforms the previous random walk\napproach to attacking convolutional neural networks on MNIST, CIFAR, and\nImageNet datasets. More interestingly, we show that the proposed algorithm can\nalso be used to attack other discrete and non-continuous machine learning\nmodels, such as Gradient Boosting Decision Trees (GBDT). \n\n"}
{"id": "1807.05306", "contents": "Title: Generative Adversarial Privacy Abstract: We present a data-driven framework called generative adversarial privacy\n(GAP). Inspired by recent advancements in generative adversarial networks\n(GANs), GAP allows the data holder to learn the privatization mechanism\ndirectly from the data. Under GAP, finding the optimal privacy mechanism is\nformulated as a constrained minimax game between a privatizer and an adversary.\nWe show that for appropriately chosen adversarial loss functions, GAP provides\nprivacy guarantees against strong information-theoretic adversaries. We also\nevaluate GAP's performance on the GENKI face database. \n\n"}
{"id": "1807.05328", "contents": "Title: On the Acceleration of L-BFGS with Second-Order Information and\n  Stochastic Batches Abstract: This paper proposes a framework of L-BFGS based on the (approximate)\nsecond-order information with stochastic batches, as a novel approach to the\nfinite-sum minimization problems. Different from the classical L-BFGS where\nstochastic batches lead to instability, we use a smooth estimate for the\nevaluations of the gradient differences while achieving acceleration by\nwell-scaling the initial Hessians. We provide theoretical analyses for both\nconvex and nonconvex cases. In addition, we demonstrate that within the popular\napplications of least-square and cross-entropy losses, the algorithm admits a\nsimple implementation in the distributed environment. Numerical experiments\nsupport the efficiency of our algorithms. \n\n"}
{"id": "1807.05779", "contents": "Title: Potential Games Design Using Local Information Abstract: Consider a multiplayer game, and assume a system level objective function,\nwhich the system wants to optimize, is given. This paper aims at accomplishing\nthis goal via potential game theory when players can only get part of other\nplayers' information. The technique is designing a set of local information\nbased utility functions, which guarantee that the designed game is potential,\nwith the system level objective function its potential function. First, the\nexistence of local information based utility functions can be verified by\nchecking whether the corresponding linear equations have a solution. Then an\nalgorithm is proposed to calculate the local information based utility\nfunctions when the utility design equations have solutions. Finally, consensus\nproblem of multiagent system is considered to demonstrate the effectiveness of\nthe proposed design procedure. \n\n"}
{"id": "1807.06046", "contents": "Title: Zap: Making Predictions Based on Online User Behavior Abstract: This paper introduces Zap, a generic machine learning pipeline for making\npredictions based on online user behavior. Zap combines well known techniques\nfor processing sequential data with more obscure techniques such as Bloom\nfilters, bucketing, and model calibration into an end-to-end solution. The\npipeline creates website- and task-specific models without knowing anything\nabout the structure of the website. It is designed to minimize the amount of\nwebsite-specific code, which is realized by factoring all website-specific\nlogic into example generators. New example generators can typically be written\nup in a few lines of code. \n\n"}
{"id": "1807.06666", "contents": "Title: Payoff Control in the Iterated Prisoner's Dilemma Abstract: Repeated game has long been the touchstone model for agents' long-run\nrelationships. Previous results suggest that it is particularly difficult for a\nrepeated game player to exert an autocratic control on the payoffs since they\nare jointly determined by all participants. This work discovers that the scale\nof a player's capability to unilaterally influence the payoffs may have been\nmuch underestimated. Under the conventional iterated prisoner's dilemma, we\ndevelop a general framework for controlling the feasible region where the\nplayers' payoff pairs lie. A control strategy player is able to confine the\npayoff pairs in her objective region, as long as this region has feasible\nlinear boundaries. With this framework, many well-known existing strategies can\nbe categorized and various new strategies with nice properties can be further\nidentified. We show that the control strategies perform well either in a\ntournament or against a human-like opponent. \n\n"}
{"id": "1807.06763", "contents": "Title: General Value Function Networks Abstract: State construction is important for learning in partially observable\nenvironments. A general purpose strategy for state construction is to learn the\nstate update using a Recurrent Neural Network (RNN), which updates the internal\nstate using the current internal state and the most recent observation. This\ninternal state provides a summary of the observed sequence, to facilitate\naccurate predictions and decision-making. At the same time, specifying and\ntraining RNNs is notoriously tricky, particularly as the common strategy to\napproximate gradients back in time, called truncated Back-prop Through Time\n(BPTT), can be sensitive to the truncation window. Further,\ndomain-expertise--which can usually help constrain the function class and so\nimprove trainability--can be difficult to incorporate into complex recurrent\nunits used within RNNs. In this work, we explore how to use multi-step\npredictions to constrain the RNN and incorporate prior knowledge. In\nparticular, we revisit the idea of using predictions to construct state and\nask: does constraining (parts of) the state to consist of predictions about the\nfuture improve RNN trainability? We formulate a novel RNN architecture, called\na General Value Function Network (GVFN), where each internal state component\ncorresponds to a prediction about the future represented as a value function.\nWe first provide an objective for optimizing GVFNs, and derive several\nalgorithms to optimize this objective. We then show that GVFNs are more robust\nto the truncation level, in many cases only requiring one-step gradient\nupdates. \n\n"}
{"id": "1807.07530", "contents": "Title: Self-Organizing Maps as a Storage and Transfer Mechanism in\n  Reinforcement Learning Abstract: The idea of reusing information from previously learned tasks (source tasks)\nfor the learning of new tasks (target tasks) has the potential to significantly\nimprove the sample efficiency reinforcement learning agents. In this work, we\ndescribe an approach to concisely store and represent learned task knowledge,\nand reuse it by allowing it to guide the exploration of an agent while it\nlearns new tasks. In order to do so, we use a measure of similarity that is\ndefined directly in the space of parameterized representations of the value\nfunctions. This similarity measure is also used as a basis for a variant of the\ngrowing self-organizing map algorithm, which is simultaneously used to enable\nthe storage of previously acquired task knowledge in an adaptive and scalable\nmanner.We empirically validate our approach in a simulated navigation\nenvironment and discuss possible extensions to this approach along with\npotential applications where it could be particularly useful. \n\n"}
{"id": "1807.08140", "contents": "Title: On the Analysis of Trajectories of Gradient Descent in the Optimization\n  of Deep Neural Networks Abstract: Theoretical analysis of the error landscape of deep neural networks has\ngarnered significant interest in recent years. In this work, we theoretically\nstudy the importance of noise in the trajectories of gradient descent towards\noptimal solutions in multi-layer neural networks. We show that adding noise (in\ndifferent ways) to a neural network while training increases the rank of the\nproduct of weight matrices of a multi-layer linear neural network. We thus\nstudy how adding noise can assist reaching a global optimum when the product\nmatrix is full-rank (under certain conditions). We establish theoretical\nfoundations between the noise induced into the neural network - either to the\ngradient, to the architecture, or to the input/output to a neural network - and\nthe rank of product of weight matrices. We corroborate our theoretical findings\nwith empirical results. \n\n"}
{"id": "1807.08169", "contents": "Title: Recent Advances in Deep Learning: An Overview Abstract: Deep Learning is one of the newest trends in Machine Learning and Artificial\nIntelligence research. It is also one of the most popular scientific research\ntrends now-a-days. Deep learning methods have brought revolutionary advances in\ncomputer vision and machine learning. Every now and then, new and new deep\nlearning techniques are being born, outperforming state-of-the-art machine\nlearning and even existing deep learning techniques. In recent years, the world\nhas seen many major breakthroughs in this field. Since deep learning is\nevolving at a huge speed, its kind of hard to keep track of the regular\nadvances especially for new researchers. In this paper, we are going to briefly\ndiscuss about recent advances in Deep Learning for past few years. \n\n"}
{"id": "1807.08253", "contents": "Title: Competitive Equilibria in Combinatorial Exchanges with Financially\n  Constrained Buyers:Computational Hardness and Algorithmic Solutions Abstract: Advances in computational optimization allow for the organization of large\ncombinatorial markets. We aim for allocations and competitive equilibrium\nprices, i.e. outcomes that are in the core. The research is motivated by the\ndesign of environmental markets, but similar problems appear in energy and\nlogistics markets or in the allocation of airport time slots. Budget\nconstraints are an important concern in many of these markets. While the\nallocation problem in combinatorial exchanges is already NP-hard with payoff-\nmaximizing bidders, we find that the allocation and pricing problem becomes\neven $\\Sigma_2^p$-hard if buyers are financially constrained. We introduce\nmixed integer bilevel linear programs (MIBLP) to compute core prices, and\npropose pricing functions based on the least core if the core is empty. We also\ndiscuss restricted but simpler cases and effective computational techniques for\nthe problem. In numerical experiments we show that in spite of the\ncomputational hardness of these problems, we can hope to solve practical\nproblem sizes, in particular if we restrict the size of the coalitions\nconsidered in the core computations. \n\n"}
{"id": "1807.08919", "contents": "Title: The Variational Homoencoder: Learning to learn high capacity generative\n  models from few examples Abstract: Hierarchical Bayesian methods can unify many related tasks (e.g. k-shot\nclassification, conditional and unconditional generation) as inference within a\nsingle generative model. However, when this generative model is expressed as a\npowerful neural network such as a PixelCNN, we show that existing learning\ntechniques typically fail to effectively use latent variables. To address this,\nwe develop a modification of the Variational Autoencoder in which encoded\nobservations are decoded to new elements from the same class. This technique,\nwhich we call a Variational Homoencoder (VHE), produces a hierarchical latent\nvariable model which better utilises latent variables. We use the VHE framework\nto learn a hierarchical PixelCNN on the Omniglot dataset, which outperforms all\nexisting models on test set likelihood and achieves strong performance on\none-shot generation and classification tasks. We additionally validate the VHE\non natural images from the YouTube Faces database. Finally, we develop\nextensions of the model that apply to richer dataset structures such as\nfactorial and hierarchical categories. \n\n"}
{"id": "1807.09386", "contents": "Title: On the Randomized Complexity of Minimizing a Convex Quadratic Function Abstract: Minimizing a convex, quadratic objective of the form\n$f_{\\mathbf{A},\\mathbf{b}}(x) := \\frac{1}{2}x^\\top \\mathbf{A} x - \\langle\n\\mathbf{b}, x \\rangle$ for $\\mathbf{A} \\succ 0 $ is a fundamental problem in\nmachine learning and optimization. In this work, we prove gradient-query\ncomplexity lower bounds for minimizing convex quadratic functions which apply\nto both deterministic and \\emph{randomized} algorithms. Specifically, for\n$\\kappa > 1$, we exhibit a distribution over $(\\mathbf{A},\\mathbf{b})$ with\ncondition number $\\mathrm{cond}(\\mathbf{A}) \\le \\kappa$, such that any\n\\emph{randomized} algorithm requires $\\Omega(\\sqrt{\\kappa})$ gradient queries\nto find a solution $\\hat x$ for which $\\|\\hat x - \\mathbf x_\\star\\| \\le\n\\epsilon_0\\|\\mathbf{x}_{\\star}\\|$, where $\\mathbf x_{\\star} =\n\\mathbf{A}^{-1}\\mathbf{b}$ is the optimal solution, and $\\epsilon_0$ a small\nconstant. Setting $\\kappa =1/\\epsilon$, this lower bound implies the minimax\nrate of $T = \\Omega(\\lambda_1(\\mathbf{A})\\|\\mathbf\nx_\\star\\|^2/\\sqrt{\\epsilon})$ queries required to minimize an arbitrary convex\nquadratic function up to error $f(\\hat{x}) - f(\\mathbf x_\\star) \\le \\epsilon$.\n  Our lower bound holds for a distribution derived from classical ensembles in\nrandom matrix theory, and relies on a careful reduction from adaptively\nestimating a planted vector $\\mathbf u$ in a deformed Wigner model. A key step\nin deriving sharp lower bounds is demonstrating that the optimization error\n$\\mathbf x_\\star - \\hat x$ cannot align too closely with $\\mathbf{u}$. To this\nend, we prove an upper bound on the cosine between $\\mathbf x_\\star - \\hat x$\nand $\\mathbf u$ in terms of the MMSE of estimating the plant $\\mathbf u$ in a\ndeformed Wigner model. We then bound the MMSE by carefully modifying a result\ndue to Lelarge and Miolane 2016, which rigorously establishes a general\nreplica-symmetric formula for planted matrix models. \n\n"}
{"id": "1807.09511", "contents": "Title: Backprop-Q: Generalized Backpropagation for Stochastic Computation\n  Graphs Abstract: In real-world scenarios, it is appealing to learn a model carrying out\nstochastic operations internally, known as stochastic computation graphs\n(SCGs), rather than learning a deterministic mapping. However, standard\nbackpropagation is not applicable to SCGs. We attempt to address this issue\nfrom the angle of cost propagation, with local surrogate costs, called\nQ-functions, constructed and learned for each stochastic node in an SCG. Then,\nthe SCG can be trained based on these surrogate costs using standard\nbackpropagation. We propose the entire framework as a solution to generalize\nbackpropagation for SCGs, which resembles an actor-critic architecture but\nbased on a graph. For broad applicability, we study a variety of SCG structures\nfrom one cost to multiple costs. We utilize recent advances in reinforcement\nlearning (RL) and variational Bayes (VB), such as off-policy critic learning\nand unbiased-and-low-variance gradient estimation, and review them in the\ncontext of SCGs. The generalized backpropagation extends transported learning\nsignals beyond gradients between stochastic nodes while preserving the benefit\nof backpropagating gradients through deterministic nodes. Experimental\nsuggestions and concerns are listed to help design and test any specific model\nusing this framework. \n\n"}
{"id": "1807.10117", "contents": "Title: Effectiveness of Scaled Exponentially-Regularized Linear Units (SERLUs) Abstract: Recently, self-normalizing neural networks (SNNs) have been proposed with the\nintention to avoid batch or weight normalization. The key step in SNNs is to\nproperly scale the exponential linear unit (referred to as SELU) to inherently\nincorporate normalization based on central limit theory. SELU is a\nmonotonically increasing function, where it has an approximately constant\nnegative output for large negative input. In this work, we propose a new\nactivation function to break the monotonicity property of SELU while still\npreserving the self-normalizing property. Differently from SELU, the new\nfunction introduces a bump-shaped function in the region of negative input by\nregularizing a linear function with a scaled exponential function, which is\nreferred to as a scaled exponentially-regularized linear unit (SERLU). The\nbump-shaped function has approximately zero response to large negative input\nwhile being able to push the output of SERLU towards zero mean statistically.\nTo effectively combat over-fitting, we develop a so-called shift-dropout for\nSERLU, which includes standard dropout as a special case. Experimental results\non MNIST, CIFAR10 and CIFAR100 show that SERLU-based neural networks provide\nconsistently promising results in comparison to other 5 activation functions\nincluding ELU, SELU, Swish, Leakly ReLU and ReLU. \n\n"}
{"id": "1807.10454", "contents": "Title: Rob-GAN: Generator, Discriminator, and Adversarial Attacker Abstract: We study two important concepts in adversarial deep learning---adversarial\ntraining and generative adversarial network (GAN). Adversarial training is the\ntechnique used to improve the robustness of discriminator by combining\nadversarial attacker and discriminator in the training phase. GAN is commonly\nused for image generation by jointly optimizing discriminator and generator. We\nshow these two concepts are indeed closely related and can be used to\nstrengthen each other---adding a generator to the adversarial training\nprocedure can improve the robustness of discriminators, and adding an\nadversarial attack to GAN training can improve the convergence speed and lead\nto better generators. Combining these two insights, we develop a framework\ncalled Rob-GAN to jointly optimize generator and discriminator in the presence\nof adversarial attacks---the generator generates fake images to fool\ndiscriminator; the adversarial attacker perturbs real images to fool the\ndiscriminator, and the discriminator wants to minimize loss under fake and\nadversarial images. Through this end-to-end training procedure, we are able to\nsimultaneously improve the convergence speed of GAN training, the quality of\nsynthetic images, and the robustness of discriminator under strong adversarial\nattacks. Experimental results demonstrate that the obtained classifier is more\nrobust than the state-of-the-art adversarial training approach, and the\ngenerator outperforms SN-GAN on ImageNet-143. \n\n"}
{"id": "1807.10836", "contents": "Title: Markets for Public Decision-making Abstract: A public decision-making problem consists of a set of issues, each with\nmultiple possible alternatives, and a set of competing agents, each with a\npreferred alternative for each issue. We study adaptations of market economies\nto this setting, focusing on binary issues. Issues have prices, and each agent\nis endowed with artificial currency that she can use to purchase probability\nfor her preferred alternatives (we allow randomized outcomes). We first show\nthat when each issue has a single price that is common to all agents, market\nequilibria can be arbitrarily bad. This negative result motivates a different\napproach. We present a novel technique called \"pairwise issue expansion\", which\ntransforms any public decision-making instance into an equivalent Fisher\nmarket, the simplest type of private goods market. This is done by expanding\neach issue into many goods: one for each pair of agents who disagree on that\nissue. We show that the equilibrium prices in the constructed Fisher market\nyield a \"pairwise pricing equilibrium\" in the original public decision-making\nproblem which maximizes Nash welfare. More broadly, pairwise issue expansion\nuncovers a powerful connection between the public decision-making and private\ngoods settings; this immediately yields several interesting results about\npublic decisions markets, and furthers the hope that we will be able to find a\nsimple iterative voting protocol that leads to near-optimum decisions. \n\n"}
{"id": "1807.10934", "contents": "Title: Bike Flow Prediction with Multi-Graph Convolutional Networks Abstract: One fundamental issue in managing bike sharing systems is the bike flow\nprediction. Due to the hardness of predicting the flow for a single station,\nrecent research works often predict the bike flow at cluster-level. While such\nstudies gain satisfactory prediction accuracy, they cannot directly guide some\nfine-grained bike sharing system management issues at station-level. In this\npaper, we revisit the problem of the station-level bike flow prediction, aiming\nto boost the prediction accuracy leveraging the breakthroughs of deep learning\ntechniques. We propose a new multi-graph convolutional neural network model to\npredict the bike flow at station-level, where the key novelty is viewing the\nbike sharing system from the graph perspective. More specifically, we construct\nmultiple inter-station graphs for a bike sharing system. In each graph, nodes\nare stations, and edges are a certain type of relations between stations. Then,\nmultiple graphs are constructed to reflect heterogeneous relationships (e.g.,\ndistance, ride record correlation). Afterward, we fuse the multiple graphs and\nthen apply the convolutional layers on the fused graph to predict station-level\nfuture bike flow. In addition to the estimated bike flow value, our model also\ngives the prediction confidence interval so as to help the bike sharing system\nmanagers make decisions. Using New York City and Chicago bike sharing data for\nexperiments, our model can outperform state-of-the-art station-level prediction\nmodels by reducing 25.1% and 17.0% of prediction error in New York City and\nChicago, respectively. \n\n"}
{"id": "1807.11914", "contents": "Title: Computing the Strategy to Commit to in Polymatrix Games (Extended\n  Version) Abstract: Leadership games provide a powerful paradigm to model many real-world\nsettings. Most literature focuses on games with a single follower who acts\noptimistically, breaking ties in favour of the leader. Unfortunately, for\nreal-world applications, this is unlikely. In this paper, we look for\nefficiently solvable games with multiple followers who play either\noptimistically or pessimistically, i.e., breaking ties in favour or against the\nleader. We study the computational complexity of finding or approximating an\noptimistic or pessimistic leader-follower equilibrium in specific classes of\nsuccinct games---polymatrix like---which are equivalent to 2-player Bayesian\ngames with uncertainty over the follower, with interdependent or independent\ntypes. Furthermore, we provide an exact algorithm to find a pessimistic\nequilibrium for those game classes. Finally, we show that in general polymatrix\ngames the computation is harder even when players are forced to play pure\nstrategies. \n\n"}
{"id": "1808.00232", "contents": "Title: Off-Policy Evaluation and Learning from Logged Bandit Feedback: Error\n  Reduction via Surrogate Policy Abstract: When learning from a batch of logged bandit feedback, the discrepancy between\nthe policy to be learned and the off-policy training data imposes statistical\nand computational challenges. Unlike classical supervised learning and online\nlearning settings, in batch contextual bandit learning, one only has access to\na collection of logged feedback from the actions taken by a historical policy,\nand expect to learn a policy that takes good actions in possibly unseen\ncontexts. Such a batch learning setting is ubiquitous in online and interactive\nsystems, such as ad platforms and recommendation systems. Existing approaches\nbased on inverse propensity weights, such as Inverse Propensity Scoring (IPS)\nand Policy Optimizer for Exponential Models (POEM), enjoy unbiasedness but\noften suffer from large mean squared error. In this work, we introduce a new\napproach named Maximum Likelihood Inverse Propensity Scoring (MLIPS) for batch\nlearning from logged bandit feedback. Instead of using the given historical\npolicy as the proposal in inverse propensity weights, we estimate a maximum\nlikelihood surrogate policy based on the logged action-context pairs, and then\nuse this surrogate policy as the proposal. We prove that MLIPS is\nasymptotically unbiased, and moreover, has a smaller nonasymptotic mean squared\nerror than IPS. Such an error reduction phenomenon is somewhat surprising as\nthe estimated surrogate policy is less accurate than the given historical\npolicy. Results on multi-label classification problems and a large- scale ad\nplacement dataset demonstrate the empirical effectiveness of MLIPS.\nFurthermore, the proposed surrogate policy technique is complementary to\nexisting error reduction techniques, and when combined, is able to consistently\nboost the performance of several widely used approaches. \n\n"}
{"id": "1808.00422", "contents": "Title: Almost Envy Freeness and Welfare Efficiency in Fair Division with Goods\n  or Bads Abstract: We consider two models of fair division with indivisible items: one for goods\nand one for bads. For goods, we study two generalized envy freeness proxies\n(EF1 and EFX for goods) and three common welfare (utilitarian, egalitarian and\nNash) efficiency notions. For bads, we study two generalized envy freeness\nproxies (1EF and XEF for goods) and two less common diswelfare (egalitarian and\nNash) efficiency notions. Some existing algorithms for goods do not work for\nbads. We thus propose several new algorithms for the model with bads. Our new\nalgorithms exhibit many nice properties. For example, with additive identical\nvaluations, an allocation that maximizes the egalitarian diswelfare or Nash\ndiswelfare is XEF and PE. Finally, we also give simple and tractable cases when\nthese envy freeness proxies and welfare efficiency are attainable in\ncombination (e.g. binary valuations, house allocations). \n\n"}
{"id": "1808.00934", "contents": "Title: Streaming Kernel PCA with $\\tilde{O}(\\sqrt{n})$ Random Features Abstract: We study the statistical and computational aspects of kernel principal\ncomponent analysis using random Fourier features and show that under mild\nassumptions, $O(\\sqrt{n} \\log n)$ features suffices to achieve\n$O(1/\\epsilon^2)$ sample complexity. Furthermore, we give a memory efficient\nstreaming algorithm based on classical Oja's algorithm that achieves this rate. \n\n"}
{"id": "1808.01080", "contents": "Title: The Complexity of Sequential Routing Games Abstract: We study routing games where every agent sequentially decides her next edge\nwhen she obtains the green light at each vertex. Because every edge only has\ncapacity to let out one agent per round, an edge acts as a FIFO waiting queue\nthat causes congestion on agents who enter. Given $n$ agents over $|V|$\nvertices, we show that for one agent, approximating a winning strategy within\n$n^{1-\\varepsilon}$ of the optimum for any $\\varepsilon>0$, or within any\npolynomial of $|V|$, are PSPACE-hard. Under perfect information, computing a\nsubgame perfect equilibrium (SPE) is PSPACE-hard and in FPSPACE. Under\nimperfect information, deciding SPE existence is PSPACE-complete. \n\n"}
{"id": "1808.01174", "contents": "Title: Generalization Error in Deep Learning Abstract: Deep learning models have lately shown great performance in various fields\nsuch as computer vision, speech recognition, speech translation, and natural\nlanguage processing. However, alongside their state-of-the-art performance, it\nis still generally unclear what is the source of their generalization ability.\nThus, an important question is what makes deep neural networks able to\ngeneralize well from the training set to new data. In this article, we provide\nan overview of the existing theory and bounds for the characterization of the\ngeneralization error of deep neural networks, combining both classical and more\nrecent theoretical and empirical results. \n\n"}
{"id": "1808.01181", "contents": "Title: Robust Spectral Filtering and Anomaly Detection Abstract: We consider a setting, where the output of a linear dynamical system (LDS)\nis, with an unknown but fixed probability, replaced by noise. There, we present\na robust method for the prediction of the outputs of the LDS and identification\nof the samples of noise, and prove guarantees on its statistical performance.\nOne application lies in anomaly detection: the samples of noise, unlikely to\nhave been generated by the dynamics, can be flagged to operators of the system\nfor further study. \n\n"}
{"id": "1808.01563", "contents": "Title: Solutions of partition function-based TU games for cooperative\n  communication networking Abstract: In networked communications nodes choose among available actions and benefit\nfrom exchanging information through edges, while continuous technological\nprogress fosters system functionings that increasingly often rely on\ncooperation. Growing attention is being placed on coalition formation, where\neach node chooses what coalition to join, while the surplus generated by\ncooperation is an amount of TU (transferable utility) quantified by a\nreal-valued function defined on partitions -or even embedded coalitions- of\nnodes. A TU-sharing rule is thus essential, as how players are rewarded\ndetermines their behavior. This work offers a new option for distributing\npartition function-based surpluses, dealing with cooperative game theory in\nterms of both global games and games in partition function form, namely lattice\nfunctions, while the sharing rule is a point-valued solution or value. The\nnovelty is grounded on the combinatorial definition of such solutions as\nlattice functions whose M\\\"obius inversion lives only on atoms, i.e. on the\nfirst level of the lattice. While simply rephrasing the traditional solution\nconcept for standard coalitional games, this leads to distribute the surplus\ngenerated by partitions across the edges of the network, as the atoms among\npartitions are unordered pairs of players. These shares of edges are further\ndivided between nodes, but the corresponding Shapley value is very different\nfrom the traditional one and leads to two alternative forms, obtained by\nfocusing either on marginal contributions along maximal chains, or else on the\nuniform division of Harsanyi dividends. The core is also addressed, and\nsupermodularity is no longer sufficient for its non-emptiness. \n\n"}
{"id": "1808.01664", "contents": "Title: Structured Adversarial Attack: Towards General Implementation and Better\n  Interpretability Abstract: When generating adversarial examples to attack deep neural networks (DNNs),\nLp norm of the added perturbation is usually used to measure the similarity\nbetween original image and adversarial example. However, such adversarial\nattacks perturbing the raw input spaces may fail to capture structural\ninformation hidden in the input. This work develops a more general attack\nmodel, i.e., the structured attack (StrAttack), which explores group sparsity\nin adversarial perturbations by sliding a mask through images aiming for\nextracting key spatial structures. An ADMM (alternating direction method of\nmultipliers)-based framework is proposed that can split the original problem\ninto a sequence of analytically solvable subproblems and can be generalized to\nimplement other attacking methods. Strong group sparsity is achieved in\nadversarial perturbations even with the same level of Lp norm distortion as the\nstate-of-the-art attacks. We demonstrate the effectiveness of StrAttack by\nextensive experimental results onMNIST, CIFAR-10, and ImageNet. We also show\nthat StrAttack provides better interpretability (i.e., better correspondence\nwith discriminative image regions)through adversarial saliency map (Papernot et\nal., 2016b) and class activation map(Zhou et al., 2016). \n\n"}
{"id": "1808.02123", "contents": "Title: Structure Learning for Relational Logistic Regression: An Ensemble\n  Approach Abstract: We consider the problem of learning Relational Logistic Regression (RLR).\nUnlike standard logistic regression, the features of RLRs are first-order\nformulae with associated weight vectors instead of scalar weights. We turn the\nproblem of learning RLR to learning these vector-weighted formulae and develop\na learning algorithm based on the recently successful functional-gradient\nboosting methods for probabilistic logic models. We derive the functional\ngradients and show how weights can be learned simultaneously in an efficient\nmanner. Our empirical evaluation on standard and novel data sets demonstrates\nthe superiority of our approach over other methods for learning RLR. \n\n"}
{"id": "1808.02696", "contents": "Title: The roll call interpretation of the Shapley value Abstract: The Shapley value is commonly illustrated by roll call votes in which players\nsupport or reject a proposal in sequence. If all sequences are equiprobable, a\nvoter's Shapley value can be interpreted as the probability of being pivotal,\ni.e., to bring about the required majority or to make this impossible for\nothers. We characterize the joint probability distributions over cooperation\npatterns that permit this roll call interpretation: individual votes may be\ninterdependent but must be exchangeable. \n\n"}
{"id": "1808.02941", "contents": "Title: On the Convergence of A Class of Adam-Type Algorithms for Non-Convex\n  Optimization Abstract: This paper studies a class of adaptive gradient based momentum algorithms\nthat update the search directions and learning rates simultaneously using past\ngradients. This class, which we refer to as the \"Adam-type\", includes the\npopular algorithms such as the Adam, AMSGrad and AdaGrad. Despite their\npopularity in training deep neural networks, the convergence of these\nalgorithms for solving nonconvex problems remains an open question. This paper\nprovides a set of mild sufficient conditions that guarantee the convergence for\nthe Adam-type methods. We prove that under our derived conditions, these\nmethods can achieve the convergence rate of order $O(\\log{T}/\\sqrt{T})$ for\nnonconvex stochastic optimization. We show the conditions are essential in the\nsense that violating them may make the algorithm diverge. Moreover, we propose\nand analyze a class of (deterministic) incremental adaptive gradient\nalgorithms, which has the same $O(\\log{T}/\\sqrt{T})$ convergence rate. Our\nstudy could also be extended to a broader class of adaptive gradient methods in\nmachine learning and optimization. \n\n"}
{"id": "1808.03206", "contents": "Title: The Buck-Passing Game Abstract: We consider a game in which players are the vertices of a directed graph.\nInitially, Nature chooses one player according to some fixed distribution and\ngives her a buck, which represents the request to perform a chore. After\ncompleting the task, the player passes the buck to one of her out-neighbors in\nthe graph. The procedure is repeated indefinitely and each player's cost is the\nasymptotic expected frequency of times that she receives the buck. We consider\na deterministic and a stochastic version of the game depending on how players\nselect the neighbor to pass the buck. In both cases we prove the existence of\npure equilibria that do not depend on the initial distribution; this is\nachieved by showing the existence of a generalized ordinal potential. We then\nuse the price of anarchy and price of stability to measure fairness of these\nequilibria. We also study a buck-holding variant of the game in which players\nwant to maximize the frequency of times they hold the buck, which includes the\nPageRank game as a special case. \n\n"}
{"id": "1808.03620", "contents": "Title: Ensemble Kalman Inversion: A Derivative-Free Technique For Machine\n  Learning Tasks Abstract: The standard probabilistic perspective on machine learning gives rise to\nempirical risk-minimization tasks that are frequently solved by stochastic\ngradient descent (SGD) and variants thereof. We present a formulation of these\ntasks as classical inverse or filtering problems and, furthermore, we propose\nan efficient, gradient-free algorithm for finding a solution to these problems\nusing ensemble Kalman inversion (EKI). Applications of our approach include\noffline and online supervised learning with deep neural networks, as well as\ngraph-based semi-supervised learning. The essence of the EKI procedure is an\nensemble based approximate gradient descent in which derivatives are replaced\nby differences from within the ensemble. We suggest several modifications to\nthe basic method, derived from empirically successful heuristics developed in\nthe context of SGD. Numerical results demonstrate wide applicability and\nrobustness of the proposed algorithm. \n\n"}
{"id": "1808.05264", "contents": "Title: DeepDownscale: a Deep Learning Strategy for High-Resolution Weather\n  Forecast Abstract: Running high-resolution physical models is computationally expensive and\nessential for many disciplines. Agriculture, transportation, and energy are\nsectors that depend on high-resolution weather models, which typically consume\nmany hours of large High Performance Computing (HPC) systems to deliver timely\nresults. Many users cannot afford to run the desired resolution and are forced\nto use low resolution output. One simple solution is to interpolate results for\nvisualization. It is also possible to combine an ensemble of low resolution\nmodels to obtain a better prediction. However, these approaches fail to capture\nthe redundant information and patterns in the low-resolution input that could\nhelp improve the quality of prediction. In this paper, we propose and evaluate\na strategy based on a deep neural network to learn a high-resolution\nrepresentation from low-resolution predictions using weather forecast as a\npractical use case. We take a supervised learning approach, since obtaining\nlabeled data can be done automatically. Our results show significant\nimprovement when compared with standard practices and the strategy is still\nlightweight enough to run on modest computer systems. \n\n"}
{"id": "1808.05671", "contents": "Title: On the Convergence of Adaptive Gradient Methods for Nonconvex\n  Optimization Abstract: Adaptive gradient methods are workhorses in deep learning. However, the\nconvergence guarantees of adaptive gradient methods for nonconvex optimization\nhave not been thoroughly studied. In this paper, we provide a fine-grained\nconvergence analysis for a general class of adaptive gradient methods including\nAMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that\nadaptive gradient methods in expectation converge to a first-order stationary\npoint. Our convergence rate is better than existing results for adaptive\ngradient methods in terms of dimension. In addition, we also prove high\nprobability bounds on the convergence rates of AMSGrad, RMSProp as well as\nAdaGrad, which have not been established before. Our analyses shed light on\nbetter understanding the mechanism behind adaptive gradient methods in\noptimizing nonconvex objectives. \n\n"}
{"id": "1808.06015", "contents": "Title: Ultra Reliable, Low Latency Vehicle-to-Infrastructure Wireless\n  Communications with Edge Computing Abstract: Ultra reliable, low latency vehicle-to-infrastructure (V2I) communications is\na key requirement for seamless operation of autonomous vehicles (AVs) in future\nsmart cities. To this end, cellular small base stations (SBSs) with edge\ncomputing capabilities can reduce the end-to-end (E2E) service delay by\nprocessing requested tasks from AVs locally, without forwarding the tasks to a\nremote cloud server. Nonetheless, due to the limited computational capabilities\nof the SBSs, coupled with the scarcity of the wireless bandwidth resources,\nminimizing the E2E latency for AVs and achieving a reliable V2I network is\nchallenging. In this paper, a novel algorithm is proposed to jointly optimize\nAVs-to-SBSs association and bandwidth allocation to maximize the reliability of\nthe V2I network. By using tools from labor matching markets, the proposed\nframework can effectively perform distributed association of AVs to SBSs, while\naccounting for the latency needs of AVs as well as the limited computational\nand bandwidth resources of SBSs. Moreover, the convergence of the proposed\nalgorithm to a core allocation between AVs and SBSs is proved and its ability\nto capture interdependent computational and transmission latencies for AVs in a\nV2I network is characterized. Simulation results show that by optimizing the\nE2E latency, the proposed algorithm substantially outperforms conventional cell\nassociation schemes, in terms of service reliability and latency. \n\n"}
{"id": "1808.06979", "contents": "Title: Thresholding at the monopoly price: an agnostic way to improve bidding\n  strategies in revenue-maximizing auctions Abstract: We address the problem of improving bidders' strategies in prior-dependent\nrevenue-maximizing auctions and introduce a simple and generic method to design\nnovel bidding strategies if the seller uses past bids to optimize her\nmechanism. We propose a simple and agnostic strategy, independent of the\ndistribution of the competition, that is robust to mechanism changes and local\n(as opposed to global) optimization of e.g. reserve prices by the seller. This\nstrategy guarantees an increase in utility compared to the truthful strategy\nfor any distribution of the competition. In textbook-style examples, for\ninstance with uniform [0,1] value distributions and two bidders, this\nno-side-information and mechanism-independent strategy yields an enormous 57%\nincrease in buyer utility for lazy second price auctions with monopoly\nreserves. When the bidder knows the distribution of the highest bid of the\ncompetition, we show how to optimize the tradeoff between reducing the reserve\nprice and beating the competition. Our formulation enables to study some\nimportant robustness properties of the strategies, showing their impact even\nwhen the seller is using a data-driven approach to set the reserve prices. In\nthis sample-size setting, we prove under what conditions, thresholding bidding\nstrategies can still improve the buyer's utility. The gist of our approach is\nto see optimal auctions in practice as a Stackelberg game where the buyer is\nthe leader, as he is the first one to move (here bid) when the seller is the\nfollower as she has no prior information on the bidder. \n\n"}
{"id": "1808.08866", "contents": "Title: A Study of Reinforcement Learning for Neural Machine Translation Abstract: Recent studies have shown that reinforcement learning (RL) is an effective\napproach for improving the performance of neural machine translation (NMT)\nsystem. However, due to its instability, successfully RL training is\nchallenging, especially in real-world systems where deep models and large\ndatasets are leveraged. In this paper, taking several large-scale translation\ntasks as testbeds, we conduct a systematic study on how to train better NMT\nmodels using reinforcement learning. We provide a comprehensive comparison of\nseveral important factors (e.g., baseline reward, reward shaping) in RL\ntraining. Furthermore, to fill in the gap that it remains unclear whether RL is\nstill beneficial when monolingual data is used, we propose a new method to\nleverage RL to further boost the performance of NMT systems trained with\nsource/target monolingual data. By integrating all our findings, we obtain\ncompetitive results on WMT14 English- German, WMT17 English-Chinese, and WMT17\nChinese-English translation tasks, especially setting a state-of-the-art\nperformance on WMT17 Chinese-English translation task. \n\n"}
{"id": "1808.08951", "contents": "Title: Water Disaggregation via Shape Features based Bayesian Discriminative\n  Sparse Coding Abstract: As the issue of freshwater shortage is increasing daily, it is critical to\ntake effective measures for water conservation. According to previous studies,\ndevice level consumption could lead to significant freshwater conservation.\nExisting water disaggregation methods focus on learning the signatures for\nappliances; however, they are lack of the mechanism to accurately discriminate\nparallel appliances' consumption. In this paper, we propose a Bayesian\nDiscriminative Sparse Coding model using Laplace Prior (BDSC-LP) to extensively\nenhance the disaggregation performance. To derive discriminative basis\nfunctions, shape features are presented to describe the low-sampling-rate water\nconsumption patterns. A Gibbs sampling based inference method is designed to\nextend the discriminative capability of the disaggregation dictionaries.\nExtensive experiments were performed to validate the effectiveness of the\nproposed model using both real-world and synthetic datasets. \n\n"}
{"id": "1808.09057", "contents": "Title: Loss Functions, Axioms, and Peer Review Abstract: It is common to see a handful of reviewers reject a highly novel paper,\nbecause they view, say, extensive experiments as far more important than\nnovelty, whereas the community as a whole would have embraced the paper. More\ngenerally, the disparate mapping of criteria scores to final recommendations by\ndifferent reviewers is a major source of inconsistency in peer review. In this\npaper we present a framework inspired by empirical risk minimization (ERM) for\nlearning the community's aggregate mapping. The key challenge that arises is\nthe specification of a loss function for ERM. We consider the class of $L(p,q)$\nloss functions, which is a matrix-extension of the standard class of $L_p$\nlosses on vectors; here the choice of the loss function amounts to choosing the\nhyperparameters $p, q \\in [1,\\infty]$. To deal with the absence of ground truth\nin our problem, we instead draw on computational social choice to identify\ndesirable values of the hyperparameters $p$ and $q$. Specifically, we\ncharacterize $p=q=1$ as the only choice of these hyperparameters that satisfies\nthree natural axiomatic properties. Finally, we implement and apply our\napproach to reviews from IJCAI 2017. \n\n"}
{"id": "1808.09226", "contents": "Title: A Note on the Complexity of Manipulating Weighted Schulze Voting Abstract: We prove that the constructive weighted coalitional manipulation problem for\nthe Schulze voting rule can be solved in polynomial time for an unbounded\nnumber of candidates and an unbounded number of manipulators. \n\n"}
{"id": "1808.09819", "contents": "Title: Approximate Exploration through State Abstraction Abstract: Although exploration in reinforcement learning is well understood from a\ntheoretical point of view, provably correct methods remain impractical. In this\npaper we study the interplay between exploration and approximation, what we\ncall approximate exploration. Our main goal is to further our theoretical\nunderstanding of pseudo-count based exploration bonuses (Bellemare et al.,\n2016), a practical exploration scheme based on density modelling. As a warm-up,\nwe quantify the performance of an exploration algorithm, MBIE-EB (Strehl and\nLittman, 2008), when explicitly combined with state aggregation. This allows us\nto confirm that, as might be expected, approximation allows the agent to trade\noff between learning speed and quality of the learned policy. Next, we show how\na given density model can be related to an abstraction and that the\ncorresponding pseudo-count bonus can act as a substitute in MBIE-EB combined\nwith this abstraction, but may lead to either under- or over-exploration. Then,\nwe show that a given density model also defines an implicit abstraction, and\nfind a surprising mismatch between pseudo-counts derived either implicitly or\nexplicitly. Finally we derive a new pseudo-count bonus alleviating this issue. \n\n"}
{"id": "1809.01130", "contents": "Title: Nash equilibrium in asymmetric multi-players zero-sum game with two\n  strategic variables and only one alien Abstract: We consider a partially asymmetric multi-players zero-sum game with two\nstrategic variables. All but one players have the same payoff functions, and\none player (Player $n$) does not. Two strategic variables are $t_i$'s and\n$s_i$'s for each player $i$. Mainly we will show the following results. 1) The\nequilibrium when all players choose $t_i$'s is equivalent to the equilibrium\nwhen all but one players choose $t_i$'s and Player $n$ chooses $s_n$ as their\nstrategic variables. 2) The equilibrium when all players choose $s_i$'s is\nequivalent to the equilibrium when all but one players choose $s_i$'s and\nPlayer $n$ chooses $t_n$ as their strategic variables. The equilibrium when all\nplayers choose $t_i$'s and the equilibrium when all players choose $s_i$'s are\nnot equivalent although they are equivalent in a symmetric game in which all\nplayers have the same payoff functions. \n\n"}
{"id": "1809.01283", "contents": "Title: Routing for Traffic Networks with Mixed Autonomy Abstract: In this work we propose a macroscopic model for studying routing on networks\nshared between human-driven and autonomous vehicles that captures the effects\nof autonomous vehicles forming platoons. We use this to study inefficiency due\nto selfish routing and bound the Price of Anarchy (PoA), the maximum ratio\nbetween total delay experienced by selfish users and the minimum possible total\ndelay. To do so, we establish two road capacity models, each corresponding to\nan assumption regarding the platooning capabilities of autonomous vehicles.\nUsing these we develop a class of road delay functions, parameterized by the\nroad capacity, that are polynomial with respect to vehicle flow. We then bound\nthe PoA and the bicriteria, another measure of the inefficiency due to selfish\nrouting. We find these bounds depend on: 1) the degree of the polynomial in the\nroad cost function and 2) the degree of asymmetry, the difference in how\nhuman-driven and autonomous traffic affect congestion. We demonstrate that\nthese bounds recover the classical bounds when no asymmetry exists. We show the\nbounds are tight in certain cases and that the PoA bound is order-optimal with\nrespect to the degree of asymmetry. \n\n"}
{"id": "1809.01717", "contents": "Title: Approximating Bimatrix Nash Equilibrium Via Trilinear Minimax Abstract: The Bimatrix Nash Equilibrium (NE) for $m \\times n$ real matrices $R$ and\n$C$, denoted as the {\\it Row} and {\\it Column} players, is characterized as\nfollows: Let $\\Delta =S_m \\times S_n$, where $S_k$ denotes the unit simplex in\n$\\mathbb{R}^k$. For a given point $p=(x,y) \\in \\Delta$, define $R[p]=x^TRy$ and\n$C[p]=x^TCy$. Consequently, there exists a subset $\\Delta_* \\subset \\Delta$\nsuch that for any $p_*=(x_*,y_*) \\in \\Delta_*$, $\\max_{p \\in \\Delta,\ny=y_*}R[p]=R[p_*]$ and $\\max_{p \\in \\Delta, x=x_* } C[p]=C[p_*]$.\n  The computational complexity of bimatrix NE falls within the class of {\\it\nPPAD-complete}. Although the von Neumann Minimax Theorem is a special case of\nbimatrix NE, we introduce a novel extension termed {\\it Trilinear Minimax\nRelaxation} (TMR) with the following implications: Let $\\lambda^*=\\min_{\\alpha\n\\in S_{2}} \\max_{p \\in \\Delta} (\\alpha_1 R[p]+ \\alpha_2C[p])$ and\n$\\lambda_*=\\max_{p \\in \\Delta} \\min_{\\alpha \\in S_{2}} (\\alpha_1 R[p]+\n\\alpha_2C[p])$.\n  $\\lambda^* \\geq \\lambda_*$.\n  $\\lambda^*$ is computable as a linear programming in $O(mn)$ time, ensuring\n$\\max_{p_* \\in \\Delta_*}\\min \\{R[p_*], C[p_*]\\} \\leq \\lambda^*$, meaning that\nin any Nash Equilibrium it is not possible to have both players' payoffs to\nexceed $\\lambda^*$.\n  $\\lambda^*=\\lambda_*$ if and only if there exists $p^* \\in \\Delta$ such that\n$\\lambda^*= \\min\\{R[p^*], C[p^*]\\}$. Such a $p^*$ serves as an approximate Nash\nEquilibrium. We analyze the cases where such $p^*$ exists and is computable.\n  Even when $\\lambda^* > \\lambda_*$, we derive approximate Nash Equilibria.\n  In summary, the aforementioned properties of TMR and its efficient\ncomputational aspects underscore its significance and relevance for Nash\nEquilibrium, irrespective of the computational complexity associated with\nbimatrix Nash Equilibrium. Finally, we extend TMR to scenarios involving three\nor more players. \n\n"}
{"id": "1809.01819", "contents": "Title: MASA: Motif-Aware State Assignment in Noisy Time Series Data Abstract: Complex systems, such as airplanes, cars, or financial markets, produce\nmultivariate time series data consisting of a large number of system\nmeasurements over a period of time. Such data can be interpreted as a sequence\nof states, where each state represents a prototype of system behavior. An\nimportant problem in this domain is to identify repeated sequences of states,\nknown as motifs. Such motifs correspond to complex behaviors that capture\ncommon sequences of state transitions. For example, in automotive data, a motif\nof \"making a turn\" might manifest as a sequence of states: slowing down,\nturning the wheel, and then speeding back up. However, discovering these motifs\nis challenging, because the individual states and state assignments are\nunknown, have different durations, and need to be jointly learned from the\nnoisy time series. Here we develop motif-aware state assignment (MASA), a\nmethod to discover common motifs in noisy time series data and leverage those\nmotifs to more robustly assign states to measurements. We formulate the problem\nof motif discovery as a large optimization problem, which we solve using an\nexpectation-maximization type approach. MASA performs well in the presence of\nnoise in the input data and is scalable to very large datasets. Experiments on\nsynthetic data show that MASA outperforms state-of-the-art baselines by up to\n38.2%, and two case studies demonstrate how our approach discovers insightful\nmotifs in the presence of noise in real-world time series data. \n\n"}
{"id": "1809.02070", "contents": "Title: ARCHER: Aggressive Rewards to Counter bias in Hindsight Experience\n  Replay Abstract: Experience replay is an important technique for addressing\nsample-inefficiency in deep reinforcement learning (RL), but faces difficulty\nin learning from binary and sparse rewards due to disproportionately few\nsuccessful experiences in the replay buffer. Hindsight experience replay (HER)\nwas recently proposed to tackle this difficulty by manipulating unsuccessful\ntransitions, but in doing so, HER introduces a significant bias in the replay\nbuffer experiences and therefore achieves a suboptimal improvement in\nsample-efficiency. In this paper, we present an analysis on the source of bias\nin HER, and propose a simple and effective method to counter the bias, to most\neffectively harness the sample-efficiency provided by HER. Our method,\nmotivated by counter-factual reasoning and called ARCHER, extends HER with a\ntrade-off to make rewards calculated for hindsight experiences numerically\ngreater than real rewards. We validate our algorithm on two continuous control\nenvironments from DeepMind Control Suite - Reacher and Finger, which simulate\nmanipulation tasks with a robotic arm - in combination with various reward\nfunctions, task complexities and goal sampling strategies. Our experiments\nconsistently demonstrate that countering bias using more aggressive hindsight\nrewards increases sample efficiency, thus establishing the greater benefit of\nARCHER in RL applications with limited computing budget. \n\n"}
{"id": "1809.02112", "contents": "Title: ANS: Adaptive Network Scaling for Deep Rectifier Reinforcement Learning\n  Models Abstract: This work provides a thorough study on how reward scaling can affect\nperformance of deep reinforcement learning agents. In particular, we would like\nto answer the question that how does reward scaling affect non-saturating ReLU\nnetworks in RL? This question matters because ReLU is one of the most effective\nactivation functions for deep learning models. We also propose an Adaptive\nNetwork Scaling framework to find a suitable scale of the rewards during\nlearning for better performance. We conducted empirical studies to justify the\nsolution. \n\n"}
{"id": "1809.02145", "contents": "Title: GANs beyond divergence minimization Abstract: Generative adversarial networks (GANs) can be interpreted as an adversarial\ngame between two players, a discriminator D and a generator G, in which D\nlearns to classify real from fake data and G learns to generate realistic data\nby \"fooling\" D into thinking that fake data is actually real data. Currently, a\ndominating view is that G actually learns by minimizing a divergence given that\nthe general objective function is a divergence when D is optimal. However, this\nview has been challenged due to inconsistencies between theory and practice. In\nthis paper, we discuss of the properties associated with most loss functions\nfor G (e.g., saturating/non-saturating f-GAN, LSGAN, WGAN, etc.). We show that\nthese loss functions are not divergences and do not have the same equilibrium\nas expected of divergences. This suggests that G does not need to minimize the\nsame objective function as D maximize, nor maximize the objective of D after\nswapping real data with fake data (non-saturating GAN) but can instead use a\nwide range of possible loss functions to learn to generate realistic data. We\ndefine GANs through two separate and independent D maximization and G\nminimization steps. We generalize the generator step to four new classes of\nloss functions, most of which are actual divergences (while traditional G loss\nfunctions are not). We test a wide variety of loss functions from these four\nclasses on a synthetic dataset and on CIFAR-10. We observe that most loss\nfunctions converge well and provide comparable data generation quality to\nnon-saturating GAN, LSGAN, and WGAN-GP generator loss functions, whether we use\ndivergences or non-divergences. These results suggest that GANs do not conform\nwell to the divergence minimization theory and form a much broader range of\nmodels than previously assumed. \n\n"}
{"id": "1809.02441", "contents": "Title: StackNet: Stacking Parameters for Continual learning Abstract: Training a neural network for a classification task typically assumes that\nthe data to train are given from the beginning. However, in the real world,\nadditional data accumulate gradually and the model requires additional training\nwithout accessing the old training data. This usually leads to the catastrophic\nforgetting problem which is inevitable for the traditional training methodology\nof neural networks. In this paper, we propose a continual learning method that\nis able to learn additional tasks while retaining the performance of previously\nlearned tasks by stacking parameters. Composed of two complementary components,\nthe index module and the StackNet, our method estimates the index of the\ncorresponding task for an input sample with the index module and utilizes a\nparticular portion of StackNet with this index. The StackNet guarantees no\ndegradation in the performance of the previously learned tasks and the index\nmodule shows high confidence in finding the origin of an input sample. Compared\nto the previous work of PackNet, our method is competitive and highly\nintuitive. \n\n"}
{"id": "1809.02499", "contents": "Title: MixUp as Locally Linear Out-Of-Manifold Regularization Abstract: MixUp is a recently proposed data-augmentation scheme, which linearly\ninterpolates a random pair of training examples and correspondingly the one-hot\nrepresentations of their labels. Training deep neural networks with such\nadditional data is shown capable of significantly improving the predictive\naccuracy of the current art. The power of MixUp, however, is primarily\nestablished empirically and its working and effectiveness have not been\nexplained in any depth. In this paper, we develop an understanding for MixUp as\na form of \"out-of-manifold regularization\", which imposes certain \"local\nlinearity\" constraints on the model's input space beyond the data manifold.\nThis analysis enables us to identify a limitation of MixUp, which we call\n\"manifold intrusion\". In a nutshell, manifold intrusion in MixUp is a form of\nunder-fitting resulting from conflicts between the synthetic labels of the\nmixed-up examples and the labels of original training data. Such a phenomenon\nusually happens when the parameters controlling the generation of mixing\npolicies are not sufficiently fine-tuned on the training data. To address this\nissue, we propose a novel adaptive version of MixUp, where the mixing policies\nare automatically learned from the data using an additional network and\nobjective function designed to avoid manifold intrusion. The proposed\nregularizer, AdaMixUp, is empirically evaluated on several benchmark datasets.\nExtensive experiments demonstrate that AdaMixUp improves upon MixUp when\napplied to the current art of deep classification models. \n\n"}
{"id": "1809.02864", "contents": "Title: Online Adaptive Methods, Universality and Acceleration Abstract: We present a novel method for convex unconstrained optimization that, without\nany modifications, ensures: (i) accelerated convergence rate for smooth\nobjectives, (ii) standard convergence rate in the general (non-smooth) setting,\nand (iii) standard convergence rate in the stochastic optimization setting. To\nthe best of our knowledge, this is the first method that simultaneously applies\nto all of the above settings. At the heart of our method is an adaptive\nlearning rate rule that employs importance weights, in the spirit of adaptive\nonline learning algorithms (Duchi et al., 2011; Levy, 2017), combined with an\nupdate that linearly couples two sequences, in the spirit of (Allen-Zhu and\nOrecchia, 2017). An empirical examination of our method demonstrates its\napplicability to the above mentioned scenarios and corroborates our theoretical\nfindings. \n\n"}
{"id": "1809.03019", "contents": "Title: Stochastic Gradient Descent Learns State Equations with Nonlinear\n  Activations Abstract: We study discrete time dynamical systems governed by the state equation\n$h_{t+1}=\\phi(Ah_t+Bu_t)$. Here $A,B$ are weight matrices, $\\phi$ is an\nactivation function, and $u_t$ is the input data. This relation is the backbone\nof recurrent neural networks (e.g. LSTMs) which have broad applications in\nsequential learning tasks. We utilize stochastic gradient descent to learn the\nweight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$. We\nprove that SGD estimate linearly converges to the ground truth weights while\nusing near-optimal sample size. Our results apply to increasing activations\nwhose derivatives are bounded away from zero. The analysis is based on i) a\nnovel SGD convergence result with nonlinear activations and ii) careful\nstatistical characterization of the state vector. Numerical experiments verify\nthe fast convergence of SGD on ReLU and leaky ReLU in consistence with our\ntheory. \n\n"}
{"id": "1809.03057", "contents": "Title: Variance Reduction in Monte Carlo Counterfactual Regret Minimization\n  (VR-MCCFR) for Extensive Form Games using Baselines Abstract: Learning strategies for imperfect information games from samples of\ninteraction is a challenging problem. A common method for this setting, Monte\nCarlo Counterfactual Regret Minimization (MCCFR), can have slow long-term\nconvergence rates due to high variance. In this paper, we introduce a variance\nreduction technique (VR-MCCFR) that applies to any sampling variant of MCCFR.\nUsing this technique, per-iteration estimated values and updates are\nreformulated as a function of sampled values and state-action baselines,\nsimilar to their use in policy gradient reinforcement learning. The new\nformulation allows estimates to be bootstrapped from other estimates within the\nsame episode, propagating the benefits of baselines along the sampled\ntrajectory; the estimates remain unbiased even when bootstrapping from other\nestimates. Finally, we show that given a perfect baseline, the variance of the\nvalue estimates can be reduced to zero. Experimental evaluation shows that\nVR-MCCFR brings an order of magnitude speedup, while the empirical variance\ndecreases by three orders of magnitude. The decreased variance allows for the\nfirst time CFR+ to be used with sampling, increasing the speedup to two orders\nof magnitude. \n\n"}
{"id": "1809.03152", "contents": "Title: A Multi-Agent Reinforcement Learning Method for Impression Allocation in\n  Online Display Advertising Abstract: In online display advertising, guaranteed contracts and real-time bidding\n(RTB) are two major ways to sell impressions for a publisher. Despite the\nincreasing popularity of RTB, there is still half of online display advertising\nrevenue generated from guaranteed contracts. Therefore, simultaneously selling\nimpressions through both guaranteed contracts and RTB is a straightforward\nchoice for a publisher to maximize its yield. However, deriving the optimal\nstrategy to allocate impressions is not a trivial task, especially when the\nenvironment is unstable in real-world applications. In this paper, we formulate\nthe impression allocation problem as an auction problem where each contract can\nsubmit virtual bids for individual impressions. With this formulation, we\nderive the optimal impression allocation strategy by solving the optimal\nbidding functions for contracts. Since the bids from contracts are decided by\nthe publisher, we propose a multi-agent reinforcement learning (MARL) approach\nto derive cooperative policies for the publisher to maximize its yield in an\nunstable environment. The proposed approach also resolves the common challenges\nin MARL such as input dimension explosion, reward credit assignment, and\nnon-stationary environment. Experimental evaluations on large-scale real\ndatasets demonstrate the effectiveness of our approach. \n\n"}
{"id": "1809.04040", "contents": "Title: Solving Imperfect-Information Games via Discounted Regret Minimization Abstract: Counterfactual regret minimization (CFR) is a family of iterative algorithms\nthat are the most popular and, in practice, fastest approach to approximately\nsolving large imperfect-information games. In this paper we introduce novel CFR\nvariants that 1) discount regrets from earlier iterations in various ways (in\nsome cases differently for positive and negative regrets), 2) reweight\niterations in various ways to obtain the output strategies, 3) use a\nnon-standard regret minimizer and/or 4) leverage \"optimistic regret matching\".\nThey lead to dramatically improved performance in many settings. For one, we\nintroduce a variant that outperforms CFR+, the prior state-of-the-art\nalgorithm, in every game tested, including large-scale realistic settings. CFR+\nis a formidable benchmark: no other algorithm has been able to outperform it.\nFinally, we show that, unlike CFR+, many of the important new variants are\ncompatible with modern imperfect-information-game pruning techniques and one is\nalso compatible with sampling in the game tree. \n\n"}
{"id": "1809.04136", "contents": "Title: Randomized Wagering Mechanisms Abstract: Wagering mechanisms are one-shot betting mechanisms that elicit agents'\npredictions of an event. For deterministic wagering mechanisms, an existing\nimpossibility result has shown incompatibility of some desirable theoretical\nproperties. In particular, Pareto optimality (no profitable side bet before\nallocation) can not be achieved together with weak incentive compatibility,\nweak budget balance and individual rationality. In this paper, we expand the\ndesign space of wagering mechanisms to allow randomization and ask whether\nthere are randomized wagering mechanisms that can achieve all previously\nconsidered desirable properties, including Pareto optimality. We answer this\nquestion positively with two classes of randomized wagering mechanisms: i) one\nsimple randomized lottery-type implementation of existing deterministic\nwagering mechanisms, and ii) another family of simple and randomized wagering\nmechanisms which we call surrogate wagering mechanisms, which are robust to\nnoisy ground truth. This family of mechanisms builds on the idea of learning\nwith noisy labels (Natarajan et al. 2013) as well as a recent extension of this\nidea to the information elicitation without verification setting (Liu and Chen\n2018). We show that a broad family of randomized wagering mechanisms satisfy\nall desirable theoretical properties. \n\n"}
{"id": "1809.04224", "contents": "Title: Access to Population-Level Signaling as a Source of Inequality Abstract: We identify and explore differential access to population-level signaling\n(also known as information design) as a source of unequal access to\nopportunity. A population-level signaler has potentially noisy observations of\na binary type for each member of a population and, based on this, produces a\nsignal about each member. A decision-maker infers types from signals and\naccepts those individuals whose type is high in expectation. We assume the\nsignaler of the disadvantaged population reveals her observations to the\ndecision-maker, whereas the signaler of the advantaged population forms signals\nstrategically. We study the expected utility of the populations as measured by\nthe fraction of accepted members, as well as the false positive rates (FPR) and\nfalse negative rates (FNR).\n  We first show the intuitive results that for a fixed environment, the\nadvantaged population has higher expected utility, higher FPR, and lower FNR,\nthan the disadvantaged one (despite having identical population quality), and\nthat more accurate observations improve the expected utility of the advantaged\npopulation while harming that of the disadvantaged one. We next explore the\nintroduction of a publicly-observable signal, such as a test score, as a\npotential intervention. Our main finding is that this natural intervention,\nintended to reduce the inequality between the populations' utilities, may\nactually exacerbate it in settings where observations and test scores are\nnoisy. \n\n"}
{"id": "1809.04382", "contents": "Title: A Framework for Approval-based Budgeting Methods Abstract: We define and study a general framework for approval-based budgeting methods\nand compare certain methods within this framework by their axiomatic and\ncomputational properties. Furthermore, we visualize their behavior on certain\nEuclidean distributions and analyze them experimentally. \n\n"}
{"id": "1809.04988", "contents": "Title: Sequential Coordination of Deep Models for Learning Visual Arithmetic Abstract: Achieving machine intelligence requires a smooth integration of perception\nand reasoning, yet models developed to date tend to specialize in one or the\nother; sophisticated manipulation of symbols acquired from rich perceptual\nspaces has so far proved elusive. Consider a visual arithmetic task, where the\ngoal is to carry out simple arithmetical algorithms on digits presented under\nnatural conditions (e.g. hand-written, placed randomly). We propose a\ntwo-tiered architecture for tackling this problem. The lower tier consists of a\nheterogeneous collection of information processing modules, which can include\npre-trained deep neural networks for locating and extracting characters from\nthe image, as well as modules performing symbolic transformations on the\nrepresentations extracted by perception. The higher tier consists of a\ncontroller, trained using reinforcement learning, which coordinates the modules\nin order to solve the high-level task. For instance, the controller may learn\nin what contexts to execute the perceptual networks and what symbolic\ntransformations to apply to their outputs. The resulting model is able to solve\na variety of tasks in the visual arithmetic domain, and has several advantages\nover standard, architecturally homogeneous feedforward networks including\nimproved sample efficiency. \n\n"}
{"id": "1809.05504", "contents": "Title: Melding the Data-Decisions Pipeline: Decision-Focused Learning for\n  Combinatorial Optimization Abstract: Creating impact in real-world settings requires artificial intelligence\ntechniques to span the full pipeline from data, to predictive models, to\ndecisions. These components are typically approached separately: a machine\nlearning model is first trained via a measure of predictive accuracy, and then\nits predictions are used as input into an optimization algorithm which produces\na decision. However, the loss function used to train the model may easily be\nmisaligned with the end goal, which is to make the best decisions possible.\nHand-tuning the loss function to align with optimization is a difficult and\nerror-prone process (which is often skipped entirely).\n  We focus on combinatorial optimization problems and introduce a general\nframework for decision-focused learning, where the machine learning model is\ndirectly trained in conjunction with the optimization algorithm to produce\nhigh-quality decisions. Technically, our contribution is a means of integrating\ncommon classes of discrete optimization problems into deep learning or other\npredictive models, which are typically trained via gradient descent. The main\nidea is to use a continuous relaxation of the discrete problem to propagate\ngradients through the optimization procedure. We instantiate this framework for\ntwo broad classes of combinatorial problems: linear programs and submodular\nmaximization. Experimental results across a variety of domains show that\ndecision-focused learning often leads to improved optimization performance\ncompared to traditional methods. We find that standard measures of accuracy are\nnot a reliable proxy for a predictive model's utility in optimization, and our\nmethod's ability to specify the true goal as the model's training objective\nyields substantial dividends across a range of decision problems. \n\n"}
{"id": "1809.06098", "contents": "Title: Policy Optimization via Importance Sampling Abstract: Policy optimization is an effective reinforcement learning approach to solve\ncontinuous control tasks. Recent achievements have shown that alternating\nonline and offline optimization is a successful choice for efficient trajectory\nreuse. However, deciding when to stop optimizing and collect new trajectories\nis non-trivial, as it requires to account for the variance of the objective\nfunction estimate. In this paper, we propose a novel, model-free, policy search\nalgorithm, POIS, applicable in both action-based and parameter-based settings.\nWe first derive a high-confidence bound for importance sampling estimation;\nthen we define a surrogate objective function, which is optimized offline\nwhenever a new batch of trajectories is collected. Finally, the algorithm is\ntested on a selection of continuous control tasks, with both linear and deep\npolicies, and compared with state-of-the-art policy optimization methods. \n\n"}
{"id": "1809.06848", "contents": "Title: On the Learning Dynamics of Deep Neural Networks Abstract: While a lot of progress has been made in recent years, the dynamics of\nlearning in deep nonlinear neural networks remain to this day largely\nmisunderstood. In this work, we study the case of binary classification and\nprove various properties of learning in such networks under strong assumptions\nsuch as linear separability of the data. Extending existing results from the\nlinear case, we confirm empirical observations by proving that the\nclassification error also follows a sigmoidal shape in nonlinear architectures.\nWe show that given proper initialization, learning expounds parallel\nindependent modes and that certain regions of parameter space might lead to\nfailed training. We also demonstrate that input norm and features' frequency in\nthe dataset lead to distinct convergence speeds which might shed some light on\nthe generalization capabilities of deep neural networks. We provide a\ncomparison between the dynamics of learning with cross-entropy and hinge\nlosses, which could prove useful to understand recent progress in the training\nof generative adversarial networks. Finally, we identify a phenomenon that we\nbaptize gradient starvation where the most frequent features in a dataset\nprevent the learning of other less frequent but equally informative features. \n\n"}
{"id": "1809.07192", "contents": "Title: Unbalanced Multi-Phase Distribution Grid Topology Estimation and Bus\n  Phase Identification Abstract: There is an increasing need for monitoring and controlling uncertainties\nbrought by distributed energy resources in distribution grids. For such goal,\naccurate multi-phase topology is the basis for correlating measurements in\nunbalanced distribution networks. Unfortunately, such topology knowledge is\noften unavailable due to limited investment, especially for \\revv{low-voltage}\ndistribution grids. Also, the bus phase labeling information is inaccurate due\nto human errors or outdated records. For this challenge, this paper utilizes\nsmart meter data for an information-theoretic approach to learn the topology of\ndistribution grids. Specifically, multi-phase unbalanced systems are converted\ninto symmetrical components, namely positive, negative, and zero sequences.\nThen, this paper proves that the Chow-Liu algorithm finds the topology by\nutilizing power flow equations and the conditional independence relationships\nimplied by the radial multi-phase structure of distribution grids with the\npresence of incorrect bus phase labels. At last, by utilizing Carson's\nequation, this paper proves that the bus phase connection can be correctly\nidentified using voltage measurements. For validation, IEEE systems are\nsimulated using three real data sets. The simulation results demonstrate that\nthe algorithm is highly accurate for finding multi-phase topology even with\nstrong load unbalancing condition and DERs. This ensures close monitoring and\ncontrolling DERs in distribution grids. \n\n"}
{"id": "1809.07803", "contents": "Title: Dynamic Weights in Multi-Objective Deep Reinforcement Learning Abstract: Many real-world decision problems are characterized by multiple conflicting\nobjectives which must be balanced based on their relative importance. In the\ndynamic weights setting the relative importance changes over time and\nspecialized algorithms that deal with such change, such as a tabular\nReinforcement Learning (RL) algorithm by Natarajan and Tadepalli (2005), are\nrequired. However, this earlier work is not feasible for RL settings that\nnecessitate the use of function approximators. We generalize across weight\nchanges and high-dimensional inputs by proposing a multi-objective Q-network\nwhose outputs are conditioned on the relative importance of objectives and we\nintroduce Diverse Experience Replay (DER) to counter the inherent\nnon-stationarity of the Dynamic Weights setting. We perform an extensive\nexperimental evaluation and compare our methods to adapted algorithms from Deep\nMulti-Task/Multi-Objective Reinforcement Learning and show that our proposed\nnetwork in combination with DER dominates these adapted algorithms across\nweight change scenarios and problem domains. \n\n"}
{"id": "1809.08346", "contents": "Title: A Meta-Learning Approach for Custom Model Training Abstract: Transfer-learning and meta-learning are two effective methods to apply\nknowledge learned from large data sources to new tasks. In few-class, few-shot\ntarget task settings (i.e. when there are only a few classes and training\nexamples available in the target task), meta-learning approaches that optimize\nfor future task learning have outperformed the typical transfer approach of\ninitializing model weights from a pre-trained starting point. But as we\nexperimentally show, meta-learning algorithms that work well in the few-class\nsetting do not generalize well in many-shot and many-class cases. In this\npaper, we propose a joint training approach that combines both\ntransfer-learning and meta-learning. Benefiting from the advantages of each,\nour method obtains improved generalization performance on unseen target tasks\nin both few- and many-class and few- and many-shot scenarios. \n\n"}
{"id": "1809.09081", "contents": "Title: Autonomous Deep Learning: Incremental Learning of Denoising Autoencoder\n  for Evolving Data Streams Abstract: The generative learning phase of Autoencoder (AE) and its successor Denosing\nAutoencoder (DAE) enhances the flexibility of data stream method in exploiting\nunlabelled samples. Nonetheless, the feasibility of DAE for data stream\nanalytic deserves in-depth study because it characterizes a fixed network\ncapacity which cannot adapt to rapidly changing environments. An automated\nconstruction of a denoising autoeconder, namely deep evolving denoising\nautoencoder (DEVDAN), is proposed in this paper. DEVDAN features an open\nstructure both in the generative phase and in the discriminative phase where\ninput features can be automatically added and discarded on the fly. A network\nsignificance (NS) method is formulated in this paper and is derived from the\nbias-variance concept. This method is capable of estimating the statistical\ncontribution of the network structure and its hidden units which precursors an\nideal state to add or prune input features. Furthermore, DEVDAN is free of the\nproblem- specific threshold and works fully in the single-pass learning\nfashion. The efficacy of DEVDAN is numerically validated using nine\nnon-stationary data stream problems simulated under the prequential\ntest-then-train protocol where DEVDAN is capable of delivering an improvement\nof classification accuracy to recently published online learning works while\nhaving flexibility in the automatic extraction of robust input features and in\nadapting to rapidly changing environments. \n\n"}
{"id": "1809.09095", "contents": "Title: On Reinforcement Learning for Full-length Game of StarCraft Abstract: StarCraft II poses a grand challenge for reinforcement learning. The main\ndifficulties of it include huge state and action space and a long-time horizon.\nIn this paper, we investigate a hierarchical reinforcement learning approach\nfor StarCraft II. The hierarchy involves two levels of abstraction. One is the\nmacro-action automatically extracted from expert's trajectories, which reduces\nthe action space in an order of magnitude yet remains effective. The other is a\ntwo-layer hierarchical architecture which is modular and easy to scale,\nenabling a curriculum transferring from simpler tasks to more complex tasks.\nThe reinforcement training algorithm for this architecture is also\ninvestigated. On a 64x64 map and using restrictive units, we achieve a winning\nrate of more than 99\\% against the difficulty level-1 built-in AI. Through the\ncurriculum transfer learning algorithm and a mixture of combat model, we can\nachieve over 93\\% winning rate of Protoss against the most difficult\nnon-cheating built-in AI (level-7) of Terran, training within two days using a\nsingle machine with only 48 CPU cores and 8 K40 GPUs. It also shows strong\ngeneralization performance, when tested against never seen opponents including\ncheating levels built-in AI and all levels of Zerg and Protoss built-in AI. We\nhope this study could shed some light on the future research of large-scale\nreinforcement learning. \n\n"}
{"id": "1809.10315", "contents": "Title: Smooth Inter-layer Propagation of Stabilized Neural Networks for\n  Classification Abstract: Recent work has studied the reasons for the remarkable performance of deep\nneural networks in image classification. We examine batch normalization on the\none hand and the dynamical systems view of residual networks on the other hand.\nOur goal is in understanding the notions of stability and smoothness of the\ninter-layer propagation of ResNets so as to explain when they contribute to\nsignificantly enhanced performance. We postulate that such stability is of\nimportance for the trained ResNet to transfer. \n\n"}
{"id": "1809.10477", "contents": "Title: Fast Stochastic Algorithms for Low-rank and Nonsmooth Matrix Problems Abstract: Composite convex optimization problems which include both a nonsmooth term\nand a low-rank promoting term have important applications in machine learning\nand signal processing, such as when one wishes to recover an unknown matrix\nthat is simultaneously low-rank and sparse. However, such problems are highly\nchallenging to solve in large-scale: the low-rank promoting term prohibits\nefficient implementations of proximal methods for composite optimization and\neven simple subgradient methods. On the other hand, methods which are tailored\nfor low-rank optimization, such as conditional gradient-type methods, which are\noften applied to a smooth approximation of the nonsmooth objective, are slow\nsince their runtime scales with both the large Lipshitz parameter of the\nsmoothed gradient vector and with $1/\\epsilon$. In this paper we develop\nefficient algorithms for \\textit{stochastic} optimization of a strongly-convex\nobjective which includes both a nonsmooth term and a low-rank promoting term.\nIn particular, to the best of our knowledge, we present the first algorithm\nthat enjoys all following critical properties for large-scale problems: i)\n(nearly) optimal sample complexity, ii) each iteration requires only a single\n\\textit{low-rank} SVD computation, and iii) overall number of thin-SVD\ncomputations scales only with $\\log{1/\\epsilon}$ (as opposed to\n$\\textrm{poly}(1/\\epsilon)$ in previous methods). We also give an algorithm for\nthe closely-related finite-sum setting. At the heart of our results lie a novel\ncombination of a variance-reduction technique and the use of a\n\\textit{weak-proximal oracle} which is key to obtaining all above three\nproperties simultaneously. \n\n"}
{"id": "1809.10491", "contents": "Title: On the Regret Minimization of Nonconvex Online Gradient Ascent for\n  Online PCA Abstract: In this paper we focus on the problem of Online Principal Component Analysis\nin the regret minimization framework. For this problem, all existing regret\nminimization algorithms for the fully-adversarial setting are based on a\npositive semidefinite convex relaxation, and hence require quadratic memory and\nSVD computation (either thin of full) on each iteration, which amounts to at\nleast quadratic runtime per iteration. This is in stark contrast to a\ncorresponding stochastic i.i.d. variant of the problem, which was studied\nextensively lately, and admits very efficient gradient ascent algorithms that\nwork directly on the natural non-convex formulation of the problem, and hence\nrequire only linear memory and linear runtime per iteration. This raises the\nquestion: can non-convex online gradient ascent algorithms be shown to minimize\nregret in online adversarial settings? In this paper we take a step forward\ntowards answering this question. We introduce an\n\\textit{adversarially-perturbed spiked-covariance model} in which, each data\npoint is assumed to follow a fixed stochastic distribution with a non-zero\nspectral gap in the covariance matrix, but is then perturbed with some\nadversarial vector. This model is a natural extension of a well studied\nstandard stochastic setting that allows for non-stationary (adversarial)\npatterns to arise in the data and hence, might serve as a significantly better\napproximation for real-world data-streams. We show that in an interesting\nregime of parameters, when the non-convex online gradient ascent algorithm is\ninitialized with a \"warm-start\" vector, it provably minimizes the regret with\nhigh probability. We further discuss the possibility of computing such a\n\"warm-start\" vector, and also the use of regularization to obtain fast regret\nrates. Our theoretical findings are supported by empirical experiments on both\nsynthetic and real-world data. \n\n"}
{"id": "1809.10842", "contents": "Title: Learning and Planning with a Semantic Model Abstract: Building deep reinforcement learning agents that can generalize and adapt to\nunseen environments remains a fundamental challenge for AI. This paper\ndescribes progresses on this challenge in the context of man-made environments,\nwhich are visually diverse but contain intrinsic semantic regularities. We\npropose a hybrid model-based and model-free approach, LEArning and Planning\nwith Semantics (LEAPS), consisting of a multi-target sub-policy that acts on\nvisual inputs, and a Bayesian model over semantic structures. When placed in an\nunseen environment, the agent plans with the semantic model to make high-level\ndecisions, proposes the next sub-target for the sub-policy to execute, and\nupdates the semantic model based on new observations. We perform experiments in\nvisual navigation tasks using House3D, a 3D environment that contains diverse\nhuman-designed indoor scenes with real-world objects. LEAPS outperforms strong\nbaselines that do not explicitly plan using the semantic content. \n\n"}
{"id": "1810.00123", "contents": "Title: Generalization and Regularization in DQN Abstract: Deep reinforcement learning algorithms have shown an impressive ability to\nlearn complex control policies in high-dimensional tasks. However, despite the\never-increasing performance on popular benchmarks, policies learned by deep\nreinforcement learning algorithms can struggle to generalize when evaluated in\nremarkably similar environments. In this paper we propose a protocol to\nevaluate generalization in reinforcement learning through different modes of\nAtari 2600 games. With that protocol we assess the generalization capabilities\nof DQN, one of the most traditional deep reinforcement learning algorithms, and\nwe provide evidence suggesting that DQN overspecializes to the training\nenvironment. We then comprehensively evaluate the impact of dropout and\n$\\ell_2$ regularization, as well as the impact of reusing learned\nrepresentations to improve the generalization capabilities of DQN. Despite\nregularization being largely underutilized in deep reinforcement learning, we\nshow that it can, in fact, help DQN learn more general features. These features\ncan be reused and fine-tuned on similar tasks, considerably improving DQN's\nsample efficiency. \n\n"}
{"id": "1810.00361", "contents": "Title: Using State Predictions for Value Regularization in Curiosity Driven\n  Deep Reinforcement Learning Abstract: Learning in sparse reward settings remains a challenge in Reinforcement\nLearning, which is often addressed by using intrinsic rewards. One promising\nstrategy is inspired by human curiosity, requiring the agent to learn to\npredict the future. In this paper a curiosity-driven agent is extended to use\nthese predictions directly for training. To achieve this, the agent predicts\nthe value function of the next state at any point in time. Subsequently, the\nconsistency of this prediction with the current value function is measured,\nwhich is then used as a regularization term in the loss function of the\nalgorithm. Experiments were made on grid-world environments as well as on a 3D\nnavigation task, both with sparse rewards. In the first case the extended agent\nis able to learn significantly faster than the baselines. \n\n"}
{"id": "1810.01176", "contents": "Title: EMI: Exploration with Mutual Information Abstract: Reinforcement learning algorithms struggle when the reward signal is very\nsparse. In these cases, naive random exploration methods essentially rely on a\nrandom walk to stumble onto a rewarding state. Recent works utilize intrinsic\nmotivation to guide the exploration via generative models, predictive forward\nmodels, or discriminative modeling of novelty. We propose EMI, which is an\nexploration method that constructs embedding representation of states and\nactions that does not rely on generative decoding of the full observation but\nextracts predictive signals that can be used to guide exploration based on\nforward prediction in the representation space. Our experiments show\ncompetitive results on challenging locomotion tasks with continuous control and\non image-based exploration tasks with discrete actions on Atari. The source\ncode is available at https://github.com/snu-mllab/EMI . \n\n"}
{"id": "1810.01266", "contents": "Title: Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented\n  Demonstrations using Directed Information Abstract: The use of imitation learning to learn a single policy for a complex task\nthat has multiple modes or hierarchical structure can be challenging. In fact,\nprevious work has shown that when the modes are known, learning separate\npolicies for each mode or sub-task can greatly improve the performance of\nimitation learning. In this work, we discover the interaction between sub-tasks\nfrom their resulting state-action trajectory sequences using a directed\ngraphical model. We propose a new algorithm based on the generative adversarial\nimitation learning framework which automatically learns sub-task policies from\nunsegmented demonstrations. Our approach maximizes the directed information\nflow in the graphical model between sub-task latent variables and their\ngenerated trajectories. We also show how our approach connects with the\nexisting Options framework, which is commonly used to learn hierarchical\npolicies. \n\n"}
{"id": "1810.01436", "contents": "Title: Efficient Estimation of Equilibria of Large Congestion Games with\n  Heterogeneous Players Abstract: Computing an equilibrium in congestion games can be challenging when the\nnumber of players is large. Yet, it is a problem to be addressed in practice,\nfor instance to forecast the state of the system and be able to control it. In\nthis work, we analyze the case of generalized atomic congestion games, with\ncoupling constraints, and with players that are heterogeneous through their\naction sets and their utility functions. We obtain an approximation of the\nvariational Nash equilibria---a notion generalizing Nash equilibria in the\npresence of coupling constraints---of a large atomic congestion game by an\nequilibrium of an auxiliary population game, where each population corresponds\nto a group of atomic players of the initial game. Because the variational\ninequalities characterizing the equilibrium of the auxiliary game have smaller\ndimension than the original problem, this approach enables the fast computation\nof an estimation of equilibria in a large congestion game with thousands of\nheterogeneous players. \n\n"}
{"id": "1810.01920", "contents": "Title: Generalized Inverse Optimization through Online Learning Abstract: Inverse optimization is a powerful paradigm for learning preferences and\nrestrictions that explain the behavior of a decision maker, based on a set of\nexternal signal and the corresponding decision pairs. However, most inverse\noptimization algorithms are designed specifically in batch setting, where all\nthe data is available in advance. As a consequence, there has been rare use of\nthese methods in an online setting suitable for real-time applications. In this\npaper, we propose a general framework for inverse optimization through online\nlearning. Specifically, we develop an online learning algorithm that uses an\nimplicit update rule which can handle noisy data. Moreover, under additional\nregularity assumptions in terms of the data and the model, we prove that our\nalgorithm converges at a rate of $\\mathcal{O}(1/\\sqrt{T})$ and is statistically\nconsistent. In our experiments, we show the online learning approach can learn\nthe parameters with great accuracy and is very robust to noises, and achieves a\ndramatic improvement in computational efficacy over the batch learning\napproach. \n\n"}
{"id": "1810.02054", "contents": "Title: Gradient Descent Provably Optimizes Over-parameterized Neural Networks Abstract: One of the mysteries in the success of neural networks is randomly\ninitialized first order methods like gradient descent can achieve zero training\nloss even though the objective function is non-convex and non-smooth. This\npaper demystifies this surprising phenomenon for two-layer fully connected ReLU\nactivated neural networks. For an $m$ hidden node shallow neural network with\nReLU activation and $n$ training data, we show as long as $m$ is large enough\nand no two inputs are parallel, randomly initialized gradient descent converges\nto a globally optimal solution at a linear convergence rate for the quadratic\nloss function.\n  Our analysis relies on the following observation: over-parameterization and\nrandom initialization jointly restrict every weight vector to be close to its\ninitialization for all iterations, which allows us to exploit a strong\nconvexity-like property to show that gradient descent converges at a global\nlinear rate to the global optimum. We believe these insights are also useful in\nanalyzing deep models and other first order methods. \n\n"}
{"id": "1810.02066", "contents": "Title: Turning Lemons into Peaches using Secure Computation Abstract: In many cases, assessing the quality of goods is hard. For example, when\npurchasing a car, it is hard to measure how pollutant the car is since there\nare infinitely many driving conditions to be tested. Typically, these\nsituations are considered under the umbrella of information asymmetry and as\nAkelrof showed may lead to a market of lemons. However, we argue that in many\nof these situations, the problem is not the missing information but the\ncomputational challenge of obtaining it. In a nut-shell, if verifying the value\nof goods requires a large amount of computation or even infinite amounts of\ncomputation, the buyer is forced to use a finite test that samples, in some\nsense, the quality of the goods. However, if the seller knows the test, then\nthe seller can over-fit the test and create goods that pass the quality test\ndespite not having the desired quality. We show different solutions to this\nsituation including a novel approach that uses secure computation to hide the\ntest from the seller to prevent over-fitting. \n\n"}
{"id": "1810.03063", "contents": "Title: Solving Large Sequential Games with the Excessive Gap Technique Abstract: There has been tremendous recent progress on equilibrium-finding algorithms\nfor zero-sum imperfect-information extensive-form games, but there has been a\npuzzling gap between theory and practice. First-order methods have\nsignificantly better theoretical convergence rates than any\ncounterfactual-regret minimization (CFR) variant. Despite this, CFR variants\nhave been favored in practice. Experiments with first-order methods have only\nbeen conducted on small- and medium-sized games because those methods are\ncomplicated to implement in this setting, and because CFR variants have been\nenhanced extensively for over a decade they perform well in practice. In this\npaper we show that a particular first-order method, a state-of-the-art variant\nof the excessive gap technique---instantiated with the dilated entropy distance\nfunction---can efficiently solve large real-world problems competitively with\nCFR and its variants. We show this on large endgames encountered by the\nLibratus poker AI, which recently beat top human poker specialist professionals\nat no-limit Texas hold'em. We show experimental results on our variant of the\nexcessive gap technique as well as a prior version. We introduce a numerically\nfriendly implementation of the smoothed best response computation associated\nwith first-order methods for extensive-form game solving. We present, to our\nknowledge, the first GPU implementation of a first-order method for\nextensive-form games. We present comparisons of several excessive gap technique\nand CFR variants. \n\n"}
{"id": "1810.03064", "contents": "Title: CSI-Net: Unified Human Body Characterization and Pose Recognition Abstract: We build CSI-Net, a unified Deep Neural Network~(DNN), to learn the\nrepresentation of WiFi signals. Using CSI-Net, we jointly solved two body\ncharacterization problems: biometrics estimation (including body fat, muscle,\nwater, and bone rates) and person recognition. We also demonstrated the\napplication of CSI-Net on two distinctive pose recognition tasks: the hand sign\nrecognition (fine-scaled action of the hand) and falling detection\n(coarse-scaled motion of the body). \n\n"}
{"id": "1810.03538", "contents": "Title: Combinatorial Attacks on Binarized Neural Networks Abstract: Binarized Neural Networks (BNNs) have recently attracted significant interest\ndue to their computational efficiency. Concurrently, it has been shown that\nneural networks may be overly sensitive to \"attacks\" - tiny adversarial changes\nin the input - which may be detrimental to their use in safety-critical\ndomains. Designing attack algorithms that effectively fool trained models is a\nkey step towards learning robust neural networks. The discrete,\nnon-differentiable nature of BNNs, which distinguishes them from their\nfull-precision counterparts, poses a challenge to gradient-based attacks. In\nthis work, we study the problem of attacking a BNN through the lens of\ncombinatorial and integer optimization. We propose a Mixed Integer Linear\nProgramming (MILP) formulation of the problem. While exact and flexible, the\nMILP quickly becomes intractable as the network and perturbation space grow. To\naddress this issue, we propose IProp, a decomposition-based algorithm that\nsolves a sequence of much smaller MILP problems. Experimentally, we evaluate\nboth proposed methods against the standard gradient-based attack (FGSM) on\nMNIST and Fashion-MNIST, and show that IProp performs favorably compared to\nFGSM, while scaling beyond the limits of the MILP. \n\n"}
{"id": "1810.04038", "contents": "Title: Understanding and Improving Recurrent Networks for Human Activity\n  Recognition by Continuous Attention Abstract: Deep neural networks, including recurrent networks, have been successfully\napplied to human activity recognition. Unfortunately, the final representation\nlearned by recurrent networks might encode some noise (irrelevant signal\ncomponents, unimportant sensor modalities, etc.). Besides, it is difficult to\ninterpret the recurrent networks to gain insight into the models' behavior. To\naddress these issues, we propose two attention models for human activity\nrecognition: temporal attention and sensor attention. These two mechanisms\nadaptively focus on important signals and sensor modalities. To further improve\nthe understandability and mean F1 score, we add continuity constraints,\nconsidering that continuous sensor signals are more robust than discrete ones.\nWe evaluate the approaches on three datasets and obtain state-of-the-art\nresults. Furthermore, qualitative analysis shows that the attention learned by\nthe models agree well with human intuition. \n\n"}
{"id": "1810.05236", "contents": "Title: Practical Design Space Exploration Abstract: Multi-objective optimization is a crucial matter in computer systems design\nspace exploration because real-world applications often rely on a trade-off\nbetween several objectives. Derivatives are usually not available or\nimpractical to compute and the feasibility of an experiment can not always be\ndetermined in advance. These problems are particularly difficult when the\nfeasible region is relatively small, and it may be prohibitive to even find a\nfeasible experiment, let alone an optimal one.\n  We introduce a new methodology and corresponding software framework,\nHyperMapper 2.0, which handles multi-objective optimization, unknown\nfeasibility constraints, and categorical/ordinal variables. This new\nmethodology also supports injection of the user prior knowledge in the search\nwhen available. All of these features are common requirements in computer\nsystems but rarely exposed in existing design space exploration systems. The\nproposed methodology follows a white-box model which is simple to understand\nand interpret (unlike, for example, neural networks) and can be used by the\nuser to better understand the results of the automatic search.\n  We apply and evaluate the new methodology to the automatic static tuning of\nhardware accelerators within the recently introduced Spatial programming\nlanguage, with minimization of design run-time and compute logic under the\nconstraint of the design fitting in a target field-programmable gate array\nchip. Our results show that HyperMapper 2.0 provides better Pareto fronts\ncompared to state-of-the-art baselines, with better or competitive hypervolume\nindicator and with 8x improvement in sampling budget for most of the benchmarks\nexplored. \n\n"}
{"id": "1810.05533", "contents": "Title: Empowerment-driven Exploration using Mutual Information Estimation Abstract: Exploration is a difficult challenge in reinforcement learning and is of\nprime importance in sparse reward environments. However, many of the state of\nthe art deep reinforcement learning algorithms, that rely on epsilon-greedy,\nfail on these environments. In such cases, empowerment can serve as an\nintrinsic reward signal to enable the agent to maximize the influence it has\nover the near future. We formulate empowerment as the channel capacity between\nstates and actions and is calculated by estimating the mutual information\nbetween the actions and the following states. The mutual information is\nestimated using Mutual Information Neural Estimator and a forward dynamics\nmodel. We demonstrate that an empowerment driven agent is able to improve\nsignificantly the score of a baseline DQN agent on the game of Montezuma's\nRevenge. \n\n"}
{"id": "1810.07155", "contents": "Title: Hunting for Discriminatory Proxies in Linear Regression Models Abstract: A machine learning model may exhibit discrimination when used to make\ndecisions involving people. One potential cause for such outcomes is that the\nmodel uses a statistical proxy for a protected demographic attribute. In this\npaper we formulate a definition of proxy use for the setting of linear\nregression and present algorithms for detecting proxies. Our definition follows\nrecent work on proxies in classification models, and characterizes a model's\nconstituent behavior that: 1) correlates closely with a protected random\nvariable, and 2) is causally influential in the overall behavior of the model.\nWe show that proxies in linear regression models can be efficiently identified\nby solving a second-order cone program, and further extend this result to\naccount for situations where the use of a certain input variable is justified\nas a `business necessity'. Finally, we present empirical results on two law\nenforcement datasets that exhibit varying degrees of racial disparity in\nprediction outcomes, demonstrating that proxies shed useful light on the causes\nof discriminatory behavior in models. \n\n"}
{"id": "1810.08575", "contents": "Title: Supervising strong learners by amplifying weak experts Abstract: Many real world learning tasks involve complex or hard-to-specify objectives,\nand using an easier-to-specify proxy can lead to poor performance or misaligned\nbehavior. One solution is to have humans provide a training signal by\ndemonstrating or judging performance, but this approach fails if the task is\ntoo complicated for a human to directly evaluate. We propose Iterated\nAmplification, an alternative training strategy which progressively builds up a\ntraining signal for difficult problems by combining solutions to easier\nsubproblems. Iterated Amplification is closely related to Expert Iteration\n(Anthony et al., 2017; Silver et al., 2017), except that it uses no external\nreward function. We present results in algorithmic environments, showing that\nIterated Amplification can efficiently learn complex behaviors. \n\n"}
{"id": "1810.08810", "contents": "Title: The Frontiers of Fairness in Machine Learning Abstract: The last few years have seen an explosion of academic and popular interest in\nalgorithmic fairness. Despite this interest and the volume and velocity of work\nthat has been produced recently, the fundamental science of fairness in machine\nlearning is still in a nascent state. In March 2018, we convened a group of\nexperts as part of a CCC visioning workshop to assess the state of the field,\nand distill the most promising research directions going forward. This report\nsummarizes the findings of that workshop. Along the way, it surveys recent\ntheoretical work in the field and points towards promising directions for\nresearch. \n\n"}
{"id": "1810.09656", "contents": "Title: Hierarchical Approaches for Reinforcement Learning in Parameterized\n  Action Space Abstract: We explore Deep Reinforcement Learning in a parameterized action space.\nSpecifically, we investigate how to achieve sample-efficient end-to-end\ntraining in these tasks. We propose a new compact architecture for the tasks\nwhere the parameter policy is conditioned on the output of the discrete action\npolicy. We also propose two new methods based on the state-of-the-art\nalgorithms Trust Region Policy Optimization (TRPO) and Stochastic Value\nGradient (SVG) to train such an architecture. We demonstrate that these methods\noutperform the state of the art method, Parameterized Action DDPG, on test\ndomains. \n\n"}
{"id": "1810.09832", "contents": "Title: Mechanism Design for Social Good Abstract: Across various domains--such as health, education, and housing--improving\nsocietal welfare involves allocating resources, setting policies, targeting\ninterventions, and regulating activities. These solutions have an immense\nimpact on the day-to-day lives of individuals, whether in the form of access to\nquality healthcare, labor market outcomes, or how votes are accounted for in a\ndemocratic society. Problems that can have an out-sized impact on individuals\nwhose opportunities have historically been limited often pose conceptual and\ntechnical challenges, requiring insights from many disciplines. Conversely, the\nlack of interdisciplinary approach can leave these urgent needs unaddressed and\ncan even exacerbate underlying socioeconomic inequalities. To realize the\nopportunities in these domains, we need to correctly set objectives and reason\nabout human behavior and actions. Doing so requires a deep grounding in the\nfield of interest and collaboration with domain experts who understand the\nsocietal implications and feasibility of proposed solutions. These insights can\nplay an instrumental role in proposing algorithmically-informed policies.\n  In this article, we describe the Mechanism Design for Social Good (MD4SG)\nresearch agenda, which involves using insights from algorithms, optimization,\nand mechanism design to improve access to opportunity. The MD4SG research\ncommunity takes an interdisciplinary, multi-stakeholder approach to improve\nsocietal welfare. We discuss three exciting research avenues within MD4SG\nrelated to improving access to opportunity in the developing world, labor\nmarkets and discrimination, and housing. For each of these, we showcase ongoing\nwork, underline new directions, and discuss potential for implementing existing\nwork in practice. \n\n"}
{"id": "1810.10132", "contents": "Title: Smoothed Online Optimization for Regression and Control Abstract: We consider Online Convex Optimization (OCO) in the setting where the costs\nare $m$-strongly convex and the online learner pays a switching cost for\nchanging decisions between rounds. We show that the recently proposed Online\nBalanced Descent (OBD) algorithm is constant competitive in this setting, with\ncompetitive ratio $3 + O(1/m)$, irrespective of the ambient dimension.\nAdditionally, we show that when the sequence of cost functions is\n$\\epsilon$-smooth, OBD has near-optimal dynamic regret and maintains strong\nper-round accuracy. We demonstrate the generality of our approach by showing\nthat the OBD framework can be used to construct competitive algorithms for a\nvariety of online problems across learning and control, including online\nvariants of ridge regression, logistic regression, maximum likelihood\nestimation, and LQR control. \n\n"}
{"id": "1810.10900", "contents": "Title: On Policies for Single-leg Revenue Management with Limited Demand\n  Information Abstract: In this paper we study the single-item revenue management problem, with no\ninformation given about the demand trajectory over time. When the item is sold\nthrough accepting/rejecting different fare classes, Ball and Queyranne (2009)\nhave established the tight competitive ratio for this problem using booking\nlimit policies, which raise the acceptance threshold as the remaining inventory\ndwindles. However, when the item is sold through dynamic pricing instead, there\nis the additional challenge that offering a low price may entice high-paying\ncustomers to substitute down. We show that despite this challenge, the same\ncompetitive ratio can still be achieved using a randomized dynamic pricing\npolicy. Our policy incorporates the price-skimming technique from Eren and\nMaglaras (2010), but importantly we show how the randomized price distribution\nshould be stochastically-increased as the remaining inventory dwindles. A key\ntechnical ingredient in our policy is a new \"valuation tracking\" subroutine,\nwhich tracks the possible values for the optimum, and follows the most\n\"inventory-conservative\" control which maintains the desired competitive ratio.\nFinally, we demonstrate the empirical effectiveness of our policy in\nsimulations, where its average-case performance surpasses all naive\nmodifications of the existing policies. \n\n"}
{"id": "1810.10952", "contents": "Title: Differential Variable Speed Limits Control for Freeway Recurrent\n  Bottlenecks via Deep Reinforcement learning Abstract: Variable speed limits (VSL) control is a flexible way to improve traffic\ncondition,increase safety and reduce emission. There is an emerging trend of\nusing reinforcement learning technique for VSL control and recent studies have\nshown promising results. Currently, deep learning is enabling reinforcement\nlearning to develope autonomous control agents for problems that were\npreviously intractable. In this paper, we propose a more effective deep\nreinforcement learning (DRL) model for differential variable speed limits\n(DVSL) control, in which the dynamic and different speed limits among lanes can\nbe imposed. The proposed DRL models use a novel actor-critic architecture which\ncan learn a large number of discrete speed limits in a continues action space.\nDifferent reward signals, e.g. total travel time, bottleneck speed, emergency\nbraking, and vehicular emission are used to train the DVSL controller, and\ncomparison between these reward signals are conducted. We test proposed DRL\nbaased DVSL controllers on a simulated freeway recurrent bottleneck. Results\nshow that the efficiency, safety and emissions can be improved by the proposed\nmethod. We also show some interesting findings through the visulization of the\ncontrol policies generated from DRL models. \n\n"}
{"id": "1810.11059", "contents": "Title: Uniform Convergence of Gradients for Non-Convex Learning and\n  Optimization Abstract: We investigate 1) the rate at which refined properties of the empirical\nrisk---in particular, gradients---converge to their population counterparts in\nstandard non-convex learning tasks, and 2) the consequences of this convergence\nfor optimization. Our analysis follows the tradition of norm-based capacity\ncontrol. We propose vector-valued Rademacher complexities as a simple,\ncomposable, and user-friendly tool to derive dimension-free uniform convergence\nbounds for gradients in non-convex learning problems. As an application of our\ntechniques, we give a new analysis of batch gradient descent methods for\nnon-convex generalized linear models and non-convex robust regression, showing\nhow to use any algorithm that finds approximate stationary points to obtain\noptimal sample complexity, even when dimension is high or possibly infinite and\nmultiple passes over the dataset are allowed.\n  Moving to non-smooth models we show----in contrast to the smooth case---that\neven for a single ReLU it is not possible to obtain dimension-independent\nconvergence rates for gradients in the worst case. On the positive side, it is\nstill possible to obtain dimension-independent rates under a new type of\ndistributional assumption. \n\n"}
{"id": "1810.11702", "contents": "Title: Multi-Agent Common Knowledge Reinforcement Learning Abstract: Cooperative multi-agent reinforcement learning often requires decentralised\npolicies, which severely limit the agents' ability to coordinate their\nbehaviour. In this paper, we show that common knowledge between agents allows\nfor complex decentralised coordination. Common knowledge arises naturally in a\nlarge number of decentralised cooperative multi-agent tasks, for example, when\nagents can reconstruct parts of each others' observations. Since agents an\nindependently agree on their common knowledge, they can execute complex\ncoordinated policies that condition on this knowledge in a fully decentralised\nfashion. We propose multi-agent common knowledge reinforcement learning\n(MACKRL), a novel stochastic actor-critic algorithm that learns a hierarchical\npolicy tree. Higher levels in the hierarchy coordinate groups of agents by\nconditioning on their common knowledge, or delegate to lower levels with\nsmaller subgroups but potentially richer common knowledge. The entire policy\ntree can be executed in a fully decentralised fashion. As the lowest policy\ntree level consists of independent policies for each agent, MACKRL reduces to\nindependently learnt decentralised policies as a special case. We demonstrate\nthat our method can exploit common knowledge for superior performance on\ncomplex decentralised coordination tasks, including a stochastic matrix game\nand challenging problems in StarCraft II unit micromanagement. \n\n"}
{"id": "1810.12069", "contents": "Title: Marginal Replay vs Conditional Replay for Continual Learning Abstract: We present a new replay-based method of continual classification learning\nthat we term \"conditional replay\" which generates samples and labels together\nby sampling from a distribution conditioned on the class. We compare\nconditional replay to another\n  replay-based continual learning paradigm (which we term \"marginal replay\")\nthat generates samples independently of their class and assigns labels in a\nseparate step.\n  The main improvement in conditional replay is that labels for generated\nsamples need not be inferred, which reduces the margin for error in complex\ncontinual classification learning tasks. We demonstrate the effectiveness of\nthis approach using novel and standard benchmarks constructed from MNIST and\nFashionMNIST data, and compare to the regularization-based \\textit{elastic\nweight consolidation} (EWC) method. \n\n"}
{"id": "1810.12660", "contents": "Title: Evolutionarily Stable Preferences Against Multiple Mutations in\n  Multi-player Games Abstract: We use the indirect evolutionary approach to study evolutionarily stable\npreferences against multiple mutations in single- and multi-population\nsettings, respectively. In both settings, n-player games are played. Each\nindividual has subjective preferences over potential outcomes, which may differ\nfrom the actual fitness values given by a material payoff function. In the two\npopulation settings, respectively, we examine necessary and sufficient\nconditions for evolutionary stability against multiple mutations; we reveal the\nconnection between the order of stability and the level of efficiency in\n$n$-player games. \n\n"}
{"id": "1811.00521", "contents": "Title: Minimizing Close-k Aggregate Loss Improves Classification Abstract: In classification, the de facto method for aggregating individual losses is\nthe average loss. When the actual metric of interest is 0-1 loss, it is common\nto minimize the average surrogate loss for some well-behaved (e.g. convex)\nsurrogate. Recently, several other aggregate losses such as the maximal loss\nand average top-$k$ loss were proposed as alternative objectives to address\nshortcomings of the average loss. However, we identify common classification\nsettings, e.g. the data is imbalanced, has too many easy or ambiguous examples,\netc., when average, maximal and average top-$k$ all suffer from suboptimal\ndecision boundaries, even on an infinitely large training set. To address this\nproblem, we propose a new classification objective called the close-$k$\naggregate loss, where we adaptively minimize the loss for points close to the\ndecision boundary. We provide theoretical guarantees for the 0-1 accuracy when\nwe optimize close-$k$ aggregate loss. We also conduct systematic experiments\nacross the PMLB and OpenML benchmark datasets. Close-$k$ achieves significant\ngains in 0-1 test accuracy, improvements of $\\geq 2$% and $p<0.05$, in over 25%\nof the datasets compared to average, maximal and average top-$k$. In contrast,\nthe previous aggregate losses outperformed close-$k$ in less than 2% of the\ndatasets. \n\n"}
{"id": "1811.01557", "contents": "Title: Representation Learning by Reconstructing Neighborhoods Abstract: Since its introduction, unsupervised representation learning has attracted a\nlot of attention from the research community, as it is demonstrated to be\nhighly effective and easy-to-apply in tasks such as dimension reduction,\nclustering, visualization, information retrieval, and semi-supervised learning.\nIn this work, we propose a novel unsupervised representation learning framework\ncalled neighbor-encoder, in which domain knowledge can be easily incorporated\ninto the learning process without modifying the general encoder-decoder\narchitecture of the classic autoencoder.In contrast to autoencoder, which\nreconstructs the input data itself, neighbor-encoder reconstructs the input\ndata's neighbors. As the proposed representation learning problem is\nessentially a neighbor reconstruction problem, domain knowledge can be easily\nincorporated in the form of an appropriate definition of similarity between\nobjects. Based on that observation, our framework can leverage any\noff-the-shelf similarity search algorithms or side information to find the\nneighbor of an input object. Applications of other algorithms (e.g.,\nassociation rule mining) in our framework are also possible, given that the\nappropriate definition of neighbor can vary in different contexts. We have\ndemonstrated the effectiveness of our framework in many diverse domains,\nincluding images, text, and time series, and for various data mining tasks\nincluding classification, clustering, and visualization. Experimental results\nshow that neighbor-encoder not only outperforms autoencoder in most of the\nscenarios we consider, but also achieves the state-of-the-art performance on\ntext document clustering. \n\n"}
{"id": "1811.01662", "contents": "Title: Matrix Completion With Variational Graph Autoencoders: Application in\n  Hyperlocal Air Quality Inference Abstract: Inferring air quality from a limited number of observations is an essential\ntask for monitoring and controlling air pollution. Existing inference methods\ntypically use low spatial resolution data collected by fixed monitoring\nstations and infer the concentration of air pollutants using additional types\nof data, e.g., meteorological and traffic information. In this work, we focus\non street-level air quality inference by utilizing data collected by mobile\nstations. We formulate air quality inference in this setting as a graph-based\nmatrix completion problem and propose a novel variational model based on graph\nconvolutional autoencoders. Our model captures effectively the spatio-temporal\ncorrelation of the measurements and does not depend on the availability of\nadditional information apart from the street-network topology. Experiments on a\nreal air quality dataset, collected with mobile stations, shows that the\nproposed model outperforms state-of-the-art approaches. \n\n"}
{"id": "1811.02351", "contents": "Title: An Incentive Analysis of some Bitcoin Fee Designs Abstract: In the Bitcoin system, miners are incentivized to join the system and\nvalidate transactions through fees paid by the users. A simple \"pay your bid\"\nauction has been employed to determine the transaction fees. Recently, Lavi,\nSattath and Zohar [LSZ17] proposed an alternative fee design, called the\nmonopolistic price (MP) mechanism, aimed at improving the revenue for the\nminers. Although MP is not strictly incentive compatible (IC), they studied how\nclose to IC the mechanism is for iid distributions, and conjectured that it is\nnearly IC asymptotically based on extensive simulations and some analysis. In\nthis paper, we prove that the MP mechanism is nearly incentive compatible for\nany iid distribution as the number of users grows large. This holds true with\nrespect to other attacks such as splitting bids. We also prove a conjecture in\n[LSZ17] that MP dominates the RSOP auction in revenue (originally defined in\nGoldberg et al. [GHKSW06] for digital goods). These results lend support to MP\nas a Bitcoin fee design candidate. Additionally, we explore some possible\nintrinsic correlations between incentive compatibility and revenue in general. \n\n"}
{"id": "1811.03909", "contents": "Title: Evidence Transfer for Improving Clustering Tasks Using External\n  Categorical Evidence Abstract: In this paper we introduce evidence transfer for clustering, a deep learning\nmethod that can incrementally manipulate the latent representations of an\nautoencoder, according to external categorical evidence, in order to improve a\nclustering outcome. By evidence transfer we define the process by which the\ncategorical outcome of an external, auxiliary task is exploited to improve a\nprimary task, in this case representation learning for clustering. Our proposed\nmethod makes no assumptions regarding the categorical evidence presented, nor\nthe structure of the latent space. We compare our method, against the baseline\nsolution by performing k-means clustering before and after its deployment.\nExperiments with three different kinds of evidence show that our method\neffectively manipulates the latent representations when introduced with real\ncorresponding evidence, while remaining robust when presented with low quality\nevidence. \n\n"}
{"id": "1811.04063", "contents": "Title: On convexity and solution concepts in cooperative interval games Abstract: Cooperative interval game is a cooperative game in which every coalition gets\nassigned some closed real interval. This models uncertainty about how much the\nmembers of a coalition get for cooperating together.\n  In this paper we study convexity, core and the Shapley value of games with\ninterval uncertainty.\n  Our motivation to do so is twofold. First, we want to capture which\nproperties are preserved when we generalize concepts from classical cooperative\ngame theory to interval games. Second, since these generalizations can be done\nin different ways, mainly with regard to the resulting level of uncertainty, we\ntry to compare them and show their relation to each other. \n\n"}
{"id": "1811.04376", "contents": "Title: Explaining Deep Learning Models using Causal Inference Abstract: Although deep learning models have been successfully applied to a variety of\ntasks, due to the millions of parameters, they are becoming increasingly opaque\nand complex. In order to establish trust for their widespread commercial use,\nit is important to formalize a principled framework to reason over these\nmodels. In this work, we use ideas from causal inference to describe a general\nframework to reason over CNN models. Specifically, we build a Structural Causal\nModel (SCM) as an abstraction over a specific aspect of the CNN. We also\nformulate a method to quantitatively rank the filters of a convolution layer\naccording to their counterfactual importance. We illustrate our approach with\npopular CNN architectures such as LeNet5, VGG19, and ResNet32. \n\n"}
{"id": "1811.04872", "contents": "Title: Pareto-Optimal Allocation of Indivisible Goods with Connectivity\n  Constraints Abstract: We study the problem of allocating indivisible items to agents with additive\nvaluations, under the additional constraint that bundles must be connected in\nan underlying item graph. Previous work has considered the existence and\ncomplexity of fair allocations. We study the problem of finding an allocation\nthat is Pareto-optimal. While it is easy to find an efficient allocation when\nthe underlying graph is a path or a star, the problem is NP-hard for many other\ngraph topologies, even for trees of bounded pathwidth or of maximum degree 3.\nWe show that on a path, there are instances where no Pareto-optimal allocation\nsatisfies envy-freeness up to one good, and that it is NP-hard to decide\nwhether such an allocation exists, even for binary valuations. We also show\nthat, for a path, it is NP-hard to find a Pareto-optimal allocation that\nsatisfies maximin share, but show that a moving-knife algorithm can find such\nan allocation when agents have binary valuations that have a non-nested\ninterval structure. \n\n"}
{"id": "1811.05321", "contents": "Title: Correction of AI systems by linear discriminants: Probabilistic\n  foundations Abstract: Artificial Intelligence (AI) systems sometimes make errors and will make\nerrors in the future, from time to time. These errors are usually unexpected,\nand can lead to dramatic consequences. Intensive development of AI and its\npractical applications makes the problem of errors more important. Total\nre-engineering of the systems can create new errors and is not always possible\ndue to the resources involved. The important challenge is to develop fast\nmethods to correct errors without damaging existing skills. We formulated the\ntechnical requirements to the 'ideal' correctors. Such correctors include\nbinary classifiers, which separate the situations with high risk of errors from\nthe situations where the AI systems work properly. Surprisingly, for\nessentially high-dimensional data such methods are possible: simple linear\nFisher discriminant can separate the situations with errors from correctly\nsolved tasks even for exponentially large samples. The paper presents the\nprobabilistic basis for fast non-destructive correction of AI systems. A series\nof new stochastic separation theorems is proven. These theorems provide new\ninstruments for fast non-iterative correction of errors of legacy AI systems.\nThe new approaches become efficient in high-dimensions, for correction of\nhigh-dimensional systems in high-dimensional world (i.e. for processing of\nessentially high-dimensional data by large systems). \n\n"}
{"id": "1811.05590", "contents": "Title: Emergence of Addictive Behaviors in Reinforcement Learning Agents Abstract: This paper presents a novel approach to the technical analysis of wireheading\nin intelligent agents. Inspired by the natural analogues of wireheading and\ntheir prevalent manifestations, we propose the modeling of such phenomenon in\nReinforcement Learning (RL) agents as psychological disorders. In a preliminary\nstep towards evaluating this proposal, we study the feasibility and dynamics of\nemergent addictive policies in Q-learning agents in the tractable environment\nof the game of Snake. We consider a slightly modified settings for this game,\nin which the environment provides a \"drug\" seed alongside the original\n\"healthy\" seed for the consumption of the snake. We adopt and extend an\nRL-based model of natural addiction to Q-learning agents in this settings, and\nderive sufficient parametric conditions for the emergence of addictive\nbehaviors in such agents. Furthermore, we evaluate our theoretical analysis\nwith three sets of simulation-based experiments. The results demonstrate the\nfeasibility of addictive wireheading in RL agents, and provide promising venues\nof further research on the psychopathological modeling of complex AI safety\nproblems. \n\n"}
{"id": "1811.06889", "contents": "Title: On the Complexity of Exploration in Goal-Driven Navigation Abstract: Building agents that can explore their environments intelligently is a\nchallenging open problem. In this paper, we make a step towards understanding\nhow a hierarchical design of the agent's policy can affect its exploration\ncapabilities. First, we design EscapeRoom environments, where the agent must\nfigure out how to navigate to the exit by accomplishing a number of\nintermediate tasks (\\emph{subgoals}), such as finding keys or opening doors.\nOur environments are procedurally generated and vary in complexity, which can\nbe controlled by the number of subgoals and relationships between them. Next,\nwe propose to measure the complexity of each environment by constructing\ndependency graphs between the goals and analytically computing \\emph{hitting\ntimes} of a random walk in the graph. We empirically evaluate Proximal Policy\nOptimization (PPO) with sparse and shaped rewards, a variation of policy\nsketches, and a hierarchical version of PPO (called HiPPO) akin to h-DQN. We\nshow that analytically estimated \\emph{hitting time} in goal dependency graphs\nis an informative metric of the environment complexity. We conjecture that the\nresult should hold for environments other than navigation. Finally, we show\nthat solving environments beyond certain level of complexity requires\nhierarchical approaches. \n\n"}
{"id": "1811.07342", "contents": "Title: Transform-Based Multilinear Dynamical System for Tensor Time Series\n  Analysis Abstract: We propose a novel multilinear dynamical system (MLDS) in a transform domain,\nnamed $\\mathcal{L}$-MLDS, to model tensor time series. With transformations\napplied to a tensor data, the latent multidimensional correlations among the\nfrontal slices are built, and thus resulting in the computational independence\nin the transform domain. This allows the exact separability of the\nmulti-dimensional problem into multiple smaller LDS problems. To estimate the\nsystem parameters, we utilize the expectation-maximization (EM) algorithm to\ndetermine the parameters of each LDS. Further, $\\mathcal{L}$-MLDSs\nsignificantly reduce the model parameters and allows parallel processing. Our\ngeneral $\\mathcal{L}$-MLDS model is implemented based on different transforms:\ndiscrete Fourier transform, discrete cosine transform and discrete wavelet\ntransform. Due to the nonlinearity of these transformations, $\\mathcal{L}$-MLDS\nis able to capture the nonlinear correlations within the data unlike the MLDS\n\\cite{rogers2013multilinear} which assumes multi-way linear correlations. Using\nfour real datasets, the proposed $\\mathcal{L}$-MLDS is shown to achieve much\nhigher prediction accuracy than the state-of-the-art MLDS and LDS with an equal\nnumber of parameters under different noise models. In particular, the relative\nerrors are reduced by $50\\% \\sim 99\\%$. Simultaneously, $\\mathcal{L}$-MLDS\nachieves an exponential improvement in the model's training time than MLDS. \n\n"}
{"id": "1811.07569", "contents": "Title: Nash equilibrium seeking in potential games with double-integrator\n  agents Abstract: In this paper, we show the equivalence between a constrained, multi-agent\ncontrol problem, modeled within the port-Hamiltonian framework, and an exact\npotential game. Specifically, critical distance-based constraints determine a\nnetwork of double-integrator agents, which can be represented as a graph.\nVirtual couplings, i.e., pairs of spring-damper, assigned to each edge of the\ngraph, allow to synthesize a distributed, gradient-based control law that\nsteers the network to an invariant set of stable configurations. We\ncharacterize the points belonging to such set as Nash equilibria of the\nassociated potential game, relating the parameters of the virtual couplings\nwith the equilibrium seeking problem, since they are crucial to shape the\ntransient behaviour (i.e., the convergence) and, ideally, the set of reachable\nequilibria. \n\n"}
{"id": "1811.07819", "contents": "Title: Learning Actionable Representations with Goal-Conditioned Policies Abstract: Representation learning is a central challenge across a range of machine\nlearning areas. In reinforcement learning, effective and functional\nrepresentations have the potential to tremendously accelerate learning progress\nand solve more challenging problems. Most prior work on representation learning\nhas focused on generative approaches, learning representations that capture all\nunderlying factors of variation in the observation space in a more disentangled\nor well-ordered manner. In this paper, we instead aim to learn functionally\nsalient representations: representations that are not necessarily complete in\nterms of capturing all factors of variation in the observation space, but\nrather aim to capture those factors of variation that are important for\ndecision making -- that are \"actionable.\" These representations are aware of\nthe dynamics of the environment, and capture only the elements of the\nobservation that are necessary for decision making rather than all factors of\nvariation, without explicit reconstruction of the observation. We show how\nthese representations can be useful to improve exploration for sparse reward\nproblems, to enable long horizon hierarchical reinforcement learning, and as a\nstate representation for learning policies for downstream tasks. We evaluate\nour method on a number of simulated environments, and compare it to prior\nmethods for representation learning, exploration, and hierarchical\nreinforcement learning. \n\n"}
{"id": "1811.08308", "contents": "Title: Economics of disagreement -- financial intuition for the R\\'enyi\n  divergence Abstract: Disagreement is an essential element of science and life in general. The\nlanguage of probabilities and statistics is often used to describe\ndisagreements quantitatively. In practice, however, we want much more than\nthat. We want disagreements to be resolved. This leaves us with a substantial\nknowledge gap which is often perceived as a lack of practical intuition\nregarding probabilistic and statistical concepts.\n  Take for instance the R\\'enyi divergence which is a well-known statistical\nquantity specifically designed as a measure of disagreement between\nprobabilistic models. Despite its widespread use in science and engineering,\nthe R\\'enyi divergence remains a highly abstract axiomatically-motivated\nmeasure. Certainly, it offers no practical insight as to how disagreements can\nbe resolved.\n  Here we propose to address disagreements using the methods of financial\neconomics. In particular, we show how a large class of disagreements can be\ntransformed into investment opportunities. The expected financial performance\nof such investments quantifies the amount of disagreement in a tangible way.\nThis provides intuition for statistical concepts such as the R\\'enyi divergence\nwhich becomes connected to the financial performance of optimized investments.\nInvestment optimization takes into account individual opinions as well as\nattitudes towards risk. The result is a market-like social mechanism by which\nfunds flow naturally to support a more accurate view. Such social mechanisms\ncan help us with difficult disagreements (e.g., financial arguments concerning\nthe future climate).\n  In terms of scientific validation, we used the findings of independent\nneurophysiological experiments as well as our own research on the equity\npremium. \n\n"}
{"id": "1811.08790", "contents": "Title: Learning Quadratic Games on Networks Abstract: Individuals, or organizations, cooperate with or compete against one another\nin a wide range of practical situations. Such strategic interactions are often\nmodeled as games played on networks, where an individual's payoff depends not\nonly on her action but also on that of her neighbors. The current literature\nhas largely focused on analyzing the characteristics of network games in the\nscenario where the structure of the network, which is represented by a graph,\nis known beforehand. It is often the case, however, that the actions of the\nplayers are readily observable while the underlying interaction network remains\nhidden. In this paper, we propose two novel frameworks for learning, from the\nobservations on individual actions, network games with linear-quadratic\npayoffs, and in particular, the structure of the interaction network. Our\nframeworks are based on the Nash equilibrium of such games and involve solving\na joint optimization problem for the graph structure and the individual\nmarginal benefits. Both synthetic and real-world experiments demonstrate the\neffectiveness of the proposed frameworks, which have theoretical as well as\npractical implications for understanding strategic interactions in a network\nenvironment. \n\n"}
{"id": "1811.09026", "contents": "Title: Bandits with Temporal Stochastic Constraints Abstract: We study the effect of impairment on stochastic multi-armed bandits and\ndevelop new ways to mitigate it. Impairment effect is the phenomena where an\nagent only accrues reward for an action if they have played it at least a few\ntimes in the recent past. It is practically motivated by repetition and recency\neffects in domains such as advertising (here consumer behavior may require\nrepeat actions by advertisers) and vocational training (here actions are\ncomplex skills that can only be mastered with repetition to get a payoff).\nImpairment can be naturally modelled as a temporal constraint on the strategy\nspace, and we provide two novel algorithms that achieve sublinear regret, each\nworking with different assumptions on the impairment effect. We introduce a new\nnotion called bucketing in our algorithm design, and show how it can\neffectively address impairment as well as a broader class of temporal\nconstraints. Our regret bounds explicitly capture the cost of impairment and\nshow that it scales (sub-)linearly with the degree of impairment. Our work\ncomplements recent work on modeling delays and corruptions, and we provide\nexperimental evidence supporting our claims. \n\n"}
{"id": "1811.09558", "contents": "Title: Regret bounds for meta Bayesian optimization with an unknown Gaussian\n  process prior Abstract: Bayesian optimization usually assumes that a Bayesian prior is given.\nHowever, the strong theoretical guarantees in Bayesian optimization are often\nregrettably compromised in practice because of unknown parameters in the prior.\nIn this paper, we adopt a variant of empirical Bayes and show that, by\nestimating the Gaussian process prior from offline data sampled from the same\nprior and constructing unbiased estimators of the posterior, variants of both\nGP-UCB and probability of improvement achieve a near-zero regret bound, which\ndecreases to a constant proportional to the observational noise as the number\nof offline data and the number of online evaluations increase. Empirically, we\nhave verified our approach on challenging simulated robotic problems featuring\ntask and motion planning. \n\n"}
{"id": "1811.09646", "contents": "Title: Core-Selecting Mechanisms in Electricity Markets Abstract: Due to its theoretical virtues, several recent works propose the use of the\nincentive-compatible Vickrey-Clarke-Groves (VCG) mechanism for electricity\nmarkets. Coalitions of participants, however, can influence the VCG outcome to\nobtain higher collective profit. To address this issue, we propose\ncore-selecting mechanisms for their coalition-proofness. We show that\ncore-selecting mechanisms generalize the economic rationale of the locational\nmarginal pricing (LMP) mechanism. Namely, these mechanisms are the exact class\nof mechanisms that ensure the existence of a competitive equilibrium in\nlinear/nonlinear prices. This implies that the LMP mechanism is also\ncore-selecting, and hence coalition-proof. In contrast to the LMP mechanism,\ncore-selecting mechanisms exist for a broad class of electricity markets, such\nas ones involving nonconvex costs and nonconvex constraint sets. In addition,\nthey can approximate truthfulness without the price-taking assumption of the\nLMP mechanism. Finally, we show that they are also budget-balanced. Our results\nare verified with case studies based on optimal power flow test systems and the\nSwiss reserve market. \n\n"}
{"id": "1811.09680", "contents": "Title: Enhancing Engagement in Token-Curated Registries via an Inflationary\n  Mechanism Abstract: Token Curated Registries (TCR) are decentralized recommendation systems that\ncan be implemented using Blockchain smart contracts. They allow participants to\nvote for or against adding items to a list through a process that involves\nstaking tokens intrinsic to the registry, with winners receiving the staked\ntokens for each vote. A TCR aims to provide incentives to create a well-curated\nlist. In this work, we consider a challenge for these systems - incentivizing\ntoken-holders to actually engage and participate in the voting process. We\npropose a novel token-inflation mechanism for enhancing engagement, whereby\nonly voting participants see their token supply increased by a pre-defined\nmultiple after each round of voting. To evaluate this proposal, we propose a\nsimple 4-class model of voters that captures all possible combinations of two\nkey dimensions: whether they are engaged (likely to vote at all for a given\nitem) or disengaged, and whether they are informed (likely to vote in a way\nthat increases the quality of the list) or uninformed, and a simple metric to\nevaluate the quality of the list as a function of the vote outcomes. We conduct\nsimulations using this model of voters and show that implementing\ntoken-inflation results in greater wealth accumulation for engaged voters. In\nparticular, when the number of informed voters is sufficiently high, our\nsimulations show that voters that are both informed and engaged see the\ngreatest benefits from participating in the registry when our proposed\ntoken-inflation mechanism is employed. We further validate this finding using a\nsimplified mathematical analysis. \n\n"}
{"id": "1811.10146", "contents": "Title: Frequency Principle in Deep Learning with General Loss Functions and Its\n  Potential Application Abstract: Previous studies have shown that deep neural networks (DNNs) with common\nsettings often capture target functions from low to high frequency, which is\ncalled Frequency Principle (F-Principle). It has also been shown that\nF-Principle can provide an understanding to the often observed good\ngeneralization ability of DNNs. However, previous studies focused on the loss\nfunction of mean square error, while various loss functions are used in\npractice. In this work, we show that the F-Principle holds for a general loss\nfunction (e.g., mean square error, cross entropy, etc.). In addition, DNN's\nF-Principle may be applied to develop numerical schemes for solving various\nproblems which would benefit from a fast converging of low frequency. As an\nexample of the potential usage of F-Principle, we apply DNN in solving\ndifferential equations, in which conventional methods (e.g., Jacobi method) is\nusually slow in solving problems due to the convergence from high to low\nfrequency. \n\n"}
{"id": "1811.11190", "contents": "Title: Semantically-aware population health risk analyses Abstract: One primary task of population health analysis is the identification of risk\nfactors that, for some subpopulation, have a significant association with some\nhealth condition. Examples include finding lifestyle factors associated with\nchronic diseases and finding genetic mutations associated with diseases in\nprecision health. We develop a combined semantic and machine learning system\nthat uses a health risk ontology and knowledge graph (KG) to dynamically\ndiscover risk factors and their associated subpopulations. Semantics and the\nnovel supervised cadre model make our system explainable. Future population\nhealth studies are easily performed and documented with provenance by\nspecifying additional input and output KG cartridges. \n\n"}
{"id": "1811.11298", "contents": "Title: Exploring Restart Distributions Abstract: We consider the generic approach of using an experience memory to help\nexploration by adapting a restart distribution. That is, given the capacity to\nreset the state with those corresponding to the agent's past observations, we\nhelp exploration by promoting faster state-space coverage via restarting the\nagent from a more diverse set of initial states, as well as allowing it to\nrestart in states associated with significant past experiences. This approach\nis compatible with both on-policy and off-policy methods. However, a caveat is\nthat altering the distribution of initial states could change the optimal\npolicies when searching within a restricted class of policies. To reduce this\nunsought learning bias, we evaluate our approach in deep reinforcement learning\nwhich benefits from the high representational capacity of deep neural networks.\nWe instantiate three variants of our approach, each inspired by an idea in the\ncontext of experience replay. Using these variants, we show that performance\ngains can be achieved, especially in hard exploration problems. \n\n"}
{"id": "1811.11353", "contents": "Title: Multi-label classification search space in the MEKA software Abstract: This supplementary material aims to describe the proposed multi-label\nclassification (MLC) search spaces based on the MEKA and WEKA softwares. First,\nwe overview 26 MLC algorithms and meta-algorithms in MEKA, presenting their\nmain characteristics, such as hyper-parameters, dependencies and constraints.\nSecond, we review 28 single-label classification (SLC) algorithms,\npreprocessing algorithms and meta-algorithms in the WEKA software. These SLC\nalgorithms were also studied because they are part of the proposed MLC search\nspaces. Fundamentally, this occurs due to the problem transformation nature of\nseveral MLC algorithms used in this work. These algorithms transform an MLC\nproblem into one or several SLC problems in the first place and solve them with\nSLC model(s) in a next step. Therefore, understanding their main\ncharacteristics is crucial to this work. Finally, we present a formal\ndescription of the search spaces by proposing a context-free grammar that\nencompasses the 54 learning algorithms. This grammar basically comprehends the\npossible combinations, the constraints and dependencies among the learning\nalgorithms. \n\n"}
{"id": "1811.12234", "contents": "Title: Machine Learning on Electronic Health Records: Models and Features\n  Usages to predict Medication Non-Adherence Abstract: Adherence can be defined as \"the extent to which patients take their\nmedications as prescribed by their healthcare providers\"[Osterberg and\nBlaschke, 2005]. World Health Organization's reports point out that, in\ndeveloped countries, only about 50% of patients with chronic diseases correctly\nfollow their treatments. This severely compromises the efficiency of long-term\ntherapy and increases the cost of health services. We propose in this paper\ndifferent models of patient drug consumption in breast cancer treatments. The\naim of these different approaches is to predict medication non-adherence while\ngiving insights to doctors of the underlying reasons of these illegitimate\ndrop-outs. Working with oncologists, we show the interest of Machine- Learning\nalgorithms fined tune by the feedback of experts to estimate a risk score of a\npatient's non-adherence and thus improve support throughout their care path. \n\n"}
{"id": "1811.12556", "contents": "Title: How to Organize your Deep Reinforcement Learning Agents: The Importance\n  of Communication Topology Abstract: In this empirical paper, we investigate how learning agents can be arranged\nin more efficient communication topologies for improved learning. This is an\nimportant problem because a common technique to improve speed and robustness of\nlearning in deep reinforcement learning and many other machine learning\nalgorithms is to run multiple learning agents in parallel. The standard\ncommunication architecture typically involves all agents intermittently\ncommunicating with each other (fully connected topology) or with a centralized\nserver (star topology). Unfortunately, optimizing the topology of communication\nover the space of all possible graphs is a hard problem, so we borrow results\nfrom the networked optimization and collective intelligence literatures which\nsuggest that certain families of network topologies can lead to strong\nimprovements over fully-connected networks. We start by introducing alternative\nnetwork topologies to DRL benchmark tasks under the Evolution Strategies\nparadigm which we call Network Evolution Strategies. We explore the relative\nperformance of the four main graph families and observe that one such family\n(Erdos-Renyi random graphs) empirically outperforms all other families,\nincluding the de facto fully-connected communication topologies. Additionally,\nthe use of alternative network topologies has a multiplicative performance\neffect: we observe that when 1000 learning agents are arranged in a carefully\ndesigned communication topology, they can compete with 3000 agents arranged in\nthe de facto fully-connected topology. Overall, our work suggests that\ndistributed machine learning algorithms would learn more efficiently if the\ncommunication topology between learning agents was optimized. \n\n"}
{"id": "1811.12655", "contents": "Title: Prior-free Data Acquisition for Accurate Statistical Estimation Abstract: We study a data analyst's problem of acquiring data from self-interested\nindividuals to obtain an accurate estimation of some statistic of a population,\nsubject to an expected budget constraint. Each data holder incurs a cost, which\nis unknown to the data analyst, to acquire and report his data. The cost can be\narbitrarily correlated with the data. The data analyst has an expected budget\nthat she can use to incentivize individuals to provide their data. The goal is\nto design a joint acquisition-estimation mechanism to optimize the performance\nof the produced estimator, without any prior information on the underlying\ndistribution of cost and data. We investigate two types of estimations:\nunbiased point estimation and confidence interval estimation.\n  Unbiased estimators: We design a truthful, individually rational, online\nmechanism to acquire data from individuals and output an unbiased estimator of\nthe population mean when the data analyst has no prior information on the\ncost-data distribution and individuals arrive in a random order. The\nperformance of this mechanism matches that of the optimal mechanism, which\nknows the true cost distribution, within a constant factor. The performance of\nan estimator is evaluated by its variance under the worst-case cost-data\ncorrelation.\n  Confidence intervals: We characterize an approximately optimal (within a\nfactor $2$) mechanism for obtaining a confidence interval of the population\nmean when the data analyst knows the true cost distribution at the beginning.\nThis mechanism is efficiently computable. We then design a truthful,\nindividually rational, online algorithm that is only worse than the\napproximately optimal mechanism by a constant factor. The performance of an\nestimator is evaluated by its expected length under the worst-case cost-data\ncorrelation. \n\n"}
{"id": "1812.00804", "contents": "Title: Deep Inverse Optimization Abstract: Given a set of observations generated by an optimization process, the goal of\ninverse optimization is to determine likely parameters of that process. We cast\ninverse optimization as a form of deep learning. Our method, called deep\ninverse optimization, is to unroll an iterative optimization process and then\nuse backpropagation to learn parameters that generate the observations. We\ndemonstrate that by backpropagating through the interior point algorithm we can\nlearn the coefficients determining the cost vector and the constraints,\nindependently or jointly, for both non-parametric and parametric linear\nprograms, starting from one or multiple observations. With this approach,\ninverse optimization can leverage concepts and algorithms from deep learning. \n\n"}
{"id": "1812.02632", "contents": "Title: Active Deep Q-learning with Demonstration Abstract: Recent research has shown that although Reinforcement Learning (RL) can\nbenefit from expert demonstration, it usually takes considerable efforts to\nobtain enough demonstration. The efforts prevent training decent RL agents with\nexpert demonstration in practice. In this work, we propose Active Reinforcement\nLearning with Demonstration (ARLD), a new framework to streamline RL in terms\nof demonstration efforts by allowing the RL agent to query for demonstration\nactively during training. Under the framework, we propose Active Deep\nQ-Network, a novel query strategy which adapts to the dynamically-changing\ndistributions during the RL training process by estimating the uncertainty of\nrecent states. The expert demonstration data within Active DQN are then\nutilized by optimizing supervised max-margin loss in addition to temporal\ndifference loss within usual DQN training. We propose two methods of estimating\nthe uncertainty based on two state-of-the-art DQN models, namely the divergence\nof bootstrapped DQN and the variance of noisy DQN. The empirical results\nvalidate that both methods not only learn faster than other passive expert\ndemonstration methods with the same amount of demonstration and but also reach\nsuper-expert level of performance across four different tasks. \n\n"}
{"id": "1812.03565", "contents": "Title: The Gap Between Model-Based and Model-Free Methods on the Linear\n  Quadratic Regulator: An Asymptotic Viewpoint Abstract: The effectiveness of model-based versus model-free methods is a long-standing\nquestion in reinforcement learning (RL). Motivated by recent empirical success\nof RL on continuous control tasks, we study the sample complexity of popular\nmodel-based and model-free algorithms on the Linear Quadratic Regulator (LQR).\nWe show that for policy evaluation, a simple model-based plugin method requires\nasymptotically less samples than the classical least-squares temporal\ndifference (LSTD) estimator to reach the same quality of solution; the sample\ncomplexity gap between the two methods can be at least a factor of state\ndimension. For policy evaluation, we study a simple family of problem instances\nand show that nominal (certainty equivalence principle) control also requires\nseveral factors of state and input dimension fewer samples than the policy\ngradient method to reach the same level of control performance on these\ninstances. Furthermore, the gap persists even when employing commonly used\nbaselines. To the best of our knowledge, this is the first theoretical result\nwhich demonstrates a separation in the sample complexity between model-based\nand model-free methods on a continuous control task. \n\n"}
{"id": "1812.04634", "contents": "Title: On the Curved Geometry of Accelerated Optimization Abstract: In this work we propose a differential geometric motivation for Nesterov's\naccelerated gradient method (AGM) for strongly-convex problems. By considering\nthe optimization procedure as occurring on a Riemannian manifold with a natural\nstructure, The AGM method can be seen as the proximal point method applied in\nthis curved space. This viewpoint can also be extended to the continuous time\ncase, where the accelerated gradient method arises from the natural\nblock-implicit Euler discretization of an ODE on the manifold. We provide an\nanalysis of the convergence rate of this ODE for quadratic objectives. \n\n"}
{"id": "1812.05217", "contents": "Title: Tight Analyses for Non-Smooth Stochastic Gradient Descent Abstract: Consider the problem of minimizing functions that are Lipschitz and strongly\nconvex, but not necessarily differentiable. We prove that after $T$ steps of\nstochastic gradient descent, the error of the final iterate is $O(\\log(T)/T)$\nwith high probability. We also construct a function from this class for which\nthe error of the final iterate of deterministic gradient descent is\n$\\Omega(\\log(T)/T)$. This shows that the upper bound is tight and that, in this\nsetting, the last iterate of stochastic gradient descent has the same general\nerror rate (with high probability) as deterministic gradient descent. This\nresolves both open questions posed by Shamir (2012).\n  An intermediate step of our analysis proves that the suffix averaging method\nachieves error $O(1/T)$ with high probability, which is optimal (for any\nfirst-order optimization method). This improves results of Rakhlin (2012) and\nHazan and Kale (2014), both of which achieved error $O(1/T)$, but only in\nexpectation, and achieved a high probability error bound of $O(\\log\n\\log(T)/T)$, which is suboptimal.\n  We prove analogous results for functions that are Lipschitz and convex, but\nnot necessarily strongly convex or differentiable. After $T$ steps of\nstochastic gradient descent, the error of the final iterate is\n$O(\\log(T)/\\sqrt{T})$ with high probability, and there exists a function for\nwhich the error of the final iterate of deterministic gradient descent is\n$\\Omega(\\log(T)/\\sqrt{T})$. \n\n"}
{"id": "1812.07351", "contents": "Title: Monte Carlo Continual Resolving for Online Strategy Computation in\n  Imperfect Information Games Abstract: Online game playing algorithms produce high-quality strategies with a\nfraction of memory and computation required by their offline alternatives.\nContinual Resolving (CR) is a recent theoretically sound approach to online\ngame playing that has been used to outperform human professionals in poker.\nHowever, parts of the algorithm were specific to poker, which enjoys many\nproperties not shared by other imperfect information games. We present a\ndomain-independent formulation of CR applicable to any two-player zero-sum\nextensive-form games that works with an abstract resolving algorithm. We\nfurther describe and implement its Monte Carlo variant (MCCR) which uses Monte\nCarlo Counterfactual Regret Minimization (MCCFR) as a resolver. We prove the\ncorrectness of CR and show an $O(T^{-1/2})$-dependence of MCCR's exploitability\non the computation time. Furthermore, we present an empirical comparison of\nMCCR with incremental tree building to Online Outcome Sampling and\nInformation-set MCTS on several domains. \n\n"}
{"id": "1812.07627", "contents": "Title: Clustering-Oriented Representation Learning with Attractive-Repulsive\n  Loss Abstract: The standard loss function used to train neural network classifiers,\ncategorical cross-entropy (CCE), seeks to maximize accuracy on the training\ndata; building useful representations is not a necessary byproduct of this\nobjective. In this work, we propose clustering-oriented representation learning\n(COREL) as an alternative to CCE in the context of a generalized\nattractive-repulsive loss framework. COREL has the consequence of building\nlatent representations that collectively exhibit the quality of natural\nclustering within the latent space of the final hidden layer, according to a\npredefined similarity function. Despite being simple to implement, COREL\nvariants outperform or perform equivalently to CCE in a variety of scenarios,\nincluding image and news article classification using both feed-forward and\nconvolutional neural networks. Analysis of the latent spaces created with\ndifferent similarity functions facilitates insights on the different use cases\nCOREL variants can satisfy, where the Cosine-COREL variant makes a consistently\nclusterable latent space, while Gaussian-COREL consistently obtains better\nclassification accuracy than CCE. \n\n"}
{"id": "1812.08305", "contents": "Title: Derivative-Free Methods for Policy Optimization: Guarantees for Linear\n  Quadratic Systems Abstract: We study derivative-free methods for policy optimization over the class of\nlinear policies. We focus on characterizing the convergence rate of these\nmethods when applied to linear-quadratic systems, and study various settings of\ndriving noise and reward feedback. We show that these methods provably converge\nto within any pre-specified tolerance of the optimal policy with a number of\nzero-order evaluations that is an explicit polynomial of the error tolerance,\ndimension, and curvature properties of the problem. Our analysis reveals some\ninteresting differences between the settings of additive driving noise and\nrandom initialization, as well as the settings of one-point and two-point\nreward feedback. Our theory is corroborated by extensive simulations of\nderivative-free methods on these systems. Along the way, we derive convergence\nrates for stochastic zero-order optimization algorithms when applied to a\ncertain class of non-convex problems. \n\n"}
{"id": "1812.08904", "contents": "Title: Pre-training with Non-expert Human Demonstration for Deep Reinforcement\n  Learning Abstract: Deep reinforcement learning (deep RL) has achieved superior performance in\ncomplex sequential tasks by using deep neural networks as function\napproximators to learn directly from raw input images. However, learning\ndirectly from raw images is data inefficient. The agent must learn feature\nrepresentation of complex states in addition to learning a policy. As a result,\ndeep RL typically suffers from slow learning speeds and often requires a\nprohibitively large amount of training time and data to reach reasonable\nperformance, making it inapplicable to real-world settings where data is\nexpensive. In this work, we improve data efficiency in deep RL by addressing\none of the two learning goals, feature learning. We leverage supervised\nlearning to pre-train on a small set of non-expert human demonstrations and\nempirically evaluate our approach using the asynchronous advantage actor-critic\nalgorithms (A3C) in the Atari domain. Our results show significant improvements\nin learning speed, even when the provided demonstration is noisy and of low\nquality. \n\n"}
{"id": "1812.10563", "contents": "Title: The Prophet Inequality Can Be Solved Optimally with a Single Set of\n  Samples Abstract: The setting of the classic prophet inequality is as follows: a gambler is\nshown the probability distributions of $n$ independent, non-negative random\nvariables with finite expectations. In their indexed order, a value is drawn\nfrom each distribution, and after every draw the gambler may choose to accept\nthe value and end the game, or discard the value permanently and continue the\ngame. What is the best performance that the gambler can achieve in comparison\nto a prophet who can always choose the highest value? Krengel, Sucheston, and\nGarling solved this problem in 1978, showing that there exists a strategy for\nwhich the gambler can achieve half as much reward as the prophet in\nexpectation. Furthermore, this result is tight.\n  In this work, we consider a setting in which the gambler is allowed much less\ninformation. Suppose that the gambler can only take one sample from each of the\ndistributions before playing the game, instead of knowing the full\ndistributions. We provide a simple and intuitive algorithm that recovers the\noriginal approximation of $\\frac{1}{2}$. Our algorithm works against even an\nalmighty adversary who always chooses a worst-case ordering, rather than the\nstandard offline adversary. The result also has implications for mechanism\ndesign -- there is much interest in designing competitive auctions with a\nfinite number of samples from value distributions rather than full\ndistributional knowledge. \n\n"}
{"id": "1812.11039", "contents": "Title: On the Benefit of Width for Neural Networks: Disappearance of Bad Basins Abstract: Wide networks are often believed to have a nice optimization landscape, but\nwhat rigorous results can we prove? To understand the benefit of width, it is\nimportant to identify the difference between wide and narrow networks. In this\nwork, we prove that from narrow to wide networks, there is a phase transition\nfrom having sub-optimal basins to no sub-optimal basins. Specifically, we prove\ntwo results: on the positive side, for any continuous activation functions, the\nloss surface of a class of wide networks has no sub-optimal basins, where\n\"basin\" is defined as the set-wise strict local minimum; on the negative side,\nfor a large class of networks with width below a threshold, we construct strict\nlocal minima that are not global. These two results together show the phase\ntransition from narrow to wide networks. \n\n"}
{"id": "1812.11149", "contents": "Title: Online Trading as a Secretary Problem Abstract: We consider the online problem in which an intermediary trades identical\nitems with a sequence of n buyers and n sellers, each of unit demand. We assume\nthat the values of the traders are selected by an adversary and the sequence is\nrandomly permuted. We give competitive algorithms for two objectives: welfare\nand gain-from-trade. \n\n"}
{"id": "1901.00137", "contents": "Title: A Theoretical Analysis of Deep Q-Learning Abstract: Despite the great empirical success of deep reinforcement learning, its\ntheoretical foundation is less well understood. In this work, we make the first\nattempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et\nal., 2015) from both algorithmic and statistical perspectives. In specific, we\nfocus on a slight simplification of DQN that fully captures its key features.\nUnder mild assumptions, we establish the algorithmic and statistical rates of\nconvergence for the action-value functions of the iterative policy sequence\nobtained by DQN. In particular, the statistical error characterizes the bias\nand variance that arise from approximating the action-value function using deep\nneural network, while the algorithmic error converges to zero at a geometric\nrate. As a byproduct, our analysis provides justifications for the techniques\nof experience replay and target network, which are crucial to the empirical\nsuccess of DQN. Furthermore, as a simple extension of DQN, we propose the\nMinimax-DQN algorithm for zero-sum Markov game with two players. Borrowing the\nanalysis of DQN, we also quantify the difference between the policies obtained\nby Minimax-DQN and the Nash equilibrium of the Markov game in terms of both the\nalgorithmic and statistical rates of convergence. \n\n"}
{"id": "1901.00210", "contents": "Title: Tighter Problem-Dependent Regret Bounds in Reinforcement Learning\n  without Domain Knowledge using Value Function Bounds Abstract: Strong worst-case performance bounds for episodic reinforcement learning\nexist but fortunately in practice RL algorithms perform much better than such\nbounds would predict. Algorithms and theory that provide strong\nproblem-dependent bounds could help illuminate the key features of what makes a\nRL problem hard and reduce the barrier to using RL algorithms in practice. As a\nstep towards this we derive an algorithm for finite horizon discrete MDPs and\nassociated analysis that both yields state-of-the art worst-case regret bounds\nin the dominant terms and yields substantially tighter bounds if the RL\nenvironment has small environmental norm, which is a function of the variance\nof the next-state value functions. An important benefit of our algorithmic is\nthat it does not require apriori knowledge of a bound on the environmental\nnorm. As a result of our analysis, we also help address an open learning theory\nquestion~\\cite{jiang2018open} about episodic MDPs with a constant upper-bound\non the sum of rewards, providing a regret bound with no $H$-dependence in the\nleading term that scales a polynomial function of the number of episodes. \n\n"}
{"id": "1901.00214", "contents": "Title: Clustering with Distributed Data Abstract: We consider $K$-means clustering in networked environments (e.g., internet of\nthings (IoT) and sensor networks) where data is inherently distributed across\nnodes and processing power at each node may be limited. We consider a\nclustering algorithm referred to as networked $K$-means, or $NK$-means, which\nrelies only on local neighborhood information exchange. Information exchange is\nlimited to low-dimensional statistics and not raw data at the agents. The\nproposed approach develops a parametric family of multi-agent clustering\nobjectives (parameterized by $\\rho$) and associated distributed $NK$-means\nalgorithms (also parameterized by $\\rho$). The $NK$-means algorithm with\nparameter $\\rho$ converges to a set of fixed points relative to the associated\nmulti-agent objective (designated as `generalized minima'). By appropriate\nchoice of $\\rho$, the set of generalized minima may be brought arbitrarily\nclose to the set of Lloyd's minima. Thus, the $NK$-means algorithm may be used\nto compute Lloyd's minima of the collective dataset up to arbitrary accuracy. \n\n"}
{"id": "1901.00246", "contents": "Title: Natively Interpretable Machine Learning and Artificial Intelligence:\n  Preliminary Results and Future Directions Abstract: Machine learning models have become more and more complex in order to better\napproximate complex functions. Although fruitful in many domains, the added\ncomplexity has come at the cost of model interpretability. The once popular\nk-nearest neighbors (kNN) approach, which finds and uses the most similar data\nfor reasoning, has received much less attention in recent decades due to\nnumerous problems when compared to other techniques. We show that many of these\nhistorical problems with kNN can be overcome, and our contribution has\napplications not only in machine learning but also in online learning, data\nsynthesis, anomaly detection, model compression, and reinforcement learning,\nwithout sacrificing interpretability. We introduce a synthesis between kNN and\ninformation theory that we hope will provide a clear path towards models that\nare innately interpretable and auditable. Through this work we hope to gather\ninterest in combining kNN with information theory as a promising path to fully\nauditable machine learning and artificial intelligence. \n\n"}
{"id": "1901.00612", "contents": "Title: Adversarial Learning of a Sampler Based on an Unnormalized Distribution Abstract: We investigate adversarial learning in the case when only an unnormalized\nform of the density can be accessed, rather than samples. With insights so\ngarnered, adversarial learning is extended to the case for which one has access\nto an unnormalized form u(x) of the target density function, but no samples.\nFurther, new concepts in GAN regularization are developed, based on learning\nfrom samples or from u(x). The proposed method is compared to alternative\napproaches, with encouraging results demonstrated across a range of\napplications, including deep soft Q-learning. \n\n"}
{"id": "1901.00838", "contents": "Title: On Finding Local Nash Equilibria (and Only Local Nash Equilibria) in\n  Zero-Sum Games Abstract: We propose local symplectic surgery, a two-timescale procedure for finding\nlocal Nash equilibria in two-player zero-sum games. We first show that previous\ngradient-based algorithms cannot guarantee convergence to local Nash equilibria\ndue to the existence of non-Nash stationary points. By taking advantage of the\ndifferential structure of the game, we construct an algorithm for which the\nlocal Nash equilibria are the only attracting fixed points. We also show that\nthe algorithm exhibits no oscillatory behaviors in neighborhoods of equilibria\nand show that it has the same per-iteration complexity as other recently\nproposed algorithms. We conclude by validating the algorithm on two numerical\nexamples: a toy example with multiple Nash equilibria and a non-Nash\nequilibrium, and the training of a small generative adversarial network (GAN). \n\n"}
{"id": "1901.01365", "contents": "Title: Hierarchical Reinforcement Learning via Advantage-Weighted Information\n  Maximization Abstract: Real-world tasks are often highly structured. Hierarchical reinforcement\nlearning (HRL) has attracted research interest as an approach for leveraging\nthe hierarchical structure of a given task in reinforcement learning (RL).\nHowever, identifying the hierarchical policy structure that enhances the\nperformance of RL is not a trivial task. In this paper, we propose an HRL\nmethod that learns a latent variable of a hierarchical policy using mutual\ninformation maximization. Our approach can be interpreted as a way to learn a\ndiscrete and latent representation of the state-action space. To learn option\npolicies that correspond to modes of the advantage function, we introduce\nadvantage-weighted importance sampling. In our HRL method, the gating policy\nlearns to select option policies based on an option-value function, and these\noption policies are optimized based on the deterministic policy gradient\nmethod. This framework is derived by leveraging the analogy between a\nmonolithic policy in standard RL and a hierarchical policy in HRL by using a\ndeterministic option policy. Experimental results indicate that our HRL\napproach can learn a diversity of options and that it can enhance the\nperformance of RL in continuous control tasks. \n\n"}
{"id": "1901.01631", "contents": "Title: Sharp Restricted Isometry Bounds for the Inexistence of Spurious Local\n  Minima in Nonconvex Matrix Recovery Abstract: Nonconvex matrix recovery is known to contain no spurious local minima under\na restricted isometry property (RIP) with a sufficiently small RIP constant\n$\\delta$. If $\\delta$ is too large, however, then counterexamples containing\nspurious local minima are known to exist. In this paper, we introduce a proof\ntechnique that is capable of establishing sharp thresholds on $\\delta$ to\nguarantee the inexistence of spurious local minima. Using the technique, we\nprove that in the case of a rank-1 ground truth, an RIP constant of\n$\\delta<1/2$ is both necessary and sufficient for exact recovery from any\narbitrary initial point (such as a random point). We also prove a local\nrecovery result: given an initial point $x_{0}$ satisfying\n$f(x_{0})\\le(1-\\delta)^{2}f(0)$, any descent algorithm that converges to\nsecond-order optimality guarantees exact recovery. \n\n"}
{"id": "1901.01994", "contents": "Title: Recurrent Control Nets for Deep Reinforcement Learning Abstract: Central Pattern Generators (CPGs) are biological neural circuits capable of\nproducing coordinated rhythmic outputs in the absence of rhythmic input. As a\nresult, they are responsible for most rhythmic motion in living organisms. This\nrhythmic control is broadly applicable to fields such as locomotive robotics\nand medical devices. In this paper, we explore the possibility of creating a\nself-sustaining CPG network for reinforcement learning that learns rhythmic\nmotion more efficiently and across more general environments than the current\nmultilayer perceptron (MLP) baseline models. Recent work introduces the\nStructured Control Net (SCN), which maintains linear and nonlinear modules for\nlocal and global control, respectively. Here, we show that time-sequence\narchitectures such as Recurrent Neural Networks (RNNs) model CPGs effectively.\nCombining previous work with RNNs and SCNs, we introduce the Recurrent Control\nNet (RCN), which adds a linear component to the, RCNs match and exceed the\nperformance of baseline MLPs and SCNs across all environment tasks. Our\nfindings confirm existing intuitions for RNNs on reinforcement learning tasks,\nand demonstrate promise of SCN-like structures in reinforcement learning. \n\n"}
{"id": "1901.02063", "contents": "Title: Reliable Agglomerative Clustering Abstract: Standard agglomerative clustering suggests establishing a new reliable\nlinkage at every step. However, in order to provide adaptive,\ndensity-consistent and flexible solutions, we study extracting all the reliable\nlinkages at each step, instead of the smallest one. Such a strategy can be\napplied with all common criteria for agglomerative hierarchical clustering. We\nalso study that this strategy with the single linkage criterion yields a\nminimum spanning tree algorithm. We perform experiments on several real-world\ndatasets to demonstrate the performance of this strategy compared to the\nstandard alternative. \n\n"}
{"id": "1901.02322", "contents": "Title: Fusion Strategies for Learning User Embeddings with Neural Networks Abstract: Growing amounts of online user data motivate the need for automated\nprocessing techniques. In case of user ratings, one interesting option is to\nuse neural networks for learning to predict ratings given an item and a user.\nWhile training for prediction, such an approach at the same time learns to map\neach user to a vector, a so-called user embedding. Such embeddings can for\nexample be valuable for estimating user similarity. However, there are various\nways how item and user information can be combined in neural networks, and it\nis unclear how the way of combining affects the resulting embeddings. In this\npaper, we run an experiment on movie ratings data, where we analyze the effect\non embedding quality caused by several fusion strategies in neural networks.\nFor evaluating embedding quality, we propose a novel measure, Pair-Distance\nCorrelation, which quantifies the condition that similar users should have\nsimilar embedding vectors. We find that the fusion strategy affects results in\nterms of both prediction performance and embedding quality. Surprisingly, we\nfind that prediction performance not necessarily reflects embedding quality.\nThis suggests that if embeddings are of interest, the common tendency to select\nmodels based on their prediction ability should be reconsidered. \n\n"}
{"id": "1901.02354", "contents": "Title: Geometrization of deep networks for the interpretability of deep\n  learning systems Abstract: How to understand deep learning systems remains an open problem. In this\npaper we propose that the answer may lie in the geometrization of deep\nnetworks. Geometrization is a bridge to connect physics, geometry, deep network\nand quantum computation and this may result in a new scheme to reveal the rule\nof the physical world. By comparing the geometry of image matching and deep\nnetworks, we show that geometrization of deep networks can be used to\nunderstand existing deep learning systems and it may also help to solve the\ninterpretability problem of deep learning systems. \n\n"}
{"id": "1901.02705", "contents": "Title: Model-Predictive Policy Learning with Uncertainty Regularization for\n  Driving in Dense Traffic Abstract: Learning a policy using only observational data is challenging because the\ndistribution of states it induces at execution time may differ from the\ndistribution observed during training. We propose to train a policy by\nunrolling a learned model of the environment dynamics over multiple time steps\nwhile explicitly penalizing two costs: the original cost the policy seeks to\noptimize, and an uncertainty cost which represents its divergence from the\nstates it is trained on. We measure this second cost by using the uncertainty\nof the dynamics model about its own predictions, using recent ideas from\nuncertainty estimation for deep networks. We evaluate our approach using a\nlarge-scale observational dataset of driving behavior recorded from traffic\ncameras, and show that we are able to learn effective driving policies from\npurely observational data, with no environment interaction. \n\n"}
{"id": "1901.03040", "contents": "Title: Quantized Epoch-SGD for Communication-Efficient Distributed Learning Abstract: Due to its efficiency and ease to implement, stochastic gradient descent\n(SGD) has been widely used in machine learning. In particular, SGD is one of\nthe most popular optimization methods for distributed learning. Recently,\nquantized SGD (QSGD), which adopts quantization to reduce the communication\ncost in SGD-based distributed learning, has attracted much attention. Although\nseveral QSGD methods have been proposed, some of them are heuristic without\ntheoretical guarantee, and others have high quantization variance which makes\nthe convergence become slow. In this paper, we propose a new method, called\nQuantized Epoch-SGD (QESGD), for communication-efficient distributed learning.\nQESGD compresses (quantizes) the parameter with variance reduction, so that it\ncan get almost the same performance as that of SGD with less communication\ncost. QESGD is implemented on the Parameter Server framework, and empirical\nresults on distributed deep learning show that QESGD can outperform other\nstate-of-the-art quantization methods to achieve the best performance. \n\n"}
{"id": "1901.03317", "contents": "Title: Accelerated Flow for Probability Distributions Abstract: This paper presents a methodology and numerical algorithms for constructing\naccelerated gradient flows on the space of probability distributions. In\nparticular, we extend the recent variational formulation of accelerated\ngradient methods in (wibisono, et. al. 2016) from vector valued variables to\nprobability distributions. The variational problem is modeled as a mean-field\noptimal control problem. The maximum principle of optimal control theory is\nused to derive Hamilton's equations for the optimal gradient flow. The\nHamilton's equation are shown to achieve the accelerated form of density\ntransport from any initial probability distribution to a target probability\ndistribution. A quantitative estimate on the asymptotic convergence rate is\nprovided based on a Lyapunov function construction, when the objective\nfunctional is displacement convex. Two numerical approximations are presented\nto implement the Hamilton's equations as a system of $N$ interacting particles.\nThe continuous limit of the Nesterov's algorithm is shown to be a special case\nwith $N=1$. The algorithm is illustrated with numerical examples. \n\n"}
{"id": "1901.04966", "contents": "Title: Identifying and Correcting Label Bias in Machine Learning Abstract: Datasets often contain biases which unfairly disadvantage certain groups, and\nclassifiers trained on such datasets can inherit these biases. In this paper,\nwe provide a mathematical formulation of how this bias can arise. We do so by\nassuming the existence of underlying, unknown, and unbiased labels which are\noverwritten by an agent who intends to provide accurate labels but may have\nbiases against certain groups. Despite the fact that we only observe the biased\nlabels, we are able to show that the bias may nevertheless be corrected by\nre-weighting the data points without changing the labels. We show, with\ntheoretical guarantees, that training on the re-weighted dataset corresponds to\ntraining on the unobserved but unbiased labels, thus leading to an unbiased\nmachine learning classifier. Our procedure is fast and robust and can be used\nwith virtually any learning algorithm. We evaluate on a number of standard\nmachine learning fairness datasets and a variety of fairness notions, finding\nthat our method outperforms standard approaches in achieving fair\nclassification. \n\n"}
{"id": "1901.05168", "contents": "Title: How Will the Presence of Autonomous Vehicles Affect the Equilibrium\n  State of Traffic Networks? Abstract: It is known that connected and autonomous vehicles are capable of maintaining\nshorter headways and distances when they form platoons of vehicles. Thus, such\ntechnologies can result in increases in the capacities of traffic networks.\nConsequently, it is envisioned that their deployment will boost the network\nmobility. In this paper, we verify the validity of this impact under selfish\nrouting behavior of drivers in traffic networks with mixed autonomy, i.e.\ntraffic networks with both regular and autonomous vehicles. We consider a\nnonatomic routing game on a network with inelastic (fixed) demands for the set\nof network O/D pairs, and study how replacing a fraction of regular vehicles by\nautonomous vehicles will affect the mobility of the network. Using the well\nknown US bureau of public roads (BPR) traffic delay models, we show that the\nresulting Wardrop equilibrium is not necessarily unique even in its weak sense\nfor networks with mixed autonomy. We state the conditions under which the total\nnetwork delay is guaranteed not to increase as a result of autonomy increase.\nHowever, we show that when these conditions do not hold, counter intuitive\nbehaviors may occur: the total delay can grow by increasing the network\nautonomy. In particular, we prove that for networks with a single O/D pair, if\nthe road degrees of asymmetry are homogeneous, the total delay is 1) unique,\nand 2) a nonincreasing continuous function of network autonomy fraction. We\nshow that for heterogeneous degrees of asymmetry, the total delay is not\nunique, and it can further grow with autonomy increase. We demonstrate that\nsimilar behaviors may be observed in networks with multiple O/D pairs. We\nfurther bound such performance degradations due to the introduction of autonomy\nin homogeneous networks. \n\n"}
{"id": "1901.05577", "contents": "Title: Generating Realistic Sequences of Customer-level Transactions for Retail\n  Datasets Abstract: In order to better engage with customers, retailers rely on extensive\ncustomer and product databases which allows them to better understand customer\nbehaviour and purchasing patterns. This has long been a challenging task as\ncustomer modelling is a multi-faceted, noisy and time-dependent problem. The\nmost common way to tackle this problem is indirectly through task-specific\nsupervised learning prediction problems, with relatively little literature on\nmodelling a customer by directly simulating their future transactions. In this\npaper we propose a method for generating realistic sequences of baskets that a\ngiven customer is likely to purchase over a period of time. Customer embedding\nrepresentations are learned using a Recurrent Neural Network (RNN) which takes\ninto account the entire sequence of transaction data. Given the customer state\nat a specific point in time, a Generative Adversarial Network (GAN) is trained\nto generate a plausible basket of products for the following week. The newly\ngenerated basket is then fed back into the RNN to update the customer's state.\nThe GAN is thus used in tandem with the RNN module in a pipeline alternating\nbetween basket generation and customer state updating steps. This allows for\nsampling over a distribution of a customer's future sequence of baskets, which\nthen can be used to gain insight into how to service the customer more\neffectively. The methodology is empirically shown to produce baskets that\nappear similar to real baskets and enjoy many common properties, including\nfrequencies of different product types, brands, and prices. Furthermore, the\ngenerated data is able to replicate most of the strongest sequential patterns\nthat exist between product types in the real data. \n\n"}
{"id": "1901.06230", "contents": "Title: Computing large market equilibria using abstractions Abstract: Computing market equilibria is an important practical problem for market\ndesign, for example in fair division of items. However, computing equilibria\nrequires large amounts of information (typically the valuation of every buyer\nfor every item) and computing power. We consider ameliorating these issues by\napplying a method used for solving complex games: constructing a coarsened\nabstraction of a given market, solving for the equilibrium in the abstraction,\nand lifting the prices and allocations back to the original market. We show how\nto bound important quantities such as regret, envy, Nash social welfare, Pareto\noptimality, and maximin share/proportionality when the abstracted prices and\nallocations are used in place of the real equilibrium. We then study two\nabstraction methods of interest for practitioners: (1) filling in unknown\nvaluations using techniques from matrix completion, (2) reducing the problem\nsize by aggregating groups of buyers/items into smaller numbers of\nrepresentative buyers/items and solving for equilibrium in this coarsened\nmarket. We find that in real data allocations/prices that are relatively close\nto equilibria can be computed from even very coarse abstractions. \n\n"}
{"id": "1901.06287", "contents": "Title: Distributed control and game design: From strategic agents to\n  programmable machines Abstract: Large scale systems are forecasted to greatly impact our future lives thanks\nto their wide ranging applications including cooperative robotics, mobility on\ndemand, resource allocation, supply chain management. While technological\ndevelopments have paved the way for the realization of such futuristic systems,\nwe have a limited grasp on how to coordinate the individual components to\nachieve the desired global objective. This thesis deals with the analysis and\ncoordination of large scale systems without the need of a centralized\nauthority.\n  In the first part of this thesis, we consider non-cooperative decision making\nproblems where each agent's objective is a function of the aggregate behavior\nof the population. First, we compare the performance of an equilibrium\nallocation with that of an optimal allocation and propose conditions under\nwhich all equilibrium allocations are efficient. Towards this goal, we prove a\nnovel result bounding the distance between the strategies at a Nash and Wardrop\nequilibrium that might be of independent interest. Second, we show how to\nderive scalable algorithms that guide agents towards an equilibrium allocation.\n  In the second part of this thesis, we consider large-scale cooperative\nproblems, where a number of agents need to be allocated to a set of resources\nwith the goal of jointly maximizing a given submodular or supermodular set\nfunction. Since this class of problems is computationally intractable, we aim\nat deriving tractable algorithms for attaining approximate solutions. We\napproach the problem from a game-theoretic perspective and ask the following:\nhow should we design agents' utilities so that any equilibrium configuration is\nalmost optimal? To answer this question we introduce a novel framework that\nallows to characterize and optimize the system performance as a function of the\nchosen utilities by means of a tractable linear program. \n\n"}
{"id": "1901.06830", "contents": "Title: Towards a Functional Fee Market for Cryptocurrencies Abstract: Blockchain-based cryptocurrencies prioritize transactions based on their\nfees, creating a unique kind of fee market. Empirically, this market has failed\nto yield stable equilibria with predictable prices for desired levels of\nservice. We argue that this is due to the absence of a dominant strategy\nequilibrium in the current fee mechanism. We propose an alternative fee setting\nmechanism that is inspired by generalized second price auctions. The design of\nsuch a mechanism is challenging because miners can use any criteria for\nincluding transactions and can manipulate the results of the auction after\nseeing the proposed fees. Nonetheless, we show that our proposed protocol is\nfree from manipulation as the number of users increases. We further show that,\nfor a large number of users and miners, the gain from manipulation is small for\nall parties. This results in users proposing fees that represent their true\nutility and lower variance of revenue for miners. Historical analysis shows\nthat Bitcoin users could have saved $272,528,000 USD in transaction fees while\nminers could have reduced the variance of fee income by an average factor of\n7.4 times. \n\n"}
{"id": "1901.07469", "contents": "Title: Estimating Buildings' Parameters over Time Including Prior Knowledge Abstract: Modeling buildings' heat dynamics is a complex process which depends on\nvarious factors including weather, building thermal capacity, insulation\npreservation, and residents' behavior. Gray-box models offer a causal inference\nof those dynamics expressed in few parameters specific to built environments.\nThese parameters can provide compelling insights into the characteristics of\nbuilding artifacts and have various applications such as forecasting HVAC\nusage, indoor temperature control monitoring of built environments, etc. In\nthis paper, we present a systematic study of modeling buildings' thermal\ncharacteristics and thus derive the parameters of built conditions with a\nBayesian approach. We build a Bayesian state-space model that can adapt and\nincorporate buildings' thermal equations and propose a generalized solution\nthat can easily adapt prior knowledge regarding the parameters. We show that a\nfaster approximate approach using variational inference for parameter\nestimation can provide similar parameters as that of a more time-consuming\nMarkov Chain Monte Carlo (MCMC) approach. We perform extensive evaluations on\ntwo datasets to understand the generative process and show that the Bayesian\napproach is more interpretable. We further study the effects of prior selection\nfor the model parameters and transfer learning, where we learn parameters from\none season and use them to fit the model in the other. We perform extensive\nevaluations on controlled and real data traces to enumerate buildings'\nparameter within a 95% credible interval. \n\n"}
{"id": "1901.07621", "contents": "Title: Single Deep Counterfactual Regret Minimization Abstract: Counterfactual Regret Minimization (CFR) is the most successful algorithm for\nfinding approximate Nash equilibria in imperfect information games. However,\nCFR's reliance on full game-tree traversals limits its scalability. For this\nreason, the game's state- and action-space is often abstracted (i.e.\nsimplified) for CFR, and the resulting strategy is then translated back to the\nfull game, which requires extensive expert-knowledge and often converges to\nhighly exploitable policies. A recently proposed method, Deep CFR, applies deep\nlearning directly to CFR, allowing the agent to intrinsically abstract and\ngeneralize over the state-space from samples, without requiring expert\nknowledge. In this paper, we introduce Single Deep CFR (SD-CFR), a simplified\nvariant of Deep CFR that has a lower overall approximation error by avoiding\nthe training of an average strategy network. We show that SD-CFR is more\nattractive from a theoretical perspective and empirically outperforms Deep CFR\nwith respect to exploitability and one-on-one play in poker. \n\n"}
{"id": "1901.07634", "contents": "Title: DTN: A Learning Rate Scheme with Convergence Rate of $\\mathcal{O}(1/t)$\n  for SGD Abstract: This paper has some inconsistent results, i.e., we made some failed claims\nbecause we did some mistakes for using the test criterion for a series.\nPrecisely, our claims on the convergence rate of $\\mathcal{O}(1/t)$ of SGD\npresented in Theorem 1, Corollary 1, Theorem 2 and Corollary 2 are wrongly\nderived because they are based on Lemma 5. In Lemma 5, we do not correctly use\nthe test criterion for a series. Hence, the result of Lemma 5 is not valid. We\nwould like to thank the community for pointing out this mistake! \n\n"}
{"id": "1901.07859", "contents": "Title: How do Mixture Density RNNs Predict the Future? Abstract: Gaining a better understanding of how and what machine learning systems learn\nis important to increase confidence in their decisions and catalyze further\nresearch. In this paper, we analyze the predictions made by a specific type of\nrecurrent neural network, mixture density RNNs (MD-RNNs). These networks learn\nto model predictions as a combination of multiple Gaussian distributions,\nmaking them particularly interesting for problems where a sequence of inputs\nmay lead to several distinct future possibilities. An example is learning\ninternal models of an environment, where different events may or may not occur,\nbut where the average over different events is not meaningful. By analyzing the\npredictions made by trained MD-RNNs, we find that their different Gaussian\ncomponents have two complementary roles: 1) Separately modeling different\nstochastic events and 2) Separately modeling scenarios governed by different\nrules. These findings increase our understanding of what is learned by\npredictive MD-RNNs, and open up new research directions for further\nunderstanding how we can benefit from their self-organizing model\ndecomposition. \n\n"}
{"id": "1901.08162", "contents": "Title: Causal Reasoning from Meta-reinforcement Learning Abstract: Discovering and exploiting the causal structure in the environment is a\ncrucial challenge for intelligent agents. Here we explore whether causal\nreasoning can emerge via meta-reinforcement learning. We train a recurrent\nnetwork with model-free reinforcement learning to solve a range of problems\nthat each contain causal structure. We find that the trained agent can perform\ncausal reasoning in novel situations in order to obtain rewards. The agent can\nselect informative interventions, draw causal inferences from observational\ndata, and make counterfactual predictions. Although established formal causal\nreasoning algorithms also exist, in this paper we show that such reasoning can\narise from model-free reinforcement learning, and suggest that causal reasoning\nin complex settings may benefit from the more end-to-end learning-based\napproaches presented here. This work also offers new strategies for structured\nexploration in reinforcement learning, by providing agents with the ability to\nperform -- and interpret -- experiments. \n\n"}
{"id": "1901.08508", "contents": "Title: Maximum Entropy Generators for Energy-Based Models Abstract: Maximum likelihood estimation of energy-based models is a challenging problem\ndue to the intractability of the log-likelihood gradient. In this work, we\npropose learning both the energy function and an amortized approximate sampling\nmechanism using a neural generator network, which provides an efficient\napproximation of the log-likelihood gradient. The resulting objective requires\nmaximizing entropy of the generated samples, which we perform using recently\nproposed nonparametric mutual information estimators. Finally, to stabilize the\nresulting adversarial game, we use a zero-centered gradient penalty derived as\na necessary condition from the score matching literature. The proposed\ntechnique can generate sharp images with Inception and FID scores competitive\nwith recent GAN techniques, does not suffer from mode collapse, and is\ncompetitive with state-of-the-art anomaly detection techniques. \n\n"}
{"id": "1901.08669", "contents": "Title: SAGA with Arbitrary Sampling Abstract: We study the problem of minimizing the average of a very large number of\nsmooth functions, which is of key importance in training supervised learning\nmodels. One of the most celebrated methods in this context is the SAGA\nalgorithm. Despite years of research on the topic, a general-purpose version of\nSAGA---one that would include arbitrary importance sampling and minibatching\nschemes---does not exist. We remedy this situation and propose a general and\nflexible variant of SAGA following the {\\em arbitrary sampling} paradigm. We\nperform an iteration complexity analysis of the method, largely possible due to\nthe construction of new stochastic Lyapunov functions. We establish linear\nconvergence rates in the smooth and strongly convex regime, and under a\nquadratic functional growth condition (i.e., in a regime not assuming strong\nconvexity). Our rates match those of the primal-dual method Quartz for which an\narbitrary sampling analysis is available, which makes a significant step\ntowards closing the gap in our understanding of complexity of primal and dual\nmethods for finite sum problems. \n\n"}
{"id": "1901.08689", "contents": "Title: Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are\n  Better Without the Outer Loop Abstract: The stochastic variance-reduced gradient method (SVRG) and its accelerated\nvariant (Katyusha) have attracted enormous attention in the machine learning\ncommunity in the last few years due to their superior theoretical properties\nand empirical behaviour on training supervised machine learning models via the\nempirical risk minimization paradigm. A key structural element in both of these\nmethods is the inclusion of an outer loop at the beginning of which a full pass\nover the training data is made in order to compute the exact gradient, which is\nthen used to construct a variance-reduced estimator of the gradient. In this\nwork we design {\\em loopless variants} of both of these methods. In particular,\nwe remove the outer loop and replace its function by a coin flip performed in\neach iteration designed to trigger, with a small probability, the computation\nof the gradient. We prove that the new methods enjoy the same superior\ntheoretical convergence properties as the original methods. However, we\ndemonstrate through numerical experiments that our methods have substantially\nsuperior practical behavior. \n\n"}
{"id": "1901.08958", "contents": "Title: Perturbed Proximal Descent to Escape Saddle Points for Non-convex and\n  Non-smooth Objective Functions Abstract: We consider the problem of finding local minimizers in non-convex and\nnon-smooth optimization. Under the assumption of strict saddle points, positive\nresults have been derived for first-order methods. We present the first known\nresults for the non-smooth case, which requires different analysis and a\ndifferent algorithm. \n\n"}
{"id": "1901.09068", "contents": "Title: Surrogate Losses for Online Learning of Stepsizes in Stochastic\n  Non-Convex Optimization Abstract: Stochastic Gradient Descent (SGD) has played a central role in machine\nlearning. However, it requires a carefully hand-picked stepsize for fast\nconvergence, which is notoriously tedious and time-consuming to tune. Over the\nlast several years, a plethora of adaptive gradient-based algorithms have\nemerged to ameliorate this problem. They have proved efficient in reducing the\nlabor of tuning in practice, but many of them lack theoretic guarantees even in\nthe convex setting. In this paper, we propose new surrogate losses to cast the\nproblem of learning the optimal stepsizes for the stochastic optimization of a\nnon-convex smooth objective function onto an online convex optimization\nproblem. This allows the use of no-regret online algorithms to compute optimal\nstepsizes on the fly. In turn, this results in a SGD algorithm with self-tuned\nstepsizes that guarantees convergence rates that are automatically adaptive to\nthe level of noise. \n\n"}
{"id": "1901.09109", "contents": "Title: DADAM: A Consensus-based Distributed Adaptive Gradient Method for Online\n  Optimization Abstract: Adaptive gradient-based optimization methods such as \\textsc{Adagrad},\n\\textsc{Rmsprop}, and \\textsc{Adam} are widely used in solving large-scale\nmachine learning problems including deep learning. A number of schemes have\nbeen proposed in the literature aiming at parallelizing them, based on\ncommunications of peripheral nodes with a central node, but incur high\ncommunications cost. To address this issue, we develop a novel consensus-based\ndistributed adaptive moment estimation method (\\textsc{Dadam}) for online\noptimization over a decentralized network that enables data parallelization, as\nwell as decentralized computation. The method is particularly useful, since it\ncan accommodate settings where access to local data is allowed. Further, as\nestablished theoretically in this work, it can outperform centralized adaptive\nalgorithms, for certain classes of loss functions used in applications. We\nanalyze the convergence properties of the proposed algorithm and provide a\ndynamic regret bound on the convergence rate of adaptive moment estimation\nmethods in both stochastic and deterministic settings. Empirical results\ndemonstrate that \\textsc{Dadam} works also well in practice and compares\nfavorably to competing online optimization methods. \n\n"}
{"id": "1901.09149", "contents": "Title: Escaping Saddle Points with Adaptive Gradient Methods Abstract: Adaptive methods such as Adam and RMSProp are widely used in deep learning\nbut are not well understood. In this paper, we seek a crisp, clean and precise\ncharacterization of their behavior in nonconvex settings. To this end, we first\nprovide a novel view of adaptive methods as preconditioned SGD, where the\npreconditioner is estimated in an online manner. By studying the preconditioner\non its own, we elucidate its purpose: it rescales the stochastic gradient noise\nto be isotropic near stationary points, which helps escape saddle points.\nFurthermore, we show that adaptive methods can efficiently estimate the\naforementioned preconditioner. By gluing together these two components, we\nprovide the first (to our knowledge) second-order convergence result for any\nadaptive method. The key insight from our analysis is that, compared to SGD,\nadaptive methods escape saddle points faster, and can converge faster overall\nto second-order stationary points. \n\n"}
{"id": "1901.09269", "contents": "Title: Distributed Learning with Compressed Gradient Differences Abstract: Training large machine learning models requires a distributed computing\napproach, with communication of the model updates being the bottleneck. For\nthis reason, several methods based on the compression (e.g., sparsification\nand/or quantization) of updates were recently proposed, including QSGD\n(Alistarh et al., 2017), TernGrad (Wen et al., 2017), SignSGD (Bernstein et\nal., 2018), and DQGD (Khirirat et al., 2018). However, none of these methods\nare able to learn the gradients, which renders them incapable of converging to\nthe true optimum in the batch mode. In this work we propose a new distributed\nlearning method -- DIANA -- which resolves this issue via compression of\ngradient differences. We perform a theoretical analysis in the strongly convex\nand nonconvex settings and show that our rates are superior to existing rates.\nWe also provide theory to support non-smooth regularizers study the difference\nbetween quantization schemes. Our analysis of block-quantization and\ndifferences between $\\ell_2$ and $\\ell_{\\infty}$ quantization closes the gaps\nin theory and practice. Finally, by applying our analysis technique to\nTernGrad, we establish the first convergence rate for this method. \n\n"}
{"id": "1901.09401", "contents": "Title: SGD: General Analysis and Improved Rates Abstract: We propose a general yet simple theorem describing the convergence of SGD\nunder the arbitrary sampling paradigm. Our theorem describes the convergence of\nan infinite array of variants of SGD, each of which is associated with a\nspecific probability law governing the data selection rule used to form\nmini-batches. This is the first time such an analysis is performed, and most of\nour variants of SGD were never explicitly considered in the literature before.\nOur analysis relies on the recently introduced notion of expected smoothness\nand does not rely on a uniform bound on the variance of the stochastic\ngradients. By specializing our theorem to different mini-batching strategies,\nsuch as sampling with replacement and independent sampling, we derive exact\nexpressions for the stepsize as a function of the mini-batch size. With this we\ncan also determine the mini-batch size that optimizes the total complexity, and\nshow explicitly that as the variance of the stochastic gradient evaluated at\nthe minimum grows, so does the optimal mini-batch size. For zero variance, the\noptimal mini-batch size is one. Moreover, we prove insightful\nstepsize-switching rules which describe when one should switch from a constant\nto a decreasing stepsize regime. \n\n"}
{"id": "1901.09437", "contents": "Title: 99% of Distributed Optimization is a Waste of Time: The Issue and How to\n  Fix it Abstract: Many popular distributed optimization methods for training machine learning\nmodels fit the following template: a local gradient estimate is computed\nindependently by each worker, then communicated to a master, which subsequently\nperforms averaging. The average is broadcast back to the workers, which use it\nto perform a gradient-type step to update the local version of the model. It is\nalso well known that many such methods, including SGD, SAGA, and accelerated\nSGD for over-parameterized models, do not scale well with the number of\nparallel workers. In this paper we observe that the above template is\nfundamentally inefficient in that too much data is unnecessarily communicated\nby the workers, which slows down the overall system. We propose a fix based on\na new update-sparsification method we develop in this work, which we suggest be\nused on top of existing methods. Namely, we develop a new variant of parallel\nblock coordinate descent based on independent sparsification of the local\ngradient estimates before communication. We demonstrate that with only $m/n$\nblocks sent by each of $n$ workers, where $m$ is the total number of parameter\nblocks, the theoretical iteration complexity of the underlying distributed\nmethods is essentially unaffected. As an illustration, this means that when\n$n=100$ parallel workers are used, the communication of $99\\%$ blocks is\nredundant, and hence a waste of time. Our theoretical claims are supported\nthrough extensive numerical experiments which demonstrate an almost perfect\nmatch with our theory on a number of synthetic and real datasets. \n\n"}
{"id": "1901.09453", "contents": "Title: On Learning Invariant Representation for Domain Adaptation Abstract: Due to the ability of deep neural nets to learn rich representations, recent\nadvances in unsupervised domain adaptation have focused on learning\ndomain-invariant features that achieve a small error on the source domain. The\nhope is that the learnt representation, together with the hypothesis learnt\nfrom the source domain, can generalize to the target domain. In this paper, we\nfirst construct a simple counterexample showing that, contrary to common\nbelief, the above conditions are not sufficient to guarantee successful domain\nadaptation. In particular, the counterexample exhibits \\emph{conditional\nshift}: the class-conditional distributions of input features change between\nsource and target domains. To give a sufficient condition for domain\nadaptation, we propose a natural and interpretable generalization upper bound\nthat explicitly takes into account the aforementioned shift. Moreover, we shed\nnew light on the problem by proving an information-theoretic lower bound on the\njoint error of \\emph{any} domain adaptation method that attempts to learn\ninvariant representations. Our result characterizes a fundamental tradeoff\nbetween learning invariant representations and achieving small joint error on\nboth domains when the marginal label distributions differ from source to\ntarget. Finally, we conduct experiments on real-world datasets that corroborate\nour theoretical findings. We believe these insights are helpful in guiding the\nfuture design of domain adaptation and representation learning algorithms. \n\n"}
{"id": "1901.09465", "contents": "Title: Deconstructing Generative Adversarial Networks Abstract: We deconstruct the performance of GANs into three components:\n  1. Formulation: we propose a perturbation view of the population target of\nGANs. Building on this interpretation, we show that GANs can be viewed as a\ngeneralization of the robust statistics framework, and propose a novel GAN\narchitecture, termed as Cascade GANs, to provably recover meaningful\nlow-dimensional generator approximations when the real distribution is\nhigh-dimensional and corrupted by outliers.\n  2. Generalization: given a population target of GANs, we design a systematic\nprinciple, projection under admissible distance, to design GANs to meet the\npopulation requirement using finite samples. We implement our principle in\nthree cases to achieve polynomial and sometimes near-optimal sample\ncomplexities: (1) learning an arbitrary generator under an arbitrary\npseudonorm; (2) learning a Gaussian location family under TV distance, where we\nutilize our principle provide a new proof for the optimality of Tukey median\nviewed as GANs; (3) learning a low-dimensional Gaussian approximation of a\nhigh-dimensional arbitrary distribution under Wasserstein distance. We\ndemonstrate a fundamental trade-off in the approximation error and statistical\nerror in GANs, and show how to apply our principle with empirical samples to\npredict how many samples are sufficient for GANs in order not to suffer from\nthe discriminator winning problem.\n  3. Optimization: we demonstrate alternating gradient descent is provably not\nlocally asymptotically stable in optimizing the GAN formulation of PCA. We\ndiagnose the problem as the minimax duality gap being non-zero, and propose a\nnew GAN architecture whose duality gap is zero, where the value of the game is\nequal to the previous minimax value (not the maximin value). We prove the new\nGAN architecture is globally asymptotically stable in optimization under\nalternating gradient descent. \n\n"}
{"id": "1901.09515", "contents": "Title: Black Box Submodular Maximization: Discrete and Continuous Settings Abstract: In this paper, we consider the problem of black box continuous submodular\nmaximization where we only have access to the function values and no\ninformation about the derivatives is provided. For a monotone and continuous\nDR-submodular function, and subject to a bounded convex body constraint, we\npropose Black-box Continuous Greedy, a derivative-free algorithm that provably\nachieves the tight $[(1-1/e)OPT-\\epsilon]$ approximation guarantee with\n$O(d/\\epsilon^3)$ function evaluations. We then extend our result to the\nstochastic setting where function values are subject to stochastic zero-mean\nnoise. It is through this stochastic generalization that we revisit the\ndiscrete submodular maximization problem and use the multi-linear extension as\na bridge between discrete and continuous settings. Finally, we extensively\nevaluate the performance of our algorithm on continuous and discrete submodular\nobjective functions using both synthetic and real data. \n\n"}
{"id": "1901.09895", "contents": "Title: Modularization of End-to-End Learning: Case Study in Arcade Games Abstract: Complex environments and tasks pose a difficult problem for holistic\nend-to-end learning approaches. Decomposition of an environment into\ninteracting controllable and non-controllable objects allows supervised\nlearning for non-controllable objects and universal value function approximator\nlearning for controllable objects. Such decomposition should lead to a shorter\nlearning time and better generalisation capability. Here, we consider\narcade-game environments as sets of interacting objects (controllable,\nnon-controllable) and propose a set of functional modules that are specialized\non mastering different types of interactions in a broad range of environments.\nThe modules utilize regression, supervised learning, and reinforcement learning\nalgorithms. Results of this case study in different Atari games suggest that\nhuman-level performance can be achieved by a learning agent within a human\namount of game experience (10-15 minutes game time) when a proper decomposition\nof an environment or a task is provided. However, automatization of such\ndecomposition remains a challenging problem. This case study shows how a model\nof a causal structure underlying an environment or a task can benefit learning\ntime and generalization capability of the agent, and argues in favor of\nexploiting modular structure in contrast to using pure end-to-end learning\napproaches. \n\n"}
{"id": "1901.09993", "contents": "Title: Label Efficient Semi-Supervised Learning via Graph Filtering Abstract: Graph-based methods have been demonstrated as one of the most effective\napproaches for semi-supervised learning, as they can exploit the connectivity\npatterns between labeled and unlabeled data samples to improve learning\nperformance. However, existing graph-based methods either are limited in their\nability to jointly model graph structures and data features, such as the\nclassical label propagation methods, or require a considerable amount of\nlabeled data for training and validation due to high model complexity, such as\nthe recent neural-network-based methods. In this paper, we address label\nefficient semi-supervised learning from a graph filtering perspective.\nSpecifically, we propose a graph filtering framework that injects graph\nsimilarity into data features by taking them as signals on the graph and\napplying a low-pass graph filter to extract useful data representations for\nclassification, where label efficiency can be achieved by conveniently\nadjusting the strength of the graph filter. Interestingly, this framework\nunifies two seemingly very different methods -- label propagation and graph\nconvolutional networks. Revisiting them under the graph filtering framework\nleads to new insights that improve their modeling capabilities and reduce model\ncomplexity. Experiments on various semi-supervised classification tasks on four\ncitation networks and one knowledge graph and one semi-supervised regression\ntask for zero-shot image recognition validate our findings and proposals. \n\n"}
{"id": "1901.10064", "contents": "Title: Committee Selection with Attribute Level Preferences Abstract: We consider the problem of committee selection from a fixed set of candidates\nwhere each candidate has multiple quantifiable attributes. To select the best\npossible committee, instead of voting for a candidate, a voter is allowed to\napprove the preferred attributes of a given candidate. Though attribute based\npreference is addressed in several contexts, committee selection problem with\nattribute approval of voters has not been attempted earlier. A committee formed\non attribute preferences is more likely to be a better representative of the\nqualities desired by the voters and is less likely to be susceptible to\ncollusion or manipulation. In this work, we provide a formal study of the\ndifferent aspects of this problem and define properties of weak unanimity,\nstrong unanimity, simple justified representations and compound justified\nrepresentation, that are required to be satisfied by the selected committee. We\nshow that none of the existing vote/approval aggregation rules satisfy these\nnew properties for attribute aggregation. We describe a greedy approach for\nattribute aggregation that satisfies the first three properties, but not the\nfourth, i.e., compound justified representation, which we prove to be\nNP-complete. Furthermore, we prove that finding a committee with justified\nrepresentation and the highest approval voting score is NP-complete. \n\n"}
{"id": "1901.10757", "contents": "Title: Distributionally Robust and Multi-Objective Nonnegative Matrix\n  Factorization Abstract: Nonnegative matrix factorization (NMF) is a linear dimensionality reduction\ntechnique for analyzing nonnegative data. A key aspect of NMF is the choice of\nthe objective function that depends on the noise model (or statistics of the\nnoise) assumed on the data. In many applications, the noise model is unknown\nand difficult to estimate. In this paper, we define a multi-objective NMF\n(MO-NMF) problem, where several objectives are combined within the same NMF\nmodel. We propose to use Lagrange duality to judiciously optimize for a set of\nweights to be used within the framework of the weighted-sum approach, that is,\nwe minimize a single objective function which is a weighted sum of the all\nobjective functions. We design a simple algorithm based on multiplicative\nupdates to minimize this weighted sum. We show how this can be used to find\ndistributionally robust NMF (DR-NMF) solutions, that is, solutions that\nminimize the largest error among all objectives, using a dual approach solved\nvia a heuristic inspired from the Frank-Wolfe algorithm. We illustrate the\neffectiveness of this approach on synthetic, document and audio data sets. The\nresults show that DR-NMF is robust to our incognizance of the noise model of\nthe NMF problem. \n\n"}
{"id": "1901.10923", "contents": "Title: Coordinating the Crowd: Inducing Desirable Equilibria in Non-Cooperative\n  Systems Abstract: Many real-world systems such as taxi systems, traffic networks and smart\ngrids involve self-interested actors that perform individual tasks in a shared\nenvironment. However, in such systems, the self-interested behaviour of agents\nproduces welfare inefficient and globally suboptimal outcomes that are\ndetrimental to all - some common examples are congestion in traffic networks,\ndemand spikes for resources in electricity grids and over-extraction of\nenvironmental resources such as fisheries. We propose an incentive-design\nmethod which modifies agents' rewards in non-cooperative multi-agent systems\nthat results in independent, self-interested agents choosing actions that\nproduce optimal system outcomes in strategic settings. Our framework combines\nmulti-agent reinforcement learning to simulate (real-world) agent behaviour and\nblack-box optimisation to determine the optimal modifications to the agents'\nrewards or incentives given some fixed budget that results in optimal system\nperformance. By modifying the reward functions and generating agents'\nequilibrium responses within a sequence of offline Markov games, our method\nenables optimal incentive structures to be determined offline through iterative\nupdates of the reward functions of a simulated game. Our theoretical results\nshow that our method converges to reward modifications that induce system\noptimality. We demonstrate the applications of our framework by tackling a\nchallenging problem within economics that involves thousands of selfish agents\nand tackle a traffic congestion problem. \n\n"}
{"id": "1901.10995", "contents": "Title: Go-Explore: a New Approach for Hard-Exploration Problems Abstract: A grand challenge in reinforcement learning is intelligent exploration,\nespecially when rewards are sparse or deceptive. Two Atari games serve as\nbenchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall.\nOn both games, current RL algorithms perform poorly, even those with intrinsic\nmotivation, which is the dominant method to improve performance on\nhard-exploration domains. To address this shortfall, we introduce a new\nalgorithm called Go-Explore. It exploits the following principles: (1) remember\npreviously visited states, (2) first return to a promising state (without\nexploration), then explore from it, and (3) solve simulated environments\nthrough any available means (including by introducing determinism), then\nrobustify via imitation learning. The combined effect of these principles is a\ndramatic performance improvement on hard-exploration problems. On Montezuma's\nRevenge, Go-Explore scores a mean of over 43k points, almost 4 times the\nprevious state of the art. Go-Explore can also harness human-provided domain\nknowledge and, when augmented with it, scores a mean of over 650k points on\nMontezuma's Revenge. Its max performance of nearly 18 million surpasses the\nhuman world record, meeting even the strictest definition of \"superhuman\"\nperformance. On Pitfall, Go-Explore with domain knowledge is the first\nalgorithm to score above zero. Its mean score of almost 60k points exceeds\nexpert human performance. Because Go-Explore produces high-performing\ndemonstrations automatically and cheaply, it also outperforms imitation\nlearning work where humans provide solution demonstrations. Go-Explore opens up\nmany new research directions into improving it and weaving its insights into\ncurrent RL algorithms. It may also enable progress on previously unsolvable\nhard-exploration problems in many domains, especially those that harness a\nsimulator during training (e.g. robotics). \n\n"}
{"id": "1901.11150", "contents": "Title: Memory-Efficient Adaptive Optimization Abstract: Adaptive gradient-based optimizers such as Adagrad and Adam are crucial for\nachieving state-of-the-art performance in machine translation and language\nmodeling. However, these methods maintain second-order statistics for each\nparameter, thus introducing significant memory overheads that restrict the size\nof the model being used as well as the number of examples in a mini-batch. We\ndescribe an effective and flexible adaptive optimization method with greatly\nreduced memory overhead. Our method retains the benefits of per-parameter\nadaptivity while allowing significantly larger models and batch sizes. We give\nconvergence guarantees for our method, and demonstrate its effectiveness in\ntraining very large translation and language models with up to 2-fold speedups\ncompared to the state-of-the-art. \n\n"}
{"id": "1901.11524", "contents": "Title: The Value Function Polytope in Reinforcement Learning Abstract: We establish geometric and topological properties of the space of value\nfunctions in finite state-action Markov decision processes. Our main\ncontribution is the characterization of the nature of its shape: a general\npolytope (Aigner et al., 2010). To demonstrate this result, we exhibit several\nproperties of the structural relationship between policies and value functions\nincluding the line theorem, which shows that the value functions of policies\nconstrained on all but one state describe a line segment. Finally, we use this\nnovel perspective to introduce visualizations to enhance the understanding of\nthe dynamics of reinforcement learning algorithms. \n\n"}
{"id": "cond-mat/0402508", "contents": "Title: Information Theory - The Bridge Connecting Bounded Rational Game Theory\n  and Statistical Physics Abstract: A long-running difficulty with conventional game theory has been how to\nmodify it to accommodate the bounded rationality of all real-world players. A\nrecurring issue in statistical physics is how best to approximate joint\nprobability distributions with decoupled (and therefore far more tractable)\ndistributions. This paper shows that the same information theoretic\nmathematical structure, known as Product Distribution (PD) theory, addresses\nboth issues. In this, PD theory not only provides a principled formulation of\nbounded rationality and a set of new types of mean field theory in statistical\nphysics. It also shows that those topics are fundamentally one and the same. \n\n"}
{"id": "cs/0204041", "contents": "Title: Trust Brokerage Systems for the Internet Abstract: This thesis addresses the problem of providing trusted individuals with\nconfidential information about other individuals, in particular, granting\naccess to databases of personal records using the World-Wide Web. It proposes\nan access rights management system for distributed databases which aims to\ncreate and implement organisation structures based on the wishes of the owners\nand of demands of the users of the databases. The dissertation describes how\ncurrent software components could be used to implement this system; it\nre-examines the theory of collective choice to develop mechanisms for\ngenerating hierarchies of authorities; it analyses organisational processes for\nstability and develops a means of measuring the similarity of their\nhierarchies. \n\n"}
{"id": "cs/0602090", "contents": "Title: On the Approximation and Smoothed Complexity of Leontief Market\n  Equilibria Abstract: We show that the problem of finding an \\epsilon-approximate Nash equilibrium\nof an n by n two-person games can be reduced to the computation of an\n(\\epsilon/n)^2-approximate market equilibrium of a Leontief economy. Together\nwith a recent result of Chen, Deng and Teng, this polynomial reduction implies\nthat the Leontief market exchange problem does not have a fully polynomial-time\napproximation scheme, that is, there is no algorithm that can compute an\n\\epsilon-approximate market equilibrium in time polynomial in m, n, and\n1/\\epsilon, unless PPAD is not in P, We also extend the analysis of our\nreduction to show, unless PPAD is not in RP, that the smoothed complexity of\nthe Scarf's general fixed-point approximation algorithm (when applying to solve\nthe approximate Leontief market exchange problem) or of any algorithm for\ncomputing an approximate market equilibrium of Leontief economies is not\npolynomial in n and 1/\\sigma, under Gaussian or uniform perturbations with\nmagnitude \\sigma. \n\n"}
{"id": "cs/0608057", "contents": "Title: Hybrid Elections Broaden Complexity-Theoretic Resistance to Control Abstract: Electoral control refers to attempts by an election's organizer (\"the chair\")\nto influence the outcome by adding/deleting/partitioning voters or candidates.\nThe groundbreaking work of Bartholdi, Tovey, and Trick [BTT92] on\n(constructive) control proposes computational complexity as a means of\nresisting control attempts: Look for election systems where the chair's task in\nseeking control is itself computationally infeasible.\n  We introduce and study a method of combining two or more candidate-anonymous\nelection schemes in such a way that the combined scheme possesses all the\nresistances to control (i.e., all the NP-hardnesses of control) possessed by\nany of its constituents: It combines their strengths. From this and new\nresistance constructions, we prove for the first time that there exists an\nelection scheme that is resistant to all twenty standard types of electoral\ncontrol. \n\n"}
{"id": "cs/0609056", "contents": "Title: Matrix Games, Linear Programming, and Linear Approximation Abstract: The following four classes of computational problems are equivalent: solving\nmatrix games, solving linear programs, best $l^{\\infty}$ linear approximation,\nbest $l^1$ linear approximation. \n\n"}
{"id": "cs/0703097", "contents": "Title: On Approximating Optimal Weighted Lobbying, and Frequency of Correctness\n  versus Average-Case Polynomial Time Abstract: We investigate issues related to two hard problems related to voting, the\noptimal weighted lobbying problem and the winner problem for Dodgson elections.\nRegarding the former, Christian et al. [CFRS06] showed that optimal lobbying is\nintractable in the sense of parameterized complexity. We provide an efficient\ngreedy algorithm that achieves a logarithmic approximation ratio for this\nproblem and even for a more general variant--optimal weighted lobbying. We\nprove that essentially no better approximation ratio than ours can be proven\nfor this greedy algorithm.\n  The problem of determining Dodgson winners is known to be complete for\nparallel access to NP [HHR97]. Homan and Hemaspaandra [HH06] proposed an\nefficient greedy heuristic for finding Dodgson winners with a guaranteed\nfrequency of success, and their heuristic is a ``frequently self-knowingly\ncorrect algorithm.'' We prove that every distributional problem solvable in\npolynomial time on the average with respect to the uniform distribution has a\nfrequently self-knowingly correct polynomial-time algorithm. Furthermore, we\nstudy some features of probability weight of correctness with respect to\nProcaccia and Rosenschein's junta distributions [PR07]. \n\n"}
{"id": "physics/0512045", "contents": "Title: Topology Induced Coarsening in Language Games Abstract: We investigate how very large populations are able to reach a global\nconsensus, out of local \"microscopic\" interaction rules, in the framework of a\nrecently introduced class of models of semiotic dynamics, the so-called Naming\nGame. We compare in particular the convergence mechanism for interacting agents\nembedded in a low-dimensional lattice with respect to the mean-field case. We\nhighlight that in low-dimensions consensus is reached through a coarsening\nprocess which requires less cognitive effort of the agents, with respect to the\nmean-field case, but takes longer to complete. In 1-d the dynamics of the\nboundaries is mapped onto a truncated Markov process from which we analytically\ncomputed the diffusion coefficient. More generally we show that the convergence\nprocess requires a memory per agent scaling as N and lasts a time N^{1+2/d} in\ndimension d<5 (d=4 being the upper critical dimension), while in mean-field\nboth memory and time scale as N^{3/2}, for a population of N agents. We present\nanalytical and numerical evidences supporting this picture. \n\n"}
{"id": "quant-ph/0211191", "contents": "Title: An invitation to Quantum Game Theory Abstract: Recent development in quantum computation and quantum information theory\nallows to extend the scope of game theory for the quantum world. The paper\npresents the history, basic ideas and recent development in quantum game\ntheory. In this context, a new application of the Ising chain model is\nproposed. \n\n"}
