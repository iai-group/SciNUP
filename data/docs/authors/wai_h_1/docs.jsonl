{"id": "0807.1550", "contents": "Title: Discernment of Hubs and Clusters in Socioeconomic Networks Abstract: Interest in the analysis of networks has grown rapidly in the new millennium.\nConsequently, we promote renewed attention to a certain methodological approach\nintroduced in 1974. Over the succeeding decade, this\ntwo-stage--double-standardization and hierarchical clustering\n(single-linkage-like)--procedure was applied to a wide variety of weighted,\ndirected networks of a socioeconomic nature, frequently revealing the presence\nof ``hubs''. These were, typically--in the numerous instances studied of\nmigration flows between geographic subdivisions within\nnations--``cosmopolitan/non-provincial'' areas, a prototypical example being\nthe French capital, Paris. Such locations emit and absorb people broadly across\ntheir respective nations. Additionally, the two-stage procedure--which ``might\nvery well be the most successful application of cluster analysis'' (R. C.\nDubes, 1985)--detected many (physically or socially) isolated, functional\ngroups (regions) of areas, such as the southern islands, Shikoku and Kyushu, of\nJapan, the Italian islands of Sardinia and Sicily, and the New England region\nof the United States. Further, we discuss a (complementary) approach developed\nin 1976, in which the max-flow/min-cut theorem was applied to\nraw/non-standardized (interindustry, as well as migration) flows. \n\n"}
{"id": "0906.0060", "contents": "Title: A Walk in Facebook: Uniform Sampling of Users in Online Social Networks Abstract: Our goal in this paper is to develop a practical framework for obtaining a\nuniform sample of users in an online social network (OSN) by crawling its\nsocial graph. Such a sample allows to estimate any user property and some\ntopological properties as well. To this end, first, we consider and compare\nseveral candidate crawling techniques. Two approaches that can produce\napproximately uniform samples are the Metropolis-Hasting random walk (MHRW) and\na re-weighted random walk (RWRW). Both have pros and cons, which we demonstrate\nthrough a comparison to each other as well as to the \"ground truth.\" In\ncontrast, using Breadth-First-Search (BFS) or an unadjusted Random Walk (RW)\nleads to substantially biased results. Second, and in addition to offline\nperformance assessment, we introduce online formal convergence diagnostics to\nassess sample quality during the data collection process. We show how these\ndiagnostics can be used to effectively determine when a random walk sample is\nof adequate size and quality. Third, as a case study, we apply the above\nmethods to Facebook and we collect the first, to the best of our knowledge,\nrepresentative sample of Facebook users. We make it publicly available and\nemploy it to characterize several key properties of Facebook. \n\n"}
{"id": "0908.0050", "contents": "Title: Online Learning for Matrix Factorization and Sparse Coding Abstract: Sparse coding--that is, modelling data vectors as sparse linear combinations\nof basis elements--is widely used in machine learning, neuroscience, signal\nprocessing, and statistics. This paper focuses on the large-scale matrix\nfactorization problem that consists of learning the basis set, adapting it to\nspecific data. Variations of this problem include dictionary learning in signal\nprocessing, non-negative matrix factorization and sparse principal component\nanalysis. In this paper, we propose to address these tasks with a new online\noptimization algorithm, based on stochastic approximations, which scales up\ngracefully to large datasets with millions of training samples, and extends\nnaturally to various matrix factorization formulations, making it suitable for\na wide range of learning problems. A proof of convergence is presented, along\nwith experiments with natural images and genomic data demonstrating that it\nleads to state-of-the-art performance in terms of speed and optimization for\nboth small and large datasets. \n\n"}
{"id": "1005.0437", "contents": "Title: A Unifying View of Multiple Kernel Learning Abstract: Recent research on multiple kernel learning has lead to a number of\napproaches for combining kernels in regularized risk minimization. The proposed\napproaches include different formulations of objectives and varying\nregularization strategies. In this paper we present a unifying general\noptimization criterion for multiple kernel learning and show how existing\nformulations are subsumed as special cases. We also derive the criterion's dual\nrepresentation, which is suitable for general smooth optimization algorithms.\nFinally, we evaluate multiple kernel learning in this framework analytically\nusing a Rademacher complexity bound on the generalization error and empirically\nin a set of experiments. \n\n"}
{"id": "1005.3579", "contents": "Title: Graph-Structured Multi-task Regression and an Efficient Optimization\n  Method for General Fused Lasso Abstract: We consider the problem of learning a structured multi-task regression, where\nthe output consists of multiple responses that are related by a graph and the\ncorrelated response variables are dependent on the common inputs in a sparse\nbut synergistic manner. Previous methods such as l1/l2-regularized multi-task\nregression assume that all of the output variables are equally related to the\ninputs, although in many real-world problems, outputs are related in a complex\nmanner. In this paper, we propose graph-guided fused lasso (GFlasso) for\nstructured multi-task regression that exploits the graph structure over the\noutput variables. We introduce a novel penalty function based on fusion penalty\nto encourage highly correlated outputs to share a common set of relevant\ninputs. In addition, we propose a simple yet efficient proximal-gradient method\nfor optimizing GFlasso that can also be applied to any optimization problems\nwith a convex smooth loss and the general class of fusion penalty defined on\narbitrary graph structures. By exploiting the structure of the non-smooth\n''fusion penalty'', our method achieves a faster convergence rate than the\nstandard first-order method, sub-gradient method, and is significantly more\nscalable than the widely adopted second-order cone-programming and\nquadratic-programming formulations. In addition, we provide an analysis of the\nconsistency property of the GFlasso model. Experimental results not only\ndemonstrate the superiority of GFlasso over the standard lasso but also show\nthe efficiency and scalability of our proximal-gradient method. \n\n"}
{"id": "1005.4717", "contents": "Title: Smoothing proximal gradient method for general structured sparse\n  regression Abstract: We study the problem of estimating high-dimensional regression models\nregularized by a structured sparsity-inducing penalty that encodes prior\nstructural information on either the input or output variables. We consider two\nwidely adopted types of penalties of this kind as motivating examples: (1) the\ngeneral overlapping-group-lasso penalty, generalized from the group-lasso\npenalty; and (2) the graph-guided-fused-lasso penalty, generalized from the\nfused-lasso penalty. For both types of penalties, due to their nonseparability\nand nonsmoothness, developing an efficient optimization method remains a\nchallenging problem. In this paper we propose a general optimization approach,\nthe smoothing proximal gradient (SPG) method, which can solve structured sparse\nregression problems with any smooth convex loss under a wide spectrum of\nstructured sparsity-inducing penalties. Our approach combines a smoothing\ntechnique with an effective proximal gradient method. It achieves a convergence\nrate significantly faster than the standard first-order methods, subgradient\nmethods, and is much more scalable than the most widely used interior-point\nmethods. The efficiency and scalability of our method are demonstrated on both\nsimulation experiments and real genetic data sets. \n\n"}
{"id": "1005.5718", "contents": "Title: Agent-based Social Psychology: from Neurocognitive Processes to Social\n  Data Abstract: Moral Foundation Theory states that groups of different observers may rely on\npartially dissimilar sets of moral foundations, thereby reaching different\nmoral valuations. The use of functional imaging techniques has revealed a\nspectrum of cognitive styles with respect to the differential handling of novel\nor corroborating information that is correlated to political affiliation. Here\nwe characterize the collective behavior of an agent-based model whose inter\nindividual interactions due to information exchange in the form of opinions are\nin qualitative agreement with experimental neuroscience data. The main\nconclusion derived connects the existence of diversity in the cognitive\nstrategies and statistics of the sets of moral foundations and suggests that\nthis connection arises from interactions between agents. Thus a simple\ninteracting agent model, whose interactions are in accord with empirical data\non conformity and learning processes, presents statistical signatures\nconsistent with moral judgment patterns of conservatives and liberals as\nobtained by survey studies of social psychology. \n\n"}
{"id": "1008.4115", "contents": "Title: Noise in Naming Games, partial synchronization and community detection\n  in social networks Abstract: The Naming Games (NG) are agent-based models for agreement dynamics, peer\npressure and herding in social networks, and protocol selection in autonomous\nad-hoc sensor networks. By introducing a small noise term to the NG, the\nresulting Markov Chain model called Noisy Naming Games (NNG) are ergodic, in\nwhich all partial consensus states are recurrent. By using Gibbs-Markov\nequivalence we show how to get the NNG's stationary distribution in terms of\nthe local specification of a related Markov Random Field (MRF). By ordering the\npartially-synchronized states according to their Gibbs energy, taken here to be\na good measure of social tension, this method offers an enhanced method for\ncommunity-detection in social interaction data. We show how the lowest Gibbs\nenergy multi-name states separate and display the hidden community structures\nwithin a social network. \n\n"}
{"id": "1010.4088", "contents": "Title: Generalized Clustering Coefficients and Milgram Condition for q-th\n  Degrees of Separation Abstract: We introduce a series of generalized clustering coefficients based on String\nformalism given by Aoyama, using adjacent matrix in networks. We numerically\nevaluate Milgram condition proposed in order to explore q-th degrees of\nseparation in scale free networks and small world networks. We find that scale\nfree network with exponent 3 just shows 6-degrees of separation. Moreover we\nfind some relations between separation numbers and generalized clustering\ncoefficient in both networks. \n\n"}
{"id": "1010.4971", "contents": "Title: Correlated couplings and robustness of coupled networks Abstract: Most real-world complex systems can be modelled by coupled networks with\nmultiple layers. How and to what extent the pattern of couplings between\nnetwork layers may influence the interlaced structure and function of coupled\nnetworks are not clearly understood. Here we study the impact of correlated\ninter-layer couplings on the network robustness of coupled networks using\npercolation concept. We found that the positive correlated inter-layer coupling\nenhaces network robustness in the sense that it lowers the percolation\nthreshold of the interlaced network than the negative correlated coupling case.\nAt the same time, however, positive inter-layer correlation leads to smaller\ngiant component size in the well-connected region, suggesting potential\ndisadvantage for network connectivity, as demonstrated also with some\nreal-world coupled network structures. \n\n"}
{"id": "1010.6032", "contents": "Title: Recurrence-based time series analysis by means of complex network\n  methods Abstract: Complex networks are an important paradigm of modern complex systems sciences\nwhich allows quantitatively assessing the structural properties of systems\ncomposed of different interacting entities. During the last years, intensive\nefforts have been spent on applying network-based concepts also for the\nanalysis of dynamically relevant higher-order statistical properties of time\nseries. Notably, many corresponding approaches are closely related with the\nconcept of recurrence in phase space. In this paper, we review recent\nmethodological advances in time series analysis based on complex networks, with\na special emphasis on methods founded on recurrence plots. The potentials and\nlimitations of the individual methods are discussed and illustrated for\nparadigmatic examples of dynamical systems as well as for real-world time\nseries. Complex network measures are shown to provide information about\nstructural features of dynamical systems that are complementary to those\ncharacterized by other methods of time series analysis and, hence,\nsubstantially enrich the knowledge gathered from other existing (linear as well\nas nonlinear) approaches. \n\n"}
{"id": "1011.0468", "contents": "Title: Efficient Triangle Counting in Large Graphs via Degree-based Vertex\n  Partitioning Abstract: The number of triangles is a computationally expensive graph statistic which\nis frequently used in complex network analysis (e.g., transitivity ratio), in\nvarious random graph models (e.g., exponential random graph model) and in\nimportant real world applications such as spam detection, uncovering of the\nhidden thematic structure of the Web and link recommendation. Counting\ntriangles in graphs with millions and billions of edges requires algorithms\nwhich run fast, use small amount of space, provide accurate estimates of the\nnumber of triangles and preferably are parallelizable.\n  In this paper we present an efficient triangle counting algorithm which can\nbe adapted to the semistreaming model. The key idea of our algorithm is to\ncombine the sampling algorithm of Tsourakakis et al. and the partitioning of\nthe set of vertices into a high degree and a low degree subset respectively as\nin the Alon, Yuster and Zwick work treating each set appropriately. We obtain a\nrunning time $O \\left(m + \\frac{m^{3/2} \\Delta \\log{n}}{t \\epsilon^2} \\right)$\nand an $\\epsilon$ approximation (multiplicative error), where $n$ is the number\nof vertices, $m$ the number of edges and $\\Delta$ the maximum number of\ntriangles an edge is contained.\n  Furthermore, we show how this algorithm can be adapted to the semistreaming\nmodel with space usage $O\\left(m^{1/2}\\log{n} + \\frac{m^{3/2} \\Delta \\log{n}}{t\n\\epsilon^2} \\right)$ and a constant number of passes (three) over the graph\nstream. We apply our methods in various networks with several millions of edges\nand we obtain excellent results. Finally, we propose a random projection based\nmethod for triangle counting and provide a sufficient condition to obtain an\nestimate with low variance. \n\n"}
{"id": "1011.1970", "contents": "Title: Using Model-based Overlapping Seed Expansion to detect highly\n  overlapping community structure Abstract: As research into community finding in social networks progresses, there is a\nneed for algorithms capable of detecting overlapping community structure. Many\nalgorithms have been proposed in recent years that are capable of assigning\neach node to more than a single community. The performance of these algorithms\ntends to degrade when the ground-truth contains a more highly overlapping\ncommunity structure, with nodes assigned to more than two communities. Such\nhighly overlapping structure is likely to exist in many social networks, such\nas Facebook friendship networks. In this paper we present a scalable algorithm,\nMOSES, based on a statistical model of community structure, which is capable of\ndetecting highly overlapping community structure, especially when there is\nvariance in the number of communities each node is in. In evaluation on\nsynthetic data MOSES is found to be superior to existing algorithms, especially\nat high levels of overlap. We demonstrate MOSES on real social network data by\nanalyzing the networks of friendship links between students of five US\nuniversities. \n\n"}
{"id": "1011.2945", "contents": "Title: Phase transitions for the cavity approach to the clique problem on\n  random graphs Abstract: We give a rigorous proof of two phase transitions for a disordered system\ndesigned to find large cliques inside Erdos random graphs. Such a system is\nassociated with a conservative probabilistic cellular automaton inspired by the\ncavity method originally introduced in spin glass theory. \n\n"}
{"id": "1012.0009", "contents": "Title: Time-Varying Graphs and Dynamic Networks Abstract: The past few years have seen intensive research efforts carried out in some\napparently unrelated areas of dynamic systems -- delay-tolerant networks,\nopportunistic-mobility networks, social networks -- obtaining closely related\ninsights. Indeed, the concepts discovered in these investigations can be viewed\nas parts of the same conceptual universe; and the formal models proposed so far\nto express some specific concepts are components of a larger formal description\nof this universe. The main contribution of this paper is to integrate the vast\ncollection of concepts, formalisms, and results found in the literature into a\nunified framework, which we call TVG (for time-varying graphs). Using this\nframework, it is possible to express directly in the same formalism not only\nthe concepts common to all those different areas, but also those specific to\neach. Based on this definitional work, employing both existing results and\noriginal observations, we present a hierarchical classification of TVGs; each\nclass corresponds to a significant property examined in the distributed\ncomputing literature. We then examine how TVGs can be used to study the\nevolution of network properties, and propose different techniques, depending on\nwhether the indicators for these properties are a-temporal (as in the majority\nof existing studies) or temporal. Finally, we briefly discuss the introduction\nof randomness in TVGs. \n\n"}
{"id": "1012.0975", "contents": "Title: Split Bregman Method for Sparse Inverse Covariance Estimation with\n  Matrix Iteration Acceleration Abstract: We consider the problem of estimating the inverse covariance matrix by\nmaximizing the likelihood function with a penalty added to encourage the\nsparsity of the resulting matrix. We propose a new approach based on the split\nBregman method to solve the regularized maximum likelihood estimation problem.\nWe show that our method is significantly faster than the widely used graphical\nlasso method, which is based on blockwise coordinate descent, on both\nartificial and real-world data. More importantly, different from the graphical\nlasso, the split Bregman based method is much more general, and can be applied\nto a class of regularization terms other than the $\\ell_1$ norm \n\n"}
{"id": "1012.1269", "contents": "Title: Identification of overlapping communities and their hierarchy by locally\n  calculating community-changing resolution levels Abstract: We propose a new local, deterministic and parameter-free algorithm that\ndetects fuzzy and crisp overlapping communities in a weighted network and\nsimultaneously reveals their hierarchy. Using a local fitness function, the\nalgorithm greedily expands natural communities of seeds until the whole graph\nis covered. The hierarchy of communities is obtained analytically by\ncalculating resolution levels at which communities grow rather than numerically\nby testing different resolution levels. This analytic procedure is not only\nmore exact than its numerical alternatives such as LFM and GCE but also much\nfaster. Critical resolution levels can be identified by searching for intervals\nin which large changes of the resolution do not lead to growth of communities.\nWe tested our algorithm on benchmark graphs and on a network of 492 papers in\ninformation science. Combined with a specific post-processing, the algorithm\ngives much more precise results on LFR benchmarks with high overlap compared to\nother algorithms and performs very similar to GCE. \n\n"}
{"id": "1012.1272", "contents": "Title: A statistical mechanics approach to Granovetter theory Abstract: In this paper we try to bridge breakthroughs in quantitative\nsociology/econometrics pioneered during the last decades by Mac Fadden,\nBrock-Durlauf, Granovetter and Watts-Strogats through introducing a minimal\nmodel able to reproduce essentially all the features of social behavior\nhighlighted by these authors. Our model relies on a pairwise Hamiltonian for\ndecision maker interactions which naturally extends the multi-populations\napproaches by shifting and biasing the pattern definitions of an Hopfield model\nof neural networks. Once introduced, the model is investigated trough graph\ntheory (to recover Granovetter and Watts-Strogats results) and statistical\nmechanics (to recover Mac-Fadden and Brock-Durlauf results). Due to internal\nsymmetries of our model, the latter is obtained as the relaxation of a proper\nMarkov process, allowing even to study its out of equilibrium properties. The\nmethod used to solve its equilibrium is an adaptation of the Hamilton-Jacobi\ntechnique recently introduced by Guerra in the spin glass scenario and the\npicture obtained is the following: just by assuming that the larger the amount\nof similarities among decision makers, the stronger their relative influence,\nthis is enough to explain both the different role of strong and weak ties in\nthe social network as well as its small world properties. As a result,\nimitative interaction strengths seem essentially a robust request (enough to\nbreak the gauge symmetry in the couplings), furthermore, this naturally leads\nto a discrete choice modelization when dealing with the external influences and\nto imitative behavior a la Curie-Weiss as the one introduced by Brock and\nDurlauf. \n\n"}
{"id": "1012.4404", "contents": "Title: Multicolored Dynamos on Toroidal Meshes Abstract: Detecting on a graph the presence of the minimum number of nodes (target set)\nthat will be able to \"activate\" a prescribed number of vertices in the graph is\ncalled the target set selection problem (TSS) proposed by Kempe, Kleinberg, and\nTardos. In TSS's settings, nodes have two possible states (active or\nnon-active) and the threshold triggering the activation of a node is given by\nthe number of its active neighbors. Dealing with fault tolerance in a majority\nbased system the two possible states are used to denote faulty or non-faulty\nnodes, and the threshold is given by the state of the majority of neighbors.\nHere, the major effort was in determining the distribution of initial faults\nleading the entire system to a faulty behavior. Such an activation pattern,\nalso known as dynamic monopoly (or shortly dynamo), was introduced by Peleg in\n1996. In this paper we extend the TSS problem's settings by representing nodes'\nstates with a \"multicolored\" set. The extended version of the problem can be\ndescribed as follows: let G be a simple connected graph where every node is\nassigned a color from a finite ordered set C = {1, . . ., k} of colors. At each\nlocal time step, each node can recolor itself, depending on the local\nconfigurations, with the color held by the majority of its neighbors. Given G,\nwe study the initial distributions of colors leading the system to a k\nmonochromatic configuration in toroidal meshes, focusing on the minimum number\nof initial k-colored nodes. We find upper and lower bounds to the size of a\ndynamo, and then special classes of dynamos, outlined by means of a new\napproach based on recoloring patterns, are characterized. \n\n"}
{"id": "1012.5913", "contents": "Title: All liaisons are dangerous when all your friends are known to us Abstract: Online Social Networks (OSNs) are used by millions of users worldwide.\nAcademically speaking, there is little doubt about the usefulness of\ndemographic studies conducted on OSNs and, hence, methods to label unknown\nusers from small labeled samples are very useful. However, from the general\npublic point of view, this can be a serious privacy concern. Thus, both topics\nare tackled in this paper: First, a new algorithm to perform user profiling in\nsocial networks is described, and its performance is reported and discussed.\nSecondly, the experiments --conducted on information usually considered\nsensitive-- reveal that by just publicizing one's contacts privacy is at risk\nand, thus, measures to minimize privacy leaks due to social graph data mining\nare outlined. \n\n"}
{"id": "1101.2135", "contents": "Title: Bounded confidence model: addressed information maintain diversity of\n  opinions Abstract: A community of agents is subject to a stream of messages, which are\nrepresented as points on a plane of issues. Messages are sent by media and by\nagents themselves. Messages from media shape the public opinion. They are\nunbiased, i.e. positive and negative opinions on a given issue appear with\nequal frequencies. In our previous work, the only criterion to receive a\nmessage by an agent is if the distance between this message and the ones\nreceived earlier does not exceed the given value of the tolerance parameter.\nHere we introduce a possibility to address a message to a given neighbour. We\nshow that this option reduces the unanimity effect, what improves the\ncollective performance. \n\n"}
{"id": "1101.3122", "contents": "Title: Digital herders and phase transition in a voting model Abstract: In this paper, we discuss a voting model with two candidates, C_1 and C_2. We\nset two types of voters--herders and independents. The voting of independent\nvoters is based on their fundamental values; on the other hand, the voting of\nherders is based on the number of votes. Herders always select the majority of\nthe previous $r$ votes, which is visible to them. We call them digital herders.\nWe can accurately calculate the distribution of votes for special cases. When\nr>=3, we find that a phase transition occurs at the upper limit of t, where t\nis the discrete time (or number of votes). As the fraction of herders\nincreases, the model features a phase transition beyond which a state where\nmost voters make the correct choice coexists with one where most of them are\nwrong. On the other hand, when r<3, there is no phase transition. In this case,\nthe herders' performance is the same as that of the independent voters.\nFinally, we recognize the behavior of human beings by conducting simple\nexperiments. \n\n"}
{"id": "1101.3761", "contents": "Title: Tagging with DHARMA, a DHT-based Approach for Resource Mapping through\n  Approximation Abstract: We introduce collaborative tagging and faceted search on structured P2P\nsystems. Since a trivial and brute force mapping of an entire folksonomy over a\nDHT-based system may reduce scalability, we propose an approximated graph\nmaintenance approach. Evaluations on real data coming from Last.fm prove that\nsuch strategies reduce vocabulary noise (i.e., representation's overfitting\nphenomena) and hotspots issues. \n\n"}
{"id": "1101.5308", "contents": "Title: Parsimonious Flooding in Geometric Random-Walks Abstract: We study the information spreading yielded by the \\emph{(Parsimonious)\n$1$-Flooding Protocol} in geometric Mobile Ad-Hoc Networks. We consider $n$\nagents on a convex plane region of diameter $D$ performing independent random\nwalks with move radius $\\rho$. At any time step, every active agent $v$ informs\nevery non-informed agent which is within distance $R$ from $v$ ($R>0$ is the\ntransmission radius). An agent is only active at the time step immediately\nafter the one in which has been informed and, after that, she is removed. At\nthe initial time step, a source agent is informed and we look at the\n\\emph{completion time} of the protocol, i.e., the first time step (if any) in\nwhich all agents are informed. This random process is equivalent to the\nwell-known \\emph{Susceptible-Infective-Removed ($SIR$}) infection process in\nMathematical Epidemiology. No analytical results are available for this random\nprocess over any explicit mobility model. The presence of removed agents makes\nthis process much more complex than the (standard) flooding. We prove optimal\nbounds on the completion time depending on the parameters $n$, $D$, $R$, and\n$\\rho$. The obtained bounds hold with high probability. We remark that our\nmethod of analysis provides a clear picture of the dynamic shape of the\ninformation spreading (or infection wave) over the time. \n\n"}
{"id": "1102.0257", "contents": "Title: Emergence through Selection: The Evolution of a Scientific Challenge Abstract: One of the most interesting scientific challenges nowadays deals with the\nanalysis and the understanding of complex networks' dynamics and how their\nprocesses lead to emergence according to the interactions among their\ncomponents. In this paper we approach the definition of new methodologies for\nthe visualization and the exploration of the dynamics at play in real dynamic\nsocial networks. We present a recently introduced formalism called TVG (for\ntime-varying graphs), which was initially developed to model and analyze\nhighly-dynamic and infrastructure-less communication networks such as mobile\nad-hoc networks, wireless sensor networks, or vehicular networks. We discuss\nits applicability to complex networks in general, and social networks in\nparticular, by showing how it enables the specification and analysis of complex\ndynamic phenomena in terms of temporal interactions, and allows to easily\nswitch the perspective between local and global dynamics. As an example, we\nchose the case of scientific communities by analyzing portion of the ArXiv\nrepository (ten years of publications in physics) focusing on the social\ndeterminants (e.g. goals and potential interactions among individuals) behind\nthe emergence and the resilience of scientific communities. We consider that\nscientific communities are at the same time communities of practice (through\nco-authorship) and that they exist also as representations in the scientists'\nmind, since references to other scientists' works is not merely an objective\nlink to a relevant work, but it reveals social objects that one manipulates,\nselect and refers to. In the paper we show the emergence/selection of a\ncommunity as a goal-driven preferential attachment toward a set of authors\namong which there are some key scientists (Nobel prizes). \n\n"}
{"id": "1102.1398", "contents": "Title: Efficient Bayesian Social Learning on Trees Abstract: We consider a set of agents who are attempting to iteratively learn the\n'state of the world' from their neighbors in a social network. Each agent\ninitially receives a noisy observation of the true state of the world. The\nagents then repeatedly 'vote' and observe the votes of some of their peers,\nfrom which they gain more information. The agents' calculations are Bayesian\nand aim to myopically maximize the expected utility at each iteration.\n  This model, introduced by Gale and Kariv (2003), is a natural approach to\nlearning on networks. However, it has been criticized, chiefly because the\nagents' decision rule appears to become computationally intractable as the\nnumber of iterations advances. For instance, a dynamic programming approach\n(part of this work) has running time that is exponentially large in \\min(n,\n(d-1)^t), where n is the number of agents.\n  We provide a new algorithm to perform the agents' computations on locally\ntree-like graphs. Our algorithm uses the dynamic cavity method to drastically\nreduce computational effort. Let d be the maximum degree and t be the iteration\nnumber. The computational effort needed per agent is exponential only in O(td)\n(note that the number of possible information sets of a neighbor at time t is\nitself exponential in td).\n  Under appropriate assumptions on the rate of convergence, we deduce that each\nagent is only required to spend polylogarithmic (in 1/\\eps) computational\neffort to approximately learn the true state of the world with error\nprobability \\eps, on regular trees of degree at least five. We provide\nnumerical and other evidence to justify our assumption on convergence rate.\n  We extend our results in various directions, including loopy graphs. Our\nresults indicate efficiency of iterative Bayesian social learning in a wide\nrange of situations, contrary to widely held beliefs. \n\n"}
{"id": "1103.0701", "contents": "Title: Analytical maximum-likelihood method to detect patterns in real networks Abstract: In order to detect patterns in real networks, randomized graph ensembles that\npreserve only part of the topology of an observed network are systematically\nused as fundamental null models. However, their generation is still\nproblematic. The existing approaches are either computationally demanding and\nbeyond analytic control, or analytically accessible but highly approximate.\nHere we propose a solution to this long-standing problem by introducing an\nexact and fast method that allows to obtain expectation values and standard\ndeviations of any topological property analytically, for any binary, weighted,\ndirected or undirected network. Remarkably, the time required to obtain the\nexpectation value of any property is as short as that required to compute the\nsame property on the single original network. Our method reveals that the null\nbehavior of various correlation properties is different from what previously\nbelieved, and highly sensitive to the particular network considered. Moreover,\nour approach shows that important structural properties (such as the modularity\nused in community detection problems) are currently based on incorrect\nexpressions, and provides the exact quantities that should replace them. \n\n"}
{"id": "1103.5027", "contents": "Title: Google matrix of the world trade network Abstract: Using the United Nations Commodity Trade Statistics Database\n[http://comtrade.un.org/db/] we construct the Google matrix of the world trade\nnetwork and analyze its properties for various trade commodities for all\ncountries and all available years from 1962 to 2009. The trade flows on this\nnetwork are classified with the help of PageRank and CheiRank algorithms\ndeveloped for the World Wide Web and other large scale directed networks. For\nthe world trade this ranking treats all countries on equal democratic grounds\nindependent of country richness. Still this method puts at the top a group of\nindustrially developed countries for trade in {\\it all commodities}. Our study\nestablishes the existence of two solid state like domains of rich and poor\ncountries which remain stable in time, while the majority of countries are\nshown to be in a gas like phase with strong rank fluctuations. A simple random\nmatrix model provides a good description of statistical distribution of\ncountries in two-dimensional rank plane. The comparison with usual ranking by\nexport and import highlights new features and possibilities of our approach. \n\n"}
{"id": "1104.2944", "contents": "Title: Global Computation in a Poorly Connected World: Fast Rumor Spreading\n  with No Dependence on Conductance Abstract: In this paper, we study the question of how efficiently a collection of\ninterconnected nodes can perform a global computation in the widely studied\nGOSSIP model of communication. In this model, nodes do not know the global\ntopology of the network, and they may only initiate contact with a single\nneighbor in each round. This model contrasts with the much less restrictive\nLOCAL model, where a node may simultaneously communicate with all of its\nneighbors in a single round. A basic question in this setting is how many\nrounds of communication are required for the information dissemination problem,\nin which each node has some piece of information and is required to collect all\nothers. In this paper, we give an algorithm that solves the information\ndissemination problem in at most $O(D+\\text{polylog}{(n)})$ rounds in a network\nof diameter $D$, withno dependence on the conductance. This is at most an\nadditive polylogarithmic factor from the trivial lower bound of $D$, which\napplies even in the LOCAL model. In fact, we prove that something stronger is\ntrue: any algorithm that requires $T$ rounds in the LOCAL model can be\nsimulated in $O(T +\\mathrm{polylog}(n))$ rounds in the GOSSIP model. We thus\nprove that these two models of distributed computation are essentially\nequivalent. \n\n"}
{"id": "1104.3083", "contents": "Title: Narrow scope for resolution-limit-free community detection Abstract: Detecting communities in large networks has drawn much attention over the\nyears. While modularity remains one of the more popular methods of community\ndetection, the so-called resolution limit remains a significant drawback. To\novercome this issue, it was recently suggested that instead of comparing the\nnetwork to a random null model, as is done in modularity, it should be compared\nto a constant factor. However, it is unclear what is meant exactly by\n\"resolution-limit-free\", that is, not suffering from the resolution limit.\nFurthermore, the question remains what other methods could be classified as\nresolution-limit-free. In this paper we suggest a rigorous definition and\nderive some basic properties of resolution-limit-free methods. More\nimportantly, we are able to prove exactly which class of community detection\nmethods are resolution-limit-free. Furthermore, we analyze which methods are\nnot resolution-limit-free, suggesting there is only a limited scope for\nresolution-limit-free community detection methods. Finally, we provide such a\nnatural formulation, and show it performs superbly. \n\n"}
{"id": "1105.0812", "contents": "Title: Compression of Flow Can Reveal Overlapping-Module Organization in\n  Networks Abstract: To better understand the overlapping modular organization of large networks\nwith respect to flow, here we introduce the map equation for overlapping\nmodules. In this information-theoretic framework, we use the correspondence\nbetween compression and regularity detection. The generalized map equation\nmeasures how well we can compress a description of flow in the network when we\npartition it into modules with possible overlaps. When we minimize the\ngeneralized map equation over overlapping network partitions, we detect modules\nthat capture flow and determine which nodes at the boundaries between modules\nshould be classified in multiple modules and to what degree. With a novel\ngreedy search algorithm, we find that some networks, for example, the neural\nnetwork of C. Elegans, are best described by modules dominated by hard\nboundaries, but that others, for example, the sparse European road network,\nhave a highly overlapping modular organization. \n\n"}
{"id": "1107.0989", "contents": "Title: Geometry of Complex Networks and Topological Centrality Abstract: We explore the geometry of complex networks in terms of an n-dimensional\nEuclidean embedding represented by the Moore-Penrose pseudo-inverse of the\ngraph Laplacian $(\\bb L^+)$. The squared distance of a node $i$ to the origin\nin this n-dimensional space $(l^+_{ii})$, yields a topological centrality index\n$(\\mathcal{C}^{*}(i) = 1/l^+_{ii})$ for node $i$. In turn, the sum of\nreciprocals of individual node structural centralities,\n$\\sum_{i}1/\\mathcal{C}^*(i) = \\sum_{i} l^+_{ii}$, i.e. the trace of $\\bb L^+$,\nyields the well-known Kirchhoff index $(\\mathcal{K})$, an overall structural\ndescriptor for the network. In addition to this geometric interpretation, we\nprovide alternative interpretations of the proposed indices to reveal their\ntrue topological characteristics: first, in terms of forced detour overheads\nand frequency of recurrences in random walks that has an interesting analogy to\nvoltage distributions in the equivalent electrical network; and then as the\naverage connectedness of $i$ in all the bi-partitions of the graph. These\ninterpretations respectively help establish the topological centrality\n$(\\mathcal{C}^{*}(i))$ of node $i$ as a measure of its overall position as well\nas its overall connectedness in the network; thus reflecting the robustness of\nnode $i$ to random multiple edge failures. Through empirical evaluations using\nsynthetic and real world networks, we demonstrate how the topological\ncentrality is better able to distinguish nodes in terms of their structural\nroles in the network and, along with Kirchhoff index, is appropriately\nsensitive to perturbations/rewirings in the network. \n\n"}
{"id": "1107.4127", "contents": "Title: Spectra of sparse regular graphs with loops Abstract: We derive exact equations that determine the spectra of undirected and\ndirected sparsely connected regular graphs containing loops of arbitrary\nlength. The implications of our results to the structural and dynamical\nproperties of networks are discussed by showing how loops influence the size of\nthe spectral gap and the propensity for synchronization. Analytical formulas\nfor the spectrum are obtained for specific length of the loops. \n\n"}
{"id": "1107.5851", "contents": "Title: Co-evolution of Content Popularity and Delivery in Mobile P2P Networks Abstract: Mobile P2P technology provides a scalable approach to content delivery to a\nlarge number of users on their mobile devices. In this work, we study the\ndissemination of a \\emph{single} content (e.g., an item of news, a song or a\nvideo clip) among a population of mobile nodes. Each node in the population is\neither a \\emph{destination} (interested in the content) or a potential\n\\emph{relay} (not yet interested in the content). There is an interest\nevolution process by which nodes not yet interested in the content (i.e.,\nrelays) can become interested (i.e., become destinations) on learning about the\npopularity of the content (i.e., the number of already interested nodes). In\nour work, the interest in the content evolves under the \\emph{linear threshold\nmodel}. The content is copied between nodes when they make random contact. For\nthis we employ a controlled epidemic spread model. We model the joint evolution\nof the copying process and the interest evolution process, and derive the joint\nfluid limit ordinary differential equations. We then study the selection of the\nparameters under the content provider's control, for the optimization of\nvarious objective functions that aim at maximizing content popularity and\nefficient content delivery. \n\n"}
{"id": "1108.3130", "contents": "Title: Localizations on Complex Networks Abstract: We study the structural characteristics of complex networks using the\nrepresentative eigenvectors of the adjacent matrix. The probability\ndistribution function of the components of the representative eigenvectors are\nproposed to describe the localization on networks where the Euclidean distance\nis invalid. Several quantities are used to describe the localization properties\nof the representative states, such as the participation ratio, the structural\nentropy, and the probability distribution function of the nearest neighbor\nlevel spacings for spectra of complex networks. Whole-cell networks in the real\nworld and the Watts-Strogatz small-world and Barabasi-Albert scale-free\nnetworks are considered. The networks have nontrivial localization properties\ndue to the nontrivial topological structures. It is found that the\nascending-order-ranked series of the occurrence probabilities at the nodes\nbehave generally multifractally. This characteristic can be used as a\nstructural measure of complex networks. \n\n"}
{"id": "1109.4631", "contents": "Title: Random Sequential Renormalization and Agglomerative Percolation in\n  Networks: Application to Erd\"os-R'enyi and Scale-free Graphs Abstract: We study the statistical behavior under random sequential\nrenormalization(RSR) of several network models including Erd\"os R'enyi (ER)\ngraphs, scale-free networks and an annealed model (AM) related to ER graphs. In\nRSR the network is locally coarse grained by choosing at each renormalization\nstep a node at random and joining it to all its neighbors. Compared to previous\n(quasi-)parallel renormalization methods [C.Song et.al], RSR allows a more\nfine-grained analysis of the renormalization group (RG) flow, and unravels new\nfeatures, that were not discussed in the previous analyses. In particular we\nfind that all networks exhibit a second order transition in their RG flow. This\nphase transition is associated with the emergence of a giant hub and can be\nviewed as a new variant of percolation, called agglomerative percolation. We\nclaim that this transition exists also in previous graph renormalization\nschemes and explains some of the scaling laws seen there. For critical trees it\nhappens as N/N0 -> 0 in the limit of large systems (where N0 is the initial\nsize of the graph and N its size at a given RSR step). In contrast, it happens\nat finite N/N0 in sparse ER graphs and in the annealed model, while it happens\nfor N/N0 -> 1 on scale-free networks. Critical exponents seem to depend on the\ntype of the graph but not on the average degree and obey usual scaling\nrelations for percolation phenomena. For the annealed model they agree with the\nexponents obtained from a mean-field theory. At late times, the networks\nexhibit a star-like structure in agreement with the results of Radicchi et. al.\nWhile degree distributions are of main interest when regarding the scheme as\nnetwork renormalization, mass distributions (which are more relevant when\nconsidering 'supernodes' as clusters) are much easier to study using the fast\nNewman-Ziff algorithm for percolation, allowing us to obtain very high\nstatistics. \n\n"}
{"id": "1110.5813", "contents": "Title: Overlapping Community Detection in Networks: the State of the Art and\n  Comparative Study Abstract: This paper reviews the state of the art in overlapping community detection\nalgorithms, quality measures, and benchmarks. A thorough comparison of\ndifferent algorithms (a total of fourteen) is provided. In addition to\ncommunity level evaluation, we propose a framework for evaluating algorithms'\nability to detect overlapping nodes, which helps to assess over-detection and\nunder-detection. After considering community level detection performance\nmeasured by Normalized Mutual Information, the Omega index, and node level\ndetection performance measured by F-score, we reached the following\nconclusions. For low overlapping density networks, SLPA, OSLOM, Game and COPRA\noffer better performance than the other tested algorithms. For networks with\nhigh overlapping density and high overlapping diversity, both SLPA and Game\nprovide relatively stable performance. However, test results also suggest that\nthe detection in such networks is still not yet fully resolved. A common\nfeature observed by various algorithms in real-world networks is the relatively\nsmall fraction of overlapping nodes (typically less than 30%), each of which\nbelongs to only 2 or 3 communities. \n\n"}
{"id": "1111.4852", "contents": "Title: Biased diffusion on Japanese inter-firm trading network: Estimation of\n  sales from network structure Abstract: To investigate the actual phenomena of transport on a complex network, we\nanalysed empirical data for an inter-firm trading network, which consists of\nabout one million Japanese firms and the sales of these firms (a sale\ncorresponds to the total in-flow into a node). First, we analysed the\nrelationships between sales and sales of nearest neighbourhoods from which we\nobtain a simple linear relationship between sales and the weighted sum of sales\nof nearest neighbourhoods (i.e., customers). In addition, we introduce a simple\nmoney transport model that is coherent with this empirical observation. In this\nmodel, a firm (i.e., customer) distributes money to its out-edges (suppliers)\nproportionally to the in-degree of destinations. From intensive numerical\nsimulations, we find that the steady flows derived from these models can\napproximately reproduce the distribution of sales of actual firms. The sales of\nindividual firms deduced from the money-transport model are shown to be\nproportional, on an average, to the real sales. \n\n"}
{"id": "1112.1313", "contents": "Title: The Target Set Selection Problem on Cycle Permutation Graphs,\n  Generalized Petersen Graphs and Torus Cordalis Abstract: In this paper we consider a fundamental problem in the area of viral\nmarketing, called T{\\scriptsize ARGET} S{\\scriptsize ET} S{\\scriptsize\nELECTION} problem.\n  In a a viral marketing setting, social networks are modeled by graphs with\npotential customers of a new product as vertices and friend relationships as\nedges, where each vertex $v$ is assigned a threshold value $\\theta(v)$. The\nthresholds represent the different latent tendencies of customers (vertices) to\nbuy the new product when their friend (neighbors) do.\n  Consider a repetitive process on social network $(G,\\theta)$ where each\nvertex $v$ is associated with two states, active and inactive, which indicate\nwhether $v$ is persuaded into buying the new product. Suppose we are given a\ntarget set $S\\subseteq V(G)$. Initially, all vertices in $G$ are inactive. At\ntime step 0, we choose all vertices in $S$ to become active.\n  Then, at every time step $t>0$, all vertices that were active in time step\n$t-1$ remain active, and we activate any vertex $v$ if at least $\\theta(v)$ of\nits neighbors were active at time step $t-1$. The activation process terminates\nwhen no more vertices can get activated. We are interested in the following\noptimization problem, called T{\\scriptsize ARGET} S{\\scriptsize ET}\nS{\\scriptsize ELECTION}: Finding a target set $S$ of smallest possible size\nthat activates all vertices of $G$. There is an important and well-studied\nthreshold called strict majority threshold, where for every vertex $v$ in $G$\nwe have $\\theta(v)=\\lceil{(d(v) +1)/2}\\rceil$ and $d(v)$ is the degree of $v$\nin $G$. In this paper, we consider the T{\\scriptsize ARGET} S{\\scriptsize ET}\nS{\\scriptsize ELECTION} problem under strict majority thresholds and focus on\nthree popular regular network structures: cycle permutation graphs, generalized\nPetersen graphs and torus cordalis. \n\n"}
{"id": "1112.2187", "contents": "Title: Chinese Restaurant Game - Part II: Applications to Wireless Networking,\n  Cloud Computing, and Online Social Networking Abstract: In Part I of this two-part paper [1], we proposed a new game, called Chinese\nrestaurant game, to analyze the social learning problem with negative network\nexternality. The best responses of agents in the Chinese restaurant game with\nimperfect signals are constructed through a recursive method, and the influence\nof both learning and network externality on the utilities of agents is studied.\nIn Part II of this two-part paper, we illustrate three applications of Chinese\nrestaurant game in wireless networking, cloud computing, and online social\nnetworking. For each application, we formulate the corresponding problem as a\nChinese restaurant game and analyze how agents learn and make strategic\ndecisions in the problem. The proposed method is compared with four\ncommon-sense methods in terms of agents' utilities and the overall system\nperformance through simulations. We find that the proposed Chinese restaurant\ngame theoretic approach indeed helps agents make better decisions and improves\nthe overall system performance. Furthermore, agents with different decision\norders have different advantages in terms of their utilities, which also\nverifies the conclusions drawn in Part I of this two-part paper. \n\n"}
{"id": "1112.3644", "contents": "Title: Community structure and scale-free collections of Erd\\\"os-R\\'enyi graphs Abstract: Community structure plays a significant role in the analysis of social\nnetworks and similar graphs, yet this structure is little understood and not\nwell captured by most models. We formally define a community to be a subgraph\nthat is internally highly connected and has no deeper substructure. We use\ntools of combinatorics to show that any such community must contain a dense\nErd\\\"os-R\\'enyi (ER) subgraph. Based on mathematical arguments, we hypothesize\nthat any graph with a heavy-tailed degree distribution and community structure\nmust contain a scale free collection of dense ER subgraphs. These theoretical\nobservations corroborate well with empirical evidence. From this, we propose\nthe Block Two-Level Erd\\\"os-R\\'enyi (BTER) model, and demonstrate that it\naccurately captures the observable properties of many real-world social\nnetworks. \n\n"}
{"id": "1112.4708", "contents": "Title: Transformation Networks: How Innovation and the Availability of\n  Technology can Increase Economic Performance Abstract: A transformation network describes how one set of resources can be\ntransformed into another via technological processes. Transformation networks\nin economics are useful because they can highlight areas for future\ninnovations, both in terms of new products, new production techniques, or\nbetter efficiency. They also make it easy to detect areas where an economy\nmight be fragile. In this paper, we use computational simulations to\ninvestigate how the density of a transformation network affects the economic\nperformance, as measured by the gross domestic product (GDP), of an artificial\neconomy. Our results show that on average, the GDP of our economy increases as\nthe density of the transformation network increases. We also find that while\nthe average performance increases, the maximum possible performance decreases\nand the minimum possible performance increases. \n\n"}
{"id": "1112.5762", "contents": "Title: Characterizing Continuous Time Random Walks on Time Varying Graphs Abstract: In this paper we study the behavior of a continuous time random walk (CTRW)\non a stationary and ergodic time varying dynamic graph. We establish conditions\nunder which the CTRW is a stationary and ergodic process. In general, the\nstationary distribution of the walker depends on the walker rate and is\ndifficult to characterize. However, we characterize the stationary distribution\nin the following cases: i) the walker rate is significantly larger or smaller\nthan the rate in which the graph changes (time-scale separation), ii) the\nwalker rate is proportional to the degree of the node that it resides on\n(coupled dynamics), and iii) the degrees of node belonging to the same\nconnected component are identical (structural constraints). We provide examples\nthat illustrate our theoretical findings. \n\n"}
{"id": "1201.3783", "contents": "Title: Network Analysis of Recurring YouTube Spam Campaigns Abstract: As the popularity of content sharing websites such as YouTube and Flickr has\nincreased, they have become targets for spam, phishing and the distribution of\nmalware. On YouTube, the facility for users to post comments can be used by\nspam campaigns to direct unsuspecting users to bogus e-commerce websites. In\nthis paper, we demonstrate how such campaigns can be tracked over time using\nnetwork motif profiling, i.e. by tracking counts of indicative network motifs.\nBy considering all motifs of up to five nodes, we identify discriminating\nmotifs that reveal two distinctly different spam campaign strategies. One of\nthese strategies uses a small number of spam user accounts to comment on a\nlarge number of videos, whereas a larger number of accounts is used with the\nother. We present an evaluation that uses motif profiling to track two active\ncampaigns matching these strategies, and identify some of the associated user\naccounts. \n\n"}
{"id": "1201.4369", "contents": "Title: Exact solution of bond percolation on small arbitrary graphs Abstract: We introduce a set of iterative equations that exactly solves the size\ndistribution of components on small arbitrary graphs after the random removal\nof edges. We also demonstrate how these equations can be used to predict the\ndistribution of the node partitions (i.e., the constrained distribution of the\nsize of each component) in undirected graphs. Besides opening the way to the\ntheoretical prediction of percolation on arbitrary graphs of large but finite\nsize, we show how our results find application in graph theory, epidemiology,\npercolation and fragmentation theory. \n\n"}
{"id": "1202.1112", "contents": "Title: Recommender Systems Abstract: The ongoing rapid expansion of the Internet greatly increases the necessity\nof effective recommender systems for filtering the abundant information.\nExtensive research for recommender systems is conducted by a broad range of\ncommunities including social and computer scientists, physicists, and\ninterdisciplinary researchers. Despite substantial theoretical and practical\nachievements, unification and comparison of different approaches are lacking,\nwhich impedes further advances. In this article, we review recent developments\nin recommender systems and discuss the major challenges. We compare and\nevaluate available algorithms and examine their roles in the future\ndevelopments. In addition to algorithms, physical aspects are described to\nillustrate macroscopic behavior of recommender systems. Potential impacts and\nfuture directions are discussed. We emphasize that recommendation has a great\nscientific depth and combines diverse research fields which makes it of\ninterests for physicists as well as interdisciplinary researchers. \n\n"}
{"id": "1202.2293", "contents": "Title: Remarks on Category-Based Routing in Social Networks Abstract: It is well known that individuals can route messages on short paths through\nsocial networks, given only simple information about the target and using only\nlocal knowledge about the topology. Sociologists conjecture that people find\nroutes greedily by passing the message to an acquaintance that has more in\ncommon with the target than themselves, e.g. if a dentist in Saarbr\\\"ucken\nwants to send a message to a specific lawyer in Munich, he may forward it to\nsomeone who is a lawyer and/or lives in Munich. Modelling this setting,\nEppstein et al. introduced the notion of category-based routing. The goal is to\nassign a set of categories to each node of a graph such that greedy routing is\npossible. By proving bounds on the number of categories a node has to be in we\ncan argue about the plausibility of the underlying sociological model. In this\npaper we substantially improve the upper bounds introduced by Eppstein et al.\nand prove new lower bounds. \n\n"}
{"id": "1202.3993", "contents": "Title: Internet Topology over Time Abstract: There are few studies that look closely at how the topology of the Internet\nevolves over time; most focus on snapshots taken at a particular point in time.\nIn this paper, we investigate the evolution of the topology of the Autonomous\nSystems graph of the Internet, examining how eight commonly-used topological\nmeasures change from January 2002 to January 2010. We find that the\ndistributions of most of the measures remain unchanged, except for average path\nlength and clustering coefficient. The average path length has slowly and\nsteadily increased since 2005 and the average clustering coefficient has\nsteadily declined. We hypothesize that these changes are due to changes in\npeering policies as the Internet evolves. We also investigate a surprising\nfeature, namely that the maximum degree has changed little, an aspect that\ncannot be captured without modeling link deletion. Our results suggest that\nevaluating models of the Internet graph by comparing steady-state generated\ntopologies to snapshots of the real data is reasonable for many measures.\nHowever, accurately matching time-variant properties is more difficult, as we\ndemonstrate by evaluating ten well-known models against the 2010 data. \n\n"}
{"id": "1202.5618", "contents": "Title: An equation-free approach to coarse-graining the dynamics of networks Abstract: We propose and illustrate an approach to coarse-graining the dynamics of\nevolving networks (networks whose connectivity changes dynamically). The\napproach is based on the equation-free framework: short bursts of detailed\nnetwork evolution simulations are coupled with lifting and restriction\noperators that translate between actual network realizations and their\n(appropriately chosen) coarse observables. This framework is used here to\naccelerate temporal simulations (through coarse projective integration), and to\nimplement coarsegrained fixed point algorithms (through matrix-free\nNewton-Krylov GMRES). The approach is illustrated through a simple network\nevolution example, for which analytical approximations to the coarse-grained\ndynamics can be independently obtained, so as to validate the computational\nresults. The scope and applicability of the approach, as well as the issue of\nselection of good coarse observables are discussed. \n\n"}
{"id": "1202.6228", "contents": "Title: PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class\n  Classification Abstract: In this work, we propose a PAC-Bayes bound for the generalization risk of the\nGibbs classifier in the multi-class classification framework. The novelty of\nour work is the critical use of the confusion matrix of a classifier as an\nerror measure; this puts our contribution in the line of work aiming at dealing\nwith performance measure that are richer than mere scalar criterion such as the\nmisclassification rate. Thanks to very recent and beautiful results on matrix\nconcentration inequalities, we derive two bounds showing that the true\nconfusion risk of the Gibbs classifier is upper-bounded by its empirical risk\nplus a term depending on the number of training examples in each class. To the\nbest of our knowledge, this is the first PAC-Bayes bounds based on confusion\nmatrices. \n\n"}
{"id": "1202.6389", "contents": "Title: Consensus and Products of Random Stochastic Matrices: Exact Rate for\n  Convergence in Probability Abstract: Distributed consensus and other linear systems with system stochastic\nmatrices $W_k$ emerge in various settings, like opinion formation in social\nnetworks, rendezvous of robots, and distributed inference in sensor networks.\nThe matrices $W_k$ are often random, due to, e.g., random packet dropouts in\nwireless sensor networks. Key in analyzing the performance of such systems is\nstudying convergence of matrix products $W_kW_{k-1}... W_1$. In this paper, we\nfind the exact exponential rate $I$ for the convergence in probability of the\nproduct of such matrices when time $k$ grows large, under the assumption that\nthe $W_k$'s are symmetric and independent identically distributed in time.\nFurther, for commonly used random models like with gossip and link failure, we\nshow that the rate $I$ is found by solving a min-cut problem and, hence, easily\ncomputable. Finally, we apply our results to optimally allocate the sensors'\ntransmission power in consensus+innovations distributed detection. \n\n"}
{"id": "1203.0453", "contents": "Title: Change-Point Detection in Time-Series Data by Relative Density-Ratio\n  Estimation Abstract: The objective of change-point detection is to discover abrupt property\nchanges lying behind time-series data. In this paper, we present a novel\nstatistical change-point detection algorithm based on non-parametric divergence\nestimation between time-series samples from two retrospective segments. Our\nmethod uses the relative Pearson divergence as a divergence measure, and it is\naccurately and efficiently estimated by a method of direct density-ratio\nestimation. Through experiments on artificial and real-world datasets including\nhuman-activity sensing, speech, and Twitter messages, we demonstrate the\nusefulness of the proposed method. \n\n"}
{"id": "1203.1105", "contents": "Title: Pairwise interaction pattern in the weighted communication network Abstract: Although recent studies show that both topological structures and human\ndynamics can strongly affect information spreading on social networks, the\ncomplicated interplay of the two significant factors has not yet been clearly\ndescribed. In this work, we find a strong pairwise interaction based on\nanalyzing the weighted network generated by the short message communication\ndataset within a Chinese tele-communication provider. The pairwise interaction\nbridges the network topological structure and human interaction dynamics, which\ncan promote local information spreading between pairs of communication partners\nand in contrast can also suppress global information (e.g., rumor) cascade and\nspreading. In addition, the pairwise interaction is the basic pattern of group\nconversations and it can greatly reduce the waiting time of communication\nevents between a pair of intimate friends. Our findings are also helpful for\ncommunication operators to design novel tariff strategies and optimize their\ncommunication services. \n\n"}
{"id": "1203.1524", "contents": "Title: On the Influence of Informed Agents on Learning and Adaptation over\n  Networks Abstract: Adaptive networks consist of a collection of agents with adaptation and\nlearning abilities. The agents interact with each other on a local level and\ndiffuse information across the network through their collaborations. In this\nwork, we consider two types of agents: informed agents and uninformed agents.\nThe former receive new data regularly and perform consultation and in-network\ntasks, while the latter do not collect data and only participate in the\nconsultation tasks. We examine the performance of adaptive networks as a\nfunction of the proportion of informed agents and their distribution in space.\nThe results reveal some interesting and surprising trade-offs between\nconvergence rate and mean-square performance. In particular, among other\nresults, it is shown that the performance of adaptive networks does not\nnecessarily improve with a larger proportion of informed agents. Instead, it is\nestablished that the larger the proportion of informed agents is, the faster\nthe convergence rate of the network becomes albeit at the expense of some\ndeterioration in mean-square performance. The results further establish that\nuninformed agents play an important role in determining the steady-state\nperformance of the network, and that it is preferable to keep some of the\nhighly connected agents uninformed. The arguments reveal an important interplay\namong three factors: the number and distribution of informed agents in the\nnetwork, the convergence rate of the learning process, and the estimation\naccuracy in steady-state. Expressions that quantify these relations are\nderived, and simulations are included to support the theoretical findings. We\nfurther apply the results to two models that are widely used to represent\nbehavior over complex networks, namely, the Erdos-Renyi and scale-free models. \n\n"}
{"id": "1203.5351", "contents": "Title: Activity driven modeling of time varying networks Abstract: Network modeling plays a critical role in identifying statistical\nregularities and structural principles common to many systems. The large\nmajority of recent modeling approaches are connectivity driven. The structural\npatterns of the network are at the basis of the mechanisms ruling the network\nformation. Connectivity driven models necessarily provide a time-aggregated\nrepresentation that may fail to describe the instantaneous and fluctuating\ndynamics of many networks. We address this challenge by defining the activity\npotential, a time invariant function characterizing the agents' interactions\nand constructing an activity driven model capable of encoding the instantaneous\ntime description of the network dynamics. The model provides an explanation of\nstructural features such as the presence of hubs, which simply originate from\nthe heterogeneous activity of agents. Within this framework, highly dynamical\nnetworks can be described analytically, allowing a quantitative discussion of\nthe biases induced by the time-aggregated representations in the analysis of\ndynamical processes. \n\n"}
{"id": "1204.1437", "contents": "Title: Fast projections onto mixed-norm balls with applications Abstract: Joint sparsity offers powerful structural cues for feature selection,\nespecially for variables that are expected to demonstrate a \"grouped\" behavior.\nSuch behavior is commonly modeled via group-lasso, multitask lasso, and related\nmethods where feature selection is effected via mixed-norms. Several mixed-norm\nbased sparse models have received substantial attention, and for some cases\nefficient algorithms are also available. Surprisingly, several constrained\nsparse models seem to be lacking scalable algorithms. We address this\ndeficiency by presenting batch and online (stochastic-gradient) optimization\nmethods, both of which rely on efficient projections onto mixed-norm balls. We\nillustrate our methods by applying them to the multitask lasso. We conclude by\nmentioning some open problems. \n\n"}
{"id": "1204.2401", "contents": "Title: Controlling complex networks: How much energy is needed? Abstract: The outstanding problem of controlling complex networks is relevant to many\nareas of science and engineering, and has the potential to generate\ntechnological breakthroughs as well. We address the physically important issue\nof the energy required for achieving control by deriving and validating scaling\nlaws for the lower and upper energy bounds. These bounds represent a reasonable\nestimate of the energy cost associated with control, and provide a step forward\nfrom the current research on controllability toward ultimate control of complex\nnetworked dynamical systems. \n\n"}
{"id": "1204.3806", "contents": "Title: PageRank model of opinion formation on social networks Abstract: We propose the PageRank model of opinion formation and investigate its rich\nproperties on real directed networks of Universities of Cambridge and Oxford,\nLiveJournal and Twitter. In this model the opinion formation of linked electors\nis weighted with their PageRank probability. We find that the society elite,\ncorresponding to the top PageRank nodes, can impose its opinion to a\nsignificant fraction of the society. However, for a homogeneous distribution of\ntwo opinions there exists a bistability range of opinions which depends on a\nconformist parameter characterizing the opinion formation. We find that\nLiveJournal and Twitter networks have a stronger tendency to a totalitar\nopinion formation. We also analyze the Sznajd model generalized for scale-free\nnetworks with the weighted PageRank vote of electors. \n\n"}
{"id": "1204.4539", "contents": "Title: Supervised Feature Selection in Graphs with Path Coding Penalties and\n  Network Flows Abstract: We consider supervised learning problems where the features are embedded in a\ngraph, such as gene expressions in a gene network. In this context, it is of\nmuch interest to automatically select a subgraph with few connected components;\nby exploiting prior knowledge, one can indeed improve the prediction\nperformance or obtain results that are easier to interpret. Regularization or\npenalty functions for selecting features in graphs have recently been proposed,\nbut they raise new algorithmic challenges. For example, they typically require\nsolving a combinatorially hard selection problem among all connected subgraphs.\nIn this paper, we propose computationally feasible strategies to select a\nsparse and well-connected subset of features sitting on a directed acyclic\ngraph (DAG). We introduce structured sparsity penalties over paths on a DAG\ncalled \"path coding\" penalties. Unlike existing regularization functions that\nmodel long-range interactions between features in a graph, path coding\npenalties are tractable. The penalties and their proximal operators involve\npath selection problems, which we efficiently solve by leveraging network flow\noptimization. We experimentally show on synthetic, image, and genomic data that\nour approach is scalable and leads to more connected subgraphs than other\nregularization functions for graphs. \n\n"}
{"id": "1205.0038", "contents": "Title: Percolation Computation in Complex Networks Abstract: K-clique percolation is an overlapping community finding algorithm which\nextracts particular structures, comprised of overlapping cliques, from complex\nnetworks. While it is conceptually straightforward, and can be elegantly\nexpressed using clique graphs, certain aspects of k-clique percolation are\ncomputationally challenging in practice. In this paper we investigate aspects\nof empirical social networks, such as the large numbers of overlapping maximal\ncliques contained within them, that make clique percolation, and clique graph\nrepresentations, computationally expensive. We motivate a simple algorithm to\nconduct clique percolation, and investigate its performance compared to current\nbest-in-class algorithms. We present improvements to this algorithm, which\nallow us to perform k-clique percolation on much larger empirical datasets. Our\napproaches perform much better than existing algorithms on networks exhibiting\npervasively overlapping community structure, especially for higher values of k.\nHowever, clique percolation remains a hard computational problem; current\nalgorithms still scale worse than some other overlapping community finding\nalgorithms. \n\n"}
{"id": "1205.0079", "contents": "Title: Complexity Analysis of the Lasso Regularization Path Abstract: The regularization path of the Lasso can be shown to be piecewise linear,\nmaking it possible to \"follow\" and explicitly compute the entire path. We\nanalyze in this paper this popular strategy, and prove that its worst case\ncomplexity is exponential in the number of variables. We then oppose this\npessimistic result to an (optimistic) approximate analysis: We show that an\napproximate path with at most O(1/sqrt(epsilon)) linear segments can always be\nobtained, where every point on the path is guaranteed to be optimal up to a\nrelative epsilon-duality gap. We complete our theoretical analysis with a\npractical algorithm to compute these approximate paths. \n\n"}
{"id": "1205.1010", "contents": "Title: Partisan Asymmetries in Online Political Activity Abstract: We examine partisan differences in the behavior, communication patterns and\nsocial interactions of more than 18,000 politically-active Twitter users to\nproduce evidence that points to changing levels of partisan engagement with the\nAmerican online political landscape. Analysis of a network defined by the\ncommunication activity of these users in proximity to the 2010 midterm\ncongressional elections reveals a highly segregated, well clustered partisan\ncommunity structure. Using cluster membership as a high-fidelity (87% accuracy)\nproxy for political affiliation, we characterize a wide range of differences in\nthe behavior, communication and social connectivity of left- and right-leaning\nTwitter users. We find that in contrast to the online political dynamics of the\n2008 campaign, right-leaning Twitter users exhibit greater levels of political\nactivity, a more tightly interconnected social structure, and a communication\nnetwork topology that facilitates the rapid and broad dissemination of\npolitical information. \n\n"}
{"id": "1205.1682", "contents": "Title: Influence Maximization in Continuous Time Diffusion Networks Abstract: The problem of finding the optimal set of source nodes in a diffusion network\nthat maximizes the spread of information, influence, and diseases in a limited\namount of time depends dramatically on the underlying temporal dynamics of the\nnetwork. However, this still remains largely unexplored to date. To this end,\ngiven a network and its temporal dynamics, we first describe how continuous\ntime Markov chains allow us to analytically compute the average total number of\nnodes reached by a diffusion process starting in a set of source nodes. We then\nshow that selecting the set of most influential source nodes in the continuous\ntime influence maximization problem is NP-hard and develop an efficient\napproximation algorithm with provable near-optimal performance. Experiments on\nsynthetic and real diffusion networks show that our algorithm outperforms other\nstate of the art algorithms by at least ~20% and is robust across different\nnetwork topologies. \n\n"}
{"id": "1205.4467", "contents": "Title: Beating the news using Social Media: the case study of American Idol Abstract: We present a contribution to the debate on the predictability of social\nevents using big data analytics. We focus on the elimination of contestants in\nthe American Idol TV shows as an example of a well defined electoral phenomenon\nthat each week draws millions of votes in the USA. We provide evidence that\nTwitter activity during the time span defined by the TV show airing and the\nvoting period following it, correlates with the contestants ranking and allows\nthe anticipation of the voting outcome. Furthermore, the fraction of Tweets\nthat contain geolocation information allows us to map the fanbase of each\ncontestant, both within the US and abroad, showing that strong regional\npolarizations occur. Although American Idol voting is just a minimal and\nsimplified version of complex societal phenomena such as political elections,\nthis work shows that the volume of information available in online systems\npermits the real time gathering of quantitative indicators anticipating the\nfuture unfolding of opinion formation events. \n\n"}
{"id": "1206.2517", "contents": "Title: Assessing the Quality of Wikipedia Pages Using Edit Longevity and\n  Contributor Centrality Abstract: In this paper we address the challenge of assessing the quality of Wikipedia\npages using scores derived from edit contribution and contributor\nauthoritativeness measures. The hypothesis is that pages with significant\ncontributions from authoritative contributors are likely to be high-quality\npages. Contributions are quantified using edit longevity measures and\ncontributor authoritativeness is scored using centrality metrics in either the\nWikipedia talk or co-author networks. The results suggest that it is useful to\ntake into account the contributor authoritativeness when assessing the\ninformation quality of Wikipedia content. The percentile visualization of the\nquality scores provides some insights about the anomalous articles, and can be\nused to help Wikipedia editors to identify Start and Stub articles that are of\nrelatively good quality. \n\n"}
{"id": "1206.2728", "contents": "Title: Effect of Closed Paths in Complex networks on Six Degrees of Separation\n  and Disorder Abstract: Milgram Condition proposed by Aoyama et al. plays an important role on the\nanalysis of \"six degrees of separation\". We have shown that the relations\nbetween Milgram condition and the generalized clustering coefficient, which was\nintroduced as an index for measuring the number of closed paths by us, are\nabsolutely different in scale free networks (Barabasi and Albert) and small\nworld networks (Watts and Strogatz, Watts). This fact implies that the effect\nof closed paths on information propagation is different in both networks. In\nthis article, we first investigate the difference and pursuit what is a crucial\nmathematical quantity for information propagation. As a result we find that a\nsort of \"disorder\" plays more important role for information propagation than\npartially closed paths included in a network. Next we inquired into it in more\ndetail by introducing two types of intermediate networks. Then we find that the\naverage of the local clustering coefficient and the generalized clustering\ncoefficients $C_{(q)}$ have some different functions and important meanings,\nrespectively. We also find that $C_{(q)}$ is close to the propagation of\ninformation on networks. Lastly, we show that realizability of six degrees of\nseparation in networks can be understood in a unified way by disorder. \n\n"}
{"id": "1206.4327", "contents": "Title: Social Influence in Social Advertising: Evidence from Field Experiments Abstract: Social advertising uses information about consumers' peers, including peer\naffiliations with a brand, product, organization, etc., to target ads and\ncontextualize their display. This approach can increase ad efficacy for two\nmain reasons: peers' affiliations reflect unobserved consumer characteristics,\nwhich are correlated along the social network; and the inclusion of social cues\n(i.e., peers' association with a brand) alongside ads affect responses via\nsocial influence processes. For these reasons, responses may be increased when\nmultiple social signals are presented with ads, and when ads are affiliated\nwith peers who are strong, rather than weak, ties.\n  We conduct two very large field experiments that identify the effect of\nsocial cues on consumer responses to ads, measured in terms of ad clicks and\nthe formation of connections with the advertised entity. In the first\nexperiment, we randomize the number of social cues present in word-of-mouth\nadvertising, and measure how responses increase as a function of the number of\ncues. The second experiment examines the effect of augmenting traditional ad\nunits with a minimal social cue (i.e., displaying a peer's affiliation below an\nad in light grey text). On average, this cue causes significant increases in ad\nperformance. Using a measurement of tie strength based on the total amount of\ncommunication between subjects and their peers, we show that these influence\neffects are greatest for strong ties. Our work has implications for ad\noptimization, user interface design, and central questions in social science\nresearch. \n\n"}
{"id": "1206.4358", "contents": "Title: Robust Detection of Dynamic Community Structure in Networks Abstract: We describe techniques for the robust detection of community structure in\nsome classes of time-dependent networks. Specifically, we consider the use of\nstatistical null models for facilitating the principled identification of\nstructural modules in semi-decomposable systems. Null models play an important\nrole both in the optimization of quality functions such as modularity and in\nthe subsequent assessment of the statistical validity of identified community\nstructure. We examine the sensitivity of such methods to model parameters and\nshow how comparisons to null models can help identify system scales. By\nconsidering a large number of optimizations, we quantify the variance of\nnetwork diagnostics over optimizations (`optimization variance') and over\nrandomizations of network structure (`randomization variance'). Because the\nmodularity quality function typically has a large number of nearly-degenerate\nlocal optima for networks constructed using real data, we develop a method to\nconstruct representative partitions that uses a null model to correct for\nstatistical noise in sets of partitions. To illustrate our results, we employ\nensembles of time-dependent networks extracted from both nonlinear oscillators\nand empirical neuroscience data. \n\n"}
{"id": "1206.4969", "contents": "Title: Community detection using spectral clustering on sparse geosocial data Abstract: In this article we identify social communities among gang members in the\nHollenbeck policing district in Los Angeles, based on sparse observations of a\ncombination of social interactions and geographic locations of the individuals.\nThis information, coming from LAPD Field Interview cards, is used to construct\na similarity graph for the individuals. We use spectral clustering to identify\nclusters in the graph, corresponding to communities in Hollenbeck, and compare\nthese with the LAPD's knowledge of the individuals' gang membership. We discuss\ndifferent ways of encoding the geosocial information using a graph structure\nand the influence on the resulting clusterings. Finally we analyze the\nrobustness of this technique with respect to noisy and incomplete data, thereby\nproviding suggestions about the relative importance of quantity versus quality\nof collected data. \n\n"}
{"id": "1207.0437", "contents": "Title: Ordinal and Cardinal Dendrograms Depicting Migration-Based\n  Regionalization of 3,000 + U. S. Counties Abstract: We have obtained a \"hierarchical regionalization\" of 3,107 county-level units\nof the United States based upon census-recorded 1995-2000 intercounty migration\nflows. The methodology employed was the two-stage (double-standardization and\nstrong component [directed graph] hierarchical clustering) algorithm described\nin the 2009 PNAS (106 [26], E66) letter (arXiv:0904.4863). Various features (e.\ng., cosmopolitan vs. provincial aspects, and indices of isolation) of the\nregionalization have been previously discussed in arXiv:0907.2393,\narXiv:0903.3623 and arXiv:0809.2768. However, due to the lengthy (38-page)\nnature of the associated dendrogram, the detailed tree structure itself was not\nreadily available for inspection. Here, we do present this (county-searchable)\ndendrogram--and invite readers to explore it, based on their particular\ninterests/locations. An ordinal scale--rather than the originally-derived\ncardinal scale of the doubly-standardized values--in which groupings/features\nwere more immediately apparent, was originally presented. Now, we append the\ncardinal-scale dendrogram. \n\n"}
{"id": "1207.1977", "contents": "Title: Estimating a Causal Order among Groups of Variables in Linear Models Abstract: The machine learning community has recently devoted much attention to the\nproblem of inferring causal relationships from statistical data. Most of this\nwork has focused on uncovering connections among scalar random variables. We\ngeneralize existing methods to apply to collections of multi-dimensional random\nvectors, focusing on techniques applicable to linear models. The performance of\nthe resulting algorithms is evaluated and compared in simulations, which show\nthat our methods can, in many cases, provide useful information on causal\nrelationships even for relatively small sample sizes. \n\n"}
{"id": "1207.3994", "contents": "Title: Model Selection for Degree-corrected Block Models Abstract: The proliferation of models for networks raises challenging problems of model\nselection: the data are sparse and globally dependent, and models are typically\nhigh-dimensional and have large numbers of latent variables. Together, these\nissues mean that the usual model-selection criteria do not work properly for\nnetworks. We illustrate these challenges, and show one way to resolve them, by\nconsidering the key network-analysis problem of dividing a graph into\ncommunities or blocks of nodes with homogeneous patterns of links to the rest\nof the network. The standard tool for doing this is the stochastic block model,\nunder which the probability of a link between two nodes is a function solely of\nthe blocks to which they belong. This imposes a homogeneous degree distribution\nwithin each block; this can be unrealistic, so degree-corrected block models\nadd a parameter for each node, modulating its over-all degree. The choice\nbetween ordinary and degree-corrected block models matters because they make\nvery different inferences about communities. We present the first principled\nand tractable approach to model selection between standard and degree-corrected\nblock models, based on new large-graph asymptotics for the distribution of\nlog-likelihood ratios under the stochastic block model, finding substantial\ndepartures from classical results for sparse graphs. We also develop\nlinear-time approximations for log-likelihoods under both the stochastic block\nmodel and the degree-corrected model, using belief propagation. Applications to\nsimulated and real networks show excellent agreement with our approximations.\nOur results thus both solve the practical problem of deciding on degree\ncorrection, and point to a general approach to model selection in network\nanalysis. \n\n"}
{"id": "1207.4421", "contents": "Title: Stochastic optimization and sparse statistical recovery: An optimal\n  algorithm for high dimensions Abstract: We develop and analyze stochastic optimization algorithms for problems in\nwhich the expected loss is strongly convex, and the optimum is (approximately)\nsparse. Previous approaches are able to exploit only one of these two\nstructures, yielding an $\\order(\\pdim/T)$ convergence rate for strongly convex\nobjectives in $\\pdim$ dimensions, and an $\\order(\\sqrt{(\\spindex \\log\n\\pdim)/T})$ convergence rate when the optimum is $\\spindex$-sparse. Our\nalgorithm is based on successively solving a series of $\\ell_1$-regularized\noptimization problems using Nesterov's dual averaging algorithm. We establish\nthat the error of our solution after $T$ iterations is at most\n$\\order((\\spindex \\log\\pdim)/T)$, with natural extensions to approximate\nsparsity. Our results apply to locally Lipschitz losses including the logistic,\nexponential, hinge and least-squares losses. By recourse to statistical minimax\nresults, we show that our convergence rates are optimal up to multiplicative\nconstant factors. The effectiveness of our approach is also confirmed in\nnumerical simulations, in which we compare to several baselines on a\nleast-squares regression problem. \n\n"}
{"id": "1207.4941", "contents": "Title: Clustering function: a measure of social influence Abstract: A commonly used characteristic of statistical dependence of adjacency\nrelations in real networks, the clustering coefficient, evaluates chances that\ntwo neighbours of a given vertex are adjacent. An extension is obtained by\nconsidering conditional probabilities that two randomly chosen vertices are\nadjacent given that they have r common neighbours. We denote such probabilities\ncl(r) and call r-> cl(r) the clustering function.\n  We compare clustering functions of several networks having non-negligible\nclustering coefficient. They show similar patterns and surprising regularity.\nWe establish a first order asymptotic (as the number of vertices tends to\ninfinity) of the clustering function of related random intersection graph\nmodels admitting nonvanishing clustering coefficient and asymptotic degree\ndistribution having a finite second moment. \n\n"}
{"id": "1207.6416", "contents": "Title: The Social Climbing Game Abstract: The structure of a society depends, to some extent, on the incentives of the\nindividuals they are composed of. We study a stylized model of this interplay,\nthat suggests that the more individuals aim at climbing the social hierarchy,\nthe more society's hierarchy gets strong. Such a dependence is sharp, in the\nsense that a persistent hierarchical order emerges abruptly when the preference\nfor social status gets larger than a threshold. This phase transition has its\norigin in the fact that the presence of a well defined hierarchy allows agents\nto climb it, thus reinforcing it, whereas in a \"disordered\" society it is\nharder for agents to find out whom they should connect to in order to become\nmore central. Interestingly, a social order emerges when agents strive harder\nto climb society and it results in a state of reduced social mobility, as a\nconsequence of ergodicity breaking, where climbing is more difficult. \n\n"}
{"id": "1208.0095", "contents": "Title: The Simmel effect and babies names Abstract: Simulations of the Simmel effect are performed for agents in a scale-free\nsocial network. The social hierarchy of an agent is determined by the degree of\nher node. Particular features, once selected by a highly connected agent,\nbecame common in lower class but soon fall out of fashion and extinct.\nNumerical results reflect the dynamics of frequency of American babies names in\n1880-2011. \n\n"}
{"id": "1208.1237", "contents": "Title: Fast and Robust Recursive Algorithms for Separable Nonnegative Matrix\n  Factorization Abstract: In this paper, we study the nonnegative matrix factorization problem under\nthe separability assumption (that is, there exists a cone spanned by a small\nsubset of the columns of the input nonnegative data matrix containing all\ncolumns), which is equivalent to the hyperspectral unmixing problem under the\nlinear mixing model and the pure-pixel assumption. We present a family of fast\nrecursive algorithms, and prove they are robust under any small perturbations\nof the input data matrix. This family generalizes several existing\nhyperspectral unmixing algorithms and hence provides for the first time a\ntheoretical justification of their better practical performance. \n\n"}
{"id": "1208.2518", "contents": "Title: Software systems through complex networks science: Review, analysis and\n  applications Abstract: Complex software systems are among most sophisticated human-made systems, yet\nonly little is known about the actual structure of 'good' software. We here\nstudy different software systems developed in Java from the perspective of\nnetwork science. The study reveals that network theory can provide a prominent\nset of techniques for the exploratory analysis of large complex software\nsystem. We further identify several applications in software engineering, and\npropose different network-based quality indicators that address software\ndesign, efficiency, reusability, vulnerability, controllability and other. We\nalso highlight various interesting findings, e.g., software systems are highly\nvulnerable to processes like bug propagation, however, they are not easily\ncontrollable. \n\n"}
{"id": "1208.3728", "contents": "Title: Online Learning with Predictable Sequences Abstract: We present methods for online linear optimization that take advantage of\nbenign (as opposed to worst-case) sequences. Specifically if the sequence\nencountered by the learner is described well by a known \"predictable process\",\nthe algorithms presented enjoy tighter bounds as compared to the typical worst\ncase bounds. Additionally, the methods achieve the usual worst-case regret\nbounds if the sequence is not benign. Our approach can be seen as a way of\nadding prior knowledge about the sequence within the paradigm of online\nlearning. The setting is shown to encompass partial and side information.\nVariance and path-length bounds can be seen as particular examples of online\nlearning with simple predictable sequences.\n  We further extend our methods and results to include competing with a set of\npossible predictable processes (models), that is \"learning\" the predictable\nprocess itself concurrently with using it to obtain better regret guarantees.\nWe show that such model selection is possible under various assumptions on the\navailable feedback. Our results suggest a promising direction of further\nresearch with potential applications to stock market and time series\nprediction. \n\n"}
{"id": "1209.1557", "contents": "Title: Learning Model-Based Sparsity via Projected Gradient Descent Abstract: Several convex formulation methods have been proposed previously for\nstatistical estimation with structured sparsity as the prior. These methods\noften require a carefully tuned regularization parameter, often a cumbersome or\nheuristic exercise. Furthermore, the estimate that these methods produce might\nnot belong to the desired sparsity model, albeit accurately approximating the\ntrue parameter. Therefore, greedy-type algorithms could often be more desirable\nin estimating structured-sparse parameters. So far, these greedy methods have\nmostly focused on linear statistical models. In this paper we study the\nprojected gradient descent with non-convex structured-sparse parameter model as\nthe constraint set. Should the cost function have a Stable Model-Restricted\nHessian the algorithm produces an approximation for the desired minimizer. As\nan example we elaborate on application of the main results to estimation in\nGeneralized Linear Model. \n\n"}
{"id": "1209.1873", "contents": "Title: Stochastic Dual Coordinate Ascent Methods for Regularized Loss\n  Minimization Abstract: Stochastic Gradient Descent (SGD) has become popular for solving large scale\nsupervised machine learning optimization problems such as SVM, due to their\nstrong theoretical guarantees. While the closely related Dual Coordinate Ascent\n(DCA) method has been implemented in various software packages, it has so far\nlacked good convergence analysis. This paper presents a new analysis of\nStochastic Dual Coordinate Ascent (SDCA) showing that this class of methods\nenjoy strong theoretical guarantees that are comparable or better than SGD.\nThis analysis justifies the effectiveness of SDCA for practical applications. \n\n"}
{"id": "1209.2486", "contents": "Title: On sampling social networking services Abstract: This article aims at summarizing the existing methods for sampling social\nnetworking services and proposing a faster confidence interval for related\nsampling methods. It also includes comparisons of common network sampling\ntechniques. \n\n"}
{"id": "1209.5998", "contents": "Title: Biased Assimilation, Homophily and the Dynamics of Polarization Abstract: Are we as a society getting more polarized, and if so, why? We try to answer\nthis question through a model of opinion formation. Empirical studies have\nshown that homophily results in polarization. However, we show that DeGroot's\nwell-known model of opinion formation based on repeated averaging can never be\npolarizing, even if individuals are arbitrarily homophilous. We generalize\nDeGroot's model to account for a phenomenon well-known in social psychology as\nbiased assimilation: when presented with mixed or inconclusive evidence on a\ncomplex issue, individuals draw undue support for their initial position\nthereby arriving at a more extreme opinion. We show that in a simple model of\nhomophilous networks, our biased opinion formation process results in either\npolarization, persistent disagreement or consensus depending on how biased\nindividuals are. In other words, homophily alone, without biased assimilation,\nis not sufficient to polarize society. Quite interestingly, biased assimilation\nalso provides insight into the following related question: do internet based\nrecommender algorithms that show us personalized content contribute to\npolarization? We make a connection between biased assimilation and the\npolarizing effects of some random-walk based recommender algorithms that are\nsimilar in spirit to some commonly used recommender algorithms. \n\n"}
{"id": "1210.0848", "contents": "Title: Enhancing Twitter Data Analysis with Simple Semantic Filtering: Example\n  in Tracking Influenza-Like Illnesses Abstract: Systems that exploit publicly available user generated content such as\nTwitter messages have been successful in tracking seasonal influenza. We\ndeveloped a novel filtering method for Influenza-Like-Illnesses (ILI)-related\nmessages using 587 million messages from Twitter micro-blogs. We first filtered\nmessages based on syndrome keywords from the BioCaster Ontology, an extant\nknowledge model of laymen's terms. We then filtered the messages according to\nsemantic features such as negation, hashtags, emoticons, humor and geography.\nThe data covered 36 weeks for the US 2009 influenza season from 30th August\n2009 to 8th May 2010. Results showed that our system achieved the highest\nPearson correlation coefficient of 98.46% (p-value<2.2e-16), an improvement of\n3.98% over the previous state-of-the-art method. The results indicate that\nsimple NLP-based enhancements to existing approaches to mine Twitter data can\nincrease the value of this inexpensive resource. \n\n"}
{"id": "1210.4007", "contents": "Title: Extending modularity by capturing the similarity attraction feature in\n  the null model Abstract: Modularity is a widely used measure for evaluating community structure in\nnetworks. The definition of modularity involves a comparison of\nwithin-community edges in the observed network and that number in an equivalent\nrandomized network. This equivalent randomized network is called the null\nmodel, which serves as a reference. To make the comparison significant, the\nnull model should characterize some features of the observed network. However,\nthe null model in the original definition of modularity is unrealistically\nmixed, in the sense that any node can be linked to any other node without\npreference and only connectivity matters. Thus, it fails to be a good\nrepresentation of real-world networks. A common feature of many real-world\nnetworks is \"similarity attraction\", i.e., edges tend to link to nodes that are\nsimilar to each other. We propose a null model that captures the similarity\nattraction feature. This null model enables us to create a framework for\ndefining a family of Dist-Modularity adapted to various networks, including\nnetworks with additional information on nodes. We demonstrate that\nDist-Modularity is useful in identifying communities at different scales. \n\n"}
{"id": "1210.5196", "contents": "Title: Matrix reconstruction with the local max norm Abstract: We introduce a new family of matrix norms, the \"local max\" norms,\ngeneralizing existing methods such as the max norm, the trace norm (nuclear\nnorm), and the weighted or smoothed weighted trace norms, which have been\nextensively used in the literature as regularizers for matrix reconstruction\nproblems. We show that this new family can be used to interpolate between the\n(weighted or unweighted) trace norm and the more conservative max norm. We test\nthis interpolation on simulated data and on the large-scale Netflix and\nMovieLens ratings data, and find improved accuracy relative to the existing\nmatrix norms. We also provide theoretical results showing learning guarantees\nfor some of the new norms. \n\n"}
{"id": "1210.7054", "contents": "Title: Large-Scale Sparse Principal Component Analysis with Application to Text\n  Data Abstract: Sparse PCA provides a linear combination of small number of features that\nmaximizes variance across data. Although Sparse PCA has apparent advantages\ncompared to PCA, such as better interpretability, it is generally thought to be\ncomputationally much more expensive. In this paper, we demonstrate the\nsurprising fact that sparse PCA can be easier than PCA in practice, and that it\ncan be reliably applied to very large data sets. This comes from a rigorous\nfeature elimination pre-processing result, coupled with the favorable fact that\nfeatures in real-life data typically have exponentially decreasing variances,\nwhich allows for many features to be eliminated. We introduce a fast block\ncoordinate ascent algorithm with much better computational complexity than the\nexisting first-order ones. We provide experimental results obtained on text\ncorpora involving millions of documents and hundreds of thousands of features.\nThese results illustrate how Sparse PCA can help organize a large corpus of\ntext data in a user-interpretable way, providing an attractive alternative\napproach to topic models. \n\n"}
{"id": "1211.0169", "contents": "Title: Multi-Stratum Networks: toward a unified model of on-line identities Abstract: One of the reasons behind the success of Social Network Analysis is its\nsimple and general graph model made of nodes (representing individuals) and\nties. However, when we focus on our daily on-line experience we must confront a\nmore complex scenario: people inhabitate several on-line spaces interacting to\nseveral communities active on various technological infrastructures like\nTwitter, Facebook, YouTube or FourSquare and with distinct social objectives.\nThis constitutes a complex network of interconnected networks where users'\nidentities are spread and where information propagates navigating through\ndifferent communities and social platforms. In this article we introduce a\nmodel for this layered scenario that we call multi-stratum network. Through a\ntheoretical discussion and the analysis of real-world data we show how not only\nfocusing on a single network may provide a very partial understanding of the\nrole of its users, but also that considering all the networks separately may\nnot reveal the information contained in the whole multi-stratum model. \n\n"}
{"id": "1211.2717", "contents": "Title: Proximal Stochastic Dual Coordinate Ascent Abstract: We introduce a proximal version of dual coordinate ascent method. We\ndemonstrate how the derived algorithmic framework can be used for numerous\nregularized loss minimization problems, including $\\ell_1$ regularization and\nstructured output SVM. The convergence rates we obtain match, and sometimes\nimprove, state-of-the-art results. \n\n"}
{"id": "1211.3295", "contents": "Title: Order-independent constraint-based causal structure learning Abstract: We consider constraint-based methods for causal structure learning, such as\nthe PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993),\nRichardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The first\nstep of all these algorithms consists of the PC-algorithm. This algorithm is\nknown to be order-dependent, in the sense that the output can depend on the\norder in which the variables are given. This order-dependence is a minor issue\nin low-dimensional settings. We show, however, that it can be very pronounced\nin high-dimensional settings, where it can lead to highly variable results. We\npropose several modifications of the PC-algorithm (and hence also of the other\nalgorithms) that remove part or all of this order-dependence. All proposed\nmodifications are consistent in high-dimensional settings under the same\nconditions as their original counterparts. We compare the PC-, FCI-, and\nRFCI-algorithms and their modifications in simulation studies and on a yeast\ngene expression data set. We show that our modifications yield similar\nperformance in low-dimensional settings and improved performance in\nhigh-dimensional settings. All software is implemented in the R-package pcalg. \n\n"}
{"id": "1212.0121", "contents": "Title: Opinion dynamics with disagreement and modulated information Abstract: Opinion dynamics concerns social processes through which populations or\ngroups of individuals agree or disagree on specific issues. As such, modelling\nopinion dynamics represents an important research area that has been\nprogressively acquiring relevance in many different domains. Existing\napproaches have mostly represented opinions through discrete binary or\ncontinuous variables by exploring a whole panoply of cases: e.g. independence,\nnoise, external effects, multiple issues. In most of these cases the crucial\ningredient is an attractive dynamics through which similar or similar enough\nagents get closer. Only rarely the possibility of explicit disagreement has\nbeen taken into account (i.e., the possibility for a repulsive interaction\namong individuals' opinions), and mostly for discrete or 1-dimensional\nopinions, through the introduction of additional model parameters. Here we\nintroduce a new model of opinion formation, which focuses on the interplay\nbetween the possibility of explicit disagreement, modulated in a\nself-consistent way by the existing opinions' overlaps between the interacting\nindividuals, and the effect of external information on the system. Opinions are\nmodelled as a vector of continuous variables related to multiple possible\nchoices for an issue. Information can be modulated to account for promoting\nmultiple possible choices. Numerical results show that extreme information\nresults in segregation and has a limited effect on the population, while milder\nmessages have better success and a cohesion effect. Additionally, the initial\ncondition plays an important role, with the population forming one or multiple\nclusters based on the initial average similarity between individuals, with a\ntransition point depending on the number of opinion choices. \n\n"}
{"id": "1212.0167", "contents": "Title: Follow Whom? Chinese Users Have Different Choice Abstract: Sina Weibo, which was launched in 2009, is the most popular Chinese\nmicro-blogging service. It has been reported that Sina Weibo has more than 400\nmillion registered users by the end of the third quarter in 2012. Sina Weibo\nand Twitter have a lot in common, however, in terms of the following\npreference, Sina Weibo users, most of whom are Chinese, behave differently\ncompared with those of Twitter.\n  This work is based on a data set of Sina Weibo which contains 80.8 million\nusers' profiles and 7.2 billion relations and a large data set of Twitter.\nFirstly some basic features of Sina Weibo and Twitter are analyzed such as\ndegree and activeness distribution, correlation between degree and activeness,\nand the degree of separation. Then the following preference is investigated by\nstudying the assortative mixing, friend similarities, following distribution,\nedge balance ratio, and ranking correlation, where edge balance ratio is newly\nproposed to measure balance property of graphs. It is found that Sina Weibo has\na lower reciprocity rate, more positive balanced relations and is more\ndisassortative. Coinciding with Asian traditional culture, the following\npreference of Sina Weibo users is more concentrated and hierarchical: they are\nmore likely to follow people at higher or the same social levels and less\nlikely to follow people lower than themselves. In contrast, the same kind of\nfollowing preference is weaker in Twitter. Twitter users are open as they\nfollow people from levels, which accords with its global characteristic and the\nprevalence of western civilization. The message forwarding behavior is studied\nby displaying the propagation levels, delays, and critical users. The following\npreference derives from not only the usage habits but also underlying reasons\nsuch as personalities and social moralities that is worthy of future research. \n\n"}
{"id": "1212.0207", "contents": "Title: Modelling Multi-Trait Scale-free Networks by Optimization Abstract: Recently, one paper in Nature(Papadopoulos, 2012) raised an old debate on the\norigin of the scale-free property of complex networks, which focuses on whether\nthe scale-free property origins from the optimization or not. Because the\nreal-world complex networks often have multiple traits, any explanation on the\nscale-free property of complex networks should be capable of explaining the\nother traits as well. This paper proposed a framework which can model\nmulti-trait scale-free networks based on optimization, and used three examples\nto demonstrate its effectiveness. The results suggested that the optimization\nis a more generalized explanation because it can not only explain the origin of\nthe scale-free property, but also the origin of the other traits in a uniform\nway. This paper provides a universal method to get ideal networks for the\nresearches such as epidemic spreading and synchronization on complex networks. \n\n"}
{"id": "1212.0518", "contents": "Title: Sublinear but Never Superlinear Preferential Attachment by Local Network\n  Growth Abstract: We investigate a class of network growth rules that are based on a\nredirection algorithm wherein new nodes are added to a network by linking to a\nrandomly chosen target node with some probability 1-r or linking to the parent\nnode of the target node with probability r. For fixed 0<r<1, the redirection\nalgorithm is equivalent to linear preferential attachment. We show that when r\nis a decaying function of the degree of the parent of the initial target, the\nredirection algorithm produces sublinear preferential attachment network\ngrowth. We also argue that no local redirection algorithm can produce\nsuperlinear preferential attachment. \n\n"}
{"id": "1212.4194", "contents": "Title: Effect of Coupling on the Epidemic Threshold in Interconnected Complex\n  Networks: A Spectral Analysis Abstract: In epidemic modeling, the term infection strength indicates the ratio of\ninfection rate and cure rate. If the infection strength is higher than a\ncertain threshold -- which we define as the epidemic threshold - then the\nepidemic spreads through the population and persists in the long run. For a\nsingle generic graph representing the contact network of the population under\nconsideration, the epidemic threshold turns out to be equal to the inverse of\nthe spectral radius of the contact graph. However, in a real world scenario it\nis not possible to isolate a population completely: there is always some\ninterconnection with another network, which partially overlaps with the contact\nnetwork. Results for epidemic threshold in interconnected networks are limited\nto homogeneous mixing populations and degree distribution arguments. In this\npaper, we adopt a spectral approach. We show how the epidemic threshold in a\ngiven network changes as a result of being coupled with another network with\nfixed infection strength. In our model, the contact network and the\ninterconnections are generic. Using bifurcation theory and algebraic graph\ntheory, we rigorously derive the epidemic threshold in interconnected networks.\nThese results have implications for the broad field of epidemic modeling and\ncontrol. Our analytical results are supported by numerical simulations. \n\n"}
{"id": "1212.5620", "contents": "Title: Topological Analysis and Mitigation Strategies for Cascading Failures in\n  Power Grid Networks Abstract: Recently, there has been a growing concern about the overload status of the\npower grid networks, and the increasing possibility of cascading failures. Many\nresearchers have studied these networks to provide design guidelines for more\nrobust power grids. Topological analysis is one of the components of system\nanalysis for its robustness. This paper presents a complex systems analysis of\npower grid networks. First, the cascading effect has been simulated on three\nwell known networks: the IEEE 300 bus test system, the IEEE 118 bus test\nsystem, and the WSCC 179 bus equivalent model. To extend the analysis to a\nlarger set of networks, we develop a network generator and generate multiple\ngraphs with characteristics similar to the IEEE test networks but with\ndifferent topologies. The generated graphs are then compared to the test\nnetworks to show the effect of topology in determining their robustness with\nrespect to cascading failures. The generated graphs turn out to be more robust\nthan the test graphs, showing the importance of topology in the robust design\nof power grids. The second part of this paper concerns the discussion of two\nnovel mitigation strategies for cascading failures: Targeted Load Reduction and\nIslanding using Distributed Sources. These new mitigation strategies are\ncompared with the Homogeneous Load Reduction strategy. Even though the\nHomogeneous Load Reduction is simpler to implement, the Targeted Load Reduction\nis much more effective. Additionally, an algorithm is presented for the\npartitioning of the network for islanding as an effort towards fault isolation\nto prevent cascading failures. The results for island formation are better if\nthe sources are well distributed, else the algorithm leads to the formation of\nsuperislands. \n\n"}
{"id": "1212.5701", "contents": "Title: ADADELTA: An Adaptive Learning Rate Method Abstract: We present a novel per-dimension learning rate method for gradient descent\ncalled ADADELTA. The method dynamically adapts over time using only first order\ninformation and has minimal computational overhead beyond vanilla stochastic\ngradient descent. The method requires no manual tuning of a learning rate and\nappears robust to noisy gradient information, different model architecture\nchoices, various data modalities and selection of hyperparameters. We show\npromising results compared to other methods on the MNIST digit classification\ntask using a single machine and on a large scale voice dataset in a distributed\ncluster environment. \n\n"}
{"id": "1301.0189", "contents": "Title: A generalized theory of preferential linking Abstract: There are diverse mechanisms driving the evolution of social networks. A key\nopen question dealing with understanding their evolution is: How various\npreferential linking mechanisms produce networks with different features? In\nthis paper we first empirically study preferential linking phenomena in an\nevolving online social network, find and validate the linear preference. We\npropose an analyzable model which captures the real growth process of the\nnetwork and reveals the underlying mechanism dominating its evolution.\nFurthermore based on preferential linking we propose a generalized model\nreproducing the evolution of online social networks, present unified analytical\nresults describing network characteristics for 27 preference scenarios, and\nexplore the relation between preferential linking mechanism and network\nfeatures. We find that within the framework of preferential linking analytical\ndegree distributions can only be the combinations of finite kinds of functions\nwhich are related to rational, logarithmic and inverse tangent functions, and\nextremely complex network structure will emerge even for very simple sublinear\npreferential linking. This work not only provides a verifiable origin for the\nemergence of various network characteristics in social networks, but bridges\nthe micro individuals' behaviors and the global organization of social\nnetworks. \n\n"}
{"id": "1301.2851", "contents": "Title: Efficient algorithm to study interconnected networks Abstract: Interconnected networks have been shown to be much more vulnerable to random\nand targeted failures than isolated ones, raising several interesting questions\nregarding the identification and mitigation of their risk. The paradigm to\naddress these questions is the percolation model, where the resilience of the\nsystem is quantified by the dependence of the size of the largest cluster on\nthe number of failures. Numerically, the major challenge is the identification\nof this cluster and the calculation of its size. Here, we propose an efficient\nalgorithm to tackle this problem. We show that the algorithm scales as O(N log\nN), where N is the number of nodes in the network, a significant improvement\ncompared to O(N^2) for a greedy algorithm, what permits studying much larger\nnetworks. Our new strategy can be applied to any network topology and\ndistribution of interdependencies, as well as any sequence of failures. \n\n"}
{"id": "1301.2995", "contents": "Title: Measuring Cultural Dynamics Through the Eurovision Song Contest Abstract: Measuring culture and its dynamics through surveys has important limitations,\nbut the emerging field of computational social science allows us to overcome\nthem by analyzing large-scale datasets. In this article, we study cultural\ndynamics through the votes in the Eurovision song contest, which are decided by\na crowd-based scheme in which viewers vote through mobile phone messages.\nTaking into account asymmetries and imperfect perception of culture, we measure\ncultural relations among European countries in terms of cultural affinity. We\npropose the Friend-or-Foe coefficient, a metric to measure voting biases among\nparticipants of a Eurovision contest. We validate how this metric represents\ncultural affinity through its relation with known cultural distances, and\nthrough numerical analysis of biased Eurovision contests. We apply this metric\nto the historical set of Eurovision contests from 1975 to 2012, finding new\npatterns of stronger modularity than using votes alone. Furthermore, we define\na measure of polarization that, when applied to empirical data, shows a sharp\nincrease within EU countries during 2010 and 2011. We empirically validate the\nrelation between this polarization and economic indicators in the EU, showing\nhow political decisions influence both the economy and the way citizens relate\nto the culture of other EU members. \n\n"}
{"id": "1301.3568", "contents": "Title: Joint Training Deep Boltzmann Machines for Classification Abstract: We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods of training DBMs require an initial learning pass that trains the model\ngreedily, one layer at a time, or do not perform well on classification tasks.\nIn our approach, we train all layers of the DBM simultaneously, using a novel\ntraining procedure called multi-prediction training. The resulting model can\neither be interpreted as a single generative model trained to maximize a\nvariational approximation to the generalized pseudolikelihood, or as a family\nof recurrent networks that share parameters and may be approximately averaged\ntogether using a novel technique we call the multi-inference trick. We show\nthat our approach performs competitively for classification and outperforms\nprevious methods in terms of accuracy of approximate inference and\nclassification with missing inputs. \n\n"}
{"id": "1301.5650", "contents": "Title: Regularization and nonlinearities for neural language models: when are\n  they needed? Abstract: Neural language models (LMs) based on recurrent neural networks (RNN) are\nsome of the most successful word and character-level LMs. Why do they work so\nwell, in particular better than linear neural LMs? Possible explanations are\nthat RNNs have an implicitly better regularization or that RNNs have a higher\ncapacity for storing patterns due to their nonlinearities or both. Here we\nargue for the first explanation in the limit of little training data and the\nsecond explanation for large amounts of text data. We show state-of-the-art\nperformance on the popular and small Penn dataset when RNN LMs are regularized\nwith random dropout. Nonetheless, we show even better performance from a\nsimplified, much less expressive linear RNN model without off-diagonal entries\nin the recurrent matrix. We call this model an impulse-response LM (IRLM).\nUsing random dropout, column normalization and annealed learning rates, IRLMs\ndevelop neurons that keep a memory of up to 50 words in the past and achieve a\nperplexity of 102.5 on the Penn dataset. On two large datasets however, the\nsame regularization methods are unsuccessful for both models and the RNN's\nexpressivity allows it to overtake the IRLM by 10 and 20 percent perplexity,\nrespectively. Despite the perplexity gap, IRLMs still outperform RNNs on the\nMicrosoft Research Sentence Completion (MRSC) task. We develop a slightly\nmodified IRLM that separates long-context units (LCUs) from short-context units\nand show that the LCUs alone achieve a state-of-the-art performance on the MRSC\ntask of 60.8%. Our analysis indicates that a fruitful direction of research for\nneural LMs lies in developing more accessible internal representations, and\nsuggests an optimization regime of very high momentum terms for effectively\ntraining such models. \n\n"}
{"id": "1301.6944", "contents": "Title: On the Consistency of the Bootstrap Approach for Support Vector Machines\n  and Related Kernel Based Methods Abstract: It is shown that bootstrap approximations of support vector machines (SVMs)\nbased on a general convex and smooth loss function and on a general kernel are\nconsistent. This result is useful to approximate the unknown finite sample\ndistribution of SVMs by the bootstrap approach. \n\n"}
{"id": "1301.7592", "contents": "Title: Paradoxes in Social Networks with Multiple Products Abstract: Recently, we introduced in arXiv:1105.2434 a model for product adoption in\nsocial networks with multiple products, where the agents, influenced by their\nneighbours, can adopt one out of several alternatives. We identify and analyze\nhere four types of paradoxes that can arise in these networks. To this end, we\nuse social network games that we recently introduced in arxiv:1202.2209. These\nparadoxes shed light on possible inefficiencies arising when one modifies the\nsets of products available to the agents forming a social network. One of the\nparadoxes corresponds to the well-known Braess paradox in congestion games and\nshows that by adding more choices to a node, the network may end up in a\nsituation that is worse for everybody. We exhibit a dual version of this, where\nremoving available choices from someone can eventually make everybody better\noff. The other paradoxes that we identify show that by adding or removing a\nproduct from the choice set of some node may lead to permanent instability.\nFinally, we also identify conditions under which some of these paradoxes cannot\narise. \n\n"}
{"id": "1302.2684", "contents": "Title: A Tensor Approach to Learning Mixed Membership Community Models Abstract: Community detection is the task of detecting hidden communities from observed\ninteractions. Guaranteed community detection has so far been mostly limited to\nmodels with non-overlapping communities such as the stochastic block model. In\nthis paper, we remove this restriction, and provide guaranteed community\ndetection for a family of probabilistic network models with overlapping\ncommunities, termed as the mixed membership Dirichlet model, first introduced\nby Airoldi et al. This model allows for nodes to have fractional memberships in\nmultiple communities and assumes that the community memberships are drawn from\na Dirichlet distribution. Moreover, it contains the stochastic block model as a\nspecial case. We propose a unified approach to learning these models via a\ntensor spectral decomposition method. Our estimator is based on low-order\nmoment tensor of the observed network, consisting of 3-star counts. Our\nlearning method is fast and is based on simple linear algebraic operations,\ne.g. singular value decomposition and tensor power iterations. We provide\nguaranteed recovery of community memberships and model parameters and present a\ncareful finite sample analysis of our learning method. As an important special\ncase, our results match the best known scaling requirements for the\n(homogeneous) stochastic block model. \n\n"}
{"id": "1302.5794", "contents": "Title: Constant Communities in Complex Networks Abstract: Identifying community structure is a fundamental problem in network analysis.\nMost community detection algorithms are based on optimizing a combinatorial\nparameter, for example modularity. This optimization is generally NP-hard, thus\nmerely changing the vertex order can alter their assignments to the community.\nHowever, there has been very less study on how vertex ordering influences the\nresults of the community detection algorithms. Here we identify and study the\nproperties of invariant groups of vertices (constant communities) whose\nassignment to communities are, quite remarkably, not affected by vertex\nordering. The percentage of constant communities can vary across different\napplications and based on empirical results we propose metrics to evaluate\nthese communities. Using constant communities as a pre-processing step, one can\nsignificantly reduce the variation of the results. Finally, we present a case\nstudy on phoneme network and illustrate that constant communities, quite\nstrikingly, form the core functional units of the larger communities. \n\n"}
{"id": "1302.6567", "contents": "Title: Teach Network Science to Teenagers Abstract: We discuss our outreach efforts to introduce school students to network\nscience and explain why networks researchers should be involved in such\noutreach activities. We provide overviews of modules that we have designed for\nthese efforts, comment on our successes and failures, and illustrate the\npotentially enormous impact of such outreach efforts. \n\n"}
{"id": "1303.0561", "contents": "Title: Top-down particle filtering for Bayesian decision trees Abstract: Decision tree learning is a popular approach for classification and\nregression in machine learning and statistics, and Bayesian\nformulations---which introduce a prior distribution over decision trees, and\nformulate learning as posterior inference given data---have been shown to\nproduce competitive performance. Unlike classic decision tree learning\nalgorithms like ID3, C4.5 and CART, which work in a top-down manner, existing\nBayesian algorithms produce an approximation to the posterior distribution by\nevolving a complete tree (or collection thereof) iteratively via local Monte\nCarlo modifications to the structure of the tree, e.g., using Markov chain\nMonte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that\ninstead works in a top-down manner, mimicking the behavior and speed of classic\nalgorithms. We demonstrate empirically that our approach delivers accuracy\ncomparable to the most popular MCMC method, but operates more than an order of\nmagnitude faster, and thus represents a better computation-accuracy tradeoff. \n\n"}
{"id": "1303.0642", "contents": "Title: Bayesian Compressed Regression Abstract: As an alternative to variable selection or shrinkage in high dimensional\nregression, we propose to randomly compress the predictors prior to analysis.\nThis dramatically reduces storage and computational bottlenecks, performing\nwell when the predictors can be projected to a low dimensional linear subspace\nwith minimal loss of information about the response. As opposed to existing\nBayesian dimensionality reduction approaches, the exact posterior distribution\nconditional on the compressed data is available analytically, speeding up\ncomputation by many orders of magnitude while also bypassing robustness issues\ndue to convergence and mixing problems with MCMC. Model averaging is used to\nreduce sensitivity to the random projection matrix, while accommodating\nuncertainty in the subspace dimension. Strong theoretical support is provided\nfor the approach by showing near parametric convergence rates for the\npredictive density in the large p small n asymptotic paradigm. Practical\nperformance relative to competitors is illustrated in simulations and real data\napplications. \n\n"}
{"id": "1303.1369", "contents": "Title: Coevolution and correlated multiplexity in multiplex networks Abstract: Distinct channels of interaction in a complex networked system define network\nlayers, which co-exist and co-operate for the system's function. Towards\nrealistic modeling and understanding such multiplex systems, we introduce and\nstudy a class of growing multiplex network models in which different network\nlayers coevolve, and examine how the entangled growth of coevolving layers can\nshape the overall network structure. We show analytically and numerically that\nthe coevolution can induce strong degree correlations across layers, as well as\nmodulate degree distributions. We further show that such a coevolution-induced\ncorrelated multiplexity can alter the system's response to dynamical process,\nexemplified by the suppressed susceptibility to a threshold cascade process. \n\n"}
{"id": "1303.2221", "contents": "Title: Clustering on Multi-Layer Graphs via Subspace Analysis on Grassmann\n  Manifolds Abstract: Relationships between entities in datasets are often of multiple nature, like\ngeographical distance, social relationships, or common interests among people\nin a social network, for example. This information can naturally be modeled by\na set of weighted and undirected graphs that form a global multilayer graph,\nwhere the common vertex set represents the entities and the edges on different\nlayers capture the similarities of the entities in term of the different\nmodalities. In this paper, we address the problem of analyzing multi-layer\ngraphs and propose methods for clustering the vertices by efficiently merging\nthe information provided by the multiple modalities. To this end, we propose to\ncombine the characteristics of individual graph layers using tools from\nsubspace analysis on a Grassmann manifold. The resulting combination can then\nbe viewed as a low dimensional representation of the original data which\npreserves the most important information from diverse relationships between\nentities. We use this information in new clustering methods and test our\nalgorithm on several synthetic and real world datasets where we demonstrate\nsuperior or competitive performances compared to baseline and state-of-the-art\ntechniques. Our generic framework further extends to numerous analysis and\nlearning problems that involve different types of information on graphs. \n\n"}
{"id": "1303.2242", "contents": "Title: Adaptive Network Dynamics and Evolution of Leadership in Collective\n  Migration Abstract: The evolution of leadership in migratory populations depends not only on\ncosts and benefits of leadership investments but also on the opportunities for\nindividuals to rely on cues from others through social interactions. We derive\nan analytically tractable adaptive dynamic network model of collective\nmigration with fast timescale migration dynamics and slow timescale adaptive\ndynamics of individual leadership investment and social interaction. For large\npopulations, our analysis of bifurcations with respect to investment cost\nexplains the observed hysteretic effect associated with recovery of migration\nin fragmented environments. Further, we show a minimum connectivity threshold\nabove which there is evolutionary branching into leader and follower\npopulations. For small populations, we show how the topology of the underlying\nsocial interaction network influences the emergence and location of leaders in\nthe adaptive system. Our model and analysis can describe other adaptive network\ndynamics involving collective tracking or collective learning of a noisy,\nunknown signal, and likewise can inform the design of robotic networks where\nagents use decentralized strategies that balance direct environmental\nmeasurements with agent interactions. \n\n"}
{"id": "1303.3245", "contents": "Title: Flow Motifs Reveal Limitations of the Static Framework to Represent\n  Human interactions Abstract: Networks are commonly used to define underlying interaction structures where\ninfections, information, or other quantities may spread. Although the standard\napproach has been to aggregate all links into a static structure, some studies\nsuggest that the time order in which the links are established may alter the\ndynamics of spreading. In this paper, we study the impact of the time ordering\nin the limits of flow on various empirical temporal networks. By using a random\nwalk dynamics, we estimate the flow on links and convert the original\nundirected network (temporal and static) into a directed flow network. We then\nintroduce the concept of flow motifs and quantify the divergence in the\nrepresentativity of motifs when using the temporal and static frameworks. We\nfind that the regularity of contacts and persistence of vertices (common in\nemail communication and face-to-face interactions) result on little differences\nin the limits of flow for both frameworks. On the other hand, in the case of\ncommunication within a dating site (and of a sexual network), the flow between\nvertices changes significantly in the temporal framework such that the static\napproximation poorly represents the structure of contacts. We have also\nobserved that cliques with 3 and 4 vertices con- taining only low-flow links\nare more represented than the same cliques with all high-flow links. The\nrepresentativity of these low-flow cliques is higher in the temporal framework.\nOur results suggest that the flow between vertices connected in cliques depend\non the topological context in which they are placed and in the time sequence in\nwhich the links are established. The structure of the clique alone does not\ncompletely characterize the potential of flow between the vertices. \n\n"}
{"id": "1303.3764", "contents": "Title: Online Social Networks: Threats and Solutions Abstract: Many online social network (OSN) users are unaware of the numerous security\nrisks that exist in these networks, including privacy violations, identity\ntheft, and sexual harassment, just to name a few. According to recent studies,\nOSN users readily expose personal and private details about themselves, such as\nrelationship status, date of birth, school name, email address, phone number,\nand even home address. This information, if put into the wrong hands, can be\nused to harm users both in the virtual world and in the real world. These risks\nbecome even more severe when the users are children. In this paper we present a\nthorough review of the different security and privacy risks which threaten the\nwell-being of OSN users in general, and children in particular. In addition, we\npresent an overview of existing solutions that can provide better protection,\nsecurity, and privacy for OSN users. We also offer simple-to-implement\nrecommendations for OSN users which can improve their security and privacy when\nusing these platforms. Furthermore, we suggest future research directions. \n\n"}
{"id": "1303.3984", "contents": "Title: Optimal Vaccine Allocation to Control Epidemic Outbreaks in Arbitrary\n  Networks Abstract: We consider the problem of controlling the propagation of an epidemic\noutbreak in an arbitrary contact network by distributing vaccination resources\nthroughout the network. We analyze a networked version of the\nSusceptible-Infected-Susceptible (SIS) epidemic model when individuals in the\nnetwork present different levels of susceptibility to the epidemic. In this\ncontext, controlling the spread of an epidemic outbreak can be written as a\nspectral condition involving the eigenvalues of a matrix that depends on the\nnetwork structure and the parameters of the model. We study the problem of\nfinding the optimal distribution of vaccines throughout the network to control\nthe spread of an epidemic outbreak. We propose a convex framework to find\ncost-optimal distribution of vaccination resources when different levels of\nvaccination are allowed. We also propose a greedy approach with quality\nguarantees for the case of all-or-nothing vaccination. We illustrate our\napproaches with numerical simulations in a real social network. \n\n"}
{"id": "1303.5145", "contents": "Title: Node-Based Learning of Multiple Gaussian Graphical Models Abstract: We consider the problem of estimating high-dimensional Gaussian graphical\nmodels corresponding to a single set of variables under several distinct\nconditions. This problem is motivated by the task of recovering transcriptional\nregulatory networks on the basis of gene expression data {containing\nheterogeneous samples, such as different disease states, multiple species, or\ndifferent developmental stages}. We assume that most aspects of the conditional\ndependence networks are shared, but that there are some structured differences\nbetween them. Rather than assuming that similarities and differences between\nnetworks are driven by individual edges, we take a node-based approach, which\nin many cases provides a more intuitive interpretation of the network\ndifferences. We consider estimation under two distinct assumptions: (1)\ndifferences between the K networks are due to individual nodes that are\nperturbed across conditions, or (2) similarities among the K networks are due\nto the presence of common hub nodes that are shared across all K networks.\nUsing a row-column overlap norm penalty function, we formulate two convex\noptimization problems that correspond to these two assumptions. We solve these\nproblems using an alternating direction method of multipliers algorithm, and we\nderive a set of necessary and sufficient conditions that allows us to decompose\nthe problem into independent subproblems so that our algorithm can be scaled to\nhigh-dimensional settings. Our proposal is illustrated on synthetic data, a\nwebpage data set, and a brain cancer gene expression data set. \n\n"}
{"id": "1303.6370", "contents": "Title: Convex Tensor Decomposition via Structured Schatten Norm Regularization Abstract: We discuss structured Schatten norms for tensor decomposition that includes\ntwo recently proposed norms (\"overlapped\" and \"latent\") for\nconvex-optimization-based tensor decomposition, and connect tensor\ndecomposition with wider literature on structured sparsity. Based on the\nproperties of the structured Schatten norms, we mathematically analyze the\nperformance of \"latent\" approach for tensor decomposition, which was\nempirically found to perform better than the \"overlapped\" approach in some\nsettings. We show theoretically that this is indeed the case. In particular,\nwhen the unknown true tensor is low-rank in a specific mode, this approach\nperforms as good as knowing the mode with the smallest rank. Along the way, we\nshow a novel duality result for structures Schatten norms, establish the\nconsistency, and discuss the identifiability of this approach. We confirm\nthrough numerical simulations that our theoretical prediction can precisely\npredict the scaling behavior of the mean squared error. \n\n"}
{"id": "1303.7434", "contents": "Title: A multi-opinion evolving voter model with infinitely many phase\n  transitions Abstract: We consider an idealized model in which individuals' changing opinions and\ntheir social network coevolve, with disagreements between neighbors in the\nnetwork resolved either through one imitating the opinion of the other or by\nreassignment of the discordant edge. Specifically, an interaction between $x$\nand one of its neighbors $y$ leads to $x$ imitating $y$ with probability\n$(1-\\alpha)$ and otherwise (i.e., with probability $\\alpha$) $x$ cutting its\ntie to $y$ in order to instead connect to a randomly chosen individual.\nBuilding on previous work about the two-opinion case, we study the\nmultiple-opinion situation, finding that the model has infinitely many phase\ntransitions. Moreover, the formulas describing the end states of these\nprocesses are remarkably simple when expressed as a function of $\\beta =\n\\alpha/(1-\\alpha)$. \n\n"}
{"id": "1304.1548", "contents": "Title: Subgraph Frequencies: Mapping the Empirical and Extremal Geography of\n  Large Graph Collections Abstract: A growing set of on-line applications are generating data that can be viewed\nas very large collections of small, dense social graphs -- these range from\nsets of social groups, events, or collaboration projects to the vast collection\nof graph neighborhoods in large social networks. A natural question is how to\nusefully define a domain-independent coordinate system for such a collection of\ngraphs, so that the set of possible structures can be compactly represented and\nunderstood within a common space. In this work, we draw on the theory of graph\nhomomorphisms to formulate and analyze such a representation, based on\ncomputing the frequencies of small induced subgraphs within each graph. We find\nthat the space of subgraph frequencies is governed both by its combinatorial\nproperties, based on extremal results that constrain all graphs, as well as by\nits empirical properties, manifested in the way that real social graphs appear\nto lie near a simple one-dimensional curve through this space.\n  We develop flexible frameworks for studying each of these aspects. For\ncapturing empirical properties, we characterize a simple stochastic generative\nmodel, a single-parameter extension of Erdos-Renyi random graphs, whose\nstationary distribution over subgraphs closely tracks the concentration of the\nreal social graph families. For the extremal properties, we develop a tractable\nlinear program for bounding the feasible space of subgraph frequencies by\nharnessing a toolkit of known extremal graph theory. Together, these two\ncomplementary frameworks shed light on a fundamental question pertaining to\nsocial graphs: what properties of social graphs are 'social' properties and\nwhat properties are 'graph' properties?\n  We conclude with a brief demonstration of how the coordinate system we\nexamine can also be used to perform classification tasks, distinguishing\nbetween social graphs of different origins. \n\n"}
{"id": "1304.2126", "contents": "Title: Braess like Paradox in a Small World Network Abstract: Braess \\cite{1} has been studied about a traffic flow on a diamond type\nnetwork and found that introducing new edges to the networks always does not\nachieve the efficiency. Some researchers studied the Braess' paradox in similar\ntype networks by introducing various types of cost functions. But whether such\nparadox occurs or not is not scarcely studied in complex networks. In this\narticle, I analytically and numerically study whether Braess like paradox\noccurs or not on Dorogovtsev-Mendes network\\cite{2}, which is a sort of small\nworld networks. The cost function needed to go along an edge is postulated to\nbe equally identified with the length between two nodes, independently of an\namount of traffic on the edge. It is also assumed the it takes a certain cost\n$c$ to pass through the center node in Dorogovtsev-Mendes network. If $c$ is\nsmall, then bypasses have the function to provide short cuts. As result of\nnumerical and theoretical analyses, while I find that any Braess' like paradox\nwill not occur when the network size becomes infinite, I can show that a\nparadoxical phenomenon appears at finite size of network. \n\n"}
{"id": "1304.5863", "contents": "Title: Commonsense Reasoning and Large Network Analysis: A Computational Study\n  of ConceptNet 4 Abstract: In this report a computational study of ConceptNet 4 is performed using tools\nfrom the field of network analysis. Part I describes the process of extracting\nthe data from the SQL database that is available online, as well as how the\nclosure of the input among the assertions in the English language is computed.\nThis part also performs a validation of the input as well as checks for the\nconsistency of the entire database. Part II investigates the structural\nproperties of ConceptNet 4. Different graphs are induced from the knowledge\nbase by fixing different parameters. The degrees and the degree distributions\nare examined, the number and sizes of connected components, the transitivity\nand clustering coefficient, the cores, information related to shortest paths in\nthe graphs, and cliques. Part III investigates non-overlapping, as well as\noverlapping communities that are found in ConceptNet 4. Finally, Part IV\ndescribes an investigation on rules. \n\n"}
{"id": "1305.0458", "contents": "Title: From the Grid to the Smart Grid, Topologically Abstract: The Smart Grid is not just about the digitalization of the Power Grid. In its\nmore visionary acceptation, it is a model of energy management in which the\nusers are engaged in producing energy as well as consuming it, while having\ninformation systems fully aware of the energy demand-response of the network\nand of dynamically varying prices. A natural question is then: to make the\nSmart Grid a reality will the Distribution Grid have to be updated? We assume a\npositive answer to the question and we consider the lower layers of Medium and\nLow Voltage to be the most affected by the change. In our previous work, we\nhave analyzed samples of the Dutch Distribution Grid in our previous work and\nwe have considered possible evolutions of these using synthetic topologies\nmodeled after studies of complex systems in other technological domains in\nanother previous work. In this paper, we take an extra important further step\nby defining a methodology for evolving any existing physical Power Grid to a\ngood Smart Grid model thus laying the foundations for a decision support system\nfor utilities and governmental organizations. In doing so, we consider several\npossible evolution strategies and apply then to the Dutch Distribution Grid. We\nshow how more connectivity is beneficial in realizing more efficient and\nreliable networks. Our proposal is topological in nature, and enhanced with\neconomic considerations of the costs of such evolutions in terms of cabling\nexpenses and economic benefits of evolving the Grid. \n\n"}
{"id": "1305.0507", "contents": "Title: Hub-Accelerator: Fast and Exact Shortest Path Computation in Large\n  Social Networks Abstract: Shortest path computation is one of the most fundamental operations for\nmanaging and analyzing large social networks. Though existing techniques are\nquite effective for finding the shortest path on large but sparse road\nnetworks, social graphs have quite different characteristics: they are\ngenerally non-spatial, non-weighted, scale-free, and they exhibit small-world\nproperties in addition to their massive size. In particular, the existence of\nhubs, those vertices with a large number of connections, explodes the search\nspace, making the shortest path computation surprisingly challenging. In this\npaper, we introduce a set of novel techniques centered around hubs,\ncollectively referred to as the Hub-Accelerator framework, to compute the\nk-degree shortest path (finding the shortest path between two vertices if their\ndistance is within k). These techniques enable us to significantly reduce the\nsearch space by either greatly limiting the expansion scope of hubs (using the\nnovel distance- preserving Hub-Network concept) or completely pruning away the\nhubs in the online search (using the Hub2-Labeling approach). The\nHub-Accelerator approaches are more than two orders of magnitude faster than\nBFS and the state-of-the-art approximate shortest path method Sketch for the\nshortest path computation. The Hub- Network approach does not introduce\nadditional index cost with light pre-computation cost; the index size and index\nconstruction cost of Hub2-Labeling are also moderate and better than or\ncomparable to the approximation indexing Sketch method. \n\n"}
{"id": "1305.1120", "contents": "Title: The predictability of consumer visitation patterns Abstract: We consider hundreds of thousands of individual economic transactions to ask:\nhow predictable are consumers in their merchant visitation patterns? Our\nresults suggest that, in the long-run, much of our seemingly elective activity\nis actually highly predictable. Notwithstanding a wide range of individual\npreferences, shoppers share regularities in how they visit merchant locations\nover time. Yet while aggregate behavior is largely predictable, the\ninterleaving of shopping events introduces important stochastic elements at\nshort time scales. These short- and long-scale patterns suggest a theoretical\nupper bound on predictability, and describe the accuracy of a Markov model in\npredicting a person's next location. We incorporate population-level transition\nprobabilities in the predictive models, and find that in many cases these\nimprove accuracy. While our results point to the elusiveness of precise\npredictions about where a person will go next, they suggest the existence, at\nlarge time-scales, of regularities across the population. \n\n"}
{"id": "1305.2581", "contents": "Title: Accelerated Mini-Batch Stochastic Dual Coordinate Ascent Abstract: Stochastic dual coordinate ascent (SDCA) is an effective technique for\nsolving regularized loss minimization problems in machine learning. This paper\nconsiders an extension of SDCA under the mini-batch setting that is often used\nin practice. Our main contribution is to introduce an accelerated mini-batch\nversion of SDCA and prove a fast convergence rate for this method. We discuss\nan implementation of our method over a parallel computing system, and compare\nthe results to both the vanilla stochastic dual coordinate ascent and to the\naccelerated deterministic gradient descent method of\n\\cite{nesterov2007gradient}. \n\n"}
{"id": "1305.3120", "contents": "Title: Optimization with First-Order Surrogate Functions Abstract: In this paper, we study optimization methods consisting of iteratively\nminimizing surrogates of an objective function. By proposing several\nalgorithmic variants and simple convergence analyses, we make two main\ncontributions. First, we provide a unified viewpoint for several first-order\noptimization techniques such as accelerated proximal gradient, block coordinate\ndescent, or Frank-Wolfe algorithms. Second, we introduce a new incremental\nscheme that experimentally matches or outperforms state-of-the-art solvers for\nlarge-scale optimization problems typically arising in machine learning. \n\n"}
{"id": "1306.0493", "contents": "Title: Graph Metrics for Temporal Networks Abstract: Temporal networks, i.e., networks in which the interactions among a set of\nelementary units change over time, can be modelled in terms of time-varying\ngraphs, which are time-ordered sequences of graphs over a set of nodes. In such\ngraphs, the concepts of node adjacency and reachability crucially depend on the\nexact temporal ordering of the links. Consequently, all the concepts and\nmetrics proposed and used for the characterisation of static complex networks\nhave to be redefined or appropriately extended to time-varying graphs, in order\nto take into account the effects of time ordering on causality. In this chapter\nwe discuss how to represent temporal networks and we review the definitions of\nwalks, paths, connectedness and connected components valid for graphs in which\nthe links fluctuate over time. We then focus on temporal node-node distance,\nand we discuss how to characterise link persistence and the temporal\nsmall-world behaviour in this class of networks. Finally, we discuss the\nextension of classic centrality measures, including closeness, betweenness and\nspectral centrality, to the case of time-varying graphs, and we review the work\non temporal motifs analysis and the definition of modularity for temporal\ngraphs. \n\n"}
{"id": "1306.2084", "contents": "Title: Logistic Tensor Factorization for Multi-Relational Data Abstract: Tensor factorizations have become increasingly popular approaches for various\nlearning tasks on structured data. In this work, we extend the RESCAL tensor\nfactorization, which has shown state-of-the-art results for multi-relational\nlearning, to account for the binary nature of adjacency tensors. We study the\nimprovements that can be gained via this approach on various benchmark datasets\nand show that the logistic extension can improve the prediction results\nsignificantly. \n\n"}
{"id": "1306.3409", "contents": "Title: Constrained fractional set programs and their application in local\n  clustering and community detection Abstract: The (constrained) minimization of a ratio of set functions is a problem\nfrequently occurring in clustering and community detection. As these\noptimization problems are typically NP-hard, one uses convex or spectral\nrelaxations in practice. While these relaxations can be solved globally\noptimally, they are often too loose and thus lead to results far away from the\noptimum. In this paper we show that every constrained minimization problem of a\nratio of non-negative set functions allows a tight relaxation into an\nunconstrained continuous optimization problem. This result leads to a flexible\nframework for solving constrained problems in network analysis. While a\nglobally optimal solution for the resulting non-convex problem cannot be\nguaranteed, we outperform the loose convex or spectral relaxations by a large\nmargin on constrained local clustering problems. \n\n"}
{"id": "1306.3524", "contents": "Title: Analysis of data in the form of graphs Abstract: We discuss the problem of extending data mining approaches to cases in which\ndata points arise in the form of individual graphs. Being able to find the\nintrinsic low-dimensionality in ensembles of graphs can be useful in a variety\nof modeling contexts, especially when coarse-graining the detailed graph\ninformation is of interest. One of the main challenges in mining graph data is\nthe definition of a suitable pairwise similarity metric in the space of graphs.\nWe explore two practical solutions to solving this problem: one based on\nfinding subgraph densities, and one using spectral information. The approach is\nillustrated on three test data sets (ensembles of graphs); two of these are\nobtained from standard graph generating algorithms, while the graphs in the\nthird example are sampled as dynamic snapshots from an evolving network\nsimulation. \n\n"}
{"id": "1306.4064", "contents": "Title: A surrogate for networks -- How scale-free is my scale-free network? Abstract: Complex networks are now being studied in a wide range of disciplines across\nscience and technology. In this paper we propose a method by which one can\nprobe the properties of experimentally obtained network data. Rather than just\nmeasuring properties of a network inferred from data, we aim to ask how typical\nis that network? What properties of the observed network are typical of all\nsuch scale free networks, and which are peculiar? To do this we propose a\nseries of methods that can be used to generate statistically likely complex\nnetworks which are both similar to the observed data and also consistent with\nan underlying null-hypothesis -- for example a particular degree distribution.\nThere is a direct analogy between the approach we propose here and the\nsurrogate data methods applied to nonlinear time series data. \n\n"}
{"id": "1306.4650", "contents": "Title: Stochastic Majorization-Minimization Algorithms for Large-Scale\n  Optimization Abstract: Majorization-minimization algorithms consist of iteratively minimizing a\nmajorizing surrogate of an objective function. Because of its simplicity and\nits wide applicability, this principle has been very popular in statistics and\nin signal processing. In this paper, we intend to make this principle scalable.\nWe introduce a stochastic majorization-minimization scheme which is able to\ndeal with large-scale or possibly infinite data sets. When applied to convex\noptimization problems under suitable assumptions, we show that it achieves an\nexpected convergence rate of $O(1/\\sqrt{n})$ after $n$ iterations, and of\n$O(1/n)$ for strongly convex functions. Equally important, our scheme almost\nsurely converges to stationary points for a large class of non-convex problems.\nWe develop several efficient algorithms based on our framework. First, we\npropose a new stochastic proximal gradient method, which experimentally matches\nstate-of-the-art solvers for large-scale $\\ell_1$-logistic regression. Second,\nwe develop an online DC programming algorithm for non-convex sparse estimation.\nFinally, we demonstrate the effectiveness of our approach for solving\nlarge-scale structured matrix factorization problems. \n\n"}
{"id": "1306.5538", "contents": "Title: Influence of Reciprocal links in Social Networks Abstract: In this Letter, we empirically study the influence of reciprocal links, in\norder to understand its role in affecting the structure and function of\ndirected social networks. Experimental results on two representative datesets,\nSina Weibo and Douban, demonstrate that the reciprocal links indeed play a more\nimportant role than non-reciprocal ones in both spreading information and\nmaintaining the network robustness. In particular, the information spreading\nprocess can be significantly enhanced by considering the reciprocal effect. In\naddition, reciprocal links are largely responsible for the connectivity and\nefficiency of directed networks. This work may shed some light on the in-depth\nunderstanding and application of the reciprocal effect in directed online\nsocial networks. \n\n"}
{"id": "1307.2893", "contents": "Title: Coexistence in preferential attachment networks Abstract: We introduce a new model of competition on growing networks. This extends the\npreferential attachment model, with the key property that node choices evolve\nsimultaneously with the network. When a new node joins the network, it chooses\nneighbours by preferential attachment, and selects its type based on the number\nof initial neighbours of each type. The model is analysed in detail, and in\nparticular, we determine the possible proportions of the various types in the\nlimit of large networks. An important qualitative feature we find is that, in\ncontrast to many current theoretical models, often several competitors will\ncoexist. This matches empirical observations in many real-world networks. \n\n"}
{"id": "1307.5944", "contents": "Title: Online Optimization in Dynamic Environments Abstract: High-velocity streams of high-dimensional data pose significant \"big data\"\nanalysis challenges across a range of applications and settings. Online\nlearning and online convex programming play a significant role in the rapid\nrecovery of important or anomalous information from these large datastreams.\nWhile recent advances in online learning have led to novel and rapidly\nconverging algorithms, these methods are unable to adapt to nonstationary\nenvironments arising in real-world problems. This paper describes a dynamic\nmirror descent framework which addresses this challenge, yielding low\ntheoretical regret bounds and accurate, adaptive, and computationally efficient\nalgorithms which are applicable to broad classes of problems. The methods are\ncapable of learning and adapting to an underlying and possibly time-varying\ndynamical model. Empirical results in the context of dynamic texture analysis,\nsolar flare detection, sequential compressed sensing of a dynamic scene,\ntraffic surveillance,tracking self-exciting point processes and network\nbehavior in the Enron email corpus support the core theoretical findings. \n\n"}
{"id": "1308.0239", "contents": "Title: Rapid rise and decay in petition signing Abstract: Contemporary collective action, much of which involves social media and other\nInternet-based platforms, leaves a digital imprint which may be harvested to\nbetter understand the dynamics of mobilization. Petition signing is an example\nof collective action which has gained in popularity with rising use of social\nmedia and provides such data for the whole population of petition signatories\nfor a given platform. This paper tracks the growth curves of all 20,000\npetitions to the UK government petitions website\n(http://epetitions.direct.gov.uk) and 1,800 petitions to the US White House\nsite (https://petitions.whitehouse.gov), analyzing the rate of growth and\noutreach mechanism. Previous research has suggested the importance of the first\nday to the ultimate success of a petition, but has not examined early growth\nwithin that day, made possible here through hourly resolution in the data. The\nanalysis shows that the vast majority of petitions do not achieve any measure\nof success; over 99 percent fail to get the 10,000 signatures required for an\nofficial response and only 0.1 percent attain the 100,000 required for a\nparliamentary debate (0.7 percent in the US). We analyze the data through a\nmultiplicative process model framework to explain the heterogeneous growth of\nsignatures at the population level. We define and measure an average outreach\nfactor for petitions and show that it decays very fast (reducing to 0.1 pervent\nafter 10 hours in the UK and 30 hours in the US). After a day or two, a\npetition's fate is virtually set. The findings challenge conventional analyses\nof collective action from economics and political science, where the production\nfunction has been assumed to follow an S-shaped curve. \n\n"}
{"id": "1308.0786", "contents": "Title: Content Distribution Strategies in Opportunistic Networks Abstract: This paper describes a mechanism for content distribution through\nopportunistic contacts between subscribers. A subset of subscribers in the\nnetwork are seeded with the content. The remaining subscribers obtain the\ninformation through opportunistic contact with a user carrying the updated\ncontent. We study how the rate of content retrieval by subscribers is affected\nby the number of initial seeders in the network. We also study the rate of\ncontent retrieval by the subscribers under coding strategies (Network Coding,\nErasure Coding) and under Flooding, Epidemic Routing. \n\n"}
{"id": "1308.1418", "contents": "Title: A Latent Social Approach to YouTube Popularity Prediction Abstract: Current works on Information Centric Networking assume the spectrum of\ncaching strategies under the Least Recently/ Frequently Used (LRFU) scheme as\nthe de-facto standard, due to the ease of implementation and easier analysis of\nsuch strategies. In this paper we predict the popularity distribution of\nYouTube videos within a campus network. We explore two broad approaches in\npredicting the popularity of videos in the network: consensus approaches based\non aggregate behavior in the network, and social approaches based on the\ninformation diffusion over an implicit network. We measure the performance of\nour approaches under a simple caching framework by picking the k most popular\nvideos according to our predicted distribution and calculating the hit rate on\nthe cache. We develop our approach by first incorporating video inter-arrival\ntime (based on the power-law distribution governing the transmission time\nbetween two receivers of the same message in scale-free networks) to the\nbaseline (LRFU), then combining with an information diffusion model over the\ninferred latent social graph that governs diffusion of videos in the network.\nWe apply techniques from latent social network inference to learn the sharing\nprobabilities between users in the network and apply a virus propagation model\nborrowed from mathematical epidemiology to estimate the number of times a video\nwill be accessed in the future. Our approach gives rise to a 14% hit rate\nimprovement over the baseline. \n\n"}
{"id": "1308.2166", "contents": "Title: Parallel Triangle Counting in Massive Streaming Graphs Abstract: The number of triangles in a graph is a fundamental metric, used in social\nnetwork analysis, link classification and recommendation, and more. Driven by\nthese applications and the trend that modern graph datasets are both large and\ndynamic, we present the design and implementation of a fast and cache-efficient\nparallel algorithm for estimating the number of triangles in a massive\nundirected graph whose edges arrive as a stream. It brings together the\nbenefits of streaming algorithms and parallel algorithms. By building on the\nstreaming algorithms framework, the algorithm has a small memory footprint. By\nleveraging the paralell cache-oblivious framework, it makes efficient use of\nthe memory hierarchy of modern multicore machines without needing to know its\nspecific parameters. We prove theoretical bounds on accuracy, memory access\ncost, and parallel runtime complexity, as well as showing empirically that the\nalgorithm yields accurate results and substantial speedups compared to an\noptimized sequential implementation.\n  (This is an expanded version of a CIKM'13 paper of the same title.) \n\n"}
{"id": "1308.2954", "contents": "Title: Trace Complexity of Network Inference Abstract: The network inference problem consists of reconstructing the edge set of a\nnetwork given traces representing the chronology of infection times as\nepidemics spread through the network. This problem is a paradigmatic\nrepresentative of prediction tasks in machine learning that require deducing a\nlatent structure from observed patterns of activity in a network, which often\nrequire an unrealistically large number of resources (e.g., amount of available\ndata, or computational time). A fundamental question is to understand which\nproperties we can predict with a reasonable degree of accuracy with the\navailable resources, and which we cannot. We define the trace complexity as the\nnumber of distinct traces required to achieve high fidelity in reconstructing\nthe topology of the unobserved network or, more generally, some of its\nproperties. We give algorithms that are competitive with, while being simpler\nand more efficient than, existing network inference approaches. Moreover, we\nprove that our algorithms are nearly optimal, by proving an\ninformation-theoretic lower bound on the number of traces that an optimal\ninference algorithm requires for performing this task in the general case.\nGiven these strong lower bounds, we turn our attention to special cases, such\nas trees and bounded-degree graphs, and to property recovery tasks, such as\nreconstructing the degree distribution without inferring the network. We show\nthat these problems require a much smaller (and more realistic) number of\ntraces, making them potentially solvable in practice. \n\n"}
{"id": "1308.3892", "contents": "Title: Do the rich get richer? An empirical analysis of the BitCoin transaction\n  network Abstract: The possibility to analyze everyday monetary transactions is limited by the\nscarcity of available data, as this kind of information is usually considered\nhighly sensitive. Present econophysics models are usually employed on presumed\nrandom networks of interacting agents, and only macroscopic properties (e.g.\nthe resulting wealth distribution) are compared to real-world data. In this\npaper, we analyze BitCoin, which is a novel digital currency system, where the\ncomplete list of transactions is publicly available. Using this dataset, we\nreconstruct the network of transactions, and extract the time and amount of\neach payment. We analyze the structure of the transaction network by measuring\nnetwork characteristics over time, such as the degree distribution, degree\ncorrelations and clustering. We find that linear preferential attachment drives\nthe growth of the network. We also study the dynamics taking place on the\ntransaction network, i.e. the flow of money. We measure temporal patterns and\nthe wealth accumulation. Investigating the microscopic statistics of money\nmovement, we find that sublinear preferential attachment governs the evolution\nof the wealth distribution. We report a scaling relation between the degree and\nwealth associated to individual nodes. \n\n"}
{"id": "1308.5273", "contents": "Title: CrowdGrader: Crowdsourcing the Evaluation of Homework Assignments Abstract: Crowdsourcing offers a practical method for ranking and scoring large amounts\nof items. To investigate the algorithms and incentives that can be used in\ncrowdsourcing quality evaluations, we built CrowdGrader, a tool that lets\nstudents submit and collaboratively grade solutions to homework assignments. We\npresent the algorithms and techniques used in CrowdGrader, and we describe our\nresults and experience in using the tool for several computer-science\nassignments.\n  CrowdGrader combines the student-provided grades into a consensus grade for\neach submission using a novel crowdsourcing algorithm that relies on a\nreputation system. The algorithm iterativerly refines inter-dependent estimates\nof the consensus grades, and of the grading accuracy of each student. On\nsynthetic data, the algorithm performs better than alternatives not based on\nreputation. On our preliminary experimental data, the performance seems\ndependent on the nature of review errors, with errors that can be ascribed to\nthe reviewer being more tractable than those arising from random external\nevents. To provide an incentive for reviewers, the grade each student receives\nin an assignment is a combination of the consensus grade received by their\nsubmissions, and of a reviewing grade capturing their reviewing effort and\naccuracy. This incentive worked well in practice. \n\n"}
{"id": "1309.0242", "contents": "Title: Ensemble approaches for improving community detection methods Abstract: Statistical estimates can often be improved by fusion of data from several\ndifferent sources. One example is so-called ensemble methods which have been\nsuccessfully applied in areas such as machine learning for classification and\nclustering. In this paper, we present an ensemble method to improve community\ndetection by aggregating the information found in an ensemble of community\nstructures. This ensemble can found by re-sampling methods, multiple runs of a\nstochastic community detection method, or by several different community\ndetection algorithms applied to the same network. The proposed method is\nevaluated using random networks with community structures and compared with two\ncommonly used community detection methods. The proposed method when applied on\na stochastic community detection algorithm performs well with low computational\ncomplexity, thus offering both a new approach to community detection and an\nadditional community detection method. \n\n"}
{"id": "1309.0787", "contents": "Title: Online Tensor Methods for Learning Latent Variable Models Abstract: We introduce an online tensor decomposition based approach for two latent\nvariable modeling problems namely, (1) community detection, in which we learn\nthe latent communities that the social actors in social networks belong to, and\n(2) topic modeling, in which we infer hidden topics of text articles. We\nconsider decomposition of moment tensors using stochastic gradient descent. We\nconduct optimization of multilinear operations in SGD and avoid directly\nforming the tensors, to save computational and storage costs. We present\noptimized algorithm in two platforms. Our GPU-based implementation exploits the\nparallelism of SIMD architectures to allow for maximum speed-up by a careful\noptimization of storage and data transfer, whereas our CPU-based implementation\nuses efficient sparse matrix computations and is suitable for large sparse\ndatasets. For the community detection problem, we demonstrate accuracy and\ncomputational efficiency on Facebook, Yelp and DBLP datasets, and for the topic\nmodeling problem, we also demonstrate good performance on the New York Times\ndataset. We compare our results to the state-of-the-art algorithms such as the\nvariational method, and report a gain of accuracy and a gain of several orders\nof magnitude in the execution time. \n\n"}
{"id": "1309.1952", "contents": "Title: A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries Abstract: We consider the problem of learning overcomplete dictionaries in the context\nof sparse coding, where each sample selects a sparse subset of dictionary\nelements. Our main result is a strategy to approximately recover the unknown\ndictionary using an efficient algorithm. Our algorithm is a clustering-style\nprocedure, where each cluster is used to estimate a dictionary element. The\nresulting solution can often be further cleaned up to obtain a high accuracy\nestimate, and we provide one simple scenario where $\\ell_1$-regularized\nregression can be used for such a second stage. \n\n"}
{"id": "1309.3321", "contents": "Title: Wedge Sampling for Computing Clustering Coefficients and Triangle Counts\n  on Large Graphs Abstract: Graphs are used to model interactions in a variety of contexts, and there is\na growing need to quickly assess the structure of such graphs. Some of the most\nuseful graph metrics are based on triangles, such as those measuring social\ncohesion. Algorithms to compute them can be extremely expensive, even for\nmoderately-sized graphs with only millions of edges. Previous work has\nconsidered node and edge sampling; in contrast, we consider wedge sampling,\nwhich provides faster and more accurate approximations than competing\ntechniques. Additionally, wedge sampling enables estimation local clustering\ncoefficients, degree-wise clustering coefficients, uniform triangle sampling,\nand directed triangle counts. Our methods come with provable and practical\nprobabilistic error estimates for all computations. We provide extensive\nresults that show our methods are both more accurate and faster than\nstate-of-the-art alternatives. \n\n"}
{"id": "1309.3888", "contents": "Title: User-Relatedness and Community Structure in Social Interaction Networks Abstract: With social media and the according social and ubiquitous applications\nfinding their way into everyday life, there is a rapidly growing amount of user\ngenerated content yielding explicit and implicit network structures. We\nconsider social activities and phenomena as proxies for user relatedness. Such\nactivities are represented in so-called social interaction networks or evidence\nnetworks, with different degrees of explicitness. We focus on evidence networks\ncontaining relations on users, which are represented by connections between\nindividual nodes. Explicit interaction networks are then created by specific\nuser actions, for example, when building a friend network. On the other hand,\nmore implicit networks capture user traces or evidences of user actions as\nobserved in Web portals, blogs, resource sharing systems, and many other social\nservices. These implicit networks can be applied for a broad range of analysis\nmethods instead of using expensive gold-standard information.\n  In this paper, we analyze different properties of a set of networks in social\nmedia. We show that there are dependencies and correlations between the\nnetworks. These allow for drawing reciprocal conclusions concerning pairs of\nnetworks, based on the assessment of structural correlations and ranking\ninterchangeability. Additionally, we show how these inter-network correlations\ncan be used for assessing the results of structural analysis techniques, e.g.,\ncommunity mining methods. \n\n"}
{"id": "1309.5124", "contents": "Title: Multi-layer graph analysis for dynamic social networks Abstract: Modern social networks frequently encompass multiple distinct types of\nconnectivity information; for instance, explicitly acknowledged friend\nrelationships might complement behavioral measures that link users according to\ntheir actions or interests. One way to represent these networks is as\nmulti-layer graphs, where each layer contains a unique set of edges over the\nsame underlying vertices (users). Edges in different layers typically have\nrelated but distinct semantics; depending on the application multiple layers\nmight be used to reduce noise through averaging, to perform multifaceted\nanalyses, or a combination of the two. However, it is not obvious how to extend\nstandard graph analysis techniques to the multi-layer setting in a flexible\nway. In this paper we develop latent variable models and methods for mining\nmulti-layer networks for connectivity patterns based on noisy data. \n\n"}
{"id": "1310.0432", "contents": "Title: Online Learning of Dynamic Parameters in Social Networks Abstract: This paper addresses the problem of online learning in a dynamic setting. We\nconsider a social network in which each individual observes a private signal\nabout the underlying state of the world and communicates with her neighbors at\neach time period. Unlike many existing approaches, the underlying state is\ndynamic, and evolves according to a geometric random walk. We view the scenario\nas an optimization problem where agents aim to learn the true state while\nsuffering the smallest possible loss. Based on the decomposition of the global\nloss function, we introduce two update mechanisms, each of which generates an\nestimate of the true state. We establish a tight bound on the rate of change of\nthe underlying state, under which individuals can track the parameter with a\nbounded variance. Then, we characterize explicit expressions for the steady\nstate mean-square deviation(MSD) of the estimates from the truth, per\nindividual. We observe that only one of the estimators recovers the optimal\nMSD, which underscores the impact of the objective function decomposition on\nthe learning quality. Finally, we provide an upper bound on the regret of the\nproposed methods, measured as an average of errors in estimating the parameter\nin a finite time. \n\n"}
{"id": "1310.2273", "contents": "Title: Semidefinite Programming Based Preconditioning for More Robust\n  Near-Separable Nonnegative Matrix Factorization Abstract: Nonnegative matrix factorization (NMF) under the separability assumption can\nprovably be solved efficiently, even in the presence of noise, and has been\nshown to be a powerful technique in document classification and hyperspectral\nunmixing. This problem is referred to as near-separable NMF and requires that\nthere exists a cone spanned by a small subset of the columns of the input\nnonnegative matrix approximately containing all columns. In this paper, we\npropose a preconditioning based on semidefinite programming making the input\nmatrix well-conditioned. This in turn can improve significantly the performance\nof near-separable NMF algorithms which is illustrated on the popular successive\nprojection algorithm (SPA). The new preconditioned SPA is provably more robust\nto noise, and outperforms SPA on several synthetic data sets. We also show how\nan active-set method allow us to apply the preconditioning on large-scale\nreal-world hyperspectral images. \n\n"}
{"id": "1310.5096", "contents": "Title: Opinion Dynamic with agents immigration Abstract: We propose a strategy for achieving maximum cooperation in evolutionary games\non complex networks. Each individual is assigned a weight that is proportional\nto the power of its degree, where the exponent alpha is an adjustable parameter\nthat controls the level of diversity among individuals in the network. During\nthe evolution, every individual chooses one of its neighbors as a reference\nwith a probability proportional to the weight of the neighbor, and updates its\nstrategy depending on their payoff difference. It is found that there exists an\noptimal value of alpha, for which the level of cooperation reaches maximum.\nThis phenomenon indicates that, although high-degree individuals play a\nprominent role in maintaining the cooperation, too strong influences from the\nhubs may counterintuitively inhibit the diffusion of cooperation. We provide a\nphysical theory, aided by numerical computations, to explain the emergence of\nthe optimal cooperation. Other pertinent quantities such as the payoff, the\ncooperator density as a function of the degree, and the payoff distribution,\nare also investigated. Our results suggest that, in order to achieve strong\ncooperation on a complex network, individuals should learn more frequently from\nneighbors with higher degrees, but only to certain extent. \n\n"}
{"id": "1310.5624", "contents": "Title: Google matrix of the citation network of Physical Review Abstract: We study the statistical properties of spectrum and eigenstates of the Google\nmatrix of the citation network of Physical Review for the period 1893 - 2009.\nThe main fraction of complex eigenvalues with largest modulus is determined\nnumerically by different methods based on high precision computations with up\nto $p=16384$ binary digits that allows to resolve hard numerical problems for\nsmall eigenvalues. The nearly nilpotent matrix structure allows to obtain a\nsemi-analytical computation of eigenvalues. We find that the spectrum is\ncharacterized by the fractal Weyl law with a fractal dimension $d_f \\approx 1$.\nIt is found that the majority of eigenvectors are located in a localized phase.\nThe statistical distribution of articles in the PageRank-CheiRank plane is\nestablished providing a better understanding of information flows on the\nnetwork. The concept of ImpactRank is proposed to determine an influence domain\nof a given article. We also discuss the properties of random matrix models of\nPerron-Frobenius operators. \n\n"}
{"id": "1311.1247", "contents": "Title: LA-CTR: A Limited Attention Collaborative Topic Regression for Social\n  Media Abstract: Probabilistic models can learn users' preferences from the history of their\nitem adoptions on a social media site, and in turn, recommend new items to\nusers based on learned preferences. However, current models ignore\npsychological factors that play an important role in shaping online social\nbehavior. One such factor is attention, the mechanism that integrates\nperceptual and cognitive features to select the items the user will consciously\nprocess and may eventually adopt. Recent research has shown that people have\nfinite attention, which constrains their online interactions, and that they\ndivide their limited attention non-uniformly over other people. We propose a\ncollaborative topic regression model that incorporates limited, non-uniformly\ndivided attention. We show that the proposed model is able to learn more\naccurate user preferences than state-of-art models, which do not take human\ncognitive factors into account. Specifically we analyze voting on news items on\nthe social news aggregator and show that our model is better able to predict\nheld out votes than alternate models. Our study demonstrates that\npsycho-socially motivated models are better able to describe and predict\nobserved behavior than models which only consider latent social structure and\ncontent. \n\n"}
{"id": "1311.2008", "contents": "Title: Statistical validation of high-dimensional models of growing networks Abstract: The abundance of models of complex networks and the current insufficient\nvalidation standards make it difficult to judge which models are strongly\nsupported by data and which are not. We focus here on likelihood maximization\nmethods for models of growing networks with many parameters and compare their\nperformance on artificial and real datasets. While high dimensionality of the\nparameter space harms the performance of direct likelihood maximization on\nartificial data, this can be improved by introducing a suitable penalization\nterm. Likelihood maximization on real data shows that the presented approach is\nable to discriminate among available network models. To make large-scale\ndatasets accessible to this kind of analysis, we propose a subset sampling\ntechnique and show that it yields substantial model evidence in a fraction of\ntime necessary for the analysis of the complete data. \n\n"}
{"id": "1311.2234", "contents": "Title: FuSSO: Functional Shrinkage and Selection Operator Abstract: We present the FuSSO, a functional analogue to the LASSO, that efficiently\nfinds a sparse set of functional input covariates to regress a real-valued\nresponse against. The FuSSO does so in a semi-parametric fashion, making no\nparametric assumptions about the nature of input functional covariates and\nassuming a linear form to the mapping of functional covariates to the response.\nWe provide a statistical backing for use of the FuSSO via proof of asymptotic\nsparsistency under various conditions. Furthermore, we observe good results on\nboth synthetic and real-world data. \n\n"}
{"id": "1311.2854", "contents": "Title: Spectral Clustering via the Power Method -- Provably Abstract: Spectral clustering is one of the most important algorithms in data mining\nand machine intelligence; however, its computational complexity limits its\napplication to truly large scale data analysis. The computational bottleneck in\nspectral clustering is computing a few of the top eigenvectors of the\n(normalized) Laplacian matrix corresponding to the graph representing the data\nto be clustered. One way to speed up the computation of these eigenvectors is\nto use the \"power method\" from the numerical linear algebra literature.\nAlthough the power method has been empirically used to speed up spectral\nclustering, the theory behind this approach, to the best of our knowledge,\nremains unexplored. This paper provides the \\emph{first} such rigorous\ntheoretical justification, arguing that a small number of power iterations\nsuffices to obtain near-optimal partitionings using the approximate\neigenvectors. Specifically, we prove that solving the $k$-means clustering\nproblem on the approximate eigenvectors obtained via the power method gives an\nadditive-error approximation to solving the $k$-means problem on the optimal\neigenvectors. \n\n"}
{"id": "1311.2889", "contents": "Title: Reinforcement Learning for Matrix Computations: PageRank as an Example Abstract: Reinforcement learning has gained wide popularity as a technique for\nsimulation-driven approximate dynamic programming. A less known aspect is that\nthe very reasons that make it effective in dynamic programming can also be\nleveraged for using it for distributed schemes for certain matrix computations\ninvolving non-negative matrices. In this spirit, we propose a reinforcement\nlearning algorithm for PageRank computation that is fashioned after analogous\nschemes for approximate dynamic programming. The algorithm has the advantage of\nease of distributed implementation and more importantly, of being model-free,\ni.e., not dependent on any specific assumptions about the transition\nprobabilities in the random web-surfer model. We analyze its convergence and\nfinite time behavior and present some supporting numerical experiments. \n\n"}
{"id": "1311.4115", "contents": "Title: A Proof Of The Block Model Threshold Conjecture Abstract: We study a random graph model named the \"block model\" in statistics and the\n\"planted partition model\" in theoretical computer science. In its simplest\nform, this is a random graph with two equal-sized clusters, with a\nbetween-class edge probability of $q$ and a within-class edge probability of\n$p$.\n  A striking conjecture of Decelle, Krzkala, Moore and Zdeborov\\'a based on\ndeep, non-rigorous ideas from statistical physics, gave a precise prediction\nfor the algorithmic threshold of clustering in the sparse planted partition\nmodel. In particular, if $p = a/n$ and $q = b/n$, $s=(a-b)/2$ and $p=(a+b)/2$\nthen Decelle et al.\\ conjectured that it is possible to efficiently cluster in\na way correlated with the true partition if $s^2 > p$ and impossible if $s^2 <\np$. By comparison, the best-known rigorous result is that of Coja-Oghlan, who\nshowed that clustering is possible if $s^2 > C p \\ln p$ for some sufficiently\nlarge $C$.\n  In a previous work, we proved that indeed it is information theoretically\nimpossible to to cluster if $s^2 < p$ and furthermore it is information\ntheoretically impossible to even estimate the model parameters from the graph\nwhen $s^2 < p$. Here we complete the proof of the conjecture by providing an\nefficient algorithm for clustering in a way that is correlated with the true\npartition when $s^2 > p$. A different independent proof of the same result was\nrecently obtained by Laurent Massoulie. \n\n"}
{"id": "1312.0169", "contents": "Title: Entropy and the Predictability of Online Life Abstract: Using mobile phone records and information theory measures, our daily lives\nhave been recently shown to follow strict statistical regularities, and our\nmovement patterns are to a large extent predictable. Here, we apply entropy and\npredictability measures to two data sets of the behavioral actions and the\nmobility of a large number of players in the virtual universe of a massive\nmultiplayer online game. We find that movements in virtual human lives follow\nthe same high levels of predictability as offline mobility, where future\nmovements can to some extent be predicted well if the temporal correlations of\nvisited places are accounted for. Time series of behavioral actions show\nsimilar high levels of predictability, even when temporal correlations are\nneglected. Entropy conditional on specific behavioral actions reveals that in\nterms of predictability negative behavior has a wider variety than positive\nactions. The actions which contain information to best predict an individual's\nsubsequent action are negative, such as attacks or enemy markings, while\npositive actions of friendship marking, trade and communication contain the\nleast amount of predictive information. These observations show that predicting\nbehavioral actions requires less information than predicting the mobility\npatterns of humans for which the additional knowledge of past visited locations\nis crucial, and that the type and sign of a social relation has an essential\nimpact on the ability to determine future behavior. \n\n"}
{"id": "1312.0232", "contents": "Title: Stochastic continuum armed bandit problem of few linear parameters in\n  high dimensions Abstract: We consider a stochastic continuum armed bandit problem where the arms are\nindexed by the $\\ell_2$ ball $B_{d}(1+\\nu)$ of radius $1+\\nu$ in\n$\\mathbb{R}^d$. The reward functions $r :B_{d}(1+\\nu) \\rightarrow \\mathbb{R}$\nare considered to intrinsically depend on $k \\ll d$ unknown linear parameters\nso that $r(\\mathbf{x}) = g(\\mathbf{A} \\mathbf{x})$ where $\\mathbf{A}$ is a full\nrank $k \\times d$ matrix. Assuming the mean reward function to be smooth we\nmake use of results from low-rank matrix recovery literature and derive an\nefficient randomized algorithm which achieves a regret bound of $O(C(k,d)\nn^{\\frac{1+k}{2+k}} (\\log n)^{\\frac{1}{2+k}})$ with high probability. Here\n$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of rounds\nor the sampling budget which is assumed to be known beforehand. \n\n"}
{"id": "1312.0860", "contents": "Title: Community Specific Temporal Topic Discovery from Social Media Abstract: Studying temporal dynamics of topics in social media is very useful to\nunderstand online user behaviors. Most of the existing work on this subject\nusually monitors the global trends, ignoring variation among communities. Since\nusers from different communities tend to have varying tastes and interests,\ncapturing community-level temporal change can improve the understanding and\nmanagement of social content. Additionally, it can further facilitate the\napplications such as community discovery, temporal prediction and online\nmarketing. However, this kind of extraction becomes challenging due to the\nintricate interactions between community and topic, and intractable\ncomputational complexity.\n  In this paper, we take a unified solution towards the community-level topic\ndynamic extraction. A probabilistic model, CosTot (Community Specific\nTopics-over-Time) is proposed to uncover the hidden topics and communities, as\nwell as capture community-specific temporal dynamics. Specifically, CosTot\nconsiders text, time, and network information simultaneously, and well\ndiscovers the interactions between community and topic over time. We then\ndiscuss the approximate inference implementation to enable scalable computation\nof model parameters, especially for large social data. Based on this, the\napplication layer support for multi-scale temporal analysis and community\nexploration is also investigated.\n  We conduct extensive experimental studies on a large real microblog dataset,\nand demonstrate the superiority of proposed model on tasks of time stamp\nprediction, link prediction and topic perplexity. \n\n"}
{"id": "1312.1666", "contents": "Title: Semi-Stochastic Gradient Descent Methods Abstract: In this paper we study the problem of minimizing the average of a large\nnumber ($n$) of smooth convex loss functions. We propose a new method, S2GD\n(Semi-Stochastic Gradient Descent), which runs for one or several epochs in\neach of which a single full gradient and a random number of stochastic\ngradients is computed, following a geometric law. The total work needed for the\nmethod to output an $\\varepsilon$-accurate solution in expectation, measured in\nthe number of passes over data, or equivalently, in units equivalent to the\ncomputation of a single gradient of the loss, is\n$O((\\kappa/n)\\log(1/\\varepsilon))$, where $\\kappa$ is the condition number.\nThis is achieved by running the method for $O(\\log(1/\\varepsilon))$ epochs,\nwith a single gradient evaluation and $O(\\kappa)$ stochastic gradient\nevaluations in each. The SVRG method of Johnson and Zhang arises as a special\ncase. If our method is limited to a single epoch only, it needs to evaluate at\nmost $O((\\kappa/\\varepsilon)\\log(1/\\varepsilon))$ stochastic gradients. In\ncontrast, SVRG requires $O(\\kappa/\\varepsilon^2)$ stochastic gradients. To\nillustrate our theoretical results, S2GD only needs the workload equivalent to\nabout 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution for\na problem with $n=10^9$ and $\\kappa=10^3$. \n\n"}
{"id": "1312.2164", "contents": "Title: Budgeted Influence Maximization for Multiple Products Abstract: The typical algorithmic problem in viral marketing aims to identify a set of\ninfluential users in a social network, who, when convinced to adopt a product,\nshall influence other users in the network and trigger a large cascade of\nadoptions. However, the host (the owner of an online social platform) often\nfaces more constraints than a single product, endless user attentions,\nunlimited budget and unbounded time; in reality, multiple products need to be\nadvertised, each user can tolerate only a small number of recommendations,\ninfluencing user has a cost and advertisers have only limited budgets, and the\nadoptions need to be maximized within a short time window.\n  Given theses myriads of user, monetary, and timing constraints, it is\nextremely challenging for the host to design principled and efficient viral\nmarket algorithms with provable guarantees. In this paper, we provide a novel\nsolution by formulating the problem as a submodular maximization in a\ncontinuous-time diffusion model under an intersection of a matroid and multiple\nknapsack constraints. We also propose an adaptive threshold greedy algorithm\nwhich can be faster than the traditional greedy algorithm with lazy evaluation,\nand scalable to networks with million of nodes. Furthermore, our mathematical\nformulation allows us to prove that the algorithm can achieve an approximation\nfactor of $k_a/(2+2 k)$ when $k_a$ out of the $k$ knapsack constraints are\nactive, which also improves over previous guarantees from combinatorial\noptimization literature. In the case when influencing each user has uniform\ncost, the approximation becomes even better to a factor of $1/3$. Extensive\nsynthetic and real world experiments demonstrate that our budgeted influence\nmaximization algorithm achieves the-state-of-the-art in terms of both\neffectiveness and scalability, often beating the next best by significant\nmargins. \n\n"}
{"id": "1312.2171", "contents": "Title: bartMachine: Machine Learning with Bayesian Additive Regression Trees Abstract: We present a new package in R implementing Bayesian additive regression trees\n(BART). The package introduces many new features for data analysis using BART\nsuch as variable selection, interaction detection, model diagnostic plots,\nincorporation of missing data and the ability to save trees for future\nprediction. It is significantly faster than the current R implementation,\nparallelized, and capable of handling both large sample sizes and\nhigh-dimensional data. \n\n"}
{"id": "1312.5179", "contents": "Title: The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited Abstract: Hypergraphs allow one to encode higher-order relationships in data and are\nthus a very flexible modeling tool. Current learning methods are either based\non approximations of the hypergraphs via graphs or on tensor methods which are\nonly applicable under special conditions. In this paper, we present a new\nlearning framework on hypergraphs which fully uses the hypergraph structure.\nThe key element is a family of regularization functionals based on the total\nvariation on hypergraphs. \n\n"}
{"id": "1312.5192", "contents": "Title: Nonlinear Eigenproblems in Data Analysis - Balanced Graph Cuts and the\n  RatioDCA-Prox Abstract: It has been recently shown that a large class of balanced graph cuts allows\nfor an exact relaxation into a nonlinear eigenproblem. We review briefly some\nof these results and propose a family of algorithms to compute nonlinear\neigenvectors which encompasses previous work as special cases. We provide a\ndetailed analysis of the properties and the convergence behavior of these\nalgorithms and then discuss their application in the area of balanced graph\ncuts. \n\n"}
{"id": "1312.5734", "contents": "Title: Time-varying Learning and Content Analytics via Sparse Factor Analysis Abstract: We propose SPARFA-Trace, a new machine learning-based framework for\ntime-varying learning and content analytics for education applications. We\ndevelop a novel message passing-based, blind, approximate Kalman filter for\nsparse factor analysis (SPARFA), that jointly (i) traces learner concept\nknowledge over time, (ii) analyzes learner concept knowledge state transitions\n(induced by interacting with learning resources, such as textbook sections,\nlecture videos, etc, or the forgetting effect), and (iii) estimates the content\norganization and intrinsic difficulty of the assessment questions. These\nquantities are estimated solely from binary-valued (correct/incorrect) graded\nlearner response data and a summary of the specific actions each learner\nperforms (e.g., answering a question or studying a learning resource) at each\ntime instance. Experimental results on two online course datasets demonstrate\nthat SPARFA-Trace is capable of tracing each learner's concept knowledge\nevolution over time, as well as analyzing the quality and content organization\nof learning resources, the question-concept associations, and the question\nintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparable\nor better performance in predicting unobserved learner responses than existing\ncollaborative filtering and knowledge tracing approaches for personalized\neducation. \n\n"}
{"id": "1312.6722", "contents": "Title: On the limiting behavior of parameter-dependent network centrality\n  measures Abstract: We consider a broad class of walk-based, parameterized node centrality\nmeasures for network analysis. These measures are expressed in terms of\nfunctions of the adjacency matrix and generalize various well-known centrality\nindices, including Katz and subgraph centrality. We show that the parameter can\nbe \"tuned\" to interpolate between degree and eigenvector centrality, which\nappear as limiting cases. Our analysis helps explain certain correlations often\nobserved between the rankings obtained using different centrality measures, and\nprovides some guidance for the tuning of parameters. We also highlight the\nroles played by the spectral gap of the adjacency matrix and by the number of\ntriangles in the network. Our analysis covers both undirected and directed\nnetworks, including weighted ones. A brief discussion of PageRank is also\ngiven. \n\n"}
{"id": "1312.7249", "contents": "Title: Maximum Coverage and Maximum Connected Covering in Social Networks with\n  Partial Topology Information Abstract: Viral marketing campaigns seek to recruit the most influential individuals to\ncover the largest target audience. This can be modeled as the well-studied\nmaximum coverage problem. There is a related problem when the recruited nodes\nare connected. It is called the maximum connected cover problem. This problem\nensures a strong coordination between the influential nodes which are the\nbackbone of the marketing campaign. In this work, we are interested on both of\nthese problems. Most of the related literature assumes knowledge about the\ntopology of the network. Even in that case, the problem is known to be NP-hard.\nIn this work, we propose heuristics to the maximum connected cover problem and\nthe maximum coverage problem with different knowledge levels about the topology\nof the network. We quantify the difference between these heuristics and the\nlocal and global greedy algorithms. \n\n"}
{"id": "1312.7308", "contents": "Title: lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits Abstract: The paper proposes a novel upper confidence bound (UCB) procedure for\nidentifying the arm with the largest mean in a multi-armed bandit game in the\nfixed confidence setting using a small number of total samples. The procedure\ncannot be improved in the sense that the number of samples required to identify\nthe best arm is within a constant factor of a lower bound based on the law of\nthe iterated logarithm (LIL). Inspired by the LIL, we construct our confidence\nbounds to explicitly account for the infinite time horizon of the algorithm. In\naddition, by using a novel stopping time for the algorithm we avoid a union\nbound over the arms that has been observed in other UCB-type algorithms. We\nprove that the algorithm is optimal up to constants and also show through\nsimulations that it provides superior performance with respect to the\nstate-of-the-art. \n\n"}
{"id": "1401.4676", "contents": "Title: Universal hierarchical behavior of citation networks Abstract: Many of the essential features of the evolution of scientific research are\nimprinted in the structure of citation networks. Connections in these networks\nimply information about the transfer of knowledge among papers, or in other\nwords, edges describe the impact of papers on other publications. This inherent\nmeaning of the edges infers that citation networks can exhibit hierarchical\nfeatures, that is typical of networks based on decision-making. In this paper,\nwe investigate the hierarchical structure of citation networks consisting of\npapers in the same field. We find that the majority of the networks follow a\nuniversal trend towards a highly hierarchical state, and i) the various fields\ndisplay differences only concerning their phase in life (distance from the\n\"birth\" of a field) or ii) the characteristic time according to which they are\napproaching the stationary state. We also show by a simple argument that the\nalterations in the behavior are related to and can be understood by the degree\nof specialization corresponding to the fields. Our results suggest that during\nthe accumulation of knowledge in a given field, some papers are gradually\nbecoming relatively more influential than most of the other papers. \n\n"}
{"id": "1401.5648", "contents": "Title: Random walk centrality for temporal networks Abstract: Nodes can be ranked according to their relative importance within the\nnetwork. Ranking algorithms based on random walks are particularly useful\nbecause they connect topological and diffusive properties of the network.\nPrevious methods based on random walks, as for example the PageRank, have\nfocused on static structures. However, several realistic networks are indeed\ndynamic, meaning that their structure changes in time. In this paper, we\npropose a centrality measure for temporal networks based on random walks which\nwe call TempoRank. While in a static network, the stationary density of the\nrandom walk is proportional to the degree or the strength of a node, we find\nthat in temporal networks, the stationary density is proportional to the\nin-strength of the so-called effective network. The stationary density also\ndepends on the sojourn probability q which regulates the tendency of the walker\nto stay in the node. We apply our method to human interaction networks and show\nthat although it is important for a node to be connected to another node with\nmany random walkers at the right moment (one of the principles of the\nPageRank), this effect is negligible in practice when the time order of link\nactivation is included. \n\n"}
{"id": "1401.7233", "contents": "Title: Measuring large-scale social networks with high resolution Abstract: This paper describes the deployment of a large-scale study designed to\nmeasure human interactions across a variety of communication channels, with\nhigh temporal resolution and spanning multiple years - the Copenhagen Networks\nStudy. Specifically, we collect data on face-to-face interactions,\ntelecommunication, social networks, location, and background information\n(personality, demographic, health, politics) for a densely connected population\nof 1,000 individuals, using state-of-art smartphones as social sensors. Here we\nprovide an overview of the related work and describe the motivation and\nresearch agenda driving the study. Additionally the paper details the\ndata-types measured, and the technical infrastructure in terms of both backend\nand phone software, as well as an outline of the deployment procedures. We\ndocument the participant privacy procedures and their underlying principles.\nThe paper is concluded with early results from data analysis, illustrating the\nimportance of multi-channel high-resolution approach to data collection. \n\n"}
{"id": "1401.7702", "contents": "Title: A Spectral Framework for Anomalous Subgraph Detection Abstract: A wide variety of application domains are concerned with data consisting of\nentities and their relationships or connections, formally represented as\ngraphs. Within these diverse application areas, a common problem of interest is\nthe detection of a subset of entities whose connectivity is anomalous with\nrespect to the rest of the data. While the detection of such anomalous\nsubgraphs has received a substantial amount of attention, no\napplication-agnostic framework exists for analysis of signal detectability in\ngraph-based data. In this paper, we describe a framework that enables such\nanalysis using the principal eigenspace of a graph's residuals matrix, commonly\ncalled the modularity matrix in community detection. Leveraging this analytical\ntool, we show that the framework has a natural power metric in the spectral\nnorm of the anomalous subgraph's adjacency matrix (signal power) and of the\nbackground graph's residuals matrix (noise power). We propose several\nalgorithms based on spectral properties of the residuals matrix, with more\ncomputationally expensive techniques providing greater detection power.\nDetection and identification performance are presented for a number of signal\nand noise models, including clusters and bipartite foregrounds embedded into\nsimple random backgrounds as well as graphs with community structure and\nrealistic degree distributions. The trends observed verify intuition gleaned\nfrom other signal processing areas, such as greater detection power when the\nsignal is embedded within a less active portion of the background. We\ndemonstrate the utility of the proposed techniques in detecting small, highly\nanomalous subgraphs in real graphs derived from Internet traffic and product\nco-purchases. \n\n"}
{"id": "1401.8042", "contents": "Title: Online Dating Recommendations: Matching Markets and Learning Preferences Abstract: Recommendation systems for online dating have recently attracted much\nattention from the research community. In this paper we proposed a two-side\nmatching framework for online dating recommendations and design an LDA model to\nlearn the user preferences from the observed user messaging behavior and user\nprofile features. Experimental results using data from a large online dating\nwebsite shows that two-sided matching improves significantly the rate of\nsuccessful matches by as much as 45%. Finally, using simulated matchings we\nshow that the the LDA model can correctly capture user preferences. \n\n"}
{"id": "1402.2594", "contents": "Title: Online Nonparametric Regression Abstract: We establish optimal rates for online regression for arbitrary classes of\nregression functions in terms of the sequential entropy introduced in (Rakhlin,\nSridharan, Tewari, 2010). The optimal rates are shown to exhibit a phase\ntransition analogous to the i.i.d./statistical learning case, studied in\n(Rakhlin, Sridharan, Tsybakov 2013). In the frequently encountered situation\nwhen sequential entropy and i.i.d. empirical entropy match, our results point\nto the interesting phenomenon that the rates for statistical learning with\nsquared loss and online nonparametric regression are the same.\n  In addition to a non-algorithmic study of minimax regret, we exhibit a\ngeneric forecaster that enjoys the established optimal rates. We also provide a\nrecipe for designing online regression algorithms that can be computationally\nefficient. We illustrate the techniques by deriving existing and new\nforecasters for the case of finite experts and for online linear regression. \n\n"}
{"id": "1402.2664", "contents": "Title: Network-Based Vertex Dissolution Abstract: We introduce a graph-theoretic vertex dissolution model that applies to a\nnumber of redistribution scenarios such as gerrymandering in political\ndistricting or work balancing in an online situation. The central aspect of our\nmodel is the deletion of certain vertices and the redistribution of their load\nto neighboring vertices in a completely balanced way.\n  We investigate how the underlying graph structure, the knowledge of which\nvertices should be deleted, and the relation between old and new vertex loads\ninfluence the computational complexity of the underlying graph problems. Our\nresults establish a clear borderline between tractable and intractable cases. \n\n"}
{"id": "1402.3032", "contents": "Title: Regularization for Multiple Kernel Learning via Sum-Product Networks Abstract: In this paper, we are interested in constructing general graph-based\nregularizers for multiple kernel learning (MKL) given a structure which is used\nto describe the way of combining basis kernels. Such structures are represented\nby sum-product networks (SPNs) in our method. Accordingly we propose a new\nconvex regularization method for MLK based on a path-dependent kernel weighting\nfunction which encodes the entire SPN structure in our method. Under certain\nconditions and from the view of probability, this function can be considered to\nfollow multinomial distributions over the weights associated with product nodes\nin SPNs. We also analyze the convexity of our regularizer and the complexity of\nour induced classifiers, and further propose an efficient wrapper algorithm to\noptimize our formulation. In our experiments, we apply our method to ...... \n\n"}
{"id": "1402.3364", "contents": "Title: Metric tree-like structures in real-life networks: an empirical study Abstract: Based on solid theoretical foundations, we present strong evidences that a\nnumber of real-life networks, taken from different domains like Internet\nmeasurements, biological data, web graphs, social and collaboration networks,\nexhibit tree-like structures from a metric point of view. We investigate few\ngraph parameters, namely, the tree-distortion and the tree-stretch, the\ntree-length and the tree-breadth, the Gromov's hyperbolicity, the\ncluster-diameter and the cluster-radius in a layering partition of a graph,\nwhich capture and quantify this phenomenon of being metrically close to a tree.\nBy bringing all those parameters together, we not only provide efficient means\nfor detecting such metric tree-like structures in large-scale networks but also\nshow how such structures can be used, for example, to efficiently and compactly\nencode approximate distance and almost shortest path information and to fast\nand accurately estimate diameters and radii of those networks. Estimating the\ndiameter and the radius of a graph or distances between its arbitrary vertices\nare fundamental primitives in many data and graph mining algorithms. \n\n"}
{"id": "1402.4325", "contents": "Title: Rich-cores in networks Abstract: A core is said to be a group of central and densely connected nodes which\ngoverns the overall behavior of a network. Profiling this meso--scale structure\ncurrently relies on a limited number of methods which are often complex, and\nhave scalability issues when dealing with very large networks. As a result, we\nare yet to fully understand its impact on network properties and dynamics. Here\nwe introduce a simple method to profile this structure by combining the\nconcepts of core/periphery and rich-club. The key challenge in addressing such\nassociation of the two concepts is to establish a way to define the membership\nof the core. The notion of a \"rich-club\" describes nodes which are essentially\nthe hub of a network, as they play a dominating role in structural and\nfunctional properties. Interestingly, the definition of a rich-club naturally\nemphasizes high degree nodes and divides a network into two subgroups. Our\napproach theoretically couples the underlying principle of a rich-club with the\nescape time of a random walker, and a rich-core is defined by examining changes\nin the associated persistence probability. The method is fast and scalable to\nlarge networks. In particular, we successfully show that the evolution of the\ncore in \\emph{C. elegans} and World Trade networks correspond to key\ndevelopment stages and responses to historical events respectively. \n\n"}
{"id": "1402.6573", "contents": "Title: A comparative analysis of the statistical properties of large mobile\n  phone calling networks Abstract: Mobile phone calling is one of the most widely used communication methods in\nmodern society. The records of calls among mobile phone users provide us a\nvaluable proxy for the understanding of human communication patterns embedded\nin social networks. Mobile phone users call each other forming a directed\ncalling network. If only reciprocal calls are considered, we obtain an\nundirected mutual calling network. The preferential communication behavior\nbetween two connected users can be statistically tested and it results in two\nBonferroni networks with statistically validated edges. We perform a\ncomparative analysis of the statistical properties of these four networks,\nwhich are constructed from the calling records of more than nine million\nindividuals in Shanghai over a period of 110 days. We find that these networks\nshare many common structural properties and also exhibit idiosyncratic features\nwhen compared with previously studied large mobile calling networks. The\nempirical findings provide us an intriguing picture of a representative large\nsocial network that might shed new lights on the modelling of large social\nnetworks. \n\n"}
{"id": "1403.1546", "contents": "Title: Measuring and modelling correlations in multiplex networks Abstract: The interactions among the elementary components of many complex systems can\nbe qualitatively different. Such systems are therefore naturally described in\nterms of multiplex or multi-layer networks, i.e. networks where each layer\nstands for a different type of interaction between the same set of nodes. There\nis today a growing interest in understanding when and why a description in\nterms of a multiplex network is necessary and more informative than a\nsingle-layer projection. Here, we contribute to this debate by presenting a\ncomprehensive study of correlations in multiplex networks. Correlations in node\nproperties, especially degree-degree correlations, have been thoroughly studied\nin single-layer networks. Here we extend this idea to investigate and\ncharacterize correlations between the different layers of a multiplex network.\nSuch correlations are intrinsically multiplex, and we first study them\nempirically by constructing and analyzing several multiplex networks from the\nreal-world. In particular, we introduce various measures to characterize\ncorrelations in the activity of the nodes and in their degree at the different\nlayers, and between activities and degrees. We show that real-world networks\nexhibit indeed non-trivial multiplex correlations. For instance, we find cases\nwhere two layers of the same multiplex network are positively correlated in\nterms of node degrees, while other two layers are negatively correlated. We\nthen focus on constructing synthetic multiplex networks, proposing a series of\nmodels to reproduce the correlations observed empirically and/or to assess\ntheir relevance. \n\n"}
{"id": "1403.3342", "contents": "Title: The Potential Benefits of Filtering Versus Hyper-Parameter Optimization Abstract: The quality of an induced model by a learning algorithm is dependent on the\nquality of the training data and the hyper-parameters supplied to the learning\nalgorithm. Prior work has shown that improving the quality of the training data\n(i.e., by removing low quality instances) or tuning the learning algorithm\nhyper-parameters can significantly improve the quality of an induced model. A\ncomparison of the two methods is lacking though. In this paper, we estimate and\ncompare the potential benefits of filtering and hyper-parameter optimization.\nEstimating the potential benefit gives an overly optimistic estimate but also\nempirically demonstrates an approximation of the maximum potential benefit of\neach method. We find that, while both significantly improve the induced model,\nimproving the quality of the training set has a greater potential effect than\nhyper-parameter optimization. \n\n"}
{"id": "1403.3909", "contents": "Title: Graph Sample and Hold: A Framework for Big-Graph Analytics Abstract: Sampling is a standard approach in big-graph analytics; the goal is to\nefficiently estimate the graph properties by consulting a sample of the whole\npopulation. A perfect sample is assumed to mirror every property of the whole\npopulation. Unfortunately, such a perfect sample is hard to collect in complex\npopulations such as graphs (e.g. web graphs, social networks etc), where an\nunderlying network connects the units of the population. Therefore, a good\nsample will be representative in the sense that graph properties of interest\ncan be estimated with a known degree of accuracy. While previous work focused\nparticularly on sampling schemes used to estimate certain graph properties\n(e.g. triangle count), much less is known for the case when we need to estimate\nvarious graph properties with the same sampling scheme. In this paper, we\npropose a generic stream sampling framework for big-graph analytics, called\nGraph Sample and Hold (gSH). To begin, the proposed framework samples from\nmassive graphs sequentially in a single pass, one edge at a time, while\nmaintaining a small state. We then show how to produce unbiased estimators for\nvarious graph properties from the sample. Given that the graph analysis\nalgorithms will run on a sample instead of the whole population, the runtime\ncomplexity of these algorithm is kept under control. Moreover, given that the\nestimators of graph properties are unbiased, the approximation error is kept\nunder control. Finally, we show the performance of the proposed framework (gSH)\non various types of graphs, such as social graphs, among others. \n\n"}
{"id": "1403.4640", "contents": "Title: Communication Communities in MOOCs Abstract: Massive Open Online Courses (MOOCs) bring together thousands of people from\ndifferent geographies and demographic backgrounds -- but to date, little is\nknown about how they learn or communicate. We introduce a new content-analysed\nMOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to\nextract communities of learners based on the nature of their online forum\nposts. We see that BNMF yields a superior probabilistic generative model for\nonline discussions when compared to other models, and that the communities it\nlearns are differentiated by their composite students' demographic and course\nperformance indicators. These findings suggest that computationally efficient\nprobabilistic generative modelling of MOOCs can reveal important insights for\neducational researchers and practitioners and help to develop more intelligent\nand responsive online learning environments. \n\n"}
{"id": "1403.5607", "contents": "Title: Bayesian Optimization with Unknown Constraints Abstract: Recent work on Bayesian optimization has shown its effectiveness in global\noptimization of difficult black-box objective functions. Many real-world\noptimization problems of interest also have constraints which are unknown a\npriori. In this paper, we study Bayesian optimization for constrained problems\nin the general case that noise may be present in the constraint functions, and\nthe objective and constraints may be evaluated independently. We provide\nmotivating practical examples, and present a general framework to solve such\nproblems. We demonstrate the effectiveness of our approach on optimizing the\nperformance of online latent Dirichlet allocation subject to topic sparsity\nconstraints, tuning a neural network given test-time memory constraints, and\noptimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed\ntime, subject to passing standard convergence diagnostics. \n\n"}
{"id": "1403.5617", "contents": "Title: On the Rise and Fall of Online Social Networks Abstract: The rise and fall of online social networks recently generated an enormous\namount of interest among people, both inside and outside of academia. Gillette\n[Businessweek magazine, 2011] did a detailed analysis of MySpace, which started\nlosing its popularity since 2008. Cannarella and Spechler [ArXiv, 2014] used a\nmodel of disease spread to explain the rise and fall of MySpace. In this paper,\nwe present a graph theoretical model that may be able to provide an alternative\nexplanation for the rise and fall of online social networks. Our model is\nmotivated by the well-known Barabasi-Albert model of generating random\nscale-free networks using preferential attachment or `rich-gets-richer'\nphenomenon. As shown by our empirical analysis, we conjecture that such an\nonline social network growth model is inherently flawed as it fails to maintain\nthe stability of such networks while ensuring their growth. In the process, we\nalso conjecture that our model of preferential attachment also exhibits\nscale-free phenomenon. \n\n"}
{"id": "1403.5787", "contents": "Title: Scalable detection of statistically significant communities and\n  hierarchies, using message-passing for modularity Abstract: Modularity is a popular measure of community structure. However, maximizing\nthe modularity can lead to many competing partitions, with almost the same\nmodularity, that are poorly correlated with each other. It can also produce\nillusory \"communities\" in random graphs where none exist. We address this\nproblem by using the modularity as a Hamiltonian at finite temperature, and\nusing an efficient Belief Propagation algorithm to obtain the consensus of many\npartitions with high modularity, rather than looking for a single partition\nthat maximizes it. We show analytically and numerically that the proposed\nalgorithm works all the way down to the detectability transition in networks\ngenerated by the stochastic block model. It also performs well on real-world\nnetworks, revealing large communities in some networks where previous work has\nclaimed no communities exist. Finally we show that by applying our algorithm\nrecursively, subdividing communities until no statistically-significant\nsubcommunities can be found, we can detect hierarchical structure in real-world\nnetworks more efficiently than previous methods. \n\n"}
{"id": "1403.7879", "contents": "Title: Triadic motifs in the dependence networks of virtual societies Abstract: In friendship networks, individuals have different numbers of friends, and\nthe closeness or intimacy between an individual and her friends is\nheterogeneous. Using a statistical filtering method to identify relationships\nabout who depends on whom, we construct dependence networks (which are\ndirected) from weighted friendship networks of avatars in more than two hundred\nvirtual societies of a massively multiplayer online role-playing game (MMORPG).\nWe investigate the evolution of triadic motifs in dependence networks. Several\nmetrics show that the virtual societies evolved through a transient stage in\nthe first two to three weeks and reached a relatively stable stage. We find\nthat the unidirectional loop motif (${\\rm{M}}_9$) is underrepresented and does\nnot appear, open motifs are also underrepresented, while other close motifs are\noverrepresented. We also find that, for most motifs, the overall level\ndifference of the three avatars in the same motif is significantly lower than\naverage, whereas the sum of ranks is only slightly larger than average. Our\nfindings show that avatars' social status plays an important role in the\nformation of triadic motifs. \n\n"}
{"id": "1404.0103", "contents": "Title: Comparative Resilience Notions and Vertex Attack Tolerance of Scale-Free\n  Networks Abstract: We are concerned with an appropriate mathematical measure of resilience in\nthe face of targeted node attacks for arbitrary degree networks, and\nsubsequently comparing the resilience of different scale-free network models\nwith the proposed measure. We strongly motivate our resilience measure termed\n\\emph{vertex attack tolerance} (VAT), which is denoted mathematically as\n$\\tau(G) = \\min_{S \\subset V} \\frac{|S|}{|V-S-C_{max}(V-S)|+1}$, where\n$C_{max}(V-S)$ is the largest connected component in $V-S$. We attempt a\nthorough comparison of VAT with several existing resilience notions:\nconductance, vertex expansion, integrity, toughness, tenacity and scattering\nnumber. Our comparisons indicate that for artbitrary degree distributions VAT\nis the only measure that fully captures both the major \\emph{bottlenecks} of a\nnetwork and the resulting \\emph{component size distribution} upon targeted node\nattacks (both captured in a manner proportional to the size of the attack set).\nFor the case of $d$-regular graphs, we prove that $\\tau(G) \\le d\\Phi(G)$, where\n$\\Phi(G)$ is the conductance of the graph $G$. Conductance and expansion are\nwell-studied measures of robustness and bottlenecks in the case of regular\ngraphs but fail to capture resilience in the case of highly heterogeneous\ndegree graphs. Regarding comparison of different scale-free graph models, our\nexperimental results indicate that PLOD graphs with degree distributions\nidentical to BA graphs of the same size exhibit consistently better vertex\nattack tolerance than the BA type graphs, although both graph types appear\nasymptotically resilient for BA generative parameter $m = 2$. BA graphs with $m\n= 1$ also appear to lack resilience, not only exhibiting very low VAT values,\nbut also great transparency in the identification of the vulnerable node sets,\nnamely the hubs, consistent with well known previous work. \n\n"}
{"id": "1404.0431", "contents": "Title: Learning Latent Block Structure in Weighted Networks Abstract: Community detection is an important task in network analysis, in which we aim\nto learn a network partition that groups together vertices with similar\ncommunity-level connectivity patterns. By finding such groups of vertices with\nsimilar structural roles, we extract a compact representation of the network's\nlarge-scale structure, which can facilitate its scientific interpretation and\nthe prediction of unknown or future interactions. Popular approaches, including\nthe stochastic block model, assume edges are unweighted, which limits their\nutility by throwing away potentially useful information. We introduce the\n`weighted stochastic block model' (WSBM), which generalizes the stochastic\nblock model to networks with edge weights drawn from any exponential family\ndistribution. This model learns from both the presence and weight of edges,\nallowing it to discover structure that would otherwise be hidden when weights\nare discarded or thresholded. We describe a Bayesian variational algorithm for\nefficiently approximating this model's posterior distribution over latent block\nstructures. We then evaluate the WSBM's performance on both edge-existence and\nedge-weight prediction tasks for a set of real-world weighted networks. In all\ncases, the WSBM performs as well or better than the best alternatives on these\ntasks. \n\n"}
{"id": "1404.1864", "contents": "Title: Sublinear algorithms for local graph centrality estimation Abstract: We study the complexity of local graph centrality estimation, with the goal\nof approximating the centrality score of a given target node while exploring\nonly a sublinear number of nodes/arcs of the graph and performing a sublinear\nnumber of elementary operations. We develop a technique, that we apply to the\nPageRank and Heat Kernel centralities, for building a low-variance score\nestimator through a local exploration of the graph. We obtain an algorithm\nthat, given any node in any graph of $m$ arcs, with probability $(1-\\delta)$\ncomputes a multiplicative $(1\\pm\\epsilon)$-approximation of its score by\nexamining only $\\tilde{O}(\\min(m^{2/3} \\Delta^{1/3} d^{-2/3},\\, m^{4/5}\nd^{-3/5}))$ nodes/arcs, where $\\Delta$ and $d$ are respectively the maximum and\naverage outdegree of the graph (omitting for readability\n$\\operatorname{poly}(\\epsilon^{-1})$ and $\\operatorname{polylog}(\\delta^{-1})$\nfactors). A similar bound holds for computational complexity. We also prove a\nlower bound of $\\Omega(\\min(m^{1/2} \\Delta^{1/2} d^{-1/2}, \\, m^{2/3}\nd^{-1/3}))$ for both query complexity and computational complexity. Moreover,\nour technique yields a $\\tilde{O}(n^{2/3})$ query complexity algorithm for the\ngraph access model of [Brautbar et al., 2010], widely used in social network\nmining; we show this algorithm is optimal up to a sublogarithmic factor. These\nare the first algorithms yielding worst-case sublinear bounds for general\ndirected graphs and any choice of the target node. \n\n"}
{"id": "1404.3181", "contents": "Title: FAST-PPR: Scaling Personalized PageRank Estimation for Large Graphs Abstract: We propose a new algorithm, FAST-PPR, for estimating personalized PageRank:\ngiven start node $s$ and target node $t$ in a directed graph, and given a\nthreshold $\\delta$, FAST-PPR estimates the Personalized PageRank $\\pi_s(t)$\nfrom $s$ to $t$, guaranteeing a small relative error as long $\\pi_s(t)>\\delta$.\nExisting algorithms for this problem have a running-time of $\\Omega(1/\\delta)$;\nin comparison, FAST-PPR has a provable average running-time guarantee of\n${O}(\\sqrt{d/\\delta})$ (where $d$ is the average in-degree of the graph). This\nis a significant improvement, since $\\delta$ is often $O(1/n)$ (where $n$ is\nthe number of nodes) for applications. We also complement the algorithm with an\n$\\Omega(1/\\sqrt{\\delta})$ lower bound for PageRank estimation, showing that the\ndependence on $\\delta$ cannot be improved.\n  We perform a detailed empirical study on numerous massive graphs, showing\nthat FAST-PPR dramatically outperforms existing algorithms. For example, on the\n2010 Twitter graph with 1.5 billion edges, for target nodes sampled by\npopularity, FAST-PPR has a $20$ factor speedup over the state of the art.\nFurthermore, an enhanced version of FAST-PPR has a $160$ factor speedup on the\nTwitter graph, and is at least $20$ times faster on all our candidate graphs. \n\n"}
{"id": "1404.5903", "contents": "Title: Most Correlated Arms Identification Abstract: We study the problem of finding the most mutually correlated arms among many\narms. We show that adaptive arms sampling strategies can have significant\nadvantages over the non-adaptive uniform sampling strategy. Our proposed\nalgorithms rely on a novel correlation estimator. The use of this accurate\nestimator allows us to get improved results for a wide range of problem\ninstances. \n\n"}
{"id": "1404.6702", "contents": "Title: A Constrained Matrix-Variate Gaussian Process for Transposable Data Abstract: Transposable data represents interactions among two sets of entities, and are\ntypically represented as a matrix containing the known interaction values.\nAdditional side information may consist of feature vectors specific to entities\ncorresponding to the rows and/or columns of such a matrix. Further information\nmay also be available in the form of interactions or hierarchies among entities\nalong the same mode (axis). We propose a novel approach for modeling\ntransposable data with missing interactions given additional side information.\nThe interactions are modeled as noisy observations from a latent noise free\nmatrix generated from a matrix-variate Gaussian process. The construction of\nrow and column covariances using side information provides a flexible mechanism\nfor specifying a-priori knowledge of the row and column correlations in the\ndata. Further, the use of such a prior combined with the side information\nenables predictions for new rows and columns not observed in the training data.\nIn this work, we combine the matrix-variate Gaussian process model with low\nrank constraints. The constrained Gaussian process approach is applied to the\nprediction of hidden associations between genes and diseases using a small set\nof observed associations as well as prior covariances induced by gene-gene\ninteraction networks and disease ontologies. The proposed approach is also\napplied to recommender systems data which involves predicting the item ratings\nof users using known associations as well as prior covariances induced by\nsocial networks. We present experimental results that highlight the performance\nof constrained matrix-variate Gaussian process as compared to state of the art\napproaches in each domain. \n\n"}
{"id": "1404.7789", "contents": "Title: Phase transitions in semisupervised clustering of sparse networks Abstract: Predicting labels of nodes in a network, such as community memberships or\ndemographic variables, is an important problem with applications in social and\nbiological networks. A recently-discovered phase transition puts fundamental\nlimits on the accuracy of these predictions if we have access only to the\nnetwork topology. However, if we know the correct labels of some fraction\n$\\alpha$ of the nodes, we can do better. We study the phase diagram of this\n\"semisupervised\" learning problem for networks generated by the stochastic\nblock model. We use the cavity method and the associated belief propagation\nalgorithm to study what accuracy can be achieved as a function of $\\alpha$. For\n$k = 2$ groups, we find that the detectability transition disappears for any\n$\\alpha > 0$, in agreement with previous work. For larger $k$ where a hard but\ndetectable regime exists, we find that the easy/hard transition (the point at\nwhich efficient algorithms can do better than chance) becomes a line of\ntransitions where the accuracy jumps discontinuously at a critical value of\n$\\alpha$. This line ends in a critical point with a second-order transition,\nbeyond which the accuracy is a continuous function of $\\alpha$. We demonstrate\nqualitatively similar transitions in two real-world networks. \n\n"}
{"id": "1405.0042", "contents": "Title: Learning with incremental iterative regularization Abstract: Within a statistical learning setting, we propose and study an iterative\nregularization algorithm for least squares defined by an incremental gradient\nmethod. In particular, we show that, if all other parameters are fixed a\npriori, the number of passes over the data (epochs) acts as a regularization\nparameter, and prove strong universal consistency, i.e. almost sure convergence\nof the risk, as well as sharp finite sample bounds for the iterates. Our\nresults are a step towards understanding the effect of multiple epochs in\nstochastic gradient techniques in machine learning and rely on integrating\nstatistical and optimization results. \n\n"}
{"id": "1405.0425", "contents": "Title: Layer aggregation and reducibility of multilayer interconnected networks Abstract: Many complex systems can be represented as networks composed by distinct\nlayers, interacting and depending on each others. For example, in biology, a\ngood description of the full protein-protein interactome requires, for some\norganisms, up to seven distinct network layers, with thousands of\nprotein-protein interactions each. A fundamental open question is then how much\ninformation is really necessary to accurately represent the structure of a\nmultilayer complex system, and if and when some of the layers can indeed be\naggregated. Here we introduce a method, based on information theory, to reduce\nthe number of layers in multilayer networks, while minimizing information loss.\nWe validate our approach on a set of synthetic benchmarks, and prove its\napplicability to an extended data set of protein-genetic interactions, showing\ncases where a strong reduction is possible and cases where it is not. Using\nthis method we can describe complex systems with an optimal trade--off between\naccuracy and complexity. \n\n"}
{"id": "1405.0483", "contents": "Title: Percolation on sparse networks Abstract: We study percolation on networks, which is used as a model of the resilience\nof networked systems such as the Internet to attack or failure and as a simple\nmodel of the spread of disease over human contact networks. We reformulate\npercolation as a message passing process and demonstrate how the resulting\nequations can be used to calculate, among other things, the size of the\npercolating cluster and the average cluster size. The calculations are exact\nfor sparse networks when the number of short loops in the network is small, but\neven on networks with many short loops we find them to be highly accurate when\ncompared with direct numerical simulations. By considering the fixed points of\nthe message passing process, we also show that the percolation threshold on a\nnetwork with few loops is given by the inverse of the leading eigenvalue of the\nso-called non-backtracking matrix. \n\n"}
{"id": "1405.1477", "contents": "Title: A Novel Approach to Finding Near-Cliques: The Triangle-Densest Subgraph\n  Problem Abstract: Many graph mining applications rely on detecting subgraphs which are\nnear-cliques. There exists a dichotomy between the results in the existing work\nrelated to this problem: on the one hand the densest subgraph problem (DSP)\nwhich maximizes the average degree over all subgraphs is solvable in polynomial\ntime but for many networks fails to find subgraphs which are near-cliques. On\nthe other hand, formulations that are geared towards finding near-cliques are\nNP-hard and frequently inapproximable due to connections with the Maximum\nClique problem.\n  In this work, we propose a formulation which combines the best of both\nworlds: it is solvable in polynomial time and finds near-cliques when the DSP\nfails. Surprisingly, our formulation is a simple variation of the DSP.\nSpecifically, we define the triangle densest subgraph problem (TDSP): given\n$G(V,E)$, find a subset of vertices $S^*$ such that $\\tau(S^*)=\\max_{S\n\\subseteq V} \\frac{t(S)}{|S|}$, where $t(S)$ is the number of triangles induced\nby the set $S$. We provide various exact and approximation algorithms which the\nsolve the TDSP efficiently. Furthermore, we show how our algorithms adapt to\nthe more general problem of maximizing the $k$-clique average density. Finally,\nwe provide empirical evidence that the TDSP should be used whenever the output\nof the DSP fails to output a near-clique. \n\n"}
{"id": "1405.2690", "contents": "Title: Policy Gradients for CVaR-Constrained MDPs Abstract: We study a risk-constrained version of the stochastic shortest path (SSP)\nproblem, where the risk measure considered is Conditional Value-at-Risk (CVaR).\nWe propose two algorithms that obtain a locally risk-optimal policy by\nemploying four tools: stochastic approximation, mini batches, policy gradients\nand importance sampling. Both the algorithms incorporate a CVaR estimation\nprocedure, along the lines of Bardou et al. [2009], which in turn is based on\nRockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio\nprinciple for estimating the gradient of the sum of one cost function\n(objective of the SSP) and the gradient of the CVaR of the sum of another cost\nfunction (in the constraint of SSP). The algorithms differ in the manner in\nwhich they approximate the CVaR estimates/necessary gradients - the first\nalgorithm uses stochastic approximation, while the second employ mini-batches\nin the spirit of Monte Carlo methods. We establish asymptotic convergence of\nboth the algorithms. Further, since estimating CVaR is related to rare-event\nsimulation, we incorporate an importance sampling based variance reduction\nscheme into our proposed algorithms. \n\n"}
{"id": "1405.3080", "contents": "Title: Accelerating Minibatch Stochastic Gradient Descent using Stratified\n  Sampling Abstract: Stochastic Gradient Descent (SGD) is a popular optimization method which has\nbeen applied to many important machine learning tasks such as Support Vector\nMachines and Deep Neural Networks. In order to parallelize SGD, minibatch\ntraining is often employed. The standard approach is to uniformly sample a\nminibatch at each step, which often leads to high variance. In this paper we\npropose a stratified sampling strategy, which divides the whole dataset into\nclusters with low within-cluster variance; we then take examples from these\nclusters using a stratified sampling technique. It is shown that the\nconvergence rate can be significantly improved by the algorithm. Encouraging\nexperimental results confirm the effectiveness of the proposed method. \n\n"}
{"id": "1405.6077", "contents": "Title: Move ordering and communities in complex networks describing the game of\n  go Abstract: We analyze the game of go from the point of view of complex networks. We\nconstruct three different directed networks of increasing complexity, defining\nnodes as local patterns on plaquettes of increasing sizes, and links as actual\nsuccessions of these patterns in databases of real games. We discuss the\npeculiarities of these networks compared to other types of networks. We explore\nthe ranking vectors and community structure of the networks and show that this\napproach enables to extract groups of moves with common strategic properties.\nWe also investigate different networks built from games with players of\ndifferent levels or from different phases of the game. We discuss how the study\nof the community structure of these networks may help to improve the computer\nsimulations of the game. More generally, we believe such studies may help to\nimprove the understanding of human decision process. \n\n"}
{"id": "1405.7183", "contents": "Title: Interactions of cultures and top people of Wikipedia from ranking of 24\n  language editions Abstract: Wikipedia is a huge global repository of human knowledge, that can be\nleveraged to investigate interwinements between cultures. With this aim, we\napply methods of Markov chains and Google matrix, for the analysis of the\nhyperlink networks of 24 Wikipedia language editions, and rank all their\narticles by PageRank, 2DRank and CheiRank algorithms. Using automatic\nextraction of people names, we obtain the top 100 historical figures, for each\nedition and for each algorithm. We investigate their spatial, temporal, and\ngender distributions in dependence of their cultural origins. Our study\ndemonstrates not only the existence of skewness with local figures, mainly\nrecognized only in their own cultures, but also the existence of global\nhistorical figures appearing in a large number of editions. By determining the\nbirth time and place of these persons, we perform an analysis of the evolution\nof such figures through 35 centuries of human history for each language, thus\nrecovering interactions and entanglement of cultures over time. We also obtain\nthe distributions of historical figures over world countries, highlighting\ngeographical aspects of cross-cultural links. Considering historical figures\nwho appear in multiple editions as interactions between cultures, we construct\na network of cultures and identify the most influential cultures according to\nthis network. \n\n"}
{"id": "1405.7631", "contents": "Title: Diffusion-Aware Sampling and Estimation in Information Diffusion\n  Networks Abstract: Partially-observed data collected by sampling methods is often being studied\nto obtain the characteristics of information diffusion networks. However, these\nmethods usually do not consider the behavior of diffusion process. In this\npaper, we propose a novel two-step (sampling/estimation) measurement framework\nby utilizing the diffusion process characteristics. To this end, we propose a\nlink-tracing based sampling design which uses the infection times as local\ninformation without any knowledge about the latent structure of diffusion\nnetwork. To correct the bias of sampled data, we introduce three estimators for\ndifferent categories; link-based, node-based, and cascade-based. To the best of\nour knowledge, this is the first attempt to introduce a complete measurement\nframework for diffusion networks. We also show that the estimator plays an\nimportant role in correcting the bias of sampling from diffusion networks. Our\ncomprehensive empirical analysis over large synthetic and real datasets\ndemonstrates that in average, the proposed framework outperforms the common BFS\nand RW sampling methods in terms of link-based characteristics by about 37% and\n35%, respectively. \n\n"}
{"id": "1406.0189", "contents": "Title: Convex Total Least Squares Abstract: We study the total least squares (TLS) problem that generalizes least squares\nregression by allowing measurement errors in both dependent and independent\nvariables. TLS is widely used in applied fields including computer vision,\nsystem identification and econometrics. The special case when all dependent and\nindependent variables have the same level of uncorrelated Gaussian noise, known\nas ordinary TLS, can be solved by singular value decomposition (SVD). However,\nSVD cannot solve many important practical TLS problems with realistic noise\nstructure, such as having varying measurement noise, known structure on the\nerrors, or large outliers requiring robust error-norms. To solve such problems,\nwe develop convex relaxation approaches for a general class of structured TLS\n(STLS). We show both theoretically and experimentally, that while the plain\nnuclear norm relaxation incurs large approximation errors for STLS, the\nre-weighted nuclear norm approach is very effective, and achieves better\naccuracy on challenging STLS problems than popular non-convex solvers. We\ndescribe a fast solution based on augmented Lagrangian formulation, and apply\nour approach to an important class of biological problems that use population\naverage measurements to infer cell-type and physiological-state specific\nexpression levels that are very hard to measure directly. \n\n"}
{"id": "1406.2082", "contents": "Title: Fast and Flexible ADMM Algorithms for Trend Filtering Abstract: This paper presents a fast and robust algorithm for trend filtering, a\nrecently developed nonparametric regression tool. It has been shown that, for\nestimating functions whose derivatives are of bounded variation, trend\nfiltering achieves the minimax optimal error rate, while other popular methods\nlike smoothing splines and kernels do not. Standing in the way of a more\nwidespread practical adoption, however, is a lack of scalable and numerically\nstable algorithms for fitting trend filtering estimates. This paper presents a\nhighly efficient, specialized ADMM routine for trend filtering. Our algorithm\nis competitive with the specialized interior point methods that are currently\nin use, and yet is far more numerically robust. Furthermore, the proposed ADMM\nimplementation is very simple, and importantly, it is flexible enough to extend\nto many interesting related problems, such as sparse trend filtering and\nisotonic trend filtering. Software for our method is freely available, in both\nthe C and R languages. \n\n"}
{"id": "1406.2392", "contents": "Title: Inferring the geographic focus of online documents from social media\n  sharing patterns Abstract: Determining the geographic focus of digital media is an essential first step\nfor modern geographic information retrieval. However, publicly-visible location\nannotations are remarkably sparse in online data. In this work, we demonstrate\na method which infers the geographic focus of an online document by examining\nthe locations of Twitter users who share links to the document.\n  We apply our geotagging technique to multiple datasets built from different\ncontent: manually-annotated news articles, GDELT, YouTube, Flickr, Twitter, and\nTumblr. \n\n"}
{"id": "1406.2587", "contents": "Title: Structural Sparsity of Complex Networks: Bounded Expansion in Random\n  Models and Real-World Graphs Abstract: This research establishes that many real-world networks exhibit bounded\nexpansion, a strong notion of structural sparsity, and demonstrates that it can\nbe leveraged to design efficient algorithms for network analysis. We analyze\nseveral common network models regarding their structural sparsity. We show\nthat, with high probability, (1) graphs sampled with a prescribed s parse\ndegree sequence; (2) perturbed bounded-degree graphs; (3) stochastic block\nmodels with small probabilities; result in graphs of bounded expansion.\n  In contrast, we show that the Kleinberg and the Barabasi-Albert model have\nunbounded expansion. We support our findings with empirical measurements on a\ncorpus of real-world networks. \n\n"}
{"id": "1406.4445", "contents": "Title: RAPID: Rapidly Accelerated Proximal Gradient Algorithms for Convex\n  Minimization Abstract: In this paper, we propose a new algorithm to speed-up the convergence of\naccelerated proximal gradient (APG) methods. In order to minimize a convex\nfunction $f(\\mathbf{x})$, our algorithm introduces a simple line search step\nafter each proximal gradient step in APG so that a biconvex function\n$f(\\theta\\mathbf{x})$ is minimized over scalar variable $\\theta>0$ while fixing\nvariable $\\mathbf{x}$. We propose two new ways of constructing the auxiliary\nvariables in APG based on the intermediate solutions of the proximal gradient\nand the line search steps. We prove that at arbitrary iteration step $t\n(t\\geq1)$, our algorithm can achieve a smaller upper-bound for the gap between\nthe current and optimal objective values than those in the traditional APG\nmethods such as FISTA, making it converge faster in practice. In fact, our\nalgorithm can be potentially applied to many important convex optimization\nproblems, such as sparse linear regression and kernel SVMs. Our experimental\nresults clearly demonstrate that our algorithm converges faster than APG in all\nof the applications above, even comparable to some sophisticated solvers. \n\n"}
{"id": "1406.5647", "contents": "Title: On semidefinite relaxations for the block model Abstract: The stochastic block model (SBM) is a popular tool for community detection in\nnetworks, but fitting it by maximum likelihood (MLE) involves a computationally\ninfeasible optimization problem. We propose a new semidefinite programming\n(SDP) solution to the problem of fitting the SBM, derived as a relaxation of\nthe MLE. We put ours and previously proposed SDPs in a unified framework, as\nrelaxations of the MLE over various sub-classes of the SBM, revealing a\nconnection to sparse PCA. Our main relaxation, which we call SDP-1, is tighter\nthan other recently proposed SDP relaxations, and thus previously established\ntheoretical guarantees carry over. However, we show that SDP-1 exactly recovers\ntrue communities over a wider class of SBMs than those covered by current\nresults. In particular, the assumption of strong assortativity of the SBM,\nimplicit in consistency conditions for previously proposed SDPs, can be relaxed\nto weak assortativity for our approach, thus significantly broadening the class\nof SBMs covered by the consistency results. We also show that strong\nassortativity is indeed a necessary condition for exact recovery for previously\nproposed SDP approaches and not an artifact of the proofs. Our analysis of SDPs\nis based on primal-dual witness constructions, which provides some insight into\nthe nature of the solutions of various SDPs. We show how to combine features\nfrom SDP-1 and already available SDPs to achieve the most flexibility in terms\nof both assortativity and block-size constraints, as our relaxation has the\ntendency to produce communities of similar sizes. This tendency makes it the\nideal tool for fitting network histograms, a method gaining popularity in the\ngraphon estimation literature, as we illustrate on an example of a social\nnetworks of dolphins. We also provide empirical evidence that SDPs outperform\nspectral methods for fitting SBMs with a large number of blocks. \n\n"}
{"id": "1406.5736", "contents": "Title: Convex Optimization Learning of Faithful Euclidean Distance\n  Representations in Nonlinear Dimensionality Reduction Abstract: Classical multidimensional scaling only works well when the noisy distances\nobserved in a high dimensional space can be faithfully represented by Euclidean\ndistances in a low dimensional space. Advanced models such as Maximum Variance\nUnfolding (MVU) and Minimum Volume Embedding (MVE) use Semi-Definite\nProgramming (SDP) to reconstruct such faithful representations. While those SDP\nmodels are capable of producing high quality configuration numerically, they\nsuffer two major drawbacks. One is that there exist no theoretically guaranteed\nbounds on the quality of the configuration. The other is that they are slow in\ncomputation when the data points are beyond moderate size. In this paper, we\npropose a convex optimization model of Euclidean distance matrices. We\nestablish a non-asymptotic error bound for the random graph model with\nsub-Gaussian noise, and prove that our model produces a matrix estimator of\nhigh accuracy when the order of the uniform sample size is roughly the degree\nof freedom of a low-rank matrix up to a logarithmic factor. Our results\npartially explain why MVU and MVE often work well. Moreover, we develop a fast\ninexact accelerated proximal gradient method. Numerical experiments show that\nthe model can produce configurations of high quality on large data points that\nthe SDP approach would struggle to cope with. \n\n"}
{"id": "1407.0375", "contents": "Title: Mathematical and Algorithmic Analysis of Network and Biological Data Abstract: This dissertation contributes to mathematical and algorithmic problems that\narise in the analysis of network and biological data. \n\n"}
{"id": "1407.1255", "contents": "Title: Dynamic message-passing equations for models with unidirectional\n  dynamics Abstract: Understanding and quantifying the dynamics of disordered out-of-equilibrium\nmodels is an important problem in many branches of science. Using the dynamic\ncavity method on time trajectories, we construct a general procedure for\nderiving the dynamic message-passing equations for a large class of models with\nunidirectional dynamics, which includes the zero-temperature random field Ising\nmodel, the susceptible-infected-recovered model, and rumor spreading models. We\nshow that unidirectionality of the dynamics is the key ingredient that makes\nthe problem solvable. These equations are applicable to single instances of the\ncorresponding problems with arbitrary initial conditions, and are\nasymptotically exact for problems defined on locally tree-like graphs. When\napplied to real-world networks, they generically provide a good analytic\napproximation of the real dynamics. \n\n"}
{"id": "1407.1425", "contents": "Title: On the relationship between Gaussian stochastic blockmodels and label\n  propagation algorithms Abstract: The problem of community detection receives great attention in recent years.\nMany methods have been proposed to discover communities in networks. In this\npaper, we propose a Gaussian stochastic blockmodel that uses Gaussian\ndistributions to fit weight of edges in networks for non-overlapping community\ndetection. The maximum likelihood estimation of this model has the same\nobjective function as general label propagation with node preference. The node\npreference of a specific vertex turns out to be a value proportional to the\nintra-community eigenvector centrality (the corresponding entry in principal\neigenvector of the adjacency matrix of the subgraph inside that vertex's\ncommunity) under maximum likelihood estimation. Additionally, the maximum\nlikelihood estimation of a constrained version of our model is highly related\nto another extension of label propagation algorithm, namely, the label\npropagation algorithm under constraint. Experiments show that the proposed\nGaussian stochastic blockmodel performs well on various benchmark networks. \n\n"}
{"id": "1407.1591", "contents": "Title: Consistency Thresholds for the Planted Bisection Model Abstract: The planted bisection model is a random graph model in which the nodes are\ndivided into two equal-sized communities and then edges are added randomly in a\nway that depends on the community membership. We establish necessary and\nsufficient conditions for the asymptotic recoverability of the planted\nbisection in this model. When the bisection is asymptotically recoverable, we\ngive an efficient algorithm that successfully recovers it. We also show that\nthe planted bisection is recoverable asymptotically if and only if with high\nprobability every node belongs to the same community as the majority of its\nneighbors.\n  Our algorithm for finding the planted bisection runs in time almost linear in\nthe number of edges. It has three stages: spectral clustering to compute an\ninitial guess, a \"replica\" stage to get almost every vertex correct, and then\nsome simple local moves to finish the job. An independent work by Abbe,\nBandeira, and Hall establishes similar (slightly weaker) results but only in\nthe case of logarithmic average degree. \n\n"}
{"id": "1407.4990", "contents": "Title: Detecting network communities beyond assortativity-related attributes Abstract: In network science, assortativity refers to the tendency of links to exist\nbetween nodes with similar attributes. In social networks, for example, links\ntend to exist between individuals of similar age, nationality, location, race,\nincome, educational level, religious belief, and language. Thus, various\nattributes jointly affect the network topology. An interesting problem is to\ndetect community structure beyond some specific assortativity-related\nattributes $\\rho$, i.e., to take out the effect of $\\rho$ on network topology\nand reveal the hidden community structure which are due to other attributes. An\napproach to this problem is to redefine the null model of the modularity\nmeasure, so as to simulate the effect of $\\rho$ on network topology. However, a\nchallenge is that we do not know to what extent the network topology is\naffected by $\\rho$ and by other attributes. In this paper, we propose\nDist-Modularity which allows us to freely choose any suitable function to\nsimulate the effect of $\\rho$. Such freedom can help us probe the effect of\n$\\rho$ and detect the hidden communities which are due to other attributes. We\ntest the effectiveness of Dist-Modularity on synthetic benchmarks and two\nreal-world networks. \n\n"}
{"id": "1407.6823", "contents": "Title: Measuring Prestige in Online Social Networks Abstract: We study the locally-defined social capital metric of Palasek (2013) for\ndetermining individuals' prestige within an online social network. From it we\nderive an equivalent global measure by considering random walks over the\nnetwork itself. This result inspires a novel expression quantifying the\nstrategic desirability of a potential social connection. We show in silico that\nideal social neighbors tend to satisfy a \"big fish in a small pond\" criterion\nand that the distribution of neighbor-desirability throughout a network is\ngoverned by anti-homophily. \n\n"}
{"id": "1407.7061", "contents": "Title: A Parallel Branch and Bound Algorithm for the Maximum Labelled Clique\n  Problem Abstract: The maximum labelled clique problem is a variant of the maximum clique\nproblem where edges in the graph are given labels, and we are not allowed to\nuse more than a certain number of distinct labels in a solution. We introduce a\nnew branch-and-bound algorithm for the problem, and explain how it may be\nparallelised. We evaluate an implementation on a set of benchmark instances,\nand show that it is consistently faster than previously published results,\nsometimes by four or five orders of magnitude. \n\n"}
{"id": "1408.2701", "contents": "Title: Epidemic processes in complex networks Abstract: In recent years the research community has accumulated overwhelming evidence\nfor the emergence of complex and heterogeneous connectivity patterns in a wide\nrange of biological and sociotechnical systems. The complex properties of\nreal-world networks have a profound impact on the behavior of equilibrium and\nnonequilibrium phenomena occurring in various systems, and the study of\nepidemic spreading is central to our understanding of the unfolding of\ndynamical processes in complex networks. The theoretical analysis of epidemic\nspreading in heterogeneous networks requires the development of novel\nanalytical frameworks, and it has produced results of conceptual and practical\nrelevance. A coherent and comprehensive review of the vast research activity\nconcerning epidemic processes is presented, detailing the successful\ntheoretical approaches as well as making their limits and assumptions clear.\nPhysicists, mathematicians, epidemiologists, computer, and social scientists\nshare a common interest in studying epidemic spreading and rely on similar\nmodels for the description of the diffusion of pathogens, knowledge, and\ninnovation. For this reason, while focusing on the main results and the\nparadigmatic models in infectious disease modeling, the major results\nconcerning generalized social contagion processes are also presented. Finally,\nthe research activity at the forefront in the study of epidemic spreading in\ncoevolving, coupled, and time-varying networks is reported. \n\n"}
{"id": "1408.2925", "contents": "Title: Identifying modular flows on multilayer networks reveals highly\n  overlapping organization in social systems Abstract: Unveiling the community structure of networks is a powerful methodology to\ncomprehend interconnected systems across the social and natural sciences. To\nidentify different types of functional modules in interaction data aggregated\nin a single network layer, researchers have developed many powerful methods.\nFor example, flow-based methods have proven useful for identifying modular\ndynamics in weighted and directed networks that capture constraints on flow in\nthe systems they represent. However, many networked systems consist of agents\nor components that exhibit multiple layers of interactions. Inevitably,\nrepresenting this intricate network of networks as a single aggregated network\nleads to information loss and may obscure the actual organization. Here we\npropose a method based on compression of network flows that can identify\nmodular flows in non-aggregated multilayer networks. Our numerical experiments\non synthetic networks show that the method can accurately identify modules that\ncannot be identified in aggregated networks or by analyzing the layers\nseparately. We capitalize on our findings and reveal the community structure of\ntwo multilayer collaboration networks: scientists affiliated to the Pierre\nAuger Observatory and scientists publishing works on networks on the arXiv.\nCompared to conventional aggregated methods, the multilayer method reveals\nsmaller modules with more overlap that better capture the actual organization. \n\n"}
{"id": "1408.3092", "contents": "Title: Convergence rate of Bayesian tensor estimator: Optimal rate without\n  restricted strong convexity Abstract: In this paper, we investigate the statistical convergence rate of a Bayesian\nlow-rank tensor estimator. Our problem setting is the regression problem where\na tensor structure underlying the data is estimated. This problem setting\noccurs in many practical applications, such as collaborative filtering,\nmulti-task learning, and spatio-temporal data analysis. The convergence rate is\nanalyzed in terms of both in-sample and out-of-sample predictive accuracies. It\nis shown that a near optimal rate is achieved without any strong convexity of\nthe observation. Moreover, we show that the method has adaptivity to the\nunknown rank of the true tensor, that is, the near optimal rate depending on\nthe true rank is achieved even if it is not known a priori. \n\n"}
{"id": "1408.4102", "contents": "Title: Estimation of Monotone Treatment Effects in Network Experiments Abstract: Randomized experiments on social networks pose statistical challenges, due to\nthe possibility of interference between units. We propose new methods for\nestimating attributable treatment effects in such settings. The methods do not\nrequire partial interference, but instead require an identifying assumption\nthat is similar to requiring nonnegative treatment effects. Network or spatial\ninformation can be used to customize the test statistic; in principle, this can\nincrease power without making assumptions on the data generating process. \n\n"}
{"id": "1408.5352", "contents": "Title: Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in\n  Polynomial Time Abstract: Sparse principal component analysis (PCA) involves nonconvex optimization for\nwhich the global solution is hard to obtain. To address this issue, one popular\napproach is convex relaxation. However, such an approach may produce suboptimal\nestimators due to the relaxation effect. To optimally estimate sparse principal\nsubspaces, we propose a two-stage computational framework named \"tighten after\nrelax\": Within the 'relax' stage, we approximately solve a convex relaxation of\nsparse PCA with early stopping to obtain a desired initial estimator; For the\n'tighten' stage, we propose a novel algorithm called sparse orthogonal\niteration pursuit (SOAP), which iteratively refines the initial estimator by\ndirectly solving the underlying nonconvex problem. A key concept of this\ntwo-stage framework is the basin of attraction. It represents a local region\nwithin which the `tighten' stage has desired computational and statistical\nguarantees. We prove that, the initial estimator obtained from the 'relax'\nstage falls into such a region, and hence SOAP geometrically converges to a\nprincipal subspace estimator which is minimax-optimal within a certain model\nclass. Unlike most existing sparse PCA estimators, our approach applies to the\nnon-spiked covariance models, and adapts to non-Gaussianity as well as\ndependent data settings. Moreover, through analyzing the computational\ncomplexity of the two stages, we illustrate an interesting phenomenon that\nlarger sample size can reduce the total iteration complexity. Our framework\nmotivates a general paradigm for solving many complex statistical problems\nwhich involve nonconvex optimization with provable guarantees. \n\n"}
{"id": "1409.0308", "contents": "Title: Searching for a Unique Style in Soccer Abstract: Is it possible to have a unique, recognizable style in soccer nowadays? We\naddress this question by proposing a method to quantify the motif\ncharacteristics of soccer teams based on their pass networks. We introduce the\nthe concept of \"flow motifs\" to characterize the statistically significant pass\nsequence patterns. It extends the idea of the network motifs, highly\nsignificant subgraphs that usually consists of three or four nodes. The\nanalysis of the motifs in the pass networks allows us to compare and\ndifferentiate the styles of different teams. Although most teams tend to apply\nhomogenous style, surprisingly, a unique strategy of soccer exists.\nSpecifically, FC Barcelona's famous tiki-taka does not consist of uncountable\nrandom passes but rather has a precise, finely constructed structure. \n\n"}
{"id": "1409.0428", "contents": "Title: Google matrix analysis of directed networks Abstract: In past ten years, modern societies developed enormous communication and\nsocial networks. Their classification and information retrieval processing\nbecome a formidable task for the society. Due to the rapid growth of World Wide\nWeb, social and communication networks, new mathematical methods have been\ninvented to characterize the properties of these networks on a more detailed\nand precise level. Various search engines are essentially using such methods.\nIt is highly important to develop new tools to classify and rank enormous\namount of network information in a way adapted to internal network structures\nand characteristics. This review describes the Google matrix analysis of\ndirected complex networks demonstrating its efficiency on various examples\nincluding World Wide Web, Wikipedia, software architecture, world trade, social\nand citation networks, brain neural networks, DNA sequences and Ulam networks.\nThe analytical and numerical matrix methods used in this analysis originate\nfrom the fields of Markov chains, quantum chaos and Random Matrix theory. \n\n"}
{"id": "1409.3207", "contents": "Title: Phase Transitions in Spectral Community Detection Abstract: Consider a network consisting of two subnetworks (communities) connected by\nsome external edges. Given the network topology, the community detection\nproblem can be cast as a graph partitioning problem that aims to identify the\nexternal edges as the graph cut that separates these two subnetworks. In this\npaper, we consider a general model where two arbitrarily connected subnetworks\nare connected by random external edges. Using random matrix theory and\nconcentration inequalities, we show that when one performs community detection\nvia spectral clustering there exists an abrupt phase transition as a function\nof the random external edge connection probability. Specifically, the community\ndetection performance transitions from almost perfect detectability to low\ndetectability near some critical value of the random external edge connection\nprobability. We derive upper and lower bounds on the critical value and show\nthat the bounds are equal to each other when two subnetwork sizes are\nidentical. Using simulated and experimental data we show how these bounds can\nbe empirically estimated to validate the detection reliability of any\ndiscovered communities. \n\n"}
{"id": "1409.7591", "contents": "Title: Topic Similarity Networks: Visual Analytics for Large Document Sets Abstract: We investigate ways in which to improve the interpretability of LDA topic\nmodels by better analyzing and visualizing their outputs. We focus on examining\nwhat we refer to as topic similarity networks: graphs in which nodes represent\nlatent topics in text collections and links represent similarity among topics.\nWe describe efficient and effective approaches to both building and labeling\nsuch networks. Visualizations of topic models based on these networks are shown\nto be a powerful means of exploring, characterizing, and summarizing large\ncollections of unstructured text documents. They help to \"tease out\"\nnon-obvious connections among different sets of documents and provide insights\ninto how topics form larger themes. We demonstrate the efficacy and\npracticality of these approaches through two case studies: 1) NSF grants for\nbasic research spanning a 14 year period and 2) the entire English portion of\nWikipedia. \n\n"}
{"id": "1409.8268", "contents": "Title: Slow poisoning and destruction of networks: Edge proximity and its\n  implications for biological and infrastructure networks Abstract: We propose a network metric, edge proximity, ${\\cal P}_e$, which demonstrates\nthe importance of specific edges in a network, hitherto not captured by\nexisting network metrics. The effects of removing edges with high ${\\cal P}_e$\nmight initially seem inconspicuous but are eventually shown to be very harmful\nfor networks. Compared to existing strategies, the removal of edges by ${\\cal\nP}_e$ leads to a remarkable increase in the diameter and average shortest path\nlength in undirected real and random networks till the first disconnection and\nwell beyond. ${\\cal P}_e$ can be consistently used to rupture the network into\ntwo nearly equal parts, thus presenting a very potent strategy to greatly harm\na network. Targeting by ${\\cal P}_e$ causes notable efficiency loss in U.S. and\nEuropean power grid networks. ${\\cal P}_e$ identifies proteins with essential\ncellular functions in protein-protein interaction networks. It pinpoints\nregulatory neural connections and important portions of the neural and brain\nnetworks, respectively. Energy flow interactions identified by ${\\cal P}_e$\nform the backbone of long food web chains. Finally, we scrutinize the potential\nof ${\\cal P}_e$ in edge controllability dynamics of directed networks. \n\n"}
{"id": "1410.0342", "contents": "Title: Generalized Low Rank Models Abstract: Principal components analysis (PCA) is a well-known technique for\napproximating a tabular data set by a low rank matrix. Here, we extend the idea\nof PCA to handle arbitrary data sets consisting of numerical, Boolean,\ncategorical, ordinal, and other data types. This framework encompasses many\nwell known techniques in data analysis, such as nonnegative matrix\nfactorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD,\nand maximum margin matrix factorization. The method handles heterogeneous data\nsets, and leads to coherent schemes for compressing, denoising, and imputing\nmissing entries across all data types simultaneously. It also admits a number\nof interesting interpretations of the low rank factors, which allow clustering\nof examples or of features. We propose several parallel algorithms for fitting\ngeneralized low rank models, and describe implementations and numerical\nresults. \n\n"}
{"id": "1410.5303", "contents": "Title: Updating and downdating techniques for optimizing network\n  communicability Abstract: The total communicability of a network (or graph) is defined as the sum of\nthe entries in the exponential of the adjacency matrix of the network, possibly\nnormalized by the number of nodes. This quantity offers a good measure of how\neasily information spreads across the network, and can be useful in the design\nof networks having certain desirable properties. The total communicability can\nbe computed quickly even for large networks using techniques based on the\nLanczos algorithm.\n  In this work we introduce some heuristics that can be used to add, delete, or\nrewire a limited number of edges in a given sparse network so that the modified\nnetwork has a large total communicability. To this end, we introduce new edge\ncentrality measures which can be used to guide in the selection of edges to be\nadded or removed.\n  Moreover, we show experimentally that the total communicability provides an\neffective and easily computable measure of how \"well-connected\" a sparse\nnetwork is. \n\n"}
{"id": "1410.6991", "contents": "Title: A provable SVD-based algorithm for learning topics in dominant admixture\n  corpus Abstract: Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents\nare drawn from admixtures of distributions over words, known as topics. The\ninference problem of recovering topics from admixtures, is NP-hard. Assuming\nseparability, a strong assumption, [4] gave the first provable algorithm for\ninference. For LDA model, [6] gave a provable algorithm using tensor-methods.\nBut [4,6] do not learn topic vectors with bounded $l_1$ error (a natural\nmeasure for probability vectors). Our aim is to develop a model which makes\nintuitive and empirically supported assumptions and to design an algorithm with\nnatural, simple components such as SVD, which provably solves the inference\nproblem for the model with bounded $l_1$ error. A topic in LDA and other models\nis essentially characterized by a group of co-occurring words. Motivated by\nthis, we introduce topic specific Catchwords, group of words which occur with\nstrictly greater frequency in a topic than any other topic individually and are\nrequired to have high frequency together rather than individually. A major\ncontribution of the paper is to show that under this more realistic assumption,\nwhich is empirically verified on real corpora, a singular value decomposition\n(SVD) based algorithm with a crucial pre-processing step of thresholding, can\nprovably recover the topics from a collection of documents drawn from Dominant\nadmixtures. Dominant admixtures are convex combination of distributions in\nwhich one distribution has a significantly higher contribution than others.\nApart from the simplicity of the algorithm, the sample complexity has near\noptimal dependence on $w_0$, the lowest probability that a topic is dominant,\nand is better than [4]. Empirical evidence shows that on several real world\ncorpora, both Catchwords and Dominant admixture assumptions hold and the\nproposed algorithm substantially outperforms the state of the art [5]. \n\n"}
{"id": "1410.8175", "contents": "Title: Randomized Rumor Spreading in Poorly Connected Small-World Networks Abstract: Push-Pull is a well-studied round-robin rumor spreading protocol defined as\nfollows: initially a node knows a rumor and wants to spread it to all nodes in\na network quickly. In each round, every informed node sends the rumor to a\nrandom neighbor, and every uninformed node contacts a random neighbor and gets\nthe rumor from her if she knows it. We analyze this protocol on random\n$k$-trees, a class of power law graphs, which are small-world and have large\nclustering coefficients, built as follows: initially we have a $k$-clique. In\nevery step a new node is born, a random $k$-clique of the current graph is\nchosen, and the new node is joined to all nodes of the $k$-clique. When $k>1$\nis fixed, we show that if initially a random node is aware of the rumor, then\nwith probability $1-o(1)$ after $O\\left((\\log n)^{1+2/k}\\cdot\\log\\log n\\cdot\nf(n)\\right)$ rounds the rumor propagates to $n-o(n)$ nodes, where $n$ is the\nnumber of nodes and $f(n)$ is any slowly growing function. Since these graphs\nhave polynomially small conductance, vertex expansion $O(1/n)$ and constant\ntreewidth, these results demonstrate that Push-Pull can be efficient even on\npoorly connected networks. On the negative side, we prove that with probability\n$1-o(1)$ the protocol needs at least\n$\\Omega\\left(n^{(k-1)/(k^2+k-1)}/f^2(n)\\right)$ rounds to inform all nodes.\nThis exponential dichotomy between time required for informing almost all and\nall nodes is striking. Our main contribution is to present, for the first time,\na natural class of random graphs in which such a phenomenon can be observed.\nOur technique for proving the upper bound carries over to a closely related\nclass of graphs, random $k$-Apollonian networks, for which we prove an upper\nbound of $O\\left((\\log n)^{c_k}\\cdot\\log\\log n\\cdot f(n)\\right)$ rounds for\ninforming $n-o(n)$ nodes with probability $1-o(1)$ when $k>2$ is fixed. Here,\n$c_k=(k^2-3)/(k-1)^2<1 + 2/k$. \n\n"}
{"id": "1410.8525", "contents": "Title: Information Theory Perspective on Network Robustness Abstract: A crucial challenge in network theory is the study of the robustness of a\nnetwork after facing a sequence of failures. In this work, we propose a\ndynamical definition of network's robustness based on Information Theory, that\nconsiders measurements of the structural changes caused by failures of the\nnetwork's components. Failures are defined here, as a temporal process defined\nin a sequence. The robustness of the network is then evaluated by measuring\ndissimilarities between topologies after each time step of the sequence,\nproviding a dynamical information about the topological damage. We thoroughly\nanalyze the efficiency of the method in capturing small perturbations by\nconsidering both, the degree and distance distributions. We found the network's\ndistance distribution more consistent in capturing network structural\ndeviations, as better reflects the consequences of the failures. Theoretical\nexamples and real networks are used to study the performance of this\nmethodology. \n\n"}
{"id": "1410.8749", "contents": "Title: What a Nasty day: Exploring Mood-Weather Relationship from Twitter Abstract: While it has long been believed in psychology that weather somehow influences\nhuman's mood, the debates have been going on for decades about how they are\ncorrelated. In this paper, we try to study this long-lasting topic by\nharnessing a new source of data compared from traditional psychological\nresearches: Twitter. We analyze 2 years' twitter data collected by twitter API\nwhich amounts to $10\\%$ of all postings and try to reveal the correlations\nbetween multiple dimensional structure of human mood with meteorological\neffects. Some of our findings confirm existing hypotheses, while others\ncontradict them. We are hopeful that our approach, along with the new data\nsource, can shed on the long-going debates on weather-mood correlation. \n\n"}
{"id": "1411.0556", "contents": "Title: Measuring the Generalized Friendship Paradox in Networks with\n  Quality-dependent Connectivity Abstract: The friendship paradox is a sociological phenomenon stating that most people\nhave fewer friends than their friends do. The generalized friendship paradox\nrefers to the same observation for attributes other than degree, and it has\nbeen observed in Twitter and scientific collaboration networks. This paper\ntakes an analytical approach to model this phenomenon. We consider a\npreferential attachment-like network growth mechanism governed by both node\ndegrees and `qualities'. We introduce measures to quantify paradoxes, and\ncontrast the results obtained in our model to those obtained for an\nuncorrelated network, where the degrees and qualities of adjacent nodes are\nuncorrelated. We shed light on the effect of the distribution of node qualities\non the friendship paradox. We consider both the mean and the median to measure\nparadoxes, and compare the results obtained by using these two statistics. \n\n"}
{"id": "1411.0861", "contents": "Title: Using Linguistic Features to Estimate Suicide Probability of Chinese\n  Microblog Users Abstract: If people with high risk of suicide can be identified through social media\nlike microblog, it is possible to implement an active intervention system to\nsave their lives. Based on this motivation, the current study administered the\nSuicide Probability Scale(SPS) to 1041 weibo users at Sina Weibo, which is a\nleading microblog service provider in China. Two NLP (Natural Language\nProcessing) methods, the Chinese edition of Linguistic Inquiry and Word Count\n(LIWC) lexicon and Latent Dirichlet Allocation (LDA), are used to extract\nlinguistic features from the Sina Weibo data. We trained predicting models by\nmachine learning algorithm based on these two types of features, to estimate\nsuicide probability based on linguistic features. The experiment results\nindicate that LDA can find topics that relate to suicide probability, and\nimprove the performance of prediction. Our study adds value in prediction of\nsuicidal probability of social network users with their behaviors. \n\n"}
{"id": "1411.1279", "contents": "Title: Streaming, Memory Limited Algorithms for Community Detection Abstract: In this paper, we consider sparse networks consisting of a finite number of\nnon-overlapping communities, i.e. disjoint clusters, so that there is higher\ndensity within clusters than across clusters. Both the intra- and inter-cluster\nedge densities vanish when the size of the graph grows large, making the\ncluster reconstruction problem nosier and hence difficult to solve. We are\ninterested in scenarios where the network size is very large, so that the\nadjacency matrix of the graph is hard to manipulate and store. The data stream\nmodel in which columns of the adjacency matrix are revealed sequentially\nconstitutes a natural framework in this setting. For this model, we develop two\nnovel clustering algorithms that extract the clusters asymptotically\naccurately. The first algorithm is {\\it offline}, as it needs to store and keep\nthe assignments of nodes to clusters, and requires a memory that scales\nlinearly with the network size. The second algorithm is {\\it online}, as it may\nclassify a node when the corresponding column is revealed and then discard this\ninformation. This algorithm requires a memory growing sub-linearly with the\nnetwork size. To construct these efficient streaming memory-limited clustering\nalgorithms, we first address the problem of clustering with partial\ninformation, where only a small proportion of the columns of the adjacency\nmatrix is observed and develop, for this setting, a new spectral algorithm\nwhich is of independent interest. \n\n"}
{"id": "1411.2370", "contents": "Title: On the Universality of Jordan Centers for Estimating Infection Sources\n  in Tree Networks Abstract: Finding the infection sources in a network when we only know the network\ntopology and infected nodes, but not the rates of infection, is a challenging\ncombinatorial problem, and it is even more difficult in practice where the\nunderlying infection spreading model is usually unknown a priori. In this\npaper, we are interested in finding a source estimator that is applicable to\nvarious spreading models, including the Susceptible-Infected (SI),\nSusceptible-Infected-Recovered (SIR), Susceptible-Infected-Recovered-Infected\n(SIRI), and Susceptible-Infected-Susceptible (SIS) models. We show that under\nthe SI, SIR and SIRI spreading models and with mild technical assumptions, the\nJordan center is the infection source associated with the most likely infection\npath in a tree network with a single infection source. This conclusion applies\nfor a wide range of spreading parameters, while it holds for regular trees\nunder the SIS model with homogeneous infection and recovery rates. Since the\nJordan center does not depend on the infection, recovery and reinfection rates,\nit can be regarded as a universal source estimator. We also consider the case\nwhere there are k>1 infection sources, generalize the Jordan center definition\nto a k-Jordan center set, and show that this is an optimal infection source set\nestimator in a tree network for the SI model. Simulation results on various\ngeneral synthetic networks and real world networks suggest that Jordan\ncenter-based estimators consistently outperform the betweenness, closeness,\ndistance, degree, eigenvector, and pagerank centrality based heuristics, even\nif the network is not a tree. \n\n"}
{"id": "1411.3776", "contents": "Title: Measuring Influence in Twitter Ecosystems using a Counting Process\n  Modeling Framework Abstract: Data extracted from social media platforms, such as Twitter, are both large\nin scale and complex in nature, since they contain both unstructured text, as\nwell as structured data, such as time stamps and interactions between users. A\nkey question for such platforms is to determine influential users, in the sense\nthat they generate interactions between members of the platform. Common\nmeasures used both in the academic literature and by companies that provide\nanalytics services are variants of the popular web-search PageRank algorithm\napplied to networks that capture connections between users. In this work, we\ndevelop a modeling framework using multivariate interacting counting processes\nto capture the detailed actions that users undertake on such platforms, namely\nposting original content, reposting and/or mentioning other users' postings.\nBased on the proposed model, we also derive a novel influence measure. We\ndiscuss estimation of the model parameters through maximum likelihood and\nestablish their asymptotic properties. The proposed model and the accompanying\ninfluence measure are illustrated on a data set covering a five year period of\nthe Twitter actions of the members of the US Senate, as well as mainstream news\norganizations and media personalities. \n\n"}
{"id": "1411.7718", "contents": "Title: Classification with Noisy Labels by Importance Reweighting Abstract: In this paper, we study a classification problem in which sample labels are\nrandomly corrupted. In this scenario, there is an unobservable sample with\nnoise-free labels. However, before being observed, the true labels are\nindependently flipped with a probability $\\rho\\in[0,0.5)$, and the random label\nnoise can be class-conditional. Here, we address two fundamental problems\nraised by this scenario. The first is how to best use the abundant surrogate\nloss functions designed for the traditional classification problem when there\nis label noise. We prove that any surrogate loss function can be used for\nclassification with noisy labels by using importance reweighting, with\nconsistency assurance that the label noise does not ultimately hinder the\nsearch for the optimal classifier of the noise-free sample. The other is the\nopen problem of how to obtain the noise rate $\\rho$. We show that the rate is\nupper bounded by the conditional probability $P(y|x)$ of the noisy sample.\nConsequently, the rate can be estimated, because the upper bound can be easily\nreached in classification problems. Experimental results on synthetic and real\ndatasets confirm the efficiency of our methods. \n\n"}
{"id": "1411.7960", "contents": "Title: The Importance of Being Earnest in Crowdsourcing Systems Abstract: This paper presents the first systematic investigation of the potential\nperformance gains for crowdsourcing systems, deriving from available\ninformation at the requester about individual worker earnestness (reputation).\nIn particular, we first formalize the optimal task assignment problem when\nworkers' reputation estimates are available, as the maximization of a monotone\n(submodular) function subject to Matroid constraints. Then, being the optimal\nproblem NP-hard, we propose a simple but efficient greedy heuristic task\nallocation algorithm. We also propose a simple ``maximum a-posteriori``\ndecision rule. Finally, we test and compare different solutions, showing that\nsystem performance can greatly benefit from information about workers'\nreputation. Our main findings are that: i) even largely inaccurate estimates of\nworkers' reputation can be effectively exploited in the task assignment to\ngreatly improve system performance; ii) the performance of the maximum\na-posteriori decision rule quickly degrades as worker reputation estimates\nbecome inaccurate; iii) when workers' reputation estimates are significantly\ninaccurate, the best performance can be obtained by combining our proposed task\nassignment algorithm with the LRA decision rule introduced in the literature. \n\n"}
{"id": "1412.1353", "contents": "Title: Curriculum Learning of Multiple Tasks Abstract: Sharing information between multiple tasks enables algorithms to achieve good\ngeneralization performance even from small amounts of training data. However,\nin a realistic scenario of multi-task learning not all tasks are equally\nrelated to each other, hence it could be advantageous to transfer information\nonly between the most related tasks. In this work we propose an approach that\nprocesses multiple tasks in a sequence with sharing between subsequent tasks\ninstead of solving all tasks jointly. Subsequently, we address the question of\ncurriculum learning of tasks, i.e. finding the best order of tasks to be\nlearned. Our approach is based on a generalization bound criterion for choosing\nthe task order that optimizes the average expected classification performance\nover all tasks. Our experimental results show that learning multiple related\ntasks sequentially can be more effective than learning them jointly, the order\nin which tasks are being solved affects the overall performance, and that our\nmodel is able to automatically discover the favourable order of tasks. \n\n"}
{"id": "1412.2154", "contents": "Title: Understanding Human Mobility from Twitter Abstract: Understanding human mobility is crucial for a broad range of applications\nfrom disease prediction to communication networks. Most efforts on studying\nhuman mobility have so far used private and low resolution data, such as call\ndata records. Here, we propose Twitter as a proxy for human mobility, as it\nrelies on publicly available data and provides high resolution positioning when\nusers opt to geotag their tweets with their current location. We analyse a\nTwitter dataset with more than six million geotagged tweets posted in\nAustralia, and we demonstrate that Twitter can be a reliable source for\nstudying human mobility patterns. Our analysis shows that geotagged tweets can\ncapture rich features of human mobility, such as the diversity of movement\norbits among individuals and of movements within and between cities. We also\nfind that short and long-distance movers both spend most of their time in large\nmetropolitan areas, in contrast with intermediate-distance movers movements,\nreflecting the impact of different modes of travel. Our study provides solid\nevidence that Twitter can indeed be a useful proxy for tracking and predicting\nhuman movement. \n\n"}
{"id": "1412.4171", "contents": "Title: Dynamics of Information Diffusion and Social Sensing Abstract: Statistical inference using social sensors is an area that has witnessed\nremarkable progress and is relevant in applications including localizing events\nfor targeted advertising, marketing, localization of natural disasters and\npredicting sentiment of investors in financial markets. This chapter presents a\ntutorial description of four important aspects of sensing-based information\ndiffusion in social networks from a communications/signal processing\nperspective. First, diffusion models for information exchange in large scale\nsocial networks together with social sensing via social media networks such as\nTwitter is considered. Second, Bayesian social learning models and risk averse\nsocial learning is considered with applications in finance and online\nreputation systems. Third, the principle of revealed preferences arising in\nmicro-economics theory is used to parse datasets to determine if social sensors\nare utility maximizers and then determine their utility functions. Finally, the\ninteraction of social sensors with YouTube channel owners is studied using time\nseries analysis methods. All four topics are explained in the context of actual\nexperimental datasets from health networks, social media and psychological\nexperiments. Also, algorithms are given that exploit the above models to infer\nunderlying events based on social sensing. The overview, insights, models and\nalgorithms presented in this chapter stem from recent developments in network\nscience, economics and signal processing. At a deeper level, this chapter\nconsiders mean field dynamics of networks, risk averse Bayesian social learning\nfiltering and quickest change detection, data incest in decision making over a\ndirected acyclic graph of social sensors, inverse optimization problems for\nutility function estimation (revealed preferences) and statistical modeling of\ninteracting social sensors in YouTube social networks. \n\n"}
{"id": "1412.6980", "contents": "Title: Adam: A Method for Stochastic Optimization Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm. \n\n"}
{"id": "1501.00738", "contents": "Title: Zipf's Law from Scale-free Geometry Abstract: The spatial distribution of people exhibits clustering across a wide range of\nscales, from household ($\\sim 10^{-2}$ km) to continental ($\\sim 10^4$ km)\nscales. Empirical data indicates simple power-law scalings for the size\ndistribution of cities (known as Zipf's law) and the population density\nfluctuations as a function of scale. Using techniques from random field theory\nand statistical physics, we show that these power laws are fundamentally a\nconsequence of the scale-free spatial clustering of human populations and the\nfact that humans inhabit a two-dimensional surface. In this sense, the\nsymmetries of scale invariance in two spatial dimensions are intimately\nconnected to urban sociology. We test our theory by empirically measuring the\npower spectrum of population density fluctuations and show that the logarithmic\nslope $\\alpha = 2.04 \\pm 0.09$, in excellent agreement with our theoretical\nprediction $\\alpha = 2$. The model enables the analytic computation of many new\npredictions by importing the mathematical formalism of random fields. \n\n"}
{"id": "1501.01903", "contents": "Title: Interests Diffusion in Social Networks Abstract: Understanding cultural phenomena on Social Networks (SNs) and exploiting the\nimplicit knowledge about their members is attracting the interest of different\nresearch communities both from the academic and the business side. The\ncommunity of complexity science is devoting significant efforts to define laws,\nmodels, and theories, which, based on acquired knowledge, are able to predict\nfuture observations (e.g. success of a product). In the mean time, the semantic\nweb community aims at engineering a new generation of advanced services by\ndefining constructs, models and methods, adding a semantic layer to SNs. In\nthis context, a leapfrog is expected to come from a hybrid approach merging the\ndisciplines above. Along this line, this work focuses on the propagation of\nindividual interests in social networks. The proposed framework consists of the\nfollowing main components: a method to gather information about the members of\nthe social networks; methods to perform some semantic analysis of the Domain of\nInterest; a procedure to infer members' interests; and an interests evolution\ntheory to predict how the interests propagate in the network. As a result, one\nachieves an analytic tool to measure individual features, such as members'\nsusceptibilities and authorities. Although the approach applies to any type of\nsocial network, here it is has been tested against the computer science\nresearch community.\n  The DBLP (Digital Bibliography and Library Project) database has been elected\nas test-case since it provides the most comprehensive list of scientific\nproduction in this field. \n\n"}
{"id": "1501.01939", "contents": "Title: Where Graph Topology Matters: The Robust Subgraph Problem Abstract: Robustness is a critical measure of the resilience of large networked\nsystems, such as transportation and communication networks. Most prior works\nfocus on the global robustness of a given graph at large, e.g., by measuring\nits overall vulnerability to external attacks or random failures. In this\npaper, we turn attention to local robustness and pose a novel problem in the\nlines of subgraph mining: given a large graph, how can we find its most robust\nlocal subgraph (RLS)?\n  We define a robust subgraph as a subset of nodes with high communicability\namong them, and formulate the RLS-PROBLEM of finding a subgraph of given size\nwith maximum robustness in the host graph. Our formulation is related to the\nrecently proposed general framework for the densest subgraph problem, however\ndiffers from it substantially in that besides the number of edges in the\nsubgraph, robustness also concerns with the placement of edges, i.e., the\nsubgraph topology. We show that the RLS-PROBLEM is NP-hard and propose two\nheuristic algorithms based on top-down and bottom-up search strategies.\nFurther, we present modifications of our algorithms to handle three practical\nvariants of the RLS-PROBLEM. Experiments on synthetic and real-world graphs\ndemonstrate that we find subgraphs with larger robustness than the densest\nsubgraphs even at lower densities, suggesting that the existing approaches are\nnot suitable for the new problem setting. \n\n"}
{"id": "1501.04260", "contents": "Title: Disease spread over randomly switched large-scale networks Abstract: In this paper we study disease spread over a randomly switched network, which\nis modeled by a stochastic switched differential equation based on the so\ncalled $N$-intertwined model for disease spread over static networks. Assuming\nthat all the edges of the network are independently switched, we present\nsufficient conditions for the convergence of infection probability to zero.\nThough the stability theory for switched linear systems can naively derive a\nnecessary and sufficient condition for the convergence, the condition cannot be\nused for large-scale networks because, for a network with $n$ agents, it\nrequires computing the maximum real eigenvalue of a matrix of size exponential\nin $n$. On the other hand, our conditions that are based also on the spectral\ntheory of random matrices can be checked by computing the maximum real\neigenvalue of a matrix of size exactly $n$. \n\n"}
{"id": "1502.00405", "contents": "Title: Monotone Increasing Properties and Their Phase Transitions in Uniform\n  Random Intersection Graphs Abstract: Uniform random intersection graphs have received much interest and been used\nin diverse applications. A uniform random intersection graph with $n$ nodes is\nconstructed as follows: each node selects a set of $K_n$ different items\nuniformly at random from the same pool of $P_n$ distinct items, and two nodes\nestablish an undirected edge in between if and only if they share at least one\nitem. For such graph denoted by $G(n, K_n, P_n)$, we present the following\nresults in this paper. First, we provide an exact analysis on the probabilities\nof $G(n, K_n, P_n)$ having a perfect matching and having a Hamilton cycle\nrespectively, under $P_n = \\omega\\big(n (\\ln n)^5\\big)$ (all asymptotic\nnotation are understood with $n \\to \\infty$). The analysis reveals that just\nlike ($k$-)connectivity shown in prior work, for both properties of perfect\nmatching containment and Hamilton cycle containment, $G(n, K_n, P_n)$ also\nexhibits phase transitions: for each property above, as $K_n$ increases, the\nlimit of the probability that $G(n, K_n, P_n)$ has the property increases from\n$0$ to $1$. Second, we compute the phase transition widths of $G(n, K_n, P_n)$\nfor $k$-connectivity (KC), perfect matching containment (PMC), and Hamilton\ncycle containment (HCC), respectively. For a graph property $R$ and a positive\nconstant $a < \\frac{1}{2}$, with the phase transition width $d_n(R, a)$ defined\nas the difference between the minimal $K_n$ ensuring $G(n, K_n, P_n)$ having\nproperty $R$ with probability at least $1-a$ or $a$, we show for any positive\nconstants $a<\\frac{1}{2}$ and $k$: (i) If $P_n=\\Omega(n)$ and $P_n=o(n\\ln n)$,\nthen $d_n(KC, a)$ is either $0$ or $1$ for each $n$ sufficiently large. (ii) If\n$P_n=\\Theta(n\\ln n)$, then $d_n(KC, a)=\\Theta(1)$. (iii) If $P_n=\\omega(n\\ln\nn)$, then $d_n(KC, a)=\\omega(1)$. (iv) If $P_n=\\omega\\big(n (\\ln n)^5\\big)$,\n$d_n(PMC, a)$ and $d_n(HCC, a)$ are both $\\omega(1)$. \n\n"}
{"id": "1502.02558", "contents": "Title: K2-ABC: Approximate Bayesian Computation with Kernel Embeddings Abstract: Complicated generative models often result in a situation where computing the\nlikelihood of observed data is intractable, while simulating from the\nconditional density given a parameter value is relatively easy. Approximate\nBayesian Computation (ABC) is a paradigm that enables simulation-based\nposterior inference in such cases by measuring the similarity between simulated\nand observed data in terms of a chosen set of summary statistics. However,\nthere is no general rule to construct sufficient summary statistics for complex\nmodels. Insufficient summary statistics will \"leak\" information, which leads to\nABC algorithms yielding samples from an incorrect (partial) posterior. In this\npaper, we propose a fully nonparametric ABC paradigm which circumvents the need\nfor manually selecting summary statistics. Our approach, K2-ABC, uses maximum\nmean discrepancy (MMD) as a dissimilarity measure between the distributions\nover observed and simulated data. MMD is easily estimated as the squared\ndifference between their empirical kernel embeddings. Experiments on a\nsimulated scenario and a real-world biological problem illustrate the\neffectiveness of the proposed algorithm. \n\n"}
{"id": "1502.03406", "contents": "Title: A survey of results on mobile phone datasets analysis Abstract: In this paper, we review some advances made recently in the study of mobile\nphone datasets. This area of research has emerged a decade ago, with the\nincreasing availability of large-scale anonymized datasets, and has grown into\na stand-alone topic. We will survey the contributions made so far on the social\nnetworks that can be constructed with such data, the study of personal\nmobility, geographical partitioning, urban planning, and help towards\ndevelopment as well as security and privacy issues. \n\n"}
{"id": "1502.03496", "contents": "Title: Spectral Sparsification of Random-Walk Matrix Polynomials Abstract: We consider a fundamental algorithmic question in spectral graph theory:\nCompute a spectral sparsifier of random-walk matrix-polynomial\n$$L_\\alpha(G)=D-\\sum_{r=1}^d\\alpha_rD(D^{-1}A)^r$$ where $A$ is the adjacency\nmatrix of a weighted, undirected graph, $D$ is the diagonal matrix of weighted\ndegrees, and $\\alpha=(\\alpha_1...\\alpha_d)$ are nonnegative coefficients with\n$\\sum_{r=1}^d\\alpha_r=1$. Recall that $D^{-1}A$ is the transition matrix of\nrandom walks on the graph. The sparsification of $L_\\alpha(G)$ appears to be\nalgorithmically challenging as the matrix power $(D^{-1}A)^r$ is defined by all\npaths of length $r$, whose precise calculation would be prohibitively\nexpensive.\n  In this paper, we develop the first nearly linear time algorithm for this\nsparsification problem: For any $G$ with $n$ vertices and $m$ edges, $d$\ncoefficients $\\alpha$, and $\\epsilon > 0$, our algorithm runs in time\n$O(d^2m\\log^2n/\\epsilon^{2})$ to construct a Laplacian matrix\n$\\tilde{L}=D-\\tilde{A}$ with $O(n\\log n/\\epsilon^{2})$ non-zeros such that\n$\\tilde{L}\\approx_{\\epsilon}L_\\alpha(G)$.\n  Matrix polynomials arise in mathematical analysis of matrix functions as well\nas numerical solutions of matrix equations. Our work is particularly motivated\nby the algorithmic problems for speeding up the classic Newton's method in\napplications such as computing the inverse square-root of the precision matrix\nof a Gaussian random field, as well as computing the $q$th-root transition (for\n$q\\geq1$) in a time-reversible Markov model. The key algorithmic step for both\napplications is the construction of a spectral sparsifier of a constant degree\nrandom-walk matrix-polynomials introduced by Newton's method. Our algorithm can\nalso be used to build efficient data structures for effective resistances for\nmulti-step time-reversible Markov models, and we anticipate that it could be\nuseful for other tasks in network analysis. \n\n"}
{"id": "1502.04622", "contents": "Title: Particle Gibbs for Bayesian Additive Regression Trees Abstract: Additive regression trees are flexible non-parametric models and popular\noff-the-shelf tools for real-world non-linear regression. In application\ndomains, such as bioinformatics, where there is also demand for probabilistic\npredictions with measures of uncertainty, the Bayesian additive regression\ntrees (BART) model, introduced by Chipman et al. (2010), is increasingly\npopular. As data sets have grown in size, however, the standard\nMetropolis-Hastings algorithms used to perform inference in BART are proving\ninadequate. In particular, these Markov chains make local changes to the trees\nand suffer from slow mixing when the data are high-dimensional or the best\nfitting trees are more than a few layers deep. We present a novel sampler for\nBART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a\ntop-down particle filtering algorithm for Bayesian decision trees\n(Lakshminarayanan et al., 2013). Rather than making local changes to individual\ntrees, the PG sampler proposes a complete tree to fit the residual. Experiments\nshow that the PG sampler outperforms existing samplers in many settings. \n\n"}
{"id": "1502.04697", "contents": "Title: From graphs to signals and back: Identification of network structures\n  using spectral analysis Abstract: Many systems comprising entities in interactions can be represented as\ngraphs, whose structure gives significant insights about how these systems\nwork. Network theory has undergone further developments, in particular in\nrelation to detection of communities in graphs, to catch this structure.\nRecently, an approach has been proposed to transform a graph into a collection\nof signals: Using a multidimensional scaling technique on a distance matrix\nrepresenting relations between vertices of the graph, points in a Euclidean\nspace are obtained and interpreted as signals, indexed by the vertices. In this\narticle, we propose several extensions to this approach, developing a framework\nto study graph structures using signal processing tools. We first extend the\ncurrent methodology, enabling us to highlight connections between properties of\nsignals and graph structures, such as communities, regularity or randomness, as\nwell as combinations of those. A robust inverse transformation method is next\ndescribed, taking into account possible changes in the signals compared to\noriginal ones. This technique uses, in addition to the relationships between\nthe points in the Euclidean space, the energy of each signal, coding the\ndifferent scales of the graph structure. These contributions open up new\nperspectives in the study of graphs, by enabling processing of graphs through\nthe processing of the corresponding collection of signals, using reliable tools\nfrom signal processing. A technique of denoising of a graph by filtering of the\ncorresponding signals is then described, suggesting considerable potential of\nthe approach. \n\n"}
{"id": "1502.05578", "contents": "Title: Network Geometry Inference using Common Neighbors Abstract: We introduce and explore a new method for inferring hidden geometric\ncoordinates of nodes in complex networks based on the number of common\nneighbors between the nodes. We compare this approach to the HyperMap method,\nwhich is based only on the connections (and disconnections) between the nodes,\ni.e., on the links that the nodes have (or do not have). We find that for high\ndegree nodes the common-neighbors approach yields a more accurate inference\nthan the link-based method, unless heuristic periodic adjustments (or\n\"correction steps\") are used in the latter. The common-neighbors approach is\ncomputationally intensive, requiring $O(t^4)$ running time to map a network of\n$t$ nodes, versus $O(t^3)$ in the link-based method. But we also develop a\nhybrid method with $O(t^3)$ running time, which combines the common-neighbors\nand link-based approaches, and explore a heuristic that reduces its running\ntime further to $O(t^2)$, without significant reduction in the mapping\naccuracy. We apply this method to the Autonomous Systems (AS) Internet, and\nreveal how soft communities of ASes evolve over time in the similarity space.\nWe further demonstrate the method's predictive power by forecasting future\nlinks between ASes. Taken altogether, our results advance our understanding of\nhow to efficiently and accurately map real networks to their latent geometric\nspaces, which is an important necessary step towards understanding the laws\nthat govern the dynamics of nodes in these spaces, and the fine-grained\ndynamics of network connections. \n\n"}
{"id": "1502.05680", "contents": "Title: Finding One Community in a Sparse Graph Abstract: We consider a random sparse graph with bounded average degree, in which a\nsubset of vertices has higher connectivity than the background. In particular,\nthe average degree inside this subset of vertices is larger than outside (but\nstill bounded). Given a realization of such graph, we aim at identifying the\nhidden subset of vertices. This can be regarded as a model for the problem of\nfinding a tightly knitted community in a social network, or a cluster in a\nrelational dataset.\n  In this paper we present two sets of contributions: $(i)$ We use the cavity\nmethod from spin glass theory to derive an exact phase diagram for the\nreconstruction problem. In particular, as the difference in edge probability\nincreases, the problem undergoes two phase transitions, a static phase\ntransition and a dynamic one. $(ii)$ We establish rigorous bounds on the\ndynamic phase transition and prove that, above a certain threshold, a local\nalgorithm (belief propagation) correctly identify most of the hidden set. Below\nthe same threshold \\emph{no local algorithm} can achieve this goal. However, in\nthis regime the subset can be identified by exhaustive search.\n  For small hidden sets and large average degree, the phase transition for\nlocal algorithms takes an intriguingly simple form. Local algorithms succeed\nwith high probability for ${\\rm deg}_{\\rm in} - {\\rm deg}_{\\rm out} >\n\\sqrt{{\\rm deg}_{\\rm out}/e}$ and fail for ${\\rm deg}_{\\rm in} - {\\rm deg}_{\\rm\nout} < \\sqrt{{\\rm deg}_{\\rm out}/e}$ (with ${\\rm deg}_{\\rm in}$, ${\\rm\ndeg}_{\\rm out}$ the average degrees inside and outside the community). We argue\nthat spectral algorithms are also ineffective in the latter regime.\n  It is an open problem whether any polynomial time algorithms might succeed\nfor ${\\rm deg}_{\\rm in} - {\\rm deg}_{\\rm out} < \\sqrt{{\\rm deg}_{\\rm out}/e}$. \n\n"}
{"id": "1503.00759", "contents": "Title: A Review of Relational Machine Learning for Knowledge Graphs Abstract: Relational machine learning studies methods for the statistical analysis of\nrelational, or graph-structured, data. In this paper, we provide a review of\nhow such statistical models can be \"trained\" on large knowledge graphs, and\nthen used to predict new facts about the world (which is equivalent to\npredicting new edges in the graph). In particular, we discuss two fundamentally\ndifferent kinds of statistical relational models, both of which can scale to\nmassive datasets. The first is based on latent feature models such as tensor\nfactorization and multiway neural networks. The second is based on mining\nobservable patterns in the graph. We also show how to combine these latent and\nobservable models to get improved modeling power at decreased computational\ncost. Finally, we discuss how such statistical models of graphs can be combined\nwith text-based information extraction methods for automatically constructing\nknowledge graphs from the Web. To this end, we also discuss Google's Knowledge\nVault project as an example of such combination. \n\n"}
{"id": "1503.02216", "contents": "Title: Higher order Matching Pursuit for Low Rank Tensor Learning Abstract: Low rank tensor learning, such as tensor completion and multilinear multitask\nlearning, has received much attention in recent years. In this paper, we\npropose higher order matching pursuit for low rank tensor learning problems\nwith a convex or a nonconvex cost function, which is a generalization of the\nmatching pursuit type methods. At each iteration, the main cost of the proposed\nmethods is only to compute a rank-one tensor, which can be done efficiently,\nmaking the proposed methods scalable to large scale problems. Moreover, storing\nthe resulting rank-one tensors is of low storage requirement, which can help to\nbreak the curse of dimensionality. The linear convergence rate of the proposed\nmethods is established in various circumstances. Along with the main methods,\nwe also provide a method of low computational complexity for approximately\ncomputing the rank-one tensors, with provable approximation ratio, which helps\nto improve the efficiency of the main methods and to analyze the convergence\nrate. Experimental results on synthetic as well as real datasets verify the\nefficiency and effectiveness of the proposed methods. \n\n"}
{"id": "1503.02531", "contents": "Title: Distilling the Knowledge in a Neural Network Abstract: A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel. \n\n"}
{"id": "1503.04567", "contents": "Title: Learning Mixed Membership Community Models in Social Tagging Networks\n  through Tensor Methods Abstract: Community detection in graphs has been extensively studied both in theory and\nin applications. However, detecting communities in hypergraphs is more\nchallenging. In this paper, we propose a tensor decomposition approach for\nguaranteed learning of communities in a special class of hypergraphs modeling\nsocial tagging systems or folksonomies. A folksonomy is a tripartite 3-uniform\nhypergraph consisting of (user, tag, resource) hyperedges. We posit a\nprobabilistic mixed membership community model, and prove that the tensor\nmethod consistently learns the communities under efficient sample complexity\nand separation requirements. \n\n"}
{"id": "1503.04877", "contents": "Title: An Automated System for Discovering Neighborhood Patterns in Ego\n  Networks Abstract: Generally, social network analysis has often focused on the topology of the\nnetwork without considering the characteristics of individuals involved in\nthem. Less attention is given to study the behavior of individuals, considering\nthey are the basic entity of a graph. Given a mobile social network graph, what\nare good features to extract key information from the nodes? How many distinct\nneighborhood patterns exist for ego nodes? What clues does such information\nprovide to study nodes over a long period of time?\n  In this report, we develop an automated system in order to discover the\noccurrences of prototypical ego-centric patterns from data. We aim to provide a\ndata-driven instrument to be used in behavioral sciences for graph\ninterpretations. We analyze social networks derived from real-world data\ncollected with smart-phones. We select 13 well-known network measures,\nespecially those concerned with ego graphs. We form eight feature subsets and\nthen assess their performance using unsupervised clustering techniques to\ndiscover distinguishing ego-centric patterns. From clustering analysis, we\ndiscover that eight distinct neighborhood patterns have emerged. This\ncategorization allows concise analysis of users' data as they change over time.\nThe results provide a fine-grained analysis for the contribution of different\nfeature sets to detect unique clustering patterns. Last, as a case study, two\ndatasets are studied over long periods to demonstrate the utility of this\nmethod. The study shows the effectiveness of the proposed approach in\ndiscovering important trends from data. \n\n"}
{"id": "1503.05826", "contents": "Title: Respondent-driven sampling bias induced by clustering and community\n  structure in social networks Abstract: Sampling hidden populations is particularly challenging using standard\nsampling methods mainly because of the lack of a sampling frame.\nRespondent-driven sampling (RDS) is an alternative methodology that exploits\nthe social contacts between peers to reach and weight individuals in these\nhard-to-reach populations. It is a snowball sampling procedure where the weight\nof the respondents is adjusted for the likelihood of being sampled due to\ndifferences in the number of contacts. In RDS, the structure of the social\ncontacts thus defines the sampling process and affects its coverage, for\ninstance by constraining the sampling within a sub-region of the network. In\nthis paper we study the bias induced by network structures such as social\ntriangles, community structure, and heterogeneities in the number of contacts,\nin the recruitment trees and in the RDS estimator. We simulate different\nscenarios of network structures and response-rates to study the potential\nbiases one may expect in real settings. We find that the prevalence of the\nestimated variable is associated with the size of the network community to\nwhich the individual belongs. Furthermore, we observe that low-degree nodes may\nbe under-sampled in certain situations if the sample and the network are of\nsimilar size. Finally, we also show that low response-rates lead to reasonably\naccurate average estimates of the prevalence but generate relatively large\nbiases. \n\n"}
{"id": "1503.07711", "contents": "Title: Ideological and Temporal Components of Network Polarization in Online\n  Political Participatory Media Abstract: Political polarization is traditionally analyzed through the ideological\nstances of groups and parties, but it also has a behavioral component that\nmanifests in the interactions between individuals. We present an empirical\nanalysis of the digital traces of politicians in politnetz.ch, a Swiss online\nplatform focused on political activity, in which politicians interact by\ncreating support links, comments, and likes. We analyze network polarization as\nthe level of intra- party cohesion with respect to inter-party connectivity,\nfinding that supports show a very strongly polarized structure with respect to\nparty alignment. The analysis of this multiplex network shows that each layer\nof interaction contains relevant information, where comment groups follow\ntopics related to Swiss politics. Our analysis reveals that polarization in the\nlayer of likes evolves in time, increasing close to the federal elections of\n2011. Furthermore, we analyze the internal social network of each party through\nmetrics related to hierarchical structures, information efficiency, and social\nresilience. Our results suggest that the online social structure of a party is\nrelated to its ideology, and reveal that the degree of connectivity across two\nparties increases when they are close in the ideological space of a multi-party\nsystem. \n\n"}
{"id": "1504.00377", "contents": "Title: Bayesian Clustering of Shapes of Curves Abstract: Unsupervised clustering of curves according to their shapes is an important\nproblem with broad scientific applications. The existing model-based clustering\ntechniques either rely on simple probability models (e.g., Gaussian) that are\nnot generally valid for shape analysis or assume the number of clusters. We\ndevelop an efficient Bayesian method to cluster curve data using an elastic\nshape metric that is based on joint registration and comparison of shapes of\ncurves. The elastic-inner product matrix obtained from the data is modeled\nusing a Wishart distribution whose parameters are assigned carefully chosen\nprior distributions to allow for automatic inference on the number of clusters.\nPosterior is sampled through an efficient Markov chain Monte Carlo procedure\nbased on the Chinese restaurant process to infer (1) the posterior distribution\non the number of clusters, and (2) clustering configuration of shapes. This\nmethod is demonstrated on a variety of synthetic data and real data examples on\nprotein structure analysis, cell shape analysis in microscopy images, and\nclustering of shaped from MPEG7 database. \n\n"}
{"id": "1504.00502", "contents": "Title: Detecting the Influence of Spreading in Social Networks with Excitable\n  Sensor Networks Abstract: Detecting spreading outbreaks in social networks with sensors is of great\nsignificance in applications. Inspired by the formation mechanism of human's\nphysical sensations to external stimuli, we propose a new method to detect the\ninfluence of spreading by constructing excitable sensor networks. Exploiting\nthe amplifying effect of excitable sensor networks, our method can better\ndetect small-scale spreading processes. At the same time, it can also\ndistinguish large-scale diffusion instances due to the self-inhibition effect\nof excitable elements. Through simulations of diverse spreading dynamics on\ntypical real-world social networks (facebook, coauthor and email social\nnetworks), we find that the excitable senor networks are capable of detecting\nand ranking spreading processes in a much wider range of influence than other\ncommonly used sensor placement methods, such as random, targeted, acquaintance\nand distance strategies. In addition, we validate the efficacy of our method\nwith diffusion data from a real-world online social system, Twitter. We find\nthat our method can detect more spreading topics in practice. Our approach\nprovides a new direction in spreading detection and should be useful for\ndesigning effective detection methods. \n\n"}
{"id": "1504.02412", "contents": "Title: Phase Transitions in Spectral Community Detection of Large Noisy\n  Networks Abstract: In this paper, we study the sensitivity of the spectral clustering based\ncommunity detection algorithm subject to a Erdos-Renyi type random noise model.\nWe prove phase transitions in community detectability as a function of the\nexternal edge connection probability and the noisy edge presence probability\nunder a general network model where two arbitrarily connected communities are\ninterconnected by random external edges. Specifically, the community detection\nperformance transitions from almost perfect detectability to low detectability\nas the inter-community edge connection probability exceeds some critical value.\nWe derive upper and lower bounds on the critical value and show that the bounds\nare identical when the two communities have the same size. The phase transition\nresults are validated using network simulations. Using the derived expressions\nfor the phase transition threshold we propose a method for estimating this\nthreshold from observed data. \n\n"}
{"id": "1504.04558", "contents": "Title: A Picture Tells a Thousand Words -- About You! User Interest Profiling\n  from User Generated Visual Content Abstract: Inference of online social network users' attributes and interests has been\nan active research topic. Accurate identification of users' attributes and\ninterests is crucial for improving the performance of personalization and\nrecommender systems. Most of the existing works have focused on textual content\ngenerated by the users and have successfully used it for predicting users'\ninterests and other identifying attributes. However, little attention has been\npaid to user generated visual content (images) that is becoming increasingly\npopular and pervasive in recent times. We posit that images posted by users on\nonline social networks are a reflection of topics they are interested in and\npropose an approach to infer user attributes from images posted by them. We\nanalyze the content of individual images and then aggregate the image-level\nknowledge to infer user-level interest distribution. We employ image-level\nsimilarity to propagate the label information between images, as well as\nutilize the image category information derived from the user created\norganization structure to further propagate the category-level knowledge for\nall images. A real life social network dataset created from Pinterest is used\nfor evaluation and the experimental results demonstrate the effectiveness of\nour proposed approach. \n\n"}
{"id": "1504.04663", "contents": "Title: TrueTop: A Sybil-Resilient System for User Influence Measurement on\n  Twitter Abstract: Influential users have great potential for accelerating information\ndissemination and acquisition on Twitter. How to measure the influence of\nTwitter users has attracted significant academic and industrial attention.\nExisting influential measurement techniques, however, are vulnerable to sybil\nusers that are thriving on Twitter. Although sybil defenses for online social\nnetworks have been extensively investigated, they commonly assume unique\nmappings from human-established trust relationships to online social\nassociations and thus do not apply to Twitter where users can freely follow\neach other. This paper presents TrueTop, the first sybil-resilient system to\nmeasure the influence of Twitter users. TrueTop is firmly rooted in two\nobservations from real Twitter datasets. First, although non-sybil users may\nincautiously follow strangers, they tend to be more careful and selective in\nretweeting, replying to, and mentioning other Twitter users. Second,\ninfluential users usually get much more retweets, replies, and mentions than\nnon-influential users. Detailed theoretical studies and synthetic simulations\nshow that TrueTop can generate very accurate influence measurement results and\nalso have strong resilience to sybil attacks. \n\n"}
{"id": "1504.05351", "contents": "Title: On the Role of Conductance, Geography and Topology in Predicting Hashtag\n  Virality Abstract: We focus on three aspects of the early spread of a hashtag in order to\npredict whether it will go viral: the network properties of the subset of users\ntweeting the hashtag, its geographical properties, and, most importantly, its\nconductance-related properties. One of our significant contributions is to\ndiscover the critical role played by the conductance based features for the\nsuccessful prediction of virality. More specifically, we show that the first\nderivative of the conductance gives an early indication of whether the hashtag\nis going to go viral or not. We present a detailed experimental evaluation of\nthe effect of our various categories of features on the virality prediction\ntask. When compared to the baselines and the state of the art techniques\nproposed in the literature our feature set is able to achieve significantly\nbetter accuracy on a large dataset of 7.7 million users and all their tweets\nover a period of month, as well as on existing datasets. \n\n"}
{"id": "1504.05655", "contents": "Title: Online Social Network Analysis: A Survey of Research Applications in\n  Computer Science Abstract: The emergence and popularization of online social networks suddenly made\navailable a large amount of data from social organization, interaction and\nhuman behavior. All this information opens new perspectives and challenges to\nthe study of social systems, being of interest to many fields. Although most\nonline social networks are recent (less than fifteen years old), a vast amount\nof scientific papers was already published on this topic, dealing with a broad\nrange of analytical methods and applications. This work describes how\ncomputational researches have approached this subject and the methods used to\nanalyze such systems. Founded on a wide though non-exaustive review of the\nliterature, a taxonomy is proposed to classify and describe different\ncategories of research. Each research category is described and the main works,\ndiscoveries and perspectives are highlighted. \n\n"}
{"id": "1505.03044", "contents": "Title: Duality between Temporal Networks and Signals: Extraction of the\n  Temporal Network Structures Abstract: We develop a framework to track the structure of temporal networks with a\nsignal processing approach. The method is based on the duality between networks\nand signals using a multidimensional scaling technique. This enables a study of\nthe network structure using frequency patterns of the corresponding signals. An\nextension is proposed for temporal networks, thereby enabling a tracking of the\nnetwork structure over time. A method to automatically extract the most\nsignificant frequency patterns and their activation coefficients over time is\nthen introduced, using nonnegative matrix factorization of the temporal\nspectra. The framework, inspired by audio decomposition, allows transforming\nback these frequency patterns into networks, to highlight the evolution of the\nunderlying structure of the network over time. The effectiveness of the method\nis first evidenced on a toy example, prior being used to study a temporal\nnetwork of face-to-face contacts. The extraction of sub-networks highlights\nsignificant structures decomposed on time intervals. \n\n"}
{"id": "1505.03410", "contents": "Title: Mind the duality gap: safer rules for the Lasso Abstract: Screening rules allow to early discard irrelevant variables from the\noptimization in Lasso problems, or its derivatives, making solvers faster. In\nthis paper, we propose new versions of the so-called $\\textit{safe rules}$ for\nthe Lasso. Based on duality gap considerations, our new rules create safe test\nregions whose diameters converge to zero, provided that one relies on a\nconverging solver. This property helps screening out more variables, for a\nwider range of regularization parameter values. In addition to faster\nconvergence, we prove that we correctly identify the active sets (supports) of\nthe solutions in finite time. While our proposed strategy can cope with any\nsolver, its performance is demonstrated using a coordinate descent algorithm\nparticularly adapted to machine learning use cases. Significant computing time\nreductions are obtained with respect to previous safe rules. \n\n"}
{"id": "1505.04824", "contents": "Title: An Asynchronous Mini-Batch Algorithm for Regularized Stochastic\n  Optimization Abstract: Mini-batch optimization has proven to be a powerful paradigm for large-scale\nlearning. However, the state of the art parallel mini-batch algorithms assume\nsynchronous operation or cyclic update orders. When worker nodes are\nheterogeneous (due to different computational capabilities or different\ncommunication delays), synchronous and cyclic operations are inefficient since\nthey will leave workers idle waiting for the slower nodes to complete their\ncomputations. In this paper, we propose an asynchronous mini-batch algorithm\nfor regularized stochastic optimization problems with smooth loss functions\nthat eliminates idle waiting and allows workers to run at their maximal update\nrates. We show that by suitably choosing the step-size values, the algorithm\nachieves a rate of the order $O(1/\\sqrt{T})$ for general convex regularization\nfunctions, and the rate $O(1/T)$ for strongly convex regularization functions,\nwhere $T$ is the number of iterations. In both cases, the impact of asynchrony\non the convergence rate of our algorithm is asymptotically negligible, and a\nnear-linear speedup in the number of workers can be expected. Theoretical\nresults are confirmed in real implementations on a distributed computing\ninfrastructure. \n\n"}
{"id": "1505.05629", "contents": "Title: Regulating Greed Over Time in Multi-Armed Bandits Abstract: In retail, there are predictable yet dramatic time-dependent patterns in\ncustomer behavior, such as periodic changes in the number of visitors, or\nincreases in customers just before major holidays. The current paradigm of\nmulti-armed bandit analysis does not take these known patterns into account.\nThis means that for applications in retail, where prices are fixed for periods\nof time, current bandit algorithms will not suffice. This work provides a\nremedy that takes the time-dependent patterns into account, and we show how\nthis remedy is implemented for the UCB, $\\varepsilon$-greedy, and UCB-L\nalgorithms, and also through a new policy called the variable arm pool\nalgorithm. In the corrected methods, exploitation (greed) is regulated over\ntime, so that more exploitation occurs during higher reward periods, and more\nexploration occurs in periods of low reward. In order to understand why regret\nis reduced with the corrected methods, we present a set of bounds that provide\ninsight into why we would want to exploit during periods of high reward, and\ndiscuss the impact on regret. Our proposed methods perform well in experiments,\nand were inspired by a high-scoring entry in the Exploration and Exploitation 3\ncontest using data from Yahoo$!$ Front Page. That entry heavily used\ntime-series methods to regulate greed over time, which was substantially more\neffective than other contextual bandit methods. \n\n"}
{"id": "1505.05956", "contents": "Title: Approximate Closest Community Search in Networks Abstract: Recently, there has been significant interest in the study of the community\nsearch problem in social and information networks: given one or more query\nnodes, find densely connected communities containing the query nodes. However,\nmost existing studies do not address the \"free rider\" issue, that is, nodes far\naway from query nodes and irrelevant to them are included in the detected\ncommunity. Some state-of-the-art models have attempted to address this issue,\nbut not only are their formulated problems NP-hard, they do not admit any\napproximations without restrictive assumptions, which may not always hold in\npractice.\n  In this paper, given an undirected graph G and a set of query nodes Q, we\nstudy community search using the k-truss based community model. We formulate\nour problem of finding a closest truss community (CTC), as finding a connected\nk-truss subgraph with the largest k that contains Q, and has the minimum\ndiameter among such subgraphs. We prove this problem is NP-hard. Furthermore,\nit is NP-hard to approximate the problem within a factor $(2-\\varepsilon)$, for\nany $\\varepsilon >0 $. However, we develop a greedy algorithmic framework,\nwhich first finds a CTC containing Q, and then iteratively removes the furthest\nnodes from Q, from the graph. The method achieves 2-approximation to the\noptimal solution. To further improve the efficiency, we make use of a compact\ntruss index and develop efficient algorithms for k-truss identification and\nmaintenance as nodes get eliminated. In addition, using bulk deletion\noptimization and local exploration strategies, we propose two more efficient\nalgorithms. One of them trades some approximation quality for efficiency while\nthe other is a very efficient heuristic. Extensive experiments on 6 real-world\nnetworks show the effectiveness and efficiency of our community model and\nsearch algorithms. \n\n"}
{"id": "1505.06476", "contents": "Title: Emergence of bimodality in controlling complex networks Abstract: Our ability to control complex systems is a fundamental challenge of\ncontemporary science. Recently introduced tools to identify the driver nodes,\nnodes through which we can achieve full control, predict the existence of\nmultiple control configurations, prompting us to classify each node in a\nnetwork based on their role in control. Accordingly a node is critical,\nintermittent or redundant if it acts as a driver node in all, some or none of\nthe control configurations. Here we develop an analytical framework to identify\nthe category of each node, leading to the discovery of two distinct control\nmodes in complex systems: centralized vs distributed control. We predict the\ncontrol mode for an arbitrary network and show that one can alter it through\nsmall structural perturbations. The uncovered bimodality has implications from\nnetwork security to organizational research and offers new insights into the\ndynamics and control of complex systems. \n\n"}
{"id": "1506.03016", "contents": "Title: Accelerated Stochastic Gradient Descent for Minimizing Finite Sums Abstract: We propose an optimization method for minimizing the finite sums of smooth\nconvex functions. Our method incorporates an accelerated gradient descent (AGD)\nand a stochastic variance reduction gradient (SVRG) in a mini-batch setting.\nUnlike SVRG, our method can be directly applied to non-strongly and strongly\nconvex problems. We show that our method achieves a lower overall complexity\nthan the recently proposed methods that supports non-strongly convex problems.\nMoreover, this method has a fast rate of convergence for strongly convex\nproblems. Our experiments show the effectiveness of our method. \n\n"}
{"id": "1506.03736", "contents": "Title: GAP Safe screening rules for sparse multi-task and multi-class models Abstract: High dimensional regression benefits from sparsity promoting regularizations.\nScreening rules leverage the known sparsity of the solution by ignoring some\nvariables in the optimization, hence speeding up solvers. When the procedure is\nproven not to discard features wrongly the rules are said to be \\emph{safe}. In\nthis paper we derive new safe rules for generalized linear models regularized\nwith $\\ell_1$ and $\\ell_1/\\ell_2$ norms. The rules are based on duality gap\ncomputations and spherical safe regions whose diameters converge to zero. This\nallows to discard safely more variables, in particular for low regularization\nparameters. The GAP Safe rule can cope with any iterative solver and we\nillustrate its performance on coordinate descent for multi-task Lasso, binary\nand multinomial logistic regression, demonstrating significant speed ups on all\ntested datasets with respect to previous safe rules. \n\n"}
{"id": "1506.04093", "contents": "Title: Adaptive Stochastic Primal-Dual Coordinate Descent for Separable Saddle\n  Point Problems Abstract: We consider a generic convex-concave saddle point problem with separable\nstructure, a form that covers a wide-ranged machine learning applications.\nUnder this problem structure, we follow the framework of primal-dual updates\nfor saddle point problems, and incorporate stochastic block coordinate descent\nwith adaptive stepsize into this framework. We theoretically show that our\nproposal of adaptive stepsize potentially achieves a sharper linear convergence\nrate compared with the existing methods. Additionally, since we can select\n\"mini-batch\" of block coordinates to update, our method is also amenable to\nparallel processing for large-scale data. We apply the proposed method to\nregularized empirical risk minimization and show that it performs comparably\nor, more often, better than state-of-the-art methods on both synthetic and\nreal-world data sets. \n\n"}
{"id": "1506.04189", "contents": "Title: Multiplex Networks with Intrinsic Fitness: Modeling the Merit-Fame\n  Interplay via Latent Layers Abstract: We consider the problem of growing multiplex networks with intrinsic fitness\nand inter-layer coupling. The model comprises two layers; one that incorporates\nfitness and another in which attachments are preferential. In the first layer,\nattachment probabilities are proportional to fitness values, and in the second\nlayer, proportional to the sum of degrees in both layers. We provide analytical\nclosed-form solutions for the joint distributions of fitness and degrees. We\nalso derive closed-form expressions for the expected value of the degree as a\nfunction of fitness. The model alleviates two shortcomings that are present in\nthe current models of growing multiplex networks: homogeneity of connections,\nand homogeneity of fitness. In this paper, we posit and analyze a growth model\nthat is heterogeneous in both senses. \n\n"}
{"id": "1506.06305", "contents": "Title: Social media affects the timing, location, and severity of school\n  shootings Abstract: Over the past two decades, school shootings within the United States have\nrepeatedly devastated communities and shaken public opinion. Many of these\nattacks appear to be `lone wolf' ones driven by specific individual\nmotivations, and the identification of precursor signals and hence actionable\npolicy measures would thus seem highly unlikely. Here, we take a system-wide\nview and investigate the timing of school attacks and the dynamical feedback\nwith social media. We identify a trend divergence in which college attacks have\ncontinued to accelerate over the last 25 years while those carried out on K-12\nschools have slowed down. We establish the copycat effect in school shootings\nand uncover a statistical association between social media chatter and the\nprobability of an attack in the following days. While hinting at causality,\nthis relationship may also help mitigate the frequency and intensity of future\nattacks. \n\n"}
{"id": "1506.07677", "contents": "Title: Manifold Optimization for Gaussian Mixture Models Abstract: We take a new look at parameter estimation for Gaussian Mixture Models\n(GMMs). In particular, we propose using \\emph{Riemannian manifold optimization}\nas a powerful counterpart to Expectation Maximization (EM). An out-of-the-box\ninvocation of manifold optimization, however, fails spectacularly: it converges\nto the same solution but vastly slower. Driven by intuition from manifold\nconvexity, we then propose a reparamerization that has remarkable empirical\nconsequences. It makes manifold optimization not only match EM---a highly\nencouraging result in itself given the poor record nonlinear programming\nmethods have had against EM so far---but also outperform EM in many practical\nsettings, while displaying much less variability in running times. We further\nhighlight the strengths of manifold optimization by developing a somewhat tuned\nmanifold LBFGS method that proves even more competitive and reliable than\nexisting manifold optimization tools. We hope that our results encourage a\nwider consideration of manifold optimization for parameter estimation problems. \n\n"}
{"id": "1506.08621", "contents": "Title: A spectral method for community detection in moderately-sparse\n  degree-corrected stochastic block models Abstract: We consider community detection in Degree-Corrected Stochastic Block Models\n(DC-SBM). We propose a spectral clustering algorithm based on a suitably\nnormalized adjacency matrix. We show that this algorithm consistently recovers\nthe block-membership of all but a vanishing fraction of nodes, in the regime\nwhere the lowest degree is of order log$(n)$ or higher. Recovery succeeds even\nfor very heterogeneous degree-distributions. The used algorithm does not rely\non parameters as input. In particular, it does not need to know the number of\ncommunities. \n\n"}
{"id": "1507.00210", "contents": "Title: Natural Neural Networks Abstract: We introduce Natural Neural Networks, a novel family of algorithms that speed\nup convergence by adapting their internal representation during training to\nimprove conditioning of the Fisher matrix. In particular, we show a specific\nexample that employs a simple and efficient reparametrization of the neural\nnetwork weights by implicitly whitening the representation obtained at each\nlayer, while preserving the feed-forward computation of the network. Such\nnetworks can be trained efficiently via the proposed Projected Natural Gradient\nDescent algorithm (PRONG), which amortizes the cost of these reparametrizations\nover many parameter updates and is closely related to the Mirror Descent online\nlearning algorithm. We highlight the benefits of our method on both\nunsupervised and supervised learning tasks, and showcase its scalability by\ntraining on the large-scale ImageNet Challenge dataset. \n\n"}
{"id": "1507.00610", "contents": "Title: Network growth with preferential attachment and without \"rich get\n  richer\" mechanism Abstract: We propose a simple preferential attachment model of growing network using\nthe complementary probability of Barab\\'asi-Albert (BA) model, i.e., $\\Pi(k_i)\n\\propto 1-\\frac{k_i}{\\sum_j k_j}$. In this network, new nodes are\npreferentially attached to not well connected nodes. Numerical simulations, in\nperfect agreement with the master equation solution, give an exponential degree\ndistribution. This suggests that the power law degree distribution is a\nconsequence of preferential attachment probability together with \"rich get\nricher\" phenomena. We also calculate the average degree of a target node at\ntime t $(<k_s(t)>)$ and its fluctuations, to have a better view of the\nmicroscopic evolution of the network, and we also compare the results with BA\nmodel. \n\n"}
{"id": "1507.00677", "contents": "Title: Distributional Smoothing with Virtual Adversarial Training Abstract: We propose local distributional smoothness (LDS), a new notion of smoothness\nfor statistical model that can be used as a regularization term to promote the\nsmoothness of the model distribution. We named the LDS based regularization as\nvirtual adversarial training (VAT). The LDS of a model at an input datapoint is\ndefined as the KL-divergence based robustness of the model distribution against\nlocal perturbation around the datapoint. VAT resembles adversarial training,\nbut distinguishes itself in that it determines the adversarial direction from\nthe model distribution alone without using the label information, making it\napplicable to semi-supervised learning. The computational cost for VAT is\nrelatively low. For neural network, the approximated gradient of the LDS can be\ncomputed with no more than three pairs of forward and back propagations. When\nwe applied our technique to supervised and semi-supervised learning for the\nMNIST dataset, it outperformed all the training methods other than the current\nstate of the art method, which is based on a highly advanced generative model.\nWe also applied our method to SVHN and NORB, and confirmed our method's\nsuperior performance over the current state of the art semi-supervised method\napplied to these datasets. \n\n"}
{"id": "1507.01484", "contents": "Title: Temporal Fidelity in Dynamic Social Networks Abstract: It has recently become possible to record detailed social interactions in\nlarge social systems with high resolution. As we study these datasets, human\nsocial interactions display patterns that emerge at multiple time scales, from\nminutes to months. On a fundamental level, understanding of the network\ndynamics can be used to inform the process of measuring social networks. The\ndetails of measurement are of particular importance when considering dynamic\nprocesses where minute-to-minute details are important, because collection of\nphysical proximity interactions with high temporal resolution is difficult and\nexpensive. Here, we consider the dynamic network of proximity-interactions\nbetween approximately 500 individuals participating in the Copenhagen Networks\nStudy. We show that in order to accurately model spreading processes in the\nnetwork, the dynamic processes that occur on the order of minutes are essential\nand must be included in the analysis. \n\n"}
{"id": "1507.01784", "contents": "Title: Rethinking LDA: moment matching for discrete ICA Abstract: We consider moment matching techniques for estimation in Latent Dirichlet\nAllocation (LDA). By drawing explicit links between LDA and discrete versions\nof independent component analysis (ICA), we first derive a new set of\ncumulant-based tensors, with an improved sample complexity. Moreover, we reuse\nstandard ICA techniques such as joint diagonalization of tensors to improve\nover existing methods based on the tensor power method. In an extensive set of\nexperiments on both synthetic and real datasets, we show that our new\ncombination of tensors and orthogonal joint diagonalization techniques\noutperforms existing moment matching methods. \n\n"}
{"id": "1507.01994", "contents": "Title: Metrics in the Space of High Order Networks Abstract: This paper presents methods to compare high order networks, defined as\nweighted complete hypergraphs collecting relationship functions between\nelements of tuples. They can be considered as generalizations of conventional\nnetworks where only relationship functions between pairs are defined. Important\nproperties between relationships of tuples of different lengths are\nestablished, particularly when relationships encode dissimilarities or\nproximities between nodes. Two families of distances are then introduced in the\nspace of high order networks. The distances measure differences between\nnetworks. We prove that they are valid metrics in the spaces of high order\ndissimilarity and proximity networks modulo permutation isomorphisms. Practical\nimplications are explored by comparing the coauthorship networks of two popular\nsignal processing researchers. The metrics succeed in identifying their\nrespective collaboration patterns. \n\n"}
{"id": "1507.02356", "contents": "Title: Intrinsic Non-stationary Covariance Function for Climate Modeling Abstract: Designing a covariance function that represents the underlying correlation is\na crucial step in modeling complex natural systems, such as climate models.\nGeospatial datasets at a global scale usually suffer from non-stationarity and\nnon-uniformly smooth spatial boundaries. A Gaussian process regression using a\nnon-stationary covariance function has shown promise for this task, as this\ncovariance function adapts to the variable correlation structure of the\nunderlying distribution. In this paper, we generalize the non-stationary\ncovariance function to address the aforementioned global scale geospatial\nissues. We define this generalized covariance function as an intrinsic\nnon-stationary covariance function, because it uses intrinsic statistics of the\nsymmetric positive definite matrices to represent the characteristic length\nscale and, thereby, models the local stochastic process. Experiments on a\nsynthetic and real dataset of relative sea level changes across the world\ndemonstrate improvements in the error metrics for the regression estimates\nusing our newly proposed approach. \n\n"}
{"id": "1507.04596", "contents": "Title: Channel-Specific Daily Patterns in Mobile Phone Communication Abstract: Humans follow circadian rhythms, visible in their activity levels as well as\nphysiological and psychological factors. Such rhythms are also visible in\nelectronic communication records, where the aggregated activity levels of e.g.\nmobile telephone calls or Wikipedia edits are known to follow their own daily\npatterns. Here, we study the daily communication patterns of 24 individuals\nover 18 months, and show that each individual has a different, persistent\ncommunication pattern. These patterns may differ for calls and text messages,\nwhich points towards calls and texts serving a different role in communication.\nFor both calls and texts, evenings play a special role. There are also\ndifferences in the daily patterns of males and females both for calls and\ntexts, both in how they communicate with individuals of the same gender vs.\nopposite gender, and also in how communication is allocated at social ties of\ndifferent nature (kin ties vs. non-kin ties). Taken together, our results show\nthat there is an unexpected richness to the daily communication patterns, from\ndifferent types of ties being activated at different times of day to different\nroles of communication channels and gender differences. \n\n"}
{"id": "1507.05790", "contents": "Title: A System for Sensing Human Sentiments to Augment a Model for Predicting\n  Rare Lake Events Abstract: Fish kill events (FKE) in the caldera lake of Taal occur rarely (only 0.5\\%\nin the last 10 years) but each event has a long-term effect on the\nenvironmental health of the lake ecosystem, as well as a devastating effect on\nthe financial and emotional aspects of the residents whose livelihood rely on\naquaculture farming. Predicting with high accuracy when within seven days and\nwhere on the vast expanse of the lake will FKEs strike will be a very important\nearly warning tool for the lake's aquaculture industry. Mathematical models to\npredict the occurrences of FKEs developed by several studies done in the past\nuse as predictors the physico-chemical characteristics of the lake water, as\nwell as the meteorological parameters above it. Some of the models, however,\ndid not provide acceptable predictive accuracy and enough early warning because\nthey were developed with unbalanced binary data set, i.e., characterized by\ndense negative examples (no FKE) and highly sparse positive examples (with\nFKE). Other models require setting up an expensive sensor network to measure\nthe water parameters not only at the surface but also at several depths.\nPresented in this paper is a system for capturing, measuring, and visualizing\nthe contextual sentiment polarity (CSP) of dated and geolocated social media\nmicroposts of residents within 10km radius of the Taal Volcano crater\n($14^\\circ$N, $121^\\circ$E). High frequency negative CSP co-occur with FKE for\ntwo occasions making human expressions a viable non-physical sensors for\nimpending FKE to augment existing mathematical models. \n\n"}
{"id": "1507.06346", "contents": "Title: Evaluation of Spectral Learning for the Identification of Hidden Markov\n  Models Abstract: Hidden Markov models have successfully been applied as models of discrete\ntime series in many fields. Often, when applied in practice, the parameters of\nthese models have to be estimated. The currently predominating identification\nmethods, such as maximum-likelihood estimation and especially\nexpectation-maximization, are iterative and prone to have problems with local\nminima. A non-iterative method employing a spectral subspace-like approach has\nrecently been proposed in the machine learning literature. This paper evaluates\nthe performance of this algorithm, and compares it to the performance of the\nexpectation-maximization algorithm, on a number of numerical examples. We find\nthat the performance is mixed; it successfully identifies some systems with\nrelatively few available observations, but fails completely for some systems\neven when a large amount of observations is available. An open question is how\nthis discrepancy can be explained. We provide some indications that it could be\nrelated to how well-conditioned some system parameters are. \n\n"}
{"id": "1508.00299", "contents": "Title: When Crowdsourcing Meets Mobile Sensing: A Social Network Perspective Abstract: Mobile sensing is an emerging technology that utilizes agent-participatory\ndata for decision making or state estimation, including multimedia\napplications. This article investigates the structure of mobile sensing schemes\nand introduces crowdsourcing methods for mobile sensing. Inspired by social\nnetwork, one can establish trust among participatory agents to leverage the\nwisdom of crowds for mobile sensing. A prototype of social network inspired\nmobile multimedia and sensing application is presented for illustrative\npurpose. Numerical experiments on real-world datasets show improved performance\nof mobile sensing via crowdsourcing. Challenges for mobile sensing with respect\nto Internet layers are discussed. \n\n"}
{"id": "1508.01819", "contents": "Title: Spectral Clustering and Block Models: A Review And A New Algorithm Abstract: We focus on spectral clustering of unlabeled graphs and review some results\non clustering methods which achieve weak or strong consistent identification in\ndata generated by such models. We also present a new algorithm which appears to\nperform optimally both theoretically using asymptotic theory and empirically. \n\n"}
{"id": "1508.01843", "contents": "Title: Vaporous Marketing: Uncovering Pervasive Electronic Cigarette\n  Advertisements on Twitter Abstract: Background: Twitter has become the \"wild-west\" of marketing and promotional\nstrategies for advertisement agencies. Electronic cigarettes have been heavily\nmarketed across Twitter feeds, offering discounts, \"kid-friendly\" flavors,\nalgorithmically generated false testimonials, and free samples. Methods:All\nelectronic cigarette keyword related tweets from a 10% sample of Twitter\nspanning January 2012 through December 2014 (approximately 850,000 total\ntweets) were identified and categorized as Automated or Organic by combining a\nkeyword classification and a machine trained Human Detection algorithm. A\nsentiment analysis using Hedonometrics was performed on Organic tweets to\nquantify the change in consumer sentiments over time. Commercialized tweets\nwere topically categorized with key phrasal pattern matching. Results:The\noverwhelming majority (80%) of tweets were classified as automated or\npromotional in nature. The majority of these tweets were coded as\ncommercialized (83.65% in 2013), up to 33% of which offered discounts or free\nsamples and appeared on over a billion twitter feeds as impressions. The\npositivity of Organic (human) classified tweets has decreased over time (5.84\nin 2013 to 5.77 in 2014) due to a relative increase in the negative words\nban,tobacco,doesn't,drug,against,poison,tax and a relative decrease in the\npositive words like haha,good,cool. Automated tweets are more positive than\norganic (6.17 versus 5.84) due to a relative increase in the marketing words\nbest,win,buy,sale,health,discount and a relative decrease in negative words\nlike bad, hate, stupid, don't. Conclusions:Due to the youth presence on Twitter\nand the clinical uncertainty of the long term health complications of\nelectronic cigarette consumption, the protection of public health warrants\nscrutiny and potential regulation of social media marketing. \n\n"}
{"id": "1508.02933", "contents": "Title: No Regret Bound for Extreme Bandits Abstract: Algorithms for hyperparameter optimization abound, all of which work well\nunder different and often unverifiable assumptions. Motivated by the general\nchallenge of sequentially choosing which algorithm to use, we study the more\nspecific task of choosing among distributions to use for random hyperparameter\noptimization. This work is naturally framed in the extreme bandit setting,\nwhich deals with sequentially choosing which distribution from a collection to\nsample in order to minimize (maximize) the single best cost (reward). Whereas\nthe distributions in the standard bandit setting are primarily characterized by\ntheir means, a number of subtleties arise when we care about the minimal cost\nas opposed to the average cost. For example, there may not be a well-defined\n\"best\" distribution as there is in the standard bandit setting. The best\ndistribution depends on the rewards that have been obtained and on the\nremaining time horizon. Whereas in the standard bandit setting, it is sensible\nto compare policies with an oracle which plays the single best arm, in the\nextreme bandit setting, there are multiple sensible oracle models. We define a\nsensible notion of \"extreme regret\" in the extreme bandit setting, which\nparallels the concept of regret in the standard bandit setting. We then prove\nthat no policy can asymptotically achieve no extreme regret. \n\n"}
{"id": "1508.04294", "contents": "Title: Effective spreading from multiple leaders identified by percolation in\n  social networks Abstract: Social networks constitute a new platform for information propagation, but\nits success is crucially dependent on the choice of spreaders who initiate the\nspreading of information. In this paper, we remove edges in a network at random\nand the network segments into isolated clusters. The most important nodes in\neach cluster then form a group of influential spreaders, such that news\npropagating from them would lead to an extensive coverage and minimal\nredundancy. The method well utilizes the similarities between the\npre-percolated state and the coverage of information propagation in each social\ncluster to obtain a set of distributed and coordinated spreaders. Our tests on\nthe Facebook networks show that this method outperforms conventional methods\nbased on centrality. The suggested way of identifying influential spreaders\nthus sheds light on a new paradigm of information propagation on social\nnetworks. \n\n"}
{"id": "1508.04819", "contents": "Title: Analyzing Organizational Routines in Online Knowledge Collaborations: A\n  Case for Sequence Analysis in CSCW Abstract: Research into socio-technical systems like Wikipedia has overlooked important\nstructural patterns in the coordination of distributed work. This paper argues\nfor a conceptual reorientation towards sequences as a fundamental unit of\nanalysis for understanding work routines in online knowledge collaboration. We\noutline a research agenda for researchers in computer-supported cooperative\nwork (CSCW) to understand the relationships, patterns, antecedents, and\nconsequences of sequential behavior using methods already developed in fields\nlike bio-informatics. Using a data set of 37,515 revisions from 16,616 unique\neditors to 96 Wikipedia articles as a case study, we analyze the prevalence and\nsignificance of different sequences of editing patterns. We illustrate the\nmixed method potential of sequence approaches by interpreting the frequent\npatterns as general classes of behavioral motifs. We conclude by discussing the\nmethodological opportunities for using sequence analysis for expanding existing\napproaches to analyzing and theorizing about co-production routines in online\nknowledge collaboration. \n\n"}
{"id": "1508.05003", "contents": "Title: AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization Abstract: We study distributed stochastic convex optimization under the delayed\ngradient model where the server nodes perform parameter updates, while the\nworker nodes compute stochastic gradients. We discuss, analyze, and experiment\nwith a setup motivated by the behavior of real-world distributed computation\nnetworks, where the machines are differently slow at different time. Therefore,\nwe allow the parameter updates to be sensitive to the actual delays\nexperienced, rather than to worst-case bounds on the maximum delay. This\nsensitivity leads to larger stepsizes, that can help gain rapid initial\nconvergence without having to wait too long for slower machines, while\nmaintaining the same asymptotic complexity. We obtain encouraging improvements\nto overall convergence for distributed experiments on real datasets with up to\nbillions of examples and features. \n\n"}
{"id": "1508.06878", "contents": "Title: Sex differences in social focus across the lifecycle in humans Abstract: Age and gender are two important factors that play crucial roles in the way\norganisms allocate their social effort. In this study, we analyse a large\nmobile phone dataset to explore the way lifehistory influences human sociality\nand the way social networks are structured. Our results indicate that these\naspects of human behaviour are strongly related to the age and gender such that\nyounger individuals have more contacts and, among them, males more than\nfemales. However, the rate of decrease in the number of contacts with age\ndiffers between males and females, such that there is a reversal in the number\nof contacts around the late 30s. We suggest that this pattern can be attributed\nto the difference in reproductive investments that are made by the two sexes.\nWe analyse the inequality in social investment patterns and suggest that the\nage and gender-related differences that we find reflect the constraints imposed\nby reproduction in a context where time (a form of social capital) is limited. \n\n"}
{"id": "1509.00114", "contents": "Title: Multi-Sensor Slope Change Detection Abstract: We develop a mixture procedure for multi-sensor systems to monitor data\nstreams for a change-point that causes a gradual degradation to a subset of the\nstreams. Observations are assumed to be initially normal random variables with\nknown constant means and variances. After the change-point, observations in the\nsubset will have increasing or decreasing means. The subset and the\nrate-of-changes are unknown. Our procedure uses a mixture statistics, which\nassumes that each sensor is affected by the change-point with probability\n$p_0$. Analytic expressions are obtained for the average run length (ARL) and\nthe expected detection delay (EDD) of the mixture procedure, which are\ndemonstrated to be quite accurate numerically. We establish the asymptotic\noptimality of the mixture procedure. Numerical examples demonstrate the good\nperformance of the proposed procedure. We also discuss an adaptive mixture\nprocedure using empirical Bayes. This paper extends our earlier work on\ndetecting an abrupt change-point that causes a mean-shift, by tackling the\nchallenges posed by the non-stationarity of the slope-change problem. \n\n"}
{"id": "1509.00670", "contents": "Title: Stay Awhile and Listen: User Interactions in a Crowdsourced Platform\n  Offering Emotional Support Abstract: Internet and online-based social systems are rising as the dominant mode of\ncommunication in society. However, the public or semi-private environment under\nwhich most online communications operate under do not make them suitable\nchannels for speaking with others about personal or emotional problems. This\nhas led to the emergence of online platforms for emotional support offering\nfree, anonymous, and confidential conversations with live listeners. Yet very\nlittle is known about the way these platforms are utilized, and if their\nfeatures and design foster strong user engagement. This paper explores the\nutilization and the interaction features of hundreds of thousands of users on 7\nCups of Tea, a leading online platform offering online emotional support. It\ndissects the level of activity of hundreds of thousands of users, the patterns\nby which they engage in conversation with each other, and uses machine learning\nmethods to find factors promoting engagement. The study may be the first to\nmeasure activities and interactions in a large-scale online social system that\nfosters peer-to-peer emotional support. \n\n"}
{"id": "1509.01608", "contents": "Title: Network Structure and Resilience of Mafia Syndicates Abstract: In this paper we present the results of the study of Sicilian Mafia\norganization by using Social Network Analysis. The study investigates the\nnetwork structure of a Mafia organization, describing its evolution and\nhighlighting its plasticity to interventions targeting membership and its\nresilience to disruption caused by police operations. We analyze two different\ndatasets about Mafia gangs built by examining different digital trails and\njudicial documents spanning a period of ten years: the former dataset includes\nthe phone contacts among suspected individuals, the latter is constituted by\nthe relationships among individuals actively involved in various criminal\noffenses. Our report illustrates the limits of traditional investigation\nmethods like tapping: criminals high up in the organization hierarchy do not\noccupy the most central positions in the criminal network, and oftentimes do\nnot appear in the reconstructed criminal network at all. However, we also\nsuggest possible strategies of intervention, as we show that although criminal\nnetworks (i.e., the network encoding mobsters and crime relationships) are\nextremely resilient to different kind of attacks, contact networks (i.e., the\nnetwork reporting suspects and reciprocated phone calls) are much more\nvulnerable and their analysis can yield extremely valuable insights. \n\n"}
{"id": "1509.03025", "contents": "Title: Fast low-rank estimation by projected gradient descent: General\n  statistical and algorithmic guarantees Abstract: Optimization problems with rank constraints arise in many applications,\nincluding matrix regression, structured PCA, matrix completion and matrix\ndecomposition problems. An attractive heuristic for solving such problems is to\nfactorize the low-rank matrix, and to run projected gradient descent on the\nnonconvex factorized optimization problem. The goal of this problem is to\nprovide a general theoretical framework for understanding when such methods\nwork well, and to characterize the nature of the resulting fixed point. We\nprovide a simple set of conditions under which projected gradient descent, when\ngiven a suitable initialization, converges geometrically to a statistically\nuseful solution. Our results are applicable even when the initial solution is\noutside any region of local convexity, and even when the problem is globally\nconcave. Working in a non-asymptotic framework, we show that our conditions are\nsatisfied for a wide range of concrete models, including matrix regression,\nstructured PCA, matrix completion with real and quantized observations, matrix\ndecomposition, and graph clustering problems. Simulation results show excellent\nagreement with the theoretical predictions. \n\n"}
{"id": "1509.03147", "contents": "Title: Two betweenness centrality measures based on Randomized Shortest Paths Abstract: This paper introduces two new closely related betweenness centrality measures\nbased on the Randomized Shortest Paths (RSP) framework, which fill a gap\nbetween traditional network centrality measures based on shortest paths and\nmore recent methods considering random walks or current flows. The framework\ndefines Boltzmann probability distributions over paths of the network which\nfocus on the shortest paths, but also take into account longer paths depending\non an inverse temperature parameter. RSP's have previously proven to be useful\nin defining distance measures on networks. In this work we study their utility\nin quantifying the importance of the nodes of a network. The proposed RSP\nbetweenness centralities combine, in an optimal way, the ideas of using the\nshortest and purely random paths for analysing the roles of network nodes,\navoiding issues involving these two paradigms. We present the derivations of\nthese measures and how they can be computed in an efficient way. In addition,\nwe show with real world examples the potential of the RSP betweenness\ncentralities in identifying interesting nodes of a network that more\ntraditional methods might fail to notice. \n\n"}
{"id": "1509.03196", "contents": "Title: The paradox of controlling complex networks: control inputs versus\n  energy requirement Abstract: In this paper, we investigate the linear controllability framework for\ncomplex networks from a physical point of view. There are three main results.\n(1) If one applies control signals as determined from the structural\ncontrollability theory, there is a high probability that the control energy\nwill diverge. Especially, if a network is deemed controllable using a single\ndriving signal, then most likely the energy will diverge. (2) The energy\nrequired for control exhibits a power-law scaling behavior. (3) Applying\nadditional control signals at proper nodes in the network can reduce and\noptimize the energy cost. We identify the fundamental structures embedded in\nthe network, the longest control chains, which determine the control energy and\ngive rise to the power-scaling behavior. (To our knowledge, this was not\nreported in any previous work on control of complex networks.) In addition, the\nissue of control precision is addressed. These results are supported by\nextensive simulations from model and real networks, physical reasoning, and\nmathematical analyses.\n  Notes on the submission history of this work: This work started in late 2012.\nThe phenomena of power-law energy scaling and energy divergence with a single\ncontroller were discovered in 2013. Strategies to reduce and optimize control\nenergy was articulated and tested in 2013. The senior co-author (YCL) gave\ntalks about these results at several conferences, including the NETSCI 2014\nSatellite entitled \"Controlling Complex Networks\" on June 2, 2014. The paper\nwas submitted to PNAS in September 2014 and was turned down. It was revised and\nsubmitted to PRX in early 2015 and was rejected. After that it was revised and\nsubmitted to Nature Communications in May 2015 and again was turned down. \n\n"}
{"id": "1509.03357", "contents": "Title: Dynamics of social contagions with heterogeneous adoption thresholds:\n  Crossover phenomena in phase transition Abstract: Heterogeneous adoption thresholds exist widely in social contagions, but were\nalways neglected in previous studies. We first propose a non-Markovian\nspreading threshold model with general adoption threshold distribution. In\norder to understand the effects of heterogeneous adoption thresholds\nquantitatively, an edge-based compartmental theory is developed for the\nproposed model. We use a binary spreading threshold model as a specific\nexample, in which some individuals have a low adoption threshold (i.e.,\nactivists) while the remaining ones hold a relatively high adoption threshold\n(i.e., bigots), to demonstrate that heterogeneous adoption thresholds markedly\naffect the final adoption size and phase transition. Interestingly, the\nfirst-order, second-order and hybrid phase transitions can be found in the\nsystem. More importantly, there are two different kinds of crossover phenomena\nin phase transition for distinct values of bigots' adoption threshold: a change\nfrom first-order or hybrid phase transition to the second-order phase\ntransition. The theoretical predictions based on the suggested theory agree\nvery well with the results of numerical simulations. \n\n"}
{"id": "1509.04037", "contents": "Title: Measuring Partial Balance in Signed Networks Abstract: Is the enemy of an enemy necessarily a friend? If not, to what extent does\nthis tend to hold? Such questions were formulated in terms of signed (social)\nnetworks and necessary and sufficient conditions for a network to be \"balanced\"\nwere obtained around 1960. Since then the idea that signed networks tend over\ntime to become more balanced has been widely used in several application areas.\nHowever, investigation of this hypothesis has been complicated by the lack of a\nstandard measure of partial balance, since complete balance is almost never\nachieved in practice. We formalize the concept of a measure of partial balance,\ndiscuss various measures, compare the measures on synthetic datasets, and\ninvestigate their axiomatic properties. The synthetic data involves\nErd\\H{o}s-R\\'enyi and specially structured random graphs. We show that some\nmeasures behave better than others in terms of axioms and ability to\ndifferentiate between graphs. We also use well-known datasets from the\nsociology and biology literature, such as Read's New Guinean tribes, gene\nregulatory networks related to two organisms, and a network involving senate\nbill co-sponsorship. Our results show that substantially different levels of\npartial balance is observed under cycle-based, eigenvalue-based, and\nfrustration-based measures. We make some recommendations for measures to be\nused in future work. \n\n"}
{"id": "1509.04740", "contents": "Title: Modeling sequences and temporal networks with dynamic community\n  structures Abstract: In evolving complex systems such as air traffic and social organizations,\ncollective effects emerge from their many components' dynamic interactions.\nWhile the dynamic interactions can be represented by temporal networks with\nnodes and links that change over time, they remain highly complex. It is\ntherefore often necessary to use methods that extract the temporal networks'\nlarge-scale dynamic community structure. However, such methods are subject to\noverfitting or suffer from effects of arbitrary, a priori imposed timescales,\nwhich should instead be extracted from data. Here we simultaneously address\nboth problems and develop a principled data-driven method that determines\nrelevant timescales and identifies patterns of dynamics that take place on\nnetworks as well as shape the networks themselves. We base our method on an\narbitrary-order Markov chain model with community structure, and develop a\nnonparametric Bayesian inference framework that identifies the simplest such\nmodel that can explain temporal interaction data. \n\n"}
{"id": "1509.05096", "contents": "Title: Out of vocabulary words decrease, running texts prevail and hashtags\n  coalesce: Twitter as an evolving sociolinguistic system Abstract: Twitter is one of the most popular social media. Due to the ease of\navailability of data, Twitter is used significantly for research purposes.\nTwitter is known to evolve in many aspects from what it was at its birth;\nnevertheless, how it evolved its own linguistic style is still relatively\nunknown. In this paper, we study the evolution of various sociolinguistic\naspects of Twitter over large time scales. To the best of our knowledge, this\nis the first comprehensive study on the evolution of such aspects of this OSN.\nWe performed quantitative analysis both on the word level as well as on the\nhashtags since it is perhaps one of the most important linguistic units of this\nsocial media. We studied the (in)formality aspects of the linguistic styles in\nTwitter and find that it is neither fully formal nor completely informal; while\non one hand, we observe that Out-Of-Vocabulary words are decreasing over time\n(pointing to a formal style), on the other hand it is quite evident that\nwhitespace usage is getting reduced with a huge prevalence of running texts\n(pointing to an informal style). We also analyze and propose quantitative\nreasons for repetition and coalescing of hashtags in Twitter. We believe that\nsuch phenomena may be strongly tied to different evolutionary aspects of human\nlanguages. \n\n"}
{"id": "1509.05113", "contents": "Title: Revealed Preference at Scale: Learning Personalized Preferences from\n  Assortment Choices Abstract: We consider the problem of learning the preferences of a heterogeneous\npopulation by observing choices from an assortment of products, ads, or other\nofferings. Our observation model takes a form common in assortment planning\napplications: each arriving customer is offered an assortment consisting of a\nsubset of all possible offerings; we observe only the assortment and the\ncustomer's single choice.\n  In this paper we propose a mixture choice model with a natural underlying\nlow-dimensional structure, and show how to estimate its parameters. In our\nmodel, the preferences of each customer or segment follow a separate parametric\nchoice model, but the underlying structure of these parameters over all the\nmodels has low dimension. We show that a nuclear-norm regularized maximum\nlikelihood estimator can learn the preferences of all customers using a number\nof observations much smaller than the number of item-customer combinations.\nThis result shows the potential for structural assumptions to speed up learning\nand improve revenues in assortment planning and customization. We provide a\nspecialized factored gradient descent algorithm and study the success of the\napproach empirically. \n\n"}
{"id": "1509.06088", "contents": "Title: Significance Analysis of High-Dimensional, Low-Sample Size Partially\n  Labeled Data Abstract: Classification and clustering are both important topics in statistical\nlearning. A natural question herein is whether predefined classes are really\ndifferent from one another, or whether clusters are really there. Specifically,\nwe may be interested in knowing whether the two classes defined by some class\nlabels (when they are provided), or the two clusters tagged by a clustering\nalgorithm (where class labels are not provided), are from the same underlying\ndistribution. Although both are challenging questions for the high-dimensional,\nlow-sample size data, there has been some recent development for both. However,\nwhen it is costly to manually place labels on observations, it is often that\nonly a small portion of the class labels is available. In this article, we\npropose a significance analysis approach for such type of data, namely\npartially labeled data. Our method makes use of the whole data and tries to\ntest the class difference as if all the labels were observed. Compared to a\ntesting method that ignores the label information, our method provides a\ngreater power, meanwhile, maintaining the size, illustrated by a comprehensive\nsimulation study. Theoretical properties of the proposed method are studied\nwith emphasis on the high-dimensional, low-sample size setting. Our simulated\nexamples help to understand when and how the information extracted from the\nlabeled data can be effective. A real data example further illustrates the\nusefulness of the proposed method. \n\n"}
{"id": "1509.07087", "contents": "Title: Deep Temporal Sigmoid Belief Networks for Sequence Modeling Abstract: Deep dynamic generative models are developed to learn sequential dependencies\nin time-series data. The multi-layered model is designed by constructing a\nhierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential\nstack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden\nstate, inherited from the previous SBNs in the sequence, and is used to\nregulate its hidden bias. Scalable learning and inference algorithms are\nderived by introducing a recognition model that yields fast sampling from the\nvariational posterior. This recognition model is trained jointly with the\ngenerative model, by maximizing its variational lower bound on the\nlog-likelihood. Experimental results on bouncing balls, polyphonic music,\nmotion capture, and text streams show that the proposed approach achieves\nstate-of-the-art predictive performance, and has the capacity to synthesize\nvarious sequences. \n\n"}
{"id": "1509.08465", "contents": "Title: How Many Political Parties Should Brazil Have? A Data-driven Method to\n  Assess and Reduce Fragmentation in Multi-Party Political Systems Abstract: In June 2013, Brazil faced the largest and most significant mass protests in\na generation. These were exacerbated by the population's disenchantment towards\nits highly fragmented party system, which is composed by a very large number of\npolitical parties. Under these circumstances, presidents are constrained by\ninformal coalition governments, bringing very harmful consequences to the\ncountry. In this work I propose ARRANGE, a dAta dRiven method foR Assessing and\nreduciNG party fragmEntation in a country. ARRANGE uses as input the roll call\ndata for congress votes on bills and amendments as a proxy for political\npreferences and ideology. With that, ARRANGE finds the minimum number of\nparties required to house all congressmen without decreasing party discipline.\nWhen applied to Brazil's historical roll call data, ARRANGE was able to\ngenerate 23 distinct configurations that, compared with the status quo, have\n(i) a significant smaller number of parties, (ii) a higher discipline of\npartisans towards their parties and (iii) a more even distribution of partisans\ninto parties. ARRANGE is fast and parsimonious, relying on a single, intuitive\nparameter. \n\n"}
{"id": "1509.08985", "contents": "Title: Generalizing Pooling Functions in Convolutional Neural Networks: Mixed,\n  Gated, and Tree Abstract: We seek to improve deep neural networks by generalizing the pooling\noperations that play a central role in current architectures. We pursue a\ncareful exploration of approaches to allow pooling to learn and to adapt to\ncomplex and variable patterns. The two primary directions lie in (1) learning a\npooling function via (two strategies of) combining of max and average pooling,\nand (2) learning a pooling function in the form of a tree-structured fusion of\npooling filters that are themselves learned. In our experiments every\ngeneralized pooling operation we explore improves performance when used in\nplace of average or max pooling. We experimentally demonstrate that the\nproposed pooling operations provide a boost in invariance properties relative\nto conventional pooling and set the state of the art on several widely adopted\nbenchmark datasets; they are also easy to implement, and can be applied within\nvarious deep neural network architectures. These benefits come with only a\nlight increase in computational overhead during training and a very modest\nincrease in the number of model parameters. \n\n"}
{"id": "1509.09254", "contents": "Title: Community detection for interaction networks Abstract: In many applications, it is common practice to obtain a network from\ninteraction counts by thresholding each pairwise count at a prescribed value.\nOur analysis calls attention to the dependence of certain methods, notably\nNewman--Girvan modularity, on the choice of threshold. Essentially, the\nthreshold either separates the network into clusters automatically, making the\nalgorithm's job trivial, or erases all structure in the data, rendering\nclustering impossible. By fitting the original interaction counts as given, we\nshow that minor modifications to classical statistical methods outperform the\nprevailing approaches for community detection from interaction datasets. We\nalso introduce a new hidden Markov model for inferring community structures\nthat vary over time. We demonstrate each of these features on three real\ndatasets: the karate club dataset, voting data from the U.S.\\ Senate\n(2001--2003), and temporal voting data for the U.S. Supreme Court (1990--2004). \n\n"}
{"id": "1510.01039", "contents": "Title: Dynamical complexity in the perception-based network formation model Abstract: Many link formation mechanisms for the evolution of social networks have been\nsuccessful to reproduce various empirical findings in social networks. However,\nthey have largely ignored the fact that individuals make decisions on whether\nto create links to other individuals based on cost and benefit of linking, and\nthe fact that individuals may use perception of the network in their decision\nmaking. In this paper, we study the evolution of social networks in terms of\nperception-based strategic link formation. Here each individual has her own\nperception of the actual network, and uses it to decide whether to create a\nlink to another individual. An individual with the least perception accuracy\ncan benefit from updating her perception using that of the most accurate\nindividual via a new link. This benefit is compared to the cost of linking in\ndecision making. Once a new link is created, it affects the accuracies of other\nindividuals' perceptions, leading to a further evolution of the actual network.\nAs for initial actual networks, we consider homogeneous and heterogeneous\ncases. The homogeneous initial actual network is modeled by Erd\\H{o}s-R\\'enyi\n(ER) random networks, while we take a star network for the heterogeneous case.\nIn any cases, individual perceptions of the actual network are modeled by ER\nrandom networks with controllable linking probability. Then the stable link\ndensity of the actual network is found to show discontinuous transitions or\njumps according to the cost of linking. As the number of jumps is the\nconsequence of the dynamical complexity, we discuss the effect of initial\nconditions on the number of jumps to find that the dynamical complexity\nstrongly depends on how much individuals initially overestimate or\nunderestimate the link density of the actual network. For the heterogeneous\ncase, the role of the highly connected individual as an information spreader is\ndiscussed. \n\n"}
{"id": "1510.02437", "contents": "Title: Distilling Model Knowledge Abstract: Top-performing machine learning systems, such as deep neural networks, large\nensembles and complex probabilistic graphical models, can be expensive to\nstore, slow to evaluate and hard to integrate into larger systems. Ideally, we\nwould like to replace such cumbersome models with simpler models that perform\nequally well.\n  In this thesis, we study knowledge distillation, the idea of extracting the\nknowledge contained in a complex model and injecting it into a more convenient\nmodel. We present a general framework for knowledge distillation, whereby a\nconvenient model of our choosing learns how to mimic a complex model, by\nobserving the latter's behaviour and being penalized whenever it fails to\nreproduce it.\n  We develop our framework within the context of three distinct machine\nlearning applications: (a) model compression, where we compress large\ndiscriminative models, such as ensembles of neural networks, into models of\nmuch smaller size; (b) compact predictive distributions for Bayesian inference,\nwhere we distil large bags of MCMC samples into compact predictive\ndistributions in closed form; (c) intractable generative models, where we\ndistil unnormalizable models such as RBMs into tractable models such as NADEs.\n  We contribute to the state of the art with novel techniques and ideas. In\nmodel compression, we describe and implement derivative matching, which allows\nfor better distillation when data is scarce. In compact predictive\ndistributions, we introduce online distillation, which allows for significant\nsavings in memory. Finally, in intractable generative models, we show how to\nuse distilled models to robustly estimate intractable quantities of the\noriginal model, such as its intractable partition function. \n\n"}
{"id": "1510.02676", "contents": "Title: Some Theory For Practical Classifier Validation Abstract: We compare and contrast two approaches to validating a trained classifier\nwhile using all in-sample data for training. One is simultaneous validation\nover an organized set of hypotheses (SVOOSH), the well-known method that began\nwith VC theory. The other is withhold and gap (WAG). WAG withholds a validation\nset, trains a holdout classifier on the remaining data, uses the validation\ndata to validate that classifier, then adds the rate of disagreement between\nthe holdout classifier and one trained using all in-sample data, which is an\nupper bound on the difference in error rates. We show that complex hypothesis\nclasses and limited training data can make WAG a favorable alternative. \n\n"}
{"id": "1510.05956", "contents": "Title: Optimal Cluster Recovery in the Labeled Stochastic Block Model Abstract: We consider the problem of community detection or clustering in the labeled\nStochastic Block Model (LSBM) with a finite number $K$ of clusters of sizes\nlinearly growing with the global population of items $n$. Every pair of items\nis labeled independently at random, and label $\\ell$ appears with probability\n$p(i,j,\\ell)$ between two items in clusters indexed by $i$ and $j$,\nrespectively. The objective is to reconstruct the clusters from the observation\nof these random labels.\n  Clustering under the SBM and their extensions has attracted much attention\nrecently. Most existing work aimed at characterizing the set of parameters such\nthat it is possible to infer clusters either positively correlated with the\ntrue clusters, or with a vanishing proportion of misclassified items, or\nexactly matching the true clusters. We find the set of parameters such that\nthere exists a clustering algorithm with at most $s$ misclassified items in\naverage under the general LSBM and for any $s=o(n)$, which solves one open\nproblem raised in \\cite{abbe2015community}. We further develop an algorithm,\nbased on simple spectral methods, that achieves this fundamental performance\nlimit within $O(n \\mbox{polylog}(n))$ computations and without the a-priori\nknowledge of the model parameters. \n\n"}
{"id": "1510.07169", "contents": "Title: Fast and Scalable Lasso via Stochastic Frank-Wolfe Methods with a\n  Convergence Guarantee Abstract: Frank-Wolfe (FW) algorithms have been often proposed over the last few years\nas efficient solvers for a variety of optimization problems arising in the\nfield of Machine Learning. The ability to work with cheap projection-free\niterations and the incremental nature of the method make FW a very effective\nchoice for many large-scale problems where computing a sparse model is\ndesirable.\n  In this paper, we present a high-performance implementation of the FW method\ntailored to solve large-scale Lasso regression problems, based on a randomized\niteration, and prove that the convergence guarantees of the standard FW method\nare preserved in the stochastic setting. We show experimentally that our\nalgorithm outperforms several existing state of the art methods, including the\nCoordinate Descent algorithm by Friedman et al. (one of the fastest known Lasso\nsolvers), on several benchmark datasets with a very large number of features,\nwithout sacrificing the accuracy of the model. Our results illustrate that the\nalgorithm is able to generate the complete regularization path on problems of\nsize up to four million variables in less than one minute. \n\n"}
{"id": "1510.07954", "contents": "Title: Core-satellite Graphs. Clustering, Assortativity and Spectral Properties Abstract: Core-satellite graphs (sometimes referred to as generalized friendship\ngraphs) are an interesting class of graphs that generalize many well known\ntypes of graphs. In this paper we show that two popular clustering measures,\nthe average Watts-Strogatz clustering coefficient and the transitivity index,\ndiverge when the graph size increases. We also show that these graphs are\ndisassortative. In addition, we completely describe the spectrum of the\nadjacency and Laplacian matrices associated with core-satellite graphs.\nFinally, we introduce the class of generalized core-satellite graphs, and we\nanalyze the spectral properties of such graphs. \n\n"}
{"id": "1510.08119", "contents": "Title: Estimating Subgraph Frequencies with or without Attributes from\n  Egocentrically Sampled Data Abstract: In this paper we show how to efficiently produce unbiased estimates of\nsubgraph frequencies from a probability sample of egocentric networks (i.e.,\nfocal nodes, their neighbors, and the induced subgraphs of ties among their\nneighbors). A key feature of our proposed method that differentiates it from\nprior methods is the use of egocentric data. Because of this, our method is\nsuitable for estimation in large unknown graphs, is easily parallelizable,\nhandles privacy sensitive network data (e.g. egonets with no neighbor labels),\nand supports counting of large subgraphs (e.g. maximal clique of size 205 in\nSection 6) by building on top of existing exact subgraph counting algorithms\nthat may not support sampling. It gracefully handles a variety of sampling\ndesigns such as uniform or weighted independence or random walk sampling. Our\nmethod can be used for subgraphs that are: (i) undirected or directed; (ii)\ninduced or non-induced; (iii) maximal or non-maximal; and (iv) potentially\nannotated with attributes. We compare our estimators on a variety of real-world\ngraphs and sampling methods and provide suggestions for their use. Simulation\nshows that our method outperforms the state-of-the-art approach for relative\nsubgraph frequencies by up to an order of magnitude for the same sample size.\nFinally, we apply our methodology to a rare sample of Facebook users across the\nsocial graph to estimate and interpret the clique size distribution and gender\ncomposition of cliques. \n\n"}
{"id": "1510.08560", "contents": "Title: Why Random Reshuffling Beats Stochastic Gradient Descent Abstract: We analyze the convergence rate of the random reshuffling (RR) method, which\nis a randomized first-order incremental algorithm for minimizing a finite sum\nof convex component functions. RR proceeds in cycles, picking a uniformly\nrandom order (permutation) and processing the component functions one at a time\naccording to this order, i.e., at each cycle, each component function is\nsampled without replacement from the collection. Though RR has been numerically\nobserved to outperform its with-replacement counterpart stochastic gradient\ndescent (SGD), characterization of its convergence rate has been a long\nstanding open question. In this paper, we answer this question by showing that\nwhen the component functions are quadratics or smooth and the sum function is\nstrongly convex, RR with iterate averaging and a diminishing stepsize\n$\\alpha_k=\\Theta(1/k^s)$ for $s\\in (1/2,1)$ converges at rate\n$\\Theta(1/k^{2s})$ with probability one in the suboptimality of the objective\nvalue, thus improving upon the $\\Omega(1/k)$ rate of SGD. Our analysis draws on\nthe theory of Polyak-Ruppert averaging and relies on decoupling the dependent\ncycle gradient error into an independent term over cycles and another term\ndominated by $\\alpha_k^2$. This allows us to apply law of large numbers to an\nappropriately weighted version of the cycle gradient errors, where the weights\ndepend on the stepsize. We also provide high probability convergence rate\nestimates that shows decay rate of different terms and allows us to propose a\nmodification of RR with convergence rate ${\\cal O}(\\frac{1}{k^2})$. \n\n"}
{"id": "1510.08692", "contents": "Title: Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale\n  Bayesian Sampling Abstract: Monte Carlo sampling for Bayesian posterior inference is a common approach\nused in machine learning. The Markov Chain Monte Carlo procedures that are used\nare often discrete-time analogues of associated stochastic differential\nequations (SDEs). These SDEs are guaranteed to leave invariant the required\nposterior distribution. An area of current research addresses the computational\nbenefits of stochastic gradient methods in this setting. Existing techniques\nrely on estimating the variance or covariance of the subsampling error, and\ntypically assume constant variance. In this article, we propose a\ncovariance-controlled adaptive Langevin thermostat that can effectively\ndissipate parameter-dependent noise while maintaining a desired target\ndistribution. The proposed method achieves a substantial speedup over popular\nalternative schemes for large-scale machine learning applications. \n\n"}
{"id": "1511.00146", "contents": "Title: Faster Stochastic Variational Inference using Proximal-Gradient Methods\n  with General Divergence Functions Abstract: Several recent works have explored stochastic gradient methods for\nvariational inference that exploit the geometry of the variational-parameter\nspace. However, the theoretical properties of these methods are not\nwell-understood and these methods typically only apply to\nconditionally-conjugate models. We present a new stochastic method for\nvariational inference which exploits the geometry of the variational-parameter\nspace and also yields simple closed-form updates even for non-conjugate models.\nWe also give a convergence-rate analysis of our method and many other previous\nmethods which exploit the geometry of the space. Our analysis generalizes\nexisting convergence results for stochastic mirror-descent on non-convex\nobjectives by using a more general class of divergence functions. Beyond giving\na theoretical justification for a variety of recent methods, our experiments\nshow that new algorithms derived in this framework lead to state of the art\nresults on a variety of problems. Further, due to its generality, we expect\nthat our theoretical analysis could also apply to other applications. \n\n"}
{"id": "1511.00546", "contents": "Title: An Impossibility Result for Reconstruction in a Degree-Corrected\n  Planted-Partition Model Abstract: We consider the Degree-Corrected Stochastic Block Model (DC-SBM): a random\ngraph on $n$ nodes, having i.i.d. weights $(\\phi_u)_{u=1}^n$ (possibly\nheavy-tailed), partitioned into $q \\geq 2$ asymptotically equal-sized clusters.\nThe model parameters are two constants $a,b > 0$ and the finite second moment\nof the weights $\\Phi^{(2)}$. Vertices $u$ and $v$ are connected by an edge with\nprobability $\\frac{\\phi_u \\phi_v}{n}a$ when they are in the same class and with\nprobability $\\frac{\\phi_u \\phi_v}{n}b$ otherwise.\n  We prove that it is information-theoretically impossible to estimate the\nclusters in a way positively correlated with the true community structure when\n$(a-b)^2 \\Phi^{(2)} \\leq q(a+b)$.\n  As by-products of our proof we obtain $(1)$ a precise coupling result for\nlocal neighbourhoods in DC-SBM's, that we use in a follow up paper [Gulikers et\nal., 2017] to establish a law of large numbers for local-functionals and $(2)$\nthat long-range interactions are weak in (power-law) DC-SBM's. \n\n"}
{"id": "1511.00573", "contents": "Title: From random walks to distances on unweighted graphs Abstract: Large unweighted directed graphs are commonly used to capture relations\nbetween entities. A fundamental problem in the analysis of such networks is to\nproperly define the similarity or dissimilarity between any two vertices.\nDespite the significance of this problem, statistical characterization of the\nproposed metrics has been limited. We introduce and develop a class of\ntechniques for analyzing random walks on graphs using stochastic calculus.\nUsing these techniques we generalize results on the degeneracy of hitting times\nand analyze a metric based on the Laplace transformed hitting time (LTHT). The\nmetric serves as a natural, provably well-behaved alternative to the expected\nhitting time. We establish a general correspondence between hitting times of\nthe Brownian motion and analogous hitting times on the graph. We show that the\nLTHT is consistent with respect to the underlying metric of a geometric graph,\npreserves clustering tendency, and remains robust against random addition of\nnon-geometric edges. Tests on simulated and real-world data show that the LTHT\nmatches theoretical predictions and outperforms alternatives. \n\n"}
{"id": "1511.02124", "contents": "Title: Barrier Frank-Wolfe for Marginal Inference Abstract: We introduce a globally-convergent algorithm for optimizing the\ntree-reweighted (TRW) variational objective over the marginal polytope. The\nalgorithm is based on the conditional gradient method (Frank-Wolfe) and moves\npseudomarginals within the marginal polytope through repeated maximum a\nposteriori (MAP) calls. This modular structure enables us to leverage black-box\nMAP solvers (both exact and approximate) for variational inference, and obtains\nmore accurate results than tree-reweighted algorithms that optimize over the\nlocal consistency relaxation. Theoretically, we bound the sub-optimality for\nthe proposed algorithm despite the TRW objective having unbounded gradients at\nthe boundary of the marginal polytope. Empirically, we demonstrate the\nincreased quality of results found by tightening the relaxation over the\nmarginal polytope as well as the spanning tree polytope on synthetic and\nreal-world instances. \n\n"}
{"id": "1511.02386", "contents": "Title: Hierarchical Variational Models Abstract: Black box variational inference allows researchers to easily prototype and\nevaluate an array of models. Recent advances allow such algorithms to scale to\nhigh dimensions. However, a central question remains: How to specify an\nexpressive variational distribution that maintains efficient computation? To\naddress this, we develop hierarchical variational models (HVMs). HVMs augment a\nvariational approximation with a prior on its parameters, which allows it to\ncapture complex structure for both discrete and continuous latent variables.\nThe algorithm we develop is black box, can be used for any HVM, and has the\nsame computational efficiency as the original approximation. We study HVMs on a\nvariety of deep discrete latent variable models. HVMs generalize other\nexpressive variational distributions and maintains higher fidelity to the\nposterior. \n\n"}
{"id": "1511.06033", "contents": "Title: EigenRec: Generalizing PureSVD for Effective and Efficient Top-N\n  Recommendations Abstract: We introduce EigenRec; a versatile and efficient Latent-Factor framework for\nTop-N Recommendations that includes the well-known PureSVD algorithm as a\nspecial case. EigenRec builds a low dimensional model of an inter-item\nproximity matrix that combines a similarity component, with a scaling operator,\ndesigned to control the influence of the prior item popularity on the final\nmodel. Seeing PureSVD within our framework provides intuition about its inner\nworkings, exposes its inherent limitations, and also, paves the path towards\npainlessly improving its recommendation performance. A comprehensive set of\nexperiments on the MovieLens and the Yahoo datasets based on widely applied\nperformance metrics, indicate that EigenRec outperforms several\nstate-of-the-art algorithms, in terms of Standard and Long-Tail recommendation\naccuracy, exhibiting low susceptibility to sparsity, even in its most extreme\nmanifestations -- the Cold-Start problems. At the same time EigenRec has an\nattractive computational profile and it can apply readily in large-scale\nrecommendation settings. \n\n"}
{"id": "1511.06342", "contents": "Title: Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning Abstract: The ability to act in multiple environments and transfer previous knowledge\nto new situations can be considered a critical aspect of any intelligent agent.\nTowards this goal, we define a novel method of multitask and transfer learning\nthat enables an autonomous agent to learn how to behave in multiple tasks\nsimultaneously, and then generalize its knowledge to new domains. This method,\ntermed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model\ncompression techniques to train a single policy network that learns how to act\nin a set of distinct tasks by using the guidance of several expert teachers. We\nthen show that the representations learnt by the deep policy network are\ncapable of generalizing to new tasks with no prior expert guidance, speeding up\nlearning in novel environments. Although our method can in general be applied\nto a wide range of problems, we use Atari games as a testing environment to\ndemonstrate these methods. \n\n"}
{"id": "1511.06390", "contents": "Title: Unsupervised and Semi-supervised Learning with Categorical Generative\n  Adversarial Networks Abstract: In this paper we present a method for learning a discriminative classifier\nfrom unlabeled or partially labeled data. Our approach is based on an objective\nfunction that trades-off mutual information between observed examples and their\npredicted categorical class distribution, against robustness of the classifier\nto an adversarial generative model. The resulting algorithm can either be\ninterpreted as a natural generalization of the generative adversarial networks\n(GAN) framework or as an extension of the regularized information maximization\n(RIM) framework to robust classification against an optimal adversary. We\nempirically evaluate our method - which we dub categorical generative\nadversarial networks (or CatGAN) - on synthetic data as well as on challenging\nimage classification tasks, demonstrating the robustness of the learned\nclassifiers. We further qualitatively assess the fidelity of samples generated\nby the adversarial generator that is learned alongside the discriminative\nclassifier, and identify links between the CatGAN objective and discriminative\nclustering algorithms (such as RIM). \n\n"}
{"id": "1511.07305", "contents": "Title: Block Matrix Formulations for Evolving Networks Abstract: Many types of pairwise interaction take the form of a fixed set of nodes with\nedges that appear and disappear over time. In the case of discrete-time\nevolution, the resulting evolving network may be represented by a time-ordered\nsequence of adjacency matrices. We consider here the issue of representing the\nsystem as a single, higher dimensional block matrix, built from the individual\ntime-slices. We focus on the task of computing network centrality measures.\nFrom a modeling perspective, we show that there is a suitable block formulation\nthat allows us to recover dynamic centrality measures respecting time's arrow.\nFrom a computational perspective, we show that the new block formulation leads\nto the design of more effective numerical algorithms. \n\n"}
{"id": "1511.07893", "contents": "Title: Numerical Investigation of Metrics for Epidemic Processes on Graphs Abstract: This study develops the epidemic hitting time (EHT) metric on graphs\nmeasuring the expected time an epidemic starting at node $a$ in a fully\nsusceptible network takes to propagate and reach node $b$. An associated EHT\ncentrality measure is then compared to degree, betweenness, spectral, and\neffective resistance centrality measures through exhaustive numerical\nsimulations on several real-world network data-sets. We find two surprising\nobservations: first, EHT centrality is highly correlated with effective\nresistance centrality; second, the EHT centrality measure is much more\ndelocalized compared to degree and spectral centrality, highlighting the role\nof peripheral nodes in epidemic spreading on graphs. \n\n"}
{"id": "1512.00553", "contents": "Title: Asymptotic behavior of the node degrees in the ensemble average of\n  adjacency matrix Abstract: Various important and useful quantities or measures that characterize the\ntopological network structure are usually investigated for a network, then they\nare averaged over the samples. In this paper, we propose an explicit\nrepresentation by the beforehand averaged adjacency matrix over samples of\ngrowing networks as a new general framework for investigating the\ncharacteristic quantities. It is applied to some network models, and shows a\ngood approximation of degree distribution asymptotically. In particular, our\napproach will be applicable through the numerical calculations instead of\nintractable theoretical analysises, when the time-course of degree is a\nmonotone increasing function like power-law or logarithm. \n\n"}
{"id": "1512.02896", "contents": "Title: Where You Are Is Who You Are: User Identification by Matching Statistics Abstract: Most users of online services have unique behavioral or usage patterns. These\nbehavioral patterns can be exploited to identify and track users by using only\nthe observed patterns in the behavior. We study the task of identifying users\nfrom statistics of their behavioral patterns. Specifically, we focus on the\nsetting in which we are given histograms of users' data collected during two\ndifferent experiments. We assume that, in the first dataset, the users'\nidentities are anonymized or hidden and that, in the second dataset, their\nidentities are known. We study the task of identifying the users by matching\nthe histograms of their data in the first dataset with the histograms from the\nsecond dataset. In recent works, the optimal algorithm for this user\nidentification task is introduced. In this paper, we evaluate the effectiveness\nof this method on three different types of datasets and in multiple scenarios.\nUsing datasets such as call data records, web browsing histories, and GPS\ntrajectories, we show that a large fraction of users can be easily identified\ngiven only histograms of their data; hence these histograms can act as users'\nfingerprints. We also verify that simultaneous identification of users achieves\nbetter performance compared to one-by-one user identification. We show that\nusing the optimal method for identification gives higher identification\naccuracy than heuristics-based approaches in practical scenarios. The accuracy\nobtained under this optimal method can thus be used to quantify the maximum\nlevel of user identification that is possible in such settings. We show that\nthe key factors affecting the accuracy of the optimal identification algorithm\nare the duration of the data collection, the number of users in the anonymized\ndataset, and the resolution of the dataset. We analyze the effectiveness of\nk-anonymization in resisting user identification attacks on these datasets. \n\n"}
{"id": "1512.03489", "contents": "Title: Moment-Based Spectral Analysis of Random Graphs with Given Expected\n  Degrees Abstract: In this paper, we analyze the limiting spectral distribution of the adjacency\nmatrix of a random graph ensemble, proposed by Chung and Lu, in which a given\nexpected degree sequence $\\overline{w}_n^{^{T}} = (w^{(n)}_1,\\ldots,w^{(n)}_n)$\nis prescribed on the ensemble. Let $\\mathbf{a}_{i,j} =1$ if there is an edge\nbetween the nodes $\\{i,j\\}$ and zero otherwise, and consider the normalized\nrandom adjacency matrix of the graph ensemble: $\\mathbf{A}_n$ $=$ $\n[\\mathbf{a}_{i,j}/\\sqrt{n}]_{i,j=1}^{n}$. The empirical spectral distribution\nof $\\mathbf{A}_n$ denoted by $\\mathbf{F}_n(\\mathord{\\cdot})$ is the empirical\nmeasure putting a mass $1/n$ at each of the $n$ real eigenvalues of the\nsymmetric matrix $\\mathbf{A}_n$. Under some technical conditions on the\nexpected degree sequence, we show that with probability one,\n$\\mathbf{F}_n(\\mathord{\\cdot})$ converges weakly to a deterministic\ndistribution $F(\\mathord{\\cdot})$. Furthermore, we fully characterize this\ndistribution by providing explicit expressions for the moments of\n$F(\\mathord{\\cdot})$. We apply our results to well-known degree distributions,\nsuch as power-law and exponential. The asymptotic expressions of the spectral\nmoments in each case provide significant insights about the bulk behavior of\nthe eigenvalue spectrum. \n\n"}
{"id": "1512.04177", "contents": "Title: Conflict and Computation on Wikipedia: a Finite-State Machine Analysis\n  of Editor Interactions Abstract: What is the boundary between a vigorous argument and a breakdown of\nrelations? What drives a group of individuals across it? Taking Wikipedia as a\ntest case, we use a hidden Markov model to approximate the computational\nstructure and social grammar of more than a decade of cooperation and conflict\namong its editors. Across a wide range of pages, we discover a bursty war/peace\nstructure where the systems can become trapped, sometimes for months, in a\ncomputational subspace associated with significantly higher levels of\nconflict-tracking \"revert\" actions. Distinct patterns of behavior characterize\nthe lower-conflict subspace, including tit-for-tat reversion. While a fraction\nof the transitions between these subspaces are associated with top-down actions\ntaken by administrators, the effects are weak. Surprisingly, we find no\nstatistical signal that transitions are associated with the appearance of\nparticularly anti-social users, and only weak association with significant news\nevents outside the system. These findings are consistent with transitions being\ndriven by decentralized processes with no clear locus of control. Models of\nbelief revision in the presence of a common resource for information-sharing\npredict the existence of two distinct phases: a disordered high-conflict phase,\nand a frozen phase with spontaneously-broken symmetry. The bistability we\nobserve empirically may be a consequence of editor turn-over, which drives the\nsystem to a critical point between them. \n\n"}
{"id": "1512.04828", "contents": "Title: Group Mobility: Detection, Tracking and Characterization Abstract: In the era of mobile computing, understanding human mobility patterns is\ncrucial in order to better design protocols and applications. Many studies\nfocus on different aspects of human mobility such as people's points of\ninterests, routes, traffic, individual mobility patterns, among others. In this\nwork, we propose to look at human mobility through a social perspective, i.e.,\nanalyze the impact of social groups in mobility patterns. We use the MIT\nReality Mining proximity trace to detect, track and investigate group's\nevolution throughout time. Our results show that group meetings happen in a\nperiodical fashion and present daily and weekly periodicity. We analyze how\ngroups' dynamics change over day hours and find that group meetings lasting\nlonger are those with less changes in members composition and with members\nhaving stronger social bonds with each other. Our findings can be used to\npropose meeting prediction algorithms, opportunistic routing and information\ndiffusion protocols, taking advantage of those revealed properties. \n\n"}
{"id": "1512.04829", "contents": "Title: Feature-Level Domain Adaptation Abstract: Domain adaptation is the supervised learning setting in which the training\nand test data are sampled from different distributions: training data is\nsampled from a source domain, whilst test data is sampled from a target domain.\nThis paper proposes and studies an approach, called feature-level domain\nadaptation (FLDA), that models the dependence between the two domains by means\nof a feature-level transfer model that is trained to describe the transfer from\nsource to target domain. Subsequently, we train a domain-adapted classifier by\nminimizing the expected loss under the resulting transfer model. For linear\nclassifiers and a large family of loss functions and transfer models, this\nexpected loss can be computed or approximated analytically, and minimized\nefficiently. Our empirical evaluation of FLDA focuses on problems comprising\nbinary and count data in which the transfer can be naturally modeled via a\ndropout distribution, which allows the classifier to adapt to differences in\nthe marginal probability of features in the source and the target domain. Our\nexperiments on several real-world problems show that FLDA performs on par with\nstate-of-the-art domain-adaptation techniques. \n\n"}
{"id": "1512.05457", "contents": "Title: In a World That Counts: Clustering and Detecting Fake Social Engagement\n  at Scale Abstract: How can web services that depend on user generated content discern fake\nsocial engagement activities by spammers from legitimate ones? In this paper,\nwe focus on the social site of YouTube and the problem of identifying bad\nactors posting inorganic contents and inflating the count of social engagement\nmetrics. We propose an effective method, Leas (Local Expansion at Scale), and\nshow how the fake engagement activities on YouTube can be tracked over time by\nanalyzing the temporal graph based on the engagement behavior pattern between\nusers and YouTube videos. With the domain knowledge of spammer seeds, we\nformulate and tackle the problem in a semi-supervised manner --- with the\nobjective of searching for individuals that have similar pattern of behavior as\nthe known seeds --- based on a graph diffusion process via local spectral\nsubspace. We offer a fast, scalable MapReduce deployment adapted from the\nlocalized spectral clustering algorithm. We demonstrate the effectiveness of\nour deployment at Google by achieving an manual review accuracy of 98% on\nYouTube Comments graph in practice. Comparing with the state-of-the-art\nalgorithm CopyCatch, Leas achieves 10 times faster running time. Leas is\nactively in use at Google, searching for daily deceptive practices on YouTube's\nengagement graph spanning over a billion users. \n\n"}
{"id": "1512.06457", "contents": "Title: Classification of weighted networks through mesoscale homological\n  features Abstract: As complex networks find applications in a growing range of disciplines, the\ndiversity of naturally occurring and model networks being studied is exploding.\nThe adoption of a well-developed collection of network taxonomies is a natural\nmethod for both organizing this data and understanding deeper relationships\nbetween networks. Most existing metrics for network structure rely on classical\ngraph-theoretic measures, extracting characteristics primarily related to\nindividual vertices or paths between them, and thus classify networks from the\nperspective of local features. Here, we describe an alternative approach to\nstudying structure in networks that relies on an algebraic-topological metric\ncalled persistent homology, which studies intrinsically mesoscale structures\ncalled cycles, constructed from cliques in the network. We present a\nclassification of 14 commonly studied weighted network models into four groups\nor classes, and discuss the structural themes arising in each class. Finally,\nwe compute the persistent homology of two real-world networks and one network\nconstructed by a common dynamical systems model, and we compare the results\nwith the three classes to obtain a better understanding of those networks. \n\n"}
{"id": "1512.06785", "contents": "Title: Beyond Classification: Latent User Interests Profiling from Visual\n  Contents Analysis Abstract: User preference profiling is an important task in modern online social\nnetworks (OSN). With the proliferation of image-centric social platforms, such\nas Pinterest, visual contents have become one of the most informative data\nstreams for understanding user preferences. Traditional approaches usually\ntreat visual content analysis as a general classification problem where one or\nmore labels are assigned to each image. Although such an approach simplifies\nthe process of image analysis, it misses the rich context and visual cues that\nplay an important role in people's perception of images. In this paper, we\nexplore the possibilities of learning a user's latent visual preferences\ndirectly from image contents. We propose a distance metric learning method\nbased on Deep Convolutional Neural Networks (CNN) to directly extract\nsimilarity information from visual contents and use the derived distance metric\nto mine individual users' fine-grained visual preferences. Through our\npreliminary experiments using data from 5,790 Pinterest users, we show that\neven for the images within the same category, each user possesses distinct and\nindividually-identifiable visual preferences that are consistent over their\nlifetime. Our results underscore the untapped potential of finer-grained visual\npreference profiling in understanding users' preferences. \n\n"}
{"id": "1512.07422", "contents": "Title: Adaptive Algorithms for Online Convex Optimization with Long-term\n  Constraints Abstract: We present an adaptive online gradient descent algorithm to solve online\nconvex optimization problems with long-term constraints , which are constraints\nthat need to be satisfied when accumulated over a finite number of rounds T ,\nbut can be violated in intermediate rounds. For some user-defined trade-off\nparameter $\\beta$ $\\in$ (0, 1), the proposed algorithm achieves cumulative\nregret bounds of O(T^max{$\\beta$,1--$\\beta$}) and O(T^(1--$\\beta$/2)) for the\nloss and the constraint violations respectively. Our results hold for convex\nlosses and can handle arbitrary convex constraints without requiring knowledge\nof the number of rounds in advance. Our contributions improve over the best\nknown cumulative regret bounds by Mahdavi, et al. (2012) that are respectively\nO(T^1/2) and O(T^3/4) for general convex domains, and respectively O(T^2/3) and\nO(T^2/3) when further restricting to polyhedral domains. We supplement the\nanalysis with experiments validating the performance of our algorithm in\npractice. \n\n"}
{"id": "1512.07797", "contents": "Title: The Lov\\'asz Hinge: A Novel Convex Surrogate for Submodular Losses Abstract: Learning with non-modular losses is an important problem when sets of\npredictions are made simultaneously. The main tools for constructing convex\nsurrogate loss functions for set prediction are margin rescaling and slack\nrescaling. In this work, we show that these strategies lead to tight convex\nsurrogates iff the underlying loss function is increasing in the number of\nincorrect predictions. However, gradient or cutting-plane computation for these\nfunctions is NP-hard for non-supermodular loss functions. We propose instead a\nnovel surrogate loss function for submodular losses, the Lov\\'asz hinge, which\nleads to O(p log p) complexity with O(p) oracle accesses to the loss function\nto compute a gradient or cutting-plane. We prove that the Lov\\'asz hinge is\nconvex and yields an extension. As a result, we have developed the first\ntractable convex surrogates in the literature for submodular losses. We\ndemonstrate the utility of this novel convex surrogate through several set\nprediction tasks, including on the PASCAL VOC and Microsoft COCO datasets. \n\n"}
{"id": "1512.08425", "contents": "Title: Convexified Modularity Maximization for Degree-corrected Stochastic\n  Block Models Abstract: The stochastic block model (SBM) is a popular framework for studying\ncommunity detection in networks. This model is limited by the assumption that\nall nodes in the same community are statistically equivalent and have equal\nexpected degrees. The degree-corrected stochastic block model (DCSBM) is a\nnatural extension of SBM that allows for degree heterogeneity within\ncommunities. This paper proposes a convexified modularity maximization approach\nfor estimating the hidden communities under DCSBM. Our approach is based on a\nconvex programming relaxation of the classical (generalized) modularity\nmaximization formulation, followed by a novel doubly-weighted $ \\ell_1 $-norm $\nk $-median procedure. We establish non-asymptotic theoretical guarantees for\nboth approximate clustering and perfect clustering. Our approximate clustering\nresults are insensitive to the minimum degree, and hold even in sparse regime\nwith bounded average degrees. In the special case of SBM, these theoretical\nresults match the best-known performance guarantees of computationally feasible\nalgorithms. Numerically, we provide an efficient implementation of our\nalgorithm, which is applied to both synthetic and real-world networks.\nExperiment results show that our method enjoys competitive performance compared\nto the state of the art in the literature. \n\n"}
{"id": "1512.08787", "contents": "Title: Matrix Completion Under Monotonic Single Index Models Abstract: Most recent results in matrix completion assume that the matrix under\nconsideration is low-rank or that the columns are in a union of low-rank\nsubspaces. In real-world settings, however, the linear structure underlying\nthese models is distorted by a (typically unknown) nonlinear transformation.\nThis paper addresses the challenge of matrix completion in the face of such\nnonlinearities. Given a few observations of a matrix that are obtained by\napplying a Lipschitz, monotonic function to a low rank matrix, our task is to\nestimate the remaining unobserved entries. We propose a novel matrix completion\nmethod that alternates between low-rank matrix estimation and monotonic\nfunction estimation to estimate the missing matrix elements. Mean squared error\nbounds provide insight into how well the matrix can be estimated based on the\nsize, rank of the matrix and properties of the nonlinear transformation.\nEmpirical results on synthetic and real-world datasets demonstrate the\ncompetitiveness of the proposed approach. \n\n"}
{"id": "1601.00062", "contents": "Title: Practical Algorithms for Learning Near-Isometric Linear Embeddings Abstract: We propose two practical non-convex approaches for learning near-isometric,\nlinear embeddings of finite sets of data points. Given a set of training points\n$\\mathcal{X}$, we consider the secant set $S(\\mathcal{X})$ that consists of all\npairwise difference vectors of $\\mathcal{X}$, normalized to lie on the unit\nsphere. The problem can be formulated as finding a symmetric and positive\nsemi-definite matrix $\\boldsymbol{\\Psi}$ that preserves the norms of all the\nvectors in $S(\\mathcal{X})$ up to a distortion parameter $\\delta$. Motivated by\nnon-negative matrix factorization, we reformulate our problem into a Frobenius\nnorm minimization problem, which is solved by the Alternating Direction Method\nof Multipliers (ADMM) and develop an algorithm, FroMax. Another method solves\nfor a projection matrix $\\boldsymbol{\\Psi}$ by minimizing the restricted\nisometry property (RIP) directly over the set of symmetric, postive\nsemi-definite matrices. Applying ADMM and a Moreau decomposition on a proximal\nmapping, we develop another algorithm, NILE-Pro, for dimensionality reduction.\nFroMax is shown to converge faster for smaller $\\delta$ while NILE-Pro\nconverges faster for larger $\\delta$. Both non-convex approaches are then\nempirically demonstrated to be more computationally efficient than prior convex\napproaches for a number of applications in machine learning and signal\nprocessing. \n\n"}
{"id": "1601.01356", "contents": "Title: From Word Embeddings to Item Recommendation Abstract: Social network platforms can use the data produced by their users to serve\nthem better. One of the services these platforms provide is recommendation\nservice. Recommendation systems can predict the future preferences of users\nusing their past preferences. In the recommendation systems literature there\nare various techniques, such as neighborhood based methods, machine-learning\nbased methods and matrix-factorization based methods. In this work, a set of\nwell known methods from natural language processing domain, namely Word2Vec, is\napplied to recommendation systems domain. Unlike previous works that use\nWord2Vec for recommendation, this work uses non-textual features, the\ncheck-ins, and it recommends venues to visit/check-in to the target users. For\nthe experiments, a Foursquare check-in dataset is used. The results show that\nuse of continuous vector space representations of items modeled by techniques\nof Word2Vec is promising for making recommendations. \n\n"}
{"id": "1601.02068", "contents": "Title: On Computationally Tractable Selection of Experiments in\n  Measurement-Constrained Regression Models Abstract: We derive computationally tractable methods to select a small subset of\nexperiment settings from a large pool of given design points. The primary focus\nis on linear regression models, while the technique extends to generalized\nlinear models and Delta's method (estimating functions of linear regression\nmodels) as well. The algorithms are based on a continuous relaxation of an\notherwise intractable combinatorial optimization problem, with sampling or\ngreedy procedures as post-processing steps. Formal approximation guarantees are\nestablished for both algorithms, and numerical results on both synthetic and\nreal-world data confirm the effectiveness of the proposed methods. \n\n"}
{"id": "1601.02513", "contents": "Title: How to learn a graph from smooth signals Abstract: We propose a framework that learns the graph structure underlying a set of\nsmooth signals. Given $X\\in\\mathbb{R}^{m\\times n}$ whose rows reside on the\nvertices of an unknown graph, we learn the edge weights\n$w\\in\\mathbb{R}_+^{m(m-1)/2}$ under the smoothness assumption that\n$\\text{tr}{X^\\top LX}$ is small. We show that the problem is a weighted\n$\\ell$-1 minimization that leads to naturally sparse solutions. We point out\nhow known graph learning or construction techniques fall within our framework\nand propose a new model that performs better than the state of the art in many\nsettings. We present efficient, scalable primal-dual based algorithms for both\nour model and the previous state of the art, and evaluate their performance on\nartificial and real data. \n\n"}
{"id": "1601.03958", "contents": "Title: Real-Time Community Detection in Large Social Networks on a Laptop Abstract: For a broad range of research, governmental and commercial applications it is\nimportant to understand the allegiances, communities and structure of key\nplayers in society. One promising direction towards extracting this information\nis to exploit the rich relational data in digital social networks (the social\ngraph). As social media data sets are very large, most approaches make use of\ndistributed computing systems for this purpose. Distributing graph processing\nrequires solving many difficult engineering problems, which has lead some\nresearchers to look at single-machine solutions that are faster and easier to\nmaintain. In this article, we present a single-machine real-time system for\nlarge-scale graph processing that allows analysts to interactively explore\ngraph structures. The key idea is that the aggregate actions of large numbers\nof users can be compressed into a data structure that encapsulates user\nsimilarities while being robust to noise and queryable in real-time. We achieve\nsingle machine real-time performance by compressing the neighbourhood of each\nvertex using minhash signatures and facilitate rapid queries through Locality\nSensitive Hashing. These techniques reduce query times from hours using\nindustrial desktop machines operating on the full graph to milliseconds on\nstandard laptops. Our method allows exploration of strongly associated regions\n(i.e. communities) of large graphs in real-time on a laptop. It has been\ndeployed in software that is actively used by social network analysts and\noffers another channel for media owners to monetise their data, helping them to\ncontinue to provide free services that are valued by billions of people\nglobally. \n\n"}
{"id": "1601.04746", "contents": "Title: Scalable Constrained Clustering: A Generalized Spectral Method Abstract: We present a simple spectral approach to the well-studied constrained\nclustering problem. It captures constrained clustering as a generalized\neigenvalue problem with graph Laplacians. The algorithm works in nearly-linear\ntime and provides concrete guarantees for the quality of the clusters, at least\nfor the case of 2-way partitioning. In practice this translates to a very fast\nimplementation that consistently outperforms existing spectral approaches both\nin speed and quality. \n\n"}
{"id": "1601.05834", "contents": "Title: Active Sensing of Social Networks Abstract: This paper develops an active sensing method to estimate the relative weight\n(or trust) agents place on their neighbors' information in a social network.\nThe model used for the regression is based on the steady state equation in the\nlinear DeGroot model under the influence of stubborn agents, i.e., agents whose\nopinions are not influenced by their neighbors. This method can be viewed as a\n\\emph{social RADAR}, where the stubborn agents excite the system and the latter\ncan be estimated through the reverberation observed from the analysis of the\nagents' opinions. The social network sensing problem can be interpreted as a\nblind compressed sensing problem with a sparse measurement matrix. We prove\nthat the network structure will be revealed when a sufficient number of\nstubborn agents independently influence a number of ordinary (non-stubborn)\nagents. We investigate the scenario with a deterministic or randomized DeGroot\nmodel and propose a consistent estimator of the steady states for the latter\nscenario. Simulation results on synthetic and real world networks support our\nfindings. \n\n"}
{"id": "1601.06439", "contents": "Title: Who Ordered This?: Exploiting Implicit User Tag Order Preferences for\n  Personalized Image Tagging Abstract: What makes a person pick certain tags over others when tagging an image? Does\nthe order that a person presents tags for a given image follow an implicit bias\nthat is personal? Can these biases be used to improve existing automated image\ntagging systems? We show that tag ordering, which has been largely overlooked\nby the image tagging community, is an important cue in understanding user\ntagging behavior and can be used to improve auto-tagging systems. Inspired by\nthe assumption that people order their tags, we propose a new way of measuring\ntag preferences, and also propose a new personalized tagging objective function\nthat explicitly considers a user's preferred tag orderings. We also provide a\n(partially) greedy algorithm that produces good solutions to our new objective\nand under certain conditions produces an optimal solution. We validate our\nmethod on a subset of Flickr images that spans 5000 users, over 5200 tags, and\nover 90,000 images. Our experiments show that exploiting personalized tag\norders improves the average performance of state-of-art approaches both on\nper-image and per-user bases. \n\n"}
{"id": "1601.06440", "contents": "Title: QUOTE: \"Querying\" Users as Oracles in Tag Engines - A Semi-Supervised\n  Learning Approach to Personalized Image Tagging Abstract: One common trend in image tagging research is to focus on visually relevant\ntags, and this tends to ignore the personal and social aspect of tags,\nespecially on photoblogging websites such as Flickr. Previous work has\ncorrectly identified that many of the tags that users provide on images are not\nvisually relevant (i.e. representative of the salient content in the image) and\nthey go on to treat such tags as noise, ignoring that the users chose to\nprovide those tags over others that could have been more visually relevant.\nAnother common assumption about user generated tags for images is that the\norder of these tags provides no useful information for the prediction of tags\non future images. This assumption also tends to define usefulness in terms of\nwhat is visually relevant to the image. For general tagging or labeling\napplications that focus on providing visual information about image content,\nthese assumptions are reasonable, but when considering personalized image\ntagging applications, these assumptions are at best too rigid, ignoring user\nchoice and preferences.\n  We challenge the aforementioned assumptions, and provide a machine learning\napproach to the problem of personalized image tagging with the following\ncontributions: 1.) We reformulate the personalized image tagging problem as a\nsearch/retrieval ranking problem, 2.) We leverage the order of tags, which does\nnot always reflect visual relevance, provided by the user in the past as a cue\nto their tag preferences, similar to click data, 3.) We propose a technique to\naugment sparse user tag data (semi-supervision), and 4.) We demonstrate the\nefficacy of our method on a subset of Flickr images, showing improvement over\nprevious state-of-art methods. \n\n"}
{"id": "1601.06448", "contents": "Title: Analysis of centrality in sublinear preferential attachment trees via\n  the CMJ branching process Abstract: We investigate centrality and root-inference properties in a class of growing\nrandom graphs known as sublinear preferential attachment trees. We show that a\ncontinuous time branching processes called the Crump-Mode-Jagers (CMJ)\nbranching process is well-suited to analyze such random trees, and prove that\nalmost surely, a unique terminal tree centroid emerges, having the property\nthat it becomes more central than any other fixed vertex in the limit of the\nrandom growth process. Our result generalizes and extends previous work\nestablishing persistent centrality in uniform and linear preferential\nattachment trees. We also show that centrality may be utilized to generate a\nfinite-sized $1-\\epsilon$ confidence set for the root node, for any $\\epsilon >\n0$ in a certain subclass of sublinear preferential attachment trees. \n\n"}
{"id": "1602.02114", "contents": "Title: Exchangeable Random Measures for Sparse and Modular Graphs with\n  Overlapping Communities Abstract: We propose a novel statistical model for sparse networks with overlapping\ncommunity structure. The model is based on representing the graph as an\nexchangeable point process, and naturally generalizes existing probabilistic\nmodels with overlapping block-structure to the sparse regime. Our construction\nbuilds on vectors of completely random measures, and has interpretable\nparameters, each node being assigned a vector representing its level of\naffiliation to some latent communities. We develop methods for simulating this\nclass of random graphs, as well as to perform posterior inference. We show that\nthe proposed approach can recover interpretable structure from two real-world\nnetworks and can handle graphs with thousands of nodes and tens of thousands of\nedges. \n\n"}
{"id": "1602.02355", "contents": "Title: Hyperparameter optimization with approximate gradient Abstract: Most models in machine learning contain at least one hyperparameter to\ncontrol for model complexity. Choosing an appropriate set of hyperparameters is\nboth crucial in terms of model accuracy and computationally challenging. In\nthis work we propose an algorithm for the optimization of continuous\nhyperparameters using inexact gradient information. An advantage of this method\nis that hyperparameters can be updated before model parameters have fully\nconverged. We also give sufficient conditions for the global convergence of\nthis method, based on regularity conditions of the involved functions and\nsummability of errors. Finally, we validate the empirical performance of this\nmethod on the estimation of regularization constants of L2-regularized logistic\nregression and kernel Ridge regression. Empirical benchmarks indicate that our\napproach is highly competitive with respect to state of the art methods. \n\n"}
{"id": "1602.03481", "contents": "Title: Achieving Budget-optimality with Adaptive Schemes in Crowdsourcing Abstract: Crowdsourcing platforms provide marketplaces where task requesters can pay to\nget labels on their data. Such markets have emerged recently as popular venues\nfor collecting annotations that are crucial in training machine learning models\nin various applications. However, as jobs are tedious and payments are low,\nerrors are common in such crowdsourced labels. A common strategy to overcome\nsuch noise in the answers is to add redundancy by getting multiple answers for\neach task and aggregating them using some methods such as majority voting. For\nsuch a system, there is a fundamental question of interest: how can we maximize\nthe accuracy given a fixed budget on how many responses we can collect on the\ncrowdsourcing system. We characterize this fundamental trade-off between the\nbudget (how many answers the requester can collect in total) and the accuracy\nin the estimated labels. In particular, we ask whether adaptive task assignment\nschemes lead to a more efficient trade-off between the accuracy and the budget.\n  Adaptive schemes, where tasks are assigned adaptively based on the data\ncollected thus far, are widely used in practical crowdsourcing systems to\nefficiently use a given fixed budget. However, existing theoretical analyses of\ncrowdsourcing systems suggest that the gain of adaptive task assignments is\nminimal. To bridge this gap, we investigate this question under a strictly more\ngeneral probabilistic model, which has been recently introduced to model\npractical crowdsourced annotations. Under this generalized Dawid-Skene model,\nwe characterize the fundamental trade-off between budget and accuracy. We\nintroduce a novel adaptive scheme that matches this fundamental limit. We\nfurther quantify the fundamental gap between adaptive and non-adaptive schemes,\nby comparing the trade-off with the one for non-adaptive schemes. Our analyses\nconfirm that the gap is significant. \n\n"}
{"id": "1602.04418", "contents": "Title: Identifiability Assumptions and Algorithm for Directed Graphical Models\n  with Feedback Abstract: Directed graphical models provide a useful framework for modeling causal or\ndirectional relationships for multivariate data. Prior work has largely focused\non identifiability and search algorithms for directed acyclic graphical (DAG)\nmodels. In many applications, feedback naturally arises and directed graphical\nmodels that permit cycles occur. In this paper we address the issue of\nidentifiability for general directed cyclic graphical (DCG) models satisfying\nthe Markov assumption. In particular, in addition to the faithfulness\nassumption which has already been introduced for cyclic models, we introduce\ntwo new identifiability assumptions, one based on selecting the model with the\nfewest edges and the other based on selecting the DCG model that entails the\nmaximum number of d-separation rules. We provide theoretical results comparing\nthese assumptions which show that: (1) selecting models with the largest number\nof d-separation rules is strictly weaker than the faithfulness assumption; (2)\nunlike for DAG models, selecting models with the fewest edges does not\nnecessarily result in a milder assumption than the faithfulness assumption. We\nalso provide connections between our two new principles and minimality\nassumptions. We use our identifiability assumptions to develop search\nalgorithms for small-scale DCG models. Our simulation study supports our\ntheoretical results, showing that the algorithms based on our two new\nprinciples generally out-perform algorithms based on the faithfulness\nassumption in terms of selecting the true skeleton for DCG models. \n\n"}
{"id": "1602.04805", "contents": "Title: DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution\n  Regression Abstract: Performing exact posterior inference in complex generative models is often\ndifficult or impossible due to an expensive to evaluate or intractable\nlikelihood function. Approximate Bayesian computation (ABC) is an inference\nframework that constructs an approximation to the true likelihood based on the\nsimilarity between the observed and simulated data as measured by a predefined\nset of summary statistics. Although the choice of appropriate problem-specific\nsummary statistics crucially influences the quality of the likelihood\napproximation and hence also the quality of the posterior sample in ABC, there\nare only few principled general-purpose approaches to the selection or\nconstruction of such summary statistics. In this paper, we develop a novel\nframework for this task using kernel-based distribution regression. We model\nthe functional relationship between data distributions and the optimal choice\n(with respect to a loss function) of summary statistics using kernel-based\ndistribution regression. We show that our approach can be implemented in a\ncomputationally and statistically efficient way using the random Fourier\nfeatures framework for large-scale kernel learning. In addition to that, our\nframework shows superior performance when compared to related methods on toy\nand real-world problems. \n\n"}
{"id": "1602.04915", "contents": "Title: Gradient Descent Converges to Minimizers Abstract: We show that gradient descent converges to a local minimizer, almost surely\nwith random initialization. This is proved by applying the Stable Manifold\nTheorem from dynamical systems theory. \n\n"}
{"id": "1602.05394", "contents": "Title: Online optimization and regret guarantees for non-additive long-term\n  constraints Abstract: We consider online optimization in the 1-lookahead setting, where the\nobjective does not decompose additively over the rounds of the online game. The\nresulting formulation enables us to deal with non-stationary and/or long-term\nconstraints , which arise, for example, in online display advertising problems.\nWe propose an on-line primal-dual algorithm for which we obtain dynamic\ncumulative regret guarantees. They depend on the convexity and the smoothness\nof the non-additive penalty, as well as terms capturing the smoothness with\nwhich the residuals of the non-stationary and long-term constraints vary over\nthe rounds. We conduct experiments on synthetic data to illustrate the benefits\nof the non-additive penalty and show vanishing regret convergence on live\ntraffic data collected by a display advertising platform in production. \n\n"}
{"id": "1602.06225", "contents": "Title: GAP Safe Screening Rules for Sparse-Group-Lasso Abstract: In high dimensional settings, sparse structures are crucial for efficiency,\neither in term of memory, computation or performance. In some contexts, it is\nnatural to handle more refined structures than pure sparsity, such as for\ninstance group sparsity. Sparse-Group Lasso has recently been introduced in the\ncontext of linear regression to enforce sparsity both at the feature level and\nat the group level. We adapt to the case of Sparse-Group Lasso recent safe\nscreening rules that discard early in the solver irrelevant features/groups.\nSuch rules have led to important speed-ups for a wide range of iterative\nmethods. Thanks to dual gap computations, we provide new safe screening rules\nfor Sparse-Group Lasso and show significant gains in term of computing time for\na coordinate descent implementation. \n\n"}
{"id": "1602.06410", "contents": "Title: Semidefinite Programs for Exact Recovery of a Hidden Community Abstract: We study a semidefinite programming (SDP) relaxation of the maximum\nlikelihood estimation for exactly recovering a hidden community of cardinality\n$K$ from an $n \\times n$ symmetric data matrix $A$, where for distinct indices\n$i,j$, $A_{ij} \\sim P$ if $i, j$ are both in the community and $A_{ij} \\sim Q$\notherwise, for two known probability distributions $P$ and $Q$. We identify a\nsufficient condition and a necessary condition for the success of SDP for the\ngeneral model. For both the Bernoulli case ($P={{\\rm Bern}}(p)$ and $Q={{\\rm\nBern}}(q)$ with $p>q$) and the Gaussian case ($P=\\mathcal{N}(\\mu,1)$ and\n$Q=\\mathcal{N}(0,1)$ with $\\mu>0$), which correspond to the problem of planted\ndense subgraph recovery and submatrix localization respectively, the general\nresults lead to the following findings: (1) If $K=\\omega( n /\\log n)$, SDP\nattains the information-theoretic recovery limits with sharp constants; (2) If\n$K=\\Theta(n/\\log n)$, SDP is order-wise optimal, but strictly suboptimal by a\nconstant factor; (3) If $K=o(n/\\log n)$ and $K \\to \\infty$, SDP is order-wise\nsuboptimal. The same critical scaling for $K$ is found to hold, up to constant\nfactors, for the performance of SDP on the stochastic block model of $n$\nvertices partitioned into multiple communities of equal size $K$. A key\ningredient in the proof of the necessary condition is a construction of a\nprimal feasible solution based on random perturbation of the true cluster\nmatrix. \n\n"}
{"id": "1602.06604", "contents": "Title: Detection of Cyber-Physical Faults and Intrusions from Physical\n  Correlations Abstract: Cyber-physical systems are critical infrastructures that are crucial both to\nthe reliable delivery of resources such as energy, and to the stable\nfunctioning of automatic and control architectures. These systems are composed\nof interdependent physical, control and communications networks described by\ndisparate mathematical models creating scientific challenges that go well\nbeyond the modeling and analysis of the individual networks. A key challenge in\ncyber-physical defense is a fast online detection and localization of faults\nand intrusions without prior knowledge of the failure type. We describe a set\nof techniques for the efficient identification of faults from correlations in\nphysical signals, assuming only a minimal amount of available system\ninformation. The performance of our detection method is illustrated on data\ncollected from a large building automation system. \n\n"}
{"id": "1602.07043", "contents": "Title: Auditing Black-box Models for Indirect Influence Abstract: Data-trained predictive models see widespread use, but for the most part they\nare used as black boxes which output a prediction or score. It is therefore\nhard to acquire a deeper understanding of model behavior, and in particular how\ndifferent features influence the model prediction. This is important when\ninterpreting the behavior of complex models, or asserting that certain\nproblematic attributes (like race or gender) are not unduly influencing\ndecisions.\n  In this paper, we present a technique for auditing black-box models, which\nlets us study the extent to which existing models take advantage of particular\nfeatures in the dataset, without knowing how the models work. Our work focuses\non the problem of indirect influence: how some features might indirectly\ninfluence outcomes via other, related features. As a result, we can find\nattribute influences even in cases where, upon further direct examination of\nthe model, the attribute is not referred to by the model at all.\n  Our approach does not require the black-box model to be retrained. This is\nimportant if (for example) the model is only accessible via an API, and\ncontrasts our work with other methods that investigate feature influence like\nfeature selection. We present experimental evidence for the effectiveness of\nour procedure using a variety of publicly available datasets and models. We\nalso validate our procedure using techniques from interpretable learning and\nfeature selection, as well as against other black-box auditing procedures. \n\n"}
{"id": "1602.07048", "contents": "Title: Structural Diversity and Homophily: A Study Across More than One Hundred\n  Big Networks Abstract: A widely recognized organizing principle of networks is structural homophily,\nwhich suggests that people with more common neighbors are more likely to\nconnect with each other. However, what influence the diverse structures\nembedded in common neighbors have on link formation is much less\nwell-understood. To explore this problem, we begin by characterizing the\nstructural diversity of common neighborhoods. Using a collection of 120\nlarge-scale networks, we demonstrate that the impact of the common neighborhood\ndiversity on link existence can vary substantially across networks. We find\nthat its positive effect on Facebook and negative effect on LinkedIn suggest\ndifferent underlying networking needs in these networks. We also discover\nstriking cases where diversity violates the principle of homophily---that is,\nwhere fewer mutual connections may lead to a higher tendency to link with each\nother. We then leverage structural diversity to develop a common neighborhood\nsignature (CNS), which we apply to a large set of networks to uncover unique\nnetwork superfamilies not discoverable by conventional methods. Our findings\nshed light on the pursuit to understand the ways in which network structures\nare organized and formed, pointing to potential advancement in designing graph\ngeneration models and recommender systems. \n\n"}
{"id": "1602.07107", "contents": "Title: A Streaming Algorithm for Crowdsourced Data Classification Abstract: We propose a streaming algorithm for the binary classification of data based\non crowdsourcing. The algorithm learns the competence of each labeller by\ncomparing her labels to those of other labellers on the same tasks and uses\nthis information to minimize the prediction error rate on each task. We provide\nperformance guarantees of our algorithm for a fixed population of independent\nlabellers. In particular, we show that our algorithm is optimal in the sense\nthat the cumulative regret compared to the optimal decision with known labeller\nerror probabilities is finite, independently of the number of tasks to label.\nThe complexity of the algorithm is linear in the number of labellers and the\nnumber of tasks, up to some logarithmic factors. Numerical experiments\nillustrate the performance of our algorithm compared to existing algorithms,\nincluding simple majority voting and expectation-maximization algorithms, on\nboth synthetic and real datasets. \n\n"}
{"id": "1602.08194", "contents": "Title: Scalable and Sustainable Deep Learning via Randomized Hashing Abstract: Current deep learning architectures are growing larger in order to learn from\ncomplex datasets. These architectures require giant matrix multiplication\noperations to train millions of parameters. Conversely, there is another\ngrowing trend to bring deep learning to low-power, embedded devices. The matrix\noperations, associated with both training and testing of deep networks, are\nvery expensive from a computational and energy standpoint. We present a novel\nhashing based technique to drastically reduce the amount of computation needed\nto train and test deep networks. Our approach combines recent ideas from\nadaptive dropouts and randomized hashing for maximum inner product search to\nselect the nodes with the highest activation efficiently. Our new algorithm for\ndeep learning reduces the overall computational cost of forward and\nback-propagation by operating on significantly fewer (sparse) nodes. As a\nconsequence, our algorithm uses only 5% of the total multiplications, while\nkeeping on average within 1% of the accuracy of the original model. A unique\nproperty of the proposed hashing based back-propagation is that the updates are\nalways sparse. Due to the sparse gradient updates, our algorithm is ideally\nsuited for asynchronous and parallel training leading to near linear speedup\nwith increasing number of cores. We demonstrate the scalability and\nsustainability (energy efficiency) of our proposed algorithm via rigorous\nexperimental evaluations on several real datasets. \n\n"}
{"id": "1603.01431", "contents": "Title: Normalization Propagation: A Parametric Technique for Removing Internal\n  Covariate Shift in Deep Networks Abstract: While the authors of Batch Normalization (BN) identify and address an\nimportant problem involved in training deep networks-- Internal Covariate\nShift-- the current solution has certain drawbacks. Specifically, BN depends on\nbatch statistics for layerwise input normalization during training which makes\nthe estimates of mean and standard deviation of input (distribution) to hidden\nlayers inaccurate for validation due to shifting parameter values (especially\nduring initial training epochs). Also, BN cannot be used with batch-size 1\nduring training. We address these drawbacks by proposing a non-adaptive\nnormalization technique for removing internal covariate shift, that we call\nNormalization Propagation. Our approach does not depend on batch statistics,\nbut rather uses a data-independent parametric estimate of mean and\nstandard-deviation in every layer thus being computationally faster compared\nwith BN. We exploit the observation that the pre-activation before Rectified\nLinear Units follow Gaussian distribution in deep networks, and that once the\nfirst and second order statistics of any given dataset are normalized, we can\nforward propagate this normalization without the need for recalculating the\napproximate statistics for hidden layers. \n\n"}
{"id": "1603.07556", "contents": "Title: The interplay between conformity and anticonformity and its polarizing\n  effect on society Abstract: Simmering debates leading to polarization are observed in many domains.\nAlthough empirical findings show a strong correlation between this phenomenon\nand modularity of a social network, still little is known about the actual\nmechanisms driving communities to conflicting opinions. In this paper, we used\nan agent-based model to check if the polarization may be induced by a\ncompetition between two types of social response: conformity and\nanticonformity. The proposed model builds on the q-voter model (Castellano et\nal. 2009b) and uses a double-clique topology in order to capture segmentation\nof a community. Our results indicate that the interplay between intra-clique\nconformity and inter-clique anticonformity may indeed lead to a polarized state\nof the entire system. We have found a dynamic phase transition controlled by\nthe fraction $L$ of cross-links between cliques. In the regime of small values\nof $L$ system is able to reach the total positive consensus. If the values of\n$L$ are large enough, anticonformity takes over and the system always ends up\nin a polarized stated. Putting it the other way around, the segmentation of the\nnetwork is not a sufficient condition for the polarization to appear. A\nsuitable level of antagonistic interactions between segments is namely required\nto arrive at a polarized steady state within our model. \n\n"}
{"id": "1603.08048", "contents": "Title: \"Did I Say Something Wrong?\" A Word-Level Analysis of Wikipedia Articles\n  for Deletion Discussions Abstract: This thesis focuses on gaining linguistic insights into textual discussions\non a word level. It was of special interest to distinguish messages that\nconstructively contribute to a discussion from those that are detrimental to\nthem. Thereby, we wanted to determine whether \"I\"- and \"You\"-messages are\nindicators for either of the two discussion styles. These messages are nowadays\noften used in guidelines for successful communication. Although their effects\nhave been successfully evaluated multiple times, a large-scale analysis has\nnever been conducted.\n  Thus, we used Wikipedia Articles for Deletion (short: AfD) discussions\ntogether with the records of blocked users and developed a fully automated\ncreation of an annotated data set. In this data set, messages were labelled\neither constructive or disruptive. We applied binary classifiers to the data to\ndetermine characteristic words for both discussion styles. Thereby, we also\ninvestigated whether function words like pronouns and conjunctions play an\nimportant role in distinguishing the two.\n  We found that \"You\"-messages were a strong indicator for disruptive messages\nwhich matches their attributed effects on communication. However, we found\n\"I\"-messages to be indicative for disruptive messages as well which is contrary\nto their attributed effects. The importance of function words could neither be\nconfirmed nor refuted. Other characteristic words for either communication\nstyle were not found. Yet, the results suggest that a different model might\nrepresent disruptive and constructive messages in textual discussions better. \n\n"}
{"id": "1603.08616", "contents": "Title: Submodular Variational Inference for Network Reconstruction Abstract: In real-world and online social networks, individuals receive and transmit\ninformation in real time. Cascading information transmissions (e.g. phone\ncalls, text messages, social media posts) may be understood as a realization of\na diffusion process operating on the network, and its branching path can be\nrepresented by a directed tree. The process only traverses and thus reveals a\nlimited portion of the edges. The network reconstruction/inference problem is\nto infer the unrevealed connections. Most existing approaches derive a\nlikelihood and attempt to find the network topology maximizing the likelihood,\na problem that is highly intractable. In this paper, we focus on the network\nreconstruction problem for a broad class of real-world diffusion processes,\nexemplified by a network diffusion scheme called respondent-driven sampling\n(RDS). We prove that under realistic and general models of network diffusion,\nthe posterior distribution of an observed RDS realization is a Bayesian\nlog-submodular model.We then propose VINE (Variational Inference for Network\nrEconstruction), a novel, accurate, and computationally efficient variational\ninference algorithm, for the network reconstruction problem under this model.\nCrucially, we do not assume any particular probabilistic model for the\nunderlying network. VINE recovers any connected graph with high accuracy as\nshown by our experimental results on real-life networks. \n\n"}
{"id": "1604.00255", "contents": "Title: Network structure, metadata and the prediction of missing nodes and\n  annotations Abstract: The empirical validation of community detection methods is often based on\navailable annotations on the nodes that serve as putative indicators of the\nlarge-scale network structure. Most often, the suitability of the annotations\nas topological descriptors itself is not assessed, and without this it is not\npossible to ultimately distinguish between actual shortcomings of the community\ndetection algorithms on one hand, and the incompleteness, inaccuracy or\nstructured nature of the data annotations themselves on the other. In this work\nwe present a principled method to access both aspects simultaneously. We\nconstruct a joint generative model for the data and metadata, and a\nnonparametric Bayesian framework to infer its parameters from annotated\ndatasets. We assess the quality of the metadata not according to its direct\nalignment with the network communities, but rather in its capacity to predict\nthe placement of edges in the network. We also show how this feature can be\nused to predict the connections to missing nodes when only the metadata is\navailable, as well as missing metadata. By investigating a wide range of\ndatasets, we show that while there are seldom exact agreements between metadata\ntokens and the inferred data groups, the metadata is often informative of the\nnetwork structure nevertheless, and can improve the prediction of missing\nnodes. This shows that the method uncovers meaningful patterns in both the data\nand metadata, without requiring or expecting a perfect agreement between the\ntwo. \n\n"}
{"id": "1604.01131", "contents": "Title: Discovering items with potential popularity on social media Abstract: Predicting the future popularity of online content is highly important in\nmany applications. Preferential attachment phenomena is encountered in scale\nfree networks.Under it's influece popular items get more popular thereby\nresulting in long tailed distribution problem. Consequently, new items which\ncan be popular (potential ones), are suppressed by the already popular items.\nThis paper proposes a novel model which is able to identify potential items. It\nidentifies the potentially popular items by considering the number of links or\nratings it has recieved in recent past along with it's popularity decay. For\nobtaining an effecient model we consider only temporal features of the content,\navoiding the cost of extracting other features. We have found that people\nfollow recent behaviours of their peers. In presence of fit or quality items\nalready popular items lose it's popularity. Prediction accuracy is measured on\nthree industrial datasets namely Movielens, Netflix and Facebook wall post.\nExperimental results show that compare to state-of-the-art model our model have\nbetter prediction accuracy. \n\n"}
{"id": "1604.01200", "contents": "Title: On Equivalence of Likelihood Maximization of Stochastic Block Model and\n  Constrained Nonnegative Matrix Factorization Abstract: Community structures detection in complex network is important for\nunderstanding not only the topological structures of the network, but also the\nfunctions of it. Stochastic block model and nonnegative matrix factorization\nare two widely used methods for community detection, which are proposed from\ndifferent perspectives. In this paper, the relations between them are studied.\nThe logarithm of likelihood function for stochastic block model can be\nreformulated under the framework of nonnegative matrix factorization. Besides\nthe model equivalence, the algorithms employed by the two methods are\ndifferent. Preliminary numerical experiments are carried out to compare the\nbehaviors of the algorithms. \n\n"}
{"id": "1604.03159", "contents": "Title: Phase Transitions and a Model Order Selection Criterion for Spectral\n  Graph Clustering Abstract: One of the longstanding open problems in spectral graph clustering (SGC) is\nthe so-called model order selection problem: automated selection of the correct\nnumber of clusters. This is equivalent to the problem of finding the number of\nconnected components or communities in an undirected graph. We propose\nautomated model order selection (AMOS), a solution to the SGC model selection\nproblem under a random interconnection model (RIM) using a novel selection\ncriterion that is based on an asymptotic phase transition analysis. AMOS can\nmore generally be applied to discovering hidden block diagonal structure in\nsymmetric non-negative matrices. Numerical experiments on simulated graphs\nvalidate the phase transition analysis, and real-world network data is used to\nvalidate the performance of the proposed model selection procedure. \n\n"}
{"id": "1604.04939", "contents": "Title: Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor\n  Analysis Abstract: Factor analysis aims to determine latent factors, or traits, which summarize\na given data set. Inter-battery factor analysis extends this notion to multiple\nviews of the data. In this paper we show how a nonlinear, nonparametric version\nof these models can be recovered through the Gaussian process latent variable\nmodel. This gives us a flexible formalism for multi-view learning where the\nlatent variables can be used both for exploratory purposes and for learning\nrepresentations that enable efficient inference for ambiguous estimation tasks.\nLearning is performed in a Bayesian manner through the formulation of a\nvariational compression scheme which gives a rigorous lower bound on the log\nlikelihood. Our Bayesian framework provides strong regularization during\ntraining, allowing the structure of the latent space to be determined\nefficiently and automatically. We demonstrate this by producing the first (to\nour knowledge) published results of learning from dozens of views, even when\ndata is scarce. We further show experimental results on several different types\nof multi-view data sets and for different kinds of tasks, including exploratory\ndata analysis, generation, ambiguity modelling through latent priors and\nclassification. \n\n"}
{"id": "1604.05449", "contents": "Title: Streaming Label Learning for Modeling Labels on the Fly Abstract: It is challenging to handle a large volume of labels in multi-label learning.\nHowever, existing approaches explicitly or implicitly assume that all the\nlabels in the learning process are given, which could be easily violated in\nchanging environments. In this paper, we define and study streaming label\nlearning (SLL), i.e., labels are arrived on the fly, to model newly arrived\nlabels with the help of the knowledge learned from past labels. The core of SLL\nis to explore and exploit the relationships between new labels and past labels\nand then inherit the relationship into hypotheses of labels to boost the\nperformance of new classifiers. In specific, we use the label\nself-representation to model the label relationship, and SLL will be divided\ninto two steps: a regression problem and a empirical risk minimization (ERM)\nproblem. Both problems are simple and can be efficiently solved. We further\nshow that SLL can generate a tighter generalization error bound for new labels\nthan the general ERM framework with trace norm or Frobenius norm\nregularization. Finally, we implement extensive experiments on various\nbenchmark datasets to validate the new setting. And results show that SLL can\neffectively handle the constantly emerging new labels and provides excellent\nclassification performance. \n\n"}
{"id": "1604.08691", "contents": "Title: A Fast Sampling Method of Exploring Graphlet Degrees of Large Directed\n  and Undirected Graphs Abstract: Exploring small connected and induced subgraph patterns (CIS patterns, or\ngraphlets) has recently attracted considerable attention. Despite recent\nefforts on computing the number of instances a specific graphlet appears in a\nlarge graph (i.e., the total number of CISes isomorphic to the graphlet),\nlittle attention has been paid to characterizing a node's graphlet degree,\ni.e., the number of CISes isomorphic to the graphlet that include the node,\nwhich is an important metric for analyzing complex networks such as social and\nbiological networks. Similar to global graphlet counting, it is challenging to\ncompute node graphlet degrees for a large graph due to the combinatorial nature\nof the problem. Unfortunately, previous methods of computing global graphlet\ncounts are not suited to solve this problem. In this paper we propose sampling\nmethods to estimate node graphlet degrees for undirected and directed graphs,\nand analyze the error of our estimates. To the best of our knowledge, we are\nthe first to study this problem and give a fast scalable solution. We conduct\nexperiments on a variety of real-word datasets that demonstrate that our\nmethods accurately and efficiently estimate node graphlet degrees for graphs\nwith millions of edges. \n\n"}
{"id": "1605.01185", "contents": "Title: Linear Bandit algorithms using the Bootstrap Abstract: This study presents two new algorithms for solving linear stochastic bandit\nproblems. The proposed methods use an approach from non-parametric statistics\ncalled bootstrapping to create confidence bounds. This is achieved without\nmaking any assumptions about the distribution of noise in the underlying\nsystem. We present the X-Random and X-Fixed bootstrap bandits which correspond\nto the two well-known approaches for conducting bootstraps on models, in the\nliterature. The proposed methods are compared to other popular solutions for\nlinear stochastic bandit problems, namely, OFUL, LinUCB and Thompson Sampling.\nThe comparisons are carried out using a simulation study on a hierarchical\nprobability meta-model, built from published data of experiments, which are run\non real systems. The model representing the response surfaces is conceptualized\nas a Bayesian Network which is presented with varying degrees of noise for the\nsimulations. One of the proposed methods, X-Random bootstrap, performs better\nthan the baselines in-terms of cumulative regret across various degrees of\nnoise and different number of trials. In certain settings the cumulative regret\nof this method is less than half of the best baseline. The X-Fixed bootstrap\nperforms comparably in most situations and particularly well when the number of\ntrials is low. The study concludes that these algorithms could be a preferred\nalternative for solving linear bandit problems, especially when the\ndistribution of the noise in the system is unknown. \n\n"}
{"id": "1605.03391", "contents": "Title: Unbiased split variable selection for random survival forests using\n  maximally selected rank statistics Abstract: The most popular approach for analyzing survival data is the Cox regression\nmodel. The Cox model may, however, be misspecified, and its proportionality\nassumption may not always be fulfilled. An alternative approach for survival\nprediction is random forests for survival outcomes. The standard split\ncriterion for random survival forests is the log-rank test statistics, which\nfavors splitting variables with many possible split points. Conditional\ninference forests avoid this split variable selection bias. However, linear\nrank statistics are utilized by default in conditional inference forests to\nselect the optimal splitting variable, which cannot detect non-linear effects\nin the independent variables. An alternative is to use maximally selected rank\nstatistics for the split point selection. As in conditional inference forests,\nsplitting variables are compared on the p-value scale. However, instead of the\nconditional Monte-Carlo approach used in conditional inference forests, p-value\napproximations are employed. We describe several p-value approximations and the\nimplementation of the proposed random forest approach. A simulation study\ndemonstrates that unbiased split variable selection is possible. However, there\nis a trade-off between unbiased split variable selection and runtime. In\nbenchmark studies of prediction performance on simulated and real datasets the\nnew method performs better than random survival forests if informative\ndichotomous variables are combined with uninformative variables with more\ncategories and better than conditional inference forests if non-linear\ncovariate effects are included. In a runtime comparison the method proves to be\ncomputationally faster than both alternatives, if a simple p-value\napproximation is used. \n\n"}
{"id": "1605.05025", "contents": "Title: The Hourglass Effect in Hierarchical Dependency Networks Abstract: Many hierarchically modular systems are structured in a way that resembles an\nhourglass. This \"hourglass effect\" means that the system generates many outputs\nfrom many inputs through a relatively small number of intermediate modules that\nare critical for the operation of the entire system, referred to as the waist\nof the hourglass. We investigate the hourglass effect in general, not\nnecessarily layered, hierarchical dependency networks. Our analysis focuses on\nthe number of source-to-target dependency paths that traverse each vertex, and\nit identifies the core of a dependency network as the smallest set of vertices\nthat collectively cover almost all dependency paths. We then examine if a given\nnetwork exhibits the hourglass property or not, comparing its core size with a\n\"flat\" (i.e., non-hierarchical) network that preserves the source dependencies\nof each target in the original network. As a possible explanation for the\nhourglass effect, we propose the Reuse Preference (RP) model that captures the\nbias of new modules to reuse intermediate modules of similar complexity instead\nof connecting directly to sources or low complexity modules. We have applied\nthe proposed framework in a diverse set of dependency networks from\ntechnological, natural and information systems, showing that all these networks\nexhibit the general hourglass property but to a varying degree and with\ndifferent waist characteristics. \n\n"}
{"id": "1605.05676", "contents": "Title: Towards information based spatiotemporal patterns as a foundation for\n  agent representation in dynamical systems Abstract: We present some arguments why existing methods for representing agents fall\nshort in applications crucial to artificial life. Using a thought experiment\ninvolving a fictitious dynamical systems model of the biosphere we argue that\nthe metabolism, motility, and the concept of counterfactual variation should be\ncompatible with any agent representation in dynamical systems. We then propose\nan information-theoretic notion of \\emph{integrated spatiotemporal patterns}\nwhich we believe can serve as the basic building block of an agent definition.\nWe argue that these patterns are capable of solving the problems mentioned\nbefore. We also test this in some preliminary experiments. \n\n"}
{"id": "1605.06432", "contents": "Title: Deep Variational Bayes Filters: Unsupervised Learning of State Space\n  Models from Raw Data Abstract: We introduce Deep Variational Bayes Filters (DVBF), a new method for\nunsupervised learning and identification of latent Markovian state space\nmodels. Leveraging recent advances in Stochastic Gradient Variational Bayes,\nDVBF can overcome intractable inference distributions via variational\ninference. Thus, it can handle highly nonlinear input data with temporal and\nspatial dependencies such as image sequences without domain knowledge. Our\nexperiments show that enabling backpropagation through transitions enforces\nstate space assumptions and significantly improves information content of the\nlatent embedding. This also enables realistic long-term prediction. \n\n"}
{"id": "1605.07174", "contents": "Title: Kernel-based Reconstruction of Graph Signals Abstract: A number of applications in engineering, social sciences, physics, and\nbiology involve inference over networks. In this context, graph signals are\nwidely encountered as descriptors of vertex attributes or features in\ngraph-structured data. Estimating such signals in all vertices given noisy\nobservations of their values on a subset of vertices has been extensively\nanalyzed in the literature of signal processing on graphs (SPoG). This paper\nadvocates kernel regression as a framework generalizing popular SPoG modeling\nand reconstruction and expanding their capabilities. Formulating signal\nreconstruction as a regression task on reproducing kernel Hilbert spaces of\ngraph signals permeates benefits from statistical learning, offers fresh\ninsights, and allows for estimators to leverage richer forms of prior\ninformation than existing alternatives. A number of SPoG notions such as\nbandlimitedness, graph filters, and the graph Fourier transform are naturally\naccommodated in the kernel framework. Additionally, this paper capitalizes on\nthe so-called representer theorem to devise simpler versions of existing\nThikhonov regularized estimators, and offers a novel probabilistic\ninterpretation of kernel methods on graphs based on graphical models. Motivated\nby the challenges of selecting the bandwidth parameter in SPoG estimators or\nthe kernel map in kernel-based methods, the present paper further proposes two\nmulti-kernel approaches with complementary strengths. Whereas the first enables\nestimation of the unknown bandwidth of bandlimited signals, the second allows\nfor efficient graph filter selection. Numerical tests with synthetic as well as\nreal data demonstrate the merits of the proposed methods relative to\nstate-of-the-art alternatives. \n\n"}
{"id": "1605.07221", "contents": "Title: Global Optimality of Local Search for Low Rank Matrix Recovery Abstract: We show that there are no spurious local minima in the non-convex factorized\nparametrization of low-rank matrix recovery from incoherent linear\nmeasurements. With noisy measurements we show all local minima are very close\nto a global optimum. Together with a curvature bound at saddle points, this\nyields a polynomial time global convergence guarantee for stochastic gradient\ndescent {\\em from random initialization}. \n\n"}
{"id": "1605.07719", "contents": "Title: Reshaped Wirtinger Flow and Incremental Algorithm for Solving Quadratic\n  System of Equations Abstract: We study the phase retrieval problem, which solves quadratic system of\nequations, i.e., recovers a vector $\\boldsymbol{x}\\in \\mathbb{R}^n$ from its\nmagnitude measurements $y_i=|\\langle \\boldsymbol{a}_i, \\boldsymbol{x}\\rangle|,\ni=1,..., m$. We develop a gradient-like algorithm (referred to as RWF\nrepresenting reshaped Wirtinger flow) by minimizing a nonconvex nonsmooth loss\nfunction. In comparison with existing nonconvex Wirtinger flow (WF) algorithm\n\\cite{candes2015phase}, although the loss function becomes nonsmooth, it\ninvolves only the second power of variable and hence reduces the complexity. We\nshow that for random Gaussian measurements, RWF enjoys geometric convergence to\na global optimal point as long as the number $m$ of measurements is on the\norder of $n$, the dimension of the unknown $\\boldsymbol{x}$. This improves the\nsample complexity of WF, and achieves the same sample complexity as truncated\nWirtinger flow (TWF) \\cite{chen2015solving}, but without truncation in gradient\nloop. Furthermore, RWF costs less computationally than WF, and runs faster\nnumerically than both WF and TWF. We further develop the incremental\n(stochastic) reshaped Wirtinger flow (IRWF) and show that IRWF converges\nlinearly to the true signal. We further establish performance guarantee of an\nexisting Kaczmarz method for the phase retrieval problem based on its\nconnection to IRWF. We also empirically demonstrate that IRWF outperforms\nexisting ITWF algorithm (stochastic version of TWF) as well as other batch\nalgorithms. \n\n"}
{"id": "1605.08361", "contents": "Title: No bad local minima: Data independent training error guarantees for\n  multilayer neural networks Abstract: We use smoothed analysis techniques to provide guarantees on the training\nloss of Multilayer Neural Networks (MNNs) at differentiable local minima.\nSpecifically, we examine MNNs with piecewise linear activation functions,\nquadratic loss and a single output, under mild over-parametrization. We prove\nthat for a MNN with one hidden layer, the training error is zero at every\ndifferentiable local minimum, for almost every dataset and dropout-like noise\nrealization. We then extend these results to the case of more than one hidden\nlayer. Our theoretical guarantees assume essentially nothing on the training\ndata, and are verified numerically. These results suggest why the highly\nnon-convex loss of such MNNs can be easily optimized using local updates (e.g.,\nstochastic gradient descent), as observed empirically. \n\n"}
{"id": "1605.09004", "contents": "Title: Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit\n  Problem Abstract: We consider the problem of \\textit{best arm identification} with a\n\\textit{fixed budget $T$}, in the $K$-armed stochastic bandit setting, with\narms distribution defined on $[0,1]$. We prove that any bandit strategy, for at\nleast one bandit problem characterized by a complexity $H$, will misidentify\nthe best arm with probability lower bounded by\n$$\\exp\\Big(-\\frac{T}{\\log(K)H}\\Big),$$ where $H$ is the sum for all sub-optimal\narms of the inverse of the squared gaps. Our result disproves formally the\ngeneral belief - coming from results in the fixed confidence setting - that\nthere must exist an algorithm for this problem whose probability of error is\nupper bounded by $\\exp(-T/H)$. This also proves that some existing strategies\nbased on the Successive Rejection of the arms are optimal - closing therefore\nthe current gap between upper and lower bounds for the fixed budget best arm\nidentification problem. \n\n"}
{"id": "1606.00704", "contents": "Title: Adversarially Learned Inference Abstract: We introduce the adversarially learned inference (ALI) model, which jointly\nlearns a generation network and an inference network using an adversarial\nprocess. The generation network maps samples from stochastic latent variables\nto the data space while the inference network maps training examples in data\nspace to the space of latent variables. An adversarial game is cast between\nthese two networks and a discriminative network is trained to distinguish\nbetween joint latent/data-space samples from the generative network and joint\nsamples from the inference network. We illustrate the ability of the model to\nlearn mutually coherent inference and generation networks through the\ninspections of model samples and reconstructions and confirm the usefulness of\nthe learned representations by obtaining a performance competitive with\nstate-of-the-art on the semi-supervised SVHN and CIFAR10 tasks. \n\n"}
{"id": "1606.00800", "contents": "Title: Multi-View Treelet Transform Abstract: Current multi-view factorization methods make assumptions that are not\nacceptable for many kinds of data, and in particular, for graphical data with\nhierarchical structure. At the same time, current hierarchical methods work\nonly in the single-view setting. We generalize the Treelet Transform to the\nMulti-View Treelet Transform (MVTT) to allow for the capture of hierarchical\nstructure when multiple views are available. Further, we show how this\ngeneralization is consistent with the existing theory and how it might be used\nin denoising empirical networks and in computing the shared response of\nfunctional brain data. \n\n"}
{"id": "1606.02346", "contents": "Title: How is a data-driven approach better than random choice in label space\n  division for multi-label classification? Abstract: We propose using five data-driven community detection approaches from social\nnetworks to partition the label space for the task of multi-label\nclassification as an alternative to random partitioning into equal subsets as\nperformed by RAkELd: modularity-maximizing fastgreedy and leading eigenvector,\ninfomap, walktrap and label propagation algorithms. We construct a label\nco-occurence graph (both weighted an unweighted versions) based on training\ndata and perform community detection to partition the label set. We include\nBinary Relevance and Label Powerset classification methods for comparison. We\nuse gini-index based Decision Trees as the base classifier. We compare educated\napproaches to label space divisions against random baselines on 12 benchmark\ndata sets over five evaluation measures. We show that in almost all cases seven\neducated guess approaches are more likely to outperform RAkELd than otherwise\nin all measures, but Hamming Loss. We show that fastgreedy and walktrap\ncommunity detection methods on weighted label co-occurence graphs are 85-92%\nmore likely to yield better F1 scores than random partitioning. Infomap on the\nunweighted label co-occurence graphs is on average 90% of the times better than\nrandom paritioning in terms of Subset Accuracy and 89% when it comes to Jaccard\nsimilarity. Weighted fastgreedy is better on average than RAkELd when it comes\nto Hamming Loss. \n\n"}
{"id": "1606.02702", "contents": "Title: Efficient Smoothed Concomitant Lasso Estimation for High Dimensional\n  Regression Abstract: In high dimensional settings, sparse structures are crucial for efficiency,\nboth in term of memory, computation and performance. It is customary to\nconsider $\\ell_1$ penalty to enforce sparsity in such scenarios. Sparsity\nenforcing methods, the Lasso being a canonical example, are popular candidates\nto address high dimension. For efficiency, they rely on tuning a parameter\ntrading data fitting versus sparsity. For the Lasso theory to hold this tuning\nparameter should be proportional to the noise level, yet the latter is often\nunknown in practice. A possible remedy is to jointly optimize over the\nregression parameter as well as over the noise level. This has been considered\nunder several names in the literature: Scaled-Lasso, Square-root Lasso,\nConcomitant Lasso estimation for instance, and could be of interest for\nconfidence sets or uncertainty quantification. In this work, after illustrating\nnumerical difficulties for the Smoothed Concomitant Lasso formulation, we\npropose a modification we coined Smoothed Concomitant Lasso, aimed at\nincreasing numerical stability. We propose an efficient and accurate solver\nleading to a computational cost no more expansive than the one for the Lasso.\nWe leverage on standard ingredients behind the success of fast Lasso solvers: a\ncoordinate descent algorithm, combined with safe screening rules to achieve\nspeed efficiency, by eliminating early irrelevant features. \n\n"}
{"id": "1606.04721", "contents": "Title: Personality Traits and Echo Chambers on Facebook Abstract: In online social networks, users tend to select information that adhere to\ntheir system of beliefs and to form polarized groups of like minded people.\nPolarization as well as its effects on online social interactions have been\nextensively investigated. Still, the relation between group formation and\npersonality traits remains unclear. A better understanding of the cognitive and\npsychological determinants of online social dynamics might help to design more\nefficient communication strategies and to challenge the digital misinformation\nthreat. In this work, we focus on users commenting posts published by US\nFacebook pages supporting scientific and conspiracy-like narratives, and we\nclassify the personality traits of those users according to their online\nbehavior. We show that different and conflicting communities are populated by\nusers showing similar psychological profiles, and that the dominant personality\nmodel is the same in both scientific and conspiracy echo chambers. Moreover, we\nobserve that the permanence within echo chambers slightly shapes users'\npsychological profiles. Our results suggest that the presence of specific\npersonality traits in individuals lead to their considerable involvement in\nsupporting narratives inside virtual echo chambers. \n\n"}
{"id": "1606.04838", "contents": "Title: Optimization Methods for Large-Scale Machine Learning Abstract: This paper provides a review and commentary on the past, present, and future\nof numerical optimization algorithms in the context of machine learning\napplications. Through case studies on text classification and the training of\ndeep neural networks, we discuss how optimization problems arise in machine\nlearning and what makes them challenging. A major theme of our study is that\nlarge-scale machine learning represents a distinctive setting in which the\nstochastic gradient (SG) method has traditionally played a central role while\nconventional gradient-based nonlinear optimization techniques typically falter.\nBased on this viewpoint, we present a comprehensive theory of a\nstraightforward, yet versatile SG algorithm, discuss its practical behavior,\nand highlight opportunities for designing algorithms with improved performance.\nThis leads to a discussion about the next generation of optimization methods\nfor large-scale machine learning, including an investigation of two main\nstreams of research on techniques that diminish noise in the stochastic\ndirections and methods that make use of second-order derivative approximations. \n\n"}
{"id": "1606.04838", "contents": "Title: Optimization Methods for Large-Scale Machine Learning Abstract: This paper provides a review and commentary on the past, present, and future\nof numerical optimization algorithms in the context of machine learning\napplications. Through case studies on text classification and the training of\ndeep neural networks, we discuss how optimization problems arise in machine\nlearning and what makes them challenging. A major theme of our study is that\nlarge-scale machine learning represents a distinctive setting in which the\nstochastic gradient (SG) method has traditionally played a central role while\nconventional gradient-based nonlinear optimization techniques typically falter.\nBased on this viewpoint, we present a comprehensive theory of a\nstraightforward, yet versatile SG algorithm, discuss its practical behavior,\nand highlight opportunities for designing algorithms with improved performance.\nThis leads to a discussion about the next generation of optimization methods\nfor large-scale machine learning, including an investigation of two main\nstreams of research on techniques that diminish noise in the stochastic\ndirections and methods that make use of second-order derivative approximations. \n\n"}
{"id": "1606.05248", "contents": "Title: Leadership Network and Team Performance in Interactive Contests Abstract: Over the years, the concept of leadership has experienced a paradigm shift -\nfrom solitary leader (centralized leadership) to de-centralized leadership or\ndistributed leadership. This paper explores the idea that centralized\nleadership, as earlier suggested, negatively impacts team performance. I\napplied the hypothesis to cricket, a sport in which leaders play an important\nrole in team's success. I generated batting partnership network and evaluated\nthe central-most player in the team, applying tools of social network analysis.\nAnalyzing 3420 matches in one day international cricket and 1979 Test matches\ninvolving 10 teams, I examined the impact of centralized leadership in outcome\nof a contest. I observed that the odds for winning a one day international\nmatch under centralized leadership is 30% higher than the odds for winning\nunder de-centralized leadership. In both forms of cricket (Test and one day\ninternational ), I failed to find evidence that distributed leadership is\nassociated with higher team performance. These results suggest important\nimplications for cricket administrators in development and management of\nworking teams. \n\n"}
{"id": "1606.05325", "contents": "Title: ACDC: $\\alpha$-Carving Decision Chain for Risk Stratification Abstract: In many healthcare settings, intuitive decision rules for risk stratification\ncan help effective hospital resource allocation. This paper introduces a novel\nvariant of decision tree algorithms that produces a chain of decisions, not a\ngeneral tree. Our algorithm, $\\alpha$-Carving Decision Chain (ACDC),\nsequentially carves out \"pure\" subsets of the majority class examples. The\nresulting chain of decision rules yields a pure subset of the minority class\nexamples. Our approach is particularly effective in exploring large and\nclass-imbalanced health datasets. Moreover, ACDC provides an interactive\ninterpretation in conjunction with visual performance metrics such as Receiver\nOperating Characteristics curve and Lift chart. \n\n"}
{"id": "1606.05694", "contents": "Title: DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using\n  Character and Word-Level CNNs Abstract: This paper describes our approach for the Detecting Stance in Tweets task\n(SemEval-2016 Task 6). We utilized recent advances in short text categorization\nusing deep learning to create word-level and character-level models. The choice\nbetween word-level and character-level models in each particular case was\ninformed through validation performance. Our final system is a combination of\nclassifiers using word-level or character-level models. We also employed novel\ndata augmentation techniques to expand and diversify our training dataset, thus\nmaking our system more robust. Our system achieved a macro-average precision,\nrecall and F1-scores of 0.67, 0.61 and 0.635 respectively. \n\n"}
{"id": "1606.06962", "contents": "Title: Towards stationary time-vertex signal processing Abstract: Graph-based methods for signal processing have shown promise for the analysis\nof data exhibiting irregular structure, such as those found in social,\ntransportation, and sensor networks. Yet, though these systems are often\ndynamic, state-of-the-art methods for signal processing on graphs ignore the\ndimension of time, treating successive graph signals independently or taking a\nglobal average. To address this shortcoming, this paper considers the\nstatistical analysis of time-varying graph signals. We introduce a novel\ndefinition of joint (time-vertex) stationarity, which generalizes the classical\ndefinition of time stationarity and the more recent definition appropriate for\ngraphs. Joint stationarity gives rise to a scalable Wiener optimization\nframework for joint denoising, semi-supervised learning, or more generally\ninversing a linear operator, that is provably optimal. Experimental results on\nreal weather data demonstrate that taking into account graph and time\ndimensions jointly can yield significant accuracy improvements in the\nreconstruction effort. \n\n"}
{"id": "1606.08561", "contents": "Title: Estimating the class prior and posterior from noisy positives and\n  unlabeled data Abstract: We develop a classification algorithm for estimating posterior distributions\nfrom positive-unlabeled data, that is robust to noise in the positive labels\nand effective for high-dimensional data. In recent years, several algorithms\nhave been proposed to learn from positive-unlabeled data; however, many of\nthese contributions remain theoretical, performing poorly on real\nhigh-dimensional data that is typically contaminated with noise. We build on\nthis previous work to develop two practical classification algorithms that\nexplicitly model the noise in the positive labels and utilize univariate\ntransforms built on discriminative classifiers. We prove that these univariate\ntransforms preserve the class prior, enabling estimation in the univariate\nspace and avoiding kernel density estimation for high-dimensional data. The\ntheoretical development and both parametric and nonparametric algorithms\nproposed here constitutes an important step towards wide-spread use of robust\nclassification algorithms for positive-unlabeled data. \n\n"}
{"id": "1606.09190", "contents": "Title: A Semi-Definite Programming approach to low dimensional embedding for\n  unsupervised clustering Abstract: This paper proposes a variant of the method of Gu\\'edon and Verhynin for\nestimating the cluster matrix in the Mixture of Gaussians framework via\nSemi-Definite Programming. A clustering oriented embedding is deduced from this\nestimate. The procedure is suitable for very high dimensional data because it\nis based on pairwise distances only. Theoretical garantees are provided and an\neigenvalue optimisation approach is proposed for computing the embedding. The\nperformance of the method is illustrated via Monte Carlo experiements and\ncomparisons with other embeddings from the literature. \n\n"}
{"id": "1607.00022", "contents": "Title: Modeling confirmation bias and polarization Abstract: Online users tend to select claims that adhere to their system of beliefs and\nto ignore dissenting information. Confirmation bias, indeed, plays a pivotal\nrole in viral phenomena. Furthermore, the wide availability of content on the\nweb fosters the aggregation of likeminded people where debates tend to enforce\ngroup polarization. Such a configuration might alter the public debate and thus\nthe formation of the public opinion. In this paper we provide a mathematical\nmodel to study online social debates and the related polarization dynamics. We\nassume the basic updating rule of the Bounded Confidence Model (BCM) and we\ndevelop two variations a) the Rewire with Bounded Confidence Model (RBCM), in\nwhich discordant links are broken until convergence is reached; and b) the\nUnbounded Confidence Model, under which the interaction among discordant pairs\nof users is allowed even with a negative feedback, either with the rewiring\nstep (RUCM) or without it (UCM). From numerical simulations we find that the\nnew models (UCM and RUCM), unlike the BCM, are able to explain the coexistence\nof two stable final opinions, often observed in reality. Lastly, we present a\nmean field approximation of the newly introduced models. \n\n"}
{"id": "1607.01032", "contents": "Title: Echo Chambers: Emotional Contagion and Group Polarization on Facebook Abstract: Recent findings showed that users on Facebook tend to select information that\nadhere to their system of beliefs and to form polarized groups -- i.e., echo\nchambers. Such a tendency dominates information cascades and might affect\npublic debates on social relevant issues. In this work we explore the\nstructural evolution of communities of interest by accounting for users\nemotions and engagement. Focusing on the Facebook pages reporting on scientific\nand conspiracy content, we characterize the evolution of the size of the two\ncommunities by fitting daily resolution data with three growth models -- i.e.\nthe Gompertz model, the Logistic model, and the Log-logistic model. Then, we\nexplore the interplay between emotional state and engagement of users in the\ngroup dynamics. Our findings show that communities' emotional behavior is\naffected by the users' involvement inside the echo chamber. Indeed, to an\nhigher involvement corresponds a more negative approach. Moreover, we observe\nthat, on average, more active users show a faster shift towards the negativity\nthan less active ones. \n\n"}
{"id": "1607.01668", "contents": "Title: Tensor Decomposition for Signal Processing and Machine Learning Abstract: Tensors or {\\em multi-way arrays} are functions of three or more indices\n$(i,j,k,\\cdots)$ -- similar to matrices (two-way arrays), which are functions\nof two indices $(r,c)$ for (row,column). Tensors have a rich history,\nstretching over almost a century, and touching upon numerous disciplines; but\nthey have only recently become ubiquitous in signal and data analytics at the\nconfluence of signal processing, statistics, data mining and machine learning.\nThis overview article aims to provide a good starting point for researchers and\npractitioners interested in learning about and working with tensors. As such,\nit focuses on fundamentals and motivation (using various application examples),\naiming to strike an appropriate balance of breadth {\\em and depth} that will\nenable someone having taken first graduate courses in matrix algebra and\nprobability to get started doing research and/or developing tensor algorithms\nand software. Some background in applied optimization is useful but not\nstrictly required. The material covered includes tensor rank and rank\ndecomposition; basic tensor factorization models and their relationships and\nproperties (including fairly good coverage of identifiability); broad coverage\nof algorithms ranging from alternating optimization to stochastic gradient;\nstatistical performance analysis; and applications ranging from source\nseparation to collaborative filtering, mixture and topic modeling,\nclassification, and multilinear subspace learning. \n\n"}
{"id": "1607.01760", "contents": "Title: Information-theoretic thresholds for community detection in sparse\n  networks Abstract: We give upper and lower bounds on the information-theoretic threshold for\ncommunity detection in the stochastic block model. Specifically, consider the\nsymmetric stochastic block model with $q$ groups, average degree $d$, and\nconnection probabilities $c_\\text{in}/n$ and $c_\\text{out}/n$ for within-group\nand between-group edges respectively; let $\\lambda =\n(c_\\text{in}-c_\\text{out})/(qd)$. We show that, when $q$ is large, and $\\lambda\n= O(1/q)$, the critical value of $d$ at which community detection becomes\npossible---in physical terms, the condensation threshold---is \\[ d_\\text{c} =\n\\Theta\\!\\left( \\frac{\\log q}{q \\lambda^2} \\right) \\, , \\] with tighter results\nin certain regimes. Above this threshold, we show that any partition of the\nnodes into $q$ groups which is as `good' as the planted one, in terms of the\nnumber of within- and between-group edges, is correlated with it. This gives an\nexponential-time algorithm that performs better than chance; specifically,\ncommunity detection becomes possible below the Kesten-Stigum bound for $q \\ge\n5$ in the disassortative case $\\lambda < 0$, and for $q \\ge 11$ in the\nassortative case $\\lambda >0$ (similar upper bounds were obtained independently\nby Abbe and Sandon). Conversely, below this threshold, we show that no\nalgorithm can label the vertices better than chance, or even distinguish the\nblock model from an \\ER\\ random graph with high probability.\n  Our lower bound on $d_\\text{c}$ uses Robinson and Wormald's small subgraph\nconditioning method, and we also give (less explicit) results for non-symmetric\nstochastic block models. In the symmetric case, we obtain explicit results by\nusing bounds on certain functions of doubly stochastic matrices due to\nAchlioptas and Naor; indeed, our lower bound on $d_\\text{c}$ is their second\nmoment lower bound on the $q$-colorability threshold for random graphs with a\ncertain effective degree. \n\n"}
{"id": "1607.02441", "contents": "Title: Generalized Hypergeometric Ensembles: Statistical Hypothesis Testing in\n  Complex Networks Abstract: Statistical ensembles of networks, i.e., probability spaces of all networks\nthat are consistent with given aggregate statistics, have become instrumental\nin the analysis of complex networks. Their numerical and analytical study\nprovides the foundation for the inference of topological patterns, the\ndefinition of network-analytic measures, as well as for model selection and\nstatistical hypothesis testing. Contributing to the foundation of these data\nanalysis techniques, in this Letter we introduce generalized hypergeometric\nensembles, a broad class of analytically tractable statistical ensembles of\nfinite, directed and weighted networks. This framework can be interpreted as a\ngeneralization of the classical configuration model, which is commonly used to\nrandomly generate networks with a given degree sequence or distribution. Our\ngeneralization rests on the introduction of dyadic link propensities, which\ncapture the degree-corrected tendencies of pairs of nodes to form edges between\neach other. Studying empirical and synthetic data, we show that our approach\nprovides broad perspectives for model selection and statistical hypothesis\ntesting in data on complex networks. \n\n"}
{"id": "1607.03313", "contents": "Title: Predicting the evolution of stationary graph signals Abstract: An emerging way of tackling the dimensionality issues arising in the modeling\nof a multivariate process is to assume that the inherent data structure can be\ncaptured by a graph. Nevertheless, though state-of-the-art graph-based methods\nhave been successful for many learning tasks, they do not consider\ntime-evolving signals and thus are not suitable for prediction. Based on the\nrecently introduced joint stationarity framework for time-vertex processes,\nthis letter considers multivariate models that exploit the graph topology so as\nto facilitate the prediction. The resulting method yields similar accuracy to\nthe joint (time-graph) mean-squared error estimator but at lower complexity,\nand outperforms purely time-based methods. \n\n"}
{"id": "1607.04579", "contents": "Title: Learning from Conditional Distributions via Dual Embeddings Abstract: Many machine learning tasks, such as learning with invariance and policy\nevaluation in reinforcement learning, can be characterized as problems of\nlearning from conditional distributions. In such problems, each sample $x$\nitself is associated with a conditional distribution $p(z|x)$ represented by\nsamples $\\{z_i\\}_{i=1}^M$, and the goal is to learn a function $f$ that links\nthese conditional distributions to target values $y$. These learning problems\nbecome very challenging when we only have limited samples or in the extreme\ncase only one sample from each conditional distribution. Commonly used\napproaches either assume that $z$ is independent of $x$, or require an\noverwhelmingly large samples from each conditional distribution.\n  To address these challenges, we propose a novel approach which employs a new\nmin-max reformulation of the learning from conditional distribution problem.\nWith such new reformulation, we only need to deal with the joint distribution\n$p(z,x)$. We also design an efficient learning algorithm, Embedding-SGD, and\nestablish theoretical sample complexity for such problems. Finally, our\nnumerical experiments on both synthetic and real-world datasets show that the\nproposed approach can significantly improve over the existing algorithms. \n\n"}
{"id": "1607.06341", "contents": "Title: Seasonal and geographical impact on human resting periods Abstract: We study the influence of seasonally and geographically related daily\ndynamics of daylight and ambient temperature on human resting or sleeping\npatterns using mobile phone data of a large number of individuals. We observe\ntwo daily inactivity periods in the people's aggregated mobile phone calling\npatterns and infer these to represent the resting times of the population. We\nfind that the nocturnal resting period is strongly influenced by the length of\ndaylight, and that its seasonal variation depends on the latitude, such that\nfor people living in two different cities separated by eight latitudinal\ndegrees, the difference in the resting period of people between the summer and\nwinter in southern cities is almost twice that in the northern cities. We also\nobserve that the duration of the afternoon resting period is influenced by the\ntemperature, and that there is a threshold from which this influence sets in.\nFinally, we observe that the yearly dynamics of the afternoon and nocturnal\nresting periods appear to be counterbalancing each other. This also lends\nsupport to the notion that the total daily resting time of people is more or\nless conserved across the year. \n\n"}
{"id": "1607.06450", "contents": "Title: Layer Normalization Abstract: Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques. \n\n"}
{"id": "1607.06993", "contents": "Title: Community Detection in Degree-Corrected Block Models Abstract: Community detection is a central problem of network data analysis. Given a\nnetwork, the goal of community detection is to partition the network nodes into\na small number of clusters, which could often help reveal interesting\nstructures. The present paper studies community detection in Degree-Corrected\nBlock Models (DCBMs). We first derive asymptotic minimax risks of the problem\nfor a misclassification proportion loss under appropriate conditions. The\nminimax risks are shown to depend on degree-correction parameters, community\nsizes, and average within and between community connectivities in an intuitive\nand interpretable way. In addition, we propose a polynomial time algorithm to\nadaptively perform consistent and even asymptotically optimal community\ndetection in DCBMs. \n\n"}
{"id": "1607.08194", "contents": "Title: Convolutional Neural Networks Analyzed via Convolutional Sparse Coding Abstract: Convolutional neural networks (CNN) have led to many state-of-the-art results\nspanning through various fields. However, a clear and profound theoretical\nunderstanding of the forward pass, the core algorithm of CNN, is still lacking.\nIn parallel, within the wide field of sparse approximation, Convolutional\nSparse Coding (CSC) has gained increasing attention in recent years. A\ntheoretical study of this model was recently conducted, establishing it as a\nreliable and stable alternative to the commonly practiced patch-based\nprocessing. Herein, we propose a novel multi-layer model, ML-CSC, in which\nsignals are assumed to emerge from a cascade of CSC layers. This is shown to be\ntightly connected to CNN, so much so that the forward pass of the CNN is in\nfact the thresholding pursuit serving the ML-CSC model. This connection brings\na fresh view to CNN, as we are able to attribute to this architecture\ntheoretical claims such as uniqueness of the representations throughout the\nnetwork, and their stable estimation, all guaranteed under simple local\nsparsity conditions. Lastly, identifying the weaknesses in the above pursuit\nscheme, we propose an alternative to the forward pass, which is connected to\ndeconvolutional, recurrent and residual networks, and has better theoretical\nguarantees. \n\n"}
{"id": "1607.08497", "contents": "Title: On Varying Topology of Complex Networks and Performance Limitations of\n  Community Detection Algorithms Abstract: One of the most widely studied problem in mining and analysis of complex\nnetworks is the detection of community structures. The problem has been\nextensively studied by researchers due to its high utility and numerous\napplications in various domains. Many algorithmic solutions have been proposed\nfor the community detection problem but the quest to find the best algorithm is\nstill on. More often than not, researchers focus on developing fast and\naccurate algorithms that can be generically applied to networks from a variety\nof domains without taking into consideration the structural and topological\nvariations in these networks.\n  In this paper, we evaluate the performance of different clustering algorithms\nas a function of varying network topology. Along with the well known LFR model\nto generate benchmark networks with communities,we also propose a new model\nnamed Naive Scale Free Model to study the behavior of community detection\nalgorithms with respect to different topological features. More specifically,\nwe are interested in the size of networks, the size of community structures,\nthe average connectivity of nodes and the ratio of inter-intra cluster edges.\nResults reveal several limitations of the current popular network clustering\nalgorithms failing to correctly find communities. This suggests the need to\nrevisit the design of current clustering algorithms that fail to incorporate\nvarying topological features of different networks. \n\n"}
{"id": "1607.08654", "contents": "Title: Characterizing Complex Networks with Forman-Ricci Curvature and\n  Associated Geometric Flows Abstract: We introduce Forman-Ricci curvature and its corresponding flow as\ncharacteristics for complex networks attempting to extend the common approach\nof node-based network analysis by edge-based characteristics. Following a\ntheoretical introduction and mathematical motivation, we apply the proposed\nnetwork-analytic methods to static and dynamic complex networks and compare the\nresults with established node-based characteristics. Our work suggests a number\nof applications for data mining, including denoising and clustering of\nexperimental data, as well as extrapolation of network evolution. \n\n"}
{"id": "1608.02024", "contents": "Title: Public Opinion Polling with Twitter Abstract: Solicited public opinion surveys reach a limited subpopulation of willing\nparticipants and are expensive to conduct, leading to poor time resolution and\na restricted pool of expert-chosen survey topics. In this study, we demonstrate\nthat unsolicited public opinion polling through sentiment analysis applied to\nTwitter correlates well with a range of traditional measures, and has\npredictive power for issues of global importance. We also examine Twitter's\npotential to canvas topics seldom surveyed, including ideas, personal feelings,\nand perceptions of commercial enterprises. Two of our major observations are\nthat appropriately filtered Twitter sentiment (1) predicts President Obama's\njob approval three months in advance, and (2) correlates well with surveyed\nconsumer sentiment. To make possible a full examination of our work and to\nenable others' research, we make public over 10,000 data sets, each a\nseven-year series of daily word counts for tweets containing a frequently used\nsearch term. \n\n"}
{"id": "1608.02861", "contents": "Title: Classification with the pot-pot plot Abstract: We propose a procedure for supervised classification that is based on\npotential functions. The potential of a class is defined as a kernel density\nestimate multiplied by the class's prior probability. The method transforms the\ndata to a potential-potential (pot-pot) plot, where each data point is mapped\nto a vector of potentials. Separation of the classes, as well as classification\nof new data points, is performed on this plot. For this, either the\n$\\alpha$-procedure ($\\alpha$-P) or $k$-nearest neighbors ($k$-NN) are employed.\nFor data that are generated from continuous distributions, these classifiers\nprove to be strongly Bayes-consistent. The potentials depend on the kernel and\nits bandwidth used in the density estimate. We investigate several variants of\nbandwidth selection, including joint and separate pre-scaling and a bandwidth\nregression approach. The new method is applied to benchmark data from the\nliterature, including simulated data sets as well as 50 sets of real data. It\ncompares favorably to known classification methods such as LDA, QDA, max kernel\ndensity estimates, $k$-NN, and $DD$-plot classification using depth functions. \n\n"}
{"id": "1608.05138", "contents": "Title: Hybrid CPU-GPU Framework for Network Motifs Abstract: Massively parallel architectures such as the GPU are becoming increasingly\nimportant due to the recent proliferation of data. In this paper, we propose a\nkey class of hybrid parallel graphlet algorithms that leverages multiple CPUs\nand GPUs simultaneously for computing k-vertex induced subgraph statistics\n(called graphlets). In addition to the hybrid multi-core CPU-GPU framework, we\nalso investigate single GPU methods (using multiple cores) and multi-GPU\nmethods that leverage all available GPUs simultaneously for computing induced\nsubgraph statistics. Both methods leverage GPU devices only, whereas the hybrid\nmulti-core CPU-GPU framework leverages all available multi-core CPUs and\nmultiple GPUs for computing graphlets in large networks. Compared to recent\napproaches, our methods are orders of magnitude faster, while also more cost\neffective enjoying superior performance per capita and per watt. In particular,\nthe methods are up to 300 times faster than the recent state-of-the-art method.\nTo the best of our knowledge, this is the first work to leverage multiple CPUs\nand GPUs simultaneously for computing induced subgraph statistics. \n\n"}
{"id": "1608.06328", "contents": "Title: Functional Multiplex PageRank Abstract: Recently it has been recognized that many complex social, technological and\nbiological networks have a multilayer nature and can be described by multiplex\nnetworks. Multiplex networks are formed by a set of nodes connected by links\nhaving different connotations forming the different layers of the multiplex.\nCharacterizing the centrality of the nodes in a multiplex network is a\nchallenging task since the centrality of the node naturally depends on the\nimportance associated to links of a certain type. Here we propose to assign to\neach node of a multiplex network a centrality called Functional Multiplex\nPageRank that is a function of the weights given to every different pattern of\nconnections (multilinks) existent in the multiplex network between any two\nnodes. Since multilinks distinguish all the possible ways in which the links in\ndifferent layers can overlap, the Functional Multiplex PageRank can describe\nimportant non-linear effects when large relevance or small relevance is\nassigned to multilinks with overlap. Here we apply the Functional Page Rank to\nthe multiplex airport networks, to the neuronal network of the nematode\nc.elegans, and to social collaboration and citation networks between\nscientists. This analysis reveals important differences existing between the\nmost central nodes of these networks, and the correlations between their so\ncalled \"pattern to success\". \n\n"}
{"id": "1608.08698", "contents": "Title: Reconstructing parameters of spreading models from partial observations Abstract: Spreading processes are often modelled as a stochastic dynamics occurring on\ntop of a given network with edge weights corresponding to the transmission\nprobabilities. Knowledge of veracious transmission probabilities is essential\nfor prediction, optimization, and control of diffusion dynamics. Unfortunately,\nin most cases the transmission rates are unknown and need to be reconstructed\nfrom the spreading data. Moreover, in realistic settings it is impossible to\nmonitor the state of each node at every time, and thus the data is highly\nincomplete. We introduce an efficient dynamic message-passing algorithm, which\nis able to reconstruct parameters of the spreading model given only partial\ninformation on the activation times of nodes in the network. The method is\ngeneralizable to a large class of dynamic models, as well to the case of\ntemporal graphs. \n\n"}
{"id": "1609.00161", "contents": "Title: Parallel Clustering of Graphs for Anonymization and Recommender Systems Abstract: Graph clustering is widely used in many data analysis applications. In this\npaper we propose several parallel graph clustering algorithms based on Monte\nCarlo simulations and expectation maximization in the context of stochastic\nblock models. We apply those algorithms to the specific problems of recommender\nsystems and social network anonymization. We compare the experimental results\nto previous propositions. \n\n"}
{"id": "1609.00978", "contents": "Title: Local Maxima in the Likelihood of Gaussian Mixture Models: Structural\n  Results and Algorithmic Consequences Abstract: We provide two fundamental results on the population (infinite-sample)\nlikelihood function of Gaussian mixture models with $M \\geq 3$ components. Our\nfirst main result shows that the population likelihood function has bad local\nmaxima even in the special case of equally-weighted mixtures of well-separated\nand spherical Gaussians. We prove that the log-likelihood value of these bad\nlocal maxima can be arbitrarily worse than that of any global optimum, thereby\nresolving an open question of Srebro (2007). Our second main result shows that\nthe EM algorithm (or a first-order variant of it) with random initialization\nwill converge to bad critical points with probability at least\n$1-e^{-\\Omega(M)}$. We further establish that a first-order variant of EM will\nnot converge to strict saddle points almost surely, indicating that the poor\nperformance of the first-order method can be attributed to the existence of bad\nlocal maxima rather than bad saddle points. Overall, our results highlight the\nnecessity of careful initialization when using the EM algorithm in practice,\neven when applied in highly favorable settings. \n\n"}
{"id": "1609.02487", "contents": "Title: Non-Backtracking Spectrum of Degree-Corrected Stochastic Block Models Abstract: Motivated by community detection, we characterise the spectrum of the\nnon-backtracking matrix $B$ in the Degree-Corrected Stochastic Block Model.\n  Specifically, we consider a random graph on $n$ vertices partitioned into two\nequal-sized clusters. The vertices have i.i.d. weights $\\{ \\phi_u \\}_{u=1}^n$\nwith second moment $\\Phi^{(2)}$. The intra-cluster connection probability for\nvertices $u$ and $v$ is $\\frac{\\phi_u \\phi_v}{n}a$ and the inter-cluster\nconnection probability is $\\frac{\\phi_u \\phi_v}{n}b$.\n  We show that with high probability, the following holds: The leading\neigenvalue of the non-backtracking matrix $B$ is asymptotic to $\\rho =\n\\frac{a+b}{2} \\Phi^{(2)}$. The second eigenvalue is asymptotic to $\\mu_2 =\n\\frac{a-b}{2} \\Phi^{(2)}$ when $\\mu_2^2 > \\rho$, but asymptotically bounded by\n$\\sqrt{\\rho}$ when $\\mu_2^2 \\leq \\rho$. All the remaining eigenvalues are\nasymptotically bounded by $\\sqrt{\\rho}$. As a result, a clustering\npositively-correlated with the true communities can be obtained based on the\nsecond eigenvector of $B$ in the regime where $\\mu_2^2 > \\rho.$\n  In a previous work we obtained that detection is impossible when $\\mu_2^2 <\n\\rho,$ meaning that there occurs a phase-transition in the sparse regime of the\nDegree-Corrected Stochastic Block Model.\n  As a corollary, we obtain that Degree-Corrected Erd\\H{o}s-R\\'enyi graphs\nasymptotically satisfy the graph Riemann hypothesis, a quasi-Ramanujan\nproperty.\n  A by-product of our proof is a weak law of large numbers for\nlocal-functionals on Degree-Corrected Stochastic Block Models, which could be\nof independent interest. \n\n"}
{"id": "1609.05877", "contents": "Title: Geometrically Convergent Distributed Optimization with Uncoordinated\n  Step-Sizes Abstract: A recent algorithmic family for distributed optimization, DIGing's, have been\nshown to have geometric convergence over time-varying undirected/directed\ngraphs. Nevertheless, an identical step-size for all agents is needed. In this\npaper, we study the convergence rates of the Adapt-Then-Combine (ATC) variation\nof the DIGing algorithm under uncoordinated step-sizes. We show that the ATC\nvariation of DIGing algorithm converges geometrically fast even if the\nstep-sizes are different among the agents. In addition, our analysis implies\nthat the ATC structure can accelerate convergence compared to the distributed\ngradient descent (DGD) structure which has been used in the original DIGing\nalgorithm. \n\n"}
{"id": "1609.06457", "contents": "Title: AMOS: An Automated Model Order Selection Algorithm for Spectral Graph\n  Clustering Abstract: One of the longstanding problems in spectral graph clustering (SGC) is the\nso-called model order selection problem: automated selection of the correct\nnumber of clusters. This is equivalent to the problem of finding the number of\nconnected components or communities in an undirected graph. In this paper, we\npropose AMOS, an automated model order selection algorithm for SGC. Based on a\nrecent analysis of clustering reliability for SGC under the random\ninterconnection model, AMOS works by incrementally increasing the number of\nclusters, estimating the quality of identified clusters, and providing a series\nof clustering reliability tests. Consequently, AMOS outputs clusters of minimal\nmodel order with statistical clustering reliability guarantees. Comparing to\nthree other automated graph clustering methods on real-world datasets, AMOS\nshows superior performance in terms of multiple external and internal\nclustering metrics. \n\n"}
{"id": "1609.07200", "contents": "Title: Multilayer Spectral Graph Clustering via Convex Layer Aggregation Abstract: Multilayer graphs are commonly used for representing different relations\nbetween entities and handling heterogeneous data processing tasks. New\nchallenges arise in multilayer graph clustering for assigning clusters to a\ncommon multilayer node set and for combining information from each layer. This\npaper presents a theoretical framework for multilayer spectral graph clustering\nof the nodes via convex layer aggregation. Under a novel multilayer signal plus\nnoise model, we provide a phase transition analysis that establishes the\nexistence of a critical value on the noise level that permits reliable cluster\nseparation. The analysis also specifies analytical upper and lower bounds on\nthe critical value, where the bounds become exact when the clusters have\nidentical sizes. Numerical experiments on synthetic multilayer graphs are\nconducted to validate the phase transition analysis and study the effect of\nlayer weights and noise levels on clustering reliability. \n\n"}
{"id": "1609.07537", "contents": "Title: A Tutorial on Distributed (Non-Bayesian) Learning: Problem, Algorithms\n  and Results Abstract: We overview some results on distributed learning with focus on a family of\nrecently proposed algorithms known as non-Bayesian social learning. We consider\ndifferent approaches to the distributed learning problem and its algorithmic\nsolutions for the case of finitely many hypotheses. The original centralized\nproblem is discussed at first, and then followed by a generalization to the\ndistributed setting. The results on convergence and convergence rate are\npresented for both asymptotic and finite time regimes. Various extensions are\ndiscussed such as those dealing with directed time-varying networks, Nesterov's\nacceleration technique and a continuum sets of hypothesis. \n\n"}
{"id": "1610.00494", "contents": "Title: One-Trial Correction of Legacy AI Systems and Stochastic Separation\n  Theorems Abstract: We consider the problem of efficient \"on the fly\" tuning of existing, or {\\it\nlegacy}, Artificial Intelligence (AI) systems. The legacy AI systems are\nallowed to be of arbitrary class, albeit the data they are using for computing\ninterim or final decision responses should posses an underlying structure of a\nhigh-dimensional topological real vector space. The tuning method that we\npropose enables dealing with errors without the need to re-train the system.\nInstead of re-training a simple cascade of perceptron nodes is added to the\nlegacy system. The added cascade modulates the AI legacy system's decisions. If\napplied repeatedly, the process results in a network of modulating rules\n\"dressing up\" and improving performance of existing AI systems. Mathematical\nrationale behind the method is based on the fundamental property of measure\nconcentration in high dimensional spaces. The method is illustrated with an\nexample of fine-tuning a deep convolutional network that has been pre-trained\nto detect pedestrians in images. \n\n"}
{"id": "1610.00970", "contents": "Title: Stochastic Optimization with Variance Reduction for Infinite Datasets\n  with Finite-Sum Structure Abstract: Stochastic optimization algorithms with variance reduction have proven\nsuccessful for minimizing large finite sums of functions. Unfortunately, these\ntechniques are unable to deal with stochastic perturbations of input data,\ninduced for example by data augmentation. In such cases, the objective is no\nlonger a finite sum, and the main candidate for optimization is the stochastic\ngradient descent method (SGD). In this paper, we introduce a variance reduction\napproach for these settings when the objective is composite and strongly\nconvex. The convergence rate outperforms SGD with a typically much smaller\nconstant factor, which depends on the variance of gradient estimates only due\nto perturbations on a single example. \n\n"}
{"id": "1610.01101", "contents": "Title: A SMART Stochastic Algorithm for Nonconvex Optimization with\n  Applications to Robust Machine Learning Abstract: In this paper, we show how to transform any optimization problem that arises\nfrom fitting a machine learning model into one that (1) detects and removes\ncontaminated data from the training set while (2) simultaneously fitting the\ntrimmed model on the uncontaminated data that remains. To solve the resulting\nnonconvex optimization problem, we introduce a fast stochastic\nproximal-gradient algorithm that incorporates prior knowledge through nonsmooth\nregularization. For datasets of size $n$, our approach requires\n$O(n^{2/3}/\\varepsilon)$ gradient evaluations to reach $\\varepsilon$-accuracy\nand, when a certain error bound holds, the complexity improves to $O(\\kappa\nn^{2/3}\\log(1/\\varepsilon))$. These rates are $n^{1/3}$ times better than those\nachieved by typical, full gradient methods. \n\n"}
{"id": "1610.01961", "contents": "Title: Fast Hierarchy Construction for Dense Subgraphs Abstract: Discovering dense subgraphs and understanding the relations among them is a\nfundamental problem in graph mining. We want to not only identify dense\nsubgraphs, but also build a hierarchy among them (e.g., larger but sparser\nsubgraphs formed by two smaller dense subgraphs). Peeling algorithms (k-core,\nk-truss, and nucleus decomposition) have been effective to locate many dense\nsubgraphs. However, constructing a hierarchical representation of density\nstructure, even correctly computing the connected k-cores and k-trusses, have\nbeen mostly overlooked. Keeping track of connected components during peeling\nrequires an additional traversal operation, which is as expensive as the\npeeling process. In this paper, we start with a thorough survey and point to\nnuances in problem formulations that lead to significant differences in\nruntimes. We then propose efficient and generic algorithms to construct the\nhierarchy of dense subgraphs for k-core, k-truss, or any nucleus decomposition.\nOur algorithms leverage the disjoint-set forest data structure to efficiently\nconstruct the hierarchy during traversal. Furthermore, we introduce a new idea\nto avoid traversal. We construct the subgraphs while visiting neighborhoods in\nthe peeling process, and build the relations to previously constructed\nsubgraphs. We also consider an existing idea to find the k-core hierarchy and\nadapt for our objectives efficiently. Experiments on different types of large\nscale real-world networks show significant speedups over naive algorithms and\nexisting alternatives. Our algorithms also outperform the hypothetical limits\nof any possible traversal-based solution. \n\n"}
{"id": "1610.03414", "contents": "Title: Maximum entropy models capture melodic styles Abstract: We introduce a Maximum Entropy model able to capture the statistics of\nmelodies in music. The model can be used to generate new melodies that emulate\nthe style of the musical corpus which was used to train it. Instead of using\nthe $n-$body interactions of $(n-1)-$order Markov models, traditionally used in\nautomatic music generation, we use a $k-$nearest neighbour model with pairwise\ninteractions only. In that way, we keep the number of parameters low and avoid\nover-fitting problems typical of Markov models. We show that long-range musical\nphrases don't need to be explicitly enforced using high-order Markov\ninteractions, but can instead emerge from multiple, competing, pairwise\ninteractions. We validate our Maximum Entropy model by contrasting how much the\ngenerated sequences capture the style of the original corpus without\nplagiarizing it. To this end we use a data-compression approach to discriminate\nthe levels of borrowing and innovation featured by the artificial sequences.\nThe results show that our modelling scheme outperforms both fixed-order and\nvariable-order Markov models. This shows that, despite being based only on\npairwise interactions, this Maximum Entropy scheme opens the possibility to\ngenerate musically sensible alterations of the original phrases, providing a\nway to generate innovation. \n\n"}
{"id": "1610.03483", "contents": "Title: Learning in Implicit Generative Models Abstract: Generative adversarial networks (GANs) provide an algorithmic framework for\nconstructing generative models with several appealing properties: they do not\nrequire a likelihood function to be specified, only a generating procedure;\nthey provide samples that are sharp and compelling; and they allow us to\nharness our knowledge of building highly accurate neural network classifiers.\nHere, we develop our understanding of GANs with the aim of forming a rich view\nof this growing area of machine learning---to build connections to the diverse\nset of statistical thinking on this topic, of which much can be gained by a\nmutual exchange of ideas. We frame GANs within the wider landscape of\nalgorithms for learning in implicit generative models--models that only specify\na stochastic procedure with which to generate data--and relate these ideas to\nmodelling problems in related fields, such as econometrics and approximate\nBayesian computation. We develop likelihood-free inference methods and\nhighlight hypothesis testing as a principle for learning in implicit generative\nmodels, using which we are able to derive the objective function used by GANs,\nand many other related objectives. The testing viewpoint directs our focus to\nthe general problem of density ratio estimation. There are four approaches for\ndensity ratio estimation, one of which is a solution using classifiers to\ndistinguish real from generated data. Other approaches such as divergence\nminimisation and moment matching have also been explored in the GAN literature,\nand we synthesise these views to form an understanding in terms of the\nrelationships between them and the wider literature, highlighting avenues for\nfuture exploration and cross-pollination. \n\n"}
{"id": "1610.04019", "contents": "Title: Voice Conversion from Non-parallel Corpora Using Variational\n  Auto-encoder Abstract: We propose a flexible framework for spectral conversion (SC) that facilitates\ntraining with unaligned corpora. Many SC frameworks require parallel corpora,\nphonetic alignments, or explicit frame-wise correspondence for learning\nconversion functions or for synthesizing a target spectrum with the aid of\nalignments. However, these requirements gravely limit the scope of practical\napplications of SC due to scarcity or even unavailability of parallel corpora.\nWe propose an SC framework based on variational auto-encoder which enables us\nto exploit non-parallel corpora. The framework comprises an encoder that learns\nspeaker-independent phonetic representations and a decoder that learns to\nreconstruct the designated speaker. It removes the requirement of parallel\ncorpora or phonetic alignments to train a spectral conversion system. We report\nobjective and subjective evaluations to validate our proposed method and\ncompare it to SC methods that have access to aligned corpora. \n\n"}
{"id": "1610.04274", "contents": "Title: Hierarchical Clustering Given Confidence Intervals of Metric Distances Abstract: This paper considers metric spaces where distances between a pair of nodes\nare represented by distance intervals. The goal is to study methods for the\ndetermination of hierarchical clusters, i.e., a family of nested partitions\nindexed by a resolution parameter, induced from the given distance intervals of\nthe metric spaces. Our construction of hierarchical clustering methods is based\non defining admissible methods to be those methods that abide to the axioms of\nvalue - nodes in a metric space with two nodes are clustered together at the\nconvex combination of the distance bounds between them - and transformation -\nwhen both distance bounds are reduced, the output may become more clustered but\nnot less. Two admissible methods are constructed and are shown to provide\nuniversal upper and lower bounds in the space of admissible methods. Practical\nimplications are explored by clustering moving points via snapshots and by\nclustering networks representing brain structural connectivity using the lower\nand upper bounds of the network distance. The proposed clustering methods\nsucceed in identifying underlying clustering structures via the maximum and\nminimum distances in all snapshots, as well as in differentiating brain\nconnectivity networks of patients from those of healthy controls. \n\n"}
{"id": "1610.04491", "contents": "Title: The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear\n  Bandits Abstract: Stochastic linear bandits are a natural and simple generalisation of\nfinite-armed bandits with numerous practical applications. Current approaches\nfocus on generalising existing techniques for finite-armed bandits, notably the\noptimism principle and Thompson sampling. While prior work has mostly been in\nthe worst-case setting, we analyse the asymptotic instance-dependent regret and\nshow matching upper and lower bounds on what is achievable. Surprisingly, our\nresults show that no algorithm based on optimism or Thompson sampling will ever\nachieve the optimal rate, and indeed, can be arbitrarily far from optimal, even\nin very simple cases. This is a disturbing result because these techniques are\nstandard tools that are widely used for sequential optimisation. For example,\nfor generalised linear bandits and reinforcement learning. \n\n"}
{"id": "1610.04674", "contents": "Title: Finite-sum Composition Optimization via Variance Reduced Gradient\n  Descent Abstract: The stochastic composition optimization proposed recently by Wang et al.\n[2014] minimizes the objective with the compositional expectation form:\n$\\min_x~(\\mathbb{E}_iF_i \\circ \\mathbb{E}_j G_j)(x).$ It summarizes many\nimportant applications in machine learning, statistics, and finance. In this\npaper, we consider the finite-sum scenario for composition optimization:\n\\[\\min_x f (x) := \\frac{1}{n} \\sum_{i = 1}^n F_i \\left(\\frac{1}{m} \\sum_{j =\n1}^m G_j (x) \\right). \\] We propose two algorithms to solve this problem by\ncombining the stochastic compositional gradient descent (SCGD) and the\nstochastic variance reduced gradient (SVRG) technique. A constant linear\nconvergence rate is proved for strongly convex optimization, which\nsubstantially improves the sublinear rate $O(K^{-0.8})$ of the best known\nalgorithm. \n\n"}
{"id": "1610.05507", "contents": "Title: Analysis and Implementation of an Asynchronous Optimization Algorithm\n  for the Parameter Server Abstract: This paper presents an asynchronous incremental aggregated gradient algorithm\nand its implementation in a parameter server framework for solving regularized\noptimization problems. The algorithm can handle both general convex (possibly\nnon-smooth) regularizers and general convex constraints. When the empirical\ndata loss is strongly convex, we establish linear convergence rate, give\nexplicit expressions for step-size choices that guarantee convergence to the\noptimum, and bound the associated convergence factors. The expressions have an\nexplicit dependence on the degree of asynchrony and recover classical results\nunder synchronous operation. Simulations and implementations on commercial\ncompute clouds validate our findings. \n\n"}
{"id": "1610.07004", "contents": "Title: Inferring Population Preferences via Mixtures of Spatial Voting Models Abstract: Understanding political phenomena requires measuring the political\npreferences of society. We introduce a model based on mixtures of spatial\nvoting models that infers the underlying distribution of political preferences\nof voters with only voting records of the population and political positions of\ncandidates in an election. Beyond offering a cost-effective alternative to\nsurveys, this method projects the political preferences of voters and\ncandidates into a shared latent preference space. This projection allows us to\ndirectly compare the preferences of the two groups, which is desirable for\npolitical science but difficult with traditional survey methods. After\nvalidating the aggregated-level inferences of this model against results of\nrelated work and on simple prediction tasks, we apply the model to better\nunderstand the phenomenon of political polarization in the Texas, New York, and\nOhio electorates. Taken at face value, inferences drawn from our model indicate\nthat the electorates in these states may be less bimodal than the distribution\nof candidates, but that the electorates are comparatively more extreme in their\nvariance. We conclude with a discussion of limitations of our method and\npotential future directions for research. \n\n"}
{"id": "1610.07448", "contents": "Title: A Framework for Parallel and Distributed Training of Neural Networks Abstract: The aim of this paper is to develop a general framework for training neural\nnetworks (NNs) in a distributed environment, where training data is partitioned\nover a set of agents that communicate with each other through a sparse,\npossibly time-varying, connectivity pattern. In such distributed scenario, the\ntraining problem can be formulated as the (regularized) optimization of a\nnon-convex social cost function, given by the sum of local (non-convex) costs,\nwhere each agent contributes with a single error term defined with respect to\nits local dataset. To devise a flexible and efficient solution, we customize a\nrecently proposed framework for non-convex optimization over networks, which\nhinges on a (primal) convexification-decomposition technique to handle\nnon-convexity, and a dynamic consensus procedure to diffuse information among\nthe agents. Several typical choices for the training criterion (e.g., squared\nloss, cross entropy, etc.) and regularization (e.g., $\\ell_2$ norm, sparsity\ninducing penalties, etc.) are included in the framework and explored along the\npaper. Convergence to a stationary solution of the social non-convex problem is\nguaranteed under mild assumptions. Additionally, we show a principled way\nallowing each agent to exploit a possible multi-core architecture (e.g., a\nlocal cloud) in order to parallelize its local optimization step, resulting in\nstrategies that are both distributed (across the agents) and parallel (inside\neach agent) in nature. A comprehensive set of experimental results validate the\nproposed approach. \n\n"}
{"id": "1610.08469", "contents": "Title: Kissing Cuisines: Exploring Worldwide Culinary Habits on the Web Abstract: Food and nutrition occupy an increasingly prevalent space on the web, and\ndishes and recipes shared online provide an invaluable mirror into culinary\ncultures and attitudes around the world. More specifically, ingredients,\nflavors, and nutrition information become strong signals of the taste\npreferences of individuals and civilizations. However, there is little\nunderstanding of these palate varieties. In this paper, we present a\nlarge-scale study of recipes published on the web and their content, aiming to\nunderstand cuisines and culinary habits around the world. Using a database of\nmore than 157K recipes from over 200 different cuisines, we analyze\ningredients, flavors, and nutritional values which distinguish dishes from\ndifferent regions, and use this knowledge to assess the predictability of\nrecipes from different cuisines. We then use country health statistics to\nunderstand the relation between these factors and health indicators of\ndifferent nations, such as obesity, diabetes, migration, and health\nexpenditure. Our results confirm the strong effects of geographical and\ncultural similarities on recipes, health indicators, and culinary preferences\nacross the globe. \n\n"}
{"id": "1610.09033", "contents": "Title: Operator Variational Inference Abstract: Variational inference is an umbrella term for algorithms which cast Bayesian\ninference as optimization. Classically, variational inference uses the\nKullback-Leibler divergence to define the optimization. Though this divergence\nhas been widely used, the resultant posterior approximation can suffer from\nundesirable statistical properties. To address this, we reexamine variational\ninference from its roots as an optimization problem. We use operators, or\nfunctions of functions, to design variational objectives. As one example, we\ndesign a variational objective with a Langevin-Stein operator. We develop a\nblack box algorithm, operator variational inference (OPVI), for optimizing any\noperator objective. Importantly, operators enable us to make explicit the\nstatistical and computational tradeoffs for variational inference. We can\ncharacterize different properties of variational objectives, such as objectives\nthat admit data subsampling---allowing inference to scale to massive data---as\nwell as objectives that admit variational programs---a rich class of posterior\napproximations that does not require a tractable density. We illustrate the\nbenefits of OPVI on a mixture model and a generative model of images. \n\n"}
{"id": "1610.09038", "contents": "Title: Professor Forcing: A New Algorithm for Training Recurrent Networks Abstract: The Teacher Forcing algorithm trains recurrent networks by supplying observed\nsequence values as inputs during training and using the network's own\none-step-ahead predictions to do multi-step sampling. We introduce the\nProfessor Forcing algorithm, which uses adversarial domain adaptation to\nencourage the dynamics of the recurrent network to be the same when training\nthe network and when sampling from the network over multiple time steps. We\napply Professor Forcing to language modeling, vocal synthesis on raw waveforms,\nhandwriting generation, and image generation. Empirically we find that\nProfessor Forcing acts as a regularizer, improving test likelihood on character\nlevel Penn Treebank and sequential MNIST. We also find that the model\nqualitatively improves samples, especially when sampling for a large number of\ntime steps. This is supported by human evaluation of sample quality. Trade-offs\nbetween Professor Forcing and Scheduled Sampling are discussed. We produce\nT-SNEs showing that Professor Forcing successfully makes the dynamics of the\nnetwork during training and sampling more similar. \n\n"}
{"id": "1610.09160", "contents": "Title: How Users Explore Ontologies on the Web: A Study of NCBO's BioPortal\n  Usage Logs Abstract: Ontologies in the biomedical domain are numerous, highly specialized and very\nexpensive to develop. Thus, a crucial prerequisite for ontology adoption and\nreuse is effective support for exploring and finding existing ontologies.\nTowards that goal, the National Center for Biomedical Ontology (NCBO) has\ndeveloped BioPortal---an online repository designed to support users in\nexploring and finding more than 500 existing biomedical ontologies. In 2016,\nBioPortal represents one of the largest portals for exploration of semantic\nbiomedical vocabularies and terminologies, which is used by many researchers\nand practitioners. While usage of this portal is high, we know very little\nabout how exactly users search and explore ontologies and what kind of usage\npatterns or user groups exist in the first place. Deeper insights into user\nbehavior on such portals can provide valuable information to devise strategies\nfor a better support of users in exploring and finding existing ontologies, and\nthereby enable better ontology reuse. To that end, we study and group users\naccording to their browsing behavior on BioPortal using data mining techniques.\nAdditionally, we use the obtained groups to characterize and compare\nexploration strategies across ontologies. In particular, we were able to\nidentify seven distinct browsing-behavior types, which all make use of\ndifferent functionality provided by BioPortal. For example, Search Explorers\nmake extensive use of the search functionality while Ontology Tree Explorers\nmainly rely on the class hierarchy to explore ontologies. Further, we show that\nspecific characteristics of ontologies influence the way users explore and\ninteract with the website. Our results may guide the development of more\nuser-oriented systems for ontology exploration on the Web. \n\n"}
{"id": "1610.09411", "contents": "Title: ESCAPE: Efficiently Counting All 5-Vertex Subgraphs Abstract: Counting the frequency of small subgraphs is a fundamental technique in\nnetwork analysis across various domains, most notably in bioinformatics and\nsocial networks. The special case of triangle counting has received much\nattention. Getting results for 4-vertex or 5-vertex patterns is highly\nchallenging, and there are few practical results known that can scale to\nmassive sizes.\n  We introduce an algorithmic framework that can be adopted to count any small\npattern in a graph and apply this framework to compute exact counts for\n\\emph{all} 5-vertex subgraphs. Our framework is built on cutting a pattern into\nsmaller ones, and using counts of smaller patterns to get larger counts.\nFurthermore, we exploit degree orientations of the graph to reduce runtimes\neven further. These methods avoid the combinatorial explosion that typical\nsubgraph counting algorithms face. We prove that it suffices to enumerate only\nfour specific subgraphs (three of them have less than 5 vertices) to exactly\ncount all 5-vertex patterns.\n  We perform extensive empirical experiments on a variety of real-world graphs.\nWe are able to compute counts of graphs with tens of millions of edges in\nminutes on a commodity machine. To the best of our knowledge, this is the first\npractical algorithm for $5$-vertex pattern counting that runs at this scale. A\nstepping stone to our main algorithm is a fast method for counting all\n$4$-vertex patterns. This algorithm is typically ten times faster than the\nstate of the art $4$-vertex counters. \n\n"}
{"id": "1611.00328", "contents": "Title: Variational Inference via $\\chi$-Upper Bound Minimization Abstract: Variational inference (VI) is widely used as an efficient alternative to\nMarkov chain Monte Carlo. It posits a family of approximating distributions $q$\nand finds the closest member to the exact posterior $p$. Closeness is usually\nmeasured via a divergence $D(q || p)$ from $q$ to $p$. While successful, this\napproach also has problems. Notably, it typically leads to underestimation of\nthe posterior variance. In this paper we propose CHIVI, a black-box variational\ninference algorithm that minimizes $D_{\\chi}(p || q)$, the $\\chi$-divergence\nfrom $p$ to $q$. CHIVI minimizes an upper bound of the model evidence, which we\nterm the $\\chi$ upper bound (CUBO). Minimizing the CUBO leads to improved\nposterior uncertainty, and it can also be used with the classical VI lower\nbound (ELBO) to provide a sandwich estimate of the model evidence. We study\nCHIVI on three models: probit regression, Gaussian process classification, and\na Cox process model of basketball plays. When compared to expectation\npropagation and classical VI, CHIVI produces better error rates and more\naccurate estimates of posterior variance. \n\n"}
{"id": "1611.00336", "contents": "Title: Stochastic Variational Deep Kernel Learning Abstract: Deep kernel learning combines the non-parametric flexibility of kernel\nmethods with the inductive biases of deep learning architectures. We propose a\nnovel deep kernel learning model and stochastic variational inference procedure\nwhich generalizes deep kernel learning approaches to enable classification,\nmulti-task learning, additive covariance structures, and stochastic gradient\ntraining. Specifically, we apply additive base kernels to subsets of output\nfeatures from deep neural architectures, and jointly learn the parameters of\nthe base kernels and deep network through a Gaussian process marginal\nlikelihood objective. Within this framework, we derive an efficient form of\nstochastic variational inference which leverages local kernel interpolation,\ninducing points, and structure exploiting algebra. We show improved performance\nover stand alone deep networks, SVMs, and state of the art scalable Gaussian\nprocesses on several classification benchmarks, including an airline delay\ndataset containing 6 million training points, CIFAR, and ImageNet. \n\n"}
{"id": "1611.01232", "contents": "Title: Deep Information Propagation Abstract: We study the behavior of untrained neural networks whose weights and biases\nare randomly distributed using mean field theory. We show the existence of\ndepth scales that naturally limit the maximum depth of signal propagation\nthrough these random networks. Our main practical result is to show that random\nnetworks may be trained precisely when information can travel through them.\nThus, the depth scales that we identify provide bounds on how deep a network\nmay be trained for a specific choice of hyperparameters. As a corollary to\nthis, we argue that in networks at the edge of chaos, one of these depth scales\ndiverges. Thus arbitrarily deep networks may be trained only sufficiently close\nto criticality. We show that the presence of dropout destroys the\norder-to-chaos critical point and therefore strongly limits the maximum\ntrainable depth for random networks. Finally, we develop a mean field theory\nfor backpropagation and we show that the ordered and chaotic phases correspond\nto regions of vanishing and exploding gradient respectively. \n\n"}
{"id": "1611.01456", "contents": "Title: Learning heat diffusion graphs Abstract: Effective information analysis generally boils down to properly identifying\nthe structure or geometry of the data, which is often represented by a graph.\nIn some applications, this structure may be partly determined by design\nconstraints or pre-determined sensing arrangements, like in road transportation\nnetworks for example. In general though, the data structure is not readily\navailable and becomes pretty difficult to define. In particular, the global\nsmoothness assumptions, that most of the existing works adopt, are often too\ngeneral and unable to properly capture localized properties of data. In this\npaper, we go beyond this classical data model and rather propose to represent\ninformation as a sparse combination of localized functions that live on a data\nstructure represented by a graph. Based on this model, we focus on the problem\nof inferring the connectivity that best explains the data samples at different\nvertices of a graph that is a priori unknown. We concentrate on the case where\nthe observed data is actually the sum of heat diffusion processes, which is a\nquite common model for data on networks or other irregular structures. We cast\na new graph learning problem and solve it with an efficient nonconvex\noptimization algorithm. Experiments on both synthetic and real world data\nfinally illustrate the benefits of the proposed graph learning framework and\nconfirm that the data structure can be efficiently learned from data\nobservations only. We believe that our algorithm will help solving key\nquestions in diverse application domains such as social and biological network\nanalysis where it is crucial to unveil proper geometry for data understanding\nand inference. \n\n"}
{"id": "1611.04500", "contents": "Title: Deep Learning with Sets and Point Clouds Abstract: We introduce a simple permutation equivariant layer for deep learning with\nset structure.This type of layer, obtained by parameter-sharing, has a simple\nimplementation and linear-time complexity in the size of each set. We use deep\npermutation-invariant networks to perform point-could classification and\nMNIST-digit summation, where in both cases the output is invariant to\npermutations of the input. In a semi-supervised setting, where the goal is make\npredictions for each instance within a set, we demonstrate the usefulness of\nthis type of layer in set-outlier detection as well as semi-supervised learning\nwith clustering side-information. \n\n"}
{"id": "1611.05780", "contents": "Title: Gap Safe screening rules for sparsity enforcing penalties Abstract: In high dimensional regression settings, sparsity enforcing penalties have\nproved useful to regularize the data-fitting term. A recently introduced\ntechnique called screening rules propose to ignore some variables in the\noptimization leveraging the expected sparsity of the solutions and consequently\nleading to faster solvers. When the procedure is guaranteed not to discard\nvariables wrongly the rules are said to be safe. In this work, we propose a\nunifying framework for generalized linear models regularized with standard\nsparsity enforcing penalties such as $\\ell_1$ or $\\ell_1/\\ell_2$ norms. Our\ntechnique allows to discard safely more variables than previously considered\nsafe rules, particularly for low regularization parameters. Our proposed Gap\nSafe rules (so called because they rely on duality gap computation) can cope\nwith any iterative solver but are particularly well suited to (block)\ncoordinate descent methods. Applied to many standard learning tasks, Lasso,\nSparse-Group Lasso, multi-task Lasso, binary and multinomial logistic\nregression, etc., we report significant speed-ups compared to previously\nproposed safe rules on all tested data sets. \n\n"}
{"id": "1611.06310", "contents": "Title: Local minima in training of neural networks Abstract: There has been a lot of recent interest in trying to characterize the error\nsurface of deep models. This stems from a long standing question. Given that\ndeep networks are highly nonlinear systems optimized by local gradient methods,\nwhy do they not seem to be affected by bad local minima? It is widely believed\nthat training of deep models using gradient methods works so well because the\nerror surface either has no local minima, or if they exist they need to be\nclose in value to the global minimum. It is known that such results hold under\nvery strong assumptions which are not satisfied by real models. In this paper\nwe present examples showing that for such theorem to be true additional\nassumptions on the data, initialization schemes and/or the model classes have\nto be made. We look at the particular case of finite size datasets. We\ndemonstrate that in this scenario one can construct counter-examples (datasets\nor initialization schemes) when the network does become susceptible to bad\nlocal minima over the weight space. \n\n"}
{"id": "1611.06314", "contents": "Title: Determining the Veracity of Rumours on Twitter Abstract: While social networks can provide an ideal platform for up-to-date\ninformation from individuals across the world, it has also proved to be a place\nwhere rumours fester and accidental or deliberate misinformation often emerges.\nIn this article, we aim to support the task of making sense from social media\ndata, and specifically, seek to build an autonomous message-classifier that\nfilters relevant and trustworthy information from Twitter. For our work, we\ncollected about 100 million public tweets, including users' past tweets, from\nwhich we identified 72 rumours (41 true, 31 false). We considered over 80\ntrustworthiness measures including the authors' profile and past behaviour, the\nsocial network connections (graphs), and the content of tweets themselves. We\nran modern machine-learning classifiers over those measures to produce\ntrustworthiness scores at various time windows from the outbreak of the rumour.\nSuch time-windows were key as they allowed useful insight into the progression\nof the rumours. From our findings, we identified that our model was\nsignificantly more accurate than similar studies in the literature. We also\nidentified critical attributes of the data that give rise to the\ntrustworthiness scores assigned. Finally we developed a software demonstration\nthat provides a visual user interface to allow the user to examine the\nanalysis. \n\n"}
{"id": "1611.07710", "contents": "Title: Spatio-Temporal Modeling of Users' Check-ins in Location-Based Social\n  Networks Abstract: Social networks are getting closer to our real physical world. People share\nthe exact location and time of their check-ins and are influenced by their\nfriends. Modeling the spatio-temporal behavior of users in social networks is\nof great importance for predicting the future behavior of users, controlling\nthe users' movements, and finding the latent influence network. It is observed\nthat users have periodic patterns in their movements. Also, they are influenced\nby the locations that their close friends recently visited. Leveraging these\ntwo observations, we propose a probabilistic model based on a doubly stochastic\npoint process with a periodic decaying kernel for the time of check-ins and a\ntime-varying multinomial distribution for the location of check-ins of users in\nthe location-based social networks. We learn the model parameters using an\nefficient EM algorithm, which distributes over the users. Experiments on\nsynthetic and real data gathered from Foursquare show that the proposed\ninference algorithm learns the parameters efficiently and our model outperforms\nthe other alternatives in the prediction of time and location of check-ins. \n\n"}
{"id": "1611.08292", "contents": "Title: Identifying Significant Predictive Bias in Classifiers Abstract: We present a novel subset scan method to detect if a probabilistic binary\nclassifier has statistically significant bias -- over or under predicting the\nrisk -- for some subgroup, and identify the characteristics of this subgroup.\nThis form of model checking and goodness-of-fit test provides a way to\ninterpretably detect the presence of classifier bias or regions of poor\nclassifier fit. This allows consideration of not just subgroups of a priori\ninterest or small dimensions, but the space of all possible subgroups of\nfeatures. To address the difficulty of considering these exponentially many\npossible subgroups, we use subset scan and parametric bootstrap-based methods.\nExtending this method, we can penalize the complexity of the detected subgroup\nand also identify subgroups with high classification errors. We demonstrate\nthese methods and find interesting results on the COMPAS crime recidivism and\ncredit delinquency data. \n\n"}
{"id": "1611.08372", "contents": "Title: A Unified Convex Surrogate for the Schatten-$p$ Norm Abstract: The Schatten-$p$ norm ($0<p<1$) has been widely used to replace the nuclear\nnorm for better approximating the rank function. However, existing methods are\neither 1) not scalable for large scale problems due to relying on singular\nvalue decomposition (SVD) in every iteration, or 2) specific to some $p$\nvalues, e.g., $1/2$, and $2/3$. In this paper, we show that for any $p$, $p_1$,\nand $p_2 >0$ satisfying $1/p=1/p_1+1/p_2$, there is an equivalence between the\nSchatten-$p$ norm of one matrix and the Schatten-$p_1$ and the Schatten-$p_2$\nnorms of its two factor matrices. We further extend the equivalence to multiple\nfactor matrices and show that all the factor norms can be convex and smooth for\nany $p>0$. In contrast, the original Schatten-$p$ norm for $0<p<1$ is\nnon-convex and non-smooth. As an example we conduct experiments on matrix\ncompletion. To utilize the convexity of the factor matrix norms, we adopt the\naccelerated proximal alternating linearized minimization algorithm and\nestablish its sequence convergence. Experiments on both synthetic and real\ndatasets exhibit its superior performance over the state-of-the-art methods.\nIts speed is also highly competitive. \n\n"}
{"id": "1611.09448", "contents": "Title: The Upper Bound on Knots in Neural Networks Abstract: Neural networks with rectified linear unit activations are essentially\nmultivariate linear splines. As such, one of many ways to measure the\n\"complexity\" or \"expressivity\" of a neural network is to count the number of\nknots in the spline model. We study the number of knots in fully-connected\nfeedforward neural networks with rectified linear unit activation functions. We\nintentionally keep the neural networks very simple, so as to make theoretical\nanalyses more approachable. An induction on the number of layers $l$ reveals a\ntight upper bound on the number of knots in $\\mathbb{R} \\to \\mathbb{R}^p$ deep\nneural networks. With $n_i \\gg 1$ neurons in layer $i = 1, \\dots, l$, the upper\nbound is approximately $n_1 \\dots n_l$. We then show that the exact upper bound\nis tight, and we demonstrate the upper bound with an example. The purpose of\nthese analyses is to pave a path for understanding the behavior of general\n$\\mathbb{R}^q \\to \\mathbb{R}^p$ neural networks. \n\n"}
{"id": "1611.10159", "contents": "Title: Bridges in Complex Networks Abstract: A bridge in a graph is an edge whose removal disconnects the graph and\nincreases the number of connected components. We calculate the fraction of\nbridges in a wide range of real-world networks and their randomized\ncounterparts. We find that real networks typically have more bridges than their\ncompletely randomized counterparts, but very similar fraction of bridges as\ntheir degree-preserving randomizations. We define a new edge centrality\nmeasure, called bridgeness, to quantify the importance of a bridge in damaging\na network. We find that certain real networks have very large average and\nvariance of bridgeness compared to their degree-preserving randomizations and\nother real networks. Finally, we offer an analytical framework to calculate the\nbridge fraction , the average and variance of bridgeness for uncorrelated\nrandom networks with arbitrary degree distributions. \n\n"}
{"id": "1612.00374", "contents": "Title: Spatial Decompositions for Large Scale SVMs Abstract: Although support vector machines (SVMs) are theoretically well understood,\ntheir underlying optimization problem becomes very expensive, if, for example,\nhundreds of thousands of samples and a non-linear kernel are considered.\nSeveral approaches have been proposed in the past to address this serious\nlimitation. In this work we investigate a decomposition strategy that learns on\nsmall, spatially defined data chunks. Our contributions are two fold: On the\ntheoretical side we establish an oracle inequality for the overall learning\nmethod using the hinge loss, and show that the resulting rates match those\nknown for SVMs solving the complete optimization problem with Gaussian kernels.\nOn the practical side we compare our approach to learning SVMs on small,\nrandomly chosen chunks. Here it turns out that for comparable training times\nour approach is significantly faster during testing and also reduces the test\nerror in most cases significantly. Furthermore, we show that our approach\neasily scales up to 10 million training samples: including hyper-parameter\nselection using cross validation, the entire training only takes a few hours on\na single machine. Finally, we report an experiment on 32 million training\nsamples. All experiments used liquidSVM (Steinwart and Thomann, 2017). \n\n"}
{"id": "1612.00984", "contents": "Title: Estimating latent feature-feature interactions in large feature-rich\n  graphs Abstract: Real-world complex networks describe connections between objects; in reality,\nthose objects are often endowed with some kind of features. How does the\npresence or absence of such features interplay with the network link structure?\nAlthough the situation here described is truly ubiquitous, there is a limited\nbody of research dealing with large graphs of this kind. Many previous works\nconsidered homophily as the only possible transmission mechanism translating\nnode features into links. Other authors, instead, developed more sophisticated\nmodels, that are able to handle complex feature interactions, but are unfit to\nscale to very large networks. We expand on the MGJ model, where interactions\nbetween pairs of features can foster or discourage link formation. In this\nwork, we will investigate how to estimate the latent feature-feature\ninteractions in this model. We shall propose two solutions: the first one\nassumes feature independence and it is essentially based on Naive Bayes; the\nsecond one, which relaxes the independence assumption assumption, is based on\nperceptrons. In fact, we show it is possible to cast the model equation in\norder to see it as the prediction rule of a perceptron. We analyze how\nclassical results for the perceptrons can be interpreted in this context; then,\nwe define a fast and simple perceptron-like algorithm for this task, which can\nprocess $10^8$ links in minutes. We then compare these two techniques, first\nwith synthetic datasets that follows our model, gaining evidence that the Naive\nindependence assumptions are detrimental in practice. Secondly, we consider a\nreal, large-scale citation network where each node (i.e., paper) can be\ndescribed by different types of characteristics; there, our algorithm can\nassess how well each set of features can explain the links, and thus finding\nmeaningful latent feature-feature interactions. \n\n"}
{"id": "1612.01158", "contents": "Title: Properties and Bayesian fitting of restricted Boltzmann machines Abstract: A restricted Boltzmann machine (RBM) is an undirected graphical model\nconstructed for discrete or continuous random variables, with two layers, one\nhidden and one visible, and no conditional dependency within a layer. In recent\nyears, RBMs have risen to prominence due to their connection to deep learning.\nBy treating a hidden layer of one RBM as the visible layer in a second RBM, a\ndeep architecture can be created. RBMs are thought to thereby have the ability\nto encode very complex and rich structures in data, making them attractive for\nsupervised learning. However, the generative behavior of RBMs is largely\nunexplored and typical fitting methodology does not easily allow for\nuncertainty quantification in addition to point estimates. In this paper, we\ndiscuss the relationship between RBM parameter specification in the binary case\nand model properties such as degeneracy, instability and uninterpretability. We\nalso describe the associated difficulties that can arise with likelihood-based\ninference and further discuss the potential Bayes fitting of such (highly\nflexible) models, especially as Gibbs sampling (quasi-Bayes) methods are often\nadvocated for the RBM model structure. \n\n"}
{"id": "1612.01216", "contents": "Title: Decentralized Frank-Wolfe Algorithm for Convex and Non-convex Problems Abstract: Decentralized optimization algorithms have received much attention due to the\nrecent advances in network information processing. However, conventional\ndecentralized algorithms based on projected gradient descent are incapable of\nhandling high dimensional constrained problems, as the projection step becomes\ncomputationally prohibitive to compute. To address this problem, this paper\nadopts a projection-free optimization approach, a.k.a.~the Frank-Wolfe (FW) or\nconditional gradient algorithm. We first develop a decentralized FW (DeFW)\nalgorithm from the classical FW algorithm. The convergence of the proposed\nalgorithm is studied by viewing the decentralized algorithm as an inexact FW\nalgorithm. Using a diminishing step size rule and letting $t$ be the iteration\nnumber, we show that the DeFW algorithm's convergence rate is ${\\cal O}(1/t)$\nfor convex objectives; is ${\\cal O}(1/t^2)$ for strongly convex objectives with\nthe optimal solution in the interior of the constraint set; and is ${\\cal\nO}(1/\\sqrt{t})$ towards a stationary point for smooth but non-convex\nobjectives. We then show that a consensus-based DeFW algorithm meets the above\nguarantees with two communication rounds per iteration. Furthermore, we\ndemonstrate the advantages of the proposed DeFW algorithm on low-complexity\nrobust matrix completion and communication efficient sparse learning. Numerical\nresults on synthetic and real data are presented to support our findings. \n\n"}
{"id": "1612.01704", "contents": "Title: A Unified Method of Detecting Core-Periphery Structure and Community\n  Structure in Networks Abstract: Core-periphery structure and community structure are two typical meso-scale\nstructures in complex networks. Though the community detection has been\nextensively investigated from different perspectives, the definition and the\ndetection of core-periphery structure have not received much attention.\nFurthermore, the detection problems of the core-periphery and community\nstructure were separately investigated. In this paper, we develop a unified\nframework to simultaneously detect core-periphery structure and community\nstructure in complex networks. Moreover, there are several extra advantages of\nour algorithm: our method can detect not only single but also multiple pairs of\ncore-periphery structures; the overlapping nodes belonging to different\ncommunities can be identified; different scales of core-periphery structures\ncan be detected by adjusting the size of core. The good performance of the\nmethod has been validated on synthetic and real complex networks. So we provide\na basic framework to detect the two typical meso-scale structures:\ncore-periphery structure and community structure. \n\n"}
{"id": "1612.01936", "contents": "Title: A Probabilistic Framework for Deep Learning Abstract: We develop a probabilistic framework for deep learning based on the Deep\nRendering Mixture Model (DRMM), a new generative probabilistic model that\nexplicitly capture variations in data due to latent task nuisance variables. We\ndemonstrate that max-sum inference in the DRMM yields an algorithm that exactly\nreproduces the operations in deep convolutional neural networks (DCNs),\nproviding a first principles derivation. Our framework provides new insights\ninto the successes and shortcomings of DCNs as well as a principled route to\ntheir improvement. DRMM training via the Expectation-Maximization (EM)\nalgorithm is a powerful alternative to DCN back-propagation, and initial\ntraining results are promising. Classification based on the DRMM and other\nvariants outperforms DCNs in supervised digit classification, training 2-3x\nfaster while achieving similar accuracy. Moreover, the DRMM is applicable to\nsemi-supervised and unsupervised learning tasks, achieving results that are\nstate-of-the-art in several categories on the MNIST benchmark and comparable to\nstate of the art on the CIFAR10 benchmark. \n\n"}
{"id": "1612.02295", "contents": "Title: Large-Margin Softmax Loss for Convolutional Neural Networks Abstract: Cross-entropy loss together with softmax is arguably one of the most common\nused supervision components in convolutional neural networks (CNNs). Despite\nits simplicity, popularity and excellent performance, the component does not\nexplicitly encourage discriminative learning of features. In this paper, we\npropose a generalized large-margin softmax (L-Softmax) loss which explicitly\nencourages intra-class compactness and inter-class separability between learned\nfeatures. Moreover, L-Softmax not only can adjust the desired margin but also\ncan avoid overfitting. We also show that the L-Softmax loss can be optimized by\ntypical stochastic gradient descent. Extensive experiments on four benchmark\ndatasets demonstrate that the deeply-learned features with L-softmax loss\nbecome more discriminative, hence significantly boosting the performance on a\nvariety of visual classification and verification tasks. \n\n"}
{"id": "1612.02516", "contents": "Title: Stochastic Primal-Dual Methods and Sample Complexity of Reinforcement\n  Learning Abstract: We study the online estimation of the optimal policy of a Markov decision\nprocess (MDP). We propose a class of Stochastic Primal-Dual (SPD) methods which\nexploit the inherent minimax duality of Bellman equations. The SPD methods\nupdate a few coordinates of the value and policy estimates as a new state\ntransition is observed. These methods use small storage and has low\ncomputational complexity per iteration. The SPD methods find an\nabsolute-$\\epsilon$-optimal policy, with high probability, using\n$\\mathcal{O}\\left(\\frac{|\\mathcal{S}|^4 |\\mathcal{A}|^2\\sigma^2\n}{(1-\\gamma)^6\\epsilon^2} \\right)$ iterations/samples for the infinite-horizon\ndiscounted-reward MDP and $\\mathcal{O}\\left(\\frac{|\\mathcal{S}|^4\n|\\mathcal{A}|^2H^6\\sigma^2 }{\\epsilon^2} \\right)$ for the finite-horizon MDP. \n\n"}
{"id": "1612.04340", "contents": "Title: End-to-End Deep Reinforcement Learning for Lane Keeping Assist Abstract: Reinforcement learning is considered to be a strong AI paradigm which can be\nused to teach machines through interaction with the environment and learning\nfrom their mistakes, but it has not yet been successfully used for automotive\napplications. There has recently been a revival of interest in the topic,\nhowever, driven by the ability of deep learning algorithms to learn good\nrepresentations of the environment. Motivated by Google DeepMind's successful\ndemonstrations of learning for games from Breakout to Go, we will propose\ndifferent methods for autonomous driving using deep reinforcement learning.\nThis is of particular interest as it is difficult to pose autonomous driving as\na supervised learning problem as it has a strong interaction with the\nenvironment including other vehicles, pedestrians and roadworks. As this is a\nrelatively new area of research for autonomous driving, we will formulate two\nmain categories of algorithms: 1) Discrete actions category, and 2) Continuous\nactions category. For the discrete actions category, we will deal with Deep\nQ-Network Algorithm (DQN) while for the continuous actions category, we will\ndeal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition to\nthat, We will also discover the performance of these two categories on an open\nsource car simulator for Racing called (TORCS) which stands for The Open Racing\ncar Simulator. Our simulation results demonstrate learning of autonomous\nmaneuvering in a scenario of complex road curvatures and simple interaction\nwith other vehicles. Finally, we explain the effect of some restricted\nconditions, put on the car during the learning phase, on the convergence time\nfor finishing its learning phase. \n\n"}
{"id": "1612.04679", "contents": "Title: IEDC: An Integrated Approach for Overlapping and Non-overlapping\n  Community Detection Abstract: Community detection is a task of fundamental importance in social network\nanalysis that can be used in a variety of knowledge-based domains. While there\nexist many works on community detection based on connectivity structures, they\nsuffer from either considering the overlapping or non-overlapping communities.\nIn this work, we propose a novel approach for general community detection\nthrough an integrated framework to extract the overlapping and non-overlapping\ncommunity structures without assuming prior structural connectivity on\nnetworks. Our general framework is based on a primary node based criterion\nwhich consists of the internal association degree along with the external\nassociation degree. The evaluation of the proposed method is investigated\nthrough the extensive simulation experiments and several benchmark real network\ndatasets. The experimental results show that the proposed method outperforms\nthe earlier state-of-the-art algorithms based on the well-known evaluation\ncriteria. \n\n"}
{"id": "1701.00406", "contents": "Title: Raising Graphs From Randomness to Reveal Information Networks Abstract: We analyze the fine-grained connections between the average degree and the\npower-law degree distribution exponent in growing information networks. Our\nstarting observation is a power-law degree distribution with a decreasing\nexponent and increasing average degree as a function of the network size. Our\nexperiments are based on three Twitter at-mention networks and three more from\nthe Koblenz Network Collection. We observe that popular network models cannot\nexplain decreasing power-law degree distribution exponent and increasing\naverage degree at the same time.\n  We propose a model that is the combination of exponential growth, and a\npower-law developing network, in which new \"homophily\" edges are continuously\nadded to nodes proportional to their current homophily degree. Parameters of\nthe average degree growth and the power-law degree distribution exponent\nfunctions depend on the ratio of the network growth exponent parameters.\nSpecifically, we connect the growth of the average degree to the decreasing\nexponent of the power-law degree distribution. Prior to our work, only one of\nthe two cases were handled. Existing models and even their combinations can\nonly reproduce some of our key new observations in growing information\nnetworks. \n\n"}
{"id": "1701.02046", "contents": "Title: Tunable GMM Kernels Abstract: The recently proposed \"generalized min-max\" (GMM) kernel can be efficiently\nlinearized, with direct applications in large-scale statistical learning and\nfast near neighbor search. The linearized GMM kernel was extensively compared\nin with linearized radial basis function (RBF) kernel. On a large number of\nclassification tasks, the tuning-free GMM kernel performs (surprisingly) well\ncompared to the best-tuned RBF kernel. Nevertheless, one would naturally expect\nthat the GMM kernel ought to be further improved if we introduce tuning\nparameters.\n  In this paper, we study three simple constructions of tunable GMM kernels:\n(i) the exponentiated-GMM (or eGMM) kernel, (ii) the powered-GMM (or pGMM)\nkernel, and (iii) the exponentiated-powered-GMM (epGMM) kernel. The pGMM kernel\ncan still be efficiently linearized by modifying the original hashing procedure\nfor the GMM kernel. On about 60 publicly available classification datasets, we\nverify that the proposed tunable GMM kernels typically improve over the\noriginal GMM kernel. On some datasets, the improvements can be astonishingly\nsignificant.\n  For example, on 11 popular datasets which were used for testing deep learning\nalgorithms and tree methods, our experiments show that the proposed tunable GMM\nkernels are strong competitors to trees and deep nets. The previous studies\ndeveloped tree methods including \"abc-robust-logitboost\" and demonstrated the\nexcellent performance on those 11 datasets (and other datasets), by\nestablishing the second-order tree-split formula and new derivatives for\nmulti-class logistic loss. Compared to tree methods like\n\"abc-robust-logitboost\" (which are slow and need substantial model sizes), the\ntunable GMM kernels produce largely comparable results. \n\n"}
{"id": "1701.02405", "contents": "Title: Discovery, Retrieval, and Analysis of 'Star Wars' botnet in Twitter Abstract: It is known that many Twitter users are bots, which are accounts controlled\nand sometimes created by computers. Twitter bots can send spam tweets,\nmanipulate public opinion and be used for online fraud. Here we report the\ndiscovery, retrieval, and analysis of the `Star Wars' botnet in Twitter, which\nconsists of more than 350,000 bots tweeting random quotations exclusively from\nStar Wars novels.\n  The botnet contains a single type of bot, showing exactly the same properties\nthroughout the botnet. It is unusually large, many times larger than other\navailable datasets. It provides a valuable source of ground truth for research\non Twitter bots. We analysed and revealed rich details on how the botnet was\ndesigned and created. As of this writing, the Star Wars bots are still alive in\nTwitter. They have survived since their creation in 2013, despite the\nincreasing efforts in recent years to detect and remove Twitter bots.We also\nreflect on the `unconventional' way in which we discovered the Star Wars bots,\nand discuss the current problems and future challenges of Twitter bot\ndetection. \n\n"}
{"id": "1701.03017", "contents": "Title: The paradigm-shift of social spambots: Evidence, theories, and tools for\n  the arms race Abstract: Recent studies in social media spam and automation provide anecdotal\nargumentation of the rise of a new generation of spambots, so-called social\nspambots. Here, for the first time, we extensively study this novel phenomenon\non Twitter and we provide quantitative evidence that a paradigm-shift exists in\nspambot design. First, we measure current Twitter's capabilities of detecting\nthe new social spambots. Later, we assess the human performance in\ndiscriminating between genuine accounts, social spambots, and traditional\nspambots. Then, we benchmark several state-of-the-art techniques proposed by\nthe academic literature. Results show that neither Twitter, nor humans, nor\ncutting-edge applications are currently capable of accurately detecting the new\nsocial spambots. Our results call for new approaches capable of turning the\ntide in the fight against this raising phenomenon. We conclude by reviewing the\nlatest literature on spambots detection and we highlight an emerging common\nresearch trend based on the analysis of collective behaviors. Insights derived\nfrom both our extensive experimental campaign and survey shed light on the most\npromising directions of research and lay the foundations for the arms race\nagainst the novel social spambots. Finally, to foster research on this novel\nphenomenon, we make publicly available to the scientific community all the\ndatasets used in this study. \n\n"}
{"id": "1701.04042", "contents": "Title: Observability transition in multiplex networks Abstract: We extend the observability model to multiplex networks. We present\nmathematical frameworks, valid under the treelike ansatz, able to describe the\nemergence of the macroscopic cluster of mutually observable nodes in both\nsynthetic and real-world multiplex networks. We show that the observability\ntransition in synthetic multiplex networks is discontinuous. In real-world\nmultiplex networks instead, edge overlap among layers is responsible for the\ndisappearance of any sign of abruptness in the emergence of the the macroscopic\ncluster of mutually observable nodes. \n\n"}
{"id": "1701.04895", "contents": "Title: Unknowable Manipulators: Social Network Curator Algorithms Abstract: For a social networking service to acquire and retain users, it must find\nways to keep them engaged. By accurately gauging their preferences, it is able\nto serve them with the subset of available content that maximises revenue for\nthe site. Without the constraints of an appropriate regulatory framework, we\nargue that a sufficiently sophisticated curator algorithm tasked with\nperforming this process may choose to explore curation strategies that are\ndetrimental to users. In particular, we suggest that such an algorithm is\ncapable of learning to manipulate its users, for several qualitative reasons:\n1. Access to vast quantities of user data combined with ongoing breakthroughs\nin the field of machine learning are leading to powerful but uninterpretable\nstrategies for decision making at scale. 2. The availability of an effective\nfeedback mechanism for assessing the short and long term user responses to\ncuration strategies. 3. Techniques from reinforcement learning have allowed\nmachines to learn automated and highly successful strategies at an abstract\nlevel, often resulting in non-intuitive yet nonetheless highly appropriate\naction selection. In this work, we consider the form that these strategies for\nuser manipulation might take and scrutinise the role that regulation should\nplay in the design of such systems. \n\n"}
{"id": "1701.04968", "contents": "Title: Multilayer Perceptron Algebra Abstract: Artificial Neural Networks(ANN) has been phenomenally successful on various\npattern recognition tasks. However, the design of neural networks rely heavily\non the experience and intuitions of individual developers. In this article, the\nauthor introduces a mathematical structure called MLP algebra on the set of all\nMultilayer Perceptron Neural Networks(MLP), which can serve as a guiding\nprinciple to build MLPs accommodating to the particular data sets, and to build\ncomplex MLPs from simpler ones. \n\n"}
{"id": "1701.05363", "contents": "Title: Stochastic Subsampling for Factorizing Huge Matrices Abstract: We present a matrix-factorization algorithm that scales to input matrices\nwith both huge number of rows and columns. Learned factors may be sparse or\ndense and/or non-negative, which makes our algorithm suitable for dictionary\nlearning, sparse component analysis, and non-negative matrix factorization. Our\nalgorithm streams matrix columns while subsampling them to iteratively learn\nthe matrix factors. At each iteration, the row dimension of a new sample is\nreduced by subsampling, resulting in lower time complexity compared to a simple\nstreaming algorithm. Our method comes with convergence guarantees to reach a\nstationary point of the matrix-factorization problem. We demonstrate its\nefficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on\npatches extracted from hyperspectral images (103 GB). For both problems, which\ninvolve different penalties on rows and columns, we obtain significant\nspeed-ups compared to state-of-the-art algorithms. \n\n"}
{"id": "1701.05804", "contents": "Title: Disentangling group and link persistence in Dynamic Stochastic Block\n  models Abstract: We study the inference of a model of dynamic networks in which both\ncommunities and links keep memory of previous network states. By considering\nmaximum likelihood inference from single snapshot observations of the network,\nwe show that link persistence makes the inference of communities harder,\ndecreasing the detectability threshold, while community persistence tends to\nmake it easier. We analytically show that communities inferred from single\nnetwork snapshot can share a maximum overlap with the underlying communities of\na specific previous instant in time. This leads to time-lagged inference: the\nidentification of past communities rather than present ones. Finally we compute\nthe time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic\n(LSD) algorithm, for community detection in dynamic networks. We analytically\nand numerically characterize the detectability transitions of such algorithm as\na function of the memory parameters of the model and we make a comparison with\na full dynamic inference. \n\n"}
{"id": "1701.06511", "contents": "Title: Aggressive Sampling for Multi-class to Binary Reduction with\n  Applications to Text Classification Abstract: We address the problem of multi-class classification in the case where the\nnumber of classes is very large. We propose a double sampling strategy on top\nof a multi-class to binary reduction strategy, which transforms the original\nmulti-class problem into a binary classification problem over pairs of\nexamples. The aim of the sampling strategy is to overcome the curse of\nlong-tailed class distributions exhibited in majority of large-scale\nmulti-class classification problems and to reduce the number of pairs of\nexamples in the expanded data. We show that this strategy does not alter the\nconsistency of the empirical risk minimization principle defined over the\ndouble sample reduction. Experiments are carried out on DMOZ and Wikipedia\ncollections with 10,000 to 100,000 classes where we show the efficiency of the\nproposed approach in terms of training and prediction time, memory consumption,\nand predictive performance with respect to state-of-the-art approaches. \n\n"}
{"id": "1701.06731", "contents": "Title: Weak Adaptive Submodularity and Group-Based Active Diagnosis with\n  Applications to State Estimation with Persistent Sensor Faults Abstract: In this paper, we consider adaptive decision-making problems for stochastic\nstate estimation with partial observations. First, we introduce the concept of\nweak adaptive submodularity, a generalization of adaptive submodularity, which\nhas found great success in solving challenging adaptive state estimation\nproblems. Then, for the problem of active diagnosis, i.e., discrete state\nestimation via active sensing, we show that an adaptive greedy policy has a\nnear-optimal performance guarantee when the reward function possesses this\nproperty. We further show that the reward function for group-based active\ndiagnosis, which arises in applications such as medical diagnosis and state\nestimation with persistent sensor faults, is also weakly adaptive submodular.\nFinally, in experiments of state estimation for an aircraft electrical system\nwith persistent sensor faults, we observe that an adaptive greedy policy\nperforms equally well as an exhaustive search. \n\n"}
{"id": "1702.00317", "contents": "Title: On SGD's Failure in Practice: Characterizing and Overcoming Stalling Abstract: Stochastic Gradient Descent (SGD) is widely used in machine learning problems\nto efficiently perform empirical risk minimization, yet, in practice, SGD is\nknown to stall before reaching the actual minimizer of the empirical risk. SGD\nstalling has often been attributed to its sensitivity to the conditioning of\nthe problem; however, as we demonstrate, SGD will stall even when applied to a\nsimple linear regression problem with unity condition number for standard\nlearning rates. Thus, in this work, we numerically demonstrate and\nmathematically argue that stalling is a crippling and generic limitation of SGD\nand its variants in practice. Once we have established the problem of stalling,\nwe generalize an existing framework for hedging against its effects, which (1)\ndeters SGD and its variants from stalling, (2) still provides convergence\nguarantees, and (3) makes SGD and its variants more practical methods for\nminimization. \n\n"}
{"id": "1702.01398", "contents": "Title: Evolution of Ego-networks in Social Media with Link Recommendations Abstract: Ego-networks are fundamental structures in social graphs, yet the process of\ntheir evolution is still widely unexplored. In an online context, a key\nquestion is how link recommender systems may skew the growth of these networks,\npossibly restraining diversity. To shed light on this matter, we analyze the\ncomplete temporal evolution of 170M ego-networks extracted from Flickr and\nTumblr, comparing links that are created spontaneously with those that have\nbeen algorithmically recommended. We find that the evolution of ego-networks is\nbursty, community-driven, and characterized by subsequent phases of explosive\ndiameter increase, slight shrinking, and stabilization. Recommendations favor\npopular and well-connected nodes, limiting the diameter expansion. With a\nmatching experiment aimed at detecting causal relationships from observational\ndata, we find that the bias introduced by the recommendations fosters global\ndiversity in the process of neighbor selection. Last, with two link prediction\nexperiments, we show how insights from our analysis can be used to improve the\neffectiveness of social recommender systems. \n\n"}
{"id": "1702.01434", "contents": "Title: Generating online social networks based on socio-demographic attributes Abstract: Recent years have seen tremendous growth of many online social networks such\nas Facebook, LinkedIn and MySpace. People connect to each other through these\nnetworks forming large social communities providing researchers rich datasets\nto understand, model and predict social interactions and behaviors. New\ncontacts in these networks can be formed due to an individual's demographic\nattributes such as age group, gender, geographic location, or due to a\nnetwork's structural dynamics such as triadic closure and preferential\nattachment, or a combination of both demographic and structural\ncharacteristics.\n  A number of network generation models have been proposed in the last decade\nto explain the structure, evolution and processes taking place in different\ntypes of networks, and notably social networks. Network generation models\nstudied in the literature primarily consider structural properties, and in some\ncases an individual's demographic profile in the formation of new social\ncontacts. These models do not present a mechanism to combine both structural\nand demographic characteristics for the formation of new links. In this paper,\nwe propose a new network generation algorithm which incorporates both these\ncharacteristics to model network formation. We use different publicly available\nFacebook datasets as benchmarks to demonstrate the correctness of the proposed\nnetwork generation model. The proposed model is flexible and thus can generate\nnetworks with varying demographic and structural properties. \n\n"}
{"id": "1702.02715", "contents": "Title: A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse\n  Gaussian Graphical Models Abstract: Estimating multiple sparse Gaussian Graphical Models (sGGMs) jointly for many\nrelated tasks (large $K$) under a high-dimensional (large $p$) situation is an\nimportant task. Most previous studies for the joint estimation of multiple\nsGGMs rely on penalized log-likelihood estimators that involve expensive and\ndifficult non-smooth optimizations. We propose a novel approach, FASJEM for\n\\underline{fa}st and \\underline{s}calable \\underline{j}oint\nstructure-\\underline{e}stimation of \\underline{m}ultiple sGGMs at a large\nscale. As the first study of joint sGGM using the Elementary Estimator\nframework, our work has three major contributions: (1) We solve FASJEM through\nan entry-wise manner which is parallelizable. (2) We choose a proximal\nalgorithm to optimize FASJEM. This improves the computational efficiency from\n$O(Kp^3)$ to $O(Kp^2)$ and reduces the memory requirement from $O(Kp^2)$ to\n$O(K)$. (3) We theoretically prove that FASJEM achieves a consistent estimation\nwith a convergence rate of $O(\\log(Kp)/n_{tot})$. On several synthetic and four\nreal-world datasets, FASJEM shows significant improvements over baselines on\naccuracy, computational complexity, and memory costs. \n\n"}
{"id": "1702.04832", "contents": "Title: Dynamic Partition Models Abstract: We present a new approach for learning compact and intuitive distributed\nrepresentations with binary encoding. Rather than summing up expert votes as in\nproducts of experts, we employ for each variable the opinion of the most\nreliable expert. Data points are hence explained through a partitioning of the\nvariables into expert supports. The partitions are dynamically adapted based on\nwhich experts are active. During the learning phase we adopt a smoothed version\nof this model that uses separate mixtures for each data dimension. In our\nexperiments we achieve accurate reconstructions of high-dimensional data points\nwith at most a dozen experts. \n\n"}
{"id": "1702.06166", "contents": "Title: Bayesian Boolean Matrix Factorisation Abstract: Boolean matrix factorisation aims to decompose a binary data matrix into an\napproximate Boolean product of two low rank, binary matrices: one containing\nmeaningful patterns, the other quantifying how the observations can be\nexpressed as a combination of these patterns. We introduce the OrMachine, a\nprobabilistic generative model for Boolean matrix factorisation and derive a\nMetropolised Gibbs sampler that facilitates efficient parallel posterior\ninference. On real world and simulated data, our method outperforms all\ncurrently existing approaches for Boolean matrix factorisation and completion.\nThis is the first method to provide full posterior inference for Boolean Matrix\nfactorisation which is relevant in applications, e.g. for controlling false\npositive rates in collaborative filtering and, crucially, improves the\ninterpretability of the inferred patterns. The proposed algorithm scales to\nlarge datasets as we demonstrate by analysing single cell gene expression data\nin 1.3 million mouse brain cells across 11 thousand genes on commodity\nhardware. \n\n"}
{"id": "1702.06527", "contents": "Title: Competition and Selection Among Conventions Abstract: In many domains, a latent competition among different conventions determines\nwhich one will come to dominate. One sees such effects in the success of\ncommunity jargon, of competing frames in political rhetoric, or of terminology\nin technical contexts. These effects have become widespread in the online\ndomain, where the data offers the potential to study competition among\nconventions at a fine-grained level.\n  In analyzing the dynamics of conventions over time, however, even with\ndetailed on-line data, one encounters two significant challenges. First, as\nconventions evolve, the underlying substance of their meaning tends to change\nas well; and such substantive changes confound investigations of social\neffects. Second, the selection of a convention takes place through the complex\ninteractions of individuals within a community, and contention between the\nusers of competing conventions plays a key role in the convention's evolution.\nAny analysis must take place in the presence of these two issues.\n  In this work we study a setting in which we can cleanly track the competition\namong conventions. Our analysis is based on the spread of low-level authoring\nconventions in the eprint arXiv over 24 years: by tracking the spread of macros\nand other author-defined conventions, we are able to study conventions that\nvary even as the underlying meaning remains constant. We find that the\ninteraction among co-authors over time plays a crucial role in the selection of\nthem; the distinction between more and less experienced members of the\ncommunity, and the distinction between conventions with visible versus\ninvisible effects, are both central to the underlying processes. Through our\nanalysis we make predictions at the population level about the ultimate success\nof different synonymous conventions over time--and at the individual level\nabout the outcome of \"fights\" between people over convention choices. \n\n"}
{"id": "1702.06777", "contents": "Title: Dialectometric analysis of language variation in Twitter Abstract: In the last few years, microblogging platforms such as Twitter have given\nrise to a deluge of textual data that can be used for the analysis of informal\ncommunication between millions of individuals. In this work, we propose an\ninformation-theoretic approach to geographic language variation using a corpus\nbased on Twitter. We test our models with tens of concepts and their associated\nkeywords detected in Spanish tweets geolocated in Spain. We employ\ndialectometric measures (cosine similarity and Jensen-Shannon divergence) to\nquantify the linguistic distance on the lexical level between cells created in\na uniform grid over the map. This can be done for a single concept or in the\ngeneral case taking into account an average of the considered variants. The\nlatter permits an analysis of the dialects that naturally emerge from the data.\nInterestingly, our results reveal the existence of two dialect macrovarieties.\nThe first group includes a region-specific speech spoken in small towns and\nrural areas whereas the second cluster encompasses cities that tend to use a\nmore uniform variety. Since the results obtained with the two different metrics\nqualitatively agree, our work suggests that social media corpora can be\nefficiently used for dialectometric analyses. \n\n"}
{"id": "1702.06877", "contents": "Title: Mean Birds: Detecting Aggression and Bullying on Twitter Abstract: In recent years, bullying and aggression against users on social media have\ngrown significantly, causing serious consequences to victims of all\ndemographics. In particular, cyberbullying affects more than half of young\nsocial media users worldwide, and has also led to teenage suicides, prompted by\nprolonged and/or coordinated digital harassment. Nonetheless, tools and\ntechnologies for understanding and mitigating it are scarce and mostly\nineffective. In this paper, we present a principled and scalable approach to\ndetect bullying and aggressive behavior on Twitter. We propose a robust\nmethodology for extracting text, user, and network-based attributes, studying\nthe properties of cyberbullies and aggressors, and what features distinguish\nthem from regular users. We find that bully users post less, participate in\nfewer online communities, and are less popular than normal users, while\naggressors are quite popular and tend to include more negativity in their\nposts. We evaluate our methodology using a corpus of 1.6M tweets posted over 3\nmonths, and show that machine learning classification algorithms can accurately\ndetect users exhibiting bullying and aggressive behavior, achieving over 90%\nAUC. \n\n"}
{"id": "1702.07752", "contents": "Title: A supervised approach to time scale detection in dynamic networks Abstract: For any stream of time-stamped edges that form a dynamic network, an\nimportant choice is the aggregation granularity that an analyst uses to bin the\ndata. Picking such a windowing of the data is often done by hand, or left up to\nthe technology that is collecting the data. However, the choice can make a big\ndifference in the properties of the dynamic network. This is the time scale\ndetection problem. In previous work, this problem is often solved with a\nheuristic as an unsupervised task. As an unsupervised problem, it is difficult\nto measure how well a given algorithm performs. In addition, we show that the\nquality of the windowing is dependent on which task an analyst wants to perform\non the network after windowing. Therefore the time scale detection problem\nshould not be handled independently from the rest of the analysis of the\nnetwork.\n  We introduce a framework that tackles both of these issues: By measuring the\nperformance of the time scale detection algorithm based on how well a given\ntask is accomplished on the resulting network, we are for the first time able\nto directly compare different time scale detection algorithms to each other.\nUsing this framework, we introduce time scale detection algorithms that take a\nsupervised approach: they leverage ground truth on training data to find a good\nwindowing of the test data. We compare the supervised approach to previous\napproaches and several baselines on real data. \n\n"}
{"id": "1702.07944", "contents": "Title: Stochastic Variance Reduction Methods for Policy Evaluation Abstract: Policy evaluation is a crucial step in many reinforcement-learning\nprocedures, which estimates a value function that predicts states' long-term\nvalue under a given policy. In this paper, we focus on policy evaluation with\nlinear function approximation over a fixed dataset. We first transform the\nempirical policy evaluation problem into a (quadratic) convex-concave saddle\npoint problem, and then present a primal-dual batch gradient method, as well as\ntwo stochastic variance reduction methods for solving the problem. These\nalgorithms scale linearly in both sample size and feature dimension. Moreover,\nthey achieve linear convergence even when the saddle-point problem has only\nstrong concavity in the dual variables but no strong convexity in the primal\nvariables. Numerical experiments on benchmark problems demonstrate the\neffectiveness of our methods. \n\n"}
{"id": "1702.08097", "contents": "Title: A Selfie is Worth a Thousand Words: Mining Personal Patterns behind User\n  Selfie-posting Behaviours Abstract: Selfies have become increasingly fashionable in the social media era. People\nare willing to share their selfies in various social media platforms such as\nFacebook, Instagram and Flicker. The popularity of selfie have caught\nresearchers' attention, especially psychologists. In computer vision and\nmachine learning areas, little attention has been paid to this phenomenon as a\nvaluable data source. In this paper, we focus on exploring the deeper personal\npatterns behind people's different kinds of selfie-posting behaviours. We\ndevelop this work based on a dataset of WeChat, one of the most extensively\nused instant messaging platform in China. In particular, we first propose an\nunsupervised approach to classify the images posted by users. Based on the\nclassification result, we construct three types of user-level features that\nreflect user preference, activity and posting habit. Based on these features,\nfor a series of selfie related tasks, we build classifiers that can accurately\npredict two sets of users with opposite selfie-posting behaviours. We have\nfound that people's interest, activity and posting habit have a great influence\non their selfie-posting behaviours. For example, the classification accuracy\nbetween selfie-posting addict and nonaddict reaches 89.36%. We also prove that\nusing user's image information to predict these behaviours achieve better\nperformance than using text information. More importantly, for each set of\nusers with a specific selfie-posting behaviour, we extract and visualize\nsignificant personal patterns about them. In addition, we cluster users and\nextract their high-level attributes, revealing the correlation between these\nattributes and users' selfie-posting behaviours. In the end, we demonstrate\nthat users' selfie-posting behaviour, as a good predictor, could predict their\ndifferent preferences toward these high-level attributes accurately. \n\n"}
{"id": "1702.08235", "contents": "Title: Variational Inference using Implicit Distributions Abstract: Generative adversarial networks (GANs) have given us a great tool to fit\nimplicit generative models to data. Implicit distributions are ones we can\nsample from easily, and take derivatives of samples with respect to model\nparameters. These models are highly expressive and we argue they can prove just\nas useful for variational inference (VI) as they are for generative modelling.\nSeveral papers have proposed GAN-like algorithms for inference, however,\nconnections to the theory of VI are not always well understood. This paper\nprovides a unifying review of existing algorithms establishing connections\nbetween variational autoencoders, adversarially learned inference, operator VI,\nGAN-based image reconstruction, and more. Secondly, the paper provides a\nframework for building new algorithms: depending on the way the variational\nbound is expressed we introduce prior-contrastive and joint-contrastive\nmethods, and show practical inference algorithms based on either density ratio\nestimation or denoising. \n\n"}
{"id": "1702.08459", "contents": "Title: Complex Networks from Classical to Quantum Abstract: Recent progress in applying complex network theory to problems in quantum\ninformation has resulted in a beneficial crossover. Complex network methods\nhave successfully been applied to transport and entanglement models while\ninformation physics is setting the stage for a theory of complex systems with\nquantum information-inspired methods. Novel quantum induced effects have been\npredicted in random graphs---where edges represent entangled links---and\nquantum computer algorithms have been proposed to offer enhancement for several\nnetwork problems. Here we review the results at the cutting edge, pinpointing\nthe similarities and the differences found at the intersection of these two\nfields. \n\n"}
{"id": "1702.08536", "contents": "Title: Fast Threshold Tests for Detecting Discrimination Abstract: Threshold tests have recently been proposed as a useful method for detecting\nbias in lending, hiring, and policing decisions. For example, in the case of\ncredit extensions, these tests aim to estimate the bar for granting loans to\nwhite and minority applicants, with a higher inferred threshold for minorities\nindicative of discrimination. This technique, however, requires fitting a\ncomplex Bayesian latent variable model for which inference is often\ncomputationally challenging. Here we develop a method for fitting threshold\ntests that is two orders of magnitude faster than the existing approach,\nreducing computation from hours to minutes. To achieve these performance gains,\nwe introduce and analyze a flexible family of probability distributions on the\ninterval [0, 1] -- which we call discriminant distributions -- that is\ncomputationally efficient to work with. We demonstrate our technique by\nanalyzing 2.7 million police stops of pedestrians in New York City. \n\n"}
{"id": "1702.08887", "contents": "Title: Stabilising Experience Replay for Deep Multi-Agent Reinforcement\n  Learning Abstract: Many real-world problems, such as network packet routing and urban traffic\ncontrol, are naturally modeled as multi-agent reinforcement learning (RL)\nproblems. However, existing multi-agent RL methods typically scale poorly in\nthe problem size. Therefore, a key challenge is to translate the success of\ndeep learning on single-agent RL to the multi-agent setting. A major stumbling\nblock is that independent Q-learning, the most popular multi-agent RL method,\nintroduces nonstationarity that makes it incompatible with the experience\nreplay memory on which deep Q-learning relies. This paper proposes two methods\nthat address this problem: 1) using a multi-agent variant of importance\nsampling to naturally decay obsolete data and 2) conditioning each agent's\nvalue function on a fingerprint that disambiguates the age of the data sampled\nfrom the replay memory. Results on a challenging decentralised variant of\nStarCraft unit micromanagement confirm that these methods enable the successful\ncombination of experience replay with multi-agent RL. \n\n"}
{"id": "1703.00102", "contents": "Title: SARAH: A Novel Method for Machine Learning Problems Using Stochastic\n  Recursive Gradient Abstract: In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH),\nas well as its practical variant SARAH+, as a novel approach to the finite-sum\nminimization problems. Different from the vanilla SGD and other modern\nstochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple\nrecursive framework for updating stochastic gradient estimates; when comparing\nto SAG/SAGA, SARAH does not require a storage of past gradients. The linear\nconvergence rate of SARAH is proven under strong convexity assumption. We also\nprove a linear convergence rate (in the strongly convex case) for an inner loop\nof SARAH, the property that SVRG does not possess. Numerical experiments\ndemonstrate the efficiency of our algorithm. \n\n"}
{"id": "1703.00410", "contents": "Title: Detecting Adversarial Samples from Artifacts Abstract: Deep neural networks (DNNs) are powerful nonlinear architectures that are\nknown to be robust to random perturbations of the input. However, these models\nare vulnerable to adversarial perturbations--small input changes crafted\nexplicitly to fool the model. In this paper, we ask whether a DNN can\ndistinguish adversarial samples from their normal and noisy counterparts. We\ninvestigate model confidence on adversarial samples by looking at Bayesian\nuncertainty estimates, available in dropout neural networks, and by performing\ndensity estimation in the subspace of deep features learned by the model. The\nresult is a method for implicit adversarial detection that is oblivious to the\nattack algorithm. We evaluate this method on a variety of standard datasets\nincluding MNIST and CIFAR-10 and show that it generalizes well across different\narchitectures and attacks. Our findings report that 85-93% ROC-AUC can be\nachieved on a number of standard classification tasks with a negative class\nthat consists of both normal and noisy samples. \n\n"}
{"id": "1703.01501", "contents": "Title: Opinion dynamics model based on cognitive biases Abstract: We present an introduction to a novel model of an individual and group\nopinion dynamics, taking into account different ways in which different sources\nof information are filtered due to cognitive biases. The agent based model,\nusing Bayesian updating of the individual belief distribution, is based on the\nrecent psychology work by Dan Kahan. Open nature of the model allows to study\nthe effects of both static and time-dependent biases and information processing\nfilters. In particular, the paper compares the effects of two important\npsychological mechanisms: the confirmation bias and the politically motivated\nreasoning. Depending on the effectiveness of the information filtering (agent\nbias), the agents confronted with an objective information source may either\nreach a consensus based on the truth, or remain divided despite the evidence.\nIn general, the model might provide an understanding into the increasingly\npolarized modern societies, especially as it allows mixing of different types\nof filters: psychological, social, and algorithmic. \n\n"}
{"id": "1703.01594", "contents": "Title: Graph sampling with determinantal processes Abstract: We present a new random sampling strategy for k-bandlimited signals defined\non graphs, based on determinantal point processes (DPP). For small graphs, ie,\nin cases where the spectrum of the graph is accessible, we exhibit a DPP\nsampling scheme that enables perfect recovery of bandlimited signals. For large\ngraphs, ie, in cases where the graph's spectrum is not accessible, we\ninvestigate, both theoretically and empirically, a sub-optimal but much faster\nDPP based on loop-erased random walks on the graph. Preliminary experiments\nshow promising results especially in cases where the number of measurements\nshould stay as small as possible and for graphs that have a strong community\nstructure. Our sampling scheme is efficient and can be applied to graphs with\nup to $10^6$ nodes. \n\n"}
{"id": "1703.06227", "contents": "Title: Discriminative Distance-Based Network Indices with Application to Link\n  Prediction Abstract: In large networks, using the length of shortest paths as the distance measure\nhas shortcomings. A well-studied shortcoming is that extending it to\ndisconnected graphs and directed graphs is controversial. The second\nshortcoming is that a huge number of vertices may have exactly the same score.\nThe third shortcoming is that in many applications, the distance between two\nvertices not only depends on the length of shortest paths, but also on the\nnumber of shortest paths. In this paper, first we develop a new distance\nmeasure between vertices of a graph that yields discriminative distance-based\ncentrality indices. This measure is proportional to the length of shortest\npaths and inversely proportional to the number of shortest paths. We present\nalgorithms for exact computation of the proposed discriminative indices.\nSecond, we develop randomized algorithms that precisely estimate average\ndiscriminative path length and average discriminative eccentricity and show\nthat they give $(\\epsilon,\\delta)$-approximations of these indices. Third, we\nperform extensive experiments over several real-world networks from different\ndomains. In our experiments, we first show that compared to the traditional\nindices, discriminative indices have usually much more discriminability. Then,\nwe show that our randomized algorithms can very precisely estimate average\ndiscriminative path length and average discriminative eccentricity, using only\nfew samples. Then, we show that real-world networks have usually a tiny average\ndiscriminative path length, bounded by a constant (e.g., 2). Fourth, in order\nto better motivate the usefulness of our proposed distance measure, we present\na novel link prediction method, that uses discriminative distance to decide\nwhich vertices are more likely to form a link in future, and show its superior\nperformance compared to the well-known existing measures. \n\n"}
{"id": "1703.06361", "contents": "Title: Which friends are more popular than you? Contact strength and the\n  friendship paradox in social networks Abstract: The friendship paradox states that in a social network, egos tend to have\nlower degree than their alters, or, \"your friends have more friends than you\ndo\". Most research has focused on the friendship paradox and its implications\nfor information transmission, but treating the network as static and\nunweighted. Yet, people can dedicate only a finite fraction of their attention\nbudget to each social interaction: a high-degree individual may have less time\nto dedicate to individual social links, forcing them to modulate the quantities\nof contact made to their different social ties. Here we study the friendship\nparadox in the context of differing contact volumes between egos and alters,\nfinding a connection between contact volume and the strength of the friendship\nparadox. The most frequently contacted alters exhibit a less pronounced\nfriendship paradox compared with the ego, whereas less-frequently contacted\nalters are more likely to be high degree and give rise to the paradox. We argue\ntherefore for a more nuanced version of the friendship paradox: \"your closest\nfriends have slightly more friends than you do\", and in certain networks even:\n\"your best friend has no more friends than you do\". We demonstrate that this\nrelationship is robust, holding in both a social media and a mobile phone\ndataset. These results have implications for information transfer and influence\nin social networks, which we explore using a simple dynamical model. \n\n"}
{"id": "1703.06975", "contents": "Title: Learning to Generate Samples from Noise through Infusion Training Abstract: In this work, we investigate a novel training procedure to learn a generative\nmodel as the transition operator of a Markov chain, such that, when applied\nrepeatedly on an unstructured random noise sample, it will denoise it into a\nsample that matches the target distribution from the training set. The novel\ntraining procedure to learn this progressive denoising operation involves\nsampling from a slightly different chain than the model chain used for\ngeneration in the absence of a denoising target. In the training chain we\ninfuse information from the training target example that we would like the\nchains to reach with a high probability. The thus learned transition operator\nis able to produce quality and varied samples in a small number of steps.\nExperiments show competitive results compared to the samples generated with a\nbasic Generative Adversarial Net \n\n"}
{"id": "1703.07056", "contents": "Title: Stochastic Primal Dual Coordinate Method with Non-Uniform Sampling Based\n  on Optimality Violations Abstract: We study primal-dual type stochastic optimization algorithms with non-uniform\nsampling. Our main theoretical contribution in this paper is to present a\nconvergence analysis of Stochastic Primal Dual Coordinate (SPDC) Method with\narbitrary sampling. Based on this theoretical framework, we propose Optimality\nViolation-based Sampling SPDC (ovsSPDC), a non-uniform sampling method based on\nOptimality Violation. We also propose two efficient heuristic variants of\novsSPDC called ovsSDPC+ and ovsSDPC++. Through intensive numerical experiments,\nwe demonstrate that the proposed method and its variants are faster than other\nstate-of-the-art primal-dual type stochastic optimization methods. \n\n"}
{"id": "1703.07285", "contents": "Title: From safe screening rules to working sets for faster Lasso-type solvers Abstract: Convex sparsity-promoting regularizations are ubiquitous in modern\nstatistical learning. By construction, they yield solutions with few non-zero\ncoefficients, which correspond to saturated constraints in the dual\noptimization formulation. Working set (WS) strategies are generic optimization\ntechniques that consist in solving simpler problems that only consider a subset\nof constraints, whose indices form the WS. Working set methods therefore\ninvolve two nested iterations: the outer loop corresponds to the definition of\nthe WS and the inner loop calls a solver for the subproblems. For the Lasso\nestimator a WS is a set of features, while for a Group Lasso it refers to a set\nof groups. In practice, WS are generally small in this context so the\nassociated feature Gram matrix can fit in memory. Here we show that the\nGauss-Southwell rule (a greedy strategy for block coordinate descent\ntechniques) leads to fast solvers in this case. Combined with a working set\nstrategy based on an aggressive use of so-called Gap Safe screening rules, we\npropose a solver achieving state-of-the-art performance on sparse learning\nproblems. Results are presented on Lasso and multi-task Lasso estimators. \n\n"}
{"id": "1703.07656", "contents": "Title: Randomizing growing networks with a time-respecting null model Abstract: Complex networks are often used to represent systems that are not static but\ngrow with time: people make new friendships, new papers are published and refer\nto the existing ones, and so forth. To assess the statistical significance of\nmeasurements made on such networks, we propose a randomization methodology---a\ntime-respecting null model---that preserves both the network's degree sequence\nand the time evolution of individual nodes' degree values. By preserving the\ntemporal linking patterns of the analyzed system, the proposed model is able to\nfactor out the effect of the system's temporal patterns on its structure. We\napply the model to the citation network of Physical Review scholarly papers and\nthe citation network of US movies. The model reveals that the two datasets are\nstrikingly different with respect to their degree-degree correlations, and we\ndiscuss the important implications of this finding on the information provided\nby paradigmatic node centrality metrics such as indegree and Google's PageRank.\nThe randomization methodology proposed here can be used to assess the\nsignificance of any structural property in growing networks, which could bring\nnew insights into the problems where null models play a critical role, such as\nthe detection of communities and network motifs. \n\n"}
{"id": "1703.07708", "contents": "Title: Prediction of employment and unemployment rates from Twitter daily\n  rhythms in the US Abstract: By modeling macro-economical indicators using digital traces of human\nactivities on mobile or social networks, we can provide important insights to\nprocesses previously assessed via paper-based surveys or polls only. We\ncollected aggregated workday activity timelines of US counties from the\nnormalized number of messages sent in each hour on the online social network\nTwitter. In this paper, we show how county employment and unemployment\nstatistics are encoded in the daily rhythm of people by decomposing the\nactivity timelines into a linear combination of two dominant patterns. The\nmixing ratio of these patterns defines a measure for each county, that\ncorrelates significantly with employment ($0.46\\pm0.02$) and unemployment rates\n($-0.34\\pm0.02$). Thus, the two dominant activity patterns can be linked to\nrhythms signaling presence or lack of regular working hours of individuals. The\nanalysis could provide policy makers a better insight into the processes\ngoverning employment, where problems could not only be identified based on the\nnumber of officially registered unemployed, but also on the basis of the\ndigital footprints people leave on different platforms. \n\n"}
{"id": "1703.09028", "contents": "Title: De-anonymization of Social Networks with Communities: When\n  Quantifications Meet Algorithms Abstract: A crucial privacy-driven issue nowadays is re-identifying anonymized social\nnetworks by mapping them to correlated cross-domain auxiliary networks. Prior\nworks are typically based on modeling social networks as random graphs\nrepresenting users and their relations, and subsequently quantify the quality\nof mappings through cost functions that are proposed without sufficient\nrationale. Also, it remains unknown how to algorithmically meet the demand of\nsuch quantifications, i.e., to find the minimizer of the cost functions. We\naddress those concerns in a more realistic social network modeling\nparameterized by community structures that can be leveraged as side information\nfor de-anonymization. By Maximum A Posteriori (MAP) estimation, our first\ncontribution is new and well justified cost functions, which, when minimized,\nenjoy superiority to previous ones in finding the correct mapping with the\nhighest probability. The feasibility of the cost functions is then for the\nfirst time algorithmically characterized. While proving the general\nmultiplicative inapproximability, we are able to propose two algorithms, which,\nrespectively, enjoy an \\epsilon-additive approximation and a conditional\noptimality in carrying out successful user re-identification. Our theoretical\nfindings are empirically validated, with a notable dataset extracted from rare\ntrue cross-domain networks that reproduce genuine social network\nde-anonymization. Both theoretical and empirical observations also manifest the\nimportance of community information in enhancing privacy inferencing. \n\n"}
{"id": "1703.10622", "contents": "Title: Diving into the shallows: a computational perspective on large-scale\n  shallow learning Abstract: In this paper we first identify a basic limitation in gradient descent-based\noptimization methods when used in conjunctions with smooth kernels. An analysis\nbased on the spectral properties of the kernel demonstrates that only a\nvanishingly small portion of the function space is reachable after a polynomial\nnumber of gradient descent iterations. This lack of approximating power\ndrastically limits gradient descent for a fixed computational budget leading to\nserious over-regularization/underfitting. The issue is purely algorithmic,\npersisting even in the limit of infinite data.\n  To address this shortcoming in practice, we introduce EigenPro iteration,\nbased on a preconditioning scheme using a small number of approximately\ncomputed eigenvectors. It can also be viewed as learning a new kernel optimized\nfor gradient descent. It turns out that injecting this small (computationally\ninexpensive and SGD-compatible) amount of approximate second-order information\nleads to major improvements in convergence. For large data, this translates\ninto significant performance boost over the standard kernel methods. In\nparticular, we are able to consistently match or improve the state-of-the-art\nresults recently reported in the literature with a small fraction of their\ncomputational budget.\n  Finally, we feel that these results show a need for a broader computational\nperspective on modern large-scale learning to complement more traditional\nstatistical and convergence analyses. In particular, many phenomena of\nlarge-scale high-dimensional inference are best understood in terms of\noptimization on infinite dimensional Hilbert spaces, where standard algorithms\ncan sometimes have properties at odds with finite-dimensional intuition. A\nsystematic analysis concentrating on the approximation power of such algorithms\nwithin a budget of computation may lead to progress both in theory and\npractice. \n\n"}
{"id": "1704.01524", "contents": "Title: Core of communities in bipartite networks Abstract: We use the information present in a bipartite network to detect cores of\ncommunities of each set of the bipartite system. Cores of communities are found\nby investigating statistically validated projected networks obtained using\ninformation present in the bipartite network. Cores of communities are highly\ninformative and robust with respect to the presence of errors or missing\nentries in the bipartite network. We assess the statistical robustness of cores\nby investigating an artificial benchmark network, the co-authorship network,\nand the actor-movie network. The accuracy and precision of the partition\nobtained with respect to the reference partition are measured in terms of the\nadjusted Rand index and of the adjusted Wallace index respectively. The\ndetection of cores is highly precise although the accuracy of the methodology\ncan be limited in some cases. \n\n"}
{"id": "1704.02890", "contents": "Title: Opinion Polarization by Learning from Social Feedback Abstract: We explore a new mechanism to explain polarization phenomena in opinion\ndynamics in which agents evaluate alternative views on the basis of the social\nfeedback obtained on expressing them. High support of the favored opinion in\nthe social environment, is treated as a positive feedback which reinforces the\nvalue associated to this opinion. In connected networks of sufficiently high\nmodularity, different groups of agents can form strong convictions of competing\nopinions. Linking the social feedback process to standard equilibrium concepts\nwe analytically characterize sufficient conditions for the stability of\nbi-polarization. While previous models have emphasized the polarization effects\nof deliberative argument-based communication, our model highlights an affective\nexperience-based route to polarization, without assumptions about negative\ninfluence or bounded confidence. \n\n"}
{"id": "1704.03976", "contents": "Title: Virtual Adversarial Training: A Regularization Method for Supervised and\n  Semi-Supervised Learning Abstract: We propose a new regularization method based on virtual adversarial loss: a\nnew measure of local smoothness of the conditional label distribution given\ninput. Virtual adversarial loss is defined as the robustness of the conditional\nlabel distribution around each input data point against local perturbation.\nUnlike adversarial training, our method defines the adversarial direction\nwithout label information and is hence applicable to semi-supervised learning.\nBecause the directions in which we smooth the model are only \"virtually\"\nadversarial, we call our method virtual adversarial training (VAT). The\ncomputational cost of VAT is relatively low. For neural networks, the\napproximated gradient of virtual adversarial loss can be computed with no more\nthan two pairs of forward- and back-propagations. In our experiments, we\napplied VAT to supervised and semi-supervised learning tasks on multiple\nbenchmark datasets. With a simple enhancement of the algorithm based on the\nentropy minimization principle, our VAT achieves state-of-the-art performance\nfor semi-supervised learning tasks on SVHN and CIFAR-10. \n\n"}
{"id": "1704.04289", "contents": "Title: Stochastic Gradient Descent as Approximate Bayesian Inference Abstract: Stochastic Gradient Descent with a constant learning rate (constant SGD)\nsimulates a Markov chain with a stationary distribution. With this perspective,\nwe derive several new results. (1) We show that constant SGD can be used as an\napproximate Bayesian posterior inference algorithm. Specifically, we show how\nto adjust the tuning parameters of constant SGD to best match the stationary\ndistribution to a posterior, minimizing the Kullback-Leibler divergence between\nthese two distributions. (2) We demonstrate that constant SGD gives rise to a\nnew variational EM algorithm that optimizes hyperparameters in complex\nprobabilistic models. (3) We also propose SGD with momentum for sampling and\nshow how to adjust the damping coefficient accordingly. (4) We analyze MCMC\nalgorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we\nquantify the approximation errors due to finite learning rates. Finally (5), we\nuse the stochastic process perspective to give a short proof of why Polyak\naveraging is optimal. Based on this idea, we propose a scalable approximate\nMCMC algorithm, the Averaged Stochastic Gradient Sampler. \n\n"}
{"id": "1704.05516", "contents": "Title: Graph Model Selection via Random Walks Abstract: In this paper, we present a novel approach based on the random walk process\nfor finding meaningful representations of a graph model. Our approach leverages\nthe transient behavior of many short random walks with novel initialization\nmechanisms to generate model discriminative features. These features are able\nto capture a more comprehensive structural signature of the underlying graph\nmodel. The resulting representation is invariant to both node permutation and\nthe size of the graph, allowing direct comparison between large classes of\ngraphs. We test our approach on two challenging model selection problems: the\ndiscrimination in the sparse regime of an Erd\\\"{o}s-Renyi model from a\nstochastic block model and the planted clique problem. Our representation\napproach achieves performance that closely matches known theoretical limits in\naddition to being computationally simple and scalable to large graphs. \n\n"}
{"id": "1704.07147", "contents": "Title: A Neural Network model with Bidirectional Whitening Abstract: We present here a new model and algorithm which performs an efficient Natural\ngradient descent for Multilayer Perceptrons. Natural gradient descent was\noriginally proposed from a point of view of information geometry, and it\nperforms the steepest descent updates on manifolds in a Riemannian space. In\nparticular, we extend an approach taken by the \"Whitened neural networks\"\nmodel. We make the whitening process not only in feed-forward direction as in\nthe original model, but also in the back-propagation phase. Its efficacy is\nshown by an application of this \"Bidirectional whitened neural networks\" model\nto a handwritten character recognition data (MNIST data). \n\n"}
{"id": "1704.07228", "contents": "Title: Learning from Comparisons and Choices Abstract: When tracking user-specific online activities, each user's preference is\nrevealed in the form of choices and comparisons. For example, a user's purchase\nhistory is a record of her choices, i.e. which item was chosen among a subset\nof offerings. A user's preferences can be observed either explicitly as in\nmovie ratings or implicitly as in viewing times of news articles. Given such\nindividualized ordinal data in the form of comparisons and choices, we address\nthe problem of collaboratively learning representations of the users and the\nitems. The learned features can be used to predict a user's preference of an\nunseen item to be used in recommendation systems. This also allows one to\ncompute similarities among users and items to be used for categorization and\nsearch. Motivated by the empirical successes of the MultiNomial Logit (MNL)\nmodel in marketing and transportation, and also more recent successes in word\nembedding and crowdsourced image embedding, we pose this problem as learning\nthe MNL model parameters that best explain the data. We propose a convex\nrelaxation for learning the MNL model, and show that it is minimax optimal up\nto a logarithmic factor by comparing its performance to a fundamental lower\nbound. This characterizes the minimax sample complexity of the problem, and\nproves that the proposed estimator cannot be improved upon other than by a\nlogarithmic factor. Further, the analysis identifies how the accuracy depends\non the topology of sampling via the spectrum of the sampling graph. This\nprovides a guideline for designing surveys when one can choose which items are\nto be compared. This is accompanied by numerical simulations on synthetic and\nreal data sets, confirming our theoretical predictions. \n\n"}
{"id": "1704.07433", "contents": "Title: Active Bias: Training More Accurate Neural Networks by Emphasizing High\n  Variance Samples Abstract: Self-paced learning and hard example mining re-weight training instances to\nimprove learning accuracy. This paper presents two improved alternatives based\non lightweight estimates of sample uncertainty in stochastic gradient descent\n(SGD): the variance in predicted probability of the correct class across\niterations of mini-batch SGD, and the proximity of the correct class\nprobability to the decision threshold. Extensive experimental results on six\ndatasets show that our methods reliably improve accuracy in various network\narchitectures, including additional gains on top of other popular training\ntechniques, such as residual learning, momentum, ADAM, batch normalization,\ndropout, and distillation. \n\n"}
{"id": "1704.07525", "contents": "Title: Graph Theoretic Properties of the Darkweb Abstract: We collect and analyze the darkweb (a.k.a. the \"onionweb\") hyperlink graph.\nWe find properties highly dissimilar to the well-studied world wide web\nhyperlink graph; for example, our analysis finds that >87% of darkweb sites\nnever link to another site. We compare our results to prior work on\nworld-wide-web and speculate about reasons for their differences. We conclude\nthat in the term \"darkweb\", the word \"web\" is a connectivity misnomer. Instead,\nit is more accurate to view the darkweb as a set of largely isolated dark\nsilos. \n\n"}
{"id": "1704.08227", "contents": "Title: Accelerating Stochastic Gradient Descent For Least Squares Regression Abstract: There is widespread sentiment that it is not possible to effectively utilize\nfast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy\nball) for the purposes of stochastic optimization due to their instability and\nerror accumulation, a notion made precise in d'Aspremont 2008 and Devolder,\nGlineur, and Nesterov 2014. This work considers these issues for the special\ncase of stochastic approximation for the least squares regression problem, and\nour main result refutes the conventional wisdom by showing that acceleration\ncan be made robust to statistical errors. In particular, this work introduces\nan accelerated stochastic gradient method that provably achieves the minimax\noptimal statistical risk faster than stochastic gradient descent. Critical to\nthe analysis is a sharp characterization of accelerated stochastic gradient\ndescent as a stochastic process. We hope this characterization gives insights\ntowards the broader question of designing simple and effective accelerated\nstochastic methods for more general convex and non-convex optimization\nproblems. \n\n"}
{"id": "1704.08756", "contents": "Title: A Network Perspective on Stratification of Multi-Label Data Abstract: In the recent years, we have witnessed the development of multi-label\nclassification methods which utilize the structure of the label space in a\ndivide and conquer approach to improve classification performance and allow\nlarge data sets to be classified efficiently. Yet most of the available data\nsets have been provided in train/test splits that did not account for\nmaintaining a distribution of higher-order relationships between labels among\nsplits or folds. We present a new approach to stratifying multi-label data for\nclassification purposes based on the iterative stratification approach proposed\nby Sechidis et. al. in an ECML PKDD 2011 paper. Our method extends the\niterative approach to take into account second-order relationships between\nlabels. Obtained results are evaluated using statistical properties of obtained\nstrata as presented by Sechidis. We also propose new statistical measures\nrelevant to second-order quality: label pairs distribution, the percentage of\nlabel pairs without positive evidence in folds and label pair - fold pairs that\nhave no positive evidence for the label pair. We verify the impact of new\nmethods on classification performance of Binary Relevance, Label Powerset and a\nfast greedy community detection based label space partitioning classifier.\nRandom Forests serve as base classifiers. We check the variation of the number\nof communities obtained per fold, and the stability of their modularity score.\nSecond-Order Iterative Stratification is compared to standard k-fold, label\nset, and iterative stratification. The proposed approach lowers the variance of\nclassification quality, improves label pair oriented measures and example\ndistribution while maintaining a competitive quality in label-oriented\nmeasures. We also witness an increase in stability of network characteristics. \n\n"}
{"id": "1704.08792", "contents": "Title: DeepArchitect: Automatically Designing and Training Deep Architectures Abstract: In deep learning, performance is strongly affected by the choice of\narchitecture and hyperparameters. While there has been extensive work on\nautomatic hyperparameter optimization for simple spaces, complex spaces such as\nthe space of deep architectures remain largely unexplored. As a result, the\nchoice of architecture is done manually by the human expert through a slow\ntrial and error process guided mainly by intuition. In this paper we describe a\nframework for automatically designing and training deep models. We propose an\nextensible and modular language that allows the human expert to compactly\nrepresent complex search spaces over architectures and their hyperparameters.\nThe resulting search spaces are tree-structured and therefore easy to traverse.\nModels can be automatically compiled to computational graphs once values for\nall hyperparameters have been chosen. We can leverage the structure of the\nsearch space to introduce different model search algorithms, such as random\nsearch, Monte Carlo tree search (MCTS), and sequential model-based optimization\n(SMBO). We present experiments comparing the different algorithms on CIFAR-10\nand show that MCTS and SMBO outperform random search. In addition, these\nexperiments show that our framework can be used effectively for model\ndiscovery, as it is possible to describe expressive search spaces and discover\ncompetitive models without much effort from the human expert. Code for our\nframework and experiments has been made publicly available. \n\n"}
{"id": "1705.00470", "contents": "Title: Learning Multimodal Transition Dynamics for Model-Based Reinforcement\n  Learning Abstract: In this paper we study how to learn stochastic, multimodal transition\ndynamics in reinforcement learning (RL) tasks. We focus on evaluating\ntransition function estimation, while we defer planning over this model to\nfuture work. Stochasticity is a fundamental property of many task environments.\nHowever, discriminative function approximators have difficulty estimating\nmultimodal stochasticity. In contrast, deep generative models do capture\ncomplex high-dimensional outcome distributions. First we discuss why, amongst\nsuch models, conditional variational inference (VI) is theoretically most\nappealing for model-based RL. Subsequently, we compare different VI models on\ntheir ability to learn complex stochasticity on simulated functions, as well as\non a typical RL gridworld with multimodal dynamics. Results show VI\nsuccessfully predicts multimodal outcomes, but also robustly ignores these for\ndeterministic parts of the transition dynamics. In summary, we show a robust\nmethod to learn multimodal transitions using function approximation, which is a\nkey preliminary for model-based RL in stochastic domains. \n\n"}
{"id": "1705.01613", "contents": "Title: Automatically Identifying Fake News in Popular Twitter Threads Abstract: Information quality in social media is an increasingly important issue, but\nweb-scale data hinders experts' ability to assess and correct much of the\ninaccurate content, or `fake news,' present in these platforms. This paper\ndevelops a method for automating fake news detection on Twitter by learning to\npredict accuracy assessments in two credibility-focused Twitter datasets:\nCREDBANK, a crowdsourced dataset of accuracy assessments for events in Twitter,\nand PHEME, a dataset of potential rumors in Twitter and journalistic\nassessments of their accuracies. We apply this method to Twitter content\nsourced from BuzzFeed's fake news dataset and show models trained against\ncrowdsourced workers outperform models based on journalists' assessment and\nmodels trained on a pooled dataset of both crowdsourced workers and\njournalists. All three datasets, aligned into a uniform format, are also\npublicly available. A feature analysis then identifies features that are most\npredictive for crowdsourced and journalistic accuracy assessments, results of\nwhich are consistent with prior work. We close with a discussion contrasting\naccuracy and credibility and why models of non-experts outperform models of\njournalists for fake news detection in Twitter. \n\n"}
{"id": "1705.02033", "contents": "Title: KATE: K-Competitive Autoencoder for Text Abstract: Autoencoders have been successful in learning meaningful representations from\nimage datasets. However, their performance on text datasets has not been widely\nstudied. Traditional autoencoders tend to learn possibly trivial\nrepresentations of text documents due to their confounding properties such as\nhigh-dimensionality, sparsity and power-law word distributions. In this paper,\nwe propose a novel k-competitive autoencoder, called KATE, for text documents.\nDue to the competition between the neurons in the hidden layer, each neuron\nbecomes specialized in recognizing specific data patterns, and overall the\nmodel can learn meaningful representations of textual data. A comprehensive set\nof experiments show that KATE can learn better representations than traditional\nautoencoders including denoising, contractive, variational, and k-sparse\nautoencoders. Our model also outperforms deep generative models, probabilistic\ntopic models, and even word representation models (e.g., Word2Vec) in terms of\nseveral downstream tasks such as document classification, regression, and\nretrieval. \n\n"}
{"id": "1705.02372", "contents": "Title: Exploration of Large Networks with Covariates via Fast and Universal\n  Latent Space Model Fitting Abstract: Latent space models are effective tools for statistical modeling and\nexploration of network data. These models can effectively model real world\nnetwork characteristics such as degree heterogeneity, transitivity, homophily,\netc. Due to their close connection to generalized linear models, it is also\nnatural to incorporate covariate information in them. The current paper\npresents two universal fitting algorithms for networks with edge covariates:\none based on nuclear norm penalization and the other based on projected\ngradient descent. Both algorithms are motivated by maximizing likelihood for a\nspecial class of inner-product models while working simultaneously for a wide\nrange of different latent space models, such as distance models, which allow\nlatent vectors to affect edge formation in flexible ways. These fitting\nmethods, especially the one based on projected gradient descent, are fast and\nscalable to large networks. We obtain their rates of convergence for both\ninner-product models and beyond. The effectiveness of the modeling approach and\nfitting algorithms is demonstrated on five real world network datasets for\ndifferent statistical tasks, including community detection with and without\nedge covariates, and network assisted learning. \n\n"}
{"id": "1705.04774", "contents": "Title: Bias and variance in the social structure of gender Abstract: The observation that individuals tend to be friends with people who are\nsimilar to themselves, commonly known as homophily, is a prominent and\nwell-studied feature of social networks. Many machine learning methods exploit\nhomophily to predict attributes of individuals based on the attributes of their\nfriends. Meanwhile, recent work has shown that gender homophily can be weak or\nnonexistent in practice, making gender prediction particularly challenging. In\nthis work, we identify another useful structural feature for predicting gender,\nan overdispersion of gender preferences introduced by individuals who have\nextreme preferences for a particular gender, regardless of their own gender. We\ncall this property monophily for \"love of one,\" and jointly characterize the\nstatistical structure of homophily and monophily in social networks in terms of\npreference bias and preference variance. For prediction, we find that this\npattern of extreme gender preferences introduces friend-of-friend correlations,\nwhere individuals are similar to their friends-of-friends without necessarily\nbeing similar to their friends. We analyze a population of online friendship\nnetworks in U.S. colleges and offline friendship networks in U.S. high schools\nand observe a fundamental difference between the success of prediction methods\nbased on friends, \"the company you keep,\" compared to methods based on\nfriends-of-friends, \"the company you're kept in.\" These findings offer an\nalternative perspective on attribute prediction in general and gender in\nparticular, complicating the already difficult task of protecting attribute\nprivacy. \n\n"}
{"id": "1705.05615", "contents": "Title: Learning Edge Representations via Low-Rank Asymmetric Projections Abstract: We propose a new method for embedding graphs while preserving directed edge\ninformation. Learning such continuous-space vector representations (or\nembeddings) of nodes in a graph is an important first step for using network\ninformation (from social networks, user-item graphs, knowledge bases, etc.) in\nmany machine learning tasks.\n  Unlike previous work, we (1) explicitly model an edge as a function of node\nembeddings, and we (2) propose a novel objective, the \"graph likelihood\", which\ncontrasts information from sampled random walks with non-existent edges.\nIndividually, both of these contributions improve the learned representations,\nespecially when there are memory constraints on the total size of the\nembeddings. When combined, our contributions enable us to significantly improve\nthe state-of-the-art by learning more concise representations that better\npreserve the graph structure.\n  We evaluate our method on a variety of link-prediction task including social\nnetworks, collaboration networks, and protein interactions, showing that our\nproposed method learn representations with error reductions of up to 76% and\n55%, on directed and undirected graphs. In addition, we show that the\nrepresentations learned by our method are quite space efficient, producing\nembeddings which have higher structure-preserving accuracy but are 10 times\nsmaller. \n\n"}
{"id": "1705.06884", "contents": "Title: A Unified Framework for Stochastic Matrix Factorization via Variance\n  Reduction Abstract: We propose a unified framework to speed up the existing stochastic matrix\nfactorization (SMF) algorithms via variance reduction. Our framework is general\nand it subsumes several well-known SMF formulations in the literature. We\nperform a non-asymptotic convergence analysis of our framework and derive\ncomputational and sample complexities for our algorithm to converge to an\n$\\epsilon$-stationary point in expectation. In addition, extensive experiments\nfor a wide class of SMF formulations demonstrate that our framework\nconsistently yields faster convergence and a more accurate output dictionary\nvis-\\`a-vis state-of-the-art frameworks. \n\n"}
{"id": "1705.07038", "contents": "Title: The Landscape of Deep Learning Algorithms Abstract: This paper studies the landscape of empirical risk of deep neural networks by\ntheoretically analyzing its convergence behavior to the population risk as well\nas its stationary points and properties. For an $l$-layer linear neural\nnetwork, we prove its empirical risk uniformly converges to its population risk\nat the rate of $\\mathcal{O}(r^{2l}\\sqrt{d\\log(l)}/\\sqrt{n})$ with training\nsample size of $n$, the total weight dimension of $d$ and the magnitude bound\n$r$ of weight of each layer. We then derive the stability and generalization\nbounds for the empirical risk based on this result. Besides, we establish the\nuniform convergence of gradient of the empirical risk to its population\ncounterpart. We prove the one-to-one correspondence of the non-degenerate\nstationary points between the empirical and population risks with convergence\nguarantees, which describes the landscape of deep neural networks. In addition,\nwe analyze these properties for deep nonlinear neural networks with sigmoid\nactivation functions. We prove similar results for convergence behavior of\ntheir empirical risks as well as the gradients and analyze properties of their\nnon-degenerate stationary points.\n  To our best knowledge, this work is the first one theoretically\ncharacterizing landscapes of deep learning algorithms. Besides, our results\nprovide the sample complexity of training a good deep neural network. We also\nprovide theoretical understanding on how the neural network depth $l$, the\nlayer width, the network size $d$ and parameter magnitude determine the neural\nnetwork landscapes. \n\n"}
{"id": "1705.07261", "contents": "Title: Stochastic Recursive Gradient Algorithm for Nonconvex Optimization Abstract: In this paper, we study and analyze the mini-batch version of StochAstic\nRecursive grAdient algoritHm (SARAH), a method employing the stochastic\nrecursive gradient, for solving empirical loss minimization for the case of\nnonconvex losses. We provide a sublinear convergence rate (to stationary\npoints) for general nonconvex functions and a linear convergence rate for\ngradient dominated functions, both of which have some advantages compared to\nother modern stochastic gradient algorithms for nonconvex losses. \n\n"}
{"id": "1705.07384", "contents": "Title: Balanced Policy Evaluation and Learning Abstract: We present a new approach to the problems of evaluating and learning\npersonalized decision policies from observational data of past contexts,\ndecisions, and outcomes. Only the outcome of the enacted decision is available\nand the historical policy is unknown. These problems arise in personalized\nmedicine using electronic health records and in internet advertising. Existing\napproaches use inverse propensity weighting (or, doubly robust versions) to\nmake historical outcome (or, residual) data look like it were generated by a\nnew policy being evaluated or learned. But this relies on a plug-in approach\nthat rejects data points with a decision that disagrees with the new policy,\nleading to high variance estimates and ineffective learning. We propose a new,\nbalance-based approach that too makes the data look like the new policy but\ndoes so directly by finding weights that optimize for balance between the\nweighted data and the target policy in the given, finite sample, which is\nequivalent to minimizing worst-case or posterior conditional mean square error.\nOur policy learner proceeds as a two-level optimization problem over policies\nand weights. We demonstrate that this approach markedly outperforms existing\nones both in evaluation and learning, which is unsurprising given the wider\nsupport of balance-based weights. We establish extensive theoretical\nconsistency guarantees and regret bounds that support this empirical success. \n\n"}
{"id": "1705.08292", "contents": "Title: The Marginal Value of Adaptive Gradient Methods in Machine Learning Abstract: Adaptive optimization methods, which perform local optimization with a metric\nconstructed from the history of iterates, are becoming increasingly popular for\ntraining deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We\nshow that for simple overparameterized problems, adaptive methods often find\ndrastically different solutions than gradient descent (GD) or stochastic\ngradient descent (SGD). We construct an illustrative binary classification\nproblem where the data is linearly separable, GD and SGD achieve zero test\nerror, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to\nhalf. We additionally study the empirical generalization capability of adaptive\nmethods on several state-of-the-art deep learning models. We observe that the\nsolutions found by adaptive methods generalize worse (often significantly\nworse) than SGD, even when these solutions have better training performance.\nThese results suggest that practitioners should reconsider the use of adaptive\nmethods to train neural networks. \n\n"}
{"id": "1705.08665", "contents": "Title: Bayesian Compression for Deep Learning Abstract: Compression and computational efficiency in deep learning have become a\nproblem of great significance. In this work, we argue that the most principled\nand effective way to attack this problem is by adopting a Bayesian point of\nview, where through sparsity inducing priors we prune large parts of the\nnetwork. We introduce two novelties in this paper: 1) we use hierarchical\npriors to prune nodes instead of individual weights, and 2) we use the\nposterior uncertainties to determine the optimal fixed point precision to\nencode the weights. Both factors significantly contribute to achieving the\nstate of the art in terms of compression rates, while still staying competitive\nwith methods designed to optimize for speed or energy efficiency. \n\n"}
{"id": "1705.09756", "contents": "Title: Evolution of Social Power in Social Networks with Dynamic Topology Abstract: The recently proposed DeGroot-Friedkin model describes the dynamical\nevolution of individual social power in a social network that holds opinion\ndiscussions on a sequence of different issues. This paper revisits that model,\nand uses nonlinear contraction analysis, among other tools, to establish\nseveral novel results. First, we show that for a social network with constant\ntopology, each individual's social power converges to its equilibrium value\nexponentially fast, whereas previous results only concluded asymptotic\nconvergence. Second, when the network topology is dynamic (i.e., the relative\ninteraction matrix may change between any two successive issues), we show that\neach individual exponentially forgets its initial social power. Specifically,\nindividual social power is dependent only on the dynamic network topology, and\ninitial (or perceived) social power is forgotten as a result of sequential\nopinion discussion. Last, we provide an explicit upper bound on an individual's\nsocial power as the number of issues discussed tends to infinity; this bound\ndepends only on the network topology. Simulations are provided to illustrate\nour results. \n\n"}
{"id": "1705.09866", "contents": "Title: Machine learning for graph-based representations of three-dimensional\n  discrete fracture networks Abstract: Structural and topological information play a key role in modeling flow and\ntransport through fractured rock in the subsurface. Discrete fracture network\n(DFN) computational suites such as dfnWorks are designed to simulate flow and\ntransport in such porous media. Flow and transport calculations reveal that a\nsmall backbone of fractures exists, where most flow and transport occurs.\nRestricting the flowing fracture network to this backbone provides a\nsignificant reduction in the network's effective size. However, the particle\ntracking simulations needed to determine the reduction are computationally\nintensive. Such methods may be impractical for large systems or for robust\nuncertainty quantification of fracture networks, where thousands of forward\nsimulations are needed to bound system behavior.\n  In this paper, we develop an alternative network reduction approach to\ncharacterizing transport in DFNs, by combining graph theoretical and machine\nlearning methods. We consider a graph representation where nodes signify\nfractures and edges denote their intersections. Using random forest and support\nvector machines, we rapidly identify a subnetwork that captures the flow\npatterns of the full DFN, based primarily on node centrality features in the\ngraph. Our supervised learning techniques train on particle-tracking backbone\npaths found by dfnWorks, but run in negligible time compared to those\nsimulations. We find that our predictions can reduce the network to\napproximately 20% of its original size, while still generating breakthrough\ncurves consistent with those of the original network. \n\n"}
{"id": "1705.10102", "contents": "Title: Structural Conditions for Projection-Cost Preservation via Randomized\n  Matrix Multiplication Abstract: Projection-cost preservation is a low-rank approximation guarantee which\nensures that the cost of any rank-$k$ projection can be preserved using a\nsmaller sketch of the original data matrix. We present a general structural\nresult outlining four sufficient conditions to achieve projection-cost\npreservation. These conditions can be satisfied using tools from the Randomized\nLinear Algebra literature. \n\n"}
{"id": "1705.10621", "contents": "Title: Community Structure Characterization Abstract: This entry discusses the problem of describing some communities identified in\na complex network of interest, in a way allowing to interpret them. We suppose\nthe community structure has already been detected through one of the many\nmethods proposed in the literature. The question is then to know how to extract\nvaluable information from this first result, in order to allow human\ninterpretation. This requires subsequent processing, which we describe in the\nrest of this entry. \n\n"}
{"id": "1706.01445", "contents": "Title: Batched Large-scale Bayesian Optimization in High-dimensional Spaces Abstract: Bayesian optimization (BO) has become an effective approach for black-box\nfunction optimization problems when function evaluations are expensive and the\noptimum can be achieved within a relatively small number of queries. However,\nmany cases, such as the ones with high-dimensional inputs, may require a much\nlarger number of observations for optimization. Despite an abundance of\nobservations thanks to parallel experiments, current BO techniques have been\nlimited to merely a few thousand observations. In this paper, we propose\nensemble Bayesian optimization (EBO) to address three current challenges in BO\nsimultaneously: (1) large-scale observations; (2) high dimensional input\nspaces; and (3) selections of batch queries that balance quality and diversity.\nThe key idea of EBO is to operate on an ensemble of additive Gaussian process\nmodels, each of which possesses a randomized strategy to divide and conquer. We\nshow unprecedented, previously impossible results of scaling up BO to tens of\nthousands of observations within minutes of computation. \n\n"}
{"id": "1706.01953", "contents": "Title: Credit card fraud detection through parenclitic network analysis Abstract: The detection of frauds in credit card transactions is a major topic in\nfinancial research, of profound economic implications. While this has hitherto\nbeen tackled through data analysis techniques, the resemblances between this\nand other problems, like the design of recommendation systems and of diagnostic\n/ prognostic medical tools, suggest that a complex network approach may yield\nimportant benefits. In this contribution we present a first hybrid data mining\n/ complex network classification algorithm, able to detect illegal instances in\na real card transaction data set. It is based on a recently proposed network\nreconstruction algorithm that allows creating representations of the deviation\nof one instance from a reference group. We show how the inclusion of features\nextracted from the network data representation improves the score obtained by a\nstandard, neural network-based classification algorithm; and additionally how\nthis combined approach can outperform a commercial fraud detection system in\nspecific operation niches. Beyond these specific results, this contribution\nrepresents a new example on how complex networks and data mining can be\nintegrated as complementary tools, with the former providing a view to data\nbeyond the capabilities of the latter. \n\n"}
{"id": "1706.02275", "contents": "Title: Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments Abstract: We explore deep reinforcement learning methods for multi-agent domains. We\nbegin by analyzing the difficulty of traditional algorithms in the multi-agent\ncase: Q-learning is challenged by an inherent non-stationarity of the\nenvironment, while policy gradient suffers from a variance that increases as\nthe number of agents grows. We then present an adaptation of actor-critic\nmethods that considers action policies of other agents and is able to\nsuccessfully learn policies that require complex multi-agent coordination.\nAdditionally, we introduce a training regimen utilizing an ensemble of policies\nfor each agent that leads to more robust multi-agent policies. We show the\nstrength of our approach compared to existing methods in cooperative as well as\ncompetitive scenarios, where agent populations are able to discover various\nphysical and informational coordination strategies. \n\n"}
{"id": "1706.02899", "contents": "Title: Assessing the Performance of Deep Learning Algorithms for Newsvendor\n  Problem Abstract: In retailer management, the Newsvendor problem has widely attracted attention\nas one of basic inventory models. In the traditional approach to solving this\nproblem, it relies on the probability distribution of the demand. In theory, if\nthe probability distribution is known, the problem can be considered as fully\nsolved. However, in any real world scenario, it is almost impossible to even\napproximate or estimate a better probability distribution for the demand. In\nrecent years, researchers start adopting machine learning approach to learn a\ndemand prediction model by using other feature information. In this paper, we\npropose a supervised learning that optimizes the demand quantities for products\nbased on feature information. We demonstrate that the original Newsvendor loss\nfunction as the training objective outperforms the recently suggested quadratic\nloss function. The new algorithm has been assessed on both the synthetic data\nand real-world data, demonstrating better performance. \n\n"}
{"id": "1706.03910", "contents": "Title: A new design principle of robust onion-like networks self-organized in\n  growth Abstract: Today's economy, production activity, and our life are sustained by social\nand technological network infrastructures, while new threats of network attacks\nby destructing loops have been found recently in network science. We inversely\ntake into account the weakness, and propose a new design principle for\nincrementally growing robust networks. The networks are self-organized by\nenhancing interwoven long loops. In particular, we consider the range-limited\napproximation of linking by intermediations in a few hops, and show the strong\nrobustness in the growth without degrading efficiency of paths. Moreover, we\ndemonstrate that the tolerance of connectivity is reformable even from\nextremely vulnerable real networks according to our proposed growing process\nwith some investment. These results may indicate a prospective direction to the\nfuture growth of our network infrastructures. \n\n"}
{"id": "1706.04161", "contents": "Title: Lost Relatives of the Gumbel Trick Abstract: The Gumbel trick is a method to sample from a discrete probability\ndistribution, or to estimate its normalizing partition function. The method\nrelies on repeatedly applying a random perturbation to the distribution in a\nparticular way, each time solving for the most likely configuration. We derive\nan entire family of related methods, of which the Gumbel trick is one member,\nand show that the new methods have superior properties in several settings with\nminimal additional computational cost. In particular, for the Gumbel trick to\nyield computational benefits for discrete graphical models, Gumbel\nperturbations on all configurations are typically replaced with so-called\nlow-rank perturbations. We show how a subfamily of our new methods adapts to\nthis setting, proving new upper and lower bounds on the log partition function\nand deriving a family of sequential samplers for the Gibbs distribution.\nFinally, we balance the discussion by showing how the simpler analytical form\nof the Gumbel trick enables additional theoretical results. \n\n"}
{"id": "1706.04730", "contents": "Title: Individual position diversity in dependence socioeconomic networks\n  increases economic output Abstract: The availability of big data recorded from massively multiplayer online\nrole-playing games (MMORPGs) allows us to gain a deeper understanding of the\npotential connection between individuals' network positions and their economic\noutputs. We use a statistical filtering method to construct dependence networks\nfrom weighted friendship networks of individuals. We investigate the 30\ndistinct motif positions in the 13 directed triadic motifs which represent\nmicroscopic dependences among individuals. Based on the structural similarity\nof motif positions, we further classify individuals into different groups. The\nnode position diversity of individuals is found to be positively correlated\nwith their economic outputs. We also find that the economic outputs of leaf\nnodes are significantly lower than that of the other nodes in the same motif.\nOur findings shed light on understanding the influence of network structure on\neconomic activities and outputs in socioeconomic system. \n\n"}
{"id": "1706.05295", "contents": "Title: Nonbacktracking Bounds on the Influence in Independent Cascade Models Abstract: This paper develops upper and lower bounds on the influence measure in a\nnetwork, more precisely, the expected number of nodes that a seed set can\ninfluence in the independent cascade model. In particular, our bounds exploit\nnonbacktracking walks, Fortuin-Kasteleyn-Ginibre (FKG) type inequalities, and\nare computed by message passing implementation. Nonbacktracking walks have\nrecently allowed for headways in community detection, and this paper shows that\ntheir use can also impact the influence computation. Further, we provide a knob\nto control the trade-off between the efficiency and the accuracy of the bounds.\nFinally, the tightness of the bounds is illustrated with simulations on various\nnetwork models. \n\n"}
{"id": "1706.06066", "contents": "Title: On Quadratic Convergence of DC Proximal Newton Algorithm for Nonconvex\n  Sparse Learning in High Dimensions Abstract: We propose a DC proximal Newton algorithm for solving nonconvex regularized\nsparse learning problems in high dimensions. Our proposed algorithm integrates\nthe proximal Newton algorithm with multi-stage convex relaxation based on the\ndifference of convex (DC) programming, and enjoys both strong computational and\nstatistical guarantees. Specifically, by leveraging a sophisticated\ncharacterization of sparse modeling structures/assumptions (i.e., local\nrestricted strong convexity and Hessian smoothness), we prove that within each\nstage of convex relaxation, our proposed algorithm achieves (local) quadratic\nconvergence, and eventually obtains a sparse approximate local optimum with\noptimal statistical properties after only a few convex relaxations. Numerical\nexperiments are provided to support our theory. \n\n"}
{"id": "1706.06083", "contents": "Title: Towards Deep Learning Models Resistant to Adversarial Attacks Abstract: Recent work has demonstrated that deep neural networks are vulnerable to\nadversarial examples---inputs that are almost indistinguishable from natural\ndata and yet classified incorrectly by the network. In fact, some of the latest\nfindings suggest that the existence of adversarial attacks may be an inherent\nweakness of deep learning models. To address this problem, we study the\nadversarial robustness of neural networks through the lens of robust\noptimization. This approach provides us with a broad and unifying view on much\nof the prior work on this topic. Its principled nature also enables us to\nidentify methods for both training and attacking neural networks that are\nreliable and, in a certain sense, universal. In particular, they specify a\nconcrete security guarantee that would protect against any adversary. These\nmethods let us train networks with significantly improved resistance to a wide\nrange of adversarial attacks. They also suggest the notion of security against\na first-order adversary as a natural and broad security guarantee. We believe\nthat robustness against such well-defined classes of adversaries is an\nimportant stepping stone towards fully resistant deep learning models. Code and\npre-trained models are available at https://github.com/MadryLab/mnist_challenge\nand https://github.com/MadryLab/cifar10_challenge. \n\n"}
{"id": "1706.07881", "contents": "Title: On Sampling Strategies for Neural Network-based Collaborative Filtering Abstract: Recent advances in neural networks have inspired people to design hybrid\nrecommendation algorithms that can incorporate both (1) user-item interaction\ninformation and (2) content information including image, audio, and text.\nDespite their promising results, neural network-based recommendation algorithms\npose extensive computational costs, making it challenging to scale and improve\nupon. In this paper, we propose a general neural network-based recommendation\nframework, which subsumes several existing state-of-the-art recommendation\nalgorithms, and address the efficiency issue by investigating sampling\nstrategies in the stochastic gradient descent training for the framework. We\ntackle this issue by first establishing a connection between the loss functions\nand the user-item interaction bipartite graph, where the loss function terms\nare defined on links while major computation burdens are located at nodes. We\ncall this type of loss functions \"graph-based\" loss functions, for which varied\nmini-batch sampling strategies can have different computational costs. Based on\nthe insight, three novel sampling strategies are proposed, which can\nsignificantly improve the training efficiency of the proposed framework (up to\n$\\times 30$ times speedup in our experiments), as well as improving the\nrecommendation performance. Theoretical analysis is also provided for both the\ncomputational cost and the convergence. We believe the study of sampling\nstrategies have further implications on general graph-based loss functions, and\nwould also enable more research under the neural network-based recommendation\nframework. \n\n"}
{"id": "1706.07911", "contents": "Title: Large-Scale Mapping of Human Activity using Geo-Tagged Videos Abstract: This paper is the first work to perform spatio-temporal mapping of human\nactivity using the visual content of geo-tagged videos. We utilize a recent\ndeep-learning based video analysis framework, termed hidden two-stream\nnetworks, to recognize a range of activities in YouTube videos. This framework\nis efficient and can run in real time or faster which is important for\nrecognizing events as they occur in streaming video or for reducing latency in\nanalyzing already captured video. This is, in turn, important for using video\nin smart-city applications. We perform a series of experiments to show our\napproach is able to accurately map activities both spatially and temporally. We\nalso demonstrate the advantages of using the visual content over the\ntags/titles. \n\n"}
{"id": "1706.09795", "contents": "Title: Feature uncertainty bounding schemes for large robust nonlinear SVM\n  classifiers Abstract: We consider the binary classification problem when data are large and subject\nto unknown but bounded uncertainties. We address the problem by formulating the\nnonlinear support vector machine training problem with robust optimization. To\ndo so, we analyze and propose two bounding schemes for uncertainties associated\nto random approximate features in low dimensional spaces. The proposed\ntechniques are based on Random Fourier Features and the Nystr\\\"om methods. The\nresulting formulations can be solved with efficient stochastic approximation\ntechniques such as stochastic (sub)-gradient, stochastic proximal gradient\ntechniques or their variants. \n\n"}
{"id": "1707.01596", "contents": "Title: Topology Estimation in Bulk Power Grids: Guarantees on Exact Recovery Abstract: The topology of a power grid affects its dynamic operation and settlement in\nthe electricity market. Real-time topology identification can enable faster\ncontrol action following an emergency scenario like failure of a line. This\narticle discusses a graphical model framework for topology estimation in bulk\npower grids (both loopy transmission and radial distribution) using\nmeasurements of voltage collected from the grid nodes. The graphical model for\nthe probability distribution of nodal voltages in linear power flow models is\nshown to include additional edges along with the operational edges in the true\ngrid. Our proposed estimation algorithms first learn the graphical model and\nsubsequently extract the operational edges using either thresholding or a\nneighborhood counting scheme. For grid topologies containing no three-node\ncycles (two buses do not share a common neighbor), we prove that an exact\nextraction of the operational topology is theoretically guaranteed. This\nincludes a majority of distribution grids that have radial topologies. For\ngrids that include cycles of length three, we provide sufficient conditions\nthat ensure existence of algorithms for exact reconstruction. In particular,\nfor grids with constant impedance per unit length and uniform injection\ncovariances, this observation leads to conditions on geographical placement of\nthe buses. The performance of algorithms is demonstrated in test case\nsimulations. \n\n"}
{"id": "1707.03134", "contents": "Title: Least Square Variational Bayesian Autoencoder with Regularization Abstract: In recent years Variation Autoencoders have become one of the most popular\nunsupervised learning of complicated distributions.Variational Autoencoder\n(VAE) provides more efficient reconstructive performance over a traditional\nautoencoder. Variational auto enocders make better approximaiton than MCMC. The\nVAE defines a generative process in terms of ancestral sampling through a\ncascade of hidden stochastic layers. They are a directed graphic models.\nVariational autoencoder is trained to maximise the variational lower bound.\nHere we are trying maximise the likelihood and also at the same time we are\ntrying to make a good approximation of the data. Its basically trading of the\ndata log-likelihood and the KL divergence from the true posterior. This paper\ndescribes the scenario in which we wish to find a point-estimate to the\nparameters $\\theta$ of some parametric model in which we generate each\nobservations by first sampling a local latent variable and then sampling the\nassociated observation. Here we use least square loss function with\nregularization in the the reconstruction of the image, the least square loss\nfunction was found to give better reconstructed images and had a faster\ntraining time. \n\n"}
{"id": "1707.03186", "contents": "Title: Community Discovery in Dynamic Networks: a Survey Abstract: Networks built to model real world phenomena are characeterised by some\nproperties that have attracted the attention of the scientific community: (i)\nthey are organised according to community structure and (ii) their structure\nevolves with time. Many researchers have worked on methods that can efficiently\nunveil substructures in complex networks, giving birth to the field of\ncommunity discovery. A novel and challenging problem started capturing\nresearcher interest recently: the identification of evolving communities. To\nmodel the evolution of a system, dynamic networks can be used: nodes and edges\nare mutable and their presence, or absence, deeply impacts the community\nstructure that composes them. The aim of this survey is to present the\ndistinctive features and challenges of dynamic community discovery, and propose\na classification of published approaches. As a \"user manual\", this work\norganizes state of art methodologies into a taxonomy, based on their rationale,\nand their specific instanciation. Given a desired definition of network\ndynamics, community characteristics and analytical needs, this survey will\nsupport researchers to identify the set of approaches that best fit their\nneeds. The proposed classification could also help researchers to choose in\nwhich direction should future research be oriented. \n\n"}
{"id": "1707.04175", "contents": "Title: Distral: Robust Multitask Reinforcement Learning Abstract: Most deep reinforcement learning algorithms are data inefficient in complex\nand rich environments, limiting their applicability to many scenarios. One\ndirection for improving data efficiency is multitask learning with shared\nneural network parameters, where efficiency may be improved through transfer\nacross related tasks. In practice, however, this is not usually observed,\nbecause gradients from different tasks can interfere negatively, making\nlearning unstable and sometimes even less data efficient. Another issue is the\ndifferent reward schemes between tasks, which can easily lead to one task\ndominating the learning of a shared model. We propose a new approach for joint\ntraining of multiple tasks, which we refer to as Distral (Distill & transfer\nlearning). Instead of sharing parameters between the different workers, we\npropose to share a \"distilled\" policy that captures common behaviour across\ntasks. Each worker is trained to solve its own task while constrained to stay\nclose to the shared policy, while the shared policy is trained by distillation\nto be the centroid of all task policies. Both aspects of the learning process\nare derived by optimizing a joint objective function. We show that our approach\nsupports efficient transfer on complex 3D environments, outperforming several\nrelated methods. Moreover, the proposed learning process is more robust and\nmore stable---attributes that are critical in deep reinforcement learning. \n\n"}
{"id": "1707.04210", "contents": "Title: UrbanFACET: Visually Profiling Cities from Mobile Device Recorded\n  Movement Data of Millions of City Residents Abstract: Cities are living systems where urban infrastructures and their functions are\ndefined and evolved due to population behaviors. Profiling the cities and\nfunctional regions has been an important topic in urban design and planning.\nThis paper studies a unique big data set which includes daily movement data of\ntens of millions of city residents, and develop a visual analytics system,\nnamely UrbanFACET, to discover and visualize the dynamical profiles of multiple\ncities and their residents. This big user movement data set, acquired from\nmobile users' agnostic check-ins at thousands of phone APPs, is well utilized\nin an integrative study and visualization together with urban structure (e.g.,\nroad network) and POI (Point of Interest) distributions. In particular, we\nnovelly develop a set of information-theory based metrics to characterize the\nmobility patterns of city areas and groups of residents. These multifaceted\nmetrics including Fluidity, vibrAncy, Commutation, divErsity, and densiTy\n(FACET) which categorize and manifest hidden urban functions and behaviors.\nUrbanFACET system further allows users to visually analyze and compare the\nmetrics over different areas and cities in metropolitan scales. The system is\nevaluated through both case studies on several big and heavily populated\ncities, and user studies involving real-world users. \n\n"}
{"id": "1707.05587", "contents": "Title: Graph learning under sparsity priors Abstract: Graph signals offer a very generic and natural representation for data that\nlives on networks or irregular structures. The actual data structure is however\noften unknown a priori but can sometimes be estimated from the knowledge of the\napplication domain. If this is not possible, the data structure has to be\ninferred from the mere signal observations. This is exactly the problem that we\naddress in this paper, under the assumption that the graph signals can be\nrepresented as a sparse linear combination of a few atoms of a structured graph\ndictionary. The dictionary is constructed on polynomials of the graph\nLaplacian, which can sparsely represent a general class of graph signals\ncomposed of localized patterns on the graph. We formulate a graph learning\nproblem, whose solution provides an ideal fit between the signal observations\nand the sparse graph signal model. As the problem is non-convex, we propose to\nsolve it by alternating between a signal sparse coding and a graph update step.\nWe provide experimental results that outline the good graph recovery\nperformance of our method, which generally compares favourably to other recent\nnetwork inference algorithms. \n\n"}
{"id": "1707.06525", "contents": "Title: Topical alignment in online social systems Abstract: Understanding the dynamics of social interactions is crucial to comprehend\nhuman behavior. The emergence of online social media has enabled access to data\nregarding people relationships at a large scale. Twitter, specifically, is an\ninformation oriented network, with users sharing and consuming information. In\nthis work, we study whether users tend to be in contact with people interested\nin similar topics, i.e., if they are topically aligned. To do so, we propose an\napproach based on the use of hashtags to extract information topics from\nTwitter messages and model users' interests. Our results show that, on average,\nusers are connected with other users similar to them. Furthermore, we show that\ntopical alignment provides interesting information that can eventually allow\ninferring users' connectivity. Our work, besides providing a way to assess the\ntopical similarity of users, quantifies topical alignment among individuals,\ncontributing to a better understanding of how complex social systems are\nstructured. \n\n"}
{"id": "1707.06618", "contents": "Title: Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex\n  Optimization Abstract: We present a unified framework to analyze the global convergence of Langevin\ndynamics based algorithms for nonconvex finite-sum optimization with $n$\ncomponent functions. At the core of our analysis is a direct analysis of the\nergodicity of the numerical approximations to Langevin dynamics, which leads to\nfaster convergence rates. Specifically, we show that gradient Langevin dynamics\n(GLD) and stochastic gradient Langevin dynamics (SGLD) converge to the almost\nminimizer within $\\tilde O\\big(nd/(\\lambda\\epsilon) \\big)$ and $\\tilde\nO\\big(d^7/(\\lambda^5\\epsilon^5) \\big)$ stochastic gradient evaluations\nrespectively, where $d$ is the problem dimension, and $\\lambda$ is the spectral\ngap of the Markov chain generated by GLD. Both results improve upon the best\nknown gradient complexity results (Raginsky et al., 2017). Furthermore, for the\nfirst time we prove the global convergence guarantee for variance reduced\nstochastic gradient Langevin dynamics (SVRG-LD) to the almost minimizer within\n$\\tilde O\\big(\\sqrt{n}d^5/(\\lambda^4\\epsilon^{5/2})\\big)$ stochastic gradient\nevaluations, which outperforms the gradient complexities of GLD and SGLD in a\nwide regime. Our theoretical analyses shed some light on using Langevin\ndynamics based algorithms for nonconvex optimization with provable guarantees. \n\n"}
{"id": "1707.07494", "contents": "Title: Data clustering using stochastic block models Abstract: It has been shown that community detection algorithms work better for\nclustering tasks than other, more popular methods, such as k-means. In fact,\nnetwork analysis based methods often outperform more widely used methods and do\nnot suffer from some of the drawbacks we notice elsewhere e.g. the number of\nclusters k usually has to be known in advance. However, stochastic block models\nwhich are known to perform well for community detection, have not yet been\ntested for this task. We discuss why these models cannot be directly applied to\nthis problem and test the performance of a generalization of stochastic block\nmodels which work on weighted graphs and compare them to other clustering\ntechniques. \n\n"}
{"id": "1707.08418", "contents": "Title: The Advantage of Evidential Attributes in Social Networks Abstract: Nowadays, there are many approaches designed for the task of detecting\ncommunities in social networks. Among them, some methods only consider the\ntopological graph structure, while others take use of both the graph structure\nand the node attributes. In real-world networks, there are many uncertain and\nnoisy attributes in the graph. In this paper, we will present how we detect\ncommunities in graphs with uncertain attributes in the first step. The\nnumerical, probabilistic as well as evidential attributes are generated\naccording to the graph structure. In the second step, some noise will be added\nto the attributes. We perform experiments on graphs with different types of\nattributes and compare the detection results in terms of the Normalized Mutual\nInformation (NMI) values. The experimental results show that the clustering\nwith evidential attributes gives better results comparing to those with\nprobabilistic and numerical attributes. This illustrates the advantages of\nevidential attributes. \n\n"}
{"id": "1708.01292", "contents": "Title: What your Facebook Profile Picture Reveals about your Personality Abstract: People spend considerable effort managing the impressions they give others.\nSocial psychologists have shown that people manage these impressions\ndifferently depending upon their personality. Facebook and other social media\nprovide a new forum for this fundamental process; hence, understanding people's\nbehaviour on social media could provide interesting insights on their\npersonality. In this paper we investigate automatic personality recognition\nfrom Facebook profile pictures. We analyze the effectiveness of four families\nof visual features and we discuss some human interpretable patterns that\nexplain the personality traits of the individuals. For example, extroverts and\nagreeable individuals tend to have warm colored pictures and to exhibit many\nfaces in their portraits, mirroring their inclination to socialize; while\nneurotic ones have a prevalence of pictures of indoor places. Then, we propose\na classification approach to automatically recognize personality traits from\nthese visual features. Finally, we compare the performance of our\nclassification approach to the one obtained by human raters and we show that\ncomputer-based classifications are significantly more accurate than averaged\nhuman-based classifications for Extraversion and Neuroticism. \n\n"}
{"id": "1708.03020", "contents": "Title: Non-stationary Stochastic Optimization under $L_{p,q}$-Variation\n  Measures Abstract: We consider a non-stationary sequential stochastic optimization problem, in\nwhich the underlying cost functions change over time under a variation budget\nconstraint. We propose an $L_{p,q}$-variation functional to quantify the\nchange, which yields less variation for dynamic function sequences whose\nchanges are constrained to short time periods or small subsets of input domain.\nUnder the $L_{p,q}$-variation constraint, we derive both upper and matching\nlower regret bounds for smooth and strongly convex function sequences, which\ngeneralize previous results in Besbes et al. (2015). Furthermore, we provide an\nupper bound for general convex function sequences with noisy gradient feedback,\nwhich matches the optimal rate as $p\\to\\infty$. Our results reveal some\nsurprising phenomena under this general variation functional, such as the curse\nof dimensionality of the function domain. The key technical novelties in our\nanalysis include affinity lemmas that characterize the distance of the\nminimizers of two convex functions with bounded Lp difference, and a cubic\nspline based construction that attains matching lower bounds. \n\n"}
{"id": "1708.03731", "contents": "Title: OpenML Benchmarking Suites Abstract: Machine learning research depends on objectively interpretable, comparable,\nand reproducible algorithm benchmarks. We advocate the use of curated,\ncomprehensive suites of machine learning tasks to standardize the setup,\nexecution, and reporting of benchmarks. We enable this through software tools\nthat help to create and leverage these benchmarking suites. These are\nseamlessly integrated into the OpenML platform, and accessible through\ninterfaces in Python, Java, and R. OpenML benchmarking suites (a) are easy to\nuse through standardized data formats, APIs, and client libraries; (b) come\nwith extensive meta-information on the included datasets; and (c) allow\nbenchmarks to be shared and reused in future studies. We then present a first,\ncarefully curated and practical benchmarking suite for classification: the\nOpenML Curated Classification benchmarking suite 2018 (OpenML-CC18). Finally,\nwe discuss use cases and applications which demonstrate the usefulness of\nOpenML benchmarking suites and the OpenML-CC18 in particular. \n\n"}
{"id": "1708.04999", "contents": "Title: Generalized least squares can overcome the critical threshold in\n  respondent-driven sampling Abstract: In order to sample marginalized and/or hard-to-reach populations,\nrespondent-driven sampling (RDS) and similar techniques reach their\nparticipants via peer referral. Under a Markov model for RDS, previous research\nhas shown that if the typical participant refers too many contacts, then the\nvariance of common estimators does not decay like $O(n^{-1})$, where $n$ is the\nsample size. This implies that confidence intervals will be far wider than\nunder a typical sampling design. Here we show that generalized least squares\n(GLS) can effectively reduce the variance of RDS estimates. In particular, a\ntheoretical analysis indicates that the variance of the GLS estimator is\n$O(n^{-1})$. We then derive two classes of feasible GLS estimators. The first\nclass is based upon a Degree Corrected Stochastic Blockmodel for the underlying\nsocial network. The second class is based upon a rank-two model. It might be of\nindependent interest that in both model classes, the theoretical results show\nthat it is possible to estimate the spectral properties of the population\nnetwork from the sampled observations. Simulations on empirical social networks\nshow that the feasible GLS (fGLS) estimators can have drastically smaller error\nand rarely increase the error. A diagnostic plot helps to identify where fGLS\nwill aid estimation. The fGLS estimators continue to outperform standard\nestimators even when they are built from a misspecified model and when there is\npreferential recruitment. \n\n"}
{"id": "1708.05329", "contents": "Title: Network Inference from Consensus Dynamics Abstract: We consider the problem of identifying the topology of a weighted, undirected\nnetwork $\\mathcal G$ from observing snapshots of multiple independent consensus\ndynamics. Specifically, we observe the opinion profiles of a group of agents\nfor a set of $M$ independent topics and our goal is to recover the precise\nrelationships between the agents, as specified by the unknown network $\\mathcal\nG$. In order to overcome the under-determinacy of the problem at hand, we\nleverage concepts from spectral graph theory and convex optimization to unveil\nthe underlying network structure. More precisely, we formulate the network\ninference problem as a convex optimization that seeks to endow the network with\ncertain desired properties -- such as sparsity -- while being consistent with\nthe spectral information extracted from the observed opinions. This is\ncomplemented with theoretical results proving consistency as the number $M$ of\ntopics grows large. We further illustrate our method by numerical experiments,\nwhich showcase the effectiveness of the technique in recovering synthetic and\nreal-world networks. \n\n"}
{"id": "1708.05789", "contents": "Title: Semi-supervised Conditional GANs Abstract: We introduce a new model for building conditional generative models in a\nsemi-supervised setting to conditionally generate data given attributes by\nadapting the GAN framework. The proposed semi-supervised GAN (SS-GAN) model\nuses a pair of stacked discriminators to learn the marginal distribution of the\ndata, and the conditional distribution of the attributes given the data\nrespectively. In the semi-supervised setting, the marginal distribution (which\nis often harder to learn) is learned from the labeled + unlabeled data, and the\nconditional distribution is learned purely from the labeled data. Our\nexperimental results demonstrate that this model performs significantly better\ncompared to existing semi-supervised conditional GAN models. \n\n"}
{"id": "1709.00537", "contents": "Title: Communication-efficient Algorithm for Distributed Sparse Learning via\n  Two-way Truncation Abstract: We propose a communicationally and computationally efficient algorithm for\nhigh-dimensional distributed sparse learning. At each iteration, local machines\ncompute the gradient on local data and the master machine solves one shifted\n$l_1$ regularized minimization problem. The communication cost is reduced from\nconstant times of the dimension number for the state-of-the-art algorithm to\nconstant times of the sparsity number via Two-way Truncation procedure.\nTheoretically, we prove that the estimation error of the proposed algorithm\ndecreases exponentially and matches that of the centralized method under mild\nassumptions. Extensive experiments on both simulated data and real data verify\nthat the proposed algorithm is efficient and has performance comparable with\nthe centralized method on solving high-dimensional sparse learning problems. \n\n"}
{"id": "1709.02909", "contents": "Title: A Simple Analysis for Exp-concave Empirical Minimization with Arbitrary\n  Convex Regularizer Abstract: In this paper, we present a simple analysis of {\\bf fast rates} with {\\it\nhigh probability} of {\\bf empirical minimization} for {\\it stochastic composite\noptimization} over a finite-dimensional bounded convex set with exponential\nconcave loss functions and an arbitrary convex regularization. To the best of\nour knowledge, this result is the first of its kind. As a byproduct, we can\ndirectly obtain the fast rate with {\\it high probability} for exponential\nconcave empirical risk minimization with and without any convex regularization,\nwhich not only extends existing results of empirical risk minimization but also\nprovides a unified framework for analyzing exponential concave empirical risk\nminimization with and without {\\it any} convex regularization. Our proof is\nvery simple only exploiting the covering number of a finite-dimensional bounded\nset and a concentration inequality of random vectors. \n\n"}
{"id": "1709.03545", "contents": "Title: Learning Graph Topological Features via GAN Abstract: Inspired by the generation power of generative adversarial networks (GANs) in\nimage domains, we introduce a novel hierarchical architecture for learning\ncharacteristic topological features from a single arbitrary input graph via\nGANs. The hierarchical architecture consisting of multiple GANs preserves both\nlocal and global topological features and automatically partitions the input\ngraph into representative stages for feature learning. The stages facilitate\nreconstruction and can be used as indicators of the importance of the\nassociated topological structures. Experiments show that our method produces\nsubgraphs retaining a wide range of topological features, even in early\nreconstruction stages (unlike a single GAN, which cannot easily identify such\nfeatures, let alone reconstruct the original graph). This paper is firstline\nresearch on combining the use of GANs and graph topological analysis. \n\n"}
{"id": "1709.04594", "contents": "Title: Revisiting Spectral Graph Clustering with Generative Community Models Abstract: The methodology of community detection can be divided into two principles:\nimposing a network model on a given graph, or optimizing a designed objective\nfunction. The former provides guarantees on theoretical detectability but falls\nshort when the graph is inconsistent with the underlying model. The latter is\nmodel-free but fails to provide quality assurance for the detected communities.\nIn this paper, we propose a novel unified framework to combine the advantages\nof these two principles. The presented method, SGC-GEN, not only considers the\ndetection error caused by the corresponding model mismatch to a given graph,\nbut also yields a theoretical guarantee on community detectability by analyzing\nSpectral Graph Clustering (SGC) under GENerative community models (GCMs).\nSGC-GEN incorporates the predictability on correct community detection with a\nmeasure of community fitness to GCMs. It resembles the formulation of\nsupervised learning problems by enabling various community detection loss\nfunctions and model mismatch metrics. We further establish a theoretical\ncondition for correct community detection using the normalized graph Laplacian\nmatrix under a GCM, which provides a novel data-driven loss function for\nSGC-GEN. In addition, we present an effective algorithm to implement SGC-GEN,\nand show that the computational complexity of SGC-GEN is comparable to the\nbaseline methods. Our experiments on 18 real-world datasets demonstrate that\nSGC-GEN possesses superior and robust performance compared to 6 baseline\nmethods under 7 representative clustering metrics. \n\n"}
{"id": "1709.05132", "contents": "Title: On the stability of network indices defined by means of matrix functions Abstract: Identifying important components in a network is one of the major goals of\nnetwork analysis. Popular and effective measures of importance of a node or a\nset of nodes are defined in terms of suitable entries of functions of matrices\n$f(A)$. These kinds of measures are particularly relevant as they are able to\ncapture the global structure of connections involving a node. However,\ncomputing the entries of $f(A)$ requires a significant computational effort. In\nthis work we address the problem of estimating the changes in the entries of\n$f(A)$ with respect to changes in the edge structure. Intuition suggests that,\nif the topology of connections in the new graph $\\tilde G$ is not significantly\ndistorted, relevant components in $G$ maintain their leading role in $\\tilde\nG$. We propose several bounds giving mathematical reasoning to such intuition\nand showing, in particular, that the magnitude of the variation of the entry\n$f(A)_{k\\ell}$ decays exponentially with the shortest-path distance in $G$ that\nseparates either $k$ or $\\ell$ from the set of nodes touched by the edges that\nare perturbed. Moreover, we propose a simple method that exploits the\ncomputation of $f(A)$ to simultaneously compute the all-pairs shortest-path\ndistances of $G$, with essentially no additional cost. As the nodes whose edge\nconnection tends to change more often or tends to be more often affected by\nnoise have marginal role in the graph and are distant from the most central\nnodes, the proposed bounds are particularly relevant. \n\n"}
{"id": "1709.06304", "contents": "Title: Scalable Estimation of Dirichlet Process Mixture Models on Distributed\n  Data Abstract: We consider the estimation of Dirichlet Process Mixture Models (DPMMs) in\ndistributed environments, where data are distributed across multiple computing\nnodes. A key advantage of Bayesian nonparametric models such as DPMMs is that\nthey allow new components to be introduced on the fly as needed. This, however,\nposts an important challenge to distributed estimation -- how to handle new\ncomponents efficiently and consistently. To tackle this problem, we propose a\nnew estimation method, which allows new components to be created locally in\nindividual computing nodes. Components corresponding to the same cluster will\nbe identified and merged via a probabilistic consolidation scheme. In this way,\nwe can maintain the consistency of estimation with very low communication cost.\nExperiments on large real-world data sets show that the proposed method can\nachieve high scalability in distributed and asynchronous environments without\ncompromising the mixing performance. \n\n"}
{"id": "1709.06680", "contents": "Title: Deep Lattice Networks and Partial Monotonic Functions Abstract: We propose learning deep models that are monotonic with respect to a\nuser-specified set of inputs by alternating layers of linear embeddings,\nensembles of lattices, and calibrators (piecewise linear functions), with\nappropriate constraints for monotonicity, and jointly training the resulting\nnetwork. We implement the layers and projections with new computational graph\nnodes in TensorFlow and use the ADAM optimizer and batched stochastic\ngradients. Experiments on benchmark and real-world datasets show that six-layer\nmonotonic deep lattice networks achieve state-of-the art performance for\nclassification and regression with monotonicity guarantees. \n\n"}
{"id": "1709.06740", "contents": "Title: Discovery of the Twitter Bursty Botnet Abstract: Many Twitter users are bots. They can be used for spamming, opinion\nmanipulation and online fraud. Recently we discovered the Star Wars botnet,\nconsisting of more than 350,000 bots tweeting random quotations exclusively\nfrom Star Wars novels. The bots were exposed because they tweeted uniformly\nfrom any location within two rectangle-shaped geographic zones covering Europe\nand the USA, including sea and desert areas in the zones. In this paper, we\nreport another unusual behaviour of the Star Wars bots, that the bots were\ncreated in bursts or batches, and they only tweeted in their first few minutes\nsince creation. Inspired by this observation, we discovered an even larger\nTwitter botnet, the Bursty botnet with more than 500,000 bots. Our preliminary\nstudy showed that the Bursty botnet was directly responsible for a large-scale\nonline spamming attack in 2012. Most bot detection algorithms have been based\non assumptions of `common' features that were supposedly shared by all bots.\nOur discovered botnets, however, do not show many of those features; instead,\nthey were detected by their distinct, unusual tweeting behaviours that were\nunknown until now. \n\n"}
{"id": "1709.07625", "contents": "Title: Total stability of kernel methods Abstract: Regularized empirical risk minimization using kernels and their corresponding\nreproducing kernel Hilbert spaces (RKHSs) plays an important role in machine\nlearning. However, the actually used kernel often depends on one or on a few\nhyperparameters or the kernel is even data dependent in a much more complicated\nmanner. Examples are Gaussian RBF kernels, kernel learning, and hierarchical\nGaussian kernels which were recently proposed for deep learning. Therefore, the\nactually used kernel is often computed by a grid search or in an iterative\nmanner and can often only be considered as an approximation to the \"ideal\" or\n\"optimal\" kernel. The paper gives conditions under which classical kernel based\nmethods based on a convex Lipschitz loss function and on a bounded and smooth\nkernel are stable, if the probability measure $P$, the regularization parameter\n$\\lambda$, and the kernel $k$ may slightly change in a simultaneous manner.\nSimilar results are also given for pairwise learning. Therefore, the topic of\nthis paper is somewhat more general than in classical robust statistics, where\nusually only the influence of small perturbations of the probability measure\n$P$ on the estimated function is considered. \n\n"}
{"id": "1709.10323", "contents": "Title: A Nonlinear Orthogonal Non-Negative Matrix Factorization Approach to\n  Subspace Clustering Abstract: A recent theoretical analysis shows the equivalence between non-negative\nmatrix factorization (NMF) and spectral clustering based approach to subspace\nclustering. As NMF and many of its variants are essentially linear, we\nintroduce a nonlinear NMF with explicit orthogonality and derive general\nkernel-based orthogonal multiplicative update rules to solve the subspace\nclustering problem. In nonlinear orthogonal NMF framework, we propose two\nsubspace clustering algorithms, named kernel-based non-negative subspace\nclustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral\nnormalized cut and ratio cut clustering. We further extend the nonlinear\northogonal NMF framework and introduce a graph regularization to obtain a\nfactorization that respects a local geometric structure of the data after the\nnonlinear mapping. The proposed NMF-based approach to subspace clustering takes\ninto account the nonlinear nature of the manifold, as well as its intrinsic\nlocal geometry, which considerably improves the clustering performance when\ncompared to the several recently proposed state-of-the-art methods. \n\n"}
{"id": "1710.00818", "contents": "Title: Continuous-Time Relationship Prediction in Dynamic Heterogeneous\n  Information Networks Abstract: Online social networks, World Wide Web, media and technological networks, and\nother types of so-called information networks are ubiquitous nowadays. These\ninformation networks are inherently heterogeneous and dynamic. They are\nheterogeneous as they consist of multi-typed objects and relations, and they\nare dynamic as they are constantly evolving over time. One of the challenging\nissues in such heterogeneous and dynamic environments is to forecast those\nrelationships in the network that will appear in the future. In this paper, we\ntry to solve the problem of continuous-time relationship prediction in dynamic\nand heterogeneous information networks. This implies predicting the time it\ntakes for a relationship to appear in the future, given its features that have\nbeen extracted by considering both heterogeneity and temporal dynamics of the\nunderlying network. To this end, we first introduce a feature extraction\nframework that combines the power of meta-path-based modeling and recurrent\nneural networks to effectively extract features suitable for relationship\nprediction regarding heterogeneity and dynamicity of the networks. Next, we\npropose a supervised non-parametric approach, called Non-Parametric Generalized\nLinear Model (NP-GLM), which infers the hidden underlying probability\ndistribution of the relationship building time given its features. We then\npresent a learning algorithm to train NP-GLM and an inference method to answer\ntime-related queries. Extensive experiments conducted on synthetic data and\nthree real-world datasets, namely Delicious, MovieLens, and DBLP, demonstrate\nthe effectiveness of NP-GLM in solving continuous-time relationship prediction\nproblem vis-a-vis competitive baselines \n\n"}
{"id": "1710.00978", "contents": "Title: Supervised Q-walk for Learning Vector Representation of Nodes in\n  Networks Abstract: Automatic feature learning algorithms are at the forefront of modern day\nmachine learning research. We present a novel algorithm, supervised Q-walk,\nwhich applies Q-learning to generate random walks on graphs such that the walks\nprove to be useful for learning node features suitable for tackling with the\nnode classification problem. We present another novel algorithm, k-hops\nneighborhood based confidence values learner, which learns confidence values of\nlabels for unlabelled nodes in the network without first learning the node\nembedding. These confidence values aid in learning an apt reward function for\nQ-learning.\n  We demonstrate the efficacy of supervised Q-walk approach over existing\nstate-of-the-art random walk based node embedding learners in solving the\nsingle / multi-label multi-class node classification problem using several real\nworld datasets.\n  Summarising, our approach represents a novel state-of-the-art technique to\nlearn features, for nodes in networks, tailor-made for dealing with the node\nclassification problem. \n\n"}
{"id": "1710.04008", "contents": "Title: A Dynamic Edge Exchangeable Model for Sparse Temporal Networks Abstract: We propose a dynamic edge exchangeable network model that can capture sparse\nconnections observed in real temporal networks, in contrast to existing models\nwhich are dense. The model achieved superior link prediction accuracy on\nmultiple data sets when compared to a dynamic variant of the blockmodel, and is\nable to extract interpretable time-varying community structures from the data.\nIn addition to sparsity, the model accounts for the effect of social influence\non vertices' future behaviours. Compared to the dynamic blockmodels, our model\nhas a smaller latent space. The compact latent space requires a smaller number\nof parameters to be estimated in variational inference and results in a\ncomputationally friendly inference algorithm. \n\n"}
{"id": "1710.04725", "contents": "Title: Hyperparameter Importance Across Datasets Abstract: With the advent of automated machine learning, automated hyperparameter\noptimization methods are by now routinely used in data mining. However, this\nprogress is not yet matched by equal progress on automatic analyses that yield\ninformation beyond performance-optimizing hyperparameter settings. In this\nwork, we aim to answer the following two questions: Given an algorithm, what\nare generally its most important hyperparameters, and what are typically good\nvalues for these? We present methodology and a framework to answer these\nquestions based on meta-learning across many datasets. We apply this\nmethodology using the experimental meta-data available on OpenML to determine\nthe most important hyperparameters of support vector machines, random forests\nand Adaboost, and to infer priors for all their hyperparameters. The results,\nobtained fully automatically, provide a quantitative basis to focus efforts in\nboth manual algorithm design and in automated hyperparameter optimization. The\nconducted experiments confirm that the hyperparameters selected by the proposed\nmethod are indeed the most important ones and that the obtained priors also\nlead to statistically significant improvements in hyperparameter optimization. \n\n"}
{"id": "1710.05741", "contents": "Title: A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised\n  Learning Abstract: This paper takes a step towards temporal reasoning in a dynamically changing\nvideo, not in the pixel space that constitutes its frames, but in a latent\nspace that describes the non-linear dynamics of the objects in its world. We\nintroduce the Kalman variational auto-encoder, a framework for unsupervised\nlearning of sequential data that disentangles two latent representations: an\nobject's representation, coming from a recognition model, and a latent state\ndescribing its dynamics. As a result, the evolution of the world can be\nimagined and missing data imputed, both without the need to generate high\ndimensional frames at each time step. The model is trained end-to-end on videos\nof a variety of simulated physical systems, and outperforms competing methods\nin generative and missing data imputation tasks. \n\n"}
{"id": "1710.06100", "contents": "Title: Primal-Dual $\\pi$ Learning: Sample Complexity and Sublinear Run Time for\n  Ergodic Markov Decision Problems Abstract: Consider the problem of approximating the optimal policy of a Markov decision\nprocess (MDP) by sampling state transitions. In contrast to existing\nreinforcement learning methods that are based on successive approximations to\nthe nonlinear Bellman equation, we propose a Primal-Dual $\\pi$ Learning method\nin light of the linear duality between the value and policy. The $\\pi$ learning\nmethod is model-free and makes primal-dual updates to the policy and value\nvectors as new data are revealed. For infinite-horizon undiscounted Markov\ndecision process with finite state space $S$ and finite action space $A$, the\n$\\pi$ learning method finds an $\\epsilon$-optimal policy using the following\nnumber of sample transitions $$ \\tilde{O}( \\frac{(\\tau\\cdot t^*_{mix})^2 |S|\n|A| }{\\epsilon^2} ),$$ where $t^*_{mix}$ is an upper bound of mixing times\nacross all policies and $\\tau$ is a parameter characterizing the range of\nstationary distributions across policies. The $\\pi$ learning method also\napplies to the computational problem of MDP where the transition probabilities\nand rewards are explicitly given as the input. In the case where each state\ntransition can be sampled in $\\tilde{O}(1)$ time, the $\\pi$ learning method\ngives a sublinear-time algorithm for solving the averaged-reward MDP. \n\n"}
{"id": "1710.06910", "contents": "Title: Characterization of Gradient Dominance and Regularity Conditions for\n  Neural Networks Abstract: The past decade has witnessed a successful application of deep learning to\nsolving many challenging problems in machine learning and artificial\nintelligence. However, the loss functions of deep neural networks (especially\nnonlinear networks) are still far from being well understood from a theoretical\naspect. In this paper, we enrich the current understanding of the landscape of\nthe square loss functions for three types of neural networks. Specifically,\nwhen the parameter matrices are square, we provide an explicit characterization\nof the global minimizers for linear networks, linear residual networks, and\nnonlinear networks with one hidden layer. Then, we establish two quadratic\ntypes of landscape properties for the square loss of these neural networks,\ni.e., the gradient dominance condition within the neighborhood of their full\nrank global minimizers, and the regularity condition along certain directions\nand within the neighborhood of their global minimizers. These two landscape\nproperties are desirable for the optimization around the global minimizers of\nthe loss function for these neural networks. \n\n"}
{"id": "1710.07236", "contents": "Title: Signed Node Relevance Measurements Abstract: In this paper, we perform the initial and comprehensive study on the problem\nof measuring node relevance on signed social networks. We design numerous\nrelevance measurements for signed social networks from both local and global\nperspectives and investigate the connection between signed relevance\nmeasurements, balance theory and signed network properties. Experimental\nresults are conducted to study the effects of signed relevance measurements\nwith four real-world datasets on signed network analysis tasks. \n\n"}
{"id": "1710.07406", "contents": "Title: First-order Methods Almost Always Avoid Saddle Points Abstract: We establish that first-order methods avoid saddle points for almost all\ninitializations. Our results apply to a wide variety of first-order methods,\nincluding gradient descent, block coordinate descent, mirror descent and\nvariants thereof. The connecting thread is that such algorithms can be studied\nfrom a dynamical systems perspective in which appropriate instantiations of the\nStable Manifold Theorem allow for a global stability analysis. Thus, neither\naccess to second-order derivative information nor randomness beyond\ninitialization is necessary to provably avoid saddle points. \n\n"}
{"id": "1710.07462", "contents": "Title: Tracking the gradients using the Hessian: A new look at variance\n  reducing stochastic methods Abstract: Our goal is to improve variance reducing stochastic methods through better\ncontrol variates. We first propose a modification of SVRG which uses the\nHessian to track gradients over time, rather than to recondition, increasing\nthe correlation of the control variates and leading to faster theoretical\nconvergence close to the optimum. We then propose accurate and computationally\nefficient approximations to the Hessian, both using a diagonal and a low-rank\nmatrix. Finally, we demonstrate the effectiveness of our method on a wide range\nof problems. \n\n"}
{"id": "1710.08446", "contents": "Title: Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At\n  Every Step Abstract: Generative adversarial networks (GANs) are a family of generative models that\ndo not minimize a single training criterion. Unlike other generative models,\nthe data distribution is learned via a game between a generator (the generative\nmodel) and a discriminator (a teacher providing training signal) that each\nminimize their own cost. GANs are designed to reach a Nash equilibrium at which\neach player cannot reduce their cost without changing the other players'\nparameters. One useful approach for the theory of GANs is to show that a\ndivergence between the training distribution and the model distribution obtains\nits minimum value at equilibrium. Several recent research directions have been\nmotivated by the idea that this divergence is the primary guide for the\nlearning process and that every step of learning should decrease the\ndivergence. We show that this view is overly restrictive. During GAN training,\nthe discriminator provides learning signal in situations where the gradients of\nthe divergences between distributions would not be useful. We provide empirical\ncounterexamples to the view of GAN training as divergence minimization.\nSpecifically, we demonstrate that GANs are able to learn distributions in\nsituations where the divergence minimization point of view predicts they would\nfail. We also show that gradient penalties motivated from the divergence\nminimization perspective are equally helpful when applied in other contexts in\nwhich the divergence minimization perspective does not predict they would be\nhelpful. This contributes to a growing body of evidence that GAN training may\nbe more usefully viewed as approaching Nash equilibria via trajectories that do\nnot necessarily minimize a specific divergence at each step. \n\n"}
{"id": "1710.08473", "contents": "Title: A Unified Framework for Long Range and Cold Start Forecasting of\n  Seasonal Profiles in Time Series Abstract: Providing long-range forecasts is a fundamental challenge in time series\nmodeling, which is only compounded by the challenge of having to form such\nforecasts when a time series has never previously been observed. The latter\nchallenge is the time series version of the cold-start problem seen in\nrecommender systems which, to our knowledge, has not been addressed in previous\nwork. A similar problem occurs when a long range forecast is required after\nonly observing a small number of time points --- a warm start forecast. With\nthese aims in mind, we focus on forecasting seasonal profiles---or baseline\ndemand---for periods on the order of a year in three cases: the long range case\nwith multiple previously observed seasonal profiles, the cold start case with\nno previous observed seasonal profiles, and the warm start case with only a\nsingle partially observed profile. Classical time series approaches that\nperform iterated step-ahead forecasts based on previous observations struggle\nto provide accurate long range predictions; in settings with little to no\nobserved data, such approaches are simply not applicable. Instead, we present a\nstraightforward framework which combines ideas from high-dimensional regression\nand matrix factorization on a carefully constructed data matrix. Key to our\nformulation and resulting performance is leveraging (1) repeated patterns over\nfixed periods of time and across series, and (2) metadata associated with the\nindividual series; without this additional data, the cold-start/warm-start\nproblems are nearly impossible to solve. We demonstrate that our framework can\naccurately forecast an array of seasonal profiles on multiple large scale\ndatasets. \n\n"}
{"id": "1710.08528", "contents": "Title: A Two-Level Classification Approach for Detecting Clickbait Posts using\n  Text-Based Features Abstract: The emergence of social media as news sources has led to the rise of\nclickbait posts attempting to attract users to click on article links without\ninforming them on the actual article content. This paper presents our efforts\nto create a clickbait detector inspired by fake news detection algorithms, and\nour submission to the Clickbait Challenge 2017. The detector is based almost\nexclusively on text-based features taken from previous work on clickbait\ndetection, our own work on fake post detection, and features we designed\nspecifically for the challenge. We use a two-level classification approach,\ncombining the outputs of 65 first-level classifiers in a second-level feature\nvector. We present our exploratory results with individual features and their\ncombinations, taken from the post text and the target article title, as well as\nfeature selection. While our own blind tests with the dataset led to an F-score\nof 0.63, our final evaluation in the Challenge only achieved an F-score of\n0.43. We explore the possible causes of this, and lay out potential future\nsteps to achieve more successful results. \n\n"}
{"id": "1710.08607", "contents": "Title: Provable and practical approximations for the degree distribution using\n  sublinear graph samples Abstract: The degree distribution is one of the most fundamental properties used in the\nanalysis of massive graphs. There is a large literature on graph sampling,\nwhere the goal is to estimate properties (especially the degree distribution)\nof a large graph through a small, random sample. The degree distribution\nestimation poses a significant challenge, due to its heavy-tailed nature and\nthe large variance in degrees.\n  We design a new algorithm, SADDLES, for this problem, using recent\nmathematical techniques from the field of sublinear algorithms. The SADDLES\nalgorithm gives provably accurate outputs for all values of the degree\ndistribution. For the analysis, we define two fatness measures of the degree\ndistribution, called the $h$-index and the $z$-index. We prove that SADDLES is\nsublinear in the graph size when these indices are large. A corollary of this\nresult is a provably sublinear algorithm for any degree distribution bounded\nbelow by a power law.\n  We deploy our new algorithm on a variety of real datasets and demonstrate its\nexcellent empirical behavior. In all instances, we get extremely accurate\napproximations for all values in the degree distribution by observing at most\n$1\\%$ of the vertices. This is a major improvement over the state-of-the-art\nsampling algorithms, which typically sample more than $10\\%$ of the vertices to\ngive comparable results. We also observe that the $h$ and $z$-indices of real\ngraphs are large, validating our theoretical analysis. \n\n"}
{"id": "1710.08619", "contents": "Title: Interpretable Deep Learning applied to Plant Stress Phenotyping Abstract: Availability of an explainable deep learning model that can be applied to\npractical real world scenarios and in turn, can consistently, rapidly and\naccurately identify specific and minute traits in applicable fields of\nbiological sciences, is scarce. Here we consider one such real world example\nviz., accurate identification, classification and quantification of biotic and\nabiotic stresses in crop research and production. Up until now, this has been\npredominantly done manually by visual inspection and require specialized\ntraining. However, such techniques are hindered by subjectivity resulting from\ninter- and intra-rater cognitive variability. Here, we demonstrate the ability\nof a machine learning framework to identify and classify a diverse set of\nfoliar stresses in the soybean plant with remarkable accuracy. We also present\nan explanation mechanism using gradient-weighted class activation mapping that\nisolates the visual symptoms used by the model to make predictions. This\nunsupervised identification of unique visual symptoms for each stress provides\na quantitative measure of stress severity, allowing for identification,\nclassification and quantification in one framework. The learnt model appears to\nbe agnostic to species and make good predictions for other (non-soybean)\nspecies, demonstrating an ability of transfer learning. \n\n"}
{"id": "1710.08936", "contents": "Title: Curvature-aided Incremental Aggregated Gradient Method Abstract: We propose a new algorithm for finite sum optimization which we call the\ncurvature-aided incremental aggregated gradient (CIAG) method. Motivated by the\nproblem of training a classifier for a d-dimensional problem, where the number\nof training data is $m$ and $m \\gg d \\gg 1$, the CIAG method seeks to\naccelerate incremental aggregated gradient (IAG) methods using aids from the\ncurvature (or Hessian) information, while avoiding the evaluation of matrix\ninverses required by the incremental Newton (IN) method. Specifically, our idea\nis to exploit the incrementally aggregated Hessian matrix to trace the full\ngradient vector at every incremental step, therefore achieving an improved\nlinear convergence rate over the state-of-the-art IAG methods. For strongly\nconvex problems, the fast linear convergence rate requires the objective\nfunction to be close to quadratic, or the initial point to be close to optimal\nsolution. Importantly, we show that running one iteration of the CIAG method\nyields the same improvement to the optimality gap as running one iteration of\nthe full gradient method, while the complexity is $O(d^2)$ for CIAG and $O(md)$\nfor the full gradient. Overall, the CIAG method strikes a balance between the\nhigh computation complexity incremental Newton-type methods and the slow IAG\nmethod. Our numerical results support the theoretical findings and show that\nthe CIAG method often converges with much fewer iterations than IAG, and\nrequires much shorter running time than IN when the problem dimension is high. \n\n"}
{"id": "1710.09076", "contents": "Title: Emergence of Leadership in Communication Abstract: We study a neuro-inspired model that mimics a discussion (or information\ndissemination) process in a network of agents. During their interaction, agents\nredistribute activity and network weights, resulting in emergence of leader(s).\nThe model is able to reproduce the basic scenarios of leadership known in\nnature and society: laissez-faire (irregular activity, weak leadership, sizable\ninter-follower interaction, autonomous sub-leaders); participative or\ndemocratic (strong leadership, but with feedback from followers); and\nautocratic (no feedback, one-way influence). Several pertinent aspects of these\nscenarios are found as well---e.g., hidden leadership (a hidden clique of\nagents driving the official autocratic leader), and successive leadership (two\nleaders influence followers by turns). We study how these scenarios emerge from\ninter-agent dynamics and how they depend on behavior rules of agents---in\nparticular, on their inertia against state changes. \n\n"}
{"id": "1710.09430", "contents": "Title: A Markov Chain Theory Approach to Characterizing the Minimax Optimality\n  of Stochastic Gradient Descent (for Least Squares) Abstract: This work provides a simplified proof of the statistical minimax optimality\nof (iterate averaged) stochastic gradient descent (SGD), for the special case\nof least squares. This result is obtained by analyzing SGD as a stochastic\nprocess and by sharply characterizing the stationary covariance matrix of this\nprocess. The finite rate optimality characterization captures the constant\nfactors and addresses model mis-specification. \n\n"}
{"id": "1710.09471", "contents": "Title: Inductive Representation Learning in Large Attributed Graphs Abstract: Graphs (networks) are ubiquitous and allow us to model entities (nodes) and\nthe dependencies (edges) between them. Learning a useful feature representation\nfrom graph data lies at the heart and success of many machine learning tasks\nsuch as classification, anomaly detection, link prediction, among many others.\nMany existing techniques use random walks as a basis for learning features or\nestimating the parameters of a graph model for a downstream prediction task.\nExamples include recent node embedding methods such as DeepWalk, node2vec, as\nwell as graph-based deep learning algorithms. However, the simple random walk\nused by these methods is fundamentally tied to the identity of the node. This\nhas three main disadvantages. First, these approaches are inherently\ntransductive and do not generalize to unseen nodes and other graphs. Second,\nthey are not space-efficient as a feature vector is learned for each node which\nis impractical for large graphs. Third, most of these approaches lack support\nfor attributed graphs.\n  To make these methods more generally applicable, we propose a framework for\ninductive network representation learning based on the notion of attributed\nrandom walk that is not tied to node identity and is instead based on learning\na function $\\Phi : \\mathrm{\\rm \\bf x} \\rightarrow w$ that maps a node attribute\nvector $\\mathrm{\\rm \\bf x}$ to a type $w$. This framework serves as a basis for\ngeneralizing existing methods such as DeepWalk, node2vec, and many other\nprevious methods that leverage traditional random walks. \n\n"}
{"id": "1710.09498", "contents": "Title: Dynamic Social Balance and Convergent Appraisals via Homophily and\n  Influence Mechanisms Abstract: Social balance theory describes allowable and forbidden configurations of the\ntopologies of signed directed social appraisal networks. In this paper, we\npropose two discrete-time dynamical systems that explain how an appraisal\nnetwork \\textcolor{blue}{converges to} social balance from an initially\nunbalanced configuration. These two models are based on two different\nsocio-psychological mechanisms respectively: the homophily mechanism and the\ninfluence mechanism. Our main theoretical contribution is a comprehensive\nanalysis for both models in three steps. First, we establish the well-posedness\nand bounded evolution of the interpersonal appraisals. Second, we fully\ncharacterize the set of equilibrium points; for both models, each equilibrium\nnetwork is composed by an arbitrary number of complete subgraphs satisfying\nstructural balance. Third, we establish the equivalence among three distinct\nproperties: non-vanishing appraisals, convergence to all-to-all appraisal\nnetworks, and finite-time achievement of social balance. In addition to\ntheoretical analysis, Monte Carlo validations illustrates how the non-vanishing\nappraisal condition holds for generic initial conditions in both models.\nMoreover, numerical comparison between the two models indicate that the\nhomophily-based model might be a more universal explanation for the formation\nof social balance. Finally, adopting the homophily-based model, we present\nnumerical results on the mediation and globalization of local conflicts, the\ncompetition for allies, and the asymptotic formation of a single versus two\nfactions. \n\n"}
{"id": "1710.09599", "contents": "Title: Watch Your Step: Learning Node Embeddings via Graph Attention Abstract: Graph embedding methods represent nodes in a continuous vector space,\npreserving information from the graph (e.g. by sampling random walks). There\nare many hyper-parameters to these methods (such as random walk length) which\nhave to be manually tuned for every graph. In this paper, we replace random\nwalk hyper-parameters with trainable parameters that we automatically learn via\nbackpropagation. In particular, we learn a novel attention model on the power\nseries of the transition matrix, which guides the random walk to optimize an\nupstream objective. Unlike previous approaches to attention models, the method\nthat we propose utilizes attention parameters exclusively on the data (e.g. on\nthe random walk), and not used by the model for inference. We experiment on\nlink prediction tasks, as we aim to produce embeddings that best-preserve the\ngraph structure, generalizing to unseen information. We improve\nstate-of-the-art on a comprehensive suite of real world datasets including\nsocial, collaboration, and biological networks. Adding attention to random\nwalks can reduce the error by 20% to 45% on datasets we attempted. Further, our\nlearned attention parameters are different for every graph, and our\nautomatically-found values agree with the optimal choice of hyper-parameter if\nwe manually tune existing methods. \n\n"}
{"id": "1710.10363", "contents": "Title: Diff-DAC: Distributed Actor-Critic for Average Multitask Deep\n  Reinforcement Learning Abstract: We propose a fully distributed actor-critic algorithm approximated by deep\nneural networks, named \\textit{Diff-DAC}, with application to single-task and\nto average multitask reinforcement learning (MRL). Each agent has access to\ndata from its local task only, but it aims to learn a policy that performs well\non average for the whole set of tasks. During the learning process, agents\ncommunicate their value-policy parameters to their neighbors, diffusing the\ninformation across the network, so that they converge to a common policy, with\nno need for a central node. The method is scalable, since the computational and\ncommunication costs per agent grow with its number of neighbors. We derive\nDiff-DAC's from duality theory and provide novel insights into the standard\nactor-critic framework, showing that it is actually an instance of the dual\nascent method that approximates the solution of a linear program. Experiments\nsuggest that Diff-DAC can outperform the single previous distributed MRL\napproach (i.e., Dist-MTLPS) and even the centralized architecture. \n\n"}
{"id": "1710.10646", "contents": "Title: On the Consistency of Quick Shift Abstract: Quick Shift is a popular mode-seeking and clustering algorithm. We present\nfinite sample statistical consistency guarantees for Quick Shift on mode and\ncluster recovery under mild distributional assumptions. We then apply our\nresults to construct a consistent modal regression algorithm. \n\n"}
{"id": "1710.10903", "contents": "Title: Graph Attention Networks Abstract: We present graph attention networks (GATs), novel neural network\narchitectures that operate on graph-structured data, leveraging masked\nself-attentional layers to address the shortcomings of prior methods based on\ngraph convolutions or their approximations. By stacking layers in which nodes\nare able to attend over their neighborhoods' features, we enable (implicitly)\nspecifying different weights to different nodes in a neighborhood, without\nrequiring any kind of costly matrix operation (such as inversion) or depending\non knowing the graph structure upfront. In this way, we address several key\nchallenges of spectral-based graph neural networks simultaneously, and make our\nmodel readily applicable to inductive as well as transductive problems. Our GAT\nmodels have achieved or matched state-of-the-art results across four\nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and\nPubmed citation network datasets, as well as a protein-protein interaction\ndataset (wherein test graphs remain unseen during training). \n\n"}
{"id": "1710.10998", "contents": "Title: Social Network De-anonymization: More Adversarial Knowledge, More Users\n  Re-Identified? Abstract: Following the trend of data trading and data publishing, many online social\nnetworks have enabled potentially sensitive data to be exchanged or shared on\nthe web. As a result, users' privacy could be exposed to malicious third\nparties since they are extremely vulnerable to de-anonymization attacks, i.e.,\nthe attacker links the anonymous nodes in the social network to their real\nidentities with the help of background knowledge. Previous work in social\nnetwork de-anonymization mostly focuses on designing accurate and efficient\nde-anonymization methods. We study this topic from a different perspective and\nattempt to investigate the intrinsic relation between the attacker's knowledge\nand the expected de-anonymization gain. One common intuition is that the more\nauxiliary information the attacker has, the more accurate de-anonymization\nbecomes. However, their relation is much more sophisticated than that. To\nsimplify the problem, we attempt to quantify background knowledge and\nde-anonymization gain under several assumptions. Our theoretical analysis and\nsimulations on synthetic and real network data show that more background\nknowledge may not necessarily lead to more de-anonymization gain in certain\ncases. Though our analysis is based on a few assumptions, the findings still\nleave intriguing implications for the attacker to make better use of the\nbackground knowledge when performing de-anonymization, and for the data owners\nto better measure the privacy risk when releasing their data to third parties. \n\n"}
{"id": "1710.11070", "contents": "Title: Convergence Rates of Latent Topic Models Under Relaxed Identifiability\n  Conditions Abstract: In this paper we study the frequentist convergence rate for the Latent\nDirichlet Allocation (Blei et al., 2003) topic models. We show that the maximum\nlikelihood estimator converges to one of the finitely many equivalent\nparameters in Wasserstein's distance metric at a rate of $n^{-1/4}$ without\nassuming separability or non-degeneracy of the underlying topics and/or the\nexistence of more than three words per document, thus generalizing the previous\nworks of Anandkumar et al. (2012, 2014) from an information-theoretical\nperspective. We also show that the $n^{-1/4}$ convergence rate is optimal in\nthe worst case. \n\n"}
{"id": "1710.11268", "contents": "Title: Theoretical and Computational Guarantees of Mean Field Variational\n  Inference for Community Detection Abstract: The mean field variational Bayes method is becoming increasingly popular in\nstatistics and machine learning. Its iterative Coordinate Ascent Variational\nInference algorithm has been widely applied to large scale Bayesian inference.\nSee Blei et al. (2017) for a recent comprehensive review. Despite the\npopularity of the mean field method there exist remarkably little fundamental\ntheoretical justifications. To the best of our knowledge, the iterative\nalgorithm has never been investigated for any high dimensional and complex\nmodel. In this paper, we study the mean field method for community detection\nunder the Stochastic Block Model. For an iterative Batch Coordinate Ascent\nVariational Inference algorithm, we show that it has a linear convergence rate\nand converges to the minimax rate within $\\log n$ iterations. This complements\nthe results of Bickel et al. (2013) which studied the global minimum of the\nmean field variational Bayes and obtained asymptotic normal estimation of\nglobal model parameters. In addition, we obtain similar optimality results for\nGibbs sampling and an iterative procedure to calculate maximum likelihood\nestimation, which can be of independent interest. \n\n"}
{"id": "1711.00982", "contents": "Title: From which world is your graph? Abstract: Discovering statistical structure from links is a fundamental problem in the\nanalysis of social networks. Choosing a misspecified model, or equivalently, an\nincorrect inference algorithm will result in an invalid analysis or even\nfalsely uncover patterns that are in fact artifacts of the model. This work\nfocuses on unifying two of the most widely used link-formation models: the\nstochastic blockmodel (SBM) and the small world (or latent space) model (SWM).\nIntegrating techniques from kernel learning, spectral graph theory, and\nnonlinear dimensionality reduction, we develop the first statistically sound\npolynomial-time algorithm to discover latent patterns in sparse graphs for both\nmodels. When the network comes from an SBM, the algorithm outputs a block\nstructure. When it is from an SWM, the algorithm outputs estimates of each\nnode's latent position. \n\n"}
{"id": "1711.01012", "contents": "Title: Policy Optimization by Genetic Distillation Abstract: Genetic algorithms have been widely used in many practical optimization\nproblems. Inspired by natural selection, operators, including mutation,\ncrossover and selection, provide effective heuristics for search and black-box\noptimization. However, they have not been shown useful for deep reinforcement\nlearning, possibly due to the catastrophic consequence of parameter crossovers\nof neural networks. Here, we present Genetic Policy Optimization (GPO), a new\ngenetic algorithm for sample-efficient deep policy optimization. GPO uses\nimitation learning for policy crossover in the state space and applies policy\ngradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as\na genetic algorithm is able to provide superior performance over the\nstate-of-the-art policy gradient methods and achieves comparable or higher\nsample efficiency. \n\n"}
{"id": "1711.01921", "contents": "Title: $A^{4}NT$: Author Attribute Anonymity by Adversarial Training of Neural\n  Machine Translation Abstract: Text-based analysis methods allow to reveal privacy relevant author\nattributes such as gender, age and identify of the text's author. Such methods\ncan compromise the privacy of an anonymous author even when the author tries to\nremove privacy sensitive content. In this paper, we propose an automatic\nmethod, called Adversarial Author Attribute Anonymity Neural Translation\n($A^4NT$), to combat such text-based adversaries. We combine\nsequence-to-sequence language models used in machine translation and generative\nadversarial networks to obfuscate author attributes. Unlike machine translation\ntechniques which need paired data, our method can be trained on unpaired\ncorpora of text containing different authors. Importantly, we propose and\nevaluate techniques to impose constraints on our $A^4NT$ to preserve the\nsemantics of the input text. $A^4NT$ learns to make minimal changes to the\ninput text to successfully fool author attribute classifiers, while aiming to\nmaintain the meaning of the input. We show through experiments on two different\ndatasets and three settings that our proposed method is effective in fooling\nthe author attribute classifiers and thereby improving the anonymity of\nauthors. \n\n"}
{"id": "1711.03321", "contents": "Title: A Separation Principle for Control in the Age of Deep Learning Abstract: We review the problem of defining and inferring a \"state\" for a control\nsystem based on complex, high-dimensional, highly uncertain measurement streams\nsuch as videos. Such a state, or representation, should contain all and only\nthe information needed for control, and discount nuisance variability in the\ndata. It should also have finite complexity, ideally modulated depending on\navailable resources. This representation is what we want to store in memory in\nlieu of the data, as it \"separates\" the control task from the measurement\nprocess. For the trivial case with no dynamics, a representation can be\ninferred by minimizing the Information Bottleneck Lagrangian in a function\nclass realized by deep neural networks. The resulting representation has much\nhigher dimension than the data, already in the millions, but it is smaller in\nthe sense of information content, retaining only what is needed for the task.\nThis process also yields representations that are invariant to nuisance factors\nand having maximally independent components. We extend these ideas to the\ndynamic case, where the representation is the posterior density of the task\nvariable given the measurements up to the current time, which is in general\nmuch simpler than the prediction density maintained by the classical Bayesian\nfilter. Again this can be finitely-parametrized using a deep neural network,\nand already some applications are beginning to emerge. No explicit assumption\nof Markovianity is needed; instead, complexity trades off approximation of an\noptimal representation, including the degree of Markovianity. \n\n"}
{"id": "1711.03905", "contents": "Title: Attend and Diagnose: Clinical Time Series Analysis using Attention\n  Models Abstract: With widespread adoption of electronic health records, there is an increased\nemphasis for predictive models that can effectively deal with clinical\ntime-series data. Powered by Recurrent Neural Network (RNN) architectures with\nLong Short-Term Memory (LSTM) units, deep neural networks have achieved\nstate-of-the-art results in several clinical prediction tasks. Despite the\nsuccess of RNNs, its sequential nature prohibits parallelized computing, thus\nmaking it inefficient particularly when processing long sequences. Recently,\narchitectures which are based solely on attention mechanisms have shown\nremarkable success in transduction tasks in NLP, while being computationally\nsuperior. In this paper, for the first time, we utilize attention models for\nclinical time-series modeling, thereby dispensing recurrence entirely. We\ndevelop the \\textit{SAnD} (Simply Attend and Diagnose) architecture, which\nemploys a masked, self-attention mechanism, and uses positional encoding and\ndense interpolation strategies for incorporating temporal order. Furthermore,\nwe develop a multi-task variant of \\textit{SAnD} to jointly infer models with\nmultiple diagnosis tasks. Using the recent MIMIC-III benchmark datasets, we\ndemonstrate that the proposed approach achieves state-of-the-art performance in\nall tasks, outperforming LSTM models and classical baselines with\nhand-engineered features. \n\n"}
{"id": "1711.04837", "contents": "Title: Improving Factor-Based Quantitative Investing by Forecasting Company\n  Fundamentals Abstract: On a periodic basis, publicly traded companies are required to report\nfundamentals: financial data such as revenue, operating income, debt, among\nothers. These data points provide some insight into the financial health of a\ncompany. Academic research has identified some factors, i.e. computed features\nof the reported data, that are known through retrospective analysis to\noutperform the market average. Two popular factors are the book value\nnormalized by market capitalization (book-to-market) and the operating income\nnormalized by the enterprise value (EBIT/EV). In this paper: we first show\nthrough simulation that if we could (clairvoyantly) select stocks using factors\ncalculated on future fundamentals (via oracle), then our portfolios would far\noutperform a standard factor approach. Motivated by this analysis, we train\ndeep neural networks to forecast future fundamentals based on a trailing\n5-years window. Quantitative analysis demonstrates a significant improvement in\nMSE over a naive strategy. Moreover, in retrospective analysis using an\nindustry-grade stock portfolio simulator (backtester), we show an improvement\nin compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor\nmodel. \n\n"}
{"id": "1711.04992", "contents": "Title: Feature importance scores and lossless feature pruning using Banzhaf\n  power indices Abstract: Understanding the influence of features in machine learning is crucial to\ninterpreting models and selecting the best features for classification. In this\nwork we propose the use of principles from coalitional game theory to reason\nabout importance of features. In particular, we propose the use of the Banzhaf\npower index as a measure of influence of features on the outcome of a\nclassifier. We show that features having Banzhaf power index of zero can be\nlosslessly pruned without damage to classifier accuracy. Computing the power\nindices does not require having access to data samples. However, if samples are\navailable, the indices can be empirically estimated. We compute Banzhaf power\nindices for a neural network classifier on real-life data, and compare the\nresults with gradient-based feature saliency, and coefficients of a logistic\nregression model with $L_1$ regularization. \n\n"}
{"id": "1711.05174", "contents": "Title: Near-Optimal Discrete Optimization for Experimental Design: A Regret\n  Minimization Approach Abstract: The experimental design problem concerns the selection of k points from a\npotentially large design pool of p-dimensional vectors, so as to maximize the\nstatistical efficiency regressed on the selected k design points. Statistical\nefficiency is measured by optimality criteria, including A(verage),\nD(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for the\nT-optimality, exact optimization is NP-hard.\n  We propose a polynomial-time regret minimization framework to achieve a\n$(1+\\varepsilon)$ approximation with only $O(p/\\varepsilon^2)$ design points,\nfor all the optimality criteria above.\n  In contrast, to the best of our knowledge, before our work, no\npolynomial-time algorithm achieves $(1+\\varepsilon)$ approximations for\nD/E/G-optimality, and the best poly-time algorithm achieving\n$(1+\\varepsilon)$-approximation for A/V-optimality requires $k =\n\\Omega(p^2/\\varepsilon)$ design points. \n\n"}
{"id": "1711.05374", "contents": "Title: Optimizing Kernel Machines using Deep Learning Abstract: Building highly non-linear and non-parametric models is central to several\nstate-of-the-art machine learning systems. Kernel methods form an important\nclass of techniques that induce a reproducing kernel Hilbert space (RKHS) for\ninferring non-linear models through the construction of similarity functions\nfrom data. These methods are particularly preferred in cases where the training\ndata sizes are limited and when prior knowledge of the data similarities is\navailable. Despite their usefulness, they are limited by the computational\ncomplexity and their inability to support end-to-end learning with a\ntask-specific objective. On the other hand, deep neural networks have become\nthe de facto solution for end-to-end inference in several learning paradigms.\nIn this article, we explore the idea of using deep architectures to perform\nkernel machine optimization, for both computational efficiency and end-to-end\ninferencing. To this end, we develop the DKMO (Deep Kernel Machine\nOptimization) framework, that creates an ensemble of dense embeddings using\nNystrom kernel approximations and utilizes deep learning to generate\ntask-specific representations through the fusion of the embeddings.\nIntuitively, the filters of the network are trained to fuse information from an\nensemble of linear subspaces in the RKHS. Furthermore, we introduce the kernel\ndropout regularization to enable improved training convergence. Finally, we\nextend this framework to the multiple kernel case, by coupling a global fusion\nlayer with pre-trained deep kernel machines for each of the constituent\nkernels. Using case studies with limited training data, and lack of explicit\nfeature sources, we demonstrate the effectiveness of our framework over\nconventional model inferencing techniques. \n\n"}
{"id": "1711.06100", "contents": "Title: Sequences, Items And Latent Links: Recommendation With Consumed Item\n  Packs Abstract: Recommenders personalize the web content by typically using collaborative\nfiltering to relate users (or items) based on explicit feedback, e.g., ratings.\nThe difficulty of collecting this feedback has recently motivated to consider\nimplicit feedback (e.g., item consumption along with the corresponding time).\n  In this paper, we introduce the notion of consumed item pack (CIP) which\nenables to link users (or items) based on their implicit analogous consumption\nbehavior. Our proposal is generic, and we show that it captures three novel\nimplicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word\nembedding-based (DEEPCIP), as well as a state-of-the-art technique using\nimplicit feedback (FISM). We show that our recommenders handle incremental\nupdates incorporating freshly consumed items. We demonstrate that all three\nrecommenders provide a recommendation quality that is competitive with\nstate-of-the-art ones, including one incorporating both explicit and implicit\nfeedback. \n\n"}
{"id": "1711.06424", "contents": "Title: A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit Abstract: Determining the appropriate batch size for mini-batch gradient descent is\nalways time consuming as it often relies on grid search. This paper considers a\nresizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed\nbandit for achieving best performance in grid search by selecting an\nappropriate batch size at each epoch with a probability defined as a function\nof its previous success/failure. This probability encourages exploration of\ndifferent batch size and then later exploitation of batch size with history of\nsuccess. At each epoch, the RMGD samples a batch size from its probability\ndistribution, then uses the selected batch size for mini-batch gradient\ndescent. After obtaining the validation loss at each epoch, the probability\ndistribution is updated to incorporate the effectiveness of the sampled batch\nsize. The RMGD essentially assists the learning process to explore the possible\ndomain of the batch size and exploit successful batch size. Experimental\nresults show that the RMGD achieves performance better than the best performing\nsingle batch size. Furthermore, it, obviously, attains this performance in a\nshorter amount of time than grid search. It is surprising that the RMGD\nachieves better performance than grid search. \n\n"}
{"id": "1711.06795", "contents": "Title: Prediction Scores as a Window into Classifier Behavior Abstract: Most multi-class classifiers make their prediction for a test sample by\nscoring the classes and selecting the one with the highest score. Analyzing\nthese prediction scores is useful to understand the classifier behavior and to\nassess its reliability. We present an interactive visualization that\nfacilitates per-class analysis of these scores. Our system, called Classilist,\nenables relating these scores to the classification correctness and to the\nunderlying samples and their features. We illustrate how such analysis reveals\nvarying behavior of different classifiers. Classilist is available for use\nonline, along with source code, video tutorials, and plugins for R, RapidMiner,\nand KNIME at https://katehara.github.io/classilist-site/. \n\n"}
{"id": "1711.06899", "contents": "Title: The Cultural Evolution of National Constitutions Abstract: We explore how ideas from infectious disease and genetics can be used to\nuncover patterns of cultural inheritance and innovation in a corpus of 591\nnational constitutions spanning 1789 - 2008. Legal \"Ideas\" are encoded as\n\"topics\" - words statistically linked in documents - derived from topic\nmodeling the corpus of constitutions. Using these topics we derive a diffusion\nnetwork for borrowing from ancestral constitutions back to the US Constitution\nof 1789 and reveal that constitutions are complex cultural recombinants. We\nfind systematic variation in patterns of borrowing from ancestral texts and\n\"biological\"-like behavior in patterns of inheritance with the distribution of\n\"offspring\" arising through a bounded preferential-attachment process. This\nprocess leads to a small number of highly innovative (influential)\nconstitutions some of which have yet to have been identified as so in the\ncurrent literature. Our findings thus shed new light on the critical nodes of\nthe constitution-making network. The constitutional network structure reflects\nperiods of intense constitution creation, and systematic patterns of variation\nin constitutional life-span and temporal influence. \n\n"}
{"id": "1711.07511", "contents": "Title: Optimistic Robust Optimization With Applications To Machine Learning Abstract: Robust Optimization has traditionally taken a pessimistic, or worst-case\nviewpoint of uncertainty which is motivated by a desire to find sets of optimal\npolicies that maintain feasibility under a variety of operating conditions. In\nthis paper, we explore an optimistic, or best-case view of uncertainty and show\nthat it can be a fruitful approach. We show that these techniques can be used\nto address a wide variety of problems. First, we apply our methods in the\ncontext of robust linear programming, providing a method for reducing\nconservatism in intuitive ways that encode economically realistic modeling\nassumptions. Second, we look at problems in machine learning and find that this\napproach is strongly connected to the existing literature. Specifically, we\nprovide a new interpretation for popular sparsity inducing non-convex\nregularization schemes. Additionally, we show that successful approaches for\ndealing with outliers and noise can be interpreted as optimistic robust\noptimization problems. Although many of the problems resulting from our\napproach are non-convex, we find that DCA or DCA-like optimization approaches\ncan be intuitive and efficient. \n\n"}
{"id": "1711.07601", "contents": "Title: Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users\n  in Real-Time Abstract: User experience in modern content discovery applications critically depends\non high-quality personalized recommendations. However, building systems that\nprovide such recommendations presents a major challenge due to a massive pool\nof items, a large number of users, and requirements for recommendations to be\nresponsive to user actions and generated on demand in real-time. Here we\npresent Pixie, a scalable graph-based real-time recommender system that we\ndeveloped and deployed at Pinterest. Given a set of user-specific pins as a\nquery, Pixie selects in real-time from billions of possible pins those that are\nmost related to the query. To generate recommendations, we develop Pixie Random\nWalk algorithm that utilizes the Pinterest object graph of 3 billion nodes and\n17 billion edges. Experiments show that recommendations provided by Pixie lead\nup to 50% higher user engagement when compared to the previous Hadoop-based\nproduction system. Furthermore, we develop a graph pruning strategy at that\nleads to an additional 58% improvement in recommendations. Last, we discuss\nsystem aspects of Pixie, where a single server executes 1,200 recommendation\nrequests per second with 60 millisecond latency. Today, systems backed by Pixie\ncontribute to more than 80% of all user engagement on Pinterest. \n\n"}
{"id": "1711.07693", "contents": "Title: Regret Analysis for Continuous Dueling Bandit Abstract: The dueling bandit is a learning framework wherein the feedback information\nin the learning process is restricted to a noisy comparison between a pair of\nactions. In this research, we address a dueling bandit problem based on a cost\nfunction over a continuous space. We propose a stochastic mirror descent\nalgorithm and show that the algorithm achieves an $O(\\sqrt{T\\log T})$-regret\nbound under strong convexity and smoothness assumptions for the cost function.\nSubsequently, we clarify the equivalence between regret minimization in dueling\nbandit and convex optimization for the cost function. Moreover, when\nconsidering a lower bound in convex optimization, our algorithm is shown to\nachieve the optimal convergence rate in convex optimization and the optimal\nregret in dueling bandit except for a logarithmic factor. \n\n"}
{"id": "1711.08365", "contents": "Title: Budget Allocation in Binary Opinion Dynamics Abstract: In this article we study the allocation of a budget to promote an opinion in\na group of agents. We assume that their opinion dynamics are based on the\nwell-known voter model. We are interested in finding the most efficient use of\na budget over time in order to manipulate a social network. We address the\nproblem using the theory of discounted Markov decision processes. Our\ncontributions can be summarized as follows: (i) we introduce the discounted\nMarkov decision process in our cases, (ii) we present the corresponding Bellman\nequations, and, (iii) we solve the Bellman equations via backward programming.\nThis work is a step towards providing a solid formulation of the budget\nallocation in social networks. \n\n"}
{"id": "1711.08426", "contents": "Title: Leverage Score Sampling for Faster Accelerated Regression and ERM Abstract: Given a matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and a vector $b\n\\in\\mathbb{R}^{d}$, we show how to compute an $\\epsilon$-approximate solution\nto the regression problem $ \\min_{x\\in\\mathbb{R}^{d}}\\frac{1}{2} \\|\\mathbf{A} x\n- b\\|_{2}^{2} $ in time $ \\tilde{O} ((n+\\sqrt{d\\cdot\\kappa_{\\text{sum}}})\\cdot\ns\\cdot\\log\\epsilon^{-1}) $ where\n$\\kappa_{\\text{sum}}=\\mathrm{tr}\\left(\\mathbf{A}^{\\top}\\mathbf{A}\\right)/\\lambda_{\\min}(\\mathbf{A}^{T}\\mathbf{A})$\nand $s$ is the maximum number of non-zero entries in a row of $\\mathbf{A}$. Our\nalgorithm improves upon the previous best running time of $ \\tilde{O}\n((n+\\sqrt{n \\cdot\\kappa_{\\text{sum}}})\\cdot s\\cdot\\log\\epsilon^{-1})$.\n  We achieve our result through a careful combination of leverage score\nsampling techniques, proximal point methods, and accelerated coordinate\ndescent. Our method not only matches the performance of previous methods, but\nfurther improves whenever leverage scores of rows are small (up to\npolylogarithmic factors). We also provide a non-linear generalization of these\nresults that improves the running time for solving a broader class of ERM\nproblems. \n\n"}
{"id": "1711.09159", "contents": "Title: Quantifying the Effects of Enforcing Disentanglement on Variational\n  Autoencoders Abstract: The notion of disentangled autoencoders was proposed as an extension to the\nvariational autoencoder by introducing a disentanglement parameter $\\beta$,\ncontrolling the learning pressure put on the possible underlying latent\nrepresentations. For certain values of $\\beta$ this kind of autoencoders is\ncapable of encoding independent input generative factors in separate elements\nof the code, leading to a more interpretable and predictable model behaviour.\nIn this paper we quantify the effects of the parameter $\\beta$ on the model\nperformance and disentanglement. After training multiple models with the same\nvalue of $\\beta$, we establish the existence of consistent variance in one of\nthe disentanglement measures, proposed in literature. The negative consequences\nof the disentanglement to the autoencoder's discriminative ability are also\nasserted while varying the amount of examples available during training. \n\n"}
{"id": "1711.09472", "contents": "Title: Community detection algorithm evaluation with ground-truth data Abstract: Community structure is of paramount importance for the understanding of\ncomplex networks. Consequently, there is a tremendous effort in order to\ndevelop efficient community detection algorithms. Unfortunately, the issue of a\nfair assessment of these algorithms is a thriving open question. If the\nground-truth community structure is available, various clustering-based metrics\nare used in order to compare it versus the one discovered by these algorithms.\nHowever, these metrics defined at the node level are fairly insensitive to the\nvariation of the overall community structure. To overcome these limitations, we\npropose to exploit the topological features of the 'community graphs' (where\nthe nodes are the communities and the links represent their interactions) in\norder to evaluate the algorithms. To illustrate our methodology, we conduct a\ncomprehensive analysis of overlapping community detection algorithms using a\nset of real-world networks with known a priori community structure. Results\nprovide a better perception of their relative performance as compared to\nclassical metrics. Moreover, they show that more emphasis should be put on the\ntopology of the community structure. We also investigate the relationship\nbetween the topological properties of the community structure and the\nalternative evaluation measures (quality metrics and clustering metrics). It\nappears clearly that they present different views of the community structure\nand that they must be combined in order to evaluate the effectiveness of\ncommunity detection algorithms. \n\n"}
{"id": "1711.11423", "contents": "Title: On reducing the communication cost of the diffusion LMS algorithm Abstract: The rise of digital and mobile communications has recently made the world\nmore connected and networked, resulting in an unprecedented volume of data\nflowing between sources, data centers, or processes. While these data may be\nprocessed in a centralized manner, it is often more suitable to consider\ndistributed strategies such as diffusion as they are scalable and can handle\nlarge amounts of data by distributing tasks over networked agents. Although it\nis relatively simple to implement diffusion strategies over a cluster, it\nappears to be challenging to deploy them in an ad-hoc network with limited\nenergy budget for communication. In this paper, we introduce a diffusion LMS\nstrategy that significantly reduces communication costs without compromising\nthe performance. Then, we analyze the proposed algorithm in the mean and\nmean-square sense. Next, we conduct numerical experiments to confirm the\ntheoretical findings. Finally, we perform large scale simulations to test the\nalgorithm efficiency in a scenario where energy is limited. \n\n"}
{"id": "1711.11499", "contents": "Title: Google matrix of Bitcoin network Abstract: We construct and study the Google matrix of Bitcoin transactions during the\ntime period from the very beginning in 2009 till April 2013. The Bitcoin\nnetwork has up to a few millions of bitcoin users and we present its main\ncharacteristics including the PageRank and CheiRank probability distributions,\nthe spectrum of eigenvalues of Google matrix and related eigenvectors. We find\nthat the spectrum has an unusual circle-type structure which we attribute to\nexisting hidden communities of nodes linked between their members. We show that\nthe Gini coefficient of the transactions for the whole period is close to unity\nshowing that the main part of wealth of the network is captured by a small\nfraction of users. \n\n"}
{"id": "1712.00205", "contents": "Title: Tensors, Learning, and 'Kolmogorov Extension' for Finite-alphabet Random\n  Vectors Abstract: Estimating the joint probability mass function (PMF) of a set of random\nvariables lies at the heart of statistical learning and signal processing.\nWithout structural assumptions, such as modeling the variables as a Markov\nchain, tree, or other graphical model, joint PMF estimation is often considered\nmission impossible - the number of unknowns grows exponentially with the number\nof variables. But who gives us the structural model? Is there a generic,\n`non-parametric' way to control joint PMF complexity without relying on a\npriori structural assumptions regarding the underlying probability model? Is it\npossible to discover the operational structure without biasing the analysis up\nfront? What if we only observe random subsets of the variables, can we still\nreliably estimate the joint PMF of all? This paper shows, perhaps surprisingly,\nthat if the joint PMF of any three variables can be estimated, then the joint\nPMF of all the variables can be provably recovered under relatively mild\nconditions. The result is reminiscent of Kolmogorov's extension theorem -\nconsistent specification of lower-dimensional distributions induces a unique\nprobability measure for the entire process. The difference is that for\nprocesses of limited complexity (rank of the high-dimensional PMF) it is\npossible to obtain complete characterization from only three-dimensional\ndistributions. In fact not all three-dimensional PMFs are needed; and under\nmore stringent conditions even two-dimensional will do. Exploiting multilinear\nalgebra, this paper proves that such higher-dimensional PMF completion can be\nguaranteed - several pertinent identifiability results are derived. It also\nprovides a practical and efficient algorithm to carry out the recovery task.\nJudiciously designed simulations and real-data experiments on movie\nrecommendation and data classification are presented to showcase the\neffectiveness of the approach. \n\n"}
{"id": "1712.00424", "contents": "Title: The reparameterization trick for acquisition functions Abstract: Bayesian optimization is a sample-efficient approach to solving global\noptimization problems. Along with a surrogate model, this approach relies on\ntheoretically motivated value heuristics (acquisition functions) to guide the\nsearch process. Maximizing acquisition functions yields the best performance;\nunfortunately, this ideal is difficult to achieve since optimizing acquisition\nfunctions per se is frequently non-trivial. This statement is especially true\nin the parallel setting, where acquisition functions are routinely non-convex,\nhigh-dimensional, and intractable. Here, we demonstrate how many popular\nacquisition functions can be formulated as Gaussian integrals amenable to the\nreparameterization trick and, ensuingly, gradient-based optimization. Further,\nwe use this reparameterized representation to derive an efficient Monte Carlo\nestimator for the upper confidence bound acquisition function in the context of\nparallel selection. \n\n"}
{"id": "1712.00468", "contents": "Title: Graph Signal Processing: Overview, Challenges and Applications Abstract: Research in Graph Signal Processing (GSP) aims to develop tools for\nprocessing data defined on irregular graph domains. In this paper we first\nprovide an overview of core ideas in GSP and their connection to conventional\ndigital signal processing. We then summarize recent developments in developing\nbasic GSP tools, including methods for sampling, filtering or graph learning.\nNext, we review progress in several application areas using GSP, including\nprocessing and analysis of sensor network data, biological data, and\napplications to image processing and machine learning. We finish by providing a\nbrief historical perspective to highlight how concepts recently developed in\nGSP build on top of prior research in other areas. \n\n"}
{"id": "1712.00595", "contents": "Title: Fast and Accurate Random Walk with Restart on Dynamic Graphs with\n  Guarantees Abstract: Given a time-evolving graph, how can we track similarity between nodes in a\nfast and accurate way, with theoretical guarantees on the convergence and the\nerror? Random Walk with Restart (RWR) is a popular measure to estimate the\nsimilarity between nodes and has been exploited in numerous applications. Many\nreal-world graphs are dynamic with frequent insertion/deletion of edges; thus,\ntracking RWR scores on dynamic graphs in an efficient way has aroused much\ninterest among data mining researchers. Recently, dynamic RWR models based on\nthe propagation of scores across a given graph have been proposed, and have\nsucceeded in outperforming previous other approaches to compute RWR\ndynamically. However, those models fail to guarantee exactness and convergence\ntime for updating RWR in a generalized form. In this paper, we propose OSP, a\nfast and accurate algorithm for computing dynamic RWR with insertion/deletion\nof nodes/edges in a directed/undirected graph. When the graph is updated, OSP\nfirst calculates offset scores around the modified edges, propagates the offset\nscores across the updated graph, and then merges them with the current RWR\nscores to get updated RWR scores. We prove the exactness of OSP and introduce\nOSP-T, a version of OSP which regulates a trade-off between accuracy and\ncomputation time by using error tolerance {\\epsilon}. Given restart probability\nc, OSP-T guarantees to return RWR scores with O ({\\epsilon} /c ) error in O\n(log ({\\epsilon}/2)/log(1-c)) iterations. Through extensive experiments, we\nshow that OSP tracks RWR exactly up to 4605x faster than existing static RWR\nmethod on dynamic graphs, and OSP-T requires up to 15x less time with 730x\nlower L1 norm error and 3.3x lower rank error than other state-of-the-art\ndynamic RWR methods. \n\n"}
{"id": "1712.01038", "contents": "Title: Vprop: Variational Inference using RMSprop Abstract: Many computationally-efficient methods for Bayesian deep learning rely on\ncontinuous optimization algorithms, but the implementation of these methods\nrequires significant changes to existing code-bases. In this paper, we propose\nVprop, a method for Gaussian variational inference that can be implemented with\ntwo minor changes to the off-the-shelf RMSprop optimizer. Vprop also reduces\nthe memory requirements of Black-Box Variational Inference by half. We derive\nVprop using the conjugate-computation variational inference method, and\nestablish its connections to Newton's method, natural-gradient methods, and\nextended Kalman filters. Overall, this paper presents Vprop as a principled,\ncomputationally-efficient, and easy-to-implement method for Bayesian deep\nlearning. \n\n"}
{"id": "1712.01066", "contents": "Title: Connecting Pixels to Privacy and Utility: Automatic Redaction of Private\n  Information in Images Abstract: Images convey a broad spectrum of personal information. If such images are\nshared on social media platforms, this personal information is leaked which\nconflicts with the privacy of depicted persons. Therefore, we aim for automated\napproaches to redact such private information and thereby protect privacy of\nthe individual. By conducting a user study we find that obfuscating the image\nregions related to the private information leads to privacy while retaining\nutility of the images. Moreover, by varying the size of the regions different\nprivacy-utility trade-offs can be achieved. Our findings argue for a \"redaction\nby segmentation\" paradigm. Hence, we propose the first sizable dataset of\nprivate images \"in the wild\" annotated with pixel and instance level labels\nacross a broad range of privacy classes. We present the first model for\nautomatic redaction of diverse private information. \n\n"}
{"id": "1712.01664", "contents": "Title: Learning a Generative Model for Validity in Complex Discrete Structures Abstract: Deep generative models have been successfully used to learn representations\nfor high-dimensional discrete spaces by representing discrete objects as\nsequences and employing powerful sequence-based deep models. Unfortunately,\nthese sequence-based models often produce invalid sequences: sequences which do\nnot represent any underlying discrete structure; invalid sequences hinder the\nutility of such models. As a step towards solving this problem, we propose to\nlearn a deep recurrent validator model, which can estimate whether a partial\nsequence can function as the beginning of a full, valid sequence. This\nvalidator provides insight as to how individual sequence elements influence the\nvalidity of the overall sequence, and can be used to constrain sequence based\nmodels to generate valid sequences -- and thus faithfully model discrete\nobjects. Our approach is inspired by reinforcement learning, where an oracle\nwhich can evaluate validity of complete sequences provides a sparse reward\nsignal. We demonstrate its effectiveness as a generative model of Python 3\nsource code for mathematical expressions, and in improving the ability of a\nvariational autoencoder trained on SMILES strings to decode valid molecular\nstructures. \n\n"}
{"id": "1712.01665", "contents": "Title: Differentially Private Dropout Abstract: Large data collections required for the training of neural networks often\ncontain sensitive information such as the medical histories of patients, and\nthe privacy of the training data must be preserved. In this paper, we introduce\na dropout technique that provides an elegant Bayesian interpretation to\ndropout, and show that the intrinsic noise added, with the primary goal of\nregularization, can be exploited to obtain a degree of differential privacy.\nThe iterative nature of training neural networks presents a challenge for\nprivacy-preserving estimation since multiple iterations increase the amount of\nnoise added. We overcome this by using a relaxed notion of differential\nprivacy, called concentrated differential privacy, which provides tighter\nestimates on the overall privacy loss. We demonstrate the accuracy of our\nprivacy-preserving dropout algorithm on benchmark datasets. \n\n"}
{"id": "1712.06132", "contents": "Title: Dynamic Boltzmann Machines for Second Order Moments and Generalized\n  Gaussian Distributions Abstract: Dynamic Boltzmann Machine (DyBM) has been shown highly efficient to predict\ntime-series data. Gaussian DyBM is a DyBM that assumes the predicted data is\ngenerated by a Gaussian distribution whose first-order moment (mean)\ndynamically changes over time but its second-order moment (variance) is fixed.\nHowever, in many financial applications, the assumption is quite limiting in\ntwo aspects. First, even when the data follows a Gaussian distribution, its\nvariance may change over time. Such variance is also related to important\ntemporal economic indicators such as the market volatility. Second, financial\ntime-series data often requires learning datasets generated by the generalized\nGaussian distribution with an additional shape parameter that is important to\napproximate heavy-tailed distributions. Addressing those aspects, we show how\nto extend DyBM that results in significant performance improvement in\npredicting financial time-series data. \n\n"}
{"id": "1712.06199", "contents": "Title: Structured Optimal Transport Abstract: Optimal Transport has recently gained interest in machine learning for\napplications ranging from domain adaptation, sentence similarities to deep\nlearning. Yet, its ability to capture frequently occurring structure beyond the\n\"ground metric\" is limited. In this work, we develop a nonlinear generalization\nof (discrete) optimal transport that is able to reflect much additional\nstructure. We demonstrate how to leverage the geometry of this new model for\nfast algorithms, and explore connections and properties. Illustrative\nexperiments highlight the benefit of the induced structured couplings for tasks\nin domain adaptation and natural language processing. \n\n"}
{"id": "1712.06245", "contents": "Title: Misspecified Nonconvex Statistical Optimization for Phase Retrieval Abstract: Existing nonconvex statistical optimization theory and methods crucially rely\non the correct specification of the underlying \"true\" statistical models. To\naddress this issue, we take a first step towards taming model misspecification\nby studying the high-dimensional sparse phase retrieval problem with\nmisspecified link functions. In particular, we propose a simple variant of the\nthresholded Wirtinger flow algorithm that, given a proper initialization,\nlinearly converges to an estimator with optimal statistical accuracy for a\nbroad family of unknown link functions. We further provide extensive numerical\nexperiments to support our theoretical findings. \n\n"}
{"id": "1712.07844", "contents": "Title: Memory-induced mechanism for self-sustaining activity in networks Abstract: We study a mechanism of activity sustaining on networks inspired by a\nwell-known model of neuronal dynamics. Our primary focus is the emergence of\nself-sustaining collective activity patterns, where no single node can stay\nactive by itself, but the activity provided initially is sustained within the\ncollective of interacting agents. In contrast to existing models of\nself-sustaining activity that are caused by (long) loops present in the\nnetwork, here we focus on tree--like structures and examine activation\nmechanisms that are due to temporal memory of the nodes. This approach is\nmotivated by applications in social media, where long network loops are rare or\nabsent. Our results suggest that under a weak behavioral noise, the nodes\nrobustly split into several clusters, with partial synchronization of nodes\nwithin each cluster. We also study the randomly-weighted version of the models\nwhere the nodes are allowed to change their connection strength (this can model\nattention redistribution), and show that it does facilitate the self-sustained\nactivity. \n\n"}
{"id": "1712.07897", "contents": "Title: Non-convex Optimization for Machine Learning Abstract: A vast majority of machine learning algorithms train their models and perform\ninference by solving optimization problems. In order to capture the learning\nand prediction problems accurately, structural constraints such as sparsity or\nlow rank are frequently imposed or else the objective itself is designed to be\na non-convex function. This is especially true of algorithms that operate in\nhigh-dimensional spaces or that train non-linear models such as tensor models\nand deep networks.\n  The freedom to express the learning problem as a non-convex optimization\nproblem gives immense modeling power to the algorithm designer, but often such\nproblems are NP-hard to solve. A popular workaround to this has been to relax\nnon-convex problems to convex ones and use traditional methods to solve the\n(convex) relaxed optimization problems. However this approach may be lossy and\nnevertheless presents significant challenges for large scale optimization.\n  On the other hand, direct approaches to non-convex optimization have met with\nresounding success in several domains and remain the methods of choice for the\npractitioner, as they frequently outperform relaxation-based techniques -\npopular heuristics include projected gradient descent and alternating\nminimization. However, these are often poorly understood in terms of their\nconvergence and other properties.\n  This monograph presents a selection of recent advances that bridge a\nlong-standing gap in our understanding of these heuristics. The monograph will\nlead the reader through several widely used non-convex optimization techniques,\nas well as applications thereof. The goal of this monograph is to both,\nintroduce the rich literature in this area, as well as equip the reader with\nthe tools and techniques needed to analyze these simple procedures for\nnon-convex problems. \n\n"}
{"id": "1712.08091", "contents": "Title: Multiview Deep Learning for Predicting Twitter Users' Location Abstract: The problem of predicting the location of users on large social networks like\nTwitter has emerged from real-life applications such as social unrest detection\nand online marketing. Twitter user geolocation is a difficult and active\nresearch topic with a vast literature. Most of the proposed methods follow\neither a content-based or a network-based approach. The former exploits\nuser-generated content while the latter utilizes the connection or interaction\nbetween Twitter users. In this paper, we introduce a novel method combining the\nstrength of both approaches. Concretely, we propose a multi-entry neural\nnetwork architecture named MENET leveraging the advances in deep learning and\nmultiview learning. The generalizability of MENET enables the integration of\nmultiple data representations. In the context of Twitter user geolocation, we\nrealize MENET with textual, network, and metadata features. Considering the\nnatural distribution of Twitter users across the concerned geographical area,\nwe subdivide the surface of the earth into multi-scale cells and train MENET\nwith the labels of the cells. We show that our method outperforms the state of\nthe art by a large margin on three benchmark datasets. \n\n"}
{"id": "1712.08449", "contents": "Title: True Asymptotic Natural Gradient Optimization Abstract: We introduce a simple algorithm, True Asymptotic Natural Gradient\nOptimization (TANGO), that converges to a true natural gradient descent in the\nlimit of small learning rates, without explicit Fisher matrix estimation.\n  For quadratic models the algorithm is also an instance of averaged stochastic\ngradient, where the parameter is a moving average of a \"fast\", constant-rate\ngradient descent. TANGO appears as a particular de-linearization of averaged\nSGD, and is sometimes quite different on non-quadratic models. This further\nconnects averaged SGD and natural gradient, both of which are arguably optimal\nasymptotically.\n  In large dimension, small learning rates will be required to approximate the\nnatural gradient well. Still, this shows it is possible to get arbitrarily\nclose to exact natural gradient descent with a lightweight algorithm. \n\n"}
{"id": "1712.08712", "contents": "Title: Persistence of the Jordan center in Random Growing Trees Abstract: The Jordan center of a graph is defined as a vertex whose maximum distance to\nother nodes in the graph is minimal, and it finds applications in facility\nlocation and source detection problems. We study properties of the Jordan\nCenter in the case of random growing trees. In particular, we consider a\nregular tree graph on which an infection starts from a root node and then\nspreads along the edges of the graph according to various random spread models.\nFor the Independent Cascade (IC) model and the discrete Susceptible Infected\n(SI) model, both of which are discrete time models, we show that as the\ninfected subgraph grows with time, the Jordan center persists on a single\nvertex after a finite number of timesteps. Finally, we also study the\ncontinuous time version of the SI model and bound the maximum distance between\nthe Jordan center and the root node at any time. \n\n"}
{"id": "1712.10282", "contents": "Title: Boosting the Actor with Dual Critic Abstract: This paper proposes a new actor-critic-style algorithm called Dual\nActor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian\ndual form of the Bellman optimality equation, which can be viewed as a\ntwo-player game between the actor and a critic-like function, which is named as\ndual critic. Compared to its actor-critic relatives, Dual-AC has the desired\nproperty that the actor and dual critic are updated cooperatively to optimize\nthe same objective function, providing a more transparent way for learning the\ncritic that is directly related to the objective function of the actor. We then\nprovide a concrete algorithm that can effectively solve the minimax\noptimization problem, using techniques of multi-step bootstrapping, path\nregularization, and stochastic dual ascent algorithm. We demonstrate that the\nproposed algorithm achieves the state-of-the-art performances across several\nbenchmarks. \n\n"}
{"id": "1712.10285", "contents": "Title: SBEED: Convergent Reinforcement Learning with Nonlinear Function\n  Approximation Abstract: When function approximation is used, solving the Bellman optimality equation\nwith stability guarantees has remained a major open problem in reinforcement\nlearning for decades. The fundamental difficulty is that the Bellman operator\nmay become an expansion in general, resulting in oscillating and even divergent\nbehavior of popular algorithms like Q-learning. In this paper, we revisit the\nBellman equation, and reformulate it into a novel primal-dual optimization\nproblem using Nesterov's smoothing technique and the Legendre-Fenchel\ntransformation. We then develop a new algorithm, called Smoothed Bellman Error\nEmbedding, to solve this optimization problem where any differentiable function\nclass may be used. We provide what we believe to be the first convergence\nguarantee for general nonlinear function approximation, and analyze the\nalgorithm's sample complexity. Empirically, our algorithm compares favorably to\nstate-of-the-art baselines in several benchmark control problems. \n\n"}
{"id": "1801.00753", "contents": "Title: Probabilistic supervised learning Abstract: Predictive modelling and supervised learning are central to modern data\nscience. With predictions from an ever-expanding number of supervised black-box\nstrategies - e.g., kernel methods, random forests, deep learning aka neural\nnetworks - being employed as a basis for decision making processes, it is\ncrucial to understand the statistical uncertainty associated with these\npredictions.\n  As a general means to approach the issue, we present an overarching framework\nfor black-box prediction strategies that not only predict the target but also\ntheir own predictions' uncertainty. Moreover, the framework allows for fair\nassessment and comparison of disparate prediction strategies. For this, we\nformally consider strategies capable of predicting full distributions from\nfeature variables, so-called probabilistic supervised learning strategies.\n  Our work draws from prior work including Bayesian statistics, information\ntheory, and modern supervised machine learning, and in a novel synthesis leads\nto (a) new theoretical insights such as a probabilistic bias-variance\ndecomposition and an entropic formulation of prediction, as well as to (b) new\nalgorithms and meta-algorithms, such as composite prediction strategies,\nprobabilistic boosting and bagging, and a probabilistic predictive independence\ntest.\n  Our black-box formulation also leads (c) to a new modular interface view on\nprobabilistic supervised learning and a modelling workflow API design, which we\nhave implemented in the newly released skpro machine learning toolbox,\nextending the familiar modelling interface and meta-modelling functionality of\nsklearn. The skpro package provides interfaces for construction, composition,\nand tuning of probabilistic supervised learning strategies, together with\norchestration features for validation and comparison of any such strategy - be\nit frequentist, Bayesian, or other. \n\n"}
{"id": "1801.01665", "contents": "Title: Political Discourse on Social Media: Echo Chambers, Gatekeepers, and the\n  Price of Bipartisanship Abstract: Echo chambers, i.e., situations where one is exposed only to opinions that\nagree with their own, are an increasing concern for the political discourse in\nmany democratic countries. This paper studies the phenomenon of political echo\nchambers on social media. We identify the two components in the phenomenon: the\nopinion that is shared ('echo'), and the place that allows its exposure\n('chamber' --- the social network), and examine closely at how these two\ncomponents interact. We define a production and consumption measure for\nsocial-media users, which captures the political leaning of the content shared\nand received by them. By comparing the two, we find that Twitter users are, to\na large degree, exposed to political opinions that agree with their own. We\nalso find that users who try to bridge the echo chambers, by sharing content\nwith diverse leaning, have to pay a 'price of bipartisanship' in terms of their\nnetwork centrality and content appreciation. In addition, we study the role of\n'gatekeepers', users who consume content with diverse leaning but produce\npartisan content (with a single-sided leaning), in the formation of echo\nchambers. Finally, we apply these findings to the task of predicting partisans\nand gatekeepers from social and content features. While partisan users turn out\nrelatively easy to identify, gatekeepers prove to be more challenging. \n\n"}
{"id": "1801.01953", "contents": "Title: Adversarial Perturbation Intensity Achieving Chosen Intra-Technique\n  Transferability Level for Logistic Regression Abstract: Machine Learning models have been shown to be vulnerable to adversarial\nexamples, ie. the manipulation of data by a attacker to defeat a defender's\nclassifier at test time. We present a novel probabilistic definition of\nadversarial examples in perfect or limited knowledge setting using prior\nprobability distributions on the defender's classifier. Using the asymptotic\nproperties of the logistic regression, we derive a closed-form expression of\nthe intensity of any adversarial perturbation, in order to achieve a given\nexpected misclassification rate. This technique is relevant in a threat model\nof known model specifications and unknown training data. To our knowledge, this\nis the first method that allows an attacker to directly choose the probability\nof attack success. We evaluate our approach on two real-world datasets. \n\n"}
{"id": "1801.02818", "contents": "Title: k-connectivity of Random Graphs and Random Geometric Graphs in Node\n  Fault Model Abstract: k-connectivity of random graphs is a fundamental property indicating\nreliability of multi-hop wireless sensor networks (WSN). WSNs comprising of\nsensor nodes with limited power resources are modeled by random graphs with\nunreliable nodes, which is known as the node fault model. In this paper, we\ninvestigate k-connectivity of random graphs in the node fault model by\nevaluating the network breakdown probability, i.e., the disconnectivity\nprobability of random graphs after stochastic node removals. Using the notion\nof a strongly typical set, we obtain universal asymptotic upper and lower\nbounds of the network breakdown probability. The bounds are applicable both to\nrandom graphs and to random geometric graphs. We then consider three\nrepresentative random graph ensembles: the Erdos-Renyi random graph as the\nsimplest case, the random intersection graph for WSNs with random key\npredistribution schemes, and the random geometric graph as a model of WSNs\ngenerated by random sensor node deployment. The bounds unveil the existence of\nthe phase transition of the network breakdown probability for those ensembles. \n\n"}
{"id": "1801.04053", "contents": "Title: Noisy Expectation-Maximization: Applications and Generalizations Abstract: We present a noise-injected version of the Expectation-Maximization (EM)\nalgorithm: the Noisy Expectation Maximization (NEM) algorithm. The NEM\nalgorithm uses noise to speed up the convergence of the EM algorithm. The NEM\ntheorem shows that injected noise speeds up the average convergence of the EM\nalgorithm to a local maximum of the likelihood surface if a positivity\ncondition holds. The generalized form of the noisy expectation-maximization\n(NEM) algorithm allow for arbitrary modes of noise injection including adding\nand multiplying noise to the data.\n  We demonstrate these noise benefits on EM algorithms for the Gaussian mixture\nmodel (GMM) with both additive and multiplicative NEM noise injection. A\nseparate theorem (not presented here) shows that the noise benefit for\nindependent identically distributed additive noise decreases with sample size\nin mixture models. This theorem implies that the noise benefit is most\npronounced if the data is sparse. Injecting blind noise only slowed\nconvergence. \n\n"}
{"id": "1801.04898", "contents": "Title: Network assembly of scientific communities of varying size and\n  specificity Abstract: How does the collaboration network of researchers coalesce around a\nscientific topic? What sort of social restructuring occurs as a new field\ndevelops? Previous empirical explorations of these questions have examined the\nevolution of co-authorship networks associated with several fields of science,\neach noting a characteristic shift in network structure as fields develop.\nHistorically, however, such studies have tended to rely on manually annotated\ndatasets and therefore only consider a handful of disciplines, calling into\nquestion the universality of the observed structural signature.To overcome this\nlimitation and test the robustness of this phenomenon, we use a comprehensive\ndataset of over 189,000 scientific articles and develop a framework for\npartitioning articles and their authors into coherent, semantically-related\ngroups representing scientific fields of varying size and specificity. We then\nuse the resulting population of fields to study the structure of evolving\nco-authorship networks. Consistent with earlier findings, we observe a global\ntopological transition as the co-authorship networks coalesce from a disjointed\naggregate into a dense giant connected component that dominates the network. We\nvalidate these results using a separate, complimentary corpus of scientific\narticles, and, overall, we find that the previously reported characteristic\nstructural evolution of a scientific field's associated co-authorship network\nis robust across a large number of scientific fields of varying size, scope,\nand specificity. Additionally, the framework developed in this study may be\nused in other scientometric contexts in order to extend studies to compare\nacross a larger range of scientific disciplines. \n\n"}
{"id": "1801.05399", "contents": "Title: A networked voting rule for democratic representation Abstract: We introduce a general framework for exploring the problem of selecting a\ncommittee of representatives with the aim of studying a networked voting rule\nbased on a decentralized large-scale platform, which can assure a strong\naccountability of the elected. The results of our simulations suggest that this\nalgorithm-based approach is able to obtain a high representativeness for\nrelatively small committees, performing even better than a classical voting\nrule based on a closed list of candidates. We show that a general relation\nbetween committee size and representatives exists in the form of an inverse\nsquare root law and that the normalized committee size approximately scales\nwith the inverse of the community size, allowing the scalability to very large\npopulations. These findings are not strongly influenced by the different\nnetworks used to describe the individuals interactions, except for the presence\nof few individuals with very high connectivity which can have a marginally\nnegative effect in the committee selection process. \n\n"}
{"id": "1801.05512", "contents": "Title: Deep Neural Networks for Survival Analysis Based on a Multi-Task\n  Framework Abstract: Survival analysis/time-to-event models are extremely useful as they can help\ncompanies predict when a customer will buy a product, churn or default on a\nloan, and therefore help them improve their ROI. In this paper, we introduce a\nnew method to calculate survival functions using the Multi-Task Logistic\nRegression (MTLR) model as its base and a deep learning architecture as its\ncore. Based on the Concordance index (C-index) and Brier score, this method\noutperforms the MTLR in all the experiments disclosed in this paper as well as\nthe Cox Proportional Hazard (CoxPH) model when nonlinear dependencies are\nfound. \n\n"}
{"id": "1801.06159", "contents": "Title: When Does Stochastic Gradient Algorithm Work Well? Abstract: In this paper, we consider a general stochastic optimization problem which is\noften at the core of supervised learning, such as deep learning and linear\nclassification. We consider a standard stochastic gradient descent (SGD) method\nwith a fixed, large step size and propose a novel assumption on the objective\nfunction, under which this method has the improved convergence rates (to a\nneighborhood of the optimal solutions). We then empirically demonstrate that\nthese assumptions hold for logistic regression and standard deep neural\nnetworks on classical data sets. Thus our analysis helps to explain when\nefficient behavior can be expected from the SGD method in training\nclassification models and deep neural networks. \n\n"}
{"id": "1801.06161", "contents": "Title: Topic Lifecycle on Social Networks: Analyzing the Effects of Semantic\n  Continuity and Social Communities Abstract: Topic lifecycle analysis on Twitter, a branch of study that investigates\nTwitter topics from their birth through lifecycle to death, has gained immense\nmainstream research popularity. In the literature, topics are often treated as\none of (a) hashtags (independent from other hashtags), (b) a burst of keywords\nin a short time span or (c) a latent concept space captured by advanced text\nanalysis methodologies, such as Latent Dirichlet Allocation (LDA). The first\ntwo approaches are not capable of recognizing topics where different users use\ndifferent hashtags to express the same concept (semantically related), while\nthe third approach misses out the user's explicit intent expressed via\nhashtags. In our work, we use a word embedding based approach to cluster\ndifferent hashtags together, and the temporal concurrency of the hashtag\nusages, thus forming topics (a semantically and temporally related group of\nhashtags).We present a novel analysis of topic lifecycles with respect to\ncommunities. We characterize the participation of social communities in the\ntopic clusters, and analyze the lifecycle of topic clusters with respect to\nsuch participation. We derive first-of-its-kind novel insights with respect to\nthe complex evolution of topics over communities and time: temporal morphing of\ntopics over hashtags within communities, how the hashtags die in some\ncommunities but morph into some other hashtags in some other communities (that,\nit is a community-level phenomenon), and how specific communities adopt to\nspecific hashtags. Our work is fundamental in the space of topic lifecycle\nmodeling and understanding in communities: it redefines our understanding of\ntopic lifecycles and shows that the social boundaries of topic lifecycles are\ndeeply ingrained with community behavior. \n\n"}
{"id": "1801.06818", "contents": "Title: Community Recovery in a Preferential Attachment Graph Abstract: A message passing algorithm is derived for recovering communities within a\ngraph generated by a variation of the Barab\\'{a}si-Albert preferential\nattachment model. The estimator is assumed to know the arrival times, or order\nof attachment, of the vertices. The derivation of the algorithm is based on\nbelief propagation under an independence assumption. Two precursors to the\nmessage passing algorithm are analyzed: the first is a degree thresholding (DT)\nalgorithm and the second is an algorithm based on the arrival times of the\nchildren (C) of a given vertex, where the children of a given vertex are the\nvertices that attached to it. Comparison of the performance of the algorithms\nshows it is beneficial to know the arrival times, not just the number, of the\nchildren. The probability of correct classification of a vertex is\nasymptotically determined by the fraction of vertices arriving before it. Two\nextensions of Algorithm C are given: the first is based on joint likelihood of\nthe children of a fixed set of vertices; it can sometimes be used to seed the\nmessage passing algorithm. The second is the message passing algorithm.\nSimulation results are given. \n\n"}
{"id": "1801.07150", "contents": "Title: A Novel Weighted Distance Measure for Multi-Attributed Graph Abstract: Due to exponential growth of complex data, graph structure has become\nincreasingly important to model various entities and their interactions, with\nmany interesting applications including, bioinformatics, social network\nanalysis, etc. Depending on the complexity of the data, the underlying graph\nmodel can be a simple directed/undirected and/or weighted/un-weighted graph to\na complex graph (aka multi-attributed graph) where vertices and edges are\nlabelled with multi-dimensional vectors. In this paper, we present a novel\nweighted distance measure based on weighted Euclidean norm which is defined as\na function of both vertex and edge attributes, and it can be used for various\ngraph analysis tasks including classification and cluster analysis. The\nproposed distance measure has flexibility to increase/decrease the weightage of\nedge labels while calculating the distance between vertex-pairs. We have also\nproposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV\nfiles containing the list of vertex vectors and edge vectors, and calculates\nthe distance between each vertex-pair using the proposed weighted distance\nmeasure. Finally, we have proposed a multi-attributed similarity graph\ngeneration algorithm, MAGSim, which reads the output of MAGDist algorithm and\ngenerates a similarity graph that can be analysed using classification and\nclustering algorithms. The significance and accuracy of the proposed distance\nmeasure and algorithms is evaluated on Iris and Twitter data sets, and it is\nfound that the similarity graph generated by our proposed method yields better\nclustering results than the existing similarity graph generation methods. \n\n"}
{"id": "1801.07316", "contents": "Title: The Hybrid Bootstrap: A Drop-in Replacement for Dropout Abstract: Regularization is an important component of predictive model building. The\nhybrid bootstrap is a regularization technique that functions similarly to\ndropout except that features are resampled from other training points rather\nthan replaced with zeros. We show that the hybrid bootstrap offers superior\nperformance to dropout. We also present a sampling based technique to simplify\nhyperparameter choice. Next, we provide an alternative sampling technique for\nconvolutional neural networks. Finally, we demonstrate the efficacy of the\nhybrid bootstrap on non-image tasks using tree-based models. \n\n"}
{"id": "1801.07988", "contents": "Title: Understanding news story chains using information retrieval and network\n  clustering techniques Abstract: Content analysis of news stories (whether manual or automatic) is a\ncornerstone of the communication studies field. However, much research is\nconducted at the level of individual news articles, despite the fact that news\nevents (especially significant ones) are frequently presented as \"stories\" by\nnews outlets: chains of connected articles covering the same event from\ndifferent angles. These stories are theoretically highly important in terms of\nincreasing public recall of news items and enhancing the agenda-setting power\nof the press. Yet thus far, the field has lacked an efficient method for\ndetecting groups of articles which form stories in a way that enables their\nanalysis.\n  In this work, we present a novel, automated method for identifying linked\nnews stories from within a corpus of articles. This method makes use of\ntechniques drawn from the field of information retrieval to identify textual\ncloseness of pairs of articles, and then clustering techniques taken from the\nfield of network analysis to group these articles into stories. We demonstrate\nthe application of the method to a corpus of 61,864 articles, and show how it\ncan efficiently identify valid story clusters within the corpus. We use the\nresults to make observations about the prevalence and dynamics of stories\nwithin the UK news media, showing that more than 50% of news production takes\nplace within stories. \n\n"}
{"id": "1801.08196", "contents": "Title: Incremental Eigenpair Computation for Graph Laplacian Matrices: Theory\n  and Applications Abstract: The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)\nof a graph Laplacian matrix have been widely used in spectral clustering and\ncommunity detection. However, in real-life applications the number of clusters\nor communities (say, $K$) is generally unknown a-priori. Consequently, the\nmajority of the existing methods either choose $K$ heuristically or they repeat\nthe clustering method with different choices of $K$ and accept the best\nclustering result. The first option, more often, yields suboptimal result,\nwhile the second option is computationally expensive. In this work, we propose\nan incremental method for constructing the eigenspectrum of the graph Laplacian\nmatrix. This method leverages the eigenstructure of graph Laplacian matrix to\nobtain the $K$-th smallest eigenpair of the Laplacian matrix given a collection\nof all previously computed $K-1$ smallest eigenpairs. Our proposed method\nadapts the Laplacian matrix such that the batch eigenvalue decomposition\nproblem transforms into an efficient sequential leading eigenpair computation\nproblem. As a practical application, we consider user-guided spectral\nclustering. Specifically, we demonstrate that users can utilize the proposed\nincremental method for effective eigenpair computation and for determining the\ndesired number of clusters based on multiple clustering metrics. \n\n"}
{"id": "1801.08694", "contents": "Title: PDNet: Semantic Segmentation integrated with a Primal-Dual Network for\n  Document binarization Abstract: Binarization of digital documents is the task of classifying each pixel in an\nimage of the document as belonging to the background (parchment/paper) or\nforeground (text/ink). Historical documents are often subjected to\ndegradations, that make the task challenging. In the current work a deep neural\nnetwork architecture is proposed that combines a fully convolutional network\nwith an unrolled primal-dual network that can be trained end-to-end to achieve\nstate of the art binarization on four out of seven datasets. Document\nbinarization is formulated as an energy minimization problem. A fully\nconvolutional neural network is trained for semantic segmentation of pixels\nthat provides labeling cost associated with each pixel. This cost estimate is\nrefined along the edges to compensate for any over or under estimation of the\nforeground class using a primal-dual approach. We provide necessary overview on\nproximal operator that facilitates theoretical underpinning required to train a\nprimal-dual network using a gradient descent algorithm. Numerical instabilities\nencountered due to the recurrent nature of primal-dual approach are handled. We\nprovide experimental results on document binarization competition dataset along\nwith network changes and hyperparameter tuning required for stability and\nperformance of the network. The network when pre-trained on synthetic dataset\nperforms better as per the competition metrics. \n\n"}
{"id": "1801.09333", "contents": "Title: Detecting the impact of public transit on the transmission of epidemics Abstract: In many developing countries, public transit plays an important role in daily\nlife. However, few existing methods have considered the influence of public\ntransit in their models. In this work, we present a dual-perspective view of\nthe epidemic spreading process of the individual that involves both\ncontamination in places (such as work places and homes) and public transit\n(such as buses and trains). In more detail, we consider a group of individuals\nwho travel to some places using public transit, and introduce public transit\ninto the epidemic spreading process. A novel modeling framework is proposed\nconsidering place-based infections and the public-transit-based infections. In\nthe urban scenario, we investigate the public transit trip contribution rate\n(PTTCR) in the epidemic spreading process of the individual, and assess the\nimpact of the public transit trip contribution rate by evaluating the volume of\ninfectious people. Scenarios for strategies such as public transit and school\nclosure were tested and analyzed. Our simulation results suggest that\nindividuals with a high public transit trip contribution rate will increase the\nvolume of infectious people when an infectious disease outbreak occurs by\naffecting the social network through the public transit trip contribution rate. \n\n"}
{"id": "1801.09829", "contents": "Title: Weighted Community Detection and Data Clustering Using Message Passing Abstract: Grouping objects into clusters based on similarities or weights between them\nis one of the most important problems in science and engineering. In this work,\nby extending message passing algorithms and spectral algorithms proposed for\nunweighted community detection problem, we develop a non-parametric method\nbased on statistical physics, by mapping the problem to Potts model at the\ncritical temperature of spin glass transition and applying belief propagation\nto solve the marginals corresponding to the Boltzmann distribution. Our\nalgorithm is robust to over-fitting and gives a principled way to determine\nwhether there are significant clusters in the data and how many clusters there\nare. We apply our method to different clustering tasks and use extensive\nnumerical experiments to illustrate the advantage of our method over existing\nalgorithms. In the community detection problem in weighted and directed\nnetworks, we show that our algorithm significantly outperforms existing\nalgorithms. In the clustering problem when the data was generated by mixture\nmodels in the sparse regime we show that our method works to the theoretical\nlimit of detectability and gives accuracy very close to that of the optimal\nBayesian inference. In the semi-supervised clustering problem, our method only\nneeds several labels to work perfectly in classic datasets. Finally, we further\ndevelop Thouless-Anderson-Palmer equations which reduce heavily the computation\ncomplexity in dense-networks but gives almost the same performance as belief\npropagation. \n\n"}
{"id": "1801.09870", "contents": "Title: Fast Power system security analysis with Guided Dropout Abstract: We propose a new method to efficiently compute load-flows (the steady-state\nof the power-grid for given productions, consumptions and grid topology),\nsubstituting conventional simulators based on differential equation solvers. We\nuse a deep feed-forward neural network trained with load-flows precomputed by\nsimulation. Our architecture permits to train a network on so-called \"n-1\"\nproblems, in which load flows are evaluated for every possible line\ndisconnection, then generalize to \"n-2\" problems without retraining (a clear\nadvantage because of the combinatorial nature of the problem). To that end, we\ndeveloped a technique bearing similarity with \"dropout\", which we named \"guided\ndropout\". \n\n"}
{"id": "1802.00130", "contents": "Title: Distributed Newton Methods for Deep Neural Networks Abstract: Deep learning involves a difficult non-convex optimization problem with a\nlarge number of weights between any two adjacent layers of a deep structure. To\nhandle large data sets or complicated networks, distributed training is needed,\nbut the calculation of function, gradient, and Hessian is expensive. In\nparticular, the communication and the synchronization cost may become a\nbottleneck. In this paper, we focus on situations where the model is\ndistributedly stored, and propose a novel distributed Newton method for\ntraining deep neural networks. By variable and feature-wise data partitions,\nand some careful designs, we are able to explicitly use the Jacobian matrix for\nmatrix-vector products in the Newton method. Some techniques are incorporated\nto reduce the running time as well as the memory consumption. First, to reduce\nthe communication cost, we propose a diagonalization method such that an\napproximate Newton direction can be obtained without communication between\nmachines. Second, we consider subsampled Gauss-Newton matrices for reducing the\nrunning time as well as the communication cost. Third, to reduce the\nsynchronization cost, we terminate the process of finding an approximate Newton\ndirection even though some nodes have not finished their tasks. Details of some\nimplementation issues in distributed environments are thoroughly investigated.\nExperiments demonstrate that the proposed method is effective for the\ndistributed training of deep neural networks. In compared with stochastic\ngradient methods, it is more robust and may give better test accuracy. \n\n"}
{"id": "1802.00393", "contents": "Title: Large Scale Crowdsourcing and Characterization of Twitter Abusive\n  Behavior Abstract: In recent years, offensive, abusive and hateful language, sexism, racism and\nother types of aggressive and cyberbullying behavior have been manifesting with\nincreased frequency, and in many online social media platforms. In fact, past\nscientific work focused on studying these forms in popular media, such as\nFacebook and Twitter. Building on such work, we present an 8-month study of the\nvarious forms of abusive behavior on Twitter, in a holistic fashion. Departing\nfrom past work, we examine a wide variety of labeling schemes, which cover\ndifferent forms of abusive behavior, at the same time. We propose an\nincremental and iterative methodology, that utilizes the power of crowdsourcing\nto annotate a large scale collection of tweets with a set of abuse-related\nlabels. In fact, by applying our methodology including statistical analysis for\nlabel merging or elimination, we identify a reduced but robust set of labels.\nFinally, we offer a first overview and findings of our collected and annotated\ndataset of 100 thousand tweets, which we make publicly available for further\nscientific exploration. \n\n"}
{"id": "1802.01400", "contents": "Title: Polarization and Fake News: Early Warning of Potential Misinformation\n  Targets Abstract: Users polarization and confirmation bias play a key role in misinformation\nspreading on online social media. Our aim is to use this information to\ndetermine in advance potential targets for hoaxes and fake news. In this paper,\nwe introduce a general framework for promptly identifying polarizing content on\nsocial media and, thus, \"predicting\" future fake news topics. We validate the\nperformances of the proposed methodology on a massive Italian Facebook dataset,\nshowing that we are able to identify topics that are susceptible to\nmisinformation with 77% accuracy. Moreover, such information may be embedded as\na new feature in an additional classifier able to recognize fake news with 91%\naccuracy. The novelty of our approach consists in taking into account a series\nof characteristics related to users behavior on online social media, making a\nfirst, important step towards the smoothing of polarization and the mitigation\nof misinformation phenomena. \n\n"}
{"id": "1802.03567", "contents": "Title: Crit\\`eres de qualit\\'e d'un classifieur g\\'en\\'eraliste Abstract: This paper considers the problem of choosing a good classifier. For each\nproblem there exist an optimal classifier, but none are optimal, regarding the\nerror rate, in all cases. Because there exists a large number of classifiers, a\nuser would rather prefer an all-purpose classifier that is easy to adjust, in\nthe hope that it will do almost as good as the optimal. In this paper we\nestablish a list of criteria that a good generalist classifier should satisfy .\nWe first discuss data analytic, these criteria are presented. Six among the\nmost popular classifiers are selected and scored according to these criteria.\nTables allow to easily appreciate the relative values of each. In the end,\nrandom forests turn out to be the best classifiers. \n\n"}
{"id": "1802.03676", "contents": "Title: Differentiable Dynamic Programming for Structured Prediction and\n  Attention Abstract: Dynamic programming (DP) solves a variety of structured combinatorial\nproblems by iteratively breaking them down into smaller subproblems. In spite\nof their versatility, DP algorithms are usually non-differentiable, which\nhampers their use as a layer in neural networks trained by backpropagation. To\naddress this issue, we propose to smooth the max operator in the dynamic\nprogramming recursion, using a strongly convex regularizer. This allows to\nrelax both the optimal value and solution of the original combinatorial\nproblem, and turns a broad class of DP algorithms into differentiable\noperators. Theoretically, we provide a new probabilistic perspective on\nbackpropagating through these DP operators, and relate them to inference in\ngraphical models. We derive two particular instantiations of our framework, a\nsmoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm\nfor time-series alignment. We showcase these instantiations on two structured\nprediction tasks and on structured and sparse attention for neural machine\ntranslation. \n\n"}
{"id": "1802.04668", "contents": "Title: Weakly supervised collective feature learning from curated media Abstract: The current state-of-the-art in feature learning relies on the supervised\nlearning of large-scale datasets consisting of target content items and their\nrespective category labels. However, constructing such large-scale\nfully-labeled datasets generally requires painstaking manual effort. One\npossible solution to this problem is to employ community contributed text tags\nas weak labels, however, the concepts underlying a single text tag strongly\ndepends on the users. We instead present a new paradigm for learning\ndiscriminative features by making full use of the human curation process on\nsocial networking services (SNSs). During the process of content curation, SNS\nusers collect content items manually from various sources and group them by\ncontext, all for their own benefit. Due to the nature of this process, we can\nassume that (1) content items in the same group share the same semantic concept\nand (2) groups sharing the same images might have related semantic concepts.\nThrough these insights, we can define human curated groups as weak labels from\nwhich our proposed framework can learn discriminative features as a\nrepresentation in the space of semantic concepts the users intended when\ncreating the groups. We show that this feature learning can be formulated as a\nproblem of link prediction for a bipartite graph whose nodes corresponds to\ncontent items and human curated groups, and propose a novel method for feature\nlearning based on sparse coding or network fine-tuning. \n\n"}
{"id": "1802.04687", "contents": "Title: Neural Relational Inference for Interacting Systems Abstract: Interacting systems are prevalent in nature, from dynamical systems in\nphysics to complex societal dynamics. The interplay of components can give rise\nto complex behavior, which can often be explained using a simple model of the\nsystem's constituent parts. In this work, we introduce the neural relational\ninference (NRI) model: an unsupervised model that learns to infer interactions\nwhile simultaneously learning the dynamics purely from observational data. Our\nmodel takes the form of a variational auto-encoder, in which the latent code\nrepresents the underlying interaction graph and the reconstruction is based on\ngraph neural networks. In experiments on simulated physical systems, we show\nthat our NRI model can accurately recover ground-truth interactions in an\nunsupervised manner. We further demonstrate that we can find an interpretable\nstructure and predict complex dynamics in real motion capture and sports\ntracking data. \n\n"}
{"id": "1802.04911", "contents": "Title: Large-Scale Sparse Inverse Covariance Estimation via Thresholding and\n  Max-Det Matrix Completion Abstract: The sparse inverse covariance estimation problem is commonly solved using an\n$\\ell_{1}$-regularized Gaussian maximum likelihood estimator known as\n\"graphical lasso\", but its computational cost becomes prohibitive for large\ndata sets. A recent line of results showed--under mild assumptions--that the\ngraphical lasso estimator can be retrieved by soft-thresholding the sample\ncovariance matrix and solving a maximum determinant matrix completion (MDMC)\nproblem. This paper proves an extension of this result, and describes a\nNewton-CG algorithm to efficiently solve the MDMC problem. Assuming that the\nthresholded sample covariance matrix is sparse with a sparse Cholesky\nfactorization, we prove that the algorithm converges to an $\\epsilon$-accurate\nsolution in $O(n\\log(1/\\epsilon))$ time and $O(n)$ memory. The algorithm is\nhighly efficient in practice: we solve the associated MDMC problems with as\nmany as 200,000 variables to 7-9 digits of accuracy in less than an hour on a\nstandard laptop computer running MATLAB. \n\n"}
{"id": "1802.05187", "contents": "Title: On the Blindspots of Convolutional Networks Abstract: Deep convolutional network has been the state-of-the-art approach for a wide\nvariety of tasks over the last few years. Its successes have, in many cases,\nturned it into the default model in quite a few domains. In this work, we will\ndemonstrate that convolutional networks have limitations that may, in some\ncases, hinder it from learning properties of the data, which are easily\nrecognizable by traditional, less demanding, models. To this end, we present a\nseries of competitive analysis studies on image recognition and text analysis\ntasks, for which convolutional networks are known to provide state-of-the-art\nresults. In our studies, we inject a truth-revealing signal, indiscernible for\nthe network, thus hitting time and again the network's blind spots. The signal\ndoes not impair the network's existing performances, but it does provide an\nopportunity for a significant performance boost by models that can capture it.\nThe various forms of the carefully designed signals shed a light on the\nstrengths and weaknesses of convolutional network, which may provide insights\nfor both theoreticians that study the power of deep architectures, and for\npractitioners that consider applying convolutional networks to the task at\nhand. \n\n"}
{"id": "1802.05355", "contents": "Title: The Role of Information Complexity and Randomization in Representation\n  Learning Abstract: A grand challenge in representation learning is to learn the different\nexplanatory factors of variation behind the high dimen- sional data. Encoder\nmodels are often determined to optimize performance on training data when the\nreal objective is to generalize well to unseen data. Although there is enough\nnumerical evidence suggesting that noise injection (during training) at the\nrepresentation level might improve the generalization ability of encoders, an\ninformation-theoretic understanding of this principle remains elusive. This\npaper presents a sample-dependent bound on the generalization gap of the\ncross-entropy loss that scales with the information complexity (IC) of the\nrepresentations, meaning the mutual information between inputs and their\nrepresentations. The IC is empirically investigated for standard multi-layer\nneural networks with SGD on MNIST and CIFAR-10 datasets; the behaviour of the\ngap and the IC appear to be in direct correlation, suggesting that SGD selects\nencoders to implicitly minimize the IC. We specialize the IC to study the role\nof Dropout on the generalization capacity of deep encoders which is shown to be\ndirectly related to the encoder capacity, being a measure of the\ndistinguishability among samples from their representations. Our results\nsupport some recent regularization methods. \n\n"}
{"id": "1802.05431", "contents": "Title: On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo Abstract: We provide convergence guarantees in Wasserstein distance for a variety of\nvariance-reduction methods: SAGA Langevin diffusion, SVRG Langevin diffusion\nand control-variate underdamped Langevin diffusion. We analyze these methods\nunder a uniform set of assumptions on the log-posterior distribution, assuming\nit to be smooth, strongly convex and Hessian Lipschitz. This is achieved by a\nnew proof technique combining ideas from finite-sum optimization and the\nanalysis of sampling methods. Our sharp theoretical bounds allow us to identify\nregimes of interest where each method performs better than the others. Our\ntheory is verified with experiments on real-world and synthetic datasets. \n\n"}
{"id": "1802.05453", "contents": "Title: Black Hole Metric: Overcoming the PageRank Normalization Problem Abstract: In network science, there is often the need to sort the graph nodes. While\nthe sorting strategy may be different, in general sorting is performed by\nexploiting the network structure. In particular, the metric PageRank has been\nused in the past decade in different ways to produce a ranking based on how\nmany neighbors point to a specific node. PageRank is simple, easy to compute\nand effective in many applications, however it comes with a price: as PageRank\nis an application of the random walker, the arc weights need to be normalized.\nThis normalization, while necessary, introduces a series of unwanted\nside-effects. In this paper, we propose a generalization of PageRank named\nBlack Hole Metric which mitigates the problem. We devise a scenario in which\nthe side-effects are particularily impactful on the ranking, test the new\nmetric in both real and synthetic networks, and show the results. \n\n"}
{"id": "1802.06015", "contents": "Title: Diversity from the Topology of Citation Networks Abstract: We study transitivity in directed acyclic graphs and its usefulness in\ncapturing nodes that act as bridges between more densely interconnected parts\nin such type of network. In transitively reduced citation networks degree\ncentrality could be used as a measure of interdisciplinarity or diversity. We\nstudy the measure's ability to capture \"diverse\" nodes in random directed\nacyclic graphs and citation networks. We show that transitively reduced degree\ncentrality is capable of capturing \"diverse\" nodes, thus this measure could be\na timely alternative to text analysis techniques for retrieving papers,\ninfluential in a variety of research fields. \n\n"}
{"id": "1802.06189", "contents": "Title: Contrast Subgraph Mining from Coherent Cores Abstract: Graph pattern mining methods can extract informative and useful patterns from\nlarge-scale graphs and capture underlying principles through the overwhelmed\ninformation. Contrast analysis serves as a keystone in various fields and has\ndemonstrated its effectiveness in mining valuable information. However, it has\nbeen long overlooked in graph pattern mining. Therefore, in this paper, we\nintroduce the concept of contrast subgraph, that is, a subset of nodes that\nhave significantly different edges or edge weights in two given graphs of the\nsame node set. The major challenge comes from the gap between the contrast and\nthe informativeness. Because of the widely existing noise edges in real-world\ngraphs, the contrast may lead to subgraphs of pure noise. To avoid such\nmeaningless subgraphs, we leverage the similarity as the cornerstone of the\ncontrast. Specifically, we first identify a coherent core, which is a small\nsubset of nodes with similar edge structures in the two graphs, and then induce\ncontrast subgraphs from the coherent cores. Moreover, we design a general\nfamily of coherence and contrast metrics and derive a polynomial-time algorithm\nto efficiently extract contrast subgraphs. Extensive experiments verify the\nnecessity of introducing coherent cores as well as the effectiveness and\nefficiency of our algorithm. Real-world applications demonstrate the tremendous\npotentials of contrast subgraph mining. \n\n"}
{"id": "1802.06807", "contents": "Title: On the Complexity of Opinions and Online Discussions Abstract: In an increasingly polarized world, demagogues who reduce complexity down to\nsimple arguments based on emotion are gaining in popularity. Are opinions and\nonline discussions falling into demagoguery? In this work, we aim to provide\ncomputational tools to investigate this question and, by doing so, explore the\nnature and complexity of online discussions and their space of opinions,\nuncovering where each participant lies.\n  More specifically, we present a modeling framework to construct latent\nrepresentations of opinions in online discussions which are consistent with\nhuman judgements, as measured by online voting. If two opinions are close in\nthe resulting latent space of opinions, it is because humans think they are\nsimilar. Our modeling framework is theoretically grounded and establishes a\nsurprising connection between opinions and voting models and the sign-rank of a\nmatrix. Moreover, it also provides a set of practical algorithms to both\nestimate the dimension of the latent space of opinions and infer where opinions\nexpressed by the participants of an online discussion lie in this space.\nExperiments on a large dataset from Yahoo! News, Yahoo! Finance, Yahoo! Sports,\nand the Newsroom app suggest that unidimensional opinion models may often be\nunable to accurately represent online discussions, provide insights into human\njudgements and opinions, and show that our framework is able to circumvent\nlanguage nuances such as sarcasm or humor by relying on human judgements\ninstead of textual analysis. \n\n"}
{"id": "1802.06903", "contents": "Title: Generalization Error Bounds with Probabilistic Guarantee for SGD in\n  Nonconvex Optimization Abstract: The success of deep learning has led to a rising interest in the\ngeneralization property of the stochastic gradient descent (SGD) method, and\nstability is one popular approach to study it. Existing works based on\nstability have studied nonconvex loss functions, but only considered the\ngeneralization error of the SGD in expectation. In this paper, we establish\nvarious generalization error bounds with probabilistic guarantee for the SGD.\nSpecifically, for both general nonconvex loss functions and gradient dominant\nloss functions, we characterize the on-average stability of the iterates\ngenerated by SGD in terms of the on-average variance of the stochastic\ngradients. Such characterization leads to improved bounds for the\ngeneralization error for SGD. We then study the regularized risk minimization\nproblem with strongly convex regularizers, and obtain improved generalization\nerror bounds for proximal SGD. With strongly convex regularizers, we further\nestablish the generalization error bounds for nonconvex loss functions under\nproximal SGD with high-probability guarantee, i.e., exponential concentration\nin probability. \n\n"}
{"id": "1802.07985", "contents": "Title: Community Detection with Metadata in a Network of Biographies of Western\n  Art Painters Abstract: In this work we look at the structure of the influences between Western art\npainters as revealed by their biographies on Wikipedia. We use a modified\nversion of modularity maximisation with metadata to detect a partition of\nartists into communities based on their artistic genre and school in which they\nbelong. We then use this community structure to discuss how influential artists\nreached beyond their own communities and had a lasting impact on others, by\nproposing modifications on standard centrality measures. \n\n"}
{"id": "1802.08757", "contents": "Title: Fully Decentralized Multi-Agent Reinforcement Learning with Networked\n  Agents Abstract: We consider the problem of \\emph{fully decentralized} multi-agent\nreinforcement learning (MARL), where the agents are located at the nodes of a\ntime-varying communication network. Specifically, we assume that the reward\nfunctions of the agents might correspond to different tasks, and are only known\nto the corresponding agent. Moreover, each agent makes individual decisions\nbased on both the information observed locally and the messages received from\nits neighbors over the network. Within this setting, the collective goal of the\nagents is to maximize the globally averaged return over the network through\nexchanging information with their neighbors. To this end, we propose two\ndecentralized actor-critic algorithms with function approximation, which are\napplicable to large-scale MARL problems where both the number of states and the\nnumber of agents are massively large. Under the decentralized structure, the\nactor step is performed individually by each agent with no need to infer the\npolicies of others. For the critic step, we propose a consensus update via\ncommunication over the network. Our algorithms are fully incremental and can be\nimplemented in an online fashion. Convergence analyses of the algorithms are\nprovided when the value functions are approximated within the class of linear\nfunctions. Extensive simulation results with both linear and nonlinear function\napproximations are presented to validate the proposed algorithms. Our work\nappears to be the first study of fully decentralized MARL algorithms for\nnetworked agents with function approximation, with provable convergence\nguarantees. \n\n"}
{"id": "1802.09210", "contents": "Title: A representer theorem for deep neural networks Abstract: We propose to optimize the activation functions of a deep neural network by\nadding a corresponding functional regularization to the cost function. We\njustify the use of a second-order total-variation criterion. This allows us to\nderive a general representer theorem for deep neural networks that makes a\ndirect connection with splines and sparsity. Specifically, we show that the\noptimal network configuration can be achieved with activation functions that\nare nonuniform linear splines with adaptive knots. The bottom line is that the\naction of each neuron is encoded by a spline whose parameters (including the\nnumber of knots) are optimized during the training procedure. The scheme\nresults in a computational structure that is compatible with the existing\ndeep-ReLU, parametric ReLU, APL (adaptive piecewise-linear) and MaxOut\narchitectures. It also suggests novel optimization challenges, while making the\nlink with $\\ell_1$ minimization and sparsity-promoting techniques explicit. \n\n"}
{"id": "1802.09511", "contents": "Title: Missing Data in Sparse Transition Matrix Estimation for Sub-Gaussian\n  Vector Autoregressive Processes Abstract: High-dimensional time series data exist in numerous areas such as finance,\ngenomics, healthcare, and neuroscience. An unavoidable aspect of all such\ndatasets is missing data, and dealing with this issue has been an important\nfocus in statistics, control, and machine learning. In this work, we consider a\nhigh-dimensional estimation problem where a dynamical system, governed by a\nstable vector autoregressive model, is randomly and only partially observed at\neach time point. Our task amounts to estimating the transition matrix, which is\nassumed to be sparse. In such a scenario, where covariates are highly\ninterdependent and partially missing, new theoretical challenges arise. While\ntransition matrix estimation in vector autoregressive models has been studied\npreviously, the missing data scenario requires separate efforts. Moreover,\nwhile transition matrix estimation can be studied from a high-dimensional\nsparse linear regression perspective, the covariates are highly dependent and\nexisting results on regularized estimation with missing data from\ni.i.d.~covariates are not applicable. At the heart of our analysis lies 1) a\nnovel concentration result when the innovation noise satisfies the convex\nconcentration property, as well as 2) a new quantity for characterizing the\ninteractions of the time-varying observation process with the underlying\ndynamical system. \n\n"}
{"id": "1802.09786", "contents": "Title: Preferential attachment mechanism of complex network growth:\n  \"rich-gets-richer\" or \"fit-gets-richer\"? Abstract: We analyze the growth models for complex networks including preferential\nattachment (A.-L. Barabasi and R. Albert, Science 286, 509 (1999)) and fitness\nmodel (Caldarelli et al., Phys. Rev. Lett. 89, 258702 (2002)) and demonstrate\nthat, under very general conditions, these two models yield the same dynamic\nequation of network growth, $\\frac{dK}{dt}=A(t)(K+K_{0})$, where $A(t)$ is the\naging constant, $K$ is the node's degree, and $K_{0}$ is the initial\nattractivity. Basing on this result, we show that the fitness model provides an\nunderlying microscopic basis for the preferential attachment mechanism. This\napproach yields long-sought explanation for the initial attractivity, an\nelusive parameter which was left unexplained within the framework of the\npreferential attachment model. We show that $K_{0}$ is mainly determined by the\nwidth of the fitness distribution. The measurements of $K_{0}$ in many complex\nnetworks usually yield the same $K_{0}\\sim 1$. This empirical universality can\nbe traced to frequently occurring lognormal fitness distribution with the width\n$\\sigma\\approx 1$. \n\n"}
{"id": "1803.00186", "contents": "Title: Smoothed analysis for low-rank solutions to semidefinite programs in\n  quadratic penalty form Abstract: Semidefinite programs (SDP) are important in learning and combinatorial\noptimization with numerous applications. In pursuit of low-rank solutions and\nlow complexity algorithms, we consider the Burer--Monteiro factorization\napproach for solving SDPs. We show that all approximate local optima are global\noptima for the penalty formulation of appropriately rank-constrained SDPs as\nlong as the number of constraints scales sub-quadratically with the desired\nrank of the optimal solution. Our result is based on a simple penalty function\nformulation of the rank-constrained SDP along with a smoothed analysis to avoid\nworst-case cost matrices. We particularize our results to two applications,\nnamely, Max-Cut and matrix completion. \n\n"}
{"id": "1803.01233", "contents": "Title: Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase\n  Procrustes Flow Abstract: We revisit the inductive matrix completion problem that aims to recover a\nrank-$r$ matrix with ambient dimension $d$ given $n$ features as the side prior\ninformation. The goal is to make use of the known $n$ features to reduce sample\nand computational complexities. We present and analyze a new gradient-based\nnon-convex optimization algorithm that converges to the true underlying matrix\nat a linear rate with sample complexity only linearly depending on $n$ and\nlogarithmically depending on $d$. To the best of our knowledge, all previous\nalgorithms either have a quadratic dependency on the number of features in\nsample complexity or a sub-linear computational convergence rate. In addition,\nwe provide experiments on both synthetic and real world data to demonstrate the\neffectiveness of our proposed algorithm. \n\n"}
{"id": "1803.01833", "contents": "Title: Marginal Singularity, and the Benefits of Labels in Covariate-Shift Abstract: We present new minimax results that concisely capture the relative benefits\nof source and target labeled data, under covariate-shift. Namely, we show that\nthe benefits of target labels are controlled by a transfer-exponent $\\gamma$\nthat encodes how singular Q is locally w.r.t. P, and interestingly allows\nsituations where transfer did not seem possible under previous insights. In\nfact, our new minimax analysis - in terms of $\\gamma$ - reveals a continuum of\nregimes ranging from situations where target labels have little benefit, to\nregimes where target labels dramatically improve classification. We then show\nthat a recently proposed semi-supervised procedure can be extended to adapt to\nunknown $\\gamma$, and therefore requests labels only when beneficial, while\nachieving minimax transfer rates. \n\n"}
{"id": "1803.01980", "contents": "Title: Learning Filter Bank Sparsifying Transforms Abstract: Data is said to follow the transform (or analysis) sparsity model if it\nbecomes sparse when acted on by a linear operator called a sparsifying\ntransform. Several algorithms have been designed to learn such a transform\ndirectly from data, and data-adaptive sparsifying transforms have demonstrated\nexcellent performance in signal restoration tasks. Sparsifying transforms are\ntypically learned using small sub-regions of data called patches, but these\nalgorithms often ignore redundant information shared between neighboring\npatches.\n  We show that many existing transform and analysis sparse representations can\nbe viewed as filter banks, thus linking the local properties of patch-based\nmodel to the global properties of a convolutional model. We propose a new\ntransform learning framework where the sparsifying transform is an undecimated\nperfect reconstruction filter bank. Unlike previous transform learning\nalgorithms, the filter length can be chosen independently of the number of\nfilter bank channels. Numerical results indicate filter bank sparsifying\ntransforms outperform existing patch-based transform learning for image\ndenoising while benefiting from additional flexibility in the design process. \n\n"}
{"id": "1803.02042", "contents": "Title: Accelerated Gradient Boosting Abstract: Gradient tree boosting is a prediction algorithm that sequentially produces a\nmodel in the form of linear combinations of decision trees, by solving an\ninfinite-dimensional optimization problem. We combine gradient boosting and\nNesterov's accelerated descent to design a new algorithm, which we call AGB\n(for Accelerated Gradient Boosting). Substantial numerical evidence is provided\non both synthetic and real-life data sets to assess the excellent performance\nof the method in a large variety of prediction problems. It is empirically\nshown that AGB is much less sensitive to the shrinkage parameter and outputs\npredictors that are considerably more sparse in the number of trees, while\nretaining the exceptional performance of gradient boosting. \n\n"}
{"id": "1803.02596", "contents": "Title: Revisiting differentially private linear regression: optimal and\n  adaptive prediction & estimation in unbounded domain Abstract: We revisit the problem of linear regression under a differential privacy\nconstraint. By consolidating existing pieces in the literature, we clarify the\ncorrect dependence of the feature, label and coefficient domains in the\noptimization error and estimation error, hence revealing the delicate price of\ndifferential privacy in statistical estimation and statistical learning.\nMoreover, we propose simple modifications of two existing DP algorithms: (a)\nposterior sampling, (b) sufficient statistics perturbation, and show that they\ncan be upgraded into **adaptive** algorithms that are able to exploit\ndata-dependent quantities and behave nearly optimally **for every instance**.\nExtensive experiments are conducted on both simulated data and real data, which\nconclude that both AdaOPS and AdaSSP outperform the existing techniques on\nnearly all 36 data sets that we test on. \n\n"}
{"id": "1803.02726", "contents": "Title: Stochastic Block Models with Multiple Continuous Attributes Abstract: The stochastic block model (SBM) is a probabilistic model for community\nstructure in networks. Typically, only the adjacency matrix is used to perform\nSBM parameter inference. In this paper, we consider circumstances in which\nnodes have an associated vector of continuous attributes that are also used to\nlearn the node-to-community assignments and corresponding SBM parameters. While\nthis assumption is not realistic for every application, our model assumes that\nthe attributes associated with the nodes in a network's community can be\ndescribed by a common multivariate Gaussian model. In this augmented,\nattributed SBM, the objective is to simultaneously learn the SBM connectivity\nprobabilities with the multivariate Gaussian parameters describing each\ncommunity. While there are recent examples in the literature that combine\nconnectivity and attribute information to inform community detection, our model\nis the first augmented stochastic block model to handle multiple continuous\nattributes. This provides the flexibility in biological data to, for example,\naugment connectivity information with continuous measurements from multiple\nexperimental modalities. Because the lack of labeled network data often makes\ncommunity detection results difficult to validate, we highlight the usefulness\nof our model for two network prediction tasks: link prediction and\ncollaborative filtering. As a result of fitting this attributed stochastic\nblock model, one can predict the attribute vector or connectivity patterns for\na new node in the event of the complementary source of information\n(connectivity or attributes, respectively). We also highlight two biological\nexamples where the attributed stochastic block model provides satisfactory\nperformance in the link prediction and collaborative filtering tasks. \n\n"}
{"id": "1803.03191", "contents": "Title: A Bayesian and Machine Learning approach to estimating Influence Model\n  parameters for IM-RO Abstract: The rise of Online Social Networks (OSNs) has caused an insurmountable amount\nof interest from advertisers and researchers seeking to monopolize on its\nfeatures. Researchers aim to develop strategies for determining how information\nis propagated among users within an OSN that is captured by diffusion or\ninfluence models. We consider the influence models for the IM-RO problem, a\nnovel formulation to the Influence Maximization (IM) problem based on\nimplementing Stochastic Dynamic Programming (SDP). In contrast to existing\napproaches involving influence spread and the theory of submodular functions,\nthe SDP method focuses on optimizing clicks and ultimately revenue to\nadvertisers in OSNs. Existing approaches to influence maximization have been\nactively researched over the past decade, with applications to multiple fields,\nhowever, our approach is a more practical variant to the original IM problem.\nIn this paper, we provide an analysis on the influence models of the IM-RO\nproblem by conducting experiments on synthetic and real-world datasets. We\npropose a Bayesian and Machine Learning approach for estimating the parameters\nof the influence models for the (Influence Maximization- Revenue Optimization)\nIM-RO problem. We present a Bayesian hierarchical model and implement the\nwell-known Naive Bayes classifier (NBC), Decision Trees classifier (DTC) and\nRandom Forest classifier (RFC) on three real-world datasets. Compared to\nprevious approaches to estimating influence model parameters, our strategy has\nthe great advantage of being directly implementable in standard software\npackages such as WinBUGS/OpenBUGS/JAGS and Apache Spark. We demonstrate the\nefficiency and usability of our methods in terms of spreading information and\ngenerating revenue for advertisers in the context of OSNs. \n\n"}
{"id": "1803.03497", "contents": "Title: Modelos de Resposta para Experimentos Randomizados em Redes Sociais de\n  Larga Escala Abstract: A/B tests are randomized experiments frequently used by companies that offer\nservices on the Web for assessing the impact of new features. During an\nexperiment, each user is randomly redirected to one of two versions of the\nwebsite, called treatments. Several response models were proposed to describe\nthe behavior of a user in a social network website, where the treatment\nassigned to her neighbors must be taken into account. However, there is no\nconsensus as to which model should be applied to a given dataset. In this work,\nwe propose a new response model, derive theoretical limits for the estimation\nerror of several models, and obtain empirical results for cases where the\nresponse model was misspecified. \n\n"}
{"id": "1803.03697", "contents": "Title: Community Interaction and Conflict on the Web Abstract: Users organize themselves into communities on web platforms. These\ncommunities can interact with one another, often leading to conflicts and toxic\ninteractions. However, little is known about the mechanisms of interactions\nbetween communities and how they impact users.\n  Here we study intercommunity interactions across 36,000 communities on\nReddit, examining cases where users of one community are mobilized by negative\nsentiment to comment in another community. We show that such conflicts tend to\nbe initiated by a handful of communities---less than 1% of communities start\n74% of conflicts. While conflicts tend to be initiated by highly active\ncommunity members, they are carried out by significantly less active members.\nWe find that conflicts are marked by formation of echo chambers, where users\nprimarily talk to other users from their own community. In the long-term,\nconflicts have adverse effects and reduce the overall activity of users in the\ntargeted communities.\n  Our analysis of user interactions also suggests strategies for mitigating the\nnegative impact of conflicts---such as increasing direct engagement between\nattackers and defenders. Further, we accurately predict whether a conflict will\noccur by creating a novel LSTM model that combines graph embeddings, user,\ncommunity, and text features. This model can be used toreate early-warning\nsystems for community moderators to prevent conflicts. Altogether, this work\npresents a data-driven view of community interactions and conflict, and paves\nthe way towards healthier online communities. \n\n"}
{"id": "1803.04223", "contents": "Title: Leveraging Crowdsourcing Data For Deep Active Learning - An Application:\n  Learning Intents in Alexa Abstract: This paper presents a generic Bayesian framework that enables any deep\nlearning model to actively learn from targeted crowds. Our framework inherits\nfrom recent advances in Bayesian deep learning, and extends existing work by\nconsidering the targeted crowdsourcing approach, where multiple annotators with\nunknown expertise contribute an uncontrolled amount (often limited) of\nannotations. Our framework leverages the low-rank structure in annotations to\nlearn individual annotator expertise, which then helps to infer the true labels\nfrom noisy and sparse annotations. It provides a unified Bayesian model to\nsimultaneously infer the true labels and train the deep learning model in order\nto reach an optimal learning efficacy. Finally, our framework exploits the\nuncertainty of the deep learning model during prediction as well as the\nannotators' estimated expertise to minimize the number of required annotations\nand annotators for optimally training the deep learning model.\n  We evaluate the effectiveness of our framework for intent classification in\nAlexa (Amazon's personal assistant), using both synthetic and real-world\ndatasets. Experiments show that our framework can accurately learn annotator\nexpertise, infer true labels, and effectively reduce the amount of annotations\nin model training as compared to state-of-the-art approaches. We further\ndiscuss the potential of our proposed framework in bridging machine learning\nand crowdsourcing towards improved human-in-the-loop systems. \n\n"}
{"id": "1803.04547", "contents": "Title: Analysis of spectral clustering algorithms for community detection: the\n  general bipartite setting Abstract: We consider spectral clustering algorithms for community detection under a\ngeneral bipartite stochastic block model (SBM). A modern spectral clustering\nalgorithm consists of three steps: (1) regularization of an appropriate\nadjacency or Laplacian matrix (2) a form of spectral truncation and (3) a\nk-means type algorithm in the reduced spectral domain. We focus on the\nadjacency-based spectral clustering and for the first step, propose a new\ndata-driven regularization that can restore the concentration of the adjacency\nmatrix even for the sparse networks. This result is based on recent work on\nregularization of random binary matrices, but avoids using unknown population\nlevel parameters, and instead estimates the necessary quantities from the data.\nWe also propose and study a novel variation of the spectral truncation step and\nshow how this variation changes the nature of the misclassification rate in a\ngeneral SBM. We then show how the consistency results can be extended to models\nbeyond SBMs, such as inhomogeneous random graph models with approximate\nclusters, including a graphon clustering problem, as well as general\nsub-Gaussian biclustering. A theme of the paper is providing a better\nunderstanding of the analysis of spectral methods for community detection and\nestablishing consistency results, under fairly general clustering models and\nfor a wide regime of degree growths, including sparse cases where the average\nexpected degree grows arbitrarily slowly. \n\n"}
{"id": "1803.05046", "contents": "Title: Caveat Emptor, Computational Social Science: Large-Scale Missing Data in\n  a Widely-Published Reddit Corpus Abstract: As researchers use computational methods to study complex social behaviors at\nscale, the validity of this computational social science depends on the\nintegrity of the data. On July 2, 2015, Jason Baumgartner published a dataset\nadvertised to include ``every publicly available Reddit comment'' which was\nquickly shared on Bittorrent and the Internet Archive. This data quickly became\nthe basis of many academic papers on topics including machine learning, social\nbehavior, politics, breaking news, and hate speech. We have discovered\nsubstantial gaps and limitations in this dataset which may contribute to bias\nin the findings of that research. In this paper, we document the dataset,\nsubstantial missing observations in the dataset, and the risks to research\nvalidity from those gaps. In summary, we identify strong risks to research that\nconsiders user histories or network analysis, moderate risks to research that\ncompares counts of participation, and lesser risk to machine learning research\nthat avoids making representative claims about behavior and participation on\nReddit. \n\n"}
{"id": "1803.05419", "contents": "Title: Generalised Structural CNNs (SCNNs) for time series data with arbitrary\n  graph topology Abstract: Deep Learning methods, specifically convolutional neural networks (CNNs),\nhave seen a lot of success in the domain of image-based data, where the data\noffers a clearly structured topology in the regular lattice of pixels. This\n4-neighbourhood topological simplicity makes the application of convolutional\nmasks straightforward for time series data, such as video applications, but\nmany high-dimensional time series data are not organised in regular lattices,\nand instead values may have adjacency relationships with non-trivial\ntopologies, such as small-world networks or trees. In our application case,\nhuman kinematics, it is currently unclear how to generalise convolutional\nkernels in a principled manner. Therefore we define and implement here a\nframework for general graph-structured CNNs for time series analysis. Our\nalgorithm automatically builds convolutional layers using the specified\nadjacency matrix of the data dimensions and convolutional masks that scale with\nthe hop distance. In the limit of a lattice-topology our method produces the\nwell-known image convolutional masks. We test our method first on synthetic\ndata of arbitrarily-connected graphs and human hand motion capture data, where\nthe hand is represented by a tree capturing the mechanical dependencies of the\njoints. We are able to demonstrate, amongst other things, that inclusion of the\ngraph structure of the data dimensions improves model prediction significantly,\nwhen compared against a benchmark CNN model with only time convolution layers. \n\n"}
{"id": "1803.06031", "contents": "Title: Optimal Bipartite Network Clustering Abstract: We study bipartite community detection in networks, or more generally the\nnetwork biclustering problem. We present a fast two-stage procedure based on\nspectral initialization followed by the application of a pseudo-likelihood\nclassifier twice. Under mild regularity conditions, we establish the weak\nconsistency of the procedure (i.e., the convergence of the misclassification\nrate to zero) under a general bipartite stochastic block model. We show that\nthe procedure is optimal in the sense that it achieves the optimal convergence\nrate that is achievable by a biclustering oracle, adaptively over the whole\nclass, up to constants. This is further formalized by deriving a minimax lower\nbound over a class of biclustering problems. The optimal rate we obtain\nsharpens some of the existing results and generalizes others to a wide regime\nof average degree growth, from sparse networks with average degrees growing\narbitrarily slowly to fairly dense networks with average degrees of order\n$\\sqrt{n}$. As a special case, we recover the known exact recovery threshold in\nthe $\\log n$ regime of sparsity. To obtain the consistency result, as part of\nthe provable version of the algorithm, we introduce a sub-block partitioning\nscheme that is also computationally attractive, allowing for distributed\nimplementation of the algorithm without sacrificing optimality. The provable\nalgorithm is derived from a general class of pseudo-likelihood biclustering\nalgorithms that employ simple EM type updates. We show the effectiveness of\nthis general class by numerical simulations. \n\n"}
{"id": "1803.07294", "contents": "Title: GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal\n  Graphs Abstract: We propose a new network architecture, Gated Attention Networks (GaAN), for\nlearning on graphs. Unlike the traditional multi-head attention mechanism,\nwhich equally consumes all attention heads, GaAN uses a convolutional\nsub-network to control each attention head's importance. We demonstrate the\neffectiveness of GaAN on the inductive node classification problem. Moreover,\nwith GaAN as a building block, we construct the Graph Gated Recurrent Unit\n(GGRU) to address the traffic speed forecasting problem. Extensive experiments\non three real-world datasets show that our GaAN framework achieves\nstate-of-the-art results on both tasks. \n\n"}
{"id": "1803.07954", "contents": "Title: Resilient Monotone Sequential Maximization Abstract: Applications in machine learning, optimization, and control require the\nsequential selection of a few system elements, such as sensors, data, or\nactuators, to optimize the system performance across multiple time steps.\nHowever, in failure-prone and adversarial environments, sensors get attacked,\ndata get deleted, and actuators fail. Thence, traditional sequential design\nparadigms become insufficient and, in contrast, resilient sequential designs\nthat adapt against system-wide attacks, deletions, or failures become\nimportant. In general, resilient sequential design problems are computationally\nhard. Also, even though they often involve objective functions that are\nmonotone and (possibly) submodular, no scalable approximation algorithms are\nknown for their solution. In this paper, we provide the first scalable\nalgorithm, that achieves the following characteristics: system-wide resiliency,\ni.e., the algorithm is valid for any number of denial-of-service attacks,\ndeletions, or failures; adaptiveness, i.e., at each time step, the algorithm\nselects system elements based on the history of inflicted attacks, deletions,\nor failures; and provable approximation performance, i.e., the algorithm\nguarantees for monotone objective functions a solution close to the optimal. We\nquantify the algorithm's approximation performance using a notion of curvature\nfor monotone (not necessarily submodular) set functions. Finally, we support\nour theoretical analyses with simulated experiments, by considering a\ncontrol-aware sensor scheduling scenario, namely, sensing-constrained robot\nnavigation. \n\n"}
{"id": "1803.08021", "contents": "Title: Error Estimation for Randomized Least-Squares Algorithms via the\n  Bootstrap Abstract: Over the course of the past decade, a variety of randomized algorithms have\nbeen proposed for computing approximate least-squares (LS) solutions in\nlarge-scale settings. A longstanding practical issue is that, for any given\ninput, the user rarely knows the actual error of an approximate solution\n(relative to the exact solution). Likewise, it is difficult for the user to\nknow precisely how much computation is needed to achieve the desired error\ntolerance. Consequently, the user often appeals to worst-case error bounds that\ntend to offer only qualitative guidance. As a more practical alternative, we\npropose a bootstrap method to compute a posteriori error estimates for\nrandomized LS algorithms. These estimates permit the user to numerically assess\nthe error of a given solution, and to predict how much work is needed to\nimprove a \"preliminary\" solution. In addition, we provide theoretical\nconsistency results for the method, which are the first such results in this\ncontext (to the best of our knowledge). From a practical standpoint, the method\nalso has considerable flexibility, insofar as it can be applied to several\npopular sketching algorithms, as well as a variety of error metrics. Moreover,\nthe extra step of error estimation does not add much cost to an underlying\nsketching algorithm. Finally, we demonstrate the effectiveness of the method\nwith empirical results. \n\n"}
{"id": "1803.08031", "contents": "Title: Primal-Dual Algorithm for Distributed Reinforcement Learning:\n  Distributed GTD Abstract: The goal of this paper is to study a distributed version of the gradient\ntemporal-difference (GTD) learning algorithm for multi-agent Markov decision\nprocesses (MDPs). The temporal difference (TD) learning is a reinforcement\nlearning (RL) algorithm which learns an infinite horizon discounted cost\nfunction (or value function) for a given fixed policy without the model\nknowledge. In the distributed RL case each agent receives local reward through\na local processing. Information exchange over sparse communication network\nallows the agents to learn the global value function corresponding to a global\nreward, which is a sum of local rewards. In this paper, the problem is\nconverted into a constrained convex optimization problem with a consensus\nconstraint. Then, we propose a primal-dual distributed GTD algorithm and prove\nthat it almost surely converges to a set of stationary points of the\noptimization problem. \n\n"}
{"id": "1803.08118", "contents": "Title: Seglearn: A Python Package for Learning Sequences and Time Series Abstract: Seglearn is an open-source python package for machine learning time series or\nsequences using a sliding window segmentation approach. The implementation\nprovides a flexible pipeline for tackling classification, regression, and\nforecasting problems with multivariate sequence and contextual data. This\npackage is compatible with scikit-learn and is listed under scikit-learn\nRelated Projects. The package depends on numpy, scipy, and scikit-learn.\nSeglearn is distributed under the BSD 3-Clause License. Documentation includes\na detailed API description, user guide, and examples. Unit tests provide a high\ndegree of code coverage. \n\n"}
{"id": "1803.08251", "contents": "Title: Life in the \"Matrix\": Human Mobility Patterns in the Cyber Space Abstract: With the wide adoption of the multi-community setting in many popular social\nmedia platforms, the increasing user engagements across multiple online\ncommunities warrant research attention. In this paper, we introduce a novel\nanalogy between the movements in the cyber space and the physical space. This\nanalogy implies a new way of studying human online activities by modelling the\nactivities across online communities in a similar fashion as the movements\namong locations. First, we quantitatively validate the analogy by comparing\nseveral important properties of human online activities and physical movements.\nOur experiments reveal striking similarities between the cyber space and the\nphysical space. Next, inspired by the established methodology on human mobility\nin the physical space, we propose a framework to study human \"mobility\" across\nonline platforms. We discover three interesting patterns of user engagements in\nonline communities. Furthermore, our experiments indicate that people with\ndifferent mobility patterns also exhibit divergent preferences to online\ncommunities. This work not only attempts to achieve a better understanding of\nhuman online activities, but also intends to open a promising research\ndirection with rich implications and applications. \n\n"}
{"id": "1803.08533", "contents": "Title: Understanding Measures of Uncertainty for Adversarial Example Detection Abstract: Measuring uncertainty is a promising technique for detecting adversarial\nexamples, crafted inputs on which the model predicts an incorrect class with\nhigh confidence. But many measures of uncertainty exist, including predictive\nen- tropy and mutual information, each capturing different types of\nuncertainty. We study these measures, and shed light on why mutual information\nseems to be effective at the task of adversarial example detection. We\nhighlight failure modes for MC dropout, a widely used approach for estimating\nuncertainty in deep models. This leads to an improved understanding of the\ndrawbacks of current methods, and a proposal to improve the quality of\nuncertainty estimates using probabilistic model ensembles. We give illustrative\nexperiments using MNIST to demonstrate the intuition underlying the different\nmeasures of uncertainty, as well as experiments on a real world Kaggle dogs vs\ncats classification dataset. \n\n"}
{"id": "1803.08586", "contents": "Title: Optimization of Smooth Functions with Noisy Observations: Local Minimax\n  Rates Abstract: We consider the problem of global optimization of an unknown non-convex\nsmooth function with zeroth-order feedback. In this setup, an algorithm is\nallowed to adaptively query the underlying function at different locations and\nreceives noisy evaluations of function values at the queried points (i.e. the\nalgorithm has access to zeroth-order information). Optimization performance is\nevaluated by the expected difference of function values at the estimated\noptimum and the true optimum. In contrast to the classical optimization setup,\nfirst-order information like gradients are not directly accessible to the\noptimization algorithm. We show that the classical minimax framework of\nanalysis, which roughly characterizes the worst-case query complexity of an\noptimization algorithm in this setting, leads to excessively pessimistic\nresults. We propose a local minimax framework to study the fundamental\ndifficulty of optimizing smooth functions with adaptive function evaluations,\nwhich provides a refined picture of the intrinsic difficulty of zeroth-order\noptimization. We show that for functions with fast level set growth around the\nglobal minimum, carefully designed optimization algorithms can identify a near\nglobal minimizer with many fewer queries. For the special case of strongly\nconvex and smooth functions, our implied convergence rates match the ones\ndeveloped for zeroth-order convex optimization problems. At the other end of\nthe spectrum, for worst-case smooth functions no algorithm can converge faster\nthan the minimax rate of estimating the entire unknown function in the\n$\\ell_\\infty$-norm. We provide an intuitive and efficient algorithm that\nattains the derived upper error bounds. \n\n"}
{"id": "1803.09151", "contents": "Title: Natural Gradients in Practice: Non-Conjugate Variational Inference in\n  Gaussian Process Models Abstract: The natural gradient method has been used effectively in conjugate Gaussian\nprocess models, but the non-conjugate case has been largely unexplored. We\nexamine how natural gradients can be used in non-conjugate stochastic settings,\ntogether with hyperparameter learning. We conclude that the natural gradient\ncan significantly improve performance in terms of wall-clock time. For\nill-conditioned posteriors the benefit of the natural gradient method is\nespecially pronounced, and we demonstrate a practical setting where ordinary\ngradients are unusable. We show how natural gradients can be computed\nefficiently and automatically in any parameterization, using automatic\ndifferentiation. Our code is integrated into the GPflow package. \n\n"}
{"id": "1803.09539", "contents": "Title: On Matching Pursuit and Coordinate Descent Abstract: Two popular examples of first-order optimization methods over linear spaces\nare coordinate descent and matching pursuit algorithms, with their randomized\nvariants. While the former targets the optimization by moving along\ncoordinates, the latter considers a generalized notion of directions.\nExploiting the connection between the two algorithms, we present a unified\nanalysis of both, providing affine invariant sublinear $\\mathcal{O}(1/t)$ rates\non smooth objectives and linear convergence on strongly convex objectives. As a\nbyproduct of our affine invariant analysis of matching pursuit, our rates for\nsteepest coordinate descent are the tightest known. Furthermore, we show the\nfirst accelerated convergence rate $\\mathcal{O}(1/t^2)$ for matching pursuit\nand steepest coordinate descent on convex objectives. \n\n"}
{"id": "1803.09546", "contents": "Title: Calibrated Prediction Intervals for Neural Network Regressors Abstract: Ongoing developments in neural network models are continually advancing the\nstate of the art in terms of system accuracy. However, the predicted labels\nshould not be regarded as the only core output; also important is a\nwell-calibrated estimate of the prediction uncertainty. Such estimates and\ntheir calibration are critical in many practical applications. Despite their\nobvious aforementioned advantage in relation to accuracy, contemporary neural\nnetworks can, generally, be regarded as poorly calibrated and as such do not\nproduce reliable output probability estimates. Further, while post-processing\ncalibration solutions can be found in the relevant literature, these tend to be\nfor systems performing classification. In this regard, we herein present two\nnovel methods for acquiring calibrated predictions intervals for neural network\nregressors: empirical calibration and temperature scaling. In experiments using\ndifferent regression tasks from the audio and computer vision domains, we find\nthat both our proposed methods are indeed capable of producing calibrated\nprediction intervals for neural network regressors with any desired confidence\nlevel, a finding that is consistent across all datasets and neural network\narchitectures we experimented with. In addition, we derive an additional\npractical recommendation for producing more accurate calibrated prediction\nintervals. We release the source code implementing our proposed methods for\ncomputing calibrated predicted intervals. The code for computing calibrated\npredicted intervals is publicly available. \n\n"}
{"id": "1803.09704", "contents": "Title: MOrdReD: Memory-based Ordinal Regression Deep Neural Networks for Time\n  Series Forecasting Abstract: Time series forecasting is ubiquitous in the modern world. Applications range\nfrom health care to astronomy, and include climate modelling, financial trading\nand monitoring of critical engineering equipment. To offer value over this\nrange of activities, models must not only provide accurate forecasts, but also\nquantify and adjust their uncertainty over time. In this work, we directly\ntackle this task with a novel, fully end-to-end deep learning method for time\nseries forecasting. By recasting time series forecasting as an ordinal\nregression task, we develop a principled methodology to assess long-term\npredictive uncertainty and describe rich multimodal, non-Gaussian behaviour,\nwhich arises regularly in applied settings.\n  Notably, our framework is a wholly general-purpose approach that requires\nlittle to no user intervention to be used. We showcase this key feature in a\nlarge-scale benchmark test with 45 datasets drawn from both, a wide range of\nreal-world application domains, as well as a comprehensive list of synthetic\nmaps. This wide comparison encompasses state-of-the-art methods in both the\nMachine Learning and Statistics modelling literature, such as the Gaussian\nProcess. We find that our approach does not only provide excellent predictive\nforecasts, shadowing true future values, but also allows us to infer valuable\ninformation, such as the predictive distribution of the occurrence of critical\nevents of interest, accurately and reliably even over long time horizons. \n\n"}
{"id": "1803.09868", "contents": "Title: Bypassing Feature Squeezing by Increasing Adversary Strength Abstract: Feature Squeezing is a recently proposed defense method which reduces the\nsearch space available to an adversary by coalescing samples that correspond to\nmany different feature vectors in the original space into a single sample. It\nhas been shown that feature squeezing defenses can be combined in a joint\ndetection framework to achieve high detection rates against state-of-the-art\nattacks. However, we demonstrate on the MNIST and CIFAR-10 datasets that by\nincreasing the adversary strength of said state-of-the-art attacks, one can\nbypass the detection framework with adversarial examples of minimal visual\ndistortion. These results suggest for proposed defenses to validate against\nstronger attack configurations. \n\n"}
{"id": "1803.10254", "contents": "Title: Disease-Atlas: Navigating Disease Trajectories with Deep Learning Abstract: Joint models for longitudinal and time-to-event data are commonly used in\nlongitudinal studies to forecast disease trajectories over time. While there\nare many advantages to joint modeling, the standard forms suffer from\nlimitations that arise from a fixed model specification, and computational\ndifficulties when applied to high-dimensional datasets. In this paper, we\npropose a deep learning approach to address these limitations, enhancing\nexisting methods with the inherent flexibility and scalability of deep neural\nnetworks, while retaining the benefits of joint modeling. Using longitudinal\ndata from a real-world medical dataset, we demonstrate improvements in\nperformance and scalability, as well as robustness in the presence of\nirregularly sampled data. \n\n"}
{"id": "1803.11287", "contents": "Title: A Stochastic Large-scale Machine Learning Algorithm for Distributed\n  Features and Observations Abstract: As the size of modern data sets exceeds the disk and memory capacities of a\nsingle computer, machine learning practitioners have resorted to parallel and\ndistributed computing. Given that optimization is one of the pillars of machine\nlearning and predictive modeling, distributed optimization methods have\nrecently garnered ample attention, in particular when either observations or\nfeatures are distributed, but not both. We propose a general stochastic\nalgorithm where observations, features, and gradient components can be sampled\nin a double distributed setting, i.e., with both features and observations\ndistributed. Very technical analyses establish convergence properties of the\nalgorithm under different conditions on the learning rate (diminishing to zero\nor constant). Computational experiments in Spark demonstrate a superior\nperformance of our algorithm versus a benchmark in early iterations of the\nalgorithm, which is due to the stochastic components of the algorithm. \n\n"}
{"id": "1804.00084", "contents": "Title: Characterizing Interconnections and Linguistic Patterns in Twitter Abstract: Social media is considered a democratic space in which people connect and\ninteract with each other regardless of their gender, race, or any other\ndemographic aspect. Despite numerous efforts that explore demographic aspects\nin social media, it is still unclear whether social media perpetuates old\ninequalities from the offline world. In this dissertation, we attempt to\nidentify gender and race of Twitter users located in the United States using\nadvanced image processing algorithms from Face++. We investigate how different\ndemographic groups connect with each other and differentiate them regarding\nlinguistic styles and also their interests. We quantify to what extent one\ngroup follows and interacts with each other and the extent to which these\nconnections and interactions reflect in inequalities in Twitter. We also\nextract linguistic features from six categories (affective attributes,\ncognitive attributes, lexical density and awareness, temporal references,\nsocial and personal concerns, and interpersonal focus) in order to identify the\nsimilarities and the differences in the messages they share in Twitter.\nFurthermore, we extract the absolute ranking difference of top phrases between\ndemographic groups. As a dimension of diversity, we use the topics of interest\nthat we retrieve from each user. Our analysis shows that users identified as\nwhite and male tend to attain higher positions, in terms of the number of\nfollowers and number of times in another user's lists, in Twitter. There are\nclear differences in the way of writing across different demographic groups in\nboth gender and race domains as well as in the topic of interest. We hope our\neffort can stimulate the development of new theories of demographic information\nin the online space. Finally, we developed a Web-based system that leverages\nthe demographic aspects of users to provide transparency to the Twitter\ntrending topics system. \n\n"}
{"id": "1804.00104", "contents": "Title: Learning Disentangled Joint Continuous and Discrete Representations Abstract: We present a framework for learning disentangled and interpretable jointly\ncontinuous and discrete representations in an unsupervised manner. By\naugmenting the continuous latent distribution of variational autoencoders with\na relaxed discrete distribution and controlling the amount of information\nencoded in each latent unit, we show how continuous and categorical factors of\nvariation can be discovered automatically from data. Experiments show that the\nframework disentangles continuous and discrete generative factors on various\ndatasets and outperforms current disentangling methods when a discrete\ngenerative factor is prominent. \n\n"}
{"id": "1804.00795", "contents": "Title: Estimation of Markov Chain via Rank-Constrained Likelihood Abstract: This paper studies the estimation of low-rank Markov chains from empirical\ntrajectories. We propose a non-convex estimator based on rank-constrained\nlikelihood maximization. Statistical upper bounds are provided for the\nKullback-Leiber divergence and the $\\ell_2$ risk between the estimator and the\ntrue transition matrix. The estimator reveals a compressed state space of the\nMarkov chain. We also develop a novel DC (difference of convex function)\nprogramming algorithm to tackle the rank-constrained non-smooth optimization\nproblem. Convergence results are established. Experiments show that the\nproposed estimator achieves better empirical performance than other popular\napproaches. \n\n"}
{"id": "1804.00982", "contents": "Title: 360{\\deg} Stance Detection Abstract: The proliferation of fake news and filter bubbles makes it increasingly\ndifficult to form an unbiased, balanced opinion towards a topic. To ameliorate\nthis, we propose 360{\\deg} Stance Detection, a tool that aggregates news with\nmultiple perspectives on a topic. It presents them on a spectrum ranging from\nsupport to opposition, enabling the user to base their opinion on multiple\npieces of diverse evidence. \n\n"}
{"id": "1804.02293", "contents": "Title: Phase Transitions of the Moran Process and Algorithmic Consequences Abstract: The Moran process is a random process that models the spread of genetic\nmutations through graphs. If the graph is connected, the process eventually\nreaches \"fixation\", where every vertex is a mutant, or \"extinction\", where no\nvertex is a mutant.\n  Our main result is an almost-tight bound on expected absorption time. For all\nepsilon > 0, we show that the expected absorption time on an n-vertex graph is\no(n^(3+epsilon)). In fact, we show that it is at most n^3 * exp(O((log log\nn)^3)) and that there is a family of graphs where it is Omega(n^3). In the\ncourse of proving our main result, we also establish a phase transition in the\nprobability of fixation, depending on the fitness parameter r of the mutation.\nWe show that no similar phase transition occurs for digraphs, where it is\nalready known that the expected absorption time can also be exponential.\nFinally, we give an improved FPRAS for approximating the probability of\nfixation. Its running time is independent of the size of the graph when the\nmaximum degree is bounded and some basic properties of the graph are given. \n\n"}
{"id": "1804.03665", "contents": "Title: An information-theoretic, all-scales approach to comparing networks Abstract: As network research becomes more sophisticated, it is more common than ever\nfor researchers to find themselves not studying a single network but needing to\nanalyze sets of networks. An important task when working with sets of networks\nis network comparison, developing a similarity or distance measure between\nnetworks so that meaningful comparisons can be drawn. The best means to\naccomplish this task remains an open area of research. Here we introduce a new\nmeasure to compare networks, the Network Portrait Divergence, that is\nmathematically principled, incorporates the topological characteristics of\nnetworks at all structural scales, and is general-purpose and applicable to all\ntypes of networks. An important feature of our measure that enables many of its\nuseful properties is that it is based on a graph invariant, the network\nportrait. We test our measure on both synthetic graphs and real world networks\ntaken from protein interaction data, neuroscience, and computational social\nscience applications. The Network Portrait Divergence reveals important\ncharacteristics of multilayer and temporal networks extracted from data. \n\n"}
{"id": "1804.03811", "contents": "Title: Estimating Time-Varying Graphical Models Abstract: In this paper, we study time-varying graphical models based on data measured\nover a temporal grid. Such models are motivated by the needs to describe and\nunderstand evolving interacting relationships among a set of random variables\nin many real applications, for instance the study of how stocks interact with\neach other and how such interactions change over time.\n  We propose a new model, LOcal Group Graphical Lasso Estimation (loggle),\nunder the assumption that the graph topology changes gradually over time.\nSpecifically, loggle uses a novel local group-lasso type penalty to efficiently\nincorporate information from neighboring time points and to impose structural\nsmoothness of the graphs. We implement an ADMM based algorithm to fit the\nloggle model. This algorithm utilizes blockwise fast computation and\npseudo-likelihood approximation to improve computational efficiency. An R\npackage loggle has also been developed.\n  We evaluate the performance of loggle by simulation experiments. We also\napply loggle to S&P 500 stock price data and demonstrate that loggle is able to\nreveal the interacting relationships among stocks and among industrial sectors\nin a time period that covers the recent global financial crisis. \n\n"}
{"id": "1804.04469", "contents": "Title: Generative models for local network community detection Abstract: Local network community detection aims to find a single community in a large\nnetwork, while inspecting only a small part of that network around a given seed\nnode. This is much cheaper than finding all communities in a network. Most\nmethods for local community detection are formulated as ad-hoc optimization\nproblems. In this work, we instead start from a generative model for networks\nwith community structure. By assuming that the network is uniform, we can\napproximate the structure of unobserved parts of the network to obtain a method\nfor local community detection. We apply this local approximation technique to\ntwo variants of the stochastic block model. To our knowledge, this results in\nthe first local community detection methods based on probabilistic models.\nInterestingly, in the limit, one of the proposed approximations corresponds to\nconductance, a popular metric in this field. Experiments on real and synthetic\ndatasets show comparable or improved results compared to state-of-the-art local\ncommunity detection algorithms. \n\n"}
{"id": "1804.05567", "contents": "Title: Constant Step Size Stochastic Gradient Descent for Probabilistic\n  Modeling Abstract: Stochastic gradient methods enable learning probabilistic models from large\namounts of data. While large step-sizes (learning rates) have shown to be best\nfor least-squares (e.g., Gaussian noise) once combined with parameter\naveraging, these are not leading to convergent algorithms in general. In this\npaper, we consider generalized linear models, that is, conditional models based\non exponential families. We propose averaging moment parameters instead of\nnatural parameters for constant-step-size stochastic gradient descent. For\nfinite-dimensional models, we show that this can sometimes (and surprisingly)\nlead to better predictions than the best linear model. For infinite-dimensional\nmodels, we show that it always converges to optimal predictions, while\naveraging natural parameters never does. We illustrate our findings with\nsimulations on synthetic data and classical benchmarks with many observations. \n\n"}
{"id": "1804.08774", "contents": "Title: Neural-Brane: Neural Bayesian Personalized Ranking for Attributed\n  Network Embedding Abstract: Network embedding methodologies, which learn a distributed vector\nrepresentation for each vertex in a network, have attracted considerable\ninterest in recent years. Existing works have demonstrated that vertex\nrepresentation learned through an embedding method provides superior\nperformance in many real-world applications, such as node classification, link\nprediction, and community detection. However, most of the existing methods for\nnetwork embedding only utilize topological information of a vertex, ignoring a\nrich set of nodal attributes (such as, user profiles of an online social\nnetwork, or textual contents of a citation network), which is abundant in all\nreal-life networks. A joint network embedding that takes into account both\nattributional and relational information entails a complete network information\nand could further enrich the learned vector representations. In this work, we\npresent Neural-Brane, a novel Neural Bayesian Personalized Ranking based\nAttributed Network Embedding. For a given network, Neural-Brane extracts latent\nfeature representation of its vertices using a designed neural network model\nthat unifies network topological information and nodal attributes; Besides, it\nutilizes Bayesian personalized ranking objective, which exploits the proximity\nordering between a similar node-pair and a dissimilar node-pair. We evaluate\nthe quality of vertex embedding produced by Neural-Brane by solving the node\nclassification and clustering tasks on four real-world datasets. Experimental\nresults demonstrate the superiority of our proposed method over the\nstate-of-the-art existing methods. \n\n"}
{"id": "1804.09060", "contents": "Title: An Information-Theoretic View for Deep Learning Abstract: Deep learning has transformed computer vision, natural language processing,\nand speech recognition\\cite{badrinarayanan2017segnet, dong2016image,\nren2017faster, ji20133d}. However, two critical questions remain obscure: (1)\nwhy do deep neural networks generalize better than shallow networks; and (2)\ndoes it always hold that a deeper network leads to better performance?\nSpecifically, letting $L$ be the number of convolutional and pooling layers in\na deep neural network, and $n$ be the size of the training sample, we derive an\nupper bound on the expected generalization error for this network, i.e.,\n  \\begin{eqnarray*}\n  \\mathbb{E}[R(W)-R_S(W)] \\leq\n\\exp{\\left(-\\frac{L}{2}\\log{\\frac{1}{\\eta}}\\right)}\\sqrt{\\frac{2\\sigma^2}{n}I(S,W)\n}\n  \\end{eqnarray*} where $\\sigma >0$ is a constant depending on the loss\nfunction, $0<\\eta<1$ is a constant depending on the information loss for each\nconvolutional or pooling layer, and $I(S, W)$ is the mutual information between\nthe training sample $S$ and the output hypothesis $W$. This upper bound shows\nthat as the number of convolutional and pooling layers $L$ increases in the\nnetwork, the expected generalization error will decrease exponentially to zero.\nLayers with strict information loss, such as the convolutional layers, reduce\nthe generalization error for the whole network; this answers the first\nquestion. However, algorithms with zero expected generalization error does not\nimply a small test error or $\\mathbb{E}[R(W)]$. This is because\n$\\mathbb{E}[R_S(W)]$ is large when the information for fitting the data is lost\nas the number of layers increases. This suggests that the claim `the deeper the\nbetter' is conditioned on a small training error or $\\mathbb{E}[R_S(W)]$.\nFinally, we show that deep learning satisfies a weak notion of stability and\nthe sample complexity of deep neural networks will decrease as $L$ increases. \n\n"}
{"id": "1804.09401", "contents": "Title: Generative Temporal Models with Spatial Memory for Partially Observed\n  Environments Abstract: In model-based reinforcement learning, generative and temporal models of\nenvironments can be leveraged to boost agent performance, either by tuning the\nagent's representations during training or via use as part of an explicit\nplanning mechanism. However, their application in practice has been limited to\nsimplistic environments, due to the difficulty of training such models in\nlarger, potentially partially-observed and 3D environments. In this work we\nintroduce a novel action-conditioned generative model of such challenging\nenvironments. The model features a non-parametric spatial memory system in\nwhich we store learned, disentangled representations of the environment.\nLow-dimensional spatial updates are computed using a state-space model that\nmakes use of knowledge on the prior dynamics of the moving agent, and\nhigh-dimensional visual observations are modelled with a Variational\nAuto-Encoder. The result is a scalable architecture capable of performing\ncoherent predictions over hundreds of time steps across a range of partially\nobserved 2D and 3D environments. \n\n"}
{"id": "1804.09629", "contents": "Title: Convergence guarantees for a class of non-convex and non-smooth\n  optimization problems Abstract: We consider the problem of finding critical points of functions that are\nnon-convex and non-smooth. Studying a fairly broad class of such problems, we\nanalyze the behavior of three gradient-based methods (gradient descent,\nproximal update, and Frank-Wolfe update). For each of these methods, we\nestablish rates of convergence for general problems, and also prove faster\nrates for continuous sub-analytic functions. We also show that our algorithms\ncan escape strict saddle points for a class of non-smooth functions, thereby\ngeneralizing known results for smooth functions. Our analysis leads to a\nsimplification of the popular CCCP algorithm, used for optimizing functions\nthat can be written as a difference of two convex functions. Our simplified\nalgorithm retains all the convergence properties of CCCP, along with a\nsignificantly lower cost per iteration. We illustrate our methods and theory\nvia applications to the problems of best subset selection, robust estimation,\nmixture density estimation, and shape-from-shading reconstruction. \n\n"}
{"id": "1804.09784", "contents": "Title: Mathematical Analysis on Out-of-Sample Extensions Abstract: Let $X=\\mathbf{X}\\cup\\mathbf{Z}$ be a data set in $\\mathbb{R}^D$, where\n$\\mathbf{X}$ is the training set and $\\mathbf{Z}$ is the test one. Many\nunsupervised learning algorithms based on kernel methods have been developed to\nprovide dimensionality reduction (DR) embedding for a given training set $\\Phi:\n\\mathbf{X} \\to \\mathbb{R}^d$ ( $d\\ll D$) that maps the high-dimensional data\n$\\mathbf{X}$ to its low-dimensional feature representation\n$\\mathbf{Y}=\\Phi(\\mathbf{X})$. However, these algorithms do not\nstraightforwardly produce DR of the test set $\\mathbf{Z}$. An out-of-sample\nextension method provides DR of $\\mathbf{Z}$ using an extension of the existent\nembedding $\\Phi$, instead of re-computing the DR embedding for the whole set\n$X$. Among various out-of-sample DR extension methods, those based on\nNystr\\\"{o}m approximation are very attractive. Many papers have developed such\nout-of-extension algorithms and shown their validity by numerical experiments.\nHowever, the mathematical theory for the DR extension still need further\nconsideration. Utilizing the reproducing kernel Hilbert space (RKHS) theory,\nthis paper develops a preliminary mathematical analysis on the out-of-sample DR\nextension operators. It treats an out-of-sample DR extension operator as an\nextension of the identity on the RKHS defined on $\\mathbf{X}$. Then the\nNystr\\\"{o}m-type DR extension turns out to be an orthogonal projection. In the\npaper, we also present the conditions for the exact DR extension and give the\nestimate for the error of the extension. \n\n"}
{"id": "1804.09874", "contents": "Title: Social Network Fusion and Mining: A Survey Abstract: Looking from a global perspective, the landscape of online social networks is\nhighly fragmented. A large number of online social networks have appeared,\nwhich can provide users with various types of services. Generally, the\ninformation available in these online social networks is of diverse categories,\nwhich can be represented as heterogeneous social networks (HSN) formally.\nMeanwhile, in such an age of online social media, users usually participate in\nmultiple online social networks simultaneously to enjoy more social networks\nservices, who can act as bridges connecting different networks together. So\nmultiple HSNs not only represent information in single network, but also fuse\ninformation from multiple networks.\n  Formally, the online social networks sharing common users are named as the\naligned social networks, and these shared users who act like anchors aligning\nthe networks are called the anchor users. The heterogeneous information\ngenerated by users' social activities in the multiple aligned social networks\nprovides social network practitioners and researchers with the opportunities to\nstudy individual user's social behaviors across multiple social platforms\nsimultaneously. This paper presents a comprehensive survey about the latest\nresearch works on multiple aligned HSNs studies based on the broad learning\nsetting, which covers 5 major research tasks, i.e., network alignment, link\nprediction, community detection, information diffusion and network embedding\nrespectively. \n\n"}
{"id": "1804.10653", "contents": "Title: Sparse Group Inductive Matrix Completion Abstract: We consider the problem of matrix completion with side information\n(\\textit{inductive matrix completion}). In real-world applications many\nside-channel features are typically non-informative making feature selection an\nimportant part of the problem. We incorporate feature selection into inductive\nmatrix completion by proposing a matrix factorization framework with\ngroup-lasso regularization on side feature parameter matrices. We demonstrate,\nthat the theoretical sample complexity for the proposed method is much lower\ncompared to its competitors in sparse problems, and propose an efficient\noptimization algorithm for the resulting low-rank matrix completion problem\nwith sparsifying regularizers. Experiments on synthetic and real-world datasets\nshow that the proposed approach outperforms other methods. \n\n"}
{"id": "1804.11062", "contents": "Title: Equivalent Lipschitz surrogates for zero-norm and rank optimization\n  problems Abstract: This paper proposes a mechanism to produce equivalent Lipschitz surrogates\nfor zero-norm and rank optimization problems by means of the global exact\npenalty for their equivalent mathematical programs with an equilibrium\nconstraint (MPECs). Specifically, we reformulate these combinatorial problems\nas equivalent MPECs by the variational characterization of the zero-norm and\nrank function, show that their penalized problems, yielded by moving the\nequilibrium constraint into the objective, are the global exact penalization,\nand obtain the equivalent Lipschitz surrogates by eliminating the dual variable\nin the global exact penalty. These surrogates, including the popular SCAD\nfunction in statistics, are also difference of two convex functions (D.C.) if\nthe function and constraint set involved in zero-norm and rank optimization\nproblems are convex. We illustrate an application by designing a multi-stage\nconvex relaxation approach to the rank plus zero-norm regularized minimization\nproblem. \n\n"}
{"id": "1804.11242", "contents": "Title: Homology-Preserving Multi-Scale Graph Skeletonization Using Mapper on\n  Graphs Abstract: Node-link diagrams are a popular method for representing graphs that capture\nrelationships between individuals, businesses, proteins, and telecommunication\nendpoints. However, node-link diagrams may fail to convey insights regarding\ngraph structures, even for moderately sized data of a few hundred nodes, due to\nvisual clutter. We propose to apply the mapper construction -- a popular tool\nin topological data analysis -- to graph visualization, which provides a strong\ntheoretical basis for summarizing the data while preserving their core\nstructures. We develop a variation of the mapper construction targeting\nweighted, undirected graphs, called {\\mog}, which generates homology-preserving\nskeletons of graphs. We further show how the adjustment of a single parameter\nenables multi-scale skeletonization of the input graph. We provide a software\ntool that enables interactive explorations of such skeletons and demonstrate\nthe effectiveness of our method for synthetic and real-world data. \n\n"}
{"id": "1805.00692", "contents": "Title: Compressed Dictionary Learning Abstract: In this paper we show that the computational complexity of the Iterative\nThresholding and K-residual-Means (ITKrM) algorithm for dictionary learning can\nbe significantly reduced by using dimensionality-reduction techniques based on\nthe Johnson-Lindenstrauss lemma. The dimensionality reduction is efficiently\ncarried out with the fast Fourier transform. We introduce the Iterative\ncompressed-Thresholding and K-Means (IcTKM) algorithm for fast dictionary\nlearning and study its convergence properties. We show that IcTKM can locally\nrecover an incoherent, overcomplete generating dictionary of $K$ atoms from\ntraining signals of sparsity level $S$ with high probability. Fast dictionary\nlearning is achieved by embedding the training data and the dictionary into $m\n< d$ dimensions, and recovery is shown to be locally stable with an embedding\ndimension which scales as low as $m = O(S \\log^4 S \\log^3 K)$. The compression\neffectively shatters the data dimension bottleneck in the computational cost of\nITKrM, reducing it by a factor $O(m/d)$. Our theoretical results are\ncomplemented with numerical simulations which demonstrate that IcTKM is a\npowerful, low-cost algorithm for learning dictionaries from high-dimensional\ndata sets. \n\n"}
{"id": "1805.01078", "contents": "Title: Exploration of Numerical Precision in Deep Neural Networks Abstract: Reduced numerical precision is a common technique to reduce computational\ncost in many Deep Neural Networks (DNNs). While it has been observed that DNNs\nare resilient to small errors and noise, no general result exists that is\ncapable of predicting a given DNN system architecture's sensitivity to reduced\nprecision. In this project, we emulate arbitrary bit-width using a specified\nfloating-point representation with a truncation method, which is applied to the\nneural network after each batch. We explore the impact of several model\nparameters on the network's training accuracy and show results on the MNIST\ndataset. We then present a preliminary theoretical investigation of the error\nscaling in both forward and backward propagations. We end with a discussion of\nthe implications of these results as well as the potential for generalization\nto other network architectures. \n\n"}
{"id": "1805.01129", "contents": "Title: Does Journaling Encourage Healthier Choices? Analyzing Healthy Eating\n  Behaviors of Food Journalers Abstract: Past research has shown the benefits of food journaling in promoting mindful\neating and healthier food choices. However, the links between journaling and\nhealthy eating have not been thoroughly examined. Beyond caloric restriction,\ndo journalers consistently and sufficiently consume healthful diets? How\ndifferent are their eating habits compared to those of average consumers who\ntend to be less conscious about health? In this study, we analyze the healthy\neating behaviors of active food journalers using data from MyFitnessPal.\nSurprisingly, our findings show that food journalers do not eat as healthily as\nthey should despite their proclivity to health eating and their food choices\nresemble those of the general populace. Furthermore, we find that the\njournaling duration is only a marginal determinant of healthy eating outcomes\nand sociodemographic factors, such as gender and regions of residence, are much\nmore predictive of healthy food choices. \n\n"}
{"id": "1805.01209", "contents": "Title: Found Graph Data and Planted Vertex Covers Abstract: A typical way in which network data is recorded is to measure all the\ninteractions among a specified set of core nodes; this produces a graph\ncontaining this core together with a potentially larger set of fringe nodes\nthat have links to the core. Interactions between pairs of nodes in the fringe,\nhowever, are not recorded by this process, and hence not present in the\nresulting graph data. For example, a phone service provider may only have\nrecords of calls in which at least one of the participants is a customer; this\ncan include calls between a customer and a non-customer, but not between pairs\nof non-customers.\n  Knowledge of which nodes belong to the core is an important piece of metadata\nthat is crucial for interpreting the network dataset. But in many cases, this\nmetadata is not available, either because it has been lost due to difficulties\nin data provenance, or because the network consists of found data obtained in\nsettings such as counter-surveillance. This leads to a natural algorithmic\nproblem, namely the recovery of the core set. Since the core set forms a vertex\ncover of the graph, we essentially have a planted vertex cover problem, but\nwith an arbitrary underlying graph. We develop a theoretical framework for\nanalyzing this planted vertex cover problem, based on results in the theory of\nfixed-parameter tractability, together with algorithms for recovering the core.\nOur algorithms are fast, simple to implement, and out-perform several methods\nbased on network core-periphery structure on various real-world datasets. \n\n"}
{"id": "1805.01509", "contents": "Title: RECS: Robust Graph Embedding Using Connection Subgraphs Abstract: The success of graph embeddings or node representation learning in a variety\nof downstream tasks, such as node classification, link prediction, and\nrecommendation systems, has led to their popularity in recent years.\nRepresentation learning algorithms aim to preserve local and global network\nstructure by identifying node neighborhood notions. However, many existing\nalgorithms generate embeddings that fail to properly preserve the network\nstructure, or lead to unstable representations due to random processes (e.g.,\nrandom walks to generate context) and, thus, cannot generate to multi-graph\nproblems. In this paper, we propose RECS, a novel, stable graph embedding\nalgorithmic framework. RECS learns graph representations using connection\nsubgraphs by employing the analogy of graphs with electrical circuits. It\npreserves both local and global connectivity patterns, and addresses the issue\nof high-degree nodes. Further, it exploits the strength of weak ties and\nmeta-data that have been neglected by baselines. The experiments show that RECS\noutperforms state-of-the-art algorithms by up to 36.85% on multi-label\nclassification problem. Further, in contrast to baselines, RECS, being\ndeterministic, is completely stable. \n\n"}
{"id": "1805.01892", "contents": "Title: Opinion modeling on social media and marketing aspects Abstract: We introduce and discuss kinetic models of opinion formation on social\nnetworks in which the distribution function depends on both the opinion and the\nconnectivity of the agents. The opinion formation model is subsequently coupled\nwith a kinetic model describing the spreading of popularity of a product on the\nweb through a social network. Numerical experiments on the underlying kinetic\nmodels show a good qualitative agreement with some measured trends of hashtags\non social media websites and illustrate how companies can take advantage of the\nnetwork structure to obtain at best the advertisement of their products. \n\n"}
{"id": "1805.01930", "contents": "Title: Enhancing the Regularization Effect of Weight Pruning in Artificial\n  Neural Networks Abstract: Artificial neural networks (ANNs) may not be worth their computational/memory\ncosts when used in mobile phones or embedded devices. Parameter-pruning\nalgorithms combat these costs, with some algorithms capable of removing over\n90% of an ANN's weights without harming the ANN's performance. Removing weights\nfrom an ANN is a form of regularization, but existing pruning algorithms do not\nsignificantly improve generalization error. We show that pruning ANNs can\nimprove generalization if pruning targets large weights instead of small\nweights. Applying our pruning algorithm to an ANN leads to a higher image\nclassification accuracy on CIFAR-10 data than applying the popular regularizer\ndropout. The pruning couples this higher accuracy with an 85% reduction of the\nANN's parameter count. \n\n"}
{"id": "1805.02087", "contents": "Title: A Constraint-Based Algorithm For Causal Discovery with Cycles, Latent\n  Variables and Selection Bias Abstract: Causal processes in nature may contain cycles, and real datasets may violate\ncausal sufficiency as well as contain selection bias. No constraint-based\ncausal discovery algorithm can currently handle cycles, latent variables and\nselection bias (CLS) simultaneously. I therefore introduce an algorithm called\nCyclic Causal Inference (CCI) that makes sound inferences with a conditional\nindependence oracle under CLS, provided that we can represent the cyclic causal\nprocess as a non-recursive linear structural equation model with independent\nerrors. Empirical results show that CCI outperforms CCD in the cyclic case as\nwell as rivals FCI and RFCI in the acyclic case. \n\n"}
{"id": "1805.04785", "contents": "Title: An Optimal Policy for Dynamic Assortment Planning Under Uncapacitated\n  Multinomial Logit Models Abstract: We study the dynamic assortment planning problem, where for each arriving\ncustomer, the seller offers an assortment of substitutable products and\ncustomer makes the purchase among offered products according to an\nuncapacitated multinomial logit (MNL) model. Since all the utility parameters\nof MNL are unknown, the seller needs to simultaneously learn customers' choice\nbehavior and make dynamic decisions on assortments based on the current\nknowledge. The goal of the seller is to maximize the expected revenue, or\nequivalently, to minimize the expected regret. Although dynamic assortment\nplanning problem has received an increasing attention in revenue management,\nmost existing policies require the estimation of mean utility for each product\nand the final regret usually involves the number of products $N$. The optimal\nregret of the dynamic assortment planning problem under the most basic and\npopular choice model---MNL model is still open. By carefully analyzing a\nrevenue potential function, we develop a trisection based policy combined with\nadaptive confidence bound construction, which achieves an {item-independent}\nregret bound of $O(\\sqrt{T})$, where $T$ is the length of selling horizon. We\nfurther establish the matching lower bound result to show the optimality of our\npolicy. There are two major advantages of the proposed policy. First, the\nregret of all our policies has no dependence on $N$. Second, our policies are\nalmost assumption free: there is no assumption on mean utility nor any\n\"separability\" condition on the expected revenues for different assortments.\nOur result also extends the unimodal bandit literature. \n\n"}
{"id": "1805.05189", "contents": "Title: Randomized Smoothing SVRG for Large-scale Nonsmooth Convex Optimization Abstract: In this paper, we consider the problem of minimizing the average of a large\nnumber of nonsmooth and convex functions. Such problems often arise in typical\nmachine learning problems as empirical risk minimization, but are\ncomputationally very challenging. We develop and analyze a new algorithm that\nachieves robust linear convergence rate, and both its time complexity and\ngradient complexity are superior than state-of-art nonsmooth algorithms and\nsubgradient-based schemes. Besides, our algorithm works without any extra error\nbound conditions on the objective function as well as the common\nstrongly-convex condition. We show that our algorithm has wide applications in\noptimization and machine learning problems, and demonstrate experimentally that\nit performs well on a large-scale ranking problem. \n\n"}
{"id": "1805.05491", "contents": "Title: Crowdbreaks: Tracking Health Trends using Public Social Media Data and\n  Crowdsourcing Abstract: In the past decade, tracking health trends using social media data has shown\ngreat promise, due to a powerful combination of massive adoption of social\nmedia around the world, and increasingly potent hardware and software that\nenables us to work with these new big data streams. At the same time, many\nchallenging problems have been identified. First, there is often a mismatch\nbetween how rapidly online data can change, and how rapidly algorithms are\nupdated, which means that there is limited reusability for algorithms trained\non past data as their performance decreases over time. Second, much of the work\nis focusing on specific issues during a specific past period in time, even\nthough public health institutions would need flexible tools to assess multiple\nevolving situations in real time. Third, most tools providing such capabilities\nare proprietary systems with little algorithmic or data transparency, and thus\nlittle buy-in from the global public health and research community. Here, we\nintroduce Crowdbreaks, an open platform which allows tracking of health trends\nby making use of continuous crowdsourced labelling of public social media\ncontent. The system is built in a way which automatizes the typical workflow\nfrom data collection, filtering, labelling and training of machine learning\nclassifiers and therefore can greatly accelerate the research process in the\npublic health domain. This work introduces the technical aspects of the\nplatform and explores its future use cases. \n\n"}
{"id": "1805.07226", "contents": "Title: Sequential Neural Likelihood: Fast Likelihood-free Inference with\n  Autoregressive Flows Abstract: We present Sequential Neural Likelihood (SNL), a new method for Bayesian\ninference in simulator models, where the likelihood is intractable but\nsimulating data from the model is possible. SNL trains an autoregressive flow\non simulated data in order to learn a model of the likelihood in the region of\nhigh posterior density. A sequential training procedure guides simulations and\nreduces simulation cost by orders of magnitude. We show that SNL is more\nrobust, more accurate and requires less tuning than related neural-based\nmethods, and we discuss diagnostics for assessing calibration, convergence and\ngoodness-of-fit. \n\n"}
{"id": "1805.07479", "contents": "Title: Semisupervised Learning on Heterogeneous Graphs and its Applications to\n  Facebook News Feed Abstract: Graph-based semi-supervised learning is a fundamental machine learning\nproblem, and has been well studied. Most studies focus on homogeneous networks\n(e.g. citation network, friend network). In the present paper, we propose the\nHeterogeneous Embedding Label Propagation (HELP) algorithm, a graph-based\nsemi-supervised deep learning algorithm, for graphs that are characterized by\nheterogeneous node types. Empirically, we demonstrate the effectiveness of this\nmethod in domain classification tasks with Facebook user-domain interaction\ngraph, and compare the performance of the proposed HELP algorithm with the\nstate of the art algorithms. We show that the HELP algorithm improves the\npredictive performance across multiple tasks, together with semantically\nmeaningful embedding that are discriminative for downstream classification or\nregression tasks. \n\n"}
{"id": "1805.07869", "contents": "Title: Learning Device Models with Recurrent Neural Networks Abstract: Recurrent neural networks (RNNs) are powerful constructs capable of modeling\ncomplex systems, up to and including Turing Machines. However, learning such\ncomplex models from finite training sets can be difficult. In this paper we\nempirically show that RNNs can learn models of computer peripheral devices\nthrough input and output state observation. This enables automated development\nof functional software-only models of hardware devices. Such models are\napplicable to any number of tasks, including device validation, driver\ndevelopment, code de-obfuscation, and reverse engineering. We show that the\nsame RNN structure successfully models six different devices from simple test\ncircuits up to a 16550 UART serial port, and verify that these models are\ncapable of producing equivalent output to real hardware. \n\n"}
{"id": "1805.07960", "contents": "Title: Stochastic Gradient Descent for Stochastic Doubly-Nonconvex Composite\n  Optimization Abstract: The stochastic gradient descent has been widely used for solving composite\noptimization problems in big data analyses. Many algorithms and convergence\nproperties have been developed. The composite functions were convex primarily\nand gradually nonconvex composite functions have been adopted to obtain more\ndesirable properties. The convergence properties have been investigated, but\nonly when either of composite functions is nonconvex. There is no convergence\nproperty when both composite functions are nonconvex, which is named the\n\\textit{doubly-nonconvex} case.To overcome this difficulty, we assume a simple\nand weak condition that the penalty function is \\textit{quasiconvex} and then\nwe obtain convergence properties for the stochastic doubly-nonconvex composite\noptimization problem.The convergence rate obtained here is of the same order as\nthe existing work.We deeply analyze the convergence rate with the constant step\nsize and mini-batch size and give the optimal convergence rate with appropriate\nsizes, which is superior to the existing work. Experimental results illustrate\nthat our method is superior to existing methods. \n\n"}
{"id": "1805.08102", "contents": "Title: PiPs: a Kernel-based Optimization Scheme for Analyzing Non-Stationary 1D\n  Signals Abstract: This paper proposes a novel kernel-based optimization scheme to handle tasks\nin the analysis, e.g., signal spectral estimation and single-channel source\nseparation of 1D non-stationary oscillatory data. The key insight of our\noptimization scheme for reconstructing the time-frequency information is that\nwhen a nonparametric regression is applied on some input values, the output\nregressed points would lie near the oscillatory pattern of the oscillatory 1D\nsignal only if these input values are a good approximation of the ground-truth\nphase function. In this work, Gaussian Process (GP) is chosen to conduct this\nnonparametric regression: the oscillatory pattern is encoded as the\nPattern-inducing Points (PiPs) which act as the training data points in the GP\nregression; while the targeted phase function is fed in to compute the\ncorrelation kernels, acting as the testing input. Better approximated phase\nfunction generates more precise kernels, thus resulting in smaller optimization\nloss error when comparing the kernel-based regression output with the original\nsignals. To the best of our knowledge, this is the first algorithm that can\nsatisfactorily handle fully non-stationary oscillatory data, close and\ncrossover frequencies, and general oscillatory patterns. Even in the example of\na signal {produced by slow variation in the parameters of a trigonometric\nexpansion}, we show that PiPs admits competitive or better performance in terms\nof accuracy and robustness than existing state-of-the-art algorithms. \n\n"}
{"id": "1805.08114", "contents": "Title: On the Convergence of Stochastic Gradient Descent with Adaptive\n  Stepsizes Abstract: Stochastic gradient descent is the method of choice for large scale\noptimization of machine learning objective functions. Yet, its performance is\ngreatly variable and heavily depends on the choice of the stepsizes. This has\nmotivated a large body of research on adaptive stepsizes. However, there is\ncurrently a gap in our theoretical understanding of these methods, especially\nin the non-convex setting. In this paper, we start closing this gap: we\ntheoretically analyze in the convex and non-convex settings a generalized\nversion of the AdaGrad stepsizes. We show sufficient conditions for these\nstepsizes to achieve almost sure asymptotic convergence of the gradients to\nzero, proving the first guarantee for generalized AdaGrad stepsizes in the\nnon-convex setting. Moreover, we show that these stepsizes allow to\nautomatically adapt to the level of noise of the stochastic gradients in both\nthe convex and non-convex settings, interpolating between $O(1/T)$ and\n$O(1/\\sqrt{T})$, up to logarithmic terms. \n\n"}
{"id": "1805.08114", "contents": "Title: On the Convergence of Stochastic Gradient Descent with Adaptive\n  Stepsizes Abstract: Stochastic gradient descent is the method of choice for large scale\noptimization of machine learning objective functions. Yet, its performance is\ngreatly variable and heavily depends on the choice of the stepsizes. This has\nmotivated a large body of research on adaptive stepsizes. However, there is\ncurrently a gap in our theoretical understanding of these methods, especially\nin the non-convex setting. In this paper, we start closing this gap: we\ntheoretically analyze in the convex and non-convex settings a generalized\nversion of the AdaGrad stepsizes. We show sufficient conditions for these\nstepsizes to achieve almost sure asymptotic convergence of the gradients to\nzero, proving the first guarantee for generalized AdaGrad stepsizes in the\nnon-convex setting. Moreover, we show that these stepsizes allow to\nautomatically adapt to the level of noise of the stochastic gradients in both\nthe convex and non-convex settings, interpolating between $O(1/T)$ and\n$O(1/\\sqrt{T})$, up to logarithmic terms. \n\n"}
{"id": "1805.08168", "contents": "Title: \"You Know What to Do\": Proactive Detection of YouTube Videos Targeted by\n  Coordinated Hate Attacks Abstract: Video sharing platforms like YouTube are increasingly targeted by aggression\nand hate attacks. Prior work has shown how these attacks often take place as a\nresult of \"raids,\" i.e., organized efforts by ad-hoc mobs coordinating from\nthird-party communities. Despite the increasing relevance of this phenomenon,\nhowever, online services often lack effective countermeasures to mitigate it.\nUnlike well-studied problems like spam and phishing, coordinated aggressive\nbehavior both targets and is perpetrated by humans, making defense mechanisms\nthat look for automated activity unsuitable. Therefore, the de-facto solution\nis to reactively rely on user reports and human moderation.\n  In this paper, we propose an automated solution to identify YouTube videos\nthat are likely to be targeted by coordinated harassers from fringe communities\nlike 4chan. First, we characterize and model YouTube videos along several axes\n(metadata, audio transcripts, thumbnails) based on a ground truth dataset of\nvideos that were targeted by raids. Then, we use an ensemble of classifiers to\ndetermine the likelihood that a video will be raided with very good results\n(AUC up to 94%). Overall, our work provides an important first step towards\ndeploying proactive systems to detect and mitigate coordinated hate attacks on\nplatforms like YouTube. \n\n"}
{"id": "1805.08266", "contents": "Title: On the Selection of Initialization and Activation Function for Deep\n  Neural Networks Abstract: The weight initialization and the activation function of deep neural networks\nhave a crucial impact on the performance of the training procedure. An\ninappropriate selection can lead to the loss of information of the input during\nforward propagation and the exponential vanishing/exploding of gradients during\nback-propagation. Understanding the theoretical properties of untrained random\nnetworks is key to identifying which deep networks may be trained successfully\nas recently demonstrated by Schoenholz et al. (2017) who showed that for deep\nfeedforward neural networks only a specific choice of hyperparameters known as\nthe `edge of chaos' can lead to good performance. We complete this analysis by\nproviding quantitative results showing that, for a class of ReLU-like\nactivation functions, the information propagates indeed deeper for an\ninitialization at the edge of chaos. By further extending this analysis, we\nidentify a class of activation functions that improve the information\npropagation over ReLU-like functions. This class includes the Swish activation,\n$\\phi_{swish}(x) = x \\cdot \\text{sigmoid}(x)$, used in Hendrycks & Gimpel\n(2016), Elfwing et al. (2017) and Ramachandran et al. (2017). This provides a\ntheoretical grounding for the excellent empirical performance of $\\phi_{swish}$\nobserved in these contributions. We complement those previous results by\nillustrating the benefit of using a random initialization on the edge of chaos\nin this context. \n\n"}
{"id": "1805.08273", "contents": "Title: Multiple Causal Inference with Latent Confounding Abstract: Causal inference from observational data requires assumptions. These\nassumptions range from measuring confounders to identifying instruments.\nTraditionally, causal inference assumptions have focused on estimation of\neffects for a single treatment. In this work, we construct techniques for\nestimation with multiple treatments in the presence of unobserved confounding.\nWe develop two assumptions based on shared confounding between treatments and\nindependence of treatments given the confounder. Together, these assumptions\nlead to a confounder estimator regularized by mutual information. For this\nestimator, we develop a tractable lower bound. To recover treatment effects, we\nuse the residual information in the treatments independent of the confounder.\nWe validate on simulations and an example from clinical medicine. \n\n"}
{"id": "1805.08306", "contents": "Title: Deep Energy Estimator Networks Abstract: Density estimation is a fundamental problem in statistical learning. This\nproblem is especially challenging for complex high-dimensional data due to the\ncurse of dimensionality. A promising solution to this problem is given here in\nan inference-free hierarchical framework that is built on score matching. We\nrevisit the Bayesian interpretation of the score function and the Parzen score\nmatching, and construct a multilayer perceptron with a scalable objective for\nlearning the energy (i.e. the unnormalized log-density), which is then\noptimized with stochastic gradient descent. In addition, the resulting deep\nenergy estimator network (DEEN) is designed as products of experts. We present\nthe utility of DEEN in learning the energy, the score function, and in\nsingle-step denoising experiments for synthetic and high-dimensional data. We\nalso diagnose stability problems in the direct estimation of the score function\nthat had been observed for denoising autoencoders. \n\n"}
{"id": "1805.08463", "contents": "Title: Variational Learning on Aggregate Outputs with Gaussian Processes Abstract: While a typical supervised learning framework assumes that the inputs and the\noutputs are measured at the same levels of granularity, many applications,\nincluding global mapping of disease, only have access to outputs at a much\ncoarser level than that of the inputs. Aggregation of outputs makes\ngeneralization to new inputs much more difficult. We consider an approach to\nthis problem based on variational learning with a model of output aggregation\nand Gaussian processes, where aggregation leads to intractability of the\nstandard evidence lower bounds. We propose new bounds and tractable\napproximations, leading to improved prediction accuracy and scalability to\nlarge datasets, while explicitly taking uncertainty into account. We develop a\nframework which extends to several types of likelihoods, including the Poisson\nmodel for aggregated count data. We apply our framework to a challenging and\nimportant problem, the fine-scale spatial modelling of malaria incidence, with\nover 1 million observations. \n\n"}
{"id": "1805.08647", "contents": "Title: Multi-Statistic Approximate Bayesian Computation with Multi-Armed\n  Bandits Abstract: Approximate Bayesian computation is an established and popular method for\nlikelihood-free inference with applications in many disciplines. The\neffectiveness of the method depends critically on the availability of well\nperforming summary statistics. Summary statistic selection relies heavily on\ndomain knowledge and carefully engineered features, and can be a laborious time\nconsuming process. Since the method is sensitive to data dimensionality, the\nprocess of selecting summary statistics must balance the need to include\ninformative statistics and the dimensionality of the feature vector. This paper\nproposes to treat the problem of dynamically selecting an appropriate summary\nstatistic from a given pool of candidate summary statistics as a multi-armed\nbandit problem. This allows approximate Bayesian computation rejection sampling\nto dynamically focus on a distribution over well performing summary statistics\nas opposed to a fixed set of statistics. The proposed method is unique in that\nit does not require any pre-processing and is scalable to a large number of\ncandidate statistics. This enables efficient use of a large library of possible\ntime series summary statistics without prior feature engineering. The proposed\napproach is compared to state-of-the-art methods for summary statistics\nselection using a challenging test problem from the systems biology literature. \n\n"}
{"id": "1805.08749", "contents": "Title: A Tropical Approach to Neural Networks with Piecewise Linear Activations Abstract: We present a new, unifying approach following some recent developments on the\ncomplexity of neural networks with piecewise linear activations. We treat\nneural network layers with piecewise linear activations as tropical\npolynomials, which generalize polynomials in the so-called $(\\max, +)$ or\ntropical algebra, with possibly real-valued exponents. Motivated by the\ndiscussion in (arXiv:1402.1869), this approach enables us to refine their upper\nbounds on linear regions of layers with ReLU or leaky ReLU activations to\n$\\min\\left\\{ 2^m, \\sum_{j=0}^n \\binom{m}{j} \\right\\}$, where $n, m$ are the\nnumber of inputs and outputs, respectively. Additionally, we recover their\nupper bounds on maxout layers. Our work follows a novel path, exclusively under\nthe lens of tropical geometry, which is independent of the improvements\nreported in (arXiv:1611.01491, arXiv:1711.02114). Finally, we present a\ngeometric approach for effective counting of linear regions using random\nsampling in order to avoid the computational overhead of exact counting\napproaches \n\n"}
{"id": "1805.09091", "contents": "Title: Neural networks for post-processing ensemble weather forecasts Abstract: Ensemble weather predictions require statistical post-processing of\nsystematic errors to obtain reliable and accurate probabilistic forecasts.\nTraditionally, this is accomplished with distributional regression models in\nwhich the parameters of a predictive distribution are estimated from a training\nperiod. We propose a flexible alternative based on neural networks that can\nincorporate nonlinear relationships between arbitrary predictor variables and\nforecast distribution parameters that are automatically learned in a\ndata-driven way rather than requiring pre-specified link functions. In a case\nstudy of 2-meter temperature forecasts at surface stations in Germany, the\nneural network approach significantly outperforms benchmark post-processing\nmethods while being computationally more affordable. Key components to this\nimprovement are the use of auxiliary predictor variables and station-specific\ninformation with the help of embeddings. Furthermore, the trained neural\nnetwork can be used to gain insight into the importance of meteorological\nvariables thereby challenging the notion of neural networks as uninterpretable\nblack boxes. Our approach can easily be extended to other statistical\npost-processing and forecasting problems. We anticipate that recent advances in\ndeep learning combined with the ever-increasing amounts of model and\nobservation data will transform the post-processing of numerical weather\nforecasts in the coming decade. \n\n"}
{"id": "1805.09360", "contents": "Title: Deep Reinforcement Learning of Marked Temporal Point Processes Abstract: In a wide variety of applications, humans interact with a complex environment\nby means of asynchronous stochastic discrete events in continuous time. Can we\ndesign online interventions that will help humans achieve certain goals in such\nasynchronous setting? In this paper, we address the above problem from the\nperspective of deep reinforcement learning of marked temporal point processes,\nwhere both the actions taken by an agent and the feedback it receives from the\nenvironment are asynchronous stochastic discrete events characterized using\nmarked temporal point processes. In doing so, we define the agent's policy\nusing the intensity and mark distribution of the corresponding process and then\nderive a flexible policy gradient method, which embeds the agent's actions and\nthe feedback it receives into real-valued vectors using deep recurrent neural\nnetworks. Our method does not make any assumptions on the functional form of\nthe intensity and mark distribution of the feedback and it allows for\narbitrarily complex reward functions. We apply our methodology to two different\napplications in personalized teaching and viral marketing and, using data\ngathered from Duolingo and Twitter, we show that it may be able to find\ninterventions to help learners and marketers achieve their goals more\neffectively than alternatives. \n\n"}
{"id": "1805.09781", "contents": "Title: Efficient Inference in Multi-task Cox Process Models Abstract: We generalize the log Gaussian Cox process (LGCP) framework to model multiple\ncorrelated point data jointly. The observations are treated as realizations of\nmultiple LGCPs, whose log intensities are given by linear combinations of\nlatent functions drawn from Gaussian process priors. The combination\ncoefficients are also drawn from Gaussian processes and can incorporate\nadditional dependencies. We derive closed-form expressions for the moments of\nthe intensity functions and develop an efficient variational inference\nalgorithm that is orders of magnitude faster than competing deterministic and\nstochastic approximations of multivariate LGCP, coregionalization models, and\nmulti-task permanental processes. Our approach outperforms these benchmarks in\nmultiple problems, offering the current state of the art in modeling\nmultivariate point processes. \n\n"}
{"id": "1805.09906", "contents": "Title: Diffusion Maps for Textual Network Embedding Abstract: Textual network embedding leverages rich text information associated with the\nnetwork to learn low-dimensional vectorial representations of vertices. Rather\nthan using typical natural language processing (NLP) approaches, recent\nresearch exploits the relationship of texts on the same edge to graphically\nembed text. However, these models neglect to measure the complete level of\nconnectivity between any two texts in the graph. We present diffusion maps for\ntextual network embedding (DMTE), integrating global structural information of\nthe graph to capture the semantic relatedness between texts, with a\ndiffusion-convolution operation applied on the text inputs. In addition, a new\nobjective function is designed to efficiently preserve the high-order proximity\nusing the graph diffusion. Experimental results show that the proposed approach\noutperforms state-of-the-art methods on the vertex-classification and\nlink-prediction tasks. \n\n"}
{"id": "1805.09921", "contents": "Title: Meta-Learning Probabilistic Inference For Prediction Abstract: This paper introduces a new framework for data efficient and versatile\nlearning. Specifically: 1) We develop ML-PIP, a general framework for\nMeta-Learning approximate Probabilistic Inference for Prediction. ML-PIP\nextends existing probabilistic interpretations of meta-learning to cover a\nbroad class of methods. 2) We introduce VERSA, an instance of the framework\nemploying a flexible and versatile amortization network that takes few-shot\nlearning datasets as inputs, with arbitrary numbers of shots, and outputs a\ndistribution over task-specific parameters in a single forward pass. VERSA\nsubstitutes optimization at test time with forward passes through inference\nnetworks, amortizing the cost of inference and relieving the need for second\nderivatives during training. 3) We evaluate VERSA on benchmark datasets where\nthe method sets new state-of-the-art results, handles arbitrary numbers of\nshots, and for classification, arbitrary numbers of classes at train and test\ntime. The power of the approach is then demonstrated through a challenging\nfew-shot ShapeNet view reconstruction task. \n\n"}
{"id": "1805.10118", "contents": "Title: Analyzing high-dimensional time-series data using kernel transfer\n  operator eigenfunctions Abstract: Kernel transfer operators, which can be regarded as approximations of\ntransfer operators such as the Perron-Frobenius or Koopman operator in\nreproducing kernel Hilbert spaces, are defined in terms of covariance and\ncross-covariance operators and have been shown to be closely related to the\nconditional mean embedding framework developed by the machine learning\ncommunity. The goal of this paper is to show how the dominant eigenfunctions of\nthese operators in combination with gradient-based optimization techniques can\nbe used to detect long-lived coherent patterns in high-dimensional time-series\ndata. The results will be illustrated using video data and a fluid flow\nexample. \n\n"}
{"id": "1805.10341", "contents": "Title: An end-to-end Differentially Private Latent Dirichlet Allocation Using a\n  Spectral Algorithm Abstract: We provide an end-to-end differentially private spectral algorithm for\nlearning LDA, based on matrix/tensor decompositions, and establish theoretical\nguarantees on utility/consistency of the estimated model parameters. The\nspectral algorithm consists of multiple algorithmic steps, named as \"{edges}\",\nto which noise could be injected to obtain differential privacy. We identify\n\\emph{subsets of edges}, named as \"{configurations}\", such that adding noise to\nall edges in such a subset guarantees differential privacy of the end-to-end\nspectral algorithm. We characterize the sensitivity of the edges with respect\nto the input and thus estimate the amount of noise to be added to each edge for\nany required privacy level. We then characterize the utility loss for each\nconfiguration as a function of injected noise. Overall, by combining the\nsensitivity and utility characterization, we obtain an end-to-end\ndifferentially private spectral algorithm for LDA and identify the\ncorresponding configuration that outperforms others in any specific regime. We\nare the first to achieve utility guarantees under the required level of\ndifferential privacy for learning in LDA. Overall our method systematically\noutperforms differentially private variational inference. \n\n"}
{"id": "1805.10348", "contents": "Title: Guaranteed Simultaneous Asymmetric Tensor Decomposition via\n  Orthogonalized Alternating Least Squares Abstract: Tensor CANDECOMP/PARAFAC (CP) decomposition is an important tool that solves\na wide class of machine learning problems. Existing popular approaches recover\ncomponents one by one, not necessarily in the order of larger components first.\nRecently developed simultaneous power method obtains only a high probability\nrecovery of top $r$ components even when the observed tensor is noiseless. We\npropose a Slicing Initialized Alternating Subspace Iteration (s-ASI) method\nthat is guaranteed to recover top $r$ components ($\\epsilon$-close)\nsimultaneously for (a)symmetric tensors almost surely under the noiseless case\n(with high probability for a bounded noise) using $O(\\log(\\log\n\\frac{1}{\\epsilon}))$ steps of tensor subspace iterations. Our s-ASI method\nintroduces a Slice-Based Initialization that runs\n$O(1/\\log(\\frac{\\lambda_r}{\\lambda_{r+1}}))$ steps of matrix subspace\niterations, where $\\lambda_r$ denotes the r-th top singular value of the\ntensor. We are the first to provide a theoretical guarantee on simultaneous\northogonal asymmetric tensor decomposition. Under the noiseless case, we are\nthe first to provide an \\emph{almost sure} theoretical guarantee on\nsimultaneous orthogonal tensor decomposition. When tensor is noisy, our\nalgorithm for asymmetric tensor is robust to noise smaller than\n$\\min\\{O(\\frac{(\\lambda_r - \\lambda_{r+1})\\epsilon}{\\sqrt{r}}),\nO(\\delta_0\\frac{\\lambda_r -\\lambda_{r+1}}{\\sqrt{d}})\\}$, where $\\delta_0$ is a\nsmall constant proportional to the probability of bad initializations in the\nnoisy setting. \n\n"}
{"id": "1805.10391", "contents": "Title: Estimating Shell-Index in a Graph with Local Information Abstract: For network scientists, it has always been an interesting problem to identify\nthe influential nodes in a given network. The k-shell decomposition method is a\nwidely used method which assigns a shell-index value to each node based on its\ninfluential power. The k-shell method requires the global information of the\nnetwork to compute the shell-index of a node that is infeasible for large-scale\nreal-world dynamic networks. In this work, we propose a method to estimate the\nshell-index of a node using its local information. We also propose\nhill-climbing based approach to hit the top-ranked nodes in a small number of\nsteps. We further discuss a method to estimate the rank of a node based on the\nproposed estimator. \n\n"}
{"id": "1805.10522", "contents": "Title: Calibrating Deep Convolutional Gaussian Processes Abstract: The wide adoption of Convolutional Neural Networks (CNNs) in applications\nwhere decision-making under uncertainty is fundamental, has brought a great\ndeal of attention to the ability of these models to accurately quantify the\nuncertainty in their predictions. Previous work on combining CNNs with Gaussian\nprocesses (GPs) has been developed under the assumption that the predictive\nprobabilities of these models are well-calibrated. In this paper we show that,\nin fact, current combinations of CNNs and GPs are miscalibrated. We proposes a\nnovel combination that considerably outperforms previous approaches on this\naspect, while achieving state-of-the-art performance on image classification\ntasks. \n\n"}
{"id": "1805.11380", "contents": "Title: Kernel embedding of maps for sequential Bayesian inference: The\n  variational mapping particle filter Abstract: In this work, a novel sequential Monte Carlo filter is introduced which aims\nat efficient sampling of high-dimensional state spaces with a limited number of\nparticles. Particles are pushed forward from the prior to the posterior density\nusing a sequence of mappings that minimizes the Kullback-Leibler divergence\nbetween the posterior and the sequence of intermediate densities. The sequence\nof mappings represents a gradient flow. A key ingredient of the mappings is\nthat they are embedded in a reproducing kernel Hilbert space, which allows for\na practical and efficient algorithm. The embedding provides a direct means to\ncalculate the gradient of the Kullback-Leibler divergence leading to quick\nconvergence using well-known gradient-based stochastic optimization algorithms.\nEvaluation of the method is conducted in the chaotic Lorenz-63 system, the\nLorenz-96 system, which is a coarse prototype of atmospheric dynamics, and an\nepidemic model that describes cholera dynamics. No resampling is required in\nthe mapping particle filter even for long recursive sequences. The number of\neffective particles remains close to the total number of particles in all the\nexperiments. \n\n"}
{"id": "1805.11454", "contents": "Title: Distributed Stochastic Gradient Tracking Methods Abstract: In this paper, we study the problem of distributed multi-agent optimization\nover a network, where each agent possesses a local cost function that is smooth\nand strongly convex. The global objective is to find a common solution that\nminimizes the average of all cost functions. Assuming agents only have access\nto unbiased estimates of the gradients of their local cost functions, we\nconsider a distributed stochastic gradient tracking method (DSGT) and a\ngossip-like stochastic gradient tracking method (GSGT). We show that, in\nexpectation, the iterates generated by each agent are attracted to a\nneighborhood of the optimal solution, where they accumulate exponentially fast\n(under a constant stepsize choice). Under DSGT, the limiting (expected) error\nbounds on the distance of the iterates from the optimal solution decrease with\nthe network size $n$, which is a comparable performance to a centralized\nstochastic gradient algorithm. Moreover, we show that when the network is\nwell-connected, GSGT incurs lower communication cost than DSGT while\nmaintaining a similar computational cost. Numerical example further\ndemonstrates the effectiveness of the proposed methods. \n\n"}
{"id": "1805.11454", "contents": "Title: Distributed Stochastic Gradient Tracking Methods Abstract: In this paper, we study the problem of distributed multi-agent optimization\nover a network, where each agent possesses a local cost function that is smooth\nand strongly convex. The global objective is to find a common solution that\nminimizes the average of all cost functions. Assuming agents only have access\nto unbiased estimates of the gradients of their local cost functions, we\nconsider a distributed stochastic gradient tracking method (DSGT) and a\ngossip-like stochastic gradient tracking method (GSGT). We show that, in\nexpectation, the iterates generated by each agent are attracted to a\nneighborhood of the optimal solution, where they accumulate exponentially fast\n(under a constant stepsize choice). Under DSGT, the limiting (expected) error\nbounds on the distance of the iterates from the optimal solution decrease with\nthe network size $n$, which is a comparable performance to a centralized\nstochastic gradient algorithm. Moreover, we show that when the network is\nwell-connected, GSGT incurs lower communication cost than DSGT while\nmaintaining a similar computational cost. Numerical example further\ndemonstrates the effectiveness of the proposed methods. \n\n"}
{"id": "1805.11494", "contents": "Title: Efficient Bayesian Inference for a Gaussian Process Density Model Abstract: We reconsider a nonparametric density model based on Gaussian processes. By\naugmenting the model with latent P\\'olya--Gamma random variables and a latent\nmarked Poisson process we obtain a new likelihood which is conjugate to the\nmodel's Gaussian process prior. The augmented posterior allows for efficient\ninference by Gibbs sampling and an approximate variational mean field approach.\nFor the latter we utilise sparse GP approximations to tackle the infinite\ndimensionality of the problem. The performance of both algorithms and\ncomparisons with other density estimators are demonstrated on artificial and\nreal datasets with up to several thousand data points. \n\n"}
{"id": "1805.11604", "contents": "Title: How Does Batch Normalization Help Optimization? Abstract: Batch Normalization (BatchNorm) is a widely adopted technique that enables\nfaster and more stable training of deep neural networks (DNNs). Despite its\npervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly\nunderstood. The popular belief is that this effectiveness stems from\ncontrolling the change of the layers' input distributions during training to\nreduce the so-called \"internal covariate shift\". In this work, we demonstrate\nthat such distributional stability of layer inputs has little to do with the\nsuccess of BatchNorm. Instead, we uncover a more fundamental impact of\nBatchNorm on the training process: it makes the optimization landscape\nsignificantly smoother. This smoothness induces a more predictive and stable\nbehavior of the gradients, allowing for faster training. \n\n"}
{"id": "1805.11769", "contents": "Title: Fast Incremental von Neumann Graph Entropy Computation: Theory,\n  Algorithm, and Applications Abstract: The von Neumann graph entropy (VNGE) facilitates measurement of information\ndivergence and distance between graphs in a graph sequence. It has been\nsuccessfully applied to various learning tasks driven by network-based data.\nWhile effective, VNGE is computationally demanding as it requires the full\neigenspectrum of the graph Laplacian matrix. In this paper, we propose a new\ncomputational framework, Fast Incremental von Neumann Graph EntRopy (FINGER),\nwhich approaches VNGE with a performance guarantee. FINGER reduces the cubic\ncomplexity of VNGE to linear complexity in the number of nodes and edges, and\nthus enables online computation based on incremental graph changes. We also\nshow asymptotic equivalence of FINGER to the exact VNGE, and derive its\napproximation error bounds. Based on FINGER, we propose efficient algorithms\nfor computing Jensen-Shannon distance between graphs. Our experimental results\non different random graph models demonstrate the computational efficiency and\nthe asymptotic equivalence of FINGER. In addition, we apply FINGER to two\nreal-world applications and one synthesized anomaly detection dataset, and\ncorroborate its superior performance over seven baseline graph similarity\nmethods. \n\n"}
{"id": "1805.11811", "contents": "Title: Stochastic Zeroth-order Optimization via Variance Reduction method Abstract: Derivative-free optimization has become an important technique used in\nmachine learning for optimizing black-box models. To conduct updates without\nexplicitly computing gradient, most current approaches iteratively sample a\nrandom search direction from Gaussian distribution and compute the estimated\ngradient along that direction. However, due to the variance in the search\ndirection, the convergence rates and query complexities of existing methods\nsuffer from a factor of $d$, where $d$ is the problem dimension. In this paper,\nwe introduce a novel Stochastic Zeroth-order method with Variance Reduction\nunder Gaussian smoothing (SZVR-G) and establish the complexity for optimizing\nnon-convex problems. With variance reduction on both sample space and search\nspace, the complexity of our algorithm is sublinear to $d$ and is strictly\nbetter than current approaches, in both smooth and non-smooth cases. Moreover,\nwe extend the proposed method to the mini-batch version. Our experimental\nresults demonstrate the superior performance of the proposed method over\nexisting derivative-free optimization techniques. Furthermore, we successfully\napply our method to conduct a universal black-box attack to deep neural\nnetworks and present some interesting results. \n\n"}
{"id": "1805.12296", "contents": "Title: Root-cause Analysis for Time-series Anomalies via Spatiotemporal\n  Graphical Modeling in Distributed Complex Systems Abstract: Performance monitoring, anomaly detection, and root-cause analysis in complex\ncyber-physical systems (CPSs) are often highly intractable due to widely\ndiverse operational modes, disparate data types, and complex fault propagation\nmechanisms. This paper presents a new data-driven framework for root-cause\nanalysis, based on a spatiotemporal graphical modeling approach built on the\nconcept of symbolic dynamics for discovering and representing causal\ninteractions among sub-systems of complex CPSs. We formulate the root-cause\nanalysis problem as a minimization problem via the proposed inference based\nmetric and present two approximate approaches for root-cause analysis, namely\nthe sequential state switching ($S^3$, based on free energy concept of a\nrestricted Boltzmann machine, RBM) and artificial anomaly association ($A^3$, a\nclassification framework using deep neural networks, DNN). Synthetic data from\ncases with failed pattern(s) and anomalous node(s) are simulated to validate\nthe proposed approaches. Real dataset based on Tennessee Eastman process (TEP)\nis also used for comparison with other approaches. The results show that: (1)\n$S^3$ and $A^3$ approaches can obtain high accuracy in root-cause analysis\nunder both pattern-based and node-based fault scenarios, in addition to\nsuccessfully handling multiple nominal operating modes, (2) the proposed\ntool-chain is shown to be scalable while maintaining high accuracy, and (3) the\nproposed framework is robust and adaptive in different fault conditions and\nperforms better in comparison with the state-of-the-art methods. \n\n"}
{"id": "1805.12421", "contents": "Title: HOPF: Higher Order Propagation Framework for Deep Collective\n  Classification Abstract: Given a graph where every node has certain attributes associated with it and\nsome nodes have labels associated with them, Collective Classification (CC) is\nthe task of assigning labels to every unlabeled node using information from the\nnode as well as its neighbors. It is often the case that a node is not only\ninfluenced by its immediate neighbors but also by higher order neighbors,\nmultiple hops away. Recent state-of-the-art models for CC learn end-to-end\ndifferentiable variations of Weisfeiler-Lehman (WL) kernels to aggregate\nmulti-hop neighborhood information. In this work, we propose a Higher Order\nPropagation Framework, HOPF, which provides an iterative inference mechanism\nfor these powerful differentiable kernels. Such a combination of classical\niterative inference mechanism with recent differentiable kernels allows the\nframework to learn graph convolutional filters that simultaneously exploit the\nattribute and label information available in the neighborhood. Further, these\niterative differentiable kernels can scale to larger hops beyond the memory\nlimitations of existing differentiable kernels. We also show that existing WL\nkernel-based models suffer from the problem of Node Information Morphing where\nthe information of the node is morphed or overwhelmed by the information of its\nneighbors when considering multiple hops. To address this, we propose a\nspecific instantiation of HOPF, called the NIP models, which preserves the node\ninformation at every propagation step. The iterative formulation of NIP models\nfurther helps in incorporating distant hop information concisely as summaries\nof the inferred labels. We do an extensive evaluation across 11 datasets from\ndifferent domains. We show that existing CC models do not provide consistent\nperformance across datasets, while the proposed NIP model with iterative\ninference is more robust. \n\n"}
{"id": "1805.12505", "contents": "Title: The long-term impact of ranking algorithms in growing networks Abstract: When we search online for content, we are constantly exposed to rankings. For\nexample, web search results are presented as a ranking, and online bookstores\noften show us lists of best-selling books. While popularity-based ranking\nalgorithms (like Google's PageRank) have been extensively studied in previous\nworks, we still lack a clear understanding of their potential systemic\nconsequences. In this work, we fill this gap by introducing a new model of\nnetwork growth that allows us to compare the properties of the networks\ngenerated under the influence of different ranking algorithms. We show that by\ncorrecting for the omnipresent age bias of popularity-based ranking algorithms,\nthe resulting networks exhibit a significantly larger agreement between the\nnodes' inherent quality and their long-term popularity, and a less concentrated\npopularity distribution. To further promote popularity diversity, we introduce\nand validate a perturbation of the original rankings where a small number of\nrandomly-selected nodes are promoted to the top of the ranking. Our findings\nmove the first steps toward a model-based understanding of the long-term impact\nof popularity-based ranking algorithms, and could be used as an informative\ntool for the design of improved information filtering tools. \n\n"}
{"id": "1805.12528", "contents": "Title: Fusion Graph Convolutional Networks Abstract: Semi-supervised node classification in attributed graphs, i.e., graphs with\nnode features, involves learning to classify unlabeled nodes given a partially\nlabeled graph. Label predictions are made by jointly modeling the node and its'\nneighborhood features. State-of-the-art models for node classification on such\nattributed graphs use differentiable recursive functions that enable\naggregation and filtering of neighborhood information from multiple hops. In\nthis work, we analyze the representation capacity of these models to regulate\ninformation from multiple hops independently. From our analysis, we conclude\nthat these models despite being powerful, have limited representation capacity\nto capture multi-hop neighborhood information effectively. Further, we also\npropose a mathematically motivated, yet simple extension to existing graph\nconvolutional networks (GCNs) which has improved representation capacity. We\nextensively evaluate the proposed model, F-GCN on eight popular datasets from\ndifferent domains. F-GCN outperforms the state-of-the-art models for\nsemi-supervised learning on six datasets while being extremely competitive on\nthe other two. \n\n"}
{"id": "1806.00101", "contents": "Title: Generative Ratio Matching Networks Abstract: Deep generative models can learn to generate realistic-looking images, but\nmany of the most effective methods are adversarial and involve a saddlepoint\noptimization, which requires a careful balancing of training between a\ngenerator network and a critic network. Maximum mean discrepancy networks\n(MMD-nets) avoid this issue by using kernel as a fixed adversary, but\nunfortunately, they have not on their own been able to match the generative\nquality of adversarial training. In this work, we take their insight of using\nkernels as fixed adversaries further and present a novel method for training\ndeep generative models that does not involve saddlepoint optimization. We call\nour method generative ratio matching or GRAM for short. In GRAM, the generator\nand the critic networks do not play a zero-sum game against each other,\ninstead, they do so against a fixed kernel. Thus GRAM networks are not only\nstable to train like MMD-nets but they also match and beat the generative\nquality of adversarially trained generative networks. \n\n"}
{"id": "1806.00148", "contents": "Title: Interpreting Deep Learning: The Machine Learning Rorschach Test? Abstract: Theoretical understanding of deep learning is one of the most important tasks\nfacing the statistics and machine learning communities. While deep neural\nnetworks (DNNs) originated as engineering methods and models of biological\nnetworks in neuroscience and psychology, they have quickly become a centerpiece\nof the machine learning toolbox. Unfortunately, DNN adoption powered by recent\nsuccesses combined with the open-source nature of the machine learning\ncommunity, has outpaced our theoretical understanding. We cannot reliably\nidentify when and why DNNs will make mistakes. In some applications like text\ntranslation these mistakes may be comical and provide for fun fodder in\nresearch talks, a single error can be very costly in tasks like medical\nimaging. As we utilize DNNs in increasingly sensitive applications, a better\nunderstanding of their properties is thus imperative. Recent advances in DNN\ntheory are numerous and include many different sources of intuition, such as\nlearning theory, sparse signal analysis, physics, chemistry, and psychology. An\ninteresting pattern begins to emerge in the breadth of possible\ninterpretations. The seemingly limitless approaches are mostly constrained by\nthe lens with which the mathematical operations are viewed. Ultimately, the\ninterpretation of DNNs appears to mimic a type of Rorschach test --- a\npsychological test wherein subjects interpret a series of seemingly ambiguous\nink-blots. Validation for DNN theory requires a convergence of the literature.\nWe must distinguish between universal results that are invariant to the\nanalysis perspective and those that are specific to a particular network\nconfiguration. Simultaneously we must deal with the fact that many standard\nstatistical tools for quantifying generalization or empirically assessing\nimportant network features are difficult to apply to DNNs. \n\n"}
{"id": "1806.00319", "contents": "Title: Learning convex bounds for linear quadratic control policy synthesis Abstract: Learning to make decisions from observed data in dynamic environments remains\na problem of fundamental importance in a number of fields, from artificial\nintelligence and robotics, to medicine and finance. This paper concerns the\nproblem of learning control policies for unknown linear dynamical systems so as\nto maximize a quadratic reward function. We present a method to optimize the\nexpected value of the reward over the posterior distribution of the unknown\nsystem parameters, given data. The algorithm involves sequential convex\nprograming, and enjoys reliable local convergence and robust stability\nguarantees. Numerical simulations and stabilization of a real-world inverted\npendulum are used to demonstrate the approach, with strong performance and\nrobustness properties observed in both. \n\n"}
{"id": "1806.00530", "contents": "Title: Efficient, Certifiably Optimal Clustering with Applications to Latent\n  Variable Graphical Models Abstract: Motivated by the task of clustering either $d$ variables or $d$ points into\n$K$ groups, we investigate efficient algorithms to solve the Peng-Wei (P-W)\n$K$-means semi-definite programming (SDP) relaxation. The P-W SDP has been\nshown in the literature to have good statistical properties in a variety of\nsettings, but remains intractable to solve in practice. To this end we propose\nFORCE, a new algorithm to solve this SDP relaxation. Compared to the naive\ninterior point method, our method reduces the computational complexity of\nsolving the SDP from $\\tilde{O}(d^7\\log\\epsilon^{-1})$ to\n$\\tilde{O}(d^{6}K^{-2}\\epsilon^{-1})$ arithmetic operations for an\n$\\epsilon$-optimal solution. Our method combines a primal first-order method\nwith a dual optimality certificate search, which when successful, allows for\nearly termination of the primal method. We show for certain variable clustering\nproblems that, with high probability, FORCE is guaranteed to find the optimal\nsolution to the SDP relaxation and provide a certificate of exact optimality.\nAs verified by our numerical experiments, this allows FORCE to solve the P-W\nSDP with dimensions in the hundreds in only tens of seconds. For a variation of\nthe P-W SDP where $K$ is not known a priori a slight modification of FORCE\nreduces the computational complexity of solving this problem as well: from\n$\\tilde{O}(d^7\\log\\epsilon^{-1})$ using a standard SDP solver to\n$\\tilde{O}(d^{4}\\epsilon^{-1})$. \n\n"}
{"id": "1806.00848", "contents": "Title: Learning graphs from data: A signal representation perspective Abstract: The construction of a meaningful graph topology plays a crucial role in the\neffective representation, processing, analysis and visualization of structured\ndata. When a natural choice of the graph is not readily available from the data\nsets, it is thus desirable to infer or learn a graph topology from the data. In\nthis tutorial overview, we survey solutions to the problem of graph learning,\nincluding classical viewpoints from statistics and physics, and more recent\napproaches that adopt a graph signal processing (GSP) perspective. We further\nemphasize the conceptual similarities and differences between classical and\nGSP-based graph inference methods, and highlight the potential advantage of the\nlatter in a number of theoretical and practical scenarios. We conclude with\nseveral open issues and challenges that are keys to the design of future signal\nprocessing and machine learning algorithms for learning graphs from data. \n\n"}
{"id": "1806.01380", "contents": "Title: A General Framework for Bandit Problems Beyond Cumulative Objectives Abstract: The stochastic multi-armed bandit (MAB) problem is a common model for\nsequential decision problems. In the standard setup, a decision maker has to\nchoose at every instant between several competing arms, each of them provides a\nscalar random variable, referred to as a \"reward.\" Nearly all research on this\ntopic considers the total cumulative reward as the criterion of interest. This\nwork focuses on other natural objectives that cannot be cast as a sum over\nrewards, but rather more involved functions of the reward stream. Unlike the\ncase of cumulative criteria, in the problems we study here the oracle policy,\nthat knows the problem parameters a priori and is used to \"center\" the regret,\nis not trivial. We provide a systematic approach to such problems, and derive\ngeneral conditions under which the oracle policy is sufficiently tractable to\nfacilitate the design of optimism-based (upper confidence bound) learning\npolicies. These conditions elucidate an interesting interplay between the arm\nreward distributions and the performance metric. Our main findings are\nillustrated for several commonly used objectives such as conditional\nvalue-at-risk, mean-variance trade-offs, Sharpe-ratio, and more. \n\n"}
{"id": "1806.01851", "contents": "Title: Pathwise Derivatives Beyond the Reparameterization Trick Abstract: We observe that gradients computed via the reparameterization trick are in\ndirect correspondence with solutions of the transport equation in the formalism\nof optimal transport. We use this perspective to compute (approximate) pathwise\ngradients for probability distributions not directly amenable to the\nreparameterization trick: Gamma, Beta, and Dirichlet. We further observe that\nwhen the reparameterization trick is applied to the Cholesky-factorized\nmultivariate Normal distribution, the resulting gradients are suboptimal in the\nsense of optimal transport. We derive the optimal gradients and show that they\nhave reduced variance in a Gaussian Process regression task. We demonstrate\nwith a variety of synthetic experiments and stochastic variational inference\ntasks that our pathwise gradients are competitive with other methods. \n\n"}
{"id": "1806.02046", "contents": "Title: Implicit regularization and solution uniqueness in over-parameterized\n  matrix sensing Abstract: We consider whether algorithmic choices in over-parameterized linear matrix\nfactorization introduce implicit regularization. We focus on noiseless matrix\nsensing over rank-$r$ positive semi-definite (PSD) matrices in $\\mathbb{R}^{n\n\\times n}$, with a sensing mechanism that satisfies restricted isometry\nproperties (RIP). The algorithm we study is \\emph{factored gradient descent},\nwhere we model the low-rankness and PSD constraints with the factorization\n$UU^\\top$, for $U \\in \\mathbb{R}^{n \\times r}$. Surprisingly, recent work\nargues that the choice of $r \\leq n$ is not pivotal: even setting $U \\in\n\\mathbb{R}^{n \\times n}$ is sufficient for factored gradient descent to find\nthe rank-$r$ solution, which suggests that operating over the factors leads to\nan implicit regularization. In this contribution, we provide a different\nperspective to the problem of implicit regularization. We show that under\ncertain conditions, the PSD constraint by itself is sufficient to lead to a\nunique rank-$r$ matrix recovery, without implicit or explicit low-rank\nregularization. \\emph{I.e.}, under assumptions, the set of PSD matrices, that\nare consistent with the observed data, is a singleton, regardless of the\nalgorithm used. \n\n"}
{"id": "1806.02146", "contents": "Title: Adversarial Auto-encoders for Speech Based Emotion Recognition Abstract: Recently, generative adversarial networks and adversarial autoencoders have\ngained a lot of attention in machine learning community due to their\nexceptional performance in tasks such as digit classification and face\nrecognition. They map the autoencoder's bottleneck layer output (termed as code\nvectors) to different noise Probability Distribution Functions (PDFs), that can\nbe further regularized to cluster based on class information. In addition, they\nalso allow a generation of synthetic samples by sampling the code vectors from\nthe mapped PDFs. Inspired by these properties, we investigate the application\nof adversarial autoencoders to the domain of emotion recognition. Specifically,\nwe conduct experiments on the following two aspects: (i) their ability to\nencode high dimensional feature vector representations for emotional utterances\ninto a compressed space (with a minimal loss of emotion class discriminability\nin the compressed space), and (ii) their ability to regenerate synthetic\nsamples in the original feature space, to be later used for purposes such as\ntraining emotion recognition classifiers. We demonstrate the promise of\nadversarial autoencoders with regards to these aspects on the Interactive\nEmotional Dyadic Motion Capture (IEMOCAP) corpus and present our analysis. \n\n"}
{"id": "1806.02371", "contents": "Title: Adversarial Attack on Graph Structured Data Abstract: Deep learning on graph structures has shown exciting results in various\napplications. However, few attentions have been paid to the robustness of such\nmodels, in contrast to numerous research work for image or text adversarial\nattack and defense. In this paper, we focus on the adversarial attacks that\nfool the model by modifying the combinatorial structure of data. We first\npropose a reinforcement learning based attack method that learns the\ngeneralizable attack policy, while only requiring prediction labels from the\ntarget classifier. Also, variants of genetic algorithms and gradient methods\nare presented in the scenario where prediction confidence or gradients are\navailable. We use both synthetic and real-world data to show that, a family of\nGraph Neural Network models are vulnerable to these attacks, in both\ngraph-level and node-level classification tasks. We also show such attacks can\nbe used to diagnose the learned classifiers. \n\n"}
{"id": "1806.02390", "contents": "Title: Variational Implicit Processes Abstract: We introduce the implicit processes (IPs), a stochastic process that places\nimplicitly defined multivariate distributions over any finite collections of\nrandom variables. IPs are therefore highly flexible implicit priors over\nfunctions, with examples including data simulators, Bayesian neural networks\nand non-linear transformations of stochastic processes. A novel and efficient\napproximate inference algorithm for IPs, namely the variational implicit\nprocesses (VIPs), is derived using generalised wake-sleep updates. This method\nreturns simple update equations and allows scalable hyper-parameter learning\nwith stochastic optimization. Experiments show that VIPs return better\nuncertainty estimates and lower errors over existing inference methods for\nchallenging models such as Bayesian neural networks, and Gaussian processes. \n\n"}
{"id": "1806.02510", "contents": "Title: Removing Algorithmic Discrimination (With Minimal Individual Error) Abstract: We address the problem of correcting group discriminations within a score\nfunction, while minimizing the individual error. Each group is described by a\nprobability density function on the set of profiles. We first solve the problem\nanalytically in the case of two populations, with a uniform bonus-malus on the\nzones where each population is a majority. We then address the general case of\nn populations, where the entanglement of populations does not allow a similar\nanalytical solution. We show that an approximate solution with an arbitrarily\nhigh level of precision can be computed with linear programming. Finally, we\naddress the inverse problem where the error should not go beyond a certain\nvalue and we seek to minimize the discrimination. \n\n"}
{"id": "1806.02654", "contents": "Title: New Hybrid Neuro-Evolutionary Algorithms for Renewable Energy and\n  Facilities Management Problems Abstract: This Ph.D. thesis deals with the optimization of several renewable energy\nresources development as well as the improvement of facilities management in\noceanic engineering and airports, using computational hybrid methods belonging\nto AI to this end. Energy is essential to our society in order to ensure a good\nquality of life. This means that predictions over the characteristics on which\nrenewable energies depend are necessary, in order to know the amount of energy\nthat will be obtained at any time. The second topic tackled in this thesis is\nrelated to the basic parameters that influence in different marine activities\nand airports, whose knowledge is necessary to develop a proper facilities\nmanagement in these environments. Within this work, a study of the\nstate-of-the-art Machine Learning have been performed to solve the problems\nassociated with the topics above-mentioned, and several contributions have been\nproposed: One of the pillars of this work is focused on the estimation of the\nmost important parameters in the exploitation of renewable resources. The\nsecond contribution of this thesis is related to feature selection problems.\nThe proposed methodologies are applied to multiple problems: the prediction of\n$H_s$, relevant for marine energy applications and marine activities, the\nestimation of WPREs, undesirable variations in the electric power produced by a\nwind farm, the prediction of global solar radiation in areas from Spain and\nAustralia, really important in terms of solar energy, and the prediction of\nlow-visibility events at airports. All of these practical issues are developed\nwith the consequent previous data analysis, normally, in terms of\nmeteorological variables. \n\n"}
{"id": "1806.03143", "contents": "Title: Black Box FDR Abstract: Analyzing large-scale, multi-experiment studies requires scientists to test\neach experimental outcome for statistical significance and then assess the\nresults as a whole. We present Black Box FDR (BB-FDR), an empirical-Bayes\nmethod for analyzing multi-experiment studies when many covariates are gathered\nper experiment. BB-FDR learns a series of black box predictive models to boost\npower and control the false discovery rate (FDR) at two stages of study\nanalysis. In Stage 1, it uses a deep neural network prior to report which\nexperiments yielded significant outcomes. In Stage 2, a separate black box\nmodel of each covariate is used to select features that have significant\npredictive power across all experiments. In benchmarks, BB-FDR outperforms\ncompeting state-of-the-art methods in both stages of analysis. We apply BB-FDR\nto two real studies on cancer drug efficacy. For both studies, BB-FDR increases\nthe proportion of significant outcomes discovered and selects variables that\nreveal key genomic drivers of drug sensitivity and resistance in cancer. \n\n"}
{"id": "1806.03146", "contents": "Title: Neural Message Passing with Edge Updates for Predicting Properties of\n  Molecules and Materials Abstract: Neural message passing on molecular graphs is one of the most promising\nmethods for predicting formation energy and other properties of molecules and\nmaterials. In this work we extend the neural message passing model with an edge\nupdate network which allows the information exchanged between atoms to depend\non the hidden state of the receiving atom. We benchmark the proposed model on\nthree publicly available datasets (QM9, The Materials Project and OQMD) and\nshow that the proposed model yields superior prediction of formation energies\nand other properties on all three datasets in comparison with the best\npublished results. Furthermore we investigate different methods for\nconstructing the graph used to represent crystalline structures and we find\nthat using a graph based on K-nearest neighbors achieves better prediction\naccuracy than using maximum distance cutoff or the Voronoi tessellation graph. \n\n"}
{"id": "1806.03198", "contents": "Title: Spreading vectors for similarity search Abstract: Discretizing multi-dimensional data distributions is a fundamental step of\nmodern indexing methods. State-of-the-art techniques learn parameters of\nquantizers on training data for optimal performance, thus adapting quantizers\nto the data. In this work, we propose to reverse this paradigm and adapt the\ndata to the quantizer: we train a neural net which last layer forms a fixed\nparameter-free quantizer, such as pre-defined points of a hyper-sphere. As a\nproxy objective, we design and train a neural network that favors uniformity in\nthe spherical latent space, while preserving the neighborhood structure after\nthe mapping. We propose a new regularizer derived from the Kozachenko--Leonenko\ndifferential entropy estimator to enforce uniformity and combine it with a\nlocality-aware triplet loss. Experiments show that our end-to-end approach\noutperforms most learned quantization methods, and is competitive with the\nstate of the art on widely adopted benchmarks. Furthermore, we show that\ntraining without the quantization step results in almost no difference in\naccuracy, but yields a generic catalyzer that can be applied with any\nsubsequent quantizer. \n\n"}
{"id": "1806.03386", "contents": "Title: A Graph Model with Indirect Co-location Links Abstract: Graph models are widely used to analyse diffusion processes embedded in\nsocial contacts and to develop applications. A range of graph models are\navailable to replicate the underlying social structures and dynamics\nrealistically. However, most of the current graph models can only consider\nconcurrent interactions among individuals in the co-located interaction\nnetworks. However, they do not account for indirect interactions that can\ntransmit spreading items to individuals who visit the same locations at\ndifferent times but within a certain time limit. The diffusion phenomena\noccurring through direct and indirect interactions is called same place\ndifferent time (SPDT) diffusion. This paper introduces a model to synthesize\nco-located interaction graphs capturing both direct interactions, where\nindividuals meet at a location, and indirect interactions, where individuals\nvisit the same location at different times within a set timeframe. We analyze\n60 million location updates made by 2 million users from a social networking\napplication to characterize the graph properties, including the space-time\ncorrelations and its time evolving characteristics, such as bursty or ongoing\nbehaviors. The generated synthetic graph reproduces diffusion dynamics of a\nrealistic contact graph, and reduces the prediction error by up to 82% when\ncompare to other contact graph models demonstrating its potential for\nforecasting epidemic spread. \n\n"}
{"id": "1806.03763", "contents": "Title: Smoothed analysis of the low-rank approach for smooth semidefinite\n  programs Abstract: We consider semidefinite programs (SDPs) of size n with equality constraints.\nIn order to overcome scalability issues, Burer and Monteiro proposed a\nfactorized approach based on optimizing over a matrix Y of size $n$ by $k$ such\nthat $X = YY^*$ is the SDP variable. The advantages of such formulation are\ntwofold: the dimension of the optimization variable is reduced and positive\nsemidefiniteness is naturally enforced. However, the problem in Y is\nnon-convex. In prior work, it has been shown that, when the constraints on the\nfactorized variable regularly define a smooth manifold, provided k is large\nenough, for almost all cost matrices, all second-order stationary points\n(SOSPs) are optimal. Importantly, in practice, one can only compute points\nwhich approximately satisfy necessary optimality conditions, leading to the\nquestion: are such points also approximately optimal? To this end, and under\nsimilar assumptions, we use smoothed analysis to show that approximate SOSPs\nfor a randomly perturbed objective function are approximate global optima, with\nk scaling like the square root of the number of constraints (up to log\nfactors). Moreover, we bound the optimality gap at the approximate solution of\nthe perturbed problem with respect to the original problem. We particularize\nour results to an SDP relaxation of phase retrieval. \n\n"}
{"id": "1806.03857", "contents": "Title: Deep Learning for Classification Tasks on Geospatial Vector Polygons Abstract: In this paper, we evaluate the accuracy of deep learning approaches on\ngeospatial vector geometry classification tasks. The purpose of this evaluation\nis to investigate the ability of deep learning models to learn from geometry\ncoordinates directly. Previous machine learning research applied to geospatial\npolygon data did not use geometries directly, but derived properties thereof.\nThese are produced by way of extracting geometry properties such as Fourier\ndescriptors. Instead, our introduced deep neural net architectures are able to\nlearn on sequences of coordinates mapped directly from polygons. In three\nclassification tasks we show that the deep learning architectures are\ncompetitive with common learning algorithms that require extracted features. \n\n"}
{"id": "1806.04465", "contents": "Title: Gaussian mixture models with Wasserstein distance Abstract: Generative models with both discrete and continuous latent variables are\nhighly motivated by the structure of many real-world data sets. They present,\nhowever, subtleties in training often manifesting in the discrete latent being\nunder leveraged. In this paper, we show that such models are more amenable to\ntraining when using the Optimal Transport framework of Wasserstein\nAutoencoders. We find our discrete latent variable to be fully leveraged by the\nmodel when trained, without any modifications to the objective function or\nsignificant fine tuning. Our model generates comparable samples to other\napproaches while using relatively simple neural networks, since the discrete\nlatent variable carries much of the descriptive burden. Furthermore, the\ndiscrete latent provides significant control over generation. \n\n"}
{"id": "1806.04743", "contents": "Title: INFERNO: Inference-Aware Neural Optimisation Abstract: Complex computer simulations are commonly required for accurate data\nmodelling in many scientific disciplines, making statistical inference\nchallenging due to the intractability of the likelihood evaluation for the\nobserved data. Furthermore, sometimes one is interested on inference drawn over\na subset of the generative model parameters while taking into account model\nuncertainty or misspecification on the remaining nuisance parameters. In this\nwork, we show how non-linear summary statistics can be constructed by\nminimising inference-motivated losses via stochastic gradient descent such they\nprovided the smallest uncertainty for the parameters of interest. As a use\ncase, the problem of confidence interval estimation for the mixture coefficient\nin a multi-dimensional two-component mixture model (i.e. signal vs background)\nis considered, where the proposed technique clearly outperforms summary\nstatistics based on probabilistic classification, which are a commonly used\nalternative but do not account for the presence of nuisance parameters. \n\n"}
{"id": "1806.04838", "contents": "Title: Partial AUC Maximization via Nonlinear Scoring Functions Abstract: We propose a method for maximizing a partial area under a receiver operating\ncharacteristic (ROC) curve (pAUC) for binary classification tasks. In binary\nclassification tasks, accuracy is the most commonly used as a measure of\nclassifier performance. In some applications such as anomaly detection and\ndiagnostic testing, accuracy is not an appropriate measure since prior\nprobabilties are often greatly biased. Although in such cases the pAUC has been\nutilized as a performance measure, few methods have been proposed for directly\nmaximizing the pAUC. This optimization is achieved by using a scoring function.\nThe conventional approach utilizes a linear function as the scoring function.\nIn contrast we newly introduce nonlinear scoring functions for this purpose.\nSpecifically, we present two types of nonlinear scoring functions based on\ngenerative models and deep neural networks. We show experimentally that\nnonlinear scoring fucntions improve the conventional methods through the\napplication of a binary classification of real and bogus objects obtained with\nthe Hyper Suprime-Cam on the Subaru telescope. \n\n"}
{"id": "1806.04910", "contents": "Title: Bilevel Programming for Hyperparameter Optimization and Meta-Learning Abstract: We introduce a framework based on bilevel programming that unifies\ngradient-based hyperparameter optimization and meta-learning. We show that an\napproximate version of the bilevel problem can be solved by taking into\nexplicit account the optimization dynamics for the inner objective. Depending\non the specific setting, the outer variables take either the meaning of\nhyperparameters in a supervised learning problem or parameters of a\nmeta-learner. We provide sufficient conditions under which solutions of the\napproximate problem converge to those of the exact problem. We instantiate our\napproach for meta-learning in the case of deep learning where representation\nlayers are treated as hyperparameters shared across a set of training episodes.\nIn experiments, we confirm our theoretical findings, present encouraging\nresults for few-shot learning and contrast the bilevel approach against\nclassical approaches for learning-to-learn. \n\n"}
{"id": "1806.05139", "contents": "Title: High-Dimensional Inference for Cluster-Based Graphical Models Abstract: Motivated by modern applications in which one constructs graphical models\nbased on a very large number of features, this paper introduces a new class of\ncluster-based graphical models, in which variable clustering is applied as an\ninitial step for reducing the dimension of the feature space. We employ model\nassisted clustering, in which the clusters contain features that are similar to\nthe same unobserved latent variable. Two different cluster-based Gaussian\ngraphical models are considered: the latent variable graph, corresponding to\nthe graphical model associated with the unobserved latent variables, and the\ncluster-average graph, corresponding to the vector of features averaged over\nclusters. Our study reveals that likelihood based inference for the latent\ngraph, not analyzed previously, is analytically intractable. Our main\ncontribution is the development and analysis of alternative estimation and\ninference strategies, for the precision matrix of an unobservable latent vector\n$Z$. We replace the likelihood of the data by an appropriate class of empirical\nrisk functions, that can be specialized to the latent graphical model and to\nthe simpler, but under-analyzed, cluster-average graphical model. The\nestimators thus derived can be used for inference on the graph structure, for\ninstance on edge strength or pattern recovery. Inference is based on the\nasymptotic limits of the entry-wise estimates of the precision matrices\nassociated with the conditional independence graphs under consideration. While\ntaking the uncertainty induced by the clustering step into account, we\nestablish Berry-Esseen central limit theorems for the proposed estimators. It\nis noteworthy that, although the clusters are estimated adaptively from the\ndata, the central limit theorems regarding the entries of the estimated graphs\nare proved under the same conditions one would use if the clusters were\nknown.... \n\n"}
{"id": "1806.05413", "contents": "Title: Learning Dynamics of Linear Denoising Autoencoders Abstract: Denoising autoencoders (DAEs) have proven useful for unsupervised\nrepresentation learning, but a thorough theoretical understanding is still\nlacking of how the input noise influences learning. Here we develop theory for\nhow noise influences learning in DAEs. By focusing on linear DAEs, we are able\nto derive analytic expressions that exactly describe their learning dynamics.\nWe verify our theoretical predictions with simulations as well as experiments\non MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise\nallows DAEs to ignore low variance directions in the inputs while learning to\nreconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs\nto standard regularised autoencoders, we show that noise has a similar\nregularisation effect to weight decay, but with faster training dynamics. We\nalso show that our theoretical predictions approximate learning dynamics on\nreal-world data and qualitatively match observed dynamics in nonlinear DAEs. \n\n"}
{"id": "1806.05438", "contents": "Title: Stochastic Gradient Descent with Exponential Convergence Rates of\n  Expected Classification Errors Abstract: We consider stochastic gradient descent and its averaging variant for binary\nclassification problems in a reproducing kernel Hilbert space. In the\ntraditional analysis using a consistency property of loss functions, it is\nknown that the expected classification error converges more slowly than the\nexpected risk even when assuming a low-noise condition on the conditional label\nprobabilities. Consequently, the resulting rate is sublinear. Therefore, it is\nimportant to consider whether much faster convergence of the expected\nclassification error can be achieved. In recent research, an exponential\nconvergence rate for stochastic gradient descent was shown under a strong\nlow-noise condition but provided theoretical analysis was limited to the\nsquared loss function, which is somewhat inadequate for binary classification\ntasks. In this paper, we show an exponential convergence of the expected\nclassification error in the final phase of the stochastic gradient descent for\na wide class of differentiable convex loss functions under similar assumptions.\nAs for the averaged stochastic gradient descent, we show that the same\nconvergence rate holds from the early phase of training. In experiments, we\nverify our analyses on the $L_2$-regularized logistic regression. \n\n"}
{"id": "1806.05823", "contents": "Title: Primal-dual residual networks Abstract: In this work, we propose a deep neural network architecture motivated by\nprimal-dual splitting methods from convex optimization. We show theoretically\nthat there exists a close relation between the derived architecture and\nresidual networks, and further investigate this connection in numerical\nexperiments. Moreover, we demonstrate how our approach can be used to unroll\noptimization algorithms for certain problems with hard constraints. Using the\nexample of speech dequantization, we show that our method can outperform\nclassical splitting methods when both are applied to the same task. \n\n"}
{"id": "1806.06362", "contents": "Title: Initialization of ReLUs for Dynamical Isometry Abstract: Deep learning relies on good initialization schemes and hyperparameter\nchoices prior to training a neural network. Random weight initializations\ninduce random network ensembles, which give rise to the trainability, training\nspeed, and sometimes also generalization ability of an instance. In addition,\nsuch ensembles provide theoretical insights into the space of candidate models\nof which one is selected during training. The results obtained so far rely on\nmean field approximations that assume infinite layer width and that study\naverage squared signals. We derive the joint signal output distribution\nexactly, without mean field assumptions, for fully-connected networks with\nGaussian weights and biases, and analyze deviations from the mean field\nresults. For rectified linear units, we further discuss limitations of the\nstandard initialization scheme, such as its lack of dynamical isometry, and\npropose a simple alternative that overcomes these by initial parameter sharing. \n\n"}
{"id": "1806.06945", "contents": "Title: Overlapping Clustering Models, and One (class) SVM to Bind Them All Abstract: People belong to multiple communities, words belong to multiple topics, and\nbooks cover multiple genres; overlapping clusters are commonplace. Many\nexisting overlapping clustering methods model each person (or word, or book) as\na non-negative weighted combination of \"exemplars\" who belong solely to one\ncommunity, with some small noise. Geometrically, each person is a point on a\ncone whose corners are these exemplars. This basic form encompasses the widely\nused Mixed Membership Stochastic Blockmodel of networks (Airoldi et al., 2008)\nand its degree-corrected variants (Jin et al., 2017), as well as topic models\nsuch as LDA (Blei et al., 2003). We show that a simple one-class SVM yields\nprovably consistent parameter inference for all such models, and scales to\nlarge datasets. Experimental results on several simulated and real datasets\nshow our algorithm (called SVM-cone) is both accurate and scalable. \n\n"}
{"id": "1806.07001", "contents": "Title: Theoretical Analysis of Image-to-Image Translation with Adversarial\n  Learning Abstract: Recently, a unified model for image-to-image translation tasks within\nadversarial learning framework has aroused widespread research interests in\ncomputer vision practitioners. Their reported empirical success however lacks\nsolid theoretical interpretations for its inherent mechanism. In this paper, we\nreformulate their model from a brand-new geometrical perspective and have\neventually reached a full interpretation on some interesting but unclear\nempirical phenomenons from their experiments. Furthermore, by extending the\ndefinition of generalization for generative adversarial nets to a broader\nsense, we have derived a condition to control the generalization capability of\ntheir model. According to our derived condition, several practical suggestions\nhave also been proposed on model design and dataset construction as a guidance\nfor further empirical researches. \n\n"}
{"id": "1806.07307", "contents": "Title: Estimation from Non-Linear Observations via Convex Programming with\n  Application to Bilinear Regression Abstract: We propose a computationally efficient estimator, formulated as a convex\nprogram, for a broad class of non-linear regression problems that involve\ndifference of convex (DC) non-linearities. The proposed method can be viewed as\na significant extension of the \"anchored regression\" method formulated and\nanalyzed in [10] for regression with convex non-linearities. Our main\nassumption, in addition to other mild statistical and computational\nassumptions, is availability of a certain approximation oracle for the average\nof the gradients of the observation functions at a ground truth. Under this\nassumption and using a PAC-Bayesian analysis we show that the proposed\nestimator produces an accurate estimate with high probability. As a concrete\nexample, we study the proposed framework in the bilinear regression problem\nwith Gaussian factors and quantify a sufficient sample complexity for exact\nrecovery. Furthermore, we describe a computationally tractable scheme that\nprovably produces the required approximation oracle in the considered bilinear\nregression problem. \n\n"}
{"id": "1806.07479", "contents": "Title: Weight Thresholding on Complex Networks Abstract: Weight thresholding is a simple technique that aims at reducing the number of\nedges in weighted networks that are otherwise too dense for the application of\nstandard graph theoretical methods. We show that the group structure of real\nweighted networks is very robust under weight thresholding, as it is maintained\neven when most of the edges are removed. This appears to be related to the\ncorrelation between topology and weight that characterizes real networks. On\nthe other hand, the behavior of other properties is generally system dependent. \n\n"}
{"id": "1806.08301", "contents": "Title: The Online Saddle Point Problem and Online Convex Optimization with\n  Knapsacks Abstract: We study the online saddle point problem, an online learning problem where at\neach iteration a pair of actions need to be chosen without knowledge of the\ncurrent and future (convex-concave) payoff functions. The objective is to\nminimize the gap between the cumulative payoffs and the saddle point value of\nthe aggregate payoff function, which we measure using a metric called\n\"SP-Regret\". The problem generalizes the online convex optimization framework\nbut here we must ensure both players incur cumulative payoffs close to that of\nthe Nash equilibrium of the sum of the games. We propose an algorithm that\nachieves SP-Regret proportional to $\\sqrt{\\ln(T)T}$ in the general case, and\n$\\log(T)$ SP-Regret for the strongly convex-concave case. We also consider the\nspecial case where the payoff functions are bilinear and the decision sets are\nthe probability simplex. In this setting we are able to design algorithms that\nreduce the bounds on SP-Regret from a linear dependence in the dimension of the\nproblem to a \\textit{logarithmic} one. We also study the problem under bandit\nfeedback and provide an algorithm that achieves sublinear SP-Regret. We then\nconsider an online convex optimization with knapsacks problem motivated by a\nwide variety of applications such as: dynamic pricing, auctions, and\ncrowdsourcing. We relate this problem to the online saddle point problem and\nestablish $O(\\sqrt{T})$ regret using a primal-dual algorithm. \n\n"}
{"id": "1806.08734", "contents": "Title: On the Spectral Bias of Neural Networks Abstract: Neural networks are known to be a class of highly expressive functions able\nto fit even random input-output mappings with $100\\%$ accuracy. In this work,\nwe present properties of neural networks that complement this aspect of\nexpressivity. By using tools from Fourier analysis, we show that deep ReLU\nnetworks are biased towards low frequency functions, meaning that they cannot\nhave local fluctuations without affecting their global behavior. Intuitively,\nthis property is in line with the observation that over-parameterized networks\nfind simple patterns that generalize across data samples. We also investigate\nhow the shape of the data manifold affects expressivity by showing evidence\nthat learning high frequencies gets \\emph{easier} with increasing manifold\ncomplexity, and present a theoretical understanding of this behavior. Finally,\nwe study the robustness of the frequency components with respect to parameter\nperturbation, to develop the intuition that the parameters must be finely tuned\nto express high frequency functions. \n\n"}
{"id": "1806.08838", "contents": "Title: Bayesian Optimization of Combinatorial Structures Abstract: The optimization of expensive-to-evaluate black-box functions over\ncombinatorial structures is an ubiquitous task in machine learning, engineering\nand the natural sciences. The combinatorial explosion of the search space and\ncostly evaluations pose challenges for current techniques in discrete\noptimization and machine learning, and critically require new algorithmic\nideas. This article proposes, to the best of our knowledge, the first algorithm\nto overcome these challenges, based on an adaptive, scalable model that\nidentifies useful combinatorial structure even when data is scarce. Our\nacquisition function pioneers the use of semidefinite programming to achieve\nefficiency and scalability. Experimental evaluations demonstrate that this\nalgorithm consistently outperforms other methods from combinatorial and\nBayesian optimization. \n\n"}
{"id": "1806.08904", "contents": "Title: Temporal Activity Path Based Character Correction in Social Networks Abstract: Vast amount of multimedia data contains massive and multifarious social\ninformation which is used to construct large-scale social networks. In a\ncomplex social network, a character should be ideally denoted by one and only\none vertex. However, it is pervasive that a character is denoted by two or more\nvertices with different names, thus it is usually considered as multiple,\ndifferent characters. This problem causes incorrectness of results in network\nanalysis and mining. The factual challenge is that character uniqueness is hard\nto correctly confirm due to lots of complicated factors, e.g. name changing and\nanonymization, leading to character duplication. Early, limited research has\nshown that previous methods depended overly upon supplementary attribute\ninformation from databases. In this paper, we propose a novel method to merge\nthe character vertices which refer to as the same entity but are denoted with\ndifferent names. With this method, we firstly build the relationship network\namong characters based on records of social activities participated, which are\nextracted from multimedia sources. Then define temporal activity paths (TAPs)\nfor each character over time. After that, we measure similarity of the TAPs for\nany two characters. If the similarity is high enough, the two vertices should\nbe considered to the same character. Based on TAPs, we can determine whether to\nmerge the two character vertices. Our experiments shown that this solution can\naccurately confirm character uniqueness in large-scale social network. \n\n"}
{"id": "1806.09077", "contents": "Title: Beyond Backprop: Online Alternating Minimization with Auxiliary\n  Variables Abstract: Despite significant recent advances in deep neural networks, training them\nremains a challenge due to the highly non-convex nature of the objective\nfunction. State-of-the-art methods rely on error backpropagation, which suffers\nfrom several well-known issues, such as vanishing and exploding gradients,\ninability to handle non-differentiable nonlinearities and to parallelize\nweight-updates across layers, and biological implausibility. These limitations\ncontinue to motivate exploration of alternative training algorithms, including\nseveral recently proposed auxiliary-variable methods which break the complex\nnested objective function into local subproblems. However, those techniques are\nmainly offline (batch), which limits their applicability to extremely large\ndatasets, as well as to online, continual or reinforcement learning. The main\ncontribution of our work is a novel online (stochastic/mini-batch) alternating\nminimization (AM) approach for training deep neural networks, together with the\nfirst theoretical convergence guarantees for AM in stochastic settings and\npromising empirical results on a variety of architectures and datasets. \n\n"}
{"id": "1806.10234", "contents": "Title: Scalable Gaussian Process Inference with Finite-data Mean and Variance\n  Guarantees Abstract: Gaussian processes (GPs) offer a flexible class of priors for nonparametric\nBayesian regression, but popular GP posterior inference methods are typically\nprohibitively slow or lack desirable finite-data guarantees on quality. We\ndevelop an approach to scalable approximate GP regression with finite-data\nguarantees on the accuracy of pointwise posterior mean and variance estimates.\nOur main contribution is a novel objective for approximate inference in the\nnonparametric setting: the preconditioned Fisher (pF) divergence. We show that\nunlike the Kullback--Leibler divergence (used in variational inference), the pF\ndivergence bounds the 2-Wasserstein distance, which in turn provides tight\nbounds the pointwise difference of the mean and variance functions. We\ndemonstrate that, for sparse GP likelihood approximations, we can minimize the\npF divergence efficiently. Our experiments show that optimizing the pF\ndivergence has the same computational requirements as variational sparse GPs\nwhile providing comparable empirical performance--in addition to our novel\nfinite-data quality guarantees. \n\n"}
{"id": "1807.00382", "contents": "Title: Survey of Graph Analysis Applications Abstract: Recently, many systems for graph analysis have been developed to address the\ngrowing needs of both industry and academia to study complex graphs. Insight\ninto the practical uses of graph analysis will allow future developments of\nsuch systems to optimize for real-world usage, instead of targeting single use\ncases or hypothetical workloads. This insight may be derived from surveys on\nthe applications of graph analysis. However, existing surveys are limited in\nthe variety of application domains, datasets, and/or graph analysis techniques\nthey study. In this work we present and apply a systematic method for\nidentifying practical use cases of graph analysis. We identify commonly used\ngraph features and analysis methods and use our findings to construct a\ntaxonomy of graph analysis applications. We conclude that practical use cases\nof graph analysis cover a diverse set of graph features and analysis methods.\nFurthermore, most applications combine multiple features and methods. Our\nfindings motivate further development of graph analysis systems to support a\nbroader set of applications and to facilitate the combination of multiple\nanalysis methods in an (interactive) workflow. \n\n"}
{"id": "1807.01065", "contents": "Title: When Gaussian Process Meets Big Data: A Review of Scalable GPs Abstract: The vast quantity of information brought by big data as well as the evolving\ncomputer hardware encourages success stories in the machine learning community.\nIn the meanwhile, it poses challenges for the Gaussian process (GP) regression,\na well-known non-parametric and interpretable Bayesian model, which suffers\nfrom cubic complexity to data size. To improve the scalability while retaining\ndesirable prediction quality, a variety of scalable GPs have been presented.\nBut they have not yet been comprehensively reviewed and analyzed in order to be\nwell understood by both academia and industry. The review of scalable GPs in\nthe GP community is timely and important due to the explosion of data size. To\nthis end, this paper is devoted to the review on state-of-the-art scalable GPs\ninvolving two main categories: global approximations which distillate the\nentire data and local approximations which divide the data for subspace\nlearning. Particularly, for global approximations, we mainly focus on sparse\napproximations comprising prior approximations which modify the prior but\nperform exact inference, posterior approximations which retain exact prior but\nperform approximate inference, and structured sparse approximations which\nexploit specific structures in kernel matrix; for local approximations, we\nhighlight the mixture/product of experts that conducts model averaging from\nmultiple local experts to boost predictions. To present a complete review,\nrecent advances for improving the scalability and capability of scalable GPs\nare reviewed. Finally, the extensions and open issues regarding the\nimplementation of scalable GPs in various scenarios are reviewed and discussed\nto inspire novel ideas for future research avenues. \n\n"}
{"id": "1807.01190", "contents": "Title: Link persistence and conditional distances in multiplex networks Abstract: Recent progress towards unraveling the hidden geometric organization of real\nmultiplexes revealed significant correlations across the hyperbolic node\ncoordinates in different network layers, which facilitated applications like\ntrans-layer link prediction and mutual navigation. But are geometric\ncorrelations alone sufficient to explain the topological relation between the\nlayers of real systems? Here we provide the negative answer to this question.\nWe show that connections in real systems tend to persist from one layer to\nanother irrespectively of their hyperbolic distances. This suggests that in\naddition to purely geometric aspects the explicit link formation process in one\nlayer impacts the topology of other layers. Based on this finding, we present a\nsimple modification to the recently developed Geometric Multiplex Model to\naccount for this effect, and show that the extended model can reproduce the\nbehavior observed in real systems. We also find that link persistence is\nsignificant in all considered multiplexes and can explain their layers' high\nedge overlap, which cannot be explained by coordinate correlations alone.\nFurthermore, by taking both link persistence and hyperbolic distance\ncorrelations into account we can improve trans-layer link prediction. These\nfindings guide the development of multiplex embedding methods, suggesting that\nsuch methods should be accounting for both coordinate correlations and link\npersistence across layers. \n\n"}
{"id": "1807.01514", "contents": "Title: Generating Synthetic but Plausible Healthcare Record Datasets Abstract: Generating datasets that \"look like\" given real ones is an interesting tasks\nfor healthcare applications of ML and many other fields of science and\nengineering. In this paper we propose a new method of general application to\nbinary datasets based on a method for learning the parameters of a latent\nvariable moment that we have previously used for clustering patient datasets.\nWe compare our method with a recent proposal (MedGan) based on generative\nadversarial methods and find that the synthetic datasets we generate are\nglobally more realistic in at least two senses: real and synthetic instances\nare harder to tell apart by Random Forests, and the MMD statistic. The most\nlikely explanation is that our method does not suffer from the \"mode collapse\"\nwhich is an admitted problem of GANs. Additionally, the generative models we\ngenerate are easy to interpret, unlike the rather obscure GANs. Our experiments\nare performed on two patient datasets containing ICD-9 diagnostic codes: the\npublicly available MIMIC-III dataset and a dataset containing admissions for\ncongestive heart failure during 7 years at Hospital de Sant Pau in Barcelona. \n\n"}
{"id": "1807.02089", "contents": "Title: Linear Bandits with Stochastic Delayed Feedback Abstract: Stochastic linear bandits are a natural and well-studied model for structured\nexploration/exploitation problems and are widely used in applications such as\nonline marketing and recommendation. One of the main challenges faced by\npractitioners hoping to apply existing algorithms is that usually the feedback\nis randomly delayed and delays are only partially observable. For example,\nwhile a purchase is usually observable some time after the display, the\ndecision of not buying is never explicitly sent to the system. In other words,\nthe learner only observes delayed positive events. We formalize this problem as\na novel stochastic delayed linear bandit and propose ${\\tt OTFLinUCB}$ and\n${\\tt OTFLinTS}$, two computationally efficient algorithms able to integrate\nnew information as it becomes available and to deal with the permanently\ncensored feedback. We prove optimal $\\tilde O(\\smash{d\\sqrt{T}})$ bounds on the\nregret of the first algorithm and study the dependency on delay-dependent\nparameters. Our model, assumptions and results are validated by experiments on\nsimulated and real data. \n\n"}
{"id": "1807.02599", "contents": "Title: From Text to Topics in Healthcare Records: An Unsupervised Graph\n  Partitioning Methodology Abstract: Electronic Healthcare Records contain large volumes of unstructured data,\nincluding extensive free text. Yet this source of detailed information often\nremains under-used because of a lack of methodologies to extract interpretable\ncontent in a timely manner. Here we apply network-theoretical tools to analyse\nfree text in Hospital Patient Incident reports from the National Health\nService, to find clusters of documents with similar content in an unsupervised\nmanner at different levels of resolution. We combine deep neural network\nparagraph vector text-embedding with multiscale Markov Stability community\ndetection applied to a sparsified similarity graph of document vectors, and\nshowcase the approach on incident reports from Imperial College Healthcare NHS\nTrust, London. The multiscale community structure reveals different levels of\nmeaning in the topics of the dataset, as shown by descriptive terms extracted\nfrom the clusters of records. We also compare a posteriori against hand-coded\ncategories assigned by healthcare personnel, and show that our approach\noutperforms LDA-based models. Our content clusters exhibit good correspondence\nwith two levels of hand-coded categories, yet they also provide further medical\ndetail in certain areas and reveal complementary descriptors of incidents\nbeyond the external classification taxonomy. \n\n"}
{"id": "1807.02811", "contents": "Title: A Tutorial on Bayesian Optimization Abstract: Bayesian optimization is an approach to optimizing objective functions that\ntake a long time (minutes or hours) to evaluate. It is best-suited for\noptimization over continuous domains of less than 20 dimensions, and tolerates\nstochastic noise in function evaluations. It builds a surrogate for the\nobjective and quantifies the uncertainty in that surrogate using a Bayesian\nmachine learning technique, Gaussian process regression, and then uses an\nacquisition function defined from this surrogate to decide where to sample. In\nthis tutorial, we describe how Bayesian optimization works, including Gaussian\nprocess regression and three common acquisition functions: expected\nimprovement, entropy search, and knowledge gradient. We then discuss more\nadvanced techniques, including running multiple function evaluations in\nparallel, multi-fidelity and multi-information source optimization,\nexpensive-to-evaluate constraints, random environmental conditions, multi-task\nBayesian optimization, and the inclusion of derivative information. We conclude\nwith a discussion of Bayesian optimization software and future research\ndirections in the field. Within our tutorial material we provide a\ngeneralization of expected improvement to noisy evaluations, beyond the\nnoise-free setting where it is more commonly applied. This generalization is\njustified by a formal decision-theoretic argument, standing in contrast to\nprevious ad hoc modifications. \n\n"}
{"id": "1807.03505", "contents": "Title: Automatic Rumor Detection on Microblogs: A Survey Abstract: The ever-increasing amount of multimedia content on modern social media\nplatforms are valuable in many applications. While the openness and convenience\nfeatures of social media also foster many rumors online. Without verification,\nthese rumors would reach thousands of users immediately and cause serious\ndamages. Many efforts have been taken to defeat online rumors automatically by\nmining the rich content provided on the open network with machine learning\ntechniques. Most rumor detection methods can be categorized in three paradigms:\nthe hand-crafted features based classification approaches, the\npropagation-based approaches and the neural networks approaches. In this\nsurvey, we introduce a formal definition of rumor in comparison with other\ndefinitions used in literatures. We summary the studies of automatic rumor\ndetection so far and present details in three paradigms of rumor detection. We\nalso give an introduction on existing datasets for rumor detection which would\nbenefit following researches in this area. We give our suggestions for future\nrumors detection on microblogs as a conclusion. \n\n"}
{"id": "1807.03783", "contents": "Title: The asymptotic behaviors of self excitation information diffusion\n  processes for a large number of individuals Abstract: The dynamics of opinion is a complex and interesting process, especially for\nthe systems with large number individuals. It is usually hard to describe the\nevolutionary features of these systems. In this paper, we study the self\nexcitation opinion model, which has been shown the superior performance in\nlearning and predicting opinions. We study the asymptotic behaviors of this\nmodel for large number of individuals, and prove that the asymptotic behaviors\nof the model in which the interaction is a multivariate self excitation process\nwith exponential function weight, can be described by a Mckean-Vlasov type\nintegro differential equation. The coupling between this equation and the\ninitial distribution captures the influence of self excitation process, which\ndecribes the mutually- exicting and recurrent nature of individuals. Finally we\nshow that the steady state distribution is a \"contraction\" of the initial\ndistribution in the linear interaction cases. \n\n"}
{"id": "1807.05031", "contents": "Title: On the Relation Between the Sharpest Directions of DNN Loss and the SGD\n  Step Length Abstract: Stochastic Gradient Descent (SGD) based training of neural networks with a\nlarge learning rate or a small batch-size typically ends in well-generalizing,\nflat regions of the weight space, as indicated by small eigenvalues of the\nHessian of the training loss. However, the curvature along the SGD trajectory\nis poorly understood. An empirical investigation shows that initially SGD\nvisits increasingly sharp regions, reaching a maximum sharpness determined by\nboth the learning rate and the batch-size of SGD. When studying the SGD\ndynamics in relation to the sharpest directions in this initial phase, we find\nthat the SGD step is large compared to the curvature and commonly fails to\nminimize the loss along the sharpest directions. Furthermore, using a reduced\nlearning rate along these directions can improve training speed while leading\nto both sharper and better generalizing solutions compared to vanilla SGD. In\nsummary, our analysis of the dynamics of SGD in the subspace of the sharpest\ndirections shows that they influence the regions that SGD steers to (where\nlarger learning rate or smaller batch size result in wider regions visited),\nthe overall training speed, and the generalization ability of the final model. \n\n"}
{"id": "1807.05411", "contents": "Title: A Unified Framework for Sparse Relaxed Regularized Regression: SR3 Abstract: Regularized regression problems are ubiquitous in statistical modeling,\nsignal processing, and machine learning. Sparse regression in particular has\nbeen instrumental in scientific model discovery, including compressed sensing\napplications, variable selection, and high-dimensional analysis. We propose a\nbroad framework for sparse relaxed regularized regression, called SR3. The key\nidea is to solve a relaxation of the regularized problem, which has three\nadvantages over the state-of-the-art: (1) solutions of the relaxed problem are\nsuperior with respect to errors, false positives, and conditioning, (2)\nrelaxation allows extremely fast algorithms for both convex and nonconvex\nformulations, and (3) the methods apply to composite regularizers such as total\nvariation (TV) and its nonconvex variants. We demonstrate the advantages of SR3\n(computational efficiency, higher accuracy, faster convergence rates, greater\nflexibility) across a range of regularized regression problems with synthetic\nand real data, including applications in compressed sensing, LASSO, matrix\ncompletion, TV regularization, and group sparsity. To promote reproducible\nresearch, we also provide a companion MATLAB package that implements these\nexamples. \n\n"}
{"id": "1807.05540", "contents": "Title: A Survey on Expert Recommendation in Community Question Answering Abstract: Community question answering (CQA) represents the type of Web applications\nwhere people can exchange knowledge via asking and answering questions. One\nsignificant challenge of most real-world CQA systems is the lack of effective\nmatching between questions and the potential good answerers, which adversely\naffects the efficient knowledge acquisition and circulation. On the one hand, a\nrequester might experience many low-quality answers without receiving a quality\nresponse in a brief time, on the other hand, an answerer might face numerous\nnew questions without being able to identify their questions of interest\nquickly. Under this situation, expert recommendation emerges as a promising\ntechnique to address the above issues. Instead of passively waiting for users\nto browse and find their questions of interest, an expert recommendation method\nraises the attention of users to the appropriate questions actively and\npromptly. The past few years have witnessed considerable efforts that address\nthe expert recommendation problem from different perspectives. These methods\nall have their issues that need to be resolved before the advantages of expert\nrecommendation can be fully embraced. In this survey, we first present an\noverview of the research efforts and state-of-the-art techniques for the expert\nrecommendation in CQA. We next summarize and compare the existing methods\nconcerning their advantages and shortcomings, followed by discussing the open\nissues and future research directions. \n\n"}
{"id": "1807.06173", "contents": "Title: A Discriminative Approach to Bayesian Filtering with Applications to\n  Human Neural Decoding Abstract: Given a stationary state-space model that relates a sequence of hidden states\nand corresponding measurements or observations, Bayesian filtering provides a\nprincipled statistical framework for inferring the posterior distribution of\nthe current state given all measurements up to the present time. For example,\nthe Apollo lunar module implemented a Kalman filter to infer its location from\na sequence of earth-based radar measurements and land safely on the moon.\n  To perform Bayesian filtering, we require a measurement model that describes\nthe conditional distribution of each observation given state. The Kalman filter\ntakes this measurement model to be linear, Gaussian. Here we show how a\nnonlinear, Gaussian approximation to the distribution of state given\nobservation can be used in conjunction with Bayes' rule to build a nonlinear,\nnon-Gaussian measurement model. The resulting approach, called the\nDiscriminative Kalman Filter (DKF), retains fast closed-form updates for the\nposterior. We argue there are many cases where the distribution of state given\nmeasurement is better-approximated as Gaussian, especially when the\ndimensionality of measurements far exceeds that of states and the Bernstein-von\nMises theorem applies. Online neural decoding for brain-computer interfaces\nprovides a motivating example, where filtering incorporates increasingly\ndetailed measurements of neural activity to provide users control over external\ndevices. Within the BrainGate2 clinical trial, the DKF successfully enabled\nthree volunteers with quadriplegia to control an on-screen cursor in real-time\nusing mental imagery alone. Participant \"T9\" used the DKF to type out messages\non a tablet PC. \n\n"}
{"id": "1807.06214", "contents": "Title: Knockoffs for the mass: new feature importance statistics with false\n  discovery guarantees Abstract: An important problem in machine learning and statistics is to identify\nfeatures that causally affect the outcome. This is often impossible to do from\npurely observational data, and a natural relaxation is to identify features\nthat are correlated with the outcome even conditioned on all other observed\nfeatures. For example, we want to identify that smoking really is correlated\nwith cancer conditioned on demographics. The knockoff procedure is a recent\nbreakthrough in statistics that, in theory, can identify truly correlated\nfeatures while guaranteeing that the false discovery is limited. The idea is to\ncreate synthetic data -- knockoffs -- that captures correlations amongst the\nfeatures. However there are substantial computational and practical challenges\nto generating and using knockoffs. This paper makes several key advances that\nenable knockoff application to be more efficient and powerful. We develop an\nefficient algorithm to generate valid knockoffs from Bayesian Networks. Then we\nsystematically evaluate knockoff test statistics and develop new statistics\nwith improved power. The paper combines new mathematical guarantees with\nsystematic experiments on real and synthetic data. \n\n"}
{"id": "1807.06343", "contents": "Title: Learning with SGD and Random Features Abstract: Sketching and stochastic gradient methods are arguably the most common\ntechniques to derive efficient large scale learning algorithms. In this paper,\nwe investigate their application in the context of nonparametric statistical\nlearning. More precisely, we study the estimator defined by stochastic gradient\nwith mini batches and random features. The latter can be seen as form of\nnonlinear sketching and used to define approximate kernel methods. The\nconsidered estimator is not explicitly penalized/constrained and regularization\nis implicit. Indeed, our study highlights how different parameters, such as\nnumber of features, iterations, step-size and mini-batch size control the\nlearning properties of the solutions. We do this by deriving optimal finite\nsample bounds, under standard assumptions. The obtained results are\ncorroborated and illustrated by numerical experiments. \n\n"}
{"id": "1807.06497", "contents": "Title: Continuous Assortment Optimization with Logit Choice Probabilities under\n  Incomplete Information Abstract: We consider assortment optimization over a continuous spectrum of products\nrepresented by the unit interval, where the seller's problem consists of\ndetermining the optimal subset of products to offer to potential customers. To\ndescribe the relation between assortment and customer choice, we propose a\nprobabilistic choice model that forms the continuous counterpart of the widely\nstudied discrete multinomial logit model. We consider the seller's problem\nunder incomplete information, propose a stochastic-approximation type of\npolicy, and show that its regret -- its performance loss compared to the\noptimal policy -- is only logarithmic in the time horizon. We complement this\nresult by showing a matching lower bound on the regret of any policy, implying\nthat our policy is asymptotically optimal. We then show that adding a capacity\nconstraint significantly changes the structure of the problem: we construct a\npolicy and show that its regret after $T$ time periods is bounded above by a\nconstant times $T^{2/3}$ (up to a logarithmic term); in addition, we show that\nthe regret of any policy is bounded from below by a positive constant times\n$T^{2/3}$, so that also in the capacitated case we obtain asymptotic\noptimality. Numerical illustrations show that our policies outperform or are on\npar with alternatives. \n\n"}
{"id": "1807.06560", "contents": "Title: Using link and content over time for embedding generation in Dynamic\n  Attributed Networks Abstract: In this work, we consider the problem of combining link, content and temporal\nanalysis for community detection and prediction in evolving networks. Such\ntemporal and content-rich networks occur in many real-life settings, such as\nbibliographic networks and question answering forums. Most of the work in the\nliterature (that uses both content and structure) deals with static snapshots\nof networks, and they do not reflect the dynamic changes occurring over\nmultiple snapshots. Incorporating dynamic changes in the communities into the\nanalysis can also provide useful insights about the changes in the network such\nas the migration of authors across communities. In this work, we propose\nChimera, a shared factorization model that can simultaneously account for graph\nlinks, content, and temporal analysis. This approach works by extracting the\nlatent semantic structure of the network in multidimensional form, but in a way\nthat takes into account the temporal continuity of these embeddings. Such an\napproach simplifies temporal analysis of the underlying network by using the\nembedding as a surrogate. A consequence of this simplification is that it is\nalso possible to use this temporal sequence of embeddings to predict future\ncommunities. We present experimental results illustrating the effectiveness of\nthe approach. \n\n"}
{"id": "1807.06921", "contents": "Title: Time-Bounded Influence Diffusion with Incentives Abstract: A widely studied model of influence diffusion in social networks represents\nthe network as a graph $G=(V,E)$ with an influence threshold $t(v)$ for each\nnode. Initially the members of an initial set $S\\subseteq V$ are influenced.\nDuring each subsequent round, the set of influenced nodes is augmented by\nincluding every node $v$ that has at least $t(v)$ previously influenced\nneighbours. The general problem is to find a small initial set that influences\nthe whole network. In this paper we extend this model by using\n\\emph{incentives} to reduce the thresholds of some nodes. The goal is to\nminimize the total of the incentives required to ensure that the process\ncompletes within a given number of rounds. The problem is hard to approximate\nin general networks. We present polynomial-time algorithms for paths, trees,\nand complete networks. \n\n"}
{"id": "1807.07162", "contents": "Title: What kind of content are you prone to tweet? Multi-topic Preference\n  Model for Tweeters Abstract: According to tastes, a person could show preference for a given category of\ncontent to a greater or lesser extent. However, quantifying people's amount of\ninterest in a certain topic is a challenging task, especially considering the\nmassive digital information they are exposed to. For example, in the context of\nTwitter, aligned with his/her preferences a user may tweet and retweet more\nabout technology than sports and do not share any music-related content. The\nproblem we address in this paper is the identification of users' implicit topic\npreferences by analyzing the content categories they tend to post on Twitter.\nOur proposal is significant given that modeling their multi-topic profile may\nbe useful to find patterns or association between preferences for categories,\ndiscover trending topics and cluster similar users to generate better group\nrecommendations of content. In the present work, we propose a method based on\nthe Mixed Gaussian Model to extract the multidimensional preference\nrepresentation for 399 Ecuadorian tweeters concerning twenty-two different\ntopics (or dimensions) which became known by manually categorizing 68.186\ntweets. Our experiment findings indicate that the proposed approach is\neffective at detecting the topic interests of users. \n\n"}
{"id": "1807.07540", "contents": "Title: Bayesian filtering unifies adaptive and non-adaptive neural network\n  optimization methods Abstract: We formulate the problem of neural network optimization as Bayesian\nfiltering, where the observations are the backpropagated gradients. While\nneural network optimization has previously been studied using natural gradient\nmethods which are closely related to Bayesian inference, they were unable to\nrecover standard optimizers such as Adam and RMSprop with a root-mean-square\ngradient normalizer, instead getting a mean-square normalizer. To recover the\nroot-mean-square normalizer, we find it necessary to account for the temporal\ndynamics of all the other parameters as they are geing optimized. The resulting\noptimizer, AdaBayes, adaptively transitions between SGD-like and Adam-like\nbehaviour, automatically recovers AdamW, a state of the art variant of Adam\nwith decoupled weight decay, and has generalisation performance competitive\nwith SGD. \n\n"}
{"id": "1807.08910", "contents": "Title: Anomaly Detection of Complex Networks Based on Intuitionistic Fuzzy Set\n  Ensemble Abstract: Ensemble learning for anomaly detection of data structured into complex\nnetwork has been barely studied due to the inconsistent performance of complex\nnetwork characteristics and lack of inherent objective function. In this paper,\nwe propose the IFSAD, a new two-phase ensemble method for anomaly detection\nbased on intuitionistic fuzzy set, and applies it to the abnormal behavior\ndetection problem in temporal complex networks. First, it constructs the\nintuitionistic fuzzy set of single network characteristic which quantifies the\ndegree of membership, non-membership and hesitation of each of network\ncharacteristic to the defined linguistic variables so that makes the unuseful\nor noise characteristics become part of the detection. To build an objective\nintuitionistic fuzzy relationship, we propose an Gaussian distribution-based\nmembership function which gives a variable hesitation degree. Then, for the\nfuzzification of multiple network characteristics, the intuitionistic fuzzy\nweighted geometric operator is adopted to fuse multiple IFSs and to avoid the\ninconsistent of multiple characteristics. Finally, the score function and\nprecision function are used to sort the fused IFS. Finally we carried out\nextensive experiments on several complex network datasets for anomaly\ndetection, and the results demonstrate the superiority of our method to\nstate-of-the-art approaches, validating the effectiveness of our method. \n\n"}
{"id": "1807.09644", "contents": "Title: Three hypergraph eigenvector centralities Abstract: Eigenvector centrality is a standard network analysis tool for determining\nthe importance of (or ranking of) entities in a connected system that is\nrepresented by a graph. However, many complex systems and datasets have natural\nmulti-way interactions that are more faithfully modeled by a hypergraph. Here\nwe extend the notion of graph eigenvector centrality to uniform hypergraphs.\nTraditional graph eigenvector centralities are given by a positive eigenvector\nof the adjacency matrix, which is guaranteed to exist by the Perron-Frobenius\ntheorem under some mild conditions. The natural representation of a hypergraph\nis a hypermatrix (colloquially, a tensor). Using recently established\nPerron-Frobenius theory for tensors, we develop three tensor eigenvectors\ncentralities for hypergraphs, each with different interpretations. We show that\nthese centralities can reveal different information on real-world data by\nanalyzing hypergraphs constructed from n-gram frequencies, co-tagging on stack\nexchange, and drug combinations observed in patient emergency room visits. \n\n"}
{"id": "1807.09846", "contents": "Title: Diffusion and consensus on weakly connected directed graphs Abstract: Let $G$ be a weakly connected directed graph with asymmetric graph Laplacian\n${\\cal L}$. Consensus and diffusion are dual dynamical processes defined on $G$\nby $\\dot x=-{\\cal L}x$ for consensus and $\\dot p=-p{\\cal L}$ for diffusion. We\nconsider both these processes as well their discrete time analogues. We define\na basis of row vectors $\\{\\bar \\gamma_i\\}_{i=1}^k$ of the left null-space of\n${\\cal L}$ and a basis of column vectors $\\{\\gamma_i\\}_{i=1}^k$ of the right\nnull-space of ${\\cal L}$ in terms of the partition of $G$ into strongly\nconnected components. This allows for complete characterization of the\nasymptotic behavior of both diffusion and consensus --- discrete and continuous\n--- in terms of these eigenvectors.\n  As an application of these ideas, we present a treatment of the pagerank\nalgorithm that is dual to the usual one. We further show that the teleporting\nfeature usually included in the algorithm is not strictly necessary.\n  This is a complete and self-contained treatment of the asymptotics of\nconsensus and diffusion on digraphs. Many of the ideas presented here can be\nfound scattered in the literature, though mostly outside mainstream mathematics\nand not always with complete proofs. This paper seeks to remedy this by\nproviding a compact and accessible survey. \n\n"}
{"id": "1807.10328", "contents": "Title: Selective Clustering Annotated using Modes of Projections Abstract: Selective clustering annotated using modes of projections (SCAMP) is a new\nclustering algorithm for data in $\\mathbb{R}^p$. SCAMP is motivated from the\npoint of view of non-parametric mixture modeling. Rather than maximizing a\nclassification likelihood to determine cluster assignments, SCAMP casts\nclustering as a search and selection problem. One consequence of this problem\nformulation is that the number of clusters is $\\textbf{not}$ a SCAMP tuning\nparameter. The search phase of SCAMP consists of finding sub-collections of the\ndata matrix, called candidate clusters, that obey shape constraints along each\ncoordinate projection. An extension of the dip test of Hartigan and Hartigan\n(1985) is developed to assist the search. Selection occurs by scoring each\ncandidate cluster with a preference function that quantifies prior belief about\nthe mixture composition. Clustering proceeds by selecting candidates to\nmaximize their total preference score. SCAMP concludes by annotating each\nselected cluster with labels that describe how cluster-level statistics compare\nto certain dataset-level quantities. SCAMP can be run multiple times on a\nsingle data matrix. Comparison of annotations obtained across iterations\nprovides a measure of clustering uncertainty. Simulation studies and\napplications to real data are considered. A C++ implementation with R interface\nis $\\href{https://github.com/RGLab/scamp}{available\\ online}$. \n\n"}
{"id": "1808.00380", "contents": "Title: A Differentially Private Kernel Two-Sample Test Abstract: Kernel two-sample testing is a useful statistical tool in determining whether\ndata samples arise from different distributions without imposing any parametric\nassumptions on those distributions. However, raw data samples can expose\nsensitive information about individuals who participate in scientific studies,\nwhich makes the current tests vulnerable to privacy breaches. Hence, we design\na new framework for kernel two-sample testing conforming to differential\nprivacy constraints, in order to guarantee the privacy of subjects in the data.\nUnlike existing differentially private parametric tests that simply add noise\nto data, kernel-based testing imposes a challenge due to a complex dependence\nof test statistics on the raw data, as these statistics correspond to\nestimators of distances between representations of probability measures in\nHilbert spaces. Our approach considers finite dimensional approximations to\nthose representations. As a result, a simple chi-squared test is obtained,\nwhere a test statistic depends on a mean and covariance of empirical\ndifferences between the samples, which we perturb for a privacy guarantee. We\ninvestigate the utility of our framework in two realistic settings and conclude\nthat our method requires only a relatively modest increase in sample size to\nachieve a similar level of power to the non-private tests in both settings. \n\n"}
{"id": "1808.00554", "contents": "Title: Traj2User: exploiting embeddings for computing similarity of users\n  mobile behavior Abstract: Semantic trajectories are high level representations of user movements where\nseveral aspects related to the movement context are represented as\nheterogeneous textual labels. With the objective of finding a meaningful\nsimilarity measure for semantically enriched trajectories, we propose\nTraj2User, a Word2Vec-inspired method for the generation of a vector\nrepresentation of user movements as user embeddings. Traj2User uses simple\nrepresentations of trajectories and delegates the definition of the similarity\nmodel to the learning process of the network. Preliminary results show that\nTraj2User is able to generate effective user embeddings. \n\n"}
{"id": "1808.00668", "contents": "Title: On the achievability of blind source separation for high-dimensional\n  nonlinear source mixtures Abstract: For many years, a combination of principal component analysis (PCA) and\nindependent component analysis (ICA) has been used for blind source separation\n(BSS). However, it remains unclear why these linear methods work well with\nreal-world data that involve nonlinear source mixtures. This work theoretically\nvalidates that a cascade of linear PCA and ICA can solve a nonlinear BSS\nproblem accurately -- when the sensory inputs are generated from hidden sources\nvia nonlinear mappings with sufficient dimensionality. Our proposed theorem,\ntermed the asymptotic linearization theorem, theoretically guarantees that\napplying linear PCA to the inputs can reliably extract a subspace spanned by\nthe linear projections from every hidden source as the major components -- and\nthus projecting the inputs onto their major eigenspace can effectively recover\na linear transformation of the hidden sources. Then, subsequent application of\nlinear ICA can separate all the true independent hidden sources accurately.\nZero-element-wise-error nonlinear BSS is asymptotically attained when the\nsource dimensionality is large and the input dimensionality is sufficiently\nlarger than the source dimensionality. Our proposed theorem is validated\nanalytically and numerically. Moreover, the same computation can be performed\nby using Hebbian-like plasticity rules, implying the biological plausibility of\nthis nonlinear BSS strategy. Our results highlight the utility of linear PCA\nand ICA for accurately and reliably recovering nonlinearly mixed sources -- and\nfurther suggest the importance of employing sensors with sufficient\ndimensionality to identify true hidden sources of real-world data. \n\n"}
{"id": "1808.02191", "contents": "Title: Privacy in Social Media: Identification, Mitigation and Applications Abstract: The increasing popularity of social media has attracted a huge number of\npeople to participate in numerous activities on a daily basis. This results in\ntremendous amounts of rich user-generated data. This data provides\nopportunities for researchers and service providers to study and better\nunderstand users' behaviors and further improve the quality of the personalized\nservices. Publishing user-generated data risks exposing individuals' privacy.\nUsers privacy in social media is an emerging task and has attracted increasing\nattention in recent years. These works study privacy issues in social media\nfrom the two different points of views: identification of vulnerabilities, and\nmitigation of privacy risks. Recent research has shown the vulnerability of\nuser-generated data against the two general types of attacks, identity\ndisclosure and attribute disclosure. These privacy issues mandate social media\ndata publishers to protect users' privacy by sanitizing user-generated data\nbefore publishing it. Consequently, various protection techniques have been\nproposed to anonymize user-generated social media data. There is a vast\nliterature on privacy of users in social media from many perspectives. In this\nsurvey, we review the key achievements of user privacy in social media. In\nparticular, we review and compare the state-of-the-art algorithms in terms of\nthe privacy leakage attacks and anonymization algorithms. We overview the\nprivacy risks from different aspects of social media and categorize the\nrelevant works into five groups 1) graph data anonymization and\nde-anonymization, 2) author identification, 3) profile attribute disclosure, 4)\nuser location and privacy, and 5) recommender systems and privacy issues. We\nalso discuss open problems and future research directions for user privacy\nissues in social media. \n\n"}
{"id": "1808.02932", "contents": "Title: Nonparametric Gaussian Mixture Models for the Multi-Armed Bandit Abstract: We here adopt Bayesian nonparametric mixture models to extend multi-armed\nbandits in general, and Thompson sampling in particular, to scenarios where\nthere is reward model uncertainty. In the stochastic multi-armed bandit, the\nreward for the played arm is generated from an unknown distribution. Reward\nuncertainty, i.e., the lack of knowledge about the reward-generating\ndistribution, induces the exploration-exploitation trade-off: a bandit agent\nneeds to simultaneously learn the properties of the reward distribution and\nsequentially decide which action to take next.\n  In this work, we extend Thompson sampling to scenarios where there is reward\nmodel uncertainty by adopting Bayesian nonparametric Gaussian mixture models\nfor flexible reward density estimation. The proposed Bayesian nonparametric\nmixture model Thompson sampling sequentially learns the reward model that best\napproximates the true, yet unknown, per-arm reward distribution, achieving\nsuccessful regret performance. We derive, based on a novel posterior\nconvergence based analysis, an asymptotic regret bound for the proposed method.\nIn addition, we empirically evaluate its performance in diverse and previously\nelusive bandit environments, e.g., with rewards not in the exponential family,\nsubject to outliers, and with different per-arm reward distributions.\n  We show that the proposed Bayesian nonparametric Thompson sampling\noutperforms, both in averaged cumulative regret and in regret volatility,\nstate-of-the-art alternatives. The proposed method is valuable in the presence\nof bandit reward model uncertainty, as it avoids stringent case-by-case model\ndesign choices, yet provides important regret savings. \n\n"}
{"id": "1808.02941", "contents": "Title: On the Convergence of A Class of Adam-Type Algorithms for Non-Convex\n  Optimization Abstract: This paper studies a class of adaptive gradient based momentum algorithms\nthat update the search directions and learning rates simultaneously using past\ngradients. This class, which we refer to as the \"Adam-type\", includes the\npopular algorithms such as the Adam, AMSGrad and AdaGrad. Despite their\npopularity in training deep neural networks, the convergence of these\nalgorithms for solving nonconvex problems remains an open question. This paper\nprovides a set of mild sufficient conditions that guarantee the convergence for\nthe Adam-type methods. We prove that under our derived conditions, these\nmethods can achieve the convergence rate of order $O(\\log{T}/\\sqrt{T})$ for\nnonconvex stochastic optimization. We show the conditions are essential in the\nsense that violating them may make the algorithm diverge. Moreover, we propose\nand analyze a class of (deterministic) incremental adaptive gradient\nalgorithms, which has the same $O(\\log{T}/\\sqrt{T})$ convergence rate. Our\nstudy could also be extended to a broader class of adaptive gradient methods in\nmachine learning and optimization. \n\n"}
{"id": "1808.03001", "contents": "Title: Compressed Sensing Using Binary Matrices of Nearly Optimal Dimensions Abstract: In this paper, we study the problem of compressed sensing using binary\nmeasurement matrices and $\\ell_1$-norm minimization (basis pursuit) as the\nrecovery algorithm. We derive new upper and lower bounds on the number of\nmeasurements to achieve robust sparse recovery with binary matrices. We\nestablish sufficient conditions for a column-regular binary matrix to satisfy\nthe robust null space property (RNSP) and show that the associated sufficient\nconditions % sparsity bounds for robust sparse recovery obtained using the RNSP\nare better by a factor of $(3 \\sqrt{3})/2 \\approx 2.6$ compared to the\nsufficient conditions obtained using the restricted isometry property (RIP).\nNext we derive universal \\textit{lower} bounds on the number of measurements\nthat any binary matrix needs to have in order to satisfy the weaker sufficient\ncondition based on the RNSP and show that bipartite graphs of girth six are\noptimal. Then we display two classes of binary matrices, namely parity check\nmatrices of array codes and Euler squares, which have girth six and are nearly\noptimal in the sense of almost satisfying the lower bound. In principle,\nrandomly generated Gaussian measurement matrices are \"order-optimal\". So we\ncompare the phase transition behavior of the basis pursuit formulation using\nbinary array codes and Gaussian matrices and show that (i) there is essentially\nno difference between the phase transition boundaries in the two cases and (ii)\nthe CPU time of basis pursuit with binary matrices is hundreds of times faster\nthan with Gaussian matrices and the storage requirements are less. Therefore it\nis suggested that binary matrices are a viable alternative to Gaussian matrices\nfor compressed sensing using basis pursuit. \\end{abstract} \n\n"}
{"id": "1808.03782", "contents": "Title: Epidemic spreading on time-varying multiplex networks Abstract: Social interactions are stratified in multiple contexts and are subject to\ncomplex temporal dynamics. The systematic study of these two features of social\nsystems has started only very recently mainly thanks to the development of\nmultiplex and time-varying networks. However, these two advancements have\nprogressed almost in parallel with very little overlap. Thus, the interplay\nbetween multiplexity and the temporal nature of connectivity patterns is poorly\nunderstood. Here, we aim to tackle this limitation by introducing a\ntime-varying model of multiplex networks. We are interested in characterizing\nhow these two properties affect contagion processes. To this end, we study SIS\nepidemic models unfolding at comparable time-scale respect to the evolution of\nthe multiplex network. We study both analytically and numerically the epidemic\nthreshold as a function of the overlap between, and the features of, each\nlayer. We found that, the overlap between layers significantly reduces the\nepidemic threshold especially when the temporal activation patterns of\noverlapping nodes are positively correlated. Furthermore, when the average\nconnectivity across layers is very different, the contagion dynamics are driven\nby the features of the more densely connected layer. Here, the epidemic\nthreshold is equivalent to that of a single layered graph and the impact of the\ndisease, in the layer driving the contagion, is independent of the overlap.\nHowever, this is not the case in the other layers where the spreading dynamics\nare sharply influenced by it. The results presented provide another step\ntowards the characterization of the properties of real networks and their\neffects on contagion phenomena \n\n"}
{"id": "1808.04878", "contents": "Title: Latent Agents in Networks: Estimation and Targeting Abstract: We consider a network of agents. Associated with each agent are her covariate\nand outcome. Agents influence each other's outcomes according to a certain\nconnection/influence structure. A subset of the agents participate on a\nplatform, and hence, are observable to it. The rest are not observable to the\nplatform and are called the latent agents. The platform does not know the\ninfluence structure of the observable or the latent parts of the network. It\nonly observes the data on past covariates and decisions of the observable\nagents. Observable agents influence each other both directly and indirectly\nthrough the influence they exert on the latent agents.\n  We investigate how the platform can estimate the dependence of the observable\nagents' outcomes on their covariates, taking the latent agents into account.\nFirst, we show that this relationship can be succinctly captured by a matrix\nand provide an algorithm for estimating it under a suitable approximate\nsparsity condition using historical data of covariates and outcomes for the\nobservable agents. We also obtain convergence rates for the proposed estimator\ndespite the high dimensionality that allows more agents than observations.\nSecond, we show that the approximate sparsity condition holds under the\nstandard conditions used in the literature. Hence, our results apply to a large\nclass of networks. Finally, we apply our results to two practical settings:\ntargeted advertising and promotional pricing. We show that by using the\navailable historical data with our estimator, it is possible to obtain\nasymptotically optimal advertising/pricing decisions, despite the presence of\nlatent agents. \n\n"}
{"id": "1808.05527", "contents": "Title: Deep Learning for Energy Markets Abstract: Deep Learning is applied to energy markets to predict extreme loads observed\nin energy grids. Forecasting energy loads and prices is challenging due to\nsharp peaks and troughs that arise due to supply and demand fluctuations from\nintraday system constraints. We propose deep spatio-temporal models and extreme\nvalue theory (EVT) to capture theses effects and in particular the tail\nbehavior of load spikes. Deep LSTM architectures with ReLU and $\\tanh$\nactivation functions can model trends and temporal dependencies while EVT\ncaptures highly volatile load spikes above a pre-specified threshold. To\nillustrate our methodology, we use hourly price and demand data from 4719 nodes\nof the PJM interconnection, and we construct a deep predictor. We show that\nDL-EVT outperforms traditional Fourier time series methods, both in-and\nout-of-sample, by capturing the observed nonlinearities in prices. Finally, we\nconclude with directions for future research. \n\n"}
{"id": "1808.06664", "contents": "Title: Out-of-Distribution Detection using Multiple Semantic Label\n  Representations Abstract: Deep Neural Networks are powerful models that attained remarkable results on\na variety of tasks. These models are shown to be extremely efficient when\ntraining and test data are drawn from the same distribution. However, it is not\nclear how a network will act when it is fed with an out-of-distribution\nexample. In this work, we consider the problem of out-of-distribution detection\nin neural networks. We propose to use multiple semantic dense representations\ninstead of sparse representation as the target label. Specifically, we propose\nto use several word representations obtained from different corpora or\narchitectures as target labels. We evaluated the proposed model on computer\nvision, and speech commands detection tasks and compared it to previous\nmethods. Results suggest that our method compares favorably with previous work.\nBesides, we present the efficiency of our approach for detecting wrongly\nclassified and adversarial examples. \n\n"}
{"id": "1808.07573", "contents": "Title: Approximation Trees: Statistical Stability in Model Distillation Abstract: This paper examines the stability of learned explanations for black-box\npredictions via model distillation with decision trees. One approach to\nintelligibility in machine learning is to use an understandable `student' model\nto mimic the output of an accurate `teacher'. Here, we consider the use of\nregression trees as a student model, in which nodes of the tree can be used as\n`explanations' for particular predictions, and the whole structure of the tree\ncan be used as a global representation of the resulting function. However,\nindividual trees are sensitive to the particular data sets used to train them,\nand an interpretation of a student model may be suspect if small changes in the\ntraining data have a large effect on it. In this context, access to outcomes\nfrom a teacher helps to stabilize the greedy splitting strategy by generating a\nmuch larger corpus of training examples than was originally available. We\ndevelop tests to ensure that enough examples are generated at each split so\nthat the same splitting rule would be chosen with high probability were the\ntree to be re trained. Further, we develop a stopping rule to indicate how deep\nthe tree should be built based on recent results on the variability of Random\nForests when these are used as the teacher. We provide concrete examples of\nthese procedures on the CAD-MDD and COMPAS data sets. \n\n"}
{"id": "1808.07769", "contents": "Title: Topology and Prediction Focused Research on Graph Convolutional Neural\n  Networks Abstract: Important advances have been made using convolutional neural network (CNN)\napproaches to solve complicated problems in areas that rely on grid structured\ndata such as image processing and object classification. Recently, research on\ngraph convolutional neural networks (GCNN) has increased dramatically as\nresearchers try to replicate the success of CNN for graph structured data.\nUnfortunately, traditional CNN methods are not readily transferable to GCNN,\ngiven the irregularity and geometric complexity of graphs. The emerging field\nof GCNN is further complicated by research papers that differ greatly in their\nscope, detail, and level of academic sophistication needed by the reader.\n  The present paper provides a review of some basic properties of GCNN. As a\nguide to the interested reader, recent examples of GCNN research are then\ngrouped according to techniques that attempt to uncover the underlying topology\nof the graph model and those that seek to generalize traditional CNN methods on\ngraph data to improve prediction of class membership. Discrete Signal\nProcessing on Graphs (DSPg) is used as a theoretical framework to better\nunderstand some of the performance gains and limitations of these recent GCNN\napproaches. A brief discussion of Topology Adaptive Graph Convolutional\nNetworks (TAGCN) is presented as an approach motivated by DSPg and future\nresearch directions using this approach are briefly discussed. \n\n"}
{"id": "1808.08195", "contents": "Title: GoT-WAVE: Temporal network alignment using graphlet-orbit transitions Abstract: Global pairwise network alignment (GPNA) aims to find a one-to-one node\nmapping between two networks that identifies conserved network regions. GPNA\nalgorithms optimize node conservation (NC) and edge conservation (EC). NC\nquantifies topological similarity between nodes. Graphlet-based degree vectors\n(GDVs) are a state-of-the-art topological NC measure. Dynamic GDVs (DGDVs) were\nused as a dynamic NC measure within the first-ever algorithms for GPNA of\ntemporal networks: DynaMAGNA++ and DynaWAVE. The latter is superior for larger\nnetworks. We recently developed a different graphlet-based measure of temporal\nnode similarity, graphlet-orbit transitions (GoTs). Here, we use GoTs instead\nof DGDVs as a new dynamic NC measure within DynaWAVE, resulting in a new\napproach, GoT-WAVE.\n  On synthetic networks, GoT-WAVE improves DynaWAVE's accuracy by 25% and speed\nby 64%. On real networks, when optimizing only dynamic NC, each method is\nsuperior ~50% of the time. While DynaWAVE benefits more from also optimizing\ndynamic EC, only GoT-WAVE can support directed edges. Hence, GoT-WAVE is a\npromising new temporal GPNA algorithm, which efficiently optimizes dynamic NC.\nFuture work on better incorporating dynamic EC may yield further improvements. \n\n"}
{"id": "1808.08317", "contents": "Title: To Cluster, or Not to Cluster: An Analysis of Clusterability Methods Abstract: Clustering is an essential data mining tool that aims to discover inherent\ncluster structure in data. For most applications, applying clustering is only\nappropriate when cluster structure is present. As such, the study of\nclusterability, which evaluates whether data possesses such structure, is an\nintegral part of cluster analysis. However, methods for evaluating\nclusterability vary radically, making it challenging to select a suitable\nmeasure. In this paper, we perform an extensive comparison of measures of\nclusterability and provide guidelines that clustering users can reference to\nselect suitable measures for their applications. \n\n"}
{"id": "1808.09645", "contents": "Title: Diffusion Approximations for Online Principal Component Estimation and\n  Global Convergence Abstract: In this paper, we propose to adopt the diffusion approximation tools to study\nthe dynamics of Oja's iteration which is an online stochastic gradient descent\nmethod for the principal component analysis. Oja's iteration maintains a\nrunning estimate of the true principal component from streaming data and enjoys\nless temporal and spatial complexities. We show that the Oja's iteration for\nthe top eigenvector generates a continuous-state discrete-time Markov chain\nover the unit sphere. We characterize the Oja's iteration in three phases using\ndiffusion approximation and weak convergence tools. Our three-phase analysis\nfurther provides a finite-sample error bound for the running estimate, which\nmatches the minimax information lower bound for principal component analysis\nunder the additional assumption of bounded samples. \n\n"}
{"id": "1808.10551", "contents": "Title: Dynamic mode decomposition in vector-valued reproducing kernel Hilbert\n  spaces for extracting dynamical structure among observables Abstract: Understanding nonlinear dynamical systems (NLDSs) is challenging in a variety\nof engineering and scientific fields. Dynamic mode decomposition (DMD), which\nis a numerical algorithm for the spectral analysis of Koopman operators, has\nbeen attracting attention as a way of obtaining global modal descriptions of\nNLDSs without requiring explicit prior knowledge. However, since existing DMD\nalgorithms are in principle formulated based on the concatenation of scalar\nobservables, it is not directly applicable to data with dependent structures\namong observables, which take, for example, the form of a sequence of graphs.\nIn this paper, we formulate Koopman spectral analysis for NLDSs with structures\namong observables and propose an estimation algorithm for this problem. This\nmethod can extract and visualize the underlying low-dimensional global dynamics\nof NLDSs with structures among observables from data, which can be useful in\nunderstanding the underlying dynamics of such NLDSs. To this end, we first\nformulate the problem of estimating spectra of the Koopman operator defined in\nvector-valued reproducing kernel Hilbert spaces, and then develop an estimation\nprocedure for this problem by reformulating tensor-based DMD. As a special case\nof our method, we propose the method named as Graph DMD, which is a numerical\nalgorithm for Koopman spectral analysis of graph dynamical systems, using a\nsequence of adjacency matrices. We investigate the empirical performance of our\nmethod by using synthetic and real-world data. \n\n"}
{"id": "1808.10585", "contents": "Title: On the Minimal Supervision for Training Any Binary Classifier from Only\n  Unlabeled Data Abstract: Empirical risk minimization (ERM), with proper loss function and\nregularization, is the common practice of supervised classification. In this\npaper, we study training arbitrary (from linear to deep) binary classifier from\nonly unlabeled (U) data by ERM. We prove that it is impossible to estimate the\nrisk of an arbitrary binary classifier in an unbiased manner given a single set\nof U data, but it becomes possible given two sets of U data with different\nclass priors. These two facts answer a fundamental question---what the minimal\nsupervision is for training any binary classifier from only U data. Following\nthese findings, we propose an ERM-based learning method from two sets of U\ndata, and then prove it is consistent. Experiments demonstrate the proposed\nmethod could train deep models and outperform state-of-the-art methods for\nlearning from two sets of U data. \n\n"}
{"id": "1809.00130", "contents": "Title: Semi-supervised Learning on Graphs with Generative Adversarial Nets Abstract: We investigate how generative adversarial nets (GANs) can help\nsemi-supervised learning on graphs. We first provide insights on working\nprinciples of adversarial learning over graphs and then present GraphSGAN, a\nnovel approach to semi-supervised learning on graphs. In GraphSGAN, generator\nand classifier networks play a novel competitive game. At equilibrium,\ngenerator generates fake samples in low-density areas between subgraphs. In\norder to discriminate fake samples from the real, classifier implicitly takes\nthe density property of subgraph into consideration. An efficient adversarial\nlearning algorithm has been developed to improve traditional normalized graph\nLaplacian regularization with a theoretical guarantee. Experimental results on\nseveral different genres of datasets show that the proposed GraphSGAN\nsignificantly outperforms several state-of-the-art methods. GraphSGAN can be\nalso trained using mini-batch, thus enjoys the scalability advantage. \n\n"}
{"id": "1809.00164", "contents": "Title: Hypergraph Modeling and Visualisation of Complex Co-occurence Networks Abstract: Finding inherent or processed links within a dataset allows to discover\npotential knowledge. The main contribution of this article is to define a\nglobal framework that enables optimal knowledge discovery by visually rendering\nco-occurences (i.e. groups of linked data instances attached to a metadata\nreference) - either inherently present or processed - from a dataset as facets.\nHypergraphs are well suited for modeling co-occurences since they support\nmulti-adicity whereas graphs only support pairwise relationships. This article\nintroduces an efficient navigation between different facets of an information\nspace based on hypergraph modelisation and visualisation. \n\n"}
{"id": "1809.01093", "contents": "Title: Adversarial Attacks on Node Embeddings via Graph Poisoning Abstract: The goal of network representation learning is to learn low-dimensional node\nembeddings that capture the graph structure and are useful for solving\ndownstream tasks. However, despite the proliferation of such methods, there is\ncurrently no study of their robustness to adversarial attacks. We provide the\nfirst adversarial vulnerability analysis on the widely used family of methods\nbased on random walks. We derive efficient adversarial perturbations that\npoison the network structure and have a negative effect on both the quality of\nthe embeddings and the downstream tasks. We further show that our attacks are\ntransferable since they generalize to many models and are successful even when\nthe attacker is restricted. \n\n"}
{"id": "1809.01286", "contents": "Title: FakeNewsNet: A Data Repository with News Content, Social Context and\n  Spatialtemporal Information for Studying Fake News on Social Media Abstract: Social media has become a popular means for people to consume news.\nMeanwhile, it also enables the wide dissemination of fake news, i.e., news with\nintentionally false information, which brings significant negative effects to\nthe society. Thus, fake news detection is attracting increasing attention.\nHowever, fake news detection is a non-trivial task, which requires multi-source\ninformation such as news content, social context, and dynamic information.\nFirst, fake news is written to fool people, which makes it difficult to detect\nfake news simply based on news contents. In addition to news contents, we need\nto explore social contexts such as user engagements and social behaviors. For\nexample, a credible user's comment that \"this is a fake news\" is a strong\nsignal for detecting fake news. Second, dynamic information such as how fake\nnews and true news propagate and how users' opinions toward news pieces are\nvery important for extracting useful patterns for (early) fake news detection\nand intervention. Thus, comprehensive datasets which contain news content,\nsocial context, and dynamic information could facilitate fake news propagation,\ndetection, and mitigation; while to the best of our knowledge, existing\ndatasets only contains one or two aspects. Therefore, in this paper, to\nfacilitate fake news related researches, we provide a fake news data repository\nFakeNewsNet, which contains two comprehensive datasets that includes news\ncontent, social context, and dynamic information. We present a comprehensive\ndescription of datasets collection, demonstrate an exploratory analysis of this\ndata repository from different perspectives, and discuss the benefits of\nFakeNewsNet for potential applications on fake news study on social media. \n\n"}
{"id": "1809.01293", "contents": "Title: Stochastic Particle-Optimization Sampling and the Non-Asymptotic\n  Convergence Theory Abstract: Particle-optimization-based sampling (POS) is a recently developed effective\nsampling technique that interactively updates a set of particles. A\nrepresentative algorithm is the Stein variational gradient descent (SVGD). We\nprove, under certain conditions, SVGD experiences a theoretical pitfall, {\\it\ni.e.}, particles tend to collapse. As a remedy, we generalize POS to a\nstochastic setting by injecting random noise into particle updates, thus\nyielding particle-optimization sampling (SPOS). Notably, for the first time, we\ndevelop {\\em non-asymptotic convergence theory} for the SPOS framework (related\nto SVGD), characterizing algorithm convergence in terms of the 1-Wasserstein\ndistance w.r.t.\\! the numbers of particles and iterations. Somewhat\nsurprisingly, with the same number of updates (not too large) for each\nparticle, our theory suggests adopting more particles does not necessarily lead\nto a better approximation of a target distribution, due to limited\ncomputational budget and numerical errors. This phenomenon is also observed in\nSVGD and verified via an experiment on synthetic data. Extensive experimental\nresults verify our theory and demonstrate the effectiveness of our proposed\nframework. \n\n"}
{"id": "1809.01369", "contents": "Title: Towards quantitative methods to assess network generative models Abstract: Assessing generative models is not an easy task. Generative models should\nsynthesize graphs which are not replicates of real networks but show\ntopological features similar to real graphs. We introduce an approach for\nassessing graph generative models using graph classifiers. The inability of an\nestablished graph classifier for distinguishing real and synthesized graphs\ncould be considered as a performance measurement for graph generators. \n\n"}
{"id": "1809.02235", "contents": "Title: A Bandit Approach to Multiple Testing with False Discovery Control Abstract: We propose an adaptive sampling approach for multiple testing which aims to\nmaximize statistical power while ensuring anytime false discovery control. We\nconsider $n$ distributions whose means are partitioned by whether they are\nbelow or equal to a baseline (nulls), versus above the baseline (actual\npositives). In addition, each distribution can be sequentially and repeatedly\nsampled. Inspired by the multi-armed bandit literature, we provide an algorithm\nthat takes as few samples as possible to exceed a target true positive\nproportion (i.e. proportion of actual positives discovered) while giving\nanytime control of the false discovery proportion (nulls predicted as actual\npositives). Our sample complexity results match known information theoretic\nlower bounds and through simulations we show a substantial performance\nimprovement over uniform sampling and an adaptive elimination style algorithm.\nGiven the simplicity of the approach, and its sample efficiency, the method has\npromise for wide adoption in the biological sciences, clinical testing for drug\ndiscovery, and online A/B/n testing problems. \n\n"}
{"id": "1809.02596", "contents": "Title: VOS: a Method for Variational Oversampling of Imbalanced Data Abstract: Class imbalanced datasets are common in real-world applications that range\nfrom credit card fraud detection to rare disease diagnostics. Several popular\nclassification algorithms assume that classes are approximately balanced, and\nhence build the accompanying objective function to maximize an overall accuracy\nrate. In these situations, optimizing the overall accuracy will lead to highly\nskewed predictions towards the majority class. Moreover, the negative business\nimpact resulting from false positives (positive samples incorrectly classified\nas negative) can be detrimental. Many methods have been proposed to address the\nclass imbalance problem, including methods such as over-sampling,\nunder-sampling and cost-sensitive methods. In this paper, we consider the\nover-sampling method, where the aim is to augment the original dataset with\nsynthetically created observations of the minority classes. In particular,\ninspired by the recent advances in generative modelling techniques (e.g.,\nVariational Inference and Generative Adversarial Networks), we introduce a new\noversampling technique based on variational autoencoders. Our experiments show\nthat the new method is superior in augmenting datasets for downstream\nclassification tasks when compared to traditional oversampling methods. \n\n"}
{"id": "1809.02709", "contents": "Title: Exploiting Edge Features in Graph Neural Networks Abstract: Edge features contain important information about graphs. However, current\nstate-of-the-art neural network models designed for graph learning, e.g. graph\nconvolutional networks (GCN) and graph attention networks (GAT), adequately\nutilize edge features, especially multi-dimensional edge features. In this\npaper, we build a new framework for a family of new graph neural network models\nthat can more sufficiently exploit edge features, including those of undirected\nor multi-dimensional edges. The proposed framework can consolidate current\ngraph neural network models; e.g. graph convolutional networks (GCN) and graph\nattention networks (GAT). The proposed framework and new models have the\nfollowing novelties: First, we propose to use doubly stochastic normalization\nof graph edge features instead of the commonly used row or symmetric\nnormalization approches used in current graph neural networks. Second, we\nconstruct new formulas for the operations in each individual layer so that they\ncan handle multi-dimensional edge features. Third, for the proposed new\nframework, edge features are adaptive across network layers. As a result, our\nproposed new framework and new models can exploit a rich source of graph\ninformation. We apply our new models to graph node classification on several\ncitation networks, whole graph classification, and regression on several\nmolecular datasets. Compared with the current state-of-the-art methods, i.e.\nGCNs and GAT, our models obtain better performance, which testify to the\nimportance of exploiting edge features in graph neural networks. \n\n"}
{"id": "1809.03267", "contents": "Title: Feature Learning for Meta-Paths in Knowledge Graphs Abstract: In this thesis, we study the problem of feature learning on heterogeneous\nknowledge graphs. These features can be used to perform tasks such as link\nprediction, classification and clustering on graphs. Knowledge graphs provide\nrich semantics encoded in the edge and node types. Meta-paths consist of these\ntypes and abstract paths in the graph. Until now, meta-paths can only be used\nas categorical features with high redundancy and are therefore unsuitable for\nmachine learning models. We propose meta-path embeddings to solve this problem\nby learning semantical and compact vector representations of them. Current\ngraph embedding methods only embed nodes and edge types and therefore miss\nsemantics encoded in the combination of them. Our method embeds meta-paths\nusing the skipgram model with an extension to deal with the redundancy and high\namount of meta-paths in big knowledge graphs. We critically evaluate our\nembedding approach by predicting links on Wikidata. The experiments indicate\nthat we learn a sensible embedding of the meta-paths but can improve it\nfurther. \n\n"}
{"id": "1809.04279", "contents": "Title: Discretely Relaxing Continuous Variables for tractable Variational\n  Inference Abstract: We explore a new research direction in Bayesian variational inference with\ndiscrete latent variable priors where we exploit Kronecker matrix algebra for\nefficient and exact computations of the evidence lower bound (ELBO). The\nproposed \"DIRECT\" approach has several advantages over its predecessors; (i) it\ncan exactly compute ELBO gradients (i.e. unbiased, zero-variance gradient\nestimates), eliminating the need for high-variance stochastic gradient\nestimators and enabling the use of quasi-Newton optimization methods; (ii) its\ntraining complexity is independent of the number of training points, permitting\ninference on large datasets; and (iii) its posterior samples consist of sparse\nand low-precision quantized integers which permit fast inference on hardware\nlimited devices. In addition, our DIRECT models can exactly compute statistical\nmoments of the parameterized predictive posterior without relying on Monte\nCarlo sampling. The DIRECT approach is not practical for all likelihoods,\nhowever, we identify a popular model structure which is practical, and\ndemonstrate accurate inference using latent variables discretized as extremely\nlow-precision 4-bit quantized integers. While the ELBO computations considered\nin the numerical studies require over $10^{2352}$ log-likelihood evaluations,\nwe train on datasets with over two-million points in just seconds. \n\n"}
{"id": "1809.04379", "contents": "Title: Bayesian Semi-supervised Learning with Graph Gaussian Processes Abstract: We propose a data-efficient Gaussian process-based Bayesian approach to the\nsemi-supervised learning problem on graphs. The proposed model shows extremely\ncompetitive performance when compared to the state-of-the-art graph neural\nnetworks on semi-supervised learning benchmark experiments, and outperforms the\nneural networks in active learning experiments where labels are scarce.\nFurthermore, the model does not require a validation data set for early\nstopping to control over-fitting. Our model can be viewed as an instance of\nempirical distribution regression weighted locally by network connectivity. We\nfurther motivate the intuitive construction of the model with a Bayesian linear\nmodel interpretation where the node features are filtered by an operator\nrelated to the graph Laplacian. The method can be easily implemented by\nadapting off-the-shelf scalable variational inference algorithms for Gaussian\nprocesses. \n\n"}
{"id": "1809.04578", "contents": "Title: Simplicity Creates Inequity: Implications for Fairness, Stereotypes, and\n  Interpretability Abstract: Algorithms are increasingly used to aid, or in some cases supplant, human\ndecision-making, particularly for decisions that hinge on predictions. As a\nresult, two additional features in addition to prediction quality have\ngenerated interest: (i) to facilitate human interaction and understanding with\nthese algorithms, we desire prediction functions that are in some fashion\nsimple or interpretable; and (ii) because they influence consequential\ndecisions, we also want them to produce equitable allocations. We develop a\nformal model to explore the relationship between the demands of simplicity and\nequity. Although the two concepts appear to be motivated by qualitatively\ndistinct goals, we show a fundamental inconsistency between them. Specifically,\nwe formalize a general framework for producing simple prediction functions, and\nin this framework we establish two basic results. First, every simple\nprediction function is strictly improvable: there exists a more complex\nprediction function that is both strictly more efficient and also strictly more\nequitable. Put another way, using a simple prediction function both reduces\nutility for disadvantaged groups and reduces overall welfare relative to other\noptions. Second, we show that simple prediction functions necessarily create\nincentives to use information about individuals' membership in a disadvantaged\ngroup --- incentives that weren't present before simplification, and that work\nagainst these individuals. Thus, simplicity transforms disadvantage into bias\nagainst the disadvantaged group. Our results are not only about algorithms but\nabout any process that produces simple models, and as such they connect to the\npsychology of stereotypes and to an earlier economics literature on statistical\ndiscrimination. \n\n"}
{"id": "1809.05578", "contents": "Title: Ensemble Clustering for Graphs Abstract: We propose an ensemble clustering algorithm for graphs (ECG), which is based\non the Louvain algorithm and the concept of consensus clustering. We validate\nour approach by replicating a recently published study comparing graph\nclustering algorithms over artificial networks, showing that ECG outperforms\nthe leading algorithms from that study. We also illustrate how the ensemble\nobtained with ECG can be used to quantify the presence of community structure\nin the graph. \n\n"}
{"id": "1809.06060", "contents": "Title: Contact Adaption during Epidemics: A Multilayer Network Formulation\n  Approach Abstract: People change their physical contacts as a preventive response to infectious\ndisease propagations. Yet, only a few mathematical models consider the coupled\ndynamics of the disease propagation and the contact adaptation process. This\npaper presents a model where each agent has a default contact neighborhood set,\nand switches to a different contact set once she becomes alert about infection\namong her default contacts. Since each agent can adopt either of two possible\nneighborhood sets, the overall contact network switches among 2^N possible\nconfigurations. Notably, a two-layer network representation can fully model the\nunderlying adaptive, state-dependent contact network. Contact adaptation\ninfluences the size of the disease prevalence and the epidemic threshold---a\ncharacteristic measure of a contact network robustness against epidemics---in a\nnonlinear fashion. Particularly, the epidemic threshold for the presented\nadaptive contact network belongs to the solution of a nonlinear\nPerron-Frobenius (NPF) problem, which does not depend on the contact adaptation\nrate monotonically. Furthermore, the network adaptation model predicts a\ncounter-intuitive scenario where adaptively changing contacts may adversely\nlead to lower network robustness against epidemic spreading if the contact\nadaptation is not fast enough. An original result for a class of NPF problems\nfacilitate the analytical developments in this paper. \n\n"}
{"id": "1809.06253", "contents": "Title: Multi-hop assortativities for networks classification Abstract: Several social, medical, engineering and biological challenges rely on\ndiscovering the functionality of networks from their structure and node\nmetadata, when it is available. For example, in chemoinformatics one might want\nto detect whether a molecule is toxic based on structure and atomic types, or\ndiscover the research field of a scientific collaboration network. Existing\ntechniques rely on counting or measuring structural patterns that are known to\nshow large variations from network to network, such as the number of triangles,\nor the assortativity of node metadata. We introduce the concept of multi-hop\nassortativity, that captures the similarity of the nodes situated at the\nextremities of a randomly selected path of a given length. We show that\nmulti-hop assortativity unifies various existing concepts and offers a\nversatile family of 'fingerprints' to characterize networks. These fingerprints\nallow in turn to recover the functionalities of a network, with the help of the\nmachine learning toolbox. Our method is evaluated empirically on established\nsocial and chemoinformatic network benchmarks. Results reveal that our\nassortativity based features are competitive providing highly accurate results\noften outperforming state of the art methods for the network classification\ntask. \n\n"}
{"id": "1809.07109", "contents": "Title: InfoSSM: Interpretable Unsupervised Learning of Nonparametric\n  State-Space Model for Multi-modal Dynamics Abstract: The goal of system identification is to learn about underlying physics\ndynamics behind the time-series data. To model the probabilistic and\nnonparametric dynamics model, Gaussian process (GP) have been widely used; GP\ncan estimate the uncertainty of prediction and avoid over-fitting. Traditional\nGPSSMs, however, are based on Gaussian transition model, thus often have\ndifficulty in describing a more complex transition model, e.g. aircraft\nmotions. To resolve the challenge, this paper proposes a framework using\nmultiple GP transition models which is capable of describing multi-modal\ndynamics. Furthermore, we extend the model to the information-theoretic\nframework, the so-called InfoSSM, by introducing a mutual information\nregularizer helping the model to learn interpretable and distinguishable\nmultiple dynamics models. Two illustrative numerical experiments in simple\nDubins vehicle and high-fidelity flight simulator are presented to demonstrate\nthe performance and interpretability of the proposed model. Finally, this paper\nintroduces a framework using InfoSSM with Bayesian filtering for air traffic\ncontrol tracking. \n\n"}
{"id": "1809.07688", "contents": "Title: Inferring Multiplex Diffusion Network via Multivariate Marked Hawkes\n  Process Abstract: Understanding the diffusion in social network is an important task. However,\nthis task is challenging since (1) the network structure is usually hidden with\nonly observations of events like \"post\" or \"repost\" associated with each node,\nand (2) the interactions between nodes encompass multiple distinct patterns\nwhich in turn affect the diffusion patterns. For instance, social interactions\nseldom develop on a single channel, and multiple relationships can bind pairs\nof people due to their various common interests. Most previous work considers\nonly one of these two challenges which is apparently unrealistic. In this\npaper, we study the problem of \\emph{inferring multiplex network} in social\nnetworks. We propose the Multiplex Diffusion Model (MDM) which incorporates the\nmultivariate marked Hawkes process and topic model to infer the multiplex\nstructure of social network. A MCMC based algorithm is developed to infer the\nlatent multiplex structure and to estimate the node-related parameters. We\nevaluate our model based on both synthetic and real-world datasets. The results\nshow that our model is more effective in terms of uncovering the multiplex\nnetwork structure. \n\n"}
{"id": "1809.08196", "contents": "Title: Analysis of Irregular Spatial Data with Machine Learning: Classification\n  of Building Patterns with a Graph Convolutional Neural Network Abstract: Machine learning methods such as convolutional neural networks (CNNs) are\nbecoming an integral part of scientific research in many disciplines, spatial\nvector data often fail to be analyzed using these powerful learning methods\nbecause of its irregularities. With the aid of graph Fourier transform and\nconvolution theorem, it is possible to convert the convolution as a point-wise\nproduct in Fourier domain and construct a learning architecture of CNN on graph\nfor the analysis task of irregular spatial data. In this study, we used the\nclassification task of building patterns as a case study to test this method,\nand experiments showed that this method has achieved outstanding results in\nidentifying regular and irregular patterns, and has significantly improved in\ncomparing with other methods. \n\n"}
{"id": "1809.08567", "contents": "Title: Identification and Visualization of the Underlying Independent Causes of\n  the Diagnostic of Diabetic Retinopathy made by a Deep Learning Classifier Abstract: Interpretability is a key factor in the design of automatic classifiers for\nmedical diagnosis. Deep learning models have been proven to be a very effective\nclassification algorithm when trained in a supervised way with enough data. The\nmain concern is the difficulty of inferring rationale interpretations from\nthem. Different attempts have been done in last years in order to convert deep\nlearning classifiers from high confidence statistical black box machines into\nself-explanatory models. In this paper we go forward into the generation of\nexplanations by identifying the independent causes that use a deep learning\nmodel for classifying an image into a certain class. We use a combination of\nIndependent Component Analysis with a Score Visualization technique. In this\npaper we study the medical problem of classifying an eye fundus image into 5\nlevels of Diabetic Retinopathy. We conclude that only 3 independent components\nare enough for the differentiation and correct classification between the 5\ndisease standard classes. We propose a method for visualizing them and\ndetecting lesions from the generated visual maps. \n\n"}
{"id": "1809.10158", "contents": "Title: 'Senator, We Sell Ads': Analysis of the 2016 Russian Facebook Ads\n  Campaign Abstract: One of the key aspects of the United States democracy is free and fair\nelections that allow for a peaceful transfer of power from one President to the\nnext. The 2016 US presidential election stands out due to suspected foreign\ninfluence before, during, and after the election. A significant portion of that\nsuspected influence was carried out via social media. In this paper, we look\nspecifically at 3,500 Facebook ads allegedly purchased by the Russian\ngovernment. These ads were released on May 10, 2018 by the US Congress House\nIntelligence Committee. We analyzed the ads using natural language processing\ntechniques to determine textual and semantic features associated with the most\neffective ones. We clustered the ads over time into the various campaigns and\nthe labeled parties associated with them. We also studied the effectiveness of\nAds on an individual, campaign and party basis. The most effective ads tend to\nhave less positive sentiment, focus on past events and are more specific and\npersonalized in nature. The more effective campaigns also show such similar\ncharacteristics. The campaigns' duration and promotion of the Ads suggest a\ndesire to sow division rather than sway the election. \n\n"}
{"id": "1809.10325", "contents": "Title: Being Corrupt Requires Being Clever, But Detecting Corruption Doesn't Abstract: We consider a variation of the problem of corruption detection on networks\nposed by Alon, Mossel, and Pemantle '15. In this model, each vertex of a graph\ncan be either truthful or corrupt. Each vertex reports about the types\n(truthful or corrupt) of all its neighbors to a central agency, where truthful\nnodes report the true types they see and corrupt nodes report adversarially.\nThe central agency aggregates these reports and attempts to find a single\ntruthful node. Inspired by real auditing networks, we pose our problem for\narbitrary graphs and consider corruption through a computational lens. We\nidentify a key combinatorial parameter of the graph $m(G)$, which is the\nminimal number of corrupted agents needed to prevent the central agency from\nidentifying a single truthful node. We give an efficient (in fact, linear time)\nalgorithm for the central agency to identify a truthful node that is successful\nwhenever the number of corrupt nodes is less than $m(G)/2$. On the other hand,\nwe prove that for any constant $\\alpha > 1$, it is NP-hard to find a subset of\nnodes $S$ in $G$ such that corrupting $S$ prevents the central agency from\nfinding one truthful node and $|S| \\leq \\alpha m(G)$, assuming the Small Set\nExpansion Hypothesis (Raghavendra and Steurer, STOC '10). We conclude that\nbeing corrupt requires being clever, while detecting corruption does not.\n  Our main technical insight is a relation between the minimum number of\ncorrupt nodes required to hide all truthful nodes and a certain notion of\nvertex separability for the underlying graph. Additionally, this insight lets\nus design an efficient algorithm for a corrupt party to decide which graphs\nrequire the fewest corrupted nodes, up to a multiplicative factor of $O(\\log\nn)$. \n\n"}
{"id": "1809.10330", "contents": "Title: Variance reduction properties of the reparameterization trick Abstract: The reparameterization trick is widely used in variational inference as it\nyields more accurate estimates of the gradient of the variational objective\nthan alternative approaches such as the score function method. Although there\nis overwhelming empirical evidence in the literature showing its success, there\nis relatively little research exploring why the reparameterization trick is so\neffective. We explore this under the idealized assumptions that the variational\napproximation is a mean-field Gaussian density and that the log of the joint\ndensity of the model parameters and the data is a quadratic function that\ndepends on the variational mean. From this, we show that the marginal variances\nof the reparameterization gradient estimator are smaller than those of the\nscore function gradient estimator. We apply the result of our idealized\nanalysis to real-world examples. \n\n"}
{"id": "1809.10482", "contents": "Title: Budgeted Multi-Objective Optimization with a Focus on the Central Part\n  of the Pareto Front -- Extended Version Abstract: Optimizing nonlinear systems involving expensive computer experiments with\nregard to conflicting objectives is a common challenge. When the number of\nexperiments is severely restricted and/or when the number of objectives\nincreases, uncovering the whole set of Pareto optimal solutions is out of\nreach, even for surrogate-based approaches: the proposed solutions are\nsub-optimal or do not cover the front well. As non-compromising optimal\nsolutions have usually little point in applications, this work restricts the\nsearch to solutions that are close to the Pareto front center. The article\nstarts by characterizing this center, which is defined for any type of front.\nNext, a Bayesian multi-objective optimization method for directing the search\ntowards it is proposed. Targeting a subset of the Pareto front allows an\nimproved optimality of the solutions and a better coverage of this zone, which\nis our main concern. A criterion for detecting convergence to the center is\ndescribed. If the criterion is triggered, a widened central part of the Pareto\nfront is targeted such that sufficiently accurate convergence to it is\nforecasted within the remaining budget. Numerical experiments show how the\nresulting algorithm, C-EHI, better locates the central part of the Pareto front\nwhen compared to state-of-the-art Bayesian algorithms. \n\n"}
{"id": "1810.00315", "contents": "Title: Convex Relaxation Methods for Community Detection Abstract: This paper surveys recent theoretical advances in convex optimization\napproaches for community detection. We introduce some important theoretical\ntechniques and results for establishing the consistency of convex community\ndetection under various statistical models. In particular, we discuss the basic\ntechniques based on the primal and dual analysis. We also present results that\ndemonstrate several distinctive advantages of convex community detection,\nincluding robustness against outlier nodes, consistency under weak\nassortativity, and adaptivity to heterogeneous degrees.\n  This survey is not intended to be a complete overview of the vast literature\non this fast-growing topic. Instead, we aim to provide a big picture of the\nremarkable recent development in this area and to make the survey accessible to\na broad audience. We hope that this expository article can serve as an\nintroductory guide for readers who are interested in using, designing, and\nanalyzing convex relaxation methods in network analysis. \n\n"}
{"id": "1810.00368", "contents": "Title: Deep Quality-Value (DQV) Learning Abstract: We introduce a novel Deep Reinforcement Learning (DRL) algorithm called Deep\nQuality-Value (DQV) Learning. DQV uses temporal-difference learning to train a\nValue neural network and uses this network for training a second Quality-value\nnetwork that learns to estimate state-action values. We first test DQV's update\nrules with Multilayer Perceptrons as function approximators on two classic RL\nproblems, and then extend DQV with the use of Deep Convolutional Neural\nNetworks, `Experience Replay' and `Target Neural Networks' for tackling four\ngames of the Atari Arcade Learning environment. Our results show that DQV\nlearns significantly faster and better than Deep Q-Learning and Double Deep\nQ-Learning, suggesting that our algorithm can potentially be a better\nperforming synchronous temporal difference algorithm than what is currently\npresent in DRL. \n\n"}
{"id": "1810.00440", "contents": "Title: Minimal Random Code Learning: Getting Bits Back from Compressed Model\n  Parameters Abstract: While deep neural networks are a highly successful model class, their large\nmemory footprint puts considerable strain on energy consumption, communication\nbandwidth, and storage requirements. Consequently, model size reduction has\nbecome an utmost goal in deep learning. A typical approach is to train a set of\ndeterministic weights, while applying certain techniques such as pruning and\nquantization, in order that the empirical weight distribution becomes amenable\nto Shannon-style coding schemes. However, as shown in this paper, relaxing\nweight determinism and using a full variational distribution over weights\nallows for more efficient coding schemes and consequently higher compression\nrates. In particular, following the classical bits-back argument, we encode the\nnetwork weights using a random sample, requiring only a number of bits\ncorresponding to the Kullback-Leibler divergence between the sampled\nvariational distribution and the encoding distribution. By imposing a\nconstraint on the Kullback-Leibler divergence, we are able to explicitly\ncontrol the compression rate, while optimizing the expected loss on the\ntraining set. The employed encoding scheme can be shown to be close to the\noptimal information-theoretical lower bound, with respect to the employed\nvariational family. Our method sets new state-of-the-art in neural network\ncompression, as it strictly dominates previous approaches in a Pareto sense: On\nthe benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, our approach yields the best\ntest performance for a fixed memory budget, and vice versa, it achieves the\nhighest compression rates for a fixed test performance. \n\n"}
{"id": "1810.01110", "contents": "Title: Link Prediction Adversarial Attack Abstract: Deep neural network has shown remarkable performance in solving computer\nvision and some graph evolved tasks, such as node classification and link\nprediction. However, the vulnerability of deep model has also been revealed by\ncarefully designed adversarial examples generated by various adversarial attack\nmethods. With the wider application of deep model in complex network analysis,\nin this paper we define and formulate the link prediction adversarial attack\nproblem and put forward a novel iterative gradient attack (IGA) based on the\ngradient information in trained graph auto-encoder (GAE). To our best\nknowledge, it is the first time link prediction adversarial attack problem is\ndefined and attack method is brought up. Not surprisingly, GAE was easily\nfooled by adversarial network with only a few links perturbed on the clean\nnetwork. By conducting comprehensive experiments on different real-world data\nsets, we can conclude that most deep model based and other state-of-art link\nprediction algorithms cannot escape the adversarial attack just like GAE. We\ncan benefit the attack as an efficient privacy protection tool from link\nprediction unknown violation, on the other hand, link prediction attack can be\na robustness evaluation metric for current link prediction algorithm in attack\ndefensibility. \n\n"}
{"id": "1810.02567", "contents": "Title: Online Learning to Rank with Features Abstract: We introduce a new model for online ranking in which the click probability\nfactors into an examination and attractiveness function and the attractiveness\nfunction is a linear function of a feature vector and an unknown parameter.\nOnly relatively mild assumptions are made on the examination function. A novel\nalgorithm for this setup is analysed, showing that the dependence on the number\nof items is replaced by a dependence on the dimension, allowing the new\nalgorithm to handle a large number of items. When reduced to the orthogonal\ncase, the regret of the algorithm improves on the state-of-the-art. \n\n"}
{"id": "1810.02814", "contents": "Title: Statistical Optimality of Interpolated Nearest Neighbor Algorithms Abstract: In the era of deep learning, understanding over-fitting phenomenon becomes\nincreasingly important. It is observed that carefully designed deep neural\nnetworks achieve small testing error even when the training error is close to\nzero. One possible explanation is that for many modern machine learning\nalgorithms, over-fitting can greatly reduce the estimation bias, while not\nincreasing the estimation variance too much. To illustrate the above idea, we\nprove that the proposed interpolated nearest neighbor algorithm achieves the\nminimax optimal rate in both regression and classification regimes, and observe\nthat they are empirically better than the traditional $k$ nearest neighbor\nmethod in some cases. \n\n"}
{"id": "1810.02894", "contents": "Title: Interval Estimation of Individual-Level Causal Effects Under Unobserved\n  Confounding Abstract: We study the problem of learning conditional average treatment effects (CATE)\nfrom observational data with unobserved confounders. The CATE function maps\nbaseline covariates to individual causal effect predictions and is key for\npersonalized assessments. Recent work has focused on how to learn CATE under\nunconfoundedness, i.e., when there are no unobserved confounders. Since CATE\nmay not be identified when unconfoundedness is violated, we develop a\nfunctional interval estimator that predicts bounds on the individual causal\neffects under realistic violations of unconfoundedness. Our estimator takes the\nform of a weighted kernel estimator with weights that vary adversarially. We\nprove that our estimator is sharp in that it converges exactly to the tightest\nbounds possible on CATE when there may be unobserved confounders. Further, we\nstudy personalized decision rules derived from our estimator and prove that\nthey achieve optimal minimax regret asymptotically. We assess our approach in a\nsimulation study as well as demonstrate its application in the case of hormone\nreplacement therapy by comparing conclusions from a real observational study\nand clinical trial. \n\n"}
{"id": "1810.03944", "contents": "Title: Transfer Metric Learning: Algorithms, Applications and Outlooks Abstract: Distance metric learning (DML) aims to find an appropriate way to reveal the\nunderlying data relationship. It is critical in many machine learning, pattern\nrecognition and data mining algorithms, and usually require large amount of\nlabel information (such as class labels or pair/triplet constraints) to achieve\nsatisfactory performance. However, the label information may be insufficient in\nreal-world applications due to the high-labeling cost, and DML may fail in this\ncase. Transfer metric learning (TML) is able to mitigate this issue for DML in\nthe domain of interest (target domain) by leveraging knowledge/information from\nother related domains (source domains). Although achieved a certain level of\ndevelopment, TML has limited success in various aspects such as selective\ntransfer, theoretical understanding, handling complex data, big data and\nextreme cases. In this survey, we present a systematic review of the TML\nliterature. In particular, we group TML into different categories according to\ndifferent settings and metric transfer strategies, such as direct metric\napproximation, subspace approximation, distance approximation, and distribution\napproximation. A summarization and insightful discussion of the various TML\napproaches and their applications will be presented. Finally, we indicate some\nchallenges and provide possible future directions. \n\n"}
{"id": "1810.04632", "contents": "Title: Non-linear process convolutions for multi-output Gaussian processes Abstract: The paper introduces a non-linear version of the process convolution\nformalism for building covariance functions for multi-output Gaussian\nprocesses. The non-linearity is introduced via Volterra series, one series per\neach output. We provide closed-form expressions for the mean function and the\ncovariance function of the approximated Gaussian process at the output of the\nVolterra series. The mean function and covariance function for the joint\nGaussian process are derived using formulae for the product moments of Gaussian\nvariables. We compare the performance of the non-linear model against the\nclassical process convolution approach in one synthetic dataset and two real\ndatasets. \n\n"}
{"id": "1810.04778", "contents": "Title: Offline Multi-Action Policy Learning: Generalization and Optimization Abstract: In many settings, a decision-maker wishes to learn a rule, or policy, that\nmaps from observable characteristics of an individual to an action. Examples\ninclude selecting offers, prices, advertisements, or emails to send to\nconsumers, as well as the problem of determining which medication to prescribe\nto a patient. While there is a growing body of literature devoted to this\nproblem, most existing results are focused on the case where data comes from a\nrandomized experiment, and further, there are only two possible actions, such\nas giving a drug to a patient or not. In this paper, we study the offline\nmulti-action policy learning problem with observational data and where the\npolicy may need to respect budget constraints or belong to a restricted policy\nclass such as decision trees. We build on the theory of efficient\nsemi-parametric inference in order to propose and implement a policy learning\nalgorithm that achieves asymptotically minimax-optimal regret. To the best of\nour knowledge, this is the first result of this type in the multi-action setup,\nand it provides a substantial performance improvement over the existing\nlearning algorithms. We then consider additional computational challenges that\narise in implementing our method for the case where the policy is restricted to\ntake the form of a decision tree. We propose two different approaches, one\nusing a mixed integer program formulation and the other using a tree-search\nbased algorithm. \n\n"}
{"id": "1810.05207", "contents": "Title: On Kernel Derivative Approximation with Random Fourier Features Abstract: Random Fourier features (RFF) represent one of the most popular and\nwide-spread techniques in machine learning to scale up kernel algorithms.\nDespite the numerous successful applications of RFFs, unfortunately, quite\nlittle is understood theoretically on their optimality and limitations of their\nperformance. Only recently, precise statistical-computational trade-offs have\nbeen established for RFFs in the approximation of kernel values, kernel ridge\nregression, kernel PCA and SVM classification. Our goal is to spark the\ninvestigation of optimality of RFF-based approximations in tasks involving not\nonly function values but derivatives, which naturally lead to optimization\nproblems with kernel derivatives. Particularly, in this paper, we focus on the\napproximation quality of RFFs for kernel derivatives and prove that the\nexisting finite-sample guarantees can be improved exponentially in terms of the\ndomain where they hold, using recent tools from unbounded empirical process\ntheory. Our result implies that the same approximation guarantee is attainable\nfor kernel derivatives using RFF as achieved for kernel values. \n\n"}
{"id": "1810.05471", "contents": "Title: Safe Grid Search with Optimal Complexity Abstract: Popular machine learning estimators involve regularization parameters that\ncan be challenging to tune, and standard strategies rely on grid search for\nthis task. In this paper, we revisit the techniques of approximating the\nregularization path up to predefined tolerance $\\epsilon$ in a unified\nframework and show that its complexity is $O(1/\\sqrt[d]{\\epsilon})$ for\nuniformly convex loss of order $d \\geq 2$ and $O(1/\\sqrt{\\epsilon})$ for\nGeneralized Self-Concordant functions. This framework encompasses least-squares\nbut also logistic regression, a case that as far as we know was not handled as\nprecisely in previous works. We leverage our technique to provide refined\nbounds on the validation error as well as a practical algorithm for\nhyperparameter tuning. The latter has global convergence guarantee when\ntargeting a prescribed accuracy on the validation set. Last but not least, our\napproach helps relieving the practitioner from the (often neglected) task of\nselecting a stopping criterion when optimizing over the training set: our\nmethod automatically calibrates this criterion based on the targeted accuracy\non the validation set. \n\n"}
{"id": "1810.05598", "contents": "Title: Tuning Fairness by Balancing Target Labels Abstract: The issue of fairness in machine learning models has recently attracted a lot\nof attention as ensuring it will ensure continued confidence of the general\npublic in the deployment of machine learning systems. We focus on mitigating\nthe harm incurred by a biased machine learning system that offers better\noutputs (e.g. loans, job interviews) for certain groups than for others. We\nshow that bias in the output can naturally be controlled in probabilistic\nmodels by introducing a latent target output. This formulation has several\nadvantages: first, it is a unified framework for several notions of group\nfairness such as Demographic Parity and Equality of Opportunity; second, it is\nexpressed as a marginalisation instead of a constrained problem; and third, it\nallows the encoding of our knowledge of what unbiased outputs should be.\nPractically, the second allows us to avoid unstable constrained optimisation\nprocedures and to reuse off-the-shelf toolboxes. The latter translates to the\nability to control the level of fairness by directly varying fairness target\nrates. In contrast, existing approaches rely on intermediate, arguably\nunintuitive, control parameters such as covariance thresholds. \n\n"}
{"id": "1810.05752", "contents": "Title: Global Convergence of EM Algorithm for Mixtures of Two Component Linear\n  Regression Abstract: The Expectation-Maximization algorithm is perhaps the most broadly used\nalgorithm for inference of latent variable problems. A theoretical\nunderstanding of its performance, however, largely remains lacking. Recent\nresults established that EM enjoys global convergence for Gaussian Mixture\nModels. For Mixed Linear Regression, however, only local convergence results\nhave been established, and those only for the high SNR regime. We show here\nthat EM converges for mixed linear regression with two components (it is known\nthat it may fail to converge for three or more), and moreover that this\nconvergence holds for random initialization. Our analysis reveals that EM\nexhibits very different behavior in Mixed Linear Regression from its behavior\nin Gaussian Mixture Models, and hence our proofs require the development of\nseveral new ideas. \n\n"}
{"id": "1810.05992", "contents": "Title: Convex Hull Approximation of Nearly Optimal Lasso Solutions Abstract: In an ordinary feature selection procedure, a set of important features is\nobtained by solving an optimization problem such as the Lasso regression\nproblem, and we expect that the obtained features explain the data well. In\nthis study, instead of the single optimal solution, we consider finding a set\nof diverse yet nearly optimal solutions. To this end, we formulate the problem\nas finding a small number of solutions such that the convex hull of these\nsolutions approximates the set of nearly optimal solutions. The proposed\nalgorithm consists of two steps: First, we randomly sample the extreme points\nof the set of nearly optimal solutions. Then, we select a small number of\npoints using a greedy algorithm. The experimental results indicate that the\nproposed algorithm can approximate the solution set well. The results also\nindicate that we can obtain Lasso solutions with a large diversity. \n\n"}
{"id": "1810.06687", "contents": "Title: To Kavanaugh or Not to Kavanaugh: That is the Polarizing Question Abstract: On October 6, 2018, the US Senate confirmed Brett Kavanaugh with the\nnarrowest margin for a successful confirmation since 1881 and where the\nsenators voted overwhelmingly along party lines. In this paper, we examine\nwhether the political polarization in the Senate is reflected among the general\npublic. To do so, we analyze the views of more than 128 thousand Twitter users.\nWe show that users supporting or opposing Kavanaugh's nomination were generally\nusing divergent hashtags, retweeting different Twitter accounts, and sharing\nlinks from different websites. We also examine characterestics of both groups. \n\n"}
{"id": "1810.06758", "contents": "Title: Discriminator Rejection Sampling Abstract: We propose a rejection sampling scheme using the discriminator of a GAN to\napproximately correct errors in the GAN generator distribution. We show that\nunder quite strict assumptions, this will allow us to recover the data\ndistribution exactly. We then examine where those strict assumptions break down\nand design a practical algorithm - called Discriminator Rejection Sampling\n(DRS) - that can be used on real data-sets. Finally, we demonstrate the\nefficacy of DRS on a mixture of Gaussians and on the SAGAN model,\nstate-of-the-art in the image generation task at the time of developing this\nwork. On ImageNet, we train an improved baseline that increases the Inception\nScore from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65\nto 14.79. We then use DRS to further improve on this baseline, improving the\nInception Score to 76.08 and the FID to 13.75. \n\n"}
{"id": "1810.07468", "contents": "Title: Hierarchical Methods of Moments Abstract: Spectral methods of moments provide a powerful tool for learning the\nparameters of latent variable models. Despite their theoretical appeal, the\napplicability of these methods to real data is still limited due to a lack of\nrobustness to model misspecification. In this paper we present a hierarchical\napproach to methods of moments to circumvent such limitations. Our method is\nbased on replacing the tensor decomposition step used in previous algorithms\nwith approximate joint diagonalization. Experiments on topic modeling show that\nour method outperforms previous tensor decomposition methods in terms of speed\nand model quality. \n\n"}
{"id": "1810.07913", "contents": "Title: Robust Sparse Reduced Rank Regression in High Dimensions Abstract: We propose robust sparse reduced rank regression for analyzing large and\ncomplex high-dimensional data with heavy-tailed random noise. The proposed\nmethod is based on a convex relaxation of a rank- and sparsity-constrained\nnon-convex optimization problem, which is then solved using the alternating\ndirection method of multipliers algorithm. We establish non-asymptotic\nestimation error bounds under both Frobenius and nuclear norms in the\nhigh-dimensional setting. This is a major contribution over existing results in\nreduced rank regression, which mainly focus on rank selection and prediction\nconsistency. Our theoretical results quantify the tradeoff between\nheavy-tailedness of the random noise and statistical bias. For random noise\nwith bounded $(1+\\delta)$th moment with $\\delta \\in (0,1)$, the rate of\nconvergence is a function of $\\delta$, and is slower than the sub-Gaussian-type\ndeviation bounds; for random noise with bounded second moment, we obtain a rate\nof convergence as if sub-Gaussian noise were assumed. Furthermore, the\ntransition between the two regimes is smooth. We illustrate the performance of\nthe proposed method via extensive numerical studies and a data application. \n\n"}
{"id": "1810.08033", "contents": "Title: Adaptivity of deep ReLU network for learning in Besov and mixed smooth\n  Besov spaces: optimal rate and curse of dimensionality Abstract: Deep learning has shown high performances in various types of tasks from\nvisual recognition to natural language processing, which indicates superior\nflexibility and adaptivity of deep learning. To understand this phenomenon\ntheoretically, we develop a new approximation and estimation error analysis of\ndeep learning with the ReLU activation for functions in a Besov space and its\nvariant with mixed smoothness. The Besov space is a considerably general\nfunction space including the Holder space and Sobolev space, and especially can\ncapture spatial inhomogeneity of smoothness. Through the analysis in the Besov\nspace, it is shown that deep learning can achieve the minimax optimal rate and\noutperform any non-adaptive (linear) estimator such as kernel ridge regression,\nwhich shows that deep learning has higher adaptivity to the spatial\ninhomogeneity of the target function than other estimators such as linear ones.\nIn addition to this, it is shown that deep learning can avoid the curse of\ndimensionality if the target function is in a mixed smooth Besov space. We also\nshow that the dependency of the convergence rate on the dimensionality is tight\ndue to its minimax optimality. These results support high adaptivity of deep\nlearning and its superior ability as a feature extractor. \n\n"}
{"id": "1810.08683", "contents": "Title: Taking Advantage of Multitask Learning for Fair Classification Abstract: A central goal of algorithmic fairness is to reduce bias in automated\ndecision making. An unavoidable tension exists between accuracy gains obtained\nby using sensitive information (e.g., gender or ethnic group) as part of a\nstatistical model, and any commitment to protect these characteristics. Often,\ndue to biases present in the data, using the sensitive information in the\nfunctional form of a classifier improves classification accuracy. In this paper\nwe show how it is possible to get the best of both worlds: optimize model\naccuracy and fairness without explicitly using the sensitive feature in the\nfunctional form of the model, thereby treating different individuals equally.\nOur method is based on two key ideas. On the one hand, we propose to use\nMultitask Learning (MTL), enhanced with fairness constraints, to jointly learn\ngroup specific classifiers that leverage information between sensitive groups.\nOn the other hand, since learning group specific models might not be permitted,\nwe propose to first predict the sensitive features by any learning method and\nthen to use the predicted sensitive feature to train MTL with fairness\nconstraints. This enables us to tackle fairness with a three-pronged approach,\nthat is, by increasing accuracy on each group, enforcing measures of fairness\nduring training, and protecting sensitive information during testing.\nExperimental results on two real datasets support our proposal, showing\nsubstantial improvements in both accuracy and fairness. \n\n"}
{"id": "1810.09781", "contents": "Title: A Social Network Analysis of Articles on Social Network Analysis Abstract: A collection of articles on the statistical modelling and inference of social\nnetworks is analysed in a network fashion. The references of these articles are\nused to construct a citation network data set, which is almost a directed\nacyclic graph because only existing articles can be cited. A mixed membership\nstochastic block model is then applied to this data set to soft cluster the\narticles. The results obtained from a Gibbs sampler give us insights into the\ninfluence and the categorisation of these articles. \n\n"}
{"id": "1810.09956", "contents": "Title: Social Status and Communication Behavior in an Evolving Social Network Abstract: The degree to which individuals can exert influence on propagation of\ninformation and opinion dynamics in online communities is highly dependent on\ntheir social status. Therefore, there is a high demand for identifying\ninfluential users in a community by predicting their social position in that\ncommunity. Moreover, understanding how people with various social status\nbehave, can shed light on the dynamics of interaction in social networks. In\nthis paper, I study an evolving online social network originated from an online\ncommunity for university students and I tackle the problem of forecasting\nusers' social status, represented as their PageRank, based on frequency of\nrecurring temporal sequences of observed behavior, i.e. behavioral motifs. I\nshow that individuals with different values of PageRank exhibit different\nbehavior even in early weeks since the online community's inception and it is\npossible to forecast future PageRank values given frequency of behavioral\nmotifs with high accuracy. \n\n"}
{"id": "1810.10053", "contents": "Title: Graph Laplacian mixture model Abstract: Graph learning methods have recently been receiving increasing interest as\nmeans to infer structure in datasets. Most of the recent approaches focus on\ndifferent relationships between a graph and data sample distributions, mostly\nin settings where all available data relate to the same graph. This is,\nhowever, not always the case, as data is often available in mixed form,\nyielding the need for methods that are able to cope with mixture data and learn\nmultiple graphs. We propose a novel generative model that represents a\ncollection of distinct data which naturally live on different graphs. We assume\nthe mapping of data to graphs is not known and investigate the problem of\njointly clustering a set of data and learning a graph for each of the clusters.\nExperiments demonstrate promising performance in data clustering and multiple\ngraph inference, and show desirable properties in terms of interpretability and\ncoping with high dimensionality on weather and traffic data, as well as digit\nclassification. \n\n"}
{"id": "1810.10775", "contents": "Title: Adversarially Robust Optimization with Gaussian Processes Abstract: In this paper, we consider the problem of Gaussian process (GP) optimization\nwith an added robustness requirement: The returned point may be perturbed by an\nadversary, and we require the function value to remain as high as possible even\nafter this perturbation. This problem is motivated by settings in which the\nunderlying functions during optimization and implementation stages are\ndifferent, or when one is interested in finding an entire region of good inputs\nrather than only a single point. We show that standard GP optimization\nalgorithms do not exhibit the desired robustness properties, and provide a\nnovel confidence-bound based algorithm StableOpt for this purpose. We\nrigorously establish the required number of samples for StableOpt to find a\nnear-optimal point, and we complement this guarantee with an\nalgorithm-independent lower bound. We experimentally demonstrate several\npotential applications of interest using real-world data sets, and we show that\nStableOpt consistently succeeds in finding a stable maximizer where several\nbaseline methods fail. \n\n"}
{"id": "1810.11155", "contents": "Title: Communication Efficient Parallel Algorithms for Optimization on\n  Manifolds Abstract: The last decade has witnessed an explosion in the development of models,\ntheory and computational algorithms for \"big data\" analysis. In particular,\ndistributed computing has served as a natural and dominating paradigm for\nstatistical inference. However, the existing literature on parallel inference\nalmost exclusively focuses on Euclidean data and parameters. While this\nassumption is valid for many applications, it is increasingly more common to\nencounter problems where the data or the parameters lie on a non-Euclidean\nspace, like a manifold for example. Our work aims to fill a critical gap in the\nliterature by generalizing parallel inference algorithms to optimization on\nmanifolds. We show that our proposed algorithm is both communication efficient\nand carries theoretical convergence guarantees. In addition, we demonstrate the\nperformance of our algorithm to the estimation of Fr\\'echet means on simulated\nspherical data and the low-rank matrix completion problem over Grassmann\nmanifolds applied to the Netflix prize data set. \n\n"}
{"id": "1810.11719", "contents": "Title: Algorithmic information distortions and incompressibility in uniform\n  multidimensional networks Abstract: This article presents a theoretical investigation of generalized encoded\nforms of networks in a uniform multidimensional space. First, we study encoded\nnetworks with (finite) arbitrary node dimensions (or aspects), such as time\ninstants or layers. In particular, we study these networks that are formalized\nin the form of multiaspect graphs. In the context of node-aligned non-uniform\n(or node-unaligned non-uniform and uniform) multidimensional spaces, previous\nresults has shown that, unlike classical graphs, the algorithmic information of\na multidimensional network is not in general dominated by the algorithmic\ninformation of the binary sequence that determines the presence or absence of\nedges. In the present work, first we demonstrate the existence of such\nalgorithmic information distortions for node-aligned uniform multidimensional\nnetworks. Secondly, we show that there are particular cases of infinite nesting\nfamilies of finite uniform multidimensional networks such that each member of\nthese families is incompressible. From these results, we also recover the\nnetwork topological properties and equivalences in irreducible information\ncontent of multidimensional networks in comparison to their isomorphic\nclassical graph counterpart in the previous literature. These results together\nestablish a universal algorithmic approach and set limitations and conditions\nfor irreducible information content analysis in comparing arbitrary networks\nwith a large number of dimensions, such as multilayer networks. \n\n"}
{"id": "1810.12233", "contents": "Title: Approximate Bayesian Computation via Population Monte Carlo and\n  Classification Abstract: Approximate Bayesian computation (ABC) methods can be used to sample from\nposterior distributions when the likelihood function is unavailable or\nintractable, as is often the case in biological systems. ABC methods suffer\nfrom inefficient particle proposals in high dimensions, and subjectivity in the\nchoice of summary statistics, discrepancy measure, and error tolerance.\nSequential Monte Carlo (SMC) methods have been combined with ABC to improve the\nefficiency of particle proposals, but suffer from subjectivity and require many\nsimulations from the likelihood function. Likelihood-Free Inference by Ratio\nEstimation (LFIRE) leverages classification to estimate the posterior density\ndirectly but does not explore the parameter space efficiently. This work\nproposes a classification approach that approximates population Monte Carlo\n(PMC), where model class probabilities from classification are used to update\nparticle weights. This approach, called Classification-PMC, blends adaptive\nproposals and classification, efficiently producing samples from the posterior\nwithout subjectivity. We show through a simulation study that\nClassification-PMC outperforms two state-of-the-art methods: ratio estimation\nand SMC ABC when it is computationally difficult to simulate from the\nlikelihood. \n\n"}
{"id": "1810.12273", "contents": "Title: Kalman Gradient Descent: Adaptive Variance Reduction in Stochastic\n  Optimization Abstract: We introduce Kalman Gradient Descent, a stochastic optimization algorithm\nthat uses Kalman filtering to adaptively reduce gradient variance in stochastic\ngradient descent by filtering the gradient estimates. We present both a\ntheoretical analysis of convergence in a non-convex setting and experimental\nresults which demonstrate improved performance on a variety of machine learning\nareas including neural networks and black box variational inference. We also\npresent a distributed version of our algorithm that enables large-dimensional\noptimization, and we extend our algorithm to SGD with momentum and RMSProp. \n\n"}
{"id": "1810.12361", "contents": "Title: Global Non-convex Optimization with Discretized Diffusions Abstract: An Euler discretization of the Langevin diffusion is known to converge to the\nglobal minimizers of certain convex and non-convex optimization problems. We\nshow that this property holds for any suitably smooth diffusion and that\ndifferent diffusions are suitable for optimizing different classes of convex\nand non-convex functions. This allows us to design diffusions suitable for\nglobally optimizing convex and non-convex functions not covered by the existing\nLangevin theory. Our non-asymptotic analysis delivers computable optimization\nand integration error bounds based on easily accessed properties of the\nobjective and chosen diffusion. Central to our approach are new explicit Stein\nfactor bounds on the solutions of Poisson equations. We complement these\nresults with improved optimization guarantees for targets other than the\nstandard Gibbs measure. \n\n"}
{"id": "1810.12881", "contents": "Title: Data Poisoning Attack against Unsupervised Node Embedding Methods Abstract: Unsupervised node embedding methods (e.g., DeepWalk, LINE, and node2vec) have\nattracted growing interests given their simplicity and effectiveness. However,\nalthough these methods have been proved effective in a variety of applications,\nnone of the existing work has analyzed the robustness of them. This could be\nvery risky if these methods are attacked by an adversarial party. In this\npaper, we take the task of link prediction as an example, which is one of the\nmost fundamental problems for graph analysis, and introduce a data positioning\nattack to node embedding methods. We give a complete characterization of\nattacker's utilities and present efficient solutions to adversarial attacks for\ntwo popular node embedding methods: DeepWalk and LINE. We evaluate our proposed\nattack model on multiple real-world graphs. Experimental results show that our\nproposed model can significantly affect the results of link prediction by\nslightly changing the graph structures (e.g., adding or removing a few edges).\nWe also show that our proposed model is very general and can be transferable\nacross different embedding methods. Finally, we conduct a case study on a\ncoauthor network to better understand our attack method. \n\n"}
{"id": "1810.13425", "contents": "Title: Understanding Deep Neural Networks through Input Uncertainties Abstract: Techniques for understanding the functioning of complex machine learning\nmodels are becoming increasingly popular, not only to improve the validation\nprocess, but also to extract new insights about the data via exploratory\nanalysis. Though a large class of such tools currently exists, most assume that\npredictions are point estimates and use a sensitivity analysis of these\nestimates to interpret the model. Using lightweight probabilistic networks we\nshow how including prediction uncertainties in the sensitivity analysis leads\nto: (i) more robust and generalizable models; and (ii) a new approach for model\ninterpretation through uncertainty decomposition. In particular, we introduce a\nnew regularization that takes both the mean and variance of a prediction into\naccount and demonstrate that the resulting networks provide improved\ngeneralization to unseen data. Furthermore, we propose a new technique to\nexplain prediction uncertainties through uncertainties in the input domain,\nthus providing new ways to validate and interpret deep learning models. \n\n"}
{"id": "1811.00542", "contents": "Title: Pymc-learn: Practical Probabilistic Machine Learning in Python Abstract: $\\textit{Pymc-learn}$ is a Python package providing a variety of\nstate-of-the-art probabilistic models for supervised and unsupervised machine\nlearning. It is inspired by $\\textit{scikit-learn}$ and focuses on bringing\nprobabilistic machine learning to non-specialists. It uses a general-purpose\nhigh-level language that mimics $\\textit{scikit-learn}$. Emphasis is put on\nease of use, productivity, flexibility, performance, documentation, and an API\nconsistent with $\\textit{scikit-learn}$. It depends on $\\textit{scikit-learn}$\nand $\\textit{pymc3}$ and is distributed under the new BSD-3 license,\nencouraging its use in both academia and industry. Source code, binaries, and\ndocumentation are available on http://github.com/pymc-learn/pymc-learn. \n\n"}
{"id": "1811.00683", "contents": "Title: Quasi-random sampling for multivariate distributions via generative\n  neural networks Abstract: Generative moment matching networks (GMMNs) are introduced for generating\nquasi-random samples from multivariate models with any underlying copula in\norder to compute estimates under variance reduction. So far, quasi-random\nsampling for multivariate distributions required a careful design, exploiting\nspecific properties (such as conditional distributions) of the implied\nparametric copula or the underlying quasi-Monte Carlo (QMC) point set, and was\nonly tractable for a small number of models. Utilizing GMMNs allows one to\nconstruct quasi-random samples for a much larger variety of multivariate\ndistributions without such restrictions, including empirical ones from real\ndata with dependence structures not well captured by parametric copulas. Once\ntrained on pseudo-random samples from a parametric model or on real data, these\nneural networks only require a multivariate standard uniform randomized QMC\npoint set as input and are thus fast in estimating expectations of interest\nunder dependence with variance reduction. Numerical examples are considered to\ndemonstrate the approach, including applications inspired by risk management\npractice. All results are reproducible with the demos GMMN_QMC_paper,\nGMMN_QMC_data and GMMN_QMC_timings as part of the R package gnn. \n\n"}
{"id": "1811.01179", "contents": "Title: Large-scale Heteroscedastic Regression via Gaussian Process Abstract: Heteroscedastic regression considering the varying noises among observations\nhas many applications in the fields like machine learning and statistics. Here\nwe focus on the heteroscedastic Gaussian process (HGP) regression which\nintegrates the latent function and the noise function together in a unified\nnon-parametric Bayesian framework. Though showing remarkable performance, HGP\nsuffers from the cubic time complexity, which strictly limits its application\nto big data. To improve the scalability, we first develop a variational sparse\ninference algorithm, named VSHGP, to handle large-scale datasets. Furthermore,\ntwo variants are developed to improve the scalability and capability of VSHGP.\nThe first is stochastic VSHGP (SVSHGP) which derives a factorized evidence\nlower bound, thus enhancing efficient stochastic variational inference. The\nsecond is distributed VSHGP (DVSHGP) which (i) follows the Bayesian committee\nmachine formalism to distribute computations over multiple local VSHGP experts\nwith many inducing points; and (ii) adopts hybrid parameters for experts to\nguard against over-fitting and capture local variety. The superiority of DVSHGP\nand SVSHGP as compared to existing scalable heteroscedastic/homoscedastic GPs\nis then extensively verified on various datasets. \n\n"}
{"id": "1811.02033", "contents": "Title: Physics-Informed Generative Adversarial Networks for Stochastic\n  Differential Equations Abstract: We developed a new class of physics-informed generative adversarial networks\n(PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic\nproblems based on a limited number of scattered measurements. Unlike standard\nGANs relying only on data for training, here we encoded into the architecture\nof GANs the governing physical laws in the form of stochastic differential\nequations (SDEs) using automatic differentiation. In particular, we applied\nWasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability\ncompared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian\nprocesses of different correlation lengths based on data realizations collected\nfrom simultaneous reads at sparsely placed sensors. We obtained good\napproximation of the generated stochastic processes to the target ones even for\na mismatch between the input noise dimensionality and the effective\ndimensionality of the target stochastic processes. We also studied the\noverfitting issue for both the discriminator and generator, and we found that\noverfitting occurs also in the generator in addition to the discriminator as\npreviously reported. Subsequently, we considered the solution of elliptic SDEs\nrequiring approximations of three stochastic processes, namely the solution,\nthe forcing, and the diffusion coefficient. We used three generators for the\nPI-GANs, two of them were feed forward deep neural networks (DNNs) while the\nother one was the neural network induced by the SDE. Depending on the data, we\nemployed one or multiple feed forward DNNs as the discriminators in PI-GANs.\nHere, we have demonstrated the accuracy and effectiveness of PI-GANs in solving\nSDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high\ndimensional problems given more sensor data with low-polynomial growth in\ncomputational cost. \n\n"}
{"id": "1811.03270", "contents": "Title: An Optimal Transport View on Generalization Abstract: We derive upper bounds on the generalization error of learning algorithms\nbased on their \\emph{algorithmic transport cost}: the expected Wasserstein\ndistance between the output hypothesis and the output hypothesis conditioned on\nan input example. The bounds provide a novel approach to study the\ngeneralization of learning algorithms from an optimal transport view and impose\nless constraints on the loss function, such as sub-gaussian or bounded. We\nfurther provide several upper bounds on the algorithmic transport cost in terms\nof total variation distance, relative entropy (or KL-divergence), and VC\ndimension, thus further bridging optimal transport theory and information\ntheory with statistical learning theory. Moreover, we also study different\nconditions for loss functions under which the generalization error of a\nlearning algorithm can be upper bounded by different probability metrics\nbetween distributions relating to the output hypothesis and/or the input data.\nFinally, under our established framework, we analyze the generalization in deep\nlearning and conclude that the generalization error in deep neural networks\n(DNNs) decreases exponentially to zero as the number of layers increases. Our\nanalyses of generalization error in deep learning mainly exploit the\nhierarchical structure in DNNs and the contraction property of $f$-divergence,\nwhich may be of independent interest in analyzing other learning models with\nhierarchical structure. \n\n"}
{"id": "1811.03862", "contents": "Title: Targeting Solutions in Bayesian Multi-Objective Optimization: Sequential\n  and Batch Versions Abstract: Multi-objective optimization aims at finding trade-off solutions to\nconflicting objectives. These constitute the Pareto optimal set. In the context\nof expensive-to-evaluate functions, it is impossible and often non-informative\nto look for the entire set. As an end-user would typically prefer a certain\npart of the objective space, we modify the Bayesian multi-objective\noptimization algorithm which uses Gaussian Processes to maximize the Expected\nHypervolume Improvement, to focus the search in the preferred region. The\ncumulated effects of the Gaussian Processes and the targeting strategy lead to\na particularly efficient convergence to the desired part of the Pareto set. To\ntake advantage of parallel computing, a multi-point extension of the targeting\ncriterion is proposed and analyzed. \n\n"}
{"id": "1811.04104", "contents": "Title: Deep Learning Super-Diffusion in Multiplex Networks Abstract: Complex network theory has shown success in understanding the emergent and\ncollective behavior of complex systems [1]. Many real-world complex systems\nwere recently discovered to be more accurately modeled as multiplex networks\n[2-6]---in which each interaction type is mapped to its own network layer;\ne.g.~multi-layer transportation networks, coupled social networks, metabolic\nand regulatory networks, etc. A salient physical phenomena emerging from\nmultiplexity is super-diffusion: exhibited by an accelerated diffusion admitted\nby the multi-layer structure as compared to any single layer. Theoretically\nsuper-diffusion was only known to be predicted using the spectral gap of the\nfull Laplacian of a multiplex network and its interacting layers. Here we turn\nto machine learning which has developed techniques to recognize, classify, and\ncharacterize complex sets of data. We show that modern machine learning\narchitectures, such as fully connected and convolutional neural networks, can\nclassify and predict the presence of super-diffusion in multiplex networks with\n94.12\\% accuracy. Such predictions can be done {\\it in situ}, without the need\nto determine spectral properties of a network. \n\n"}
{"id": "1811.04175", "contents": "Title: CED: Credible Early Detection of Social Media Rumors Abstract: Rumors spread dramatically fast through online social media services, and\npeople are exploring methods to detect rumors automatically. Existing methods\ntypically learn semantic representations of all reposts to a rumor candidate\nfor prediction. However, it is crucial to efficiently detect rumors as early as\npossible before they cause severe social disruption, which has not been well\naddressed by previous works. In this paper, we present a novel early rumor\ndetection model, Credible Early Detection (CED). By regarding all reposts to a\nrumor candidate as a sequence, the proposed model will seek an early\npoint-in-time for making a credible prediction. We conduct experiments on three\nreal-world datasets, and the results demonstrate that our proposed model can\nremarkably reduce the time span for prediction by more than 85%, with better\naccuracy performance than all state-of-the-art baselines. \n\n"}
{"id": "1811.04576", "contents": "Title: Estimation of Dimensions Contributing to Detected Anomalies with\n  Variational Autoencoders Abstract: Anomaly detection using dimensionality reduction has been an essential\ntechnique for monitoring multidimensional data. Although deep learning-based\nmethods have been well studied for their remarkable detection performance,\ntheir interpretability is still a problem. In this paper, we propose a novel\nalgorithm for estimating the dimensions contributing to the detected anomalies\nby using variational autoencoders (VAEs). Our algorithm is based on an\napproximative probabilistic model that considers the existence of anomalies in\nthe data, and by maximizing the log-likelihood, we estimate which dimensions\ncontribute to determining data as an anomaly. The experiments results with\nbenchmark datasets show that our algorithm extracts the contributing dimensions\nmore accurately than baseline methods. \n\n"}
{"id": "1811.04624", "contents": "Title: Importance Weighted Evolution Strategies Abstract: Evolution Strategies (ES) emerged as a scalable alternative to popular\nReinforcement Learning (RL) techniques, providing an almost perfect speedup\nwhen distributed across hundreds of CPU cores thanks to a reduced communication\noverhead. Despite providing large improvements in wall-clock time, ES is data\ninefficient when compared to competing RL methods. One of the main causes of\nsuch inefficiency is the collection of large batches of experience, which are\ndiscarded after each policy update. In this work, we study how to perform more\nthan one update per batch of experience by means of Importance Sampling while\npreserving the scalability of the original method. The proposed method,\nImportance Weighted Evolution Strategies (IW-ES), shows promising results and\nis a first step towards designing efficient ES algorithms. \n\n"}
{"id": "1811.04646", "contents": "Title: Global sensitivity analysis for optimization with variable selection Abstract: The optimization of high dimensional functions is a key issue in engineering\nproblems but it frequently comes at a cost that is not acceptable since it\nusually involves a complex and expensive computer code. Engineers often\novercome this limitation by first identifying which parameters drive the most\nthe function variations: non-influential variables are set to a fixed value and\nthe optimization procedure is carried out with the remaining influential\nvariables. Such variable selection is performed through influence measures that\nare meaningful for regression problems. However it does not account for the\nspecific structure of optimization problems where we would like to identify\nwhich variables most lead to constraints satisfaction and low values of the\nobjective function. In this paper, we propose a new sensitivity analysis that\naccounts for the specific aspects of optimization problems. In particular, we\nintroduce an influence measure based on the Hilbert-Schmidt Independence\nCriterion to characterize whether a design variable matters to reach low values\nof the objective function and to satisfy the constraints. This sensitivity\nmeasure makes it possible to sort the inputs and reduce the problem dimension.\nWe compare a random and a greedy strategies to set the values of the\nnon-influential variables before conducting a local optimization. Applications\nto several test-cases show that this variable selection and the greedy strategy\nsignificantly reduce the number of function evaluations at a limited cost in\nterms of solution performance. \n\n"}
{"id": "1811.04655", "contents": "Title: Not Just Depressed: Bipolar Disorder Prediction on Reddit Abstract: Bipolar disorder, an illness characterized by manic and depressive episodes,\naffects more than 60 million people worldwide. We present a preliminary study\non bipolar disorder prediction from user-generated text on Reddit, which relies\non users' self-reported labels. Our benchmark classifiers for bipolar disorder\nprediction outperform the baselines and reach accuracy and F1-scores of above\n86%. Feature analysis shows interesting differences in language use between\nusers with bipolar disorders and the control group, including differences in\nthe use of emotion-expressive words. \n\n"}
{"id": "1811.05232", "contents": "Title: Theoretical Analysis of Adversarial Learning: A Minimax Approach Abstract: Here we propose a general theoretical method for analyzing the risk bound in\nthe presence of adversaries. Specifically, we try to fit the adversarial\nlearning problem into the minimax framework. We first show that the original\nadversarial learning problem can be reduced to a minimax statistical learning\nproblem by introducing a transport map between distributions. Then, we prove a\nnew risk bound for this minimax problem in terms of covering numbers under a\nweak version of Lipschitz condition. Our method can be applied to multi-class\nclassification problems and commonly used loss functions such as the hinge and\nramp losses. As some illustrative examples, we derive the adversarial risk\nbounds for SVMs, deep neural networks, and PCA, and our bounds have two\ndata-dependent terms, which can be optimized for achieving adversarial\nrobustness. \n\n"}
{"id": "1811.05614", "contents": "Title: SepNE: Bringing Separability to Network Embedding Abstract: Many successful methods have been proposed for learning low dimensional\nrepresentations on large-scale networks, while almost all existing methods are\ndesigned in inseparable processes, learning embeddings for entire networks even\nwhen only a small proportion of nodes are of interest. This leads to great\ninconvenience, especially on super-large or dynamic networks, where these\nmethods become almost impossible to implement. In this paper, we formalize the\nproblem of separated matrix factorization, based on which we elaborate a novel\nobjective function that preserves both local and global information. We further\npropose SepNE, a simple and flexible network embedding algorithm which\nindependently learns representations for different subsets of nodes in\nseparated processes. By implementing separability, our algorithm reduces the\nredundant efforts to embed irrelevant nodes, yielding scalability to\nsuper-large networks, automatic implementation in distributed learning and\nfurther adaptations. We demonstrate the effectiveness of this approach on\nseveral real-world networks with different scales and subjects. With comparable\naccuracy, our approach significantly outperforms state-of-the-art baselines in\nrunning times on large networks. \n\n"}
{"id": "1811.05711", "contents": "Title: From Free Text to Clusters of Content in Health Records: An Unsupervised\n  Graph Partitioning Approach Abstract: Electronic Healthcare records contain large volumes of unstructured data in\ndifferent forms. Free text constitutes a large portion of such data, yet this\nsource of richly detailed information often remains under-used in practice\nbecause of a lack of suitable methodologies to extract interpretable content in\na timely manner. Here we apply network-theoretical tools to the analysis of\nfree text in Hospital Patient Incident reports in the English National Health\nService, to find clusters of reports in an unsupervised manner and at different\nlevels of resolution based directly on the free text descriptions contained\nwithin them. To do so, we combine recently developed deep neural network\ntext-embedding methodologies based on paragraph vectors with multi-scale Markov\nStability community detection applied to a similarity graph of documents\nobtained from sparsified text vector similarities. We showcase the approach\nwith the analysis of incident reports submitted in Imperial College Healthcare\nNHS Trust, London. The multiscale community structure reveals levels of meaning\nwith different resolution in the topics of the dataset, as shown by relevant\ndescriptive terms extracted from the groups of records, as well as by comparing\na posteriori against hand-coded categories assigned by healthcare personnel.\nOur content communities exhibit good correspondence with well-defined\nhand-coded categories, yet our results also provide further medical detail in\ncertain areas as well as revealing complementary descriptors of incidents\nbeyond the external classification. We also discuss how the method can be used\nto monitor reports over time and across different healthcare providers, and to\ndetect emerging trends that fall outside of pre-existing categories. \n\n"}
{"id": "1811.05868", "contents": "Title: Pitfalls of Graph Neural Network Evaluation Abstract: Semi-supervised node classification in graphs is a fundamental problem in\ngraph mining, and the recently proposed graph neural networks (GNNs) have\nachieved unparalleled results on this task. Due to their massive success, GNNs\nhave attracted a lot of attention, and many novel architectures have been put\nforward. In this paper we show that existing evaluation strategies for GNN\nmodels have serious shortcomings. We show that using the same\ntrain/validation/test splits of the same datasets, as well as making\nsignificant changes to the training procedure (e.g. early stopping criteria)\nprecludes a fair comparison of different architectures. We perform a thorough\nempirical evaluation of four prominent GNN models and show that considering\ndifferent splits of the data leads to dramatically different rankings of\nmodels. Even more importantly, our findings suggest that simpler GNN\narchitectures are able to outperform the more sophisticated ones if the\nhyperparameters and the training procedure are tuned fairly for all models. \n\n"}
{"id": "1811.05932", "contents": "Title: Streaming Network Embedding through Local Actions Abstract: Recently, considerable research attention has been paid to network embedding,\na popular approach to construct feature vectors of vertices. Due to the curse\nof dimensionality and sparsity in graphical datasets, this approach has become\nindispensable for machine learning tasks over large networks. The majority of\nexisting literature has considered this technique under the assumption that the\nnetwork is static. However, networks in many applications, nodes and edges\naccrue to a growing network as a streaming. A small number of very recent\nresults have addressed the problem of embedding for dynamic networks. However,\nthey either rely on knowledge of vertex attributes, suffer high-time complexity\nor need to be re-trained without closed-form expression. Thus the approach of\nadapting the existing methods to the streaming environment faces non-trivial\ntechnical challenges.\n  These challenges motivate developing new approaches to the problems of\nstreaming network embedding. In this paper, We propose a new framework that is\nable to generate latent features for new vertices with high efficiency and low\ncomplexity under specified iteration rounds. We formulate a constrained\noptimization problem for the modification of the representation resulting from\na stream arrival. We show this problem has no closed-form solution and instead\ndevelop an online approximation solution. Our solution follows three steps: (1)\nidentify vertices affected by new vertices, (2) generate latent features for\nnew vertices, and (3) update the latent features of the most affected vertices.\nThe generated representations are provably feasible and not far from the\noptimal ones in terms of expectation. Multi-class classification and clustering\non five real-world networks demonstrate that our model can efficiently update\nvertex representations and simultaneously achieve comparable or even better\nperformance. \n\n"}
{"id": "1811.06055", "contents": "Title: Minimax Rates in Network Analysis: Graphon Estimation, Community\n  Detection and Hypothesis Testing Abstract: This paper surveys some recent developments in fundamental limits and optimal\nalgorithms for network analysis. We focus on minimax optimal rates in three\nfundamental problems of network analysis: graphon estimation, community\ndetection, and hypothesis testing. For each problem, we review state-of-the-art\nresults in the literature followed by general principles behind the optimal\nprocedures that lead to minimax estimation and testing. This allows us to\nconnect problems in network analysis to other statistical inference problems\nfrom a general perspective. \n\n"}
{"id": "1811.06693", "contents": "Title: Exploring Media Bias and Toxicity in South Asian Political Discourse Abstract: Media outlets and political campaigners recognise social media as a means for\nwidely disseminating news and opinions. In particular, Twitter is used by\npolitical groups all over the world to spread political messages, engage their\nsupporters, drive election campaigns, and challenge their critics. Further,\nnews agencies, many of which aim to give an impression of balance, are often of\na particular political persuasion which is reflected in the content they\nproduce. Driven by the potential for political and media organisations to\ninfluence public opinion, our aim is to quantify the nature of political\ndiscourse by these organisations through their use of social media. In this\nstudy, we analyse the sentiments, toxicity, and bias exhibited by the most\nprominent Pakistani and Indian political parties and media houses, and the\npattern by which these political parties utilise Twitter. We found that media\nbias and toxicity exist in the political discourse of these two developing\nnations. \n\n"}
{"id": "1811.07174", "contents": "Title: Link Prediction in Dynamic Graphs for Recommendation Abstract: Recent advances in employing neural networks on graph domains helped push the\nstate of the art in link prediction tasks, particularly in recommendation\nservices. However, the use of temporal contextual information, often modeled as\ndynamic graphs that encode the evolution of user-item relationships over time,\nhas been overlooked in link prediction problems. In this paper, we consider the\nhypothesis that leveraging such information enables models to make better\npredictions, proposing a new neural network approach for this. Our experiments,\nperformed on the widely used ML-100k and ML-1M datasets, show that our approach\nproduces better predictions in scenarios where the pattern of user-item\nrelationships change over time. In addition, they suggest that existing\napproaches are significantly impacted by those changes. \n\n"}
{"id": "1811.07957", "contents": "Title: Model change detection with application to machine learning Abstract: Model change detection is studied, in which there are two sets of samples\nthat are independently and identically distributed (i.i.d.) according to a\npre-change probabilistic model with parameter $\\theta$, and a post-change model\nwith parameter $\\theta'$, respectively. The goal is to detect whether the\nchange in the model is significant, i.e., whether the difference between the\npre-change parameter and the post-change parameter $\\|\\theta-\\theta'\\|_2$ is\nlarger than a pre-determined threshold $\\rho$. The problem is considered in a\nNeyman-Pearson setting, where the goal is to maximize the probability of\ndetection under a false alarm constraint. Since the generalized likelihood\nratio test (GLRT) is difficult to compute in this problem, we construct an\nempirical difference test (EDT), which approximates the GLRT and has low\ncomputational complexity. Moreover, we provide an approximation method to set\nthe threshold of the EDT to meet the false alarm constraint. Experiments with\nlinear regression and logistic regression are conducted to validate the\nproposed algorithms. \n\n"}
{"id": "1811.08790", "contents": "Title: Learning Quadratic Games on Networks Abstract: Individuals, or organizations, cooperate with or compete against one another\nin a wide range of practical situations. Such strategic interactions are often\nmodeled as games played on networks, where an individual's payoff depends not\nonly on her action but also on that of her neighbors. The current literature\nhas largely focused on analyzing the characteristics of network games in the\nscenario where the structure of the network, which is represented by a graph,\nis known beforehand. It is often the case, however, that the actions of the\nplayers are readily observable while the underlying interaction network remains\nhidden. In this paper, we propose two novel frameworks for learning, from the\nobservations on individual actions, network games with linear-quadratic\npayoffs, and in particular, the structure of the interaction network. Our\nframeworks are based on the Nash equilibrium of such games and involve solving\na joint optimization problem for the graph structure and the individual\nmarginal benefits. Both synthetic and real-world experiments demonstrate the\neffectiveness of the proposed frameworks, which have theoretical as well as\npractical implications for understanding strategic interactions in a network\nenvironment. \n\n"}
{"id": "1811.08929", "contents": "Title: Self-Adversarially Learned Bayesian Sampling Abstract: Scalable Bayesian sampling is playing an important role in modern machine\nlearning, especially in the fast-developed unsupervised-(deep)-learning models.\nWhile tremendous progresses have been achieved via scalable Bayesian sampling\nsuch as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient\ndescent (SVGD), the generated samples are typically highly correlated.\nMoreover, their sample-generation processes are often criticized to be\ninefficient. In this paper, we propose a novel self-adversarial learning\nframework that automatically learns a conditional generator to mimic the\nbehavior of a Markov kernel (transition kernel). High-quality samples can be\nefficiently generated by direct forward passes though a learned generator. Most\nimportantly, the learning process adopts a self-learning paradigm, requiring no\ninformation on existing Markov kernels, e.g., knowledge of how to draw samples\nfrom them. Specifically, our framework learns to use current samples, either\nfrom the generator or pre-provided training data, to update the generator such\nthat the generated samples progressively approach a target distribution, thus\nit is called self-learning. Experiments on both synthetic and real datasets\nverify advantages of our framework, outperforming related methods in terms of\nboth sampling efficiency and sample quality. \n\n"}
{"id": "1811.09702", "contents": "Title: Homogeneity-Based Transmissive Process to Model True and False News in\n  Social Networks Abstract: An overwhelming number of true and false news stories are posted and shared\nin social networks, and users diffuse the stories based on multiple factors.\nDiffusion of news stories from one user to another depends not only on the\nstories' content and the genuineness but also on the alignment of the topical\ninterests between the users. In this paper, we propose a novel Bayesian\nnonparametric model that incorporates homogeneity of news stories as the key\ncomponent that regulates the topical similarity between the posting and sharing\nusers' topical interests. Our model extends hierarchical Dirichlet process to\nmodel the topics of the news stories and incorporates Bayesian Gaussian process\nlatent variable model to discover the homogeneity values. We train our model on\na real-world social network dataset and find homogeneity values of news stories\nthat strongly relate to their labels of genuineness and their contents.\nFinally, we show that the supervised version of our model predicts the labels\nof news stories better than the state-of-the-art neural network and Bayesian\nmodels. \n\n"}
{"id": "1811.10734", "contents": "Title: DynamicGEM: A Library for Dynamic Graph Embedding Methods Abstract: DynamicGEM is an open-source Python library for learning node representations\nof dynamic graphs. It consists of state-of-the-art algorithms for defining\nembeddings of nodes whose connections evolve over time. The library also\ncontains the evaluation framework for four downstream tasks on the network:\ngraph reconstruction, static and temporal link prediction, node classification,\nand temporal visualization. We have implemented various metrics to evaluate the\nstate-of-the-art methods, and examples of evolving networks from various\ndomains. We have easy-to-use functions to call and evaluate the methods and\nhave extensive usage documentation. Furthermore, DynamicGEM provides a template\nto add new algorithms with ease to facilitate further research on the topic. \n\n"}
{"id": "1811.11083", "contents": "Title: Generative Adversarial Network Training is a Continual Learning Problem Abstract: Generative Adversarial Networks (GANs) have proven to be a powerful framework\nfor learning to draw samples from complex distributions. However, GANs are also\nnotoriously difficult to train, with mode collapse and oscillations a common\nproblem. We hypothesize that this is at least in part due to the evolution of\nthe generator distribution and the catastrophic forgetting tendency of neural\nnetworks, which leads to the discriminator losing the ability to remember\nsynthesized samples from previous instantiations of the generator. Recognizing\nthis, our contributions are twofold. First, we show that GAN training makes for\na more interesting and realistic benchmark for continual learning methods\nevaluation than some of the more canonical datasets. Second, we propose\nleveraging continual learning techniques to augment the discriminator,\npreserving its ability to recognize previous generator samples. We show that\nthe resulting methods add only a light amount of computation, involve minimal\nchanges to the model, and result in better overall performance on the examined\nimage and text generation tasks. \n\n"}
{"id": "1811.11152", "contents": "Title: Knots in random neural networks Abstract: The weights of a neural network are typically initialized at random, and one\ncan think of the functions produced by such a network as having been generated\nby a prior over some function space. Studying random networks, then, is useful\nfor a Bayesian understanding of the network evolution in early stages of\ntraining. In particular, one can investigate why neural networks with huge\nnumbers of parameters do not immediately overfit. We analyze the properties of\nrandom scalar-input feed-forward rectified linear unit architectures, which are\nrandom linear splines. With weights and biases sampled from certain common\ndistributions, empirical tests show that the number of knots in the spline\nproduced by the network is equal to the number of neurons, to very close\napproximation. We describe our progress towards a completely analytic\nexplanation of this phenomenon. In particular, we show that random single-layer\nneural networks are equivalent to integrated random walks with variable step\nsizes. That each neuron produces one knot on average is equivalent to the\nassociated integrated random walk having one zero crossing on average. We\nexplore how properties of the integrated random walk, including the step sizes\nand initial conditions, affect the number of crossings. The number of knots in\nrandom neural networks can be related to the behavior of extreme learning\nmachines, but it also establishes a prior preventing optimizers from\nimmediately overfitting to noisy training data. \n\n"}
{"id": "1811.11368", "contents": "Title: First-order Newton-type Estimator for Distributed Estimation and\n  Inference Abstract: This paper studies distributed estimation and inference for a general\nstatistical problem with a convex loss that could be non-differentiable. For\nthe purpose of efficient computation, we restrict ourselves to stochastic\nfirst-order optimization, which enjoys low per-iteration complexity. To\nmotivate the proposed method, we first investigate the theoretical properties\nof a straightforward Divide-and-Conquer Stochastic Gradient Descent (DC-SGD)\napproach. Our theory shows that there is a restriction on the number of\nmachines and this restriction becomes more stringent when the dimension $p$ is\nlarge. To overcome this limitation, this paper proposes a new multi-round\ndistributed estimation procedure that approximates the Newton step only using\nstochastic subgradient. The key component in our method is the proposal of a\ncomputationally efficient estimator of $\\Sigma^{-1} w$, where $\\Sigma$ is the\npopulation Hessian matrix and $w$ is any given vector. Instead of estimating\n$\\Sigma$ (or $\\Sigma^{-1}$) that usually requires the second-order\ndifferentiability of the loss, the proposed First-Order Newton-type Estimator\n(FONE) directly estimates the vector of interest $\\Sigma^{-1} w$ as a whole and\nis applicable to non-differentiable losses. Our estimator also facilitates the\ninference for the empirical risk minimizer. It turns out that the key term in\nthe limiting covariance has the form of $\\Sigma^{-1} w$, which can be estimated\nby FONE. \n\n"}
{"id": "1811.12159", "contents": "Title: Systematic Biases in Link Prediction: comparing heuristic and graph\n  embedding based methods Abstract: Link prediction is a popular research topic in network analysis. In the last\nfew years, new techniques based on graph embedding have emerged as a powerful\nalternative to heuristics. In this article, we study the problem of systematic\nbiases in the prediction, and show that some methods based on graph embedding\noffer less biased results than those based on heuristics, despite reaching\nlower scores according to usual quality scores. We discuss the relevance of\nthis finding in the context of the filter bubble problem and the algorithmic\nfairness of recommender systems. \n\n"}
{"id": "1811.12386", "contents": "Title: Tree-Structured Recurrent Switching Linear Dynamical Systems for\n  Multi-Scale Modeling Abstract: Many real-world systems studied are governed by complex, nonlinear dynamics.\nBy modeling these dynamics, we can gain insight into how these systems work,\nmake predictions about how they will behave, and develop strategies for\ncontrolling them. While there are many methods for modeling nonlinear dynamical\nsystems, existing techniques face a trade off between offering interpretable\ndescriptions and making accurate predictions. Here, we develop a class of\nmodels that aims to achieve both simultaneously, smoothly interpolating between\nsimple descriptions and more complex, yet also more accurate models. Our\nprobabilistic model achieves this multi-scale property through a hierarchy of\nlocally linear dynamics that jointly approximate global nonlinear dynamics. We\ncall it the tree-structured recurrent switching linear dynamical system. To fit\nthis model, we present a fully-Bayesian sampling procedure using Polya-Gamma\ndata augmentation to allow for fast and conjugate Gibbs sampling. Through a\nvariety of synthetic and real examples, we show how these models outperform\nexisting methods in both interpretability and predictive capability. \n\n"}
{"id": "1812.00071", "contents": "Title: Stochastic Gradient MCMC with Repulsive Forces Abstract: We propose a unifying view of two different Bayesian inference algorithms,\nStochastic Gradient Markov Chain Monte Carlo (SG-MCMC) and Stein Variational\nGradient Descent (SVGD), leading to improved and efficient novel sampling\nschemes. We show that SVGD combined with a noise term can be framed as a\nmultiple chain SG-MCMC method. Instead of treating each parallel chain\nindependently from others, our proposed algorithm implements a repulsive force\nbetween particles, avoiding collapse and facilitating a better exploration of\nthe parameter space. We also show how the addition of this noise term is\nnecessary to obtain a valid SG-MCMC sampler, a significant difference with\nSVGD. Experiments with both synthetic distributions and real datasets\nillustrate the benefits of the proposed scheme. \n\n"}
{"id": "1812.02356", "contents": "Title: dynnode2vec: Scalable Dynamic Network Embedding Abstract: Network representation learning in low dimensional vector space has attracted\nconsiderable attention in both academic and industrial domains. Most real-world\nnetworks are dynamic with addition/deletion of nodes and edges. The existing\ngraph embedding methods are designed for static networks and they cannot\ncapture evolving patterns in a large dynamic network. In this paper, we propose\na dynamic embedding method, dynnode2vec, based on the well-known graph\nembedding method node2vec. Node2vec is a random walk based embedding method for\nstatic networks. Applying static network embedding in dynamic settings has two\ncrucial problems: 1) Generating random walks for every time step is time\nconsuming 2) Embedding vector spaces in each timestamp are different. In order\nto tackle these challenges, dynnode2vec uses evolving random walks and\ninitializes the current graph embedding with previous embedding vectors. We\ndemonstrate the advantages of the proposed dynamic network embedding by\nconducting empirical evaluations on several large dynamic network datasets. \n\n"}
{"id": "1812.02633", "contents": "Title: MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Abstract: We consider the problem of handling missing data with deep latent variable\nmodels (DLVMs). First, we present a simple technique to train DLVMs when the\ntraining set contains missing-at-random data. Our approach, called MIWAE, is\nbased on the importance-weighted autoencoder (IWAE), and maximises a\npotentially tight lower bound of the log-likelihood of the observed data.\nCompared to the original IWAE, our algorithm does not induce any additional\ncomputational overhead due to the missing data. We also develop Monte Carlo\ntechniques for single and multiple imputation using a DLVM trained on an\nincomplete data set. We illustrate our approach by training a convolutional\nDLVM on a static binarisation of MNIST that contains 50% of missing pixels.\nLeveraging multiple imputation, a convolutional network trained on these\nincomplete digits has a test performance similar to one trained on complete\ndata. On various continuous and binary data sets, we also show that MIWAE\nprovides accurate single imputations, and is highly competitive with\nstate-of-the-art methods. \n\n"}
{"id": "1812.02768", "contents": "Title: SqueezeFit: Label-aware dimensionality reduction by semidefinite\n  programming Abstract: Given labeled points in a high-dimensional vector space, we seek a\nlow-dimensional subspace such that projecting onto this subspace maintains some\nprescribed distance between points of differing labels. Intended applications\ninclude compressive classification. Taking inspiration from large margin\nnearest neighbor classification, this paper introduces a semidefinite\nrelaxation of this problem. Unlike its predecessors, this relaxation is\namenable to theoretical analysis, allowing us to provably recover a planted\nprojection operator from the data. \n\n"}
{"id": "1812.03002", "contents": "Title: Scale-free network clustering in hyperbolic and other random graphs Abstract: Random graphs with power-law degrees can model scale-free networks as sparse\ntopologies with strong degree heterogeneity. Mathematical analysis of such\nrandom graphs proved successful in explaining scale-free network properties\nsuch as resilience, navigability and small distances. We introduce a\nvariational principle to explain how vertices tend to cluster in triangles as a\nfunction of their degrees. We apply the variational principle to the hyperbolic\nmodel that quickly gains popularity as a model for scale-free networks with\nlatent geometries and clustering. We show that clustering in the hyperbolic\nmodel is non-vanishing and self-averaging, so that a single random graph sample\nis a good representation in the large-network limit. We also demonstrate the\nvariational principle for some classical random graphs including the\npreferential attachment model and the configuration model. \n\n"}
{"id": "1812.03934", "contents": "Title: Stagewise Training Accelerates Convergence of Testing Error Over SGD Abstract: Stagewise training strategy is widely used for learning neural networks,\nwhich runs a stochastic algorithm (e.g., SGD) starting with a relatively large\nstep size (aka learning rate) and geometrically decreasing the step size after\na number of iterations. It has been observed that the stagewise SGD has much\nfaster convergence than the vanilla SGD with a polynomially decaying step size\nin terms of both training error and testing error. {\\it But how to explain this\nphenomenon has been largely ignored by existing studies.} This paper provides\nsome theoretical evidence for explaining this faster convergence. In\nparticular, we consider a stagewise training strategy for minimizing empirical\nrisk that satisfies the Polyak-\\L ojasiewicz (PL) condition, which has been\nobserved/proved for neural networks and also holds for a broad family of convex\nfunctions. For convex loss functions and two classes of \"nice-behaviored\"\nnon-convex objectives that are close to a convex function, we establish faster\nconvergence of stagewise training than the vanilla SGD under the PL condition\non both training error and testing error. Experiments on stagewise learning of\ndeep residual networks exhibits that it satisfies one type of non-convexity\nassumption and therefore can be explained by our theory. Of independent\ninterest, the testing error bounds for the considered non-convex loss functions\nare dimensionality and norm independent. \n\n"}
{"id": "1812.04202", "contents": "Title: Deep Learning on Graphs: A Survey Abstract: Deep learning has been shown to be successful in a number of domains, ranging\nfrom acoustics, images, to natural language processing. However, applying deep\nlearning to the ubiquitous graph data is non-trivial because of the unique\ncharacteristics of graphs. Recently, substantial research efforts have been\ndevoted to applying deep learning methods to graphs, resulting in beneficial\nadvances in graph analysis techniques. In this survey, we comprehensively\nreview the different types of deep learning methods on graphs. We divide the\nexisting methods into five categories based on their model architectures and\ntraining strategies: graph recurrent neural networks, graph convolutional\nnetworks, graph autoencoders, graph reinforcement learning, and graph\nadversarial methods. We then provide a comprehensive overview of these methods\nin a systematic manner mainly by following their development history. We also\nanalyze the differences and compositions of different methods. Finally, we\nbriefly outline the applications in which they have been used and discuss\npotential future research directions. \n\n"}
{"id": "1812.05796", "contents": "Title: AdaFlow: Domain-Adaptive Density Estimator with Application to Anomaly\n  Detection and Unpaired Cross-Domain Translation Abstract: We tackle unsupervised anomaly detection (UAD), a problem of detecting data\nthat significantly differ from normal data. UAD is typically solved by using\ndensity estimation. Recently, deep neural network (DNN)-based density\nestimators, such as Normalizing Flows, have been attracting attention. However,\none of their drawbacks is the difficulty in adapting them to the change in the\nnormal data's distribution. To address this difficulty, we propose AdaFlow, a\nnew DNN-based density estimator that can be easily adapted to the change of the\ndistribution. AdaFlow is a unified model of a Normalizing Flow and Adaptive\nBatch-Normalizations, a module that enables DNNs to adapt to new distributions.\nAdaFlow can be adapted to a new distribution by just conducting forward\npropagation once per sample; hence, it can be used on devices that have limited\ncomputational resources. We have confirmed the effectiveness of the proposed\nmodel through an anomaly detection in a sound task. We also propose a method of\napplying AdaFlow to the unpaired cross-domain translation problem, in which one\nhas to train a cross-domain translation model with only unpaired samples. We\nhave confirmed that our model can be used for the cross-domain translation\nproblem through experiments on image datasets. \n\n"}
{"id": "1812.06944", "contents": "Title: Domain Adaptation on Graphs by Learning Graph Topologies: Theoretical\n  Analysis and an Algorithm Abstract: Traditional machine learning algorithms assume that the training and test\ndata have the same distribution, while this assumption does not necessarily\nhold in real applications. Domain adaptation methods take into account the\ndeviations in the data distribution. In this work, we study the problem of\ndomain adaptation on graphs. We consider a source graph and a target graph\nconstructed with samples drawn from data manifolds. We study the problem of\nestimating the unknown class labels on the target graph using the label\ninformation on the source graph and the similarity between the two graphs. We\nparticularly focus on a setting where the target label function is learnt such\nthat its spectrum is similar to that of the source label function. We first\npropose a theoretical analysis of domain adaptation on graphs and present\nperformance bounds that characterize the target classification error in terms\nof the properties of the graphs and the data manifolds. We show that the\nclassification performance improves as the topologies of the graphs get more\nbalanced, i.e., as the numbers of neighbors of different graph nodes become\nmore proportionate, and weak edges with small weights are avoided. Our results\nalso suggest that graph edges between too distant data samples should be\navoided for good generalization performance. We then propose a graph domain\nadaptation algorithm inspired by our theoretical findings, which estimates the\nlabel functions while learning the source and target graph topologies at the\nsame time. The joint graph learning and label estimation problem is formulated\nthrough an objective function relying on our performance bounds, which is\nminimized with an alternating optimization scheme. Experiments on synthetic and\nreal data sets suggest that the proposed method outperforms baseline\napproaches. \n\n"}
{"id": "1812.07813", "contents": "Title: Matrix Completion under Low-Rank Missing Mechanism Abstract: Matrix completion is a modern missing data problem where both the missing\nstructure and the underlying parameter are high dimensional. Although missing\nstructure is a key component to any missing data problems, existing matrix\ncompletion methods often assume a simple uniform missing mechanism. In this\nwork, we study matrix completion from corrupted data under a novel low-rank\nmissing mechanism. The probability matrix of observation is estimated via a\nhigh dimensional low-rank matrix estimation procedure, and further used to\ncomplete the target matrix via inverse probabilities weighting. Due to both\nhigh dimensional and extreme (i.e., very small) nature of the true probability\nmatrix, the effect of inverse probability weighting requires careful study. We\nderive optimal asymptotic convergence rates of the proposed estimators for both\nthe observation probabilities and the target matrix. \n\n"}
{"id": "1812.08398", "contents": "Title: Low-rank Interaction with Sparse Additive Effects Model for Large Data\n  Frames Abstract: Many applications of machine learning involve the analysis of large data\nframes-matrices collecting heterogeneous measurements (binary, numerical,\ncounts, etc.) across samples-with missing values. Low-rank models, as studied\nby Udell et al. [30], are popular in this framework for tasks such as\nvisualization, clustering and missing value imputation. Yet, available methods\nwith statistical guarantees and efficient optimization do not allow explicit\nmodeling of main additive effects such as row and column, or covariate effects.\nIn this paper, we introduce a low-rank interaction and sparse additive effects\n(LORIS) model which combines matrix regression on a dictionary and low-rank\ndesign, to estimate main effects and interactions simultaneously. We provide\nstatistical guarantees in the form of upper bounds on the estimation error of\nboth components. Then, we introduce a mixed coordinate gradient descent (MCGD)\nmethod which provably converges sub-linearly to an optimal solution and is\ncomputationally efficient for large scale data sets. We show on simulated and\nsurvey data that the method has a clear advantage over current practices, which\nconsist in dealing separately with additive effects in a preprocessing step. \n\n"}
{"id": "1812.08425", "contents": "Title: A Survey of Hierarchy Identification in Social Networks Abstract: Humans are social by nature. Throughout history, people have formed\ncommunities and built relationships. Most relationships with coworkers,\nfriends, and family are developed during face-to-face interactions. These\nrelationships are established through explicit means of communications such as\nwords and implicit such as intonation, body language, etc. By analyzing human\ninteractions we can derive information about the relationships and influence\namong conversation participants. However, with the development of the Internet,\npeople started to communicate through text in online social networks.\nInterestingly, they brought their communicational habits to the Internet. Many\nsocial network users form relationships with each other and establish\ncommunities with leaders and followers. Recognizing these hierarchical\nrelationships is an important task because it will help to understand social\nnetworks and predict future trends, improve recommendations, better target\nadvertisement, and improve national security by identifying leaders of\nanonymous terror groups. In this work, I provide an overview of current\nresearch in this area and present the state-of-the-art approaches to deal with\nthe problem of identifying hierarchical relationships in social networks. \n\n"}
{"id": "1812.08733", "contents": "Title: Heteroscedastic Gaussian processes for uncertainty modeling in\n  large-scale crowdsourced traffic data Abstract: Accurately modeling traffic speeds is a fundamental part of efficient\nintelligent transportation systems. Nowadays, with the widespread deployment of\nGPS-enabled devices, it has become possible to crowdsource the collection of\nspeed information to road users (e.g. through mobile applications or dedicated\nin-vehicle devices). Despite its rather wide spatial coverage, crowdsourced\nspeed data also brings very important challenges, such as the highly variable\nmeasurement noise in the data due to a variety of driving behaviors and sample\nsizes. When not properly accounted for, this noise can severely compromise any\napplication that relies on accurate traffic data. In this article, we propose\nthe use of heteroscedastic Gaussian processes (HGP) to model the time-varying\nuncertainty in large-scale crowdsourced traffic data. Furthermore, we develop a\nHGP conditioned on sample size and traffic regime (SRC-HGP), which makes use of\nsample size information (probe vehicles per minute) as well as previous\nobserved speeds, in order to more accurately model the uncertainty in observed\nspeeds. Using 6 months of crowdsourced traffic data from Copenhagen, we\nempirically show that the proposed heteroscedastic models produce significantly\nbetter predictive distributions when compared to current state-of-the-art\nmethods for both speed imputation and short-term forecasting tasks. \n\n"}
{"id": "1812.09430", "contents": "Title: Dynamic Graph Representation Learning via Self-Attention Networks Abstract: Learning latent representations of nodes in graphs is an important and\nubiquitous task with widespread applications such as link prediction, node\nclassification, and graph visualization. Previous methods on graph\nrepresentation learning mainly focus on static graphs, however, many real-world\ngraphs are dynamic and evolve over time. In this paper, we present Dynamic\nSelf-Attention Network (DySAT), a novel neural architecture that operates on\ndynamic graphs and learns node representations that capture both structural\nproperties and temporal evolutionary patterns. Specifically, DySAT computes\nnode representations by jointly employing self-attention layers along two\ndimensions: structural neighborhood and temporal dynamics. We conduct link\nprediction experiments on two classes of graphs: communication networks and\nbipartite rating networks. Our experimental results show that DySAT has a\nsignificant performance gain over several different state-of-the-art graph\nembedding baselines. \n\n"}
{"id": "1812.09444", "contents": "Title: Deep autoregressive neural networks for high-dimensional inverse\n  problems in groundwater contaminant source identification Abstract: Identification of a groundwater contaminant source simultaneously with the\nhydraulic conductivity in highly-heterogeneous media often results in a\nhigh-dimensional inverse problem. In this study, a deep autoregressive neural\nnetwork-based surrogate method is developed for the forward model to allow us\nto solve efficiently such high-dimensional inverse problems. The surrogate is\ntrained using limited evaluations of the forward model. Since the relationship\nbetween the time-varying inputs and outputs of the forward transport model is\ncomplex, we propose an autoregressive strategy, which treats the output at the\nprevious time step as input to the network for predicting the output at the\ncurrent time step. We employ a dense convolutional encoder-decoder network\narchitecture in which the high-dimensional input and output fields of the model\nare treated as images to leverage the robust capability of convolutional\nnetworks in image-like data processing. An iterative local updating ensemble\nsmoother (ILUES) algorithm is used as the inversion framework. The proposed\nmethod is evaluated using a synthetic contaminant source identification problem\nwith 686 uncertain input parameters. Results indicate that, with relatively\nlimited training data, the deep autoregressive neural network consisting of 27\nconvolutional layers is capable of providing an accurate approximation for the\nhigh-dimensional model input-output relationship. The autoregressive strategy\nsubstantially improves the network's accuracy and computational efficiency. The\napplication of the surrogate-based ILUES in solving the inverse problem shows\nthat it can achieve accurate inversion results and predictive uncertainty\nestimates. \n\n"}
{"id": "1812.09747", "contents": "Title: Enhancing Discrete Choice Models with Representation Learning Abstract: In discrete choice modeling (DCM), model misspecifications may lead to\nlimited predictability and biased parameter estimates. In this paper, we\npropose a new approach for estimating choice models in which we divide the\nsystematic part of the utility specification into (i) a knowledge-driven part,\nand (ii) a data-driven one, which learns a new representation from available\nexplanatory variables. Our formulation increases the predictive power of\nstandard DCM without sacrificing their interpretability. We show the\neffectiveness of our formulation by augmenting the utility specification of the\nMultinomial Logit (MNL) and the Nested Logit (NL) models with a new non-linear\nrepresentation arising from a Neural Network (NN), leading to new choice models\nreferred to as the Learning Multinomial Logit (L-MNL) and Learning Nested Logit\n(L-NL) models. Using multiple publicly available datasets based on revealed and\nstated preferences, we show that our models outperform the traditional ones,\nboth in terms of predictive performance and accuracy in parameter estimation.\nAll source code of the models are shared to promote open science. \n\n"}
{"id": "1812.10869", "contents": "Title: Hypergraph Clustering: A Modularity Maximization Approach Abstract: Clustering on hypergraphs has been garnering increased attention with\npotential applications in network analysis, VLSI design and computer vision,\namong others. In this work, we generalize the framework of modularity\nmaximization for clustering on hypergraphs. To this end, we introduce a\nhypergraph null model, analogous to the configuration model on undirected\ngraphs, and a node-degree preserving reduction to work with this model. This is\nused to define a modularity function that can be maximized using the popular\nand fast Louvain algorithm. We additionally propose a refinement over this\nclustering, by reweighting cut hyperedges in an iterative fashion. The efficacy\nand efficiency of our methods are demonstrated on several real-world datasets. \n\n"}
{"id": "1812.11755", "contents": "Title: Approximate Inference for Multiplicative Latent Force Models Abstract: Latent force models are a class of hybrid models for dynamic systems,\ncombining simple mechanistic models with flexible Gaussian process (GP)\nperturbations. An extension of this framework to include multiplicative\ninteractions between the state and GP terms allows strong a priori control of\nthe model geometry at the expense of tractable inference. In this paper we\nconsider two methods of carrying out inference within this broader class of\nmodels. The first is based on an adaptive gradient matching approximation, and\nthe second is constructed around mixtures of local approximations to the\nsolution. We compare the performance of both methods on simulated data, and\nalso demonstrate an application of the multiplicative latent force model on\nmotion capture data. \n\n"}
{"id": "1901.00172", "contents": "Title: Supervised Multiscale Dimension Reduction for Spatial Interaction\n  Networks Abstract: We introduce a multiscale supervised dimension reduction method for SPatial\nInteraction Network (SPIN) data, which consist of a collection of spatially\ncoordinated interactions. This type of predictor arises when the sampling unit\nof data is composed of a collection of primitive variables, each of them being\nessentially unique, so that it becomes necessary to group the variables in\norder to simplify the representation and enhance interpretability. In this\npaper, we introduce an empirical Bayes approach called spinlets, which first\nconstructs a partitioning tree to guide the reduction over multiple spatial\ngranularities, and then refines the representation of predictors according to\nthe relevance to the response. We consider an inverse Poisson regression model\nand propose a new multiscale generalized double Pareto prior, which is induced\nvia a tree-structured parameter expansion scheme. Our approach is motivated by\nan application in soccer analytics, in which we obtain compact vectorial\nrepresentations and readily interpretable visualizations of the complex network\nobjects, supervised by the response of interest. \n\n"}
{"id": "1901.00740", "contents": "Title: Characterizing Long-Running Political Phenomena on Social Media Abstract: Social media provides many opportunities to monitor and evaluate political\nphenomena such as referendums and elections. In this study, we propose a set of\napproaches to analyze long-running political events on social media with a\nreal-world experiment: the debate about Brexit, i.e., the process through which\nthe United Kingdom activated the option of leaving the European Union. We\naddress the following research questions: Could Twitter-based stance\nclassification be used to demonstrate public stance with respect to political\nevents? What is the most efficient and comprehensive approach to measuring the\nimpact of politicians on social media? Which of the polarized sides of the\ndebate is more responsive to politician messages and the main issues of the\nBrexit process? What is the share of bot accounts in the Brexit discussion and\nwhich side are they for? By combining the user stance classification, topic\ndiscovery, sentiment analysis, and bot detection, we show that it is possible\nto obtain useful insights about political phenomena from social media data. We\nare able to detect relevant topics in the discussions, such as the demand for a\nnew referendum, and to understand the position of social media users with\nrespect to the different topics in the debate. Our comparative and temporal\nanalysis of political accounts can detect the critical periods of the Brexit\nprocess and the impact they have on the debate. \n\n"}
{"id": "1901.00781", "contents": "Title: Learning a Generator Model from Terminal Bus Data Abstract: In this work we investigate approaches to reconstruct generator models from\nmeasurements available at the generator terminal bus using machine learning\n(ML) techniques. The goal is to develop an emulator which is trained online and\nis capable of fast predictive computations. The training is illustrated on\nsynthetic data generated based on available open-source dynamical generator\nmodel. Two ML techniques were developed and tested: (a) standard vector\nauto-regressive (VAR) model; and (b) novel customized long short-term memory\n(LSTM) deep learning model. Trade-offs in reconstruction ability between\ncomputationally light but linear AR model and powerful but computationally\ndemanding LSTM model are established and analyzed. \n\n"}
{"id": "1901.01346", "contents": "Title: Efficient Representation Learning Using Random Walks for Dynamic Graphs Abstract: An important part of many machine learning workflows on graphs is vertex\nrepresentation learning, i.e., learning a low-dimensional vector representation\nfor each vertex in the graph. Recently, several powerful techniques for\nunsupervised representation learning have been demonstrated to give the\nstate-of-the-art performance in downstream tasks such as vertex classification\nand edge prediction. These techniques rely on random walks performed on the\ngraph in order to capture its structural properties. These structural\nproperties are then encoded in the vector representation space.\n  However, most contemporary representation learning methods only apply to\nstatic graphs while real-world graphs are often dynamic and change over time.\nStatic representation learning methods are not able to update the vector\nrepresentations when the graph changes; therefore, they must re-generate the\nvector representations on an updated static snapshot of the graph regardless of\nthe extent of the change in the graph. In this work, we propose computationally\nefficient algorithms for vertex representation learning that extend random walk\nbased methods to dynamic graphs. The computation complexity of our algorithms\ndepends upon the extent and rate of changes (the number of edges changed per\nupdate) and on the density of the graph. We empirically evaluate our algorithms\non real world datasets for downstream machine learning tasks of multi-class and\nmulti-label vertex classification. The results show that our algorithms can\nachieve competitive results to the state-of-the-art methods while being\ncomputationally efficient. \n\n"}
{"id": "1901.02166", "contents": "Title: K-Core Minimization: A Game Theoretic Approach Abstract: K-cores are maximal induced subgraphs where all vertices have degree at least\nk. These dense patterns have applications in community detection, network\nvisualization and protein function prediction. However, k-cores can be quite\nunstable to network modifications, which motivates the question: How resilient\nis the k-core structure of a network, such as the Web or Facebook, to edge\ndeletions? We investigate this question from an algorithmic perspective. More\nspecifically, we study the problem of computing a small set of edges for which\nthe removal minimizes the $k$-core structure of a network. This paper provides\na comprehensive characterization of the hardness of the k-core minimization\nproblem (KCM), including innaproximability and fixed-parameter intractability.\nMotivated by such a challenge in terms of algorithm design, we propose a novel\nalgorithm inspired by Shapley value -- a cooperative game-theoretic concept --\nthat is able to leverage the strong interdependencies in the effects of edge\nremovals in the search space. As computing Shapley values is also NP-hard, we\nefficiently approximate them using a randomized algorithm with probabilistic\nguarantees. Our experiments, using several real datasets, show that the\nproposed algorithm outperforms competing solutions in terms of k-core\nminimization while being able to handle large graphs. Moreover, we illustrate\nhow KCM can be applied in the analysis of the k-core resilience of networks. \n\n"}
{"id": "1901.02353", "contents": "Title: On neighbourhood degree sequences of complex networks Abstract: Network topology is a fundamental aspect of network science that allows us to\ngather insights into the complicated relational architectures of the world we\ninhabit. We provide a first specific study of neighbourhood degree sequences in\ncomplex networks. We consider how to explicitly characterise important physical\nconcepts such as similarity, heterogeneity and organisation in these sequences,\nas well as updating the notion of hierarchical complexity to reflect previously\nunnoticed organisational principles. We also point out that neighbourhood\ndegree sequences are related to a powerful subtree kernel for unlabelled graph\nclassification. We study these newly defined sequence properties in a\ncomprehensive array of graph models and over 200 real-world networks. We find\nthat these indices are neither highly correlated with each other nor with\nclassical network indices. Importantly, the sequences of a wide variety of real\nworld networks are found to have greater similarity and organisation than is\nexpected for networks of their given degree distributions. Notably, while\nbiological, social and technological networks all showed consistently large\nneighbourhood similarity and organisation, hierarchical complexity was not a\nconsistent feature of real world networks. Neighbourhood degree sequences are\nan interesting tool for describing unique and important characteristics of\ncomplex networks. \n\n"}
{"id": "1901.03209", "contents": "Title: Variable Importance Clouds: A Way to Explore Variable Importance for the\n  Set of Good Models Abstract: Variable importance is central to scientific studies, including the social\nsciences and causal inference, healthcare, and other domains. However, current\nnotions of variable importance are often tied to a specific predictive model.\nThis is problematic: what if there were multiple well-performing predictive\nmodels, and a specific variable is important to some of them and not to others?\nIn that case, we may not be able to tell from a single well-performing model\nwhether a variable is always important in predicting the outcome. Rather than\ndepending on variable importance for a single predictive model, we would like\nto explore variable importance for all approximately-equally-accurate\npredictive models. This work introduces the concept of a variable importance\ncloud, which maps every variable to its importance for every good predictive\nmodel. We show properties of the variable importance cloud and draw connections\nto other areas of statistics. We introduce variable importance diagrams as a\nprojection of the variable importance cloud into two dimensions for\nvisualization purposes. Experiments with criminal justice, marketing data, and\nimage classification tasks illustrate how variables can change dramatically in\nimportance for approximately-equally-accurate predictive models \n\n"}
{"id": "1901.04436", "contents": "Title: Bayesian Learning of Neural Network Architectures Abstract: In this paper we propose a Bayesian method for estimating architectural\nparameters of neural networks, namely layer size and network depth. We do this\nby learning concrete distributions over these parameters. Our results show that\nregular networks with a learnt structure can generalise better on small\ndatasets, while fully stochastic networks can be more robust to parameter\ninitialisation. The proposed method relies on standard neural variational\nlearning and, unlike randomised architecture search, does not require a\nretraining of the model, thus keeping the computational overhead at minimum. \n\n"}
{"id": "1901.04653", "contents": "Title: Normalized Flat Minima: Exploring Scale Invariant Definition of Flat\n  Minima for Neural Networks using PAC-Bayesian Analysis Abstract: The notion of flat minima has played a key role in the generalization studies\nof deep learning models. However, existing definitions of the flatness are\nknown to be sensitive to the rescaling of parameters. The issue suggests that\nthe previous definitions of the flatness might not be a good measure of\ngeneralization, because generalization is invariant to such rescalings. In this\npaper, from the PAC-Bayesian perspective, we scrutinize the discussion\nconcerning the flat minima and introduce the notion of normalized flat minima,\nwhich is free from the known scale dependence issues. Additionally, we\nhighlight the scale dependence of existing matrix-norm based generalization\nerror bounds similar to the existing flat minima definitions. Our modified\nnotion of the flatness does not suffer from the insufficiency, either,\nsuggesting it might provide better hierarchy in the hypothesis class. \n\n"}
{"id": "1901.04747", "contents": "Title: Spectral estimation for detecting low-dimensional structure in networks\n  using arbitrary null models Abstract: Discovering low-dimensional structure in real-world networks requires a\nsuitable null model that defines the absence of meaningful structure. Here we\nintroduce a spectral approach for detecting a network's low-dimensional\nstructure, and the nodes that participate in it, using any null model. We use\ngenerative models to estimate the expected eigenvalue distribution under a\nspecified null model, and then detect where the data network's eigenspectra\nexceed the estimated bounds. On synthetic networks, this spectral estimation\napproach cleanly detects transitions between random and community structure,\nrecovers the number and membership of communities, and removes noise nodes. On\nreal networks spectral estimation finds either a significant fraction of noise\nnodes or no departure from a null model, in stark contrast to traditional\ncommunity detection methods. Across all analyses, we find the choice of null\nmodel can strongly alter conclusions about the presence of network structure.\nOur spectral estimation approach is therefore a promising basis for detecting\nlow-dimensional structure in real-world networks, or lack thereof. \n\n"}
{"id": "1901.05562", "contents": "Title: Differentially-Private Two-Party Egocentric Betweenness Centrality Abstract: We describe a novel protocol for computing the egocentric betweenness\ncentrality of a node when relevant edge information is spread between two\nmutually distrusting parties such as two telecommunications providers. While\neach node belongs to one network or the other, its ego network might include\nedges unknown to its network provider. We develop a protocol of\ndifferentially-private mechanisms to hide each network's internal edge\nstructure from the other; and contribute a new two-stage stratified sampler for\nexponential improvement to time and space efficiency. Empirical results on\nseveral open graph data sets demonstrate practical relative error rates while\ndelivering strong privacy guarantees, such as 16% error on a Facebook data set. \n\n"}
{"id": "1901.05947", "contents": "Title: Stochastic Gradient Descent on a Tree: an Adaptive and Robust Approach\n  to Stochastic Convex Optimization Abstract: Online minimization of an unknown convex function over the interval $[0,1]$\nis considered under first-order stochastic bandit feedback, which returns a\nrandom realization of the gradient of the function at each query point. Without\nknowing the distribution of the random gradients, a learning algorithm\nsequentially chooses query points with the objective of minimizing regret\ndefined as the expected cumulative loss of the function values at the query\npoints in excess to the minimum value of the function. An approach based on\ndevising a biased random walk on an infinite-depth binary tree constructed\nthrough successive partitioning of the domain of the function is developed.\nEach move of the random walk is guided by a sequential test based on confidence\nbounds on the empirical mean constructed using the law of the iterated\nlogarithm. With no tuning parameters, this learning algorithm is robust to\nheavy-tailed noise with infinite variance and adaptive to unknown function\ncharacteristics (specifically, convex, strongly convex, and nonsmooth). It\nachieves the corresponding optimal regret orders (up to a $\\sqrt{\\log T}$ or a\n$\\log\\log T$ factor) in each class of functions and offers better or matching\nregret orders than the classical stochastic gradient descent approach which\nrequires the knowledge of the function characteristics for tuning the sequence\nof step-sizes. \n\n"}
{"id": "1901.06003", "contents": "Title: Gromov-Wasserstein Learning for Graph Matching and Node Embedding Abstract: A novel Gromov-Wasserstein learning framework is proposed to jointly match\n(align) graphs and learn embedding vectors for the associated graph nodes.\nUsing Gromov-Wasserstein discrepancy, we measure the dissimilarity between two\ngraphs and find their correspondence, according to the learned optimal\ntransport. The node embeddings associated with the two graphs are learned under\nthe guidance of the optimal transport, the distance of which not only reflects\nthe topological structure of each graph but also yields the correspondence\nacross the graphs. These two learning steps are mutually-beneficial, and are\nunified here by minimizing the Gromov-Wasserstein discrepancy with structural\nregularizers. This framework leads to an optimization problem that is solved by\na proximal point method. We apply the proposed method to matching problems in\nreal-world networks, and demonstrate its superior performance compared to\nalternative approaches. \n\n"}
{"id": "1901.06116", "contents": "Title: Nonconvex Rectangular Matrix Completion via Gradient Descent without\n  $\\ell_{2,\\infty}$ Regularization Abstract: The analysis of nonconvex matrix completion has recently attracted much\nattention in the community of machine learning thanks to its computational\nconvenience. Existing analysis on this problem, however, usually relies on\n$\\ell_{2,\\infty}$ projection or regularization that involves unknown model\nparameters, although they are observed to be unnecessary in numerical\nsimulations, see, e.g., Zheng and Lafferty [2016]. In this paper, we extend the\nanalysis of the vanilla gradient descent for positive semidefinite matrix\ncompletion proposed in Ma et al. [2017] to the rectangular case, and more\nsignificantly, improve the required sampling rate from\n$O(\\operatorname{poly}(\\kappa)\\mu^3 r^3 \\log^3 n/n )$ to $O(\\mu^2 r^2\n\\kappa^{14} \\log n/n )$. Our technical ideas and contributions are potentially\nuseful in improving the leave-one-out analysis in other related problems. \n\n"}
{"id": "1901.06437", "contents": "Title: Combating Fake News: A Survey on Identification and Mitigation\n  Techniques Abstract: The proliferation of fake news on social media has opened up new directions\nof research for timely identification and containment of fake news, and\nmitigation of its widespread impact on public opinion. While much of the\nearlier research was focused on identification of fake news based on its\ncontents or by exploiting users' engagements with the news on social media,\nthere has been a rising interest in proactive intervention strategies to\ncounter the spread of misinformation and its impact on society. In this survey,\nwe describe the modern-day problem of fake news and, in particular, highlight\nthe technical challenges associated with it. We discuss existing methods and\ntechniques applicable to both identification and mitigation, with a focus on\nthe significant advances in each method and their advantages and limitations.\nIn addition, research has often been limited by the quality of existing\ndatasets and their specific application contexts. To alleviate this problem, we\ncomprehensively compile and summarize characteristic features of available\ndatasets. Furthermore, we outline new directions of research to facilitate\nfuture development of effective and interdisciplinary solutions. \n\n"}
{"id": "1901.07445", "contents": "Title: Accelerated Linear Convergence of Stochastic Momentum Methods in\n  Wasserstein Distances Abstract: Momentum methods such as Polyak's heavy ball (HB) method, Nesterov's\naccelerated gradient (AG) as well as accelerated projected gradient (APG)\nmethod have been commonly used in machine learning practice, but their\nperformance is quite sensitive to noise in the gradients. We study these\nmethods under a first-order stochastic oracle model where noisy estimates of\nthe gradients are available. For strongly convex problems, we show that the\ndistribution of the iterates of AG converges with the accelerated\n$O(\\sqrt{\\kappa}\\log(1/\\varepsilon))$ linear rate to a ball of radius\n$\\varepsilon$ centered at a unique invariant distribution in the 1-Wasserstein\nmetric where $\\kappa$ is the condition number as long as the noise variance is\nsmaller than an explicit upper bound we can provide. Our analysis also\ncertifies linear convergence rates as a function of the stepsize, momentum\nparameter and the noise variance; recovering the accelerated rates in the\nnoiseless case and quantifying the level of noise that can be tolerated to\nachieve a given performance. In the special case of strongly convex quadratic\nobjectives, we can show accelerated linear rates in the $p$-Wasserstein metric\nfor any $p\\geq 1$ with improved sensitivity to noise for both AG and HB through\na non-asymptotic analysis under some additional assumptions on the noise\nstructure. Our analysis for HB and AG also leads to improved non-asymptotic\nconvergence bounds in suboptimality for both deterministic and stochastic\nsettings which is of independent interest. To the best of our knowledge, these\nare the first linear convergence results for stochastic momentum methods under\nthe stochastic oracle model. We also extend our results to the APG method and\nweakly convex functions showing accelerated rates when the noise magnitude is\nsufficiently small. \n\n"}
{"id": "1901.07922", "contents": "Title: Incremental Principal Component Analysis Exact implementation and\n  continuity corrections Abstract: This paper describes some applications of an incremental implementation of\nthe principal component analysis (PCA). The algorithm updates the\ntransformation coefficients matrix on-line for each new sample, without the\nneed to keep all the samples in memory. The algorithm is formally equivalent to\nthe usual batch version, in the sense that given a sample set the\ntransformation coefficients at the end of the process are the same. The\nimplications of applying the PCA in real time are discussed with the help of\ndata analysis examples. In particular we focus on the problem of the continuity\nof the PCs during an on-line analysis. \n\n"}
{"id": "1901.08096", "contents": "Title: Recurrent Neural Filters: Learning Independent Bayesian Filtering Steps\n  for Time Series Prediction Abstract: Despite the recent popularity of deep generative state space models, few\ncomparisons have been made between network architectures and the inference\nsteps of the Bayesian filtering framework -- with most models simultaneously\napproximating both state transition and update steps with a single recurrent\nneural network (RNN). In this paper, we introduce the Recurrent Neural Filter\n(RNF), a novel recurrent autoencoder architecture that learns distinct\nrepresentations for each Bayesian filtering step, captured by a series of\nencoders and decoders. Testing this on three real-world time series datasets,\nwe demonstrate that the decoupled representations learnt not only improve the\naccuracy of one-step-ahead forecasts while providing realistic uncertainty\nestimates, but also facilitate multistep prediction through the separation of\nencoder stages. \n\n"}
{"id": "1901.08255", "contents": "Title: Confidence-based Graph Convolutional Networks for Semi-Supervised\n  Learning Abstract: Predicting properties of nodes in a graph is an important problem with\napplications in a variety of domains. Graph-based Semi-Supervised Learning\n(SSL) methods aim to address this problem by labeling a small subset of the\nnodes as seeds and then utilizing the graph structure to predict label scores\nfor the rest of the nodes in the graph. Recently, Graph Convolutional Networks\n(GCNs) have achieved impressive performance on the graph-based SSL task. In\naddition to label scores, it is also desirable to have confidence scores\nassociated with them. Unfortunately, confidence estimation in the context of\nGCN has not been previously explored. We fill this important gap in this paper\nand propose ConfGCN, which estimates labels scores along with their confidences\njointly in GCN-based setting. ConfGCN uses these estimated confidences to\ndetermine the influence of one node on another during neighborhood aggregation,\nthereby acquiring anisotropic capabilities. Through extensive analysis and\nexperiments on standard benchmarks, we find that ConfGCN is able to outperform\nstate-of-the-art baselines. We have made ConfGCN's source code available to\nencourage reproducible research. \n\n"}
{"id": "1901.08585", "contents": "Title: Graph heat mixture model learning Abstract: Graph inference methods have recently attracted a great interest from the\nscientific community, due to the large value they bring in data interpretation\nand analysis. However, most of the available state-of-the-art methods focus on\nscenarios where all available data can be explained through the same graph, or\ngroups corresponding to each graph are known a priori. In this paper, we argue\nthat this is not always realistic and we introduce a generative model for mixed\nsignals following a heat diffusion process on multiple graphs. We propose an\nexpectation-maximisation algorithm that can successfully separate signals into\ncorresponding groups, and infer multiple graphs that govern their behaviour. We\ndemonstrate the benefits of our method on both synthetic and real data. \n\n"}
{"id": "1901.08941", "contents": "Title: Computational landscape of user behavior on social media Abstract: With the increasing abundance of 'digital footprints' left by human\ninteractions in online environments, e.g., social media and app use, the\nability to model complex human behavior has become increasingly possible. Many\napproaches have been proposed, however, most previous model frameworks are\nfairly restrictive. We introduce a new social modeling approach that enables\nthe creation of models directly from data with minimal a priori restrictions on\nthe model class. In particular, we infer the minimally complex, maximally\npredictive representation of an individual's behavior when viewed in isolation\nand as driven by a social input. We then apply this framework to a\nheterogeneous catalog of human behavior collected from fifteen thousand users\non the microblogging platform Twitter. The models allow us to describe how a\nuser processes their past behavior and their social inputs. Despite the\ndiversity of observed user behavior, most models inferred fall into a small\nsubclass of all possible finite-state processes. Thus, our work demonstrates\nthat user behavior, while quite complex, belies simple underlying computational\nstructures. \n\n"}
{"id": "1901.09021", "contents": "Title: Complexity of Linear Regions in Deep Networks Abstract: It is well-known that the expressivity of a neural network depends on its\narchitecture, with deeper networks expressing more complex functions. In the\ncase of networks that compute piecewise linear functions, such as those with\nReLU activation, the number of distinct linear regions is a natural measure of\nexpressivity. It is possible to construct networks with merely a single region,\nor for which the number of linear regions grows exponentially with depth; it is\nnot clear where within this range most networks fall in practice, either before\nor after training. In this paper, we provide a mathematical framework to count\nthe number of linear regions of a piecewise linear network and measure the\nvolume of the boundaries between these regions. In particular, we prove that\nfor networks at initialization, the average number of regions along any\none-dimensional subspace grows linearly in the total number of neurons, far\nbelow the exponential upper bound. We also find that the average distance to\nthe nearest region boundary at initialization scales like the inverse of the\nnumber of neurons. Our theory suggests that, even after training, the number of\nlinear regions is far below exponential, an intuition that matches our\nempirical observations. We conclude that the practical expressivity of neural\nnetworks is likely far below that of the theoretical maximum, and that this gap\ncan be quantified. \n\n"}
{"id": "1901.09680", "contents": "Title: The Intrinsic Scale of Networks is Small Abstract: We define the intrinsic scale at which a network begins to reveal its\nidentity as the scale at which subgraphs in the network (created by a random\nwalk) are distinguishable from similar sized subgraphs in a perturbed copy of\nthe network. We conduct an extensive study of intrinsic scale for several\nnetworks, ranging from structured (e.g. road networks) to ad-hoc and\nunstructured (e.g. crowd sourced information networks), to biological. We find:\n(a) The intrinsic scale is surprisingly small (7-20 vertices), even though the\nnetworks are many orders of magnitude larger. (b) The intrinsic scale\nquantifies ``structure'' in a network -- networks which are explicitly\nconstructed for specific tasks have smaller intrinsic scale. (c) The structure\nat different scales can be fragile (easy to disrupt) or robust. \n\n"}
{"id": "1901.09681", "contents": "Title: Network Lens: Node Classification in Topologically Heterogeneous\n  Networks Abstract: We study the problem of identifying different behaviors occurring in\ndifferent parts of a large heterogenous network. We zoom in to the network\nusing lenses of different sizes to capture the local structure of the network.\nThese network signatures are then weighted to provide a set of predicted labels\nfor every node. We achieve a peak accuracy of $\\sim42\\%$ (random=$11\\%$) on two\nnetworks with $\\sim100,000$ and $\\sim1,000,000$ nodes each. Further, we perform\nbetter than random even when the given node is connected to up to 5 different\ntypes of networks. Finally, we perform this analysis on homogeneous networks\nand show that highly structured networks have high homogeneity. \n\n"}
{"id": "1901.09839", "contents": "Title: Interpreting Deep Neural Networks Through Variable Importance Abstract: While the success of deep neural networks (DNNs) is well-established across a\nvariety of domains, our ability to explain and interpret these methods is\nlimited. Unlike previously proposed local methods which try to explain\nparticular classification decisions, we focus on global interpretability and\nask a universally applicable question: given a trained model, which features\nare the most important? In the context of neural networks, a feature is rarely\nimportant on its own, so our strategy is specifically designed to leverage\npartial covariance structures and incorporate variable dependence into feature\nranking. Our methodological contributions in this paper are two-fold. First, we\npropose an effect size analogue for DNNs that is appropriate for applications\nwith highly collinear predictors (ubiquitous in computer vision). Second, we\nextend the recently proposed \"RelATive cEntrality\" (RATE) measure (Crawford et\nal., 2019) to the Bayesian deep learning setting. RATE applies an information\ntheoretic criterion to the posterior distribution of effect sizes to assess\nfeature significance. We apply our framework to three broad application areas:\ncomputer vision, natural language processing, and social science. \n\n"}
{"id": "1901.10234", "contents": "Title: Representation Learning for Heterogeneous Information Networks via\n  Embedding Events Abstract: Network representation learning (NRL) has been widely used to help analyze\nlarge-scale networks through mapping original networks into a low-dimensional\nvector space. However, existing NRL methods ignore the impact of properties of\nrelations on the object relevance in heterogeneous information networks (HINs).\nTo tackle this issue, this paper proposes a new NRL framework, called\nEvent2vec, for HINs to consider both quantities and properties of relations\nduring the representation learning process. Specifically, an event (i.e., a\ncomplete semantic unit) is used to represent the relation among multiple\nobjects, and both event-driven first-order and second-order proximities are\ndefined to measure the object relevance according to the quantities and\nproperties of relations. We theoretically prove how event-driven proximities\ncan be preserved in the embedding space by Event2vec, which utilizes event\nembeddings to facilitate learning the object embeddings. Experimental studies\ndemonstrate the advantages of Event2vec over state-of-the-art algorithms on\nfour real-world datasets and three network analysis tasks (including network\nreconstruction, link prediction, and node classification). \n\n"}
{"id": "1901.10334", "contents": "Title: Rank-one Convexification for Sparse Regression Abstract: Sparse regression models are increasingly prevalent due to their ease of\ninterpretability and superior out-of-sample performance. However, the exact\nmodel of sparse regression with an $\\ell_0$ constraint restricting the support\nof the estimators is a challenging (\\NP-hard) non-convex optimization problem.\nIn this paper, we derive new strong convex relaxations for sparse regression.\nThese relaxations are based on the ideal (convex-hull) formulations for\nrank-one quadratic terms with indicator variables. The new relaxations can be\nformulated as semidefinite optimization problems in an extended space and are\nstronger and more general than the state-of-the-art formulations, including the\nperspective reformulation and formulations with the reverse Huber penalty and\nthe minimax concave penalty functions. Furthermore, the proposed rank-one\nstrengthening can be interpreted as a \\textit{non-separable, non-convex,\nunbiased} sparsity-inducing regularizer, which dynamically adjusts its penalty\naccording to the shape of the error function without inducing bias for the\nsparse solutions. In our computational experiments with benchmark datasets, the\nproposed conic formulations are solved within seconds and result in\nnear-optimal solutions (with 0.4\\% optimality gap) for non-convex\n$\\ell_0$-problems. Moreover, the resulting estimators also outperform\nalternative convex approaches from a statistical perspective, achieving high\nprediction accuracy and good interpretability. \n\n"}
{"id": "1901.10426", "contents": "Title: Kernel embedded nonlinear observational mappings in the variational\n  mapping particle filter Abstract: Recently, some works have suggested methods to combine variational\nprobabilistic inference with Monte Carlo sampling. One promising approach is\nvia local optimal transport. In this approach, a gradient steepest descent\nmethod based on local optimal transport principles is formulated to transform\ndeterministically point samples from an intermediate density to a posterior\ndensity. The local mappings that transform the intermediate densities are\nembedded in a reproducing kernel Hilbert space (RKHS). This variational mapping\nmethod requires the evaluation of the log-posterior density gradient and\ntherefore the adjoint of the observational operator. In this work, we evaluate\nnonlinear observational mappings in the variational mapping method using two\napproximations that avoid the adjoint, an ensemble based approximation in which\nthe gradient is approximated by the particle covariances in the state and\nobservational spaces the so-called ensemble space and an RKHS approximation in\nwhich the observational mapping is embedded in an RKHS and the gradient is\nderived there. The approximations are evaluated for highly nonlinear\nobservational operators and in a low-dimensional chaotic dynamical system. The\nRKHS approximation is shown to be highly successful and superior to the\nensemble approximation. \n\n"}
{"id": "1901.10655", "contents": "Title: On the Calibration of Multiclass Classification with Rejection Abstract: We investigate the problem of multiclass classification with rejection, where\na classifier can choose not to make a prediction to avoid critical\nmisclassification. First, we consider an approach based on simultaneous\ntraining of a classifier and a rejector, which achieves the state-of-the-art\nperformance in the binary case. We analyze this approach for the multiclass\ncase and derive a general condition for calibration to the Bayes-optimal\nsolution, which suggests that calibration is hard to achieve by general loss\nfunctions unlike the binary case. Next, we consider another traditional\napproach based on confidence scores, in which the existing work focuses on a\nspecific class of losses. We propose rejection criteria for more general losses\nfor this approach and guarantee calibration to the Bayes-optimal solution.\nFinally, we conduct experiments to validate the relevance of our theoretical\nfindings. \n\n"}
{"id": "1901.10902", "contents": "Title: InfoBot: Transfer and Exploration via the Information Bottleneck Abstract: A central challenge in reinforcement learning is discovering effective\npolicies for tasks where rewards are sparsely distributed. We postulate that in\nthe absence of useful reward signals, an effective exploration strategy should\nseek out {\\it decision states}. These states lie at critical junctions in the\nstate space from where the agent can transition to new, potentially unexplored\nregions. We propose to learn about decision states from prior experience. By\ntraining a goal-conditioned policy with an information bottleneck, we can\nidentify decision states by examining where the model actually leverages the\ngoal state. We find that this simple mechanism effectively identifies decision\nstates, even in partially observed settings. In effect, the model learns the\nsensory cues that correlate with potential subgoals. In new environments, this\nmodel can then identify novel subgoals for further exploration, guiding the\nagent through a sequence of potential decision states and through new regions\nof the state space. \n\n"}
{"id": "1901.11213", "contents": "Title: Multi-GCN: Graph Convolutional Networks for Multi-View Networks, with\n  Applications to Global Poverty Abstract: With the rapid expansion of mobile phone networks in developing countries,\nlarge-scale graph machine learning has gained sudden relevance in the study of\nglobal poverty. Recent applications range from humanitarian response and\npoverty estimation to urban planning and epidemic containment. Yet the vast\nmajority of computational tools and algorithms used in these applications do\nnot account for the multi-view nature of social networks: people are related in\nmyriad ways, but most graph learning models treat relations as binary. In this\npaper, we develop a graph-based convolutional network for learning on\nmulti-view networks. We show that this method outperforms state-of-the-art\nsemi-supervised learning algorithms on three different prediction tasks using\nmobile phone datasets from three different developing countries. We also show\nthat, while designed specifically for use in poverty research, the algorithm\nalso outperforms existing benchmarks on a broader set of learning tasks on\nmulti-view networks, including node labelling in citation networks. \n\n"}
{"id": "1901.11281", "contents": "Title: Conversational Networks for Automatic Online Moderation Abstract: Moderation of user-generated content in an online community is a challenge\nthat has great socio-economical ramifications. However, the costs incurred by\ndelegating this work to human agents are high. For this reason, an automatic\nsystem able to detect abuse in user-generated content is of great interest.\nThere are a number of ways to tackle this problem, but the most commonly seen\nin practice are word filtering or regular expression matching. The main\nlimitations are their vulnerability to intentional obfuscation on the part of\nthe users, and their context-insensitive nature. Moreover, they are\nlanguage-dependent and may require appropriate corpora for training. In this\npaper, we propose a system for automatic abuse detection that completely\ndisregards message content. We first extract a conversational network from raw\nchat logs and characterize it through topological measures. We then use these\nas features to train a classifier on our abuse detection task. We thoroughly\nassess our system on a dataset of user comments originating from a French\nMassively Multiplayer Online Game. We identify the most appropriate network\nextraction parameters and discuss the discriminative power of our features,\nrelatively to their topological and temporal nature. Our method reaches an\nF-measure of 83.89 when using the full feature set, improving on existing\napproaches. With a selection of the most discriminative features, we\ndramatically cut computing time while retaining most of the performance\n(82.65). \n\n"}
{"id": "1901.11463", "contents": "Title: Distributionally Robust Removal of Malicious Nodes from Networks Abstract: An important problem in networked systems is detection and removal of\nsuspected malicious nodes. A crucial consideration in such settings is the\nuncertainty endemic in detection, coupled with considerations of network\nconnectivity, which impose indirect costs from mistakely removing benign nodes\nas well as failing to remove malicious nodes. A recent approach proposed to\naddress this problem directly tackles these considerations, but has a\nsignificant limitation: it assumes that the decision maker has accurate\nknowledge of the joint maliciousness probability of the nodes on the network.\nThis is clearly not the case in practice, where such a distribution is at best\nan estimate from limited evidence. To address this problem, we propose a\ndistributionally robust framework for optimal node removal. While the problem\nis NP-Hard, we propose a principled algorithmic technique for solving it\napproximately based on duality combined with Semidefinite Programming\nrelaxation. A combination of both theoretical and empirical analysis, the\nlatter using both synthetic and real data, provide strong evidence that our\nalgorithmic approach is highly effective and, in particular, is significantly\nmore robust than the state of the art. \n\n"}
